WEBVTT

00:00.000 --> 00:02.920
 The following is a conversation with Jim Keller,

00:02.920 --> 00:04.960
 his second time in the podcast.

00:04.960 --> 00:08.480
 Jim is a legendary microprocessor architect

00:08.480 --> 00:11.080
 and is widely seen as one of the greatest

00:11.080 --> 00:14.640
 engineering minds of the computing age.

00:14.640 --> 00:18.840
 In a peculiar twist of space time in our simulation,

00:18.840 --> 00:22.200
 Jim is also a brother in law of Jordan Peterson.

00:22.200 --> 00:25.320
 We talk about this and about computing,

00:25.320 --> 00:29.200
 artificial intelligence, consciousness, and life.

00:29.200 --> 00:31.280
 Quick mention of our sponsors.

00:31.280 --> 00:33.800
 Athletic Greens all in one nutrition drink,

00:33.800 --> 00:36.640
 Brooklyn and Sheets, ExpressVPN,

00:36.640 --> 00:39.640
 and Bell Campo grass fed meat.

00:39.640 --> 00:41.720
 Click the sponsor links to get a discount

00:41.720 --> 00:43.960
 and to support this podcast.

00:43.960 --> 00:46.240
 As a side note, let me say that Jim is someone

00:46.240 --> 00:50.200
 who on a personal level inspired me to be myself.

00:50.200 --> 00:53.360
 There was something in his words on and off the mic

00:53.360 --> 00:56.240
 or perhaps that he even paid attention to me at all

00:56.240 --> 00:59.160
 that almost told me, you're all right kid.

00:59.160 --> 01:01.840
 A kind of pat on the back that can make the difference

01:01.840 --> 01:03.640
 between the mind that flourishes

01:03.640 --> 01:05.760
 and a mind that is broken down

01:05.760 --> 01:08.160
 by the cynicism of the world.

01:08.160 --> 01:10.440
 So I guess that's just my brief few words

01:10.440 --> 01:12.800
 of thank you to Jim and in general,

01:12.800 --> 01:15.480
 gratitude for the people who have given me a chance

01:15.480 --> 01:19.000
 on this podcast and my work and in life.

01:19.000 --> 01:21.200
 If you enjoy this thing, subscribe on YouTube,

01:21.200 --> 01:24.280
 review it on Apple podcast, follow on Spotify,

01:24.280 --> 01:26.400
 support on Patreon or connect with me

01:26.400 --> 01:28.600
 on Twitter, Alex Friedman.

01:28.600 --> 01:32.380
 And now here's my conversation with Jim Keller.

01:33.400 --> 01:35.920
 What's the value and effectiveness of theory

01:35.920 --> 01:38.120
 versus engineering, this dichotomy

01:38.120 --> 01:43.120
 in building good software or hardware systems?

01:43.440 --> 01:45.560
 Well, it's good designs both.

01:46.480 --> 01:48.720
 I guess that's pretty obvious.

01:48.720 --> 01:50.840
 But engineering, do you mean, you know,

01:50.840 --> 01:53.280
 reduction of practice of known methods?

01:53.280 --> 01:55.960
 And then science is the pursuit of discovering things

01:55.960 --> 01:57.800
 that people don't understand

01:57.800 --> 02:00.360
 or solving unknown problems.

02:00.360 --> 02:02.000
 Definitions are interesting here,

02:02.000 --> 02:04.160
 but I was thinking more in theory,

02:04.160 --> 02:06.760
 constructing models that kind of generalize

02:06.760 --> 02:08.480
 about how things work.

02:08.480 --> 02:12.800
 And engineering is actually building stuff,

02:12.800 --> 02:16.220
 the pragmatic like, okay, we have these nice models,

02:16.220 --> 02:17.960
 but how do we actually get things to work?

02:17.960 --> 02:20.800
 Maybe economics is a nice example.

02:20.800 --> 02:22.480
 Like economists have all these models

02:22.480 --> 02:23.680
 of how the economy works

02:23.680 --> 02:26.720
 and how different policies will have an effect.

02:26.720 --> 02:29.280
 But then there's the actual, okay,

02:29.280 --> 02:30.520
 let's call it engineering

02:30.520 --> 02:33.280
 of like actually deploying the policies.

02:33.280 --> 02:36.420
 So computer design is almost all engineering

02:36.420 --> 02:38.240
 and reduction of practice of known methods.

02:38.240 --> 02:43.240
 Now, because of the complexity of the computers we build,

02:43.600 --> 02:45.000
 you know, you could think you're,

02:45.000 --> 02:46.640
 well, we'll just go write some code

02:46.640 --> 02:49.200
 and then we'll verify it and then we'll put it together.

02:49.200 --> 02:50.960
 And then you find out that the combination

02:50.960 --> 02:53.240
 of all that stuff is complicated.

02:53.240 --> 02:54.720
 And then you have to be inventive

02:54.720 --> 02:56.960
 to figure out how to do it, right?

02:56.960 --> 02:59.800
 So that's, that's definitely happens a lot.

02:59.800 --> 03:04.480
 And then every so often some big idea happens,

03:04.480 --> 03:06.400
 but it might be one person.

03:06.400 --> 03:07.720
 And that idea is in what,

03:07.720 --> 03:10.440
 in the space of engineering or is it in the space?

03:10.440 --> 03:11.420
 Well, I'll give you an example.

03:11.420 --> 03:13.160
 So one of the limits of computer performance

03:13.160 --> 03:14.920
 is branch prediction.

03:14.920 --> 03:17.540
 So, and there's a whole bunch of ideas

03:17.540 --> 03:19.480
 about how good you could predict a branch.

03:19.480 --> 03:21.640
 And people said, there's a limit to it.

03:21.640 --> 03:23.520
 It's an asymptotic curve.

03:23.520 --> 03:24.960
 And somebody came up with a better way

03:24.960 --> 03:26.480
 to do branch prediction.

03:26.480 --> 03:28.280
 That was a lot better.

03:28.280 --> 03:29.760
 And he published a paper on it

03:29.760 --> 03:32.800
 and every computer in the world now uses it.

03:32.800 --> 03:34.640
 And it was one idea.

03:34.640 --> 03:38.000
 So the engineers who build branch prediction hardware

03:38.000 --> 03:40.520
 were happy to drop the one kind of training array

03:40.520 --> 03:42.420
 and put it in another one.

03:42.420 --> 03:44.880
 So it was, it was a real idea.

03:44.880 --> 03:46.480
 And branch prediction is,

03:46.480 --> 03:50.600
 is one of the key problems underlying all of sort of

03:50.600 --> 03:53.840
 the lowest level of software it boils down to branch prediction.

03:53.840 --> 03:54.880
 It boils down to uncertainty.

03:54.880 --> 03:56.320
 Computers are limited by, you know,

03:56.320 --> 03:58.680
 single thread computers limited by two things.

03:58.680 --> 04:01.440
 The predictability of the path of the branches

04:01.440 --> 04:04.160
 and the predictability of the locality of data.

04:05.360 --> 04:07.120
 So we have predictors that now predict

04:07.120 --> 04:09.080
 both of those pretty well.

04:09.080 --> 04:11.920
 So memory is, you know, a couple hundred cycles away.

04:11.920 --> 04:14.580
 Local cash is a couple of cycles away.

04:14.580 --> 04:15.760
 When you're executing fast,

04:15.760 --> 04:19.060
 virtually all the data has to be in the local cash.

04:19.060 --> 04:21.360
 So a simple program says, you know,

04:21.360 --> 04:23.280
 add one to every element in an array.

04:23.280 --> 04:26.680
 It's really easy to see what the stream of data will be.

04:26.680 --> 04:29.120
 But you might have a more complicated program that's, you know,

04:29.120 --> 04:31.080
 says get a, get an element of this array,

04:31.080 --> 04:32.800
 look at something, make a decision,

04:32.800 --> 04:35.200
 go get another element, it's kind of random.

04:35.200 --> 04:37.760
 And you can think that's really unpredictable.

04:37.760 --> 04:39.200
 And then you make this big predictor

04:39.200 --> 04:40.720
 that looks at this kind of pattern.

04:40.720 --> 04:42.440
 And you realize, well, if you get this data

04:42.440 --> 04:44.520
 and this data, then you probably want that one.

04:44.520 --> 04:45.960
 And if you get this one and this one

04:45.960 --> 04:47.960
 and this one, you probably want that one.

04:47.960 --> 04:49.920
 And is that theory or is that engineering?

04:49.920 --> 04:53.200
 Like the paper that was written, was it asymptotic

04:53.200 --> 04:55.480
 kind of discussion, or is it more like,

04:55.480 --> 04:57.920
 here's a hack that works well?

04:57.920 --> 04:59.120
 It's a little bit of both.

04:59.120 --> 05:01.320
 Like there's information theory in it, I think somewhere.

05:01.320 --> 05:02.160
 Okay.

05:02.160 --> 05:03.920
 So it's actually trying to prove some kind of stuff.

05:03.920 --> 05:06.400
 But once you know the method,

05:06.400 --> 05:08.680
 implementing it is an engineering problem.

05:09.600 --> 05:10.840
 Now there's a flip side of this,

05:10.840 --> 05:13.440
 which is in a big design team,

05:13.440 --> 05:18.440
 what percentage of people think their plan

05:19.000 --> 05:20.840
 or their life's work is engineering

05:20.840 --> 05:23.520
 versus inventing things.

05:23.520 --> 05:27.560
 So lots of companies will reward you for filing patents.

05:27.560 --> 05:29.320
 Some many big companies get stuck

05:29.320 --> 05:31.160
 because to get promoted, you have to come up

05:31.160 --> 05:33.000
 with something new.

05:33.000 --> 05:34.760
 And then what happens is everybody's trying

05:34.760 --> 05:39.160
 to do some random new thing, 99% of which doesn't matter.

05:39.160 --> 05:41.160
 And the basics get neglected.

05:41.160 --> 05:46.160
 And, or they get to, there's a dichotomy, they think,

05:46.200 --> 05:49.480
 like the cell library and the basic CAD tools,

05:49.480 --> 05:53.200
 or basic software validation methods.

05:53.200 --> 05:54.680
 That's simple stuff.

05:54.680 --> 05:56.880
 They wanna work on the exciting stuff.

05:56.880 --> 05:58.880
 And then they spend lots of time trying to figure out

05:58.880 --> 06:00.680
 how to patent something.

06:00.680 --> 06:02.200
 And that's mostly useless.

06:02.200 --> 06:04.520
 But the breakthroughs are on simple stuff.

06:04.520 --> 06:08.920
 No, no, you have to do the simple stuff really well.

06:08.920 --> 06:11.440
 If you're building a building out of bricks,

06:11.440 --> 06:13.240
 you want great bricks.

06:13.240 --> 06:14.920
 So you go to two places to sell bricks.

06:14.920 --> 06:17.960
 So one guy says, yeah, they're over there in an ugly pile.

06:17.960 --> 06:19.840
 And the other guy is like lovingly tells you

06:19.840 --> 06:22.280
 about the 50 kinds of bricks and how hard they are

06:22.280 --> 06:25.040
 and how beautiful they are and how square they are.

06:25.040 --> 06:28.200
 And, you know, which one are you gonna buy bricks from?

06:28.200 --> 06:30.400
 Which is gonna make a better house.

06:30.400 --> 06:31.880
 So you're talking about the craftsman,

06:31.880 --> 06:33.480
 the person who understands bricks,

06:33.480 --> 06:35.160
 who loves bricks, who loves the variety.

06:35.160 --> 06:36.080
 That's a good word.

06:36.080 --> 06:39.360
 You know, good engineering is great craftsmanship.

06:39.360 --> 06:44.360
 And when you start thinking engineering is about invention

06:44.840 --> 06:47.920
 and you set up a system that rewards invention,

06:47.920 --> 06:50.640
 the craftsmanship gets neglected.

06:50.640 --> 06:53.480
 Okay, so maybe one perspective is the theory.

06:53.480 --> 06:57.640
 The science overemphasizes invention

06:57.640 --> 07:00.400
 and engineering emphasizes craftsmanship.

07:00.400 --> 07:02.840
 And therefore, like, so if you,

07:02.840 --> 07:04.320
 it doesn't matter what you do, theory.

07:04.320 --> 07:06.200
 But everybody does, like read the tech rags.

07:06.200 --> 07:07.880
 They're always talking about some breakthrough

07:07.880 --> 07:10.640
 or innovation and everybody thinks

07:10.640 --> 07:12.480
 that's the most important thing.

07:12.480 --> 07:13.920
 But the number of innovative ideas

07:13.920 --> 07:16.000
 is actually relatively low.

07:16.000 --> 07:17.240
 We need them, right?

07:17.240 --> 07:19.840
 And innovation creates a whole new opportunity.

07:19.840 --> 07:24.040
 Like when some guy invented the internet, right?

07:24.040 --> 07:25.920
 Like that was a big thing.

07:25.920 --> 07:28.240
 The million people that wrote software against that

07:28.240 --> 07:31.200
 were mostly doing engineering and software writing.

07:31.200 --> 07:34.280
 So the elaboration of that idea was huge.

07:34.280 --> 07:35.560
 I don't know if you know Brandon and I,

07:35.560 --> 07:38.200
 he wrote JavaScript in 10 days.

07:38.200 --> 07:39.520
 That's an interesting story.

07:39.520 --> 07:42.400
 It makes me wonder, and it was, you know,

07:42.400 --> 07:44.200
 famously for many years considered

07:44.200 --> 07:47.640
 to be a pretty crappy programming language.

07:47.640 --> 07:48.760
 Still is perhaps.

07:48.760 --> 07:51.120
 It's been improving sort of consistently.

07:51.120 --> 07:55.600
 But the interesting thing about that guy is,

07:55.600 --> 07:58.520
 you know, he doesn't get any awards.

07:58.520 --> 08:00.720
 You don't get a Nobel Prize or a Fields Medal

08:00.720 --> 08:05.720
 or a crappy piece of, you know, software code.

08:06.640 --> 08:08.720
 That is currently the number one programming language

08:08.720 --> 08:12.680
 in the world and runs now is increasingly running

08:12.680 --> 08:13.760
 the back end of the internet.

08:13.760 --> 08:17.680
 Well, does he know why everybody uses it?

08:17.680 --> 08:19.360
 Like that would be an interesting thing.

08:19.360 --> 08:22.400
 Was it the right thing at the right time?

08:22.400 --> 08:24.960
 Cause like when stuff like JavaScript came out,

08:24.960 --> 08:26.320
 like there was a move from, you know,

08:26.320 --> 08:29.640
 writing C programs and C++ to,

08:29.640 --> 08:32.400
 let's call what they call managed code frameworks

08:32.400 --> 08:35.240
 where you write simple code, it might be interpreted,

08:35.240 --> 08:37.800
 it has lots of libraries, productivity is high,

08:37.800 --> 08:39.560
 and you don't have to be an expert.

08:39.560 --> 08:41.360
 So, you know, Java was supposed to solve

08:41.360 --> 08:43.800
 all the world's problems, it was complicated.

08:43.800 --> 08:45.240
 JavaScript came out, you know,

08:45.240 --> 08:47.680
 after a bunch of other scripting languages.

08:47.680 --> 08:50.400
 I'm not an expert on it, but was it the right thing

08:50.400 --> 08:53.880
 at the right time or was there something, you know,

08:53.880 --> 08:56.320
 clever cause he wasn't the only one.

08:56.320 --> 08:57.440
 There's a few elements.

08:57.440 --> 08:59.800
 And maybe if he figured out what it was,

08:59.800 --> 09:00.920
 then he'd get a prize.

09:02.040 --> 09:03.360
 Like that.

09:03.360 --> 09:06.880
 Yeah, you know, maybe his problem is he hasn't defined this.

09:06.880 --> 09:09.520
 Or he just needs a good promoter.

09:09.520 --> 09:11.920
 Well, I think there was a bunch of blog posts

09:11.920 --> 09:14.840
 written about it, which is like wrong is right,

09:14.840 --> 09:19.320
 which is like doing the crappy thing fast,

09:19.320 --> 09:21.360
 just like hacking together the thing

09:21.360 --> 09:23.240
 that answers some of the needs

09:23.240 --> 09:26.080
 and then iterating over time, listening to developers,

09:26.080 --> 09:28.320
 like listening to people who actually use the thing.

09:28.320 --> 09:31.520
 This is something you can do more in software,

09:31.520 --> 09:33.760
 but the right time, like you have to sense,

09:33.760 --> 09:35.120
 you have to have a good instinct

09:35.120 --> 09:37.560
 of when is the right time for the right tool

09:37.560 --> 09:42.560
 and make it super simple and just get it out there.

09:42.720 --> 09:45.200
 The problem is this is true with hardware,

09:45.200 --> 09:46.400
 this is less true with software,

09:46.400 --> 09:48.440
 is there's a backward compatibility

09:48.440 --> 09:51.720
 that just drags behind you as, you know,

09:51.720 --> 09:53.800
 as you try to fix all the mistakes of the past.

09:53.800 --> 09:56.480
 But the timing was good.

09:56.480 --> 09:57.480
 There's something about that.

09:57.480 --> 09:58.840
 It wasn't accidental.

09:58.840 --> 10:02.600
 You have to like give yourself over to the,

10:02.600 --> 10:05.360
 you have to have this like broad sense

10:05.360 --> 10:09.280
 of what's needed now, both scientifically

10:09.280 --> 10:12.280
 and like the community and just like this.

10:12.280 --> 10:15.680
 It was obvious that there was no,

10:15.680 --> 10:17.960
 the interesting thing about JavaScript

10:17.960 --> 10:20.880
 is everything that ran in the browser at the time,

10:20.880 --> 10:24.440
 like Java and I think other like scheme,

10:24.440 --> 10:25.920
 other programming languages,

10:25.920 --> 10:30.520
 they were all in a separate external container.

10:30.520 --> 10:32.520
 And then JavaScript was literally

10:32.520 --> 10:34.600
 just injected into the webpage.

10:34.600 --> 10:36.400
 It was the dumbest possible thing

10:36.400 --> 10:39.360
 running in the same thread as everything else.

10:39.360 --> 10:43.120
 And like it was inserted as a comment.

10:43.120 --> 10:47.520
 So JavaScript code is inserted as a comment in the HTML code.

10:47.520 --> 10:51.320
 And it was, I mean, there's, it's either genius

10:51.320 --> 10:53.080
 or super dumb, but it's like.

10:53.080 --> 10:55.720
 Right, so it had no apparatus for like a virtual machine

10:55.720 --> 10:56.720
 and container.

10:56.720 --> 10:58.960
 It just executed in the framework of the program

10:58.960 --> 10:59.800
 that's already running.

10:59.800 --> 11:02.760
 And it was, and then because something

11:02.760 --> 11:05.960
 about that accessibility, the ease of its use

11:07.280 --> 11:10.080
 resulted in then developers innovating

11:10.080 --> 11:11.400
 of how to actually use it.

11:11.400 --> 11:13.680
 I mean, I don't even know what to make of that,

11:13.680 --> 11:18.360
 but it does seem to echo across different software,

11:18.360 --> 11:19.760
 like stories of different software.

11:19.760 --> 11:22.920
 PHP has the same story, really crappy language.

11:22.920 --> 11:24.440
 They just took over the world.

11:25.440 --> 11:28.360
 Well, let's have a joke that the random length instructions,

11:28.360 --> 11:30.680
 variable length instructions, that's always one,

11:30.680 --> 11:33.080
 even though they're obviously worse.

11:33.080 --> 11:35.400
 Like nobody knows why x86 is,

11:35.400 --> 11:38.000
 or you'd be the worst architecture, you know,

11:38.000 --> 11:40.520
 on the planet is one of the most popular ones.

11:40.520 --> 11:42.880
 Well, I mean, isn't that also the story of risk

11:42.880 --> 11:46.240
 versus, I mean, is that simplicity?

11:46.240 --> 11:47.440
 There's something about simplicity

11:47.440 --> 11:52.440
 that us in this evolutionary process is valued.

11:53.480 --> 11:58.480
 If it's simple, it spreads faster, it seems like.

11:58.800 --> 11:59.960
 Or is that not always true?

11:59.960 --> 12:01.120
 That's not always true.

12:01.120 --> 12:04.280
 Yeah, it could be simple as good, but too simple as bad.

12:04.280 --> 12:06.440
 So why did risk win, you think, so far?

12:06.440 --> 12:07.280
 Did risk win?

12:08.680 --> 12:10.560
 In the long archivist tree.

12:10.560 --> 12:11.400
 We don't know.

12:11.400 --> 12:12.720
 So who's gonna win?

12:12.720 --> 12:14.200
 What's risk, what's risk,

12:14.200 --> 12:17.560
 and who's gonna win in that space in these instruction sets?

12:17.560 --> 12:18.920
 AI software's gonna win,

12:18.920 --> 12:22.160
 but there'll be little computers that run little programs

12:22.160 --> 12:23.840
 like normal all over the place.

12:24.920 --> 12:28.520
 But we're going through another transformation, so.

12:28.520 --> 12:32.400
 But you think instruction sets underneath it all will change?

12:32.400 --> 12:33.640
 Yeah, they evolve slowly.

12:33.640 --> 12:35.480
 They don't matter very much.

12:35.480 --> 12:36.800
 They don't matter very much, okay.

12:36.800 --> 12:39.760
 I mean, the limits of performance are, you know,

12:39.760 --> 12:41.640
 predictability of instructions and data.

12:41.640 --> 12:43.360
 I mean, that's the big thing.

12:43.360 --> 12:46.720
 And then the usability of it is some, you know,

12:47.960 --> 12:52.160
 quality of design, quality of tools, availability.

12:52.160 --> 12:56.440
 Like right now, X86 is proprietary with Intel and AMD,

12:56.440 --> 12:59.480
 but they can change it any way they want independently.

12:59.480 --> 13:01.640
 Right, ARM is proprietary to ARM,

13:01.640 --> 13:03.680
 and they won't let anybody else change it.

13:03.680 --> 13:05.680
 So it's like a sole point.

13:05.680 --> 13:09.080
 And RISC 5 is open source, so anybody can change it,

13:09.080 --> 13:10.640
 which is super cool.

13:10.640 --> 13:12.480
 But that also might mean it gets changed

13:12.480 --> 13:13.640
 in too many random ways,

13:13.640 --> 13:17.720
 that there's no common subset of it that people can use.

13:17.720 --> 13:19.920
 Do you like open or do you like closed?

13:19.920 --> 13:21.520
 Like if you were to bet all your money

13:21.520 --> 13:23.320
 on one or the other, RISC 5 versus it?

13:23.320 --> 13:24.240
 No idea.

13:24.240 --> 13:25.080
 It's case dependent?

13:25.080 --> 13:26.360
 Well, X86 oddly enough,

13:26.360 --> 13:28.320
 when Intel first started developing it,

13:28.320 --> 13:30.280
 they licensed it like seven people.

13:30.280 --> 13:33.080
 So it was the open architecture.

13:33.080 --> 13:35.360
 And then they moved faster than others

13:35.360 --> 13:37.480
 and also bought one or two of them.

13:37.480 --> 13:40.280
 But there was seven different people making X86,

13:40.280 --> 13:45.280
 because at the time there was 6502 and Z80s and 8086.

13:46.960 --> 13:48.680
 And you could argue everybody thought

13:48.680 --> 13:50.960
 Z80 was the better instruction set,

13:50.960 --> 13:54.480
 but that was proprietary to one place.

13:54.480 --> 13:56.160
 Oh, and the 6800.

13:56.160 --> 13:59.480
 So there's like four or five different microprocessors.

13:59.480 --> 14:02.400
 Intel went open, got the market share

14:02.400 --> 14:04.720
 because people felt like they had multiple sources from it,

14:04.720 --> 14:07.680
 and then over time it narrowed down to two players.

14:07.680 --> 14:12.680
 So why, you as a historian, why did Intel win

14:12.920 --> 14:17.280
 for so long with their processors?

14:17.280 --> 14:18.120
 I mean, I mean.

14:18.120 --> 14:18.960
 They were great.

14:18.960 --> 14:21.040
 Their process development was great.

14:21.040 --> 14:24.320
 So it's just looking back to JavaScript and Brandenike

14:24.320 --> 14:28.960
 is Microsoft and Netscape and all these internet browsers.

14:28.960 --> 14:31.760
 Microsoft won the browser game

14:31.760 --> 14:36.000
 because they aggressively stole other people's ideas.

14:36.000 --> 14:37.840
 Like right after they did it.

14:37.840 --> 14:39.120
 You know, I don't know

14:39.120 --> 14:41.200
 if Intel was stealing other people's ideas.

14:41.200 --> 14:42.040
 They started making.

14:42.040 --> 14:42.880
 In a good way, stealing them,

14:42.880 --> 14:43.800
 because we're just a clarify.

14:43.800 --> 14:48.280
 They started making RAMs, random access memories.

14:48.280 --> 14:53.000
 And then at the time when the Japanese manufacturers came up,

14:53.000 --> 14:54.920
 you know, they were getting out competed on that

14:54.920 --> 14:56.600
 and they pivoted the microprocessors

14:56.600 --> 14:57.760
 and they made the first, you know,

14:57.760 --> 14:59.920
 integrated microprocessor grant programs.

14:59.920 --> 15:03.880
 It was the 4004 or something.

15:03.880 --> 15:04.880
 Who was behind that pivot?

15:04.880 --> 15:05.880
 That's a hell of a pivot.

15:05.880 --> 15:06.880
 Andy Grove.

15:06.880 --> 15:08.800
 And he was great.

15:08.800 --> 15:10.160
 That's a hell of a pivot.

15:10.160 --> 15:13.880
 And then they led semiconductor industry.

15:13.880 --> 15:16.000
 Like they were just a little company, IBM,

15:16.000 --> 15:19.040
 all kinds of big companies had boatloads of money

15:19.040 --> 15:21.200
 and they out innovated everybody.

15:21.200 --> 15:22.200
 Out of the innovative.

15:22.200 --> 15:23.040
 Okay.

15:23.040 --> 15:23.880
 Yeah.

15:23.880 --> 15:24.720
 So it's not like marketing.

15:24.720 --> 15:26.280
 It's not, you know, stuff.

15:26.280 --> 15:29.360
 Their processor designs were pretty good.

15:29.360 --> 15:34.360
 I think the, you know, Core 2 was probably the first one

15:34.360 --> 15:36.200
 I thought was great.

15:36.200 --> 15:37.600
 It was a really fast processor

15:37.600 --> 15:38.960
 and then Haswell was great.

15:40.200 --> 15:42.240
 What makes a great processor?

15:42.240 --> 15:43.880
 Oh, if you just look at its performance

15:43.880 --> 15:46.440
 versus everybody else, it's, you know,

15:46.440 --> 15:49.880
 the size of it, you know, usability of it.

15:49.880 --> 15:51.840
 So it's not specific, some kind of element

15:51.840 --> 15:52.680
 that makes you beautiful.

15:52.680 --> 15:55.160
 It's just like literally just raw performance.

15:55.160 --> 15:57.160
 Is that how you think of bioprocessors?

15:57.160 --> 15:59.760
 It's just like raw performance.

15:59.760 --> 16:01.320
 Of course.

16:01.320 --> 16:02.360
 It's like a horse race.

16:02.360 --> 16:04.280
 The fastest one wins.

16:04.280 --> 16:05.120
 Now.

16:05.120 --> 16:05.960
 You don't care how.

16:05.960 --> 16:10.600
 Well, there's the fastest in the environment.

16:10.600 --> 16:13.040
 Like for years, you made the fastest one you could

16:13.040 --> 16:14.960
 and then people started to have power limits.

16:14.960 --> 16:17.640
 So then you made the fastest at the right PowerPoint.

16:17.640 --> 16:20.480
 And then, and then when we started doing multiprocessors,

16:20.480 --> 16:23.800
 like if you could scale your processors more

16:23.800 --> 16:25.680
 than the other guy, you could be 10% faster

16:25.680 --> 16:28.440
 on like a single thread, but you have more threads.

16:28.440 --> 16:30.000
 So there's lots of variability.

16:30.000 --> 16:35.000
 And then ARM really explored, like, you know,

16:35.560 --> 16:37.440
 they have the A series and the R series

16:37.440 --> 16:40.320
 and the M series, like a family of processors

16:40.320 --> 16:41.960
 for all these different design points

16:41.960 --> 16:44.600
 from like unbelievably small and simple.

16:44.600 --> 16:46.520
 And so then when you're doing the design,

16:46.520 --> 16:49.360
 it's sort of like this big palette of CPUs.

16:49.360 --> 16:51.480
 Like they're the only ones with a credible,

16:51.480 --> 16:53.160
 you know, top to bottom palette.

16:54.660 --> 16:56.880
 What do you mean a credible top to bottom?

16:56.880 --> 16:58.600
 Well, there's people that make microcontrollers

16:58.600 --> 17:00.480
 that are small, but they don't have a fast one.

17:00.480 --> 17:02.080
 There's people who make fast processors,

17:02.080 --> 17:04.880
 but don't have a little, a medium one or a small one.

17:04.880 --> 17:07.080
 Is that hard to do that full palette?

17:07.080 --> 17:09.360
 That seems like a, it's a lot of different.

17:09.360 --> 17:13.360
 So what's the difference in the ARM folks and Intel,

17:13.360 --> 17:15.620
 in terms of the way they're approaching this problem?

17:15.620 --> 17:19.200
 Well, Intel, almost all their processors or designs

17:19.200 --> 17:21.720
 were, you know, very custom high end,

17:21.720 --> 17:23.400
 you know, for the last 15, 20 years.

17:23.400 --> 17:24.880
 It's the fastest horse possible.

17:24.880 --> 17:25.840
 Yeah.

17:25.840 --> 17:27.520
 In one horses.

17:27.520 --> 17:30.400
 Yeah. And the architecture that are really good,

17:30.400 --> 17:33.360
 but the company itself was fairly insular

17:33.360 --> 17:36.280
 to what's going on in the industry with CAD tools and stuff.

17:36.280 --> 17:39.840
 And there's this debate about custom design versus synthesis

17:39.840 --> 17:41.320
 and how do you approach that?

17:41.320 --> 17:45.680
 I'd say Intel was slow on the cutting to synthesize processors.

17:45.680 --> 17:49.100
 ARM came in from the bottom and they generated IP,

17:49.100 --> 17:50.840
 which went to all kinds of customers.

17:50.840 --> 17:52.760
 So they had very little say in how the customer

17:52.760 --> 17:54.960
 implemented their IP.

17:54.960 --> 17:59.440
 So ARM is super friendly to the synthesis IP environment.

17:59.440 --> 18:00.280
 Whereas Intel said,

18:00.280 --> 18:03.200
 we're going to make this great client chip or server chip

18:03.200 --> 18:05.440
 with our own CAD tools, with our own process,

18:05.440 --> 18:08.140
 with our own, you know, other supporting IP

18:08.140 --> 18:10.240
 and everything only works with our stuff.

18:11.340 --> 18:16.340
 So is that, is ARM winning the mobile platform space

18:16.440 --> 18:17.680
 in terms of process?

18:17.680 --> 18:22.680
 And so in that way you're describing is why they're winning.

18:22.680 --> 18:24.920
 Well, they had lots of people doing lots

18:24.920 --> 18:26.440
 of different experiments.

18:26.440 --> 18:29.440
 So they controlled the processor architecture and IP,

18:29.440 --> 18:32.040
 but they let people put in lots of different chips.

18:32.040 --> 18:35.240
 And there was a lot of variability in what happened there.

18:35.240 --> 18:37.160
 Whereas Intel, when they made their mobile,

18:37.160 --> 18:38.440
 their foray into mobile,

18:38.440 --> 18:41.720
 they had one team doing one part, right?

18:41.720 --> 18:43.160
 So it wasn't 10 experiments.

18:43.160 --> 18:46.000
 And then their mindset was PC mindset,

18:46.000 --> 18:48.040
 Microsoft software mindset,

18:48.040 --> 18:49.920
 and that brought a whole bunch of things along

18:49.920 --> 18:52.560
 that the mobile world, the embedded world don't do.

18:52.560 --> 18:55.480
 You think it was possible for Intel to pivot hard

18:55.480 --> 18:58.280
 and win the mobile market?

18:58.280 --> 19:00.120
 That's a hell of a difficult thing to do, right?

19:00.120 --> 19:02.080
 For a huge company to just pivot.

19:03.440 --> 19:05.560
 I mean, it's so interesting to,

19:05.560 --> 19:07.440
 because we'll talk about your current work.

19:07.440 --> 19:11.160
 It's like, it's clear that PCs were dominating

19:11.160 --> 19:14.240
 for several decades, like desktop computers.

19:14.240 --> 19:17.800
 And then mobile, it's unclear.

19:17.800 --> 19:19.360
 It's a leadership question.

19:19.360 --> 19:23.080
 Like Apple under Steve Jobs, when he came back,

19:23.080 --> 19:24.800
 they pivoted multiple times.

19:25.880 --> 19:29.160
 They built iPads and iTunes and phones and tablets

19:29.160 --> 19:32.000
 and great Macs, like who knew computers

19:32.000 --> 19:33.440
 should be made out of aluminum?

19:33.440 --> 19:34.280
 Nobody knew that.

19:35.320 --> 19:37.200
 That they're great, it's super fun.

19:37.200 --> 19:38.040
 That was Steve?

19:38.040 --> 19:41.440
 Yeah, Steve Jobs, like they pivoted multiple times.

19:41.440 --> 19:45.880
 And the old Intel, they did that multiple times.

19:45.880 --> 19:48.440
 They made DRAMs and processors and processes

19:48.440 --> 19:50.920
 and I got to ask this,

19:50.920 --> 19:53.080
 what was it like working with Steve Jobs?

19:53.080 --> 19:54.440
 I didn't work with him.

19:54.440 --> 19:55.720
 Did you interact with him?

19:55.720 --> 19:56.560
 Twice.

19:57.480 --> 19:59.880
 I said hi to him twice in the cafeteria.

19:59.880 --> 20:01.080
 What did you say?

20:01.080 --> 20:01.920
 Hi.

20:01.920 --> 20:05.120
 You said, hey fellas, he was friendly.

20:06.000 --> 20:08.280
 He was wandering around and with somebody,

20:08.280 --> 20:09.240
 you couldn't find the table

20:09.240 --> 20:13.720
 because the cafeteria was packed and I gave my table.

20:13.720 --> 20:16.080
 But I worked for Mike Colbert who talked to,

20:16.080 --> 20:19.280
 like Mike was the unofficial CTO of Apple

20:19.280 --> 20:22.200
 and a brilliant guy and he worked for Steve for 25 years,

20:22.200 --> 20:25.640
 maybe more and he talked to Steve multiple times a day.

20:26.720 --> 20:29.400
 And he was one of the people that could put up with Steve's,

20:29.400 --> 20:31.760
 let's say brilliance and intensity.

20:31.760 --> 20:35.720
 And Steve really liked him and Steve trusted Mike

20:35.720 --> 20:39.080
 to translate the shit he thought up

20:39.080 --> 20:40.920
 into engineering products at work

20:40.920 --> 20:43.160
 and then Mike ran a group called Platform Architecture

20:43.160 --> 20:44.800
 and I was in that group.

20:44.800 --> 20:46.400
 So many times I'd be sitting with Mike

20:46.400 --> 20:48.720
 and the phone would ring and it'd be Steve

20:48.720 --> 20:50.440
 and Mike would hold the phone like this

20:50.440 --> 20:52.360
 because Steve would be yelling about something or other.

20:52.360 --> 20:53.200
 Yeah.

20:53.200 --> 20:54.160
 And then he would translate.

20:54.160 --> 20:55.920
 And he translated and then he would say,

20:55.920 --> 20:58.320
 Steve wants us to do this.

20:58.320 --> 20:59.520
 So.

20:59.520 --> 21:01.120
 Was Steve a good engineer or no?

21:01.120 --> 21:02.440
 I don't know.

21:02.440 --> 21:03.800
 He was a great idea guy.

21:03.800 --> 21:04.640
 Idea person.

21:04.640 --> 21:07.600
 And he's a really good selector for talent.

21:07.600 --> 21:09.600
 Yeah, that seems to be one of the key elements

21:09.600 --> 21:10.800
 of leadership, right?

21:10.800 --> 21:12.800
 And then he was a really good first principles guy.

21:12.800 --> 21:15.080
 Like somebody say something couldn't be done

21:15.080 --> 21:20.080
 and he would just think that's obviously wrong, right?

21:20.320 --> 21:24.440
 But maybe it's hard to do, maybe it's expensive to do,

21:24.440 --> 21:26.200
 maybe we need different people.

21:26.200 --> 21:27.320
 There's like a whole bunch of,

21:27.320 --> 21:29.640
 if you want to do something hard,

21:29.640 --> 21:31.600
 maybe it takes time, maybe you have to iterate.

21:31.600 --> 21:33.760
 There's a whole bunch of things that you could think about

21:33.760 --> 21:36.400
 but saying it can't be done is stupid.

21:36.400 --> 21:38.120
 How would you compare?

21:38.120 --> 21:42.880
 So it seems like Elon Musk is more engineering centric

21:42.880 --> 21:43.720
 but it's also,

21:43.720 --> 21:45.680
 I think he considered himself a designer too.

21:45.680 --> 21:47.040
 He has a design mind.

21:47.040 --> 21:50.560
 Steve Jobs feels like he's much more idea space,

21:50.560 --> 21:52.760
 design space versus engineering.

21:52.760 --> 21:53.960
 Just make it happen.

21:53.960 --> 21:57.200
 Like the world should be this way, just figure it out.

21:57.200 --> 21:58.720
 But he used computers.

21:58.720 --> 22:01.880
 You know, he had computer people talk to him all the time.

22:01.880 --> 22:03.400
 Like Mike was a really good computer guy.

22:03.400 --> 22:04.840
 He knew what computers could do.

22:04.840 --> 22:06.320
 Computer meaning computer hardware,

22:06.320 --> 22:10.960
 like hardware software, all the pieces and then he would,

22:10.960 --> 22:14.520
 you know, have an idea about what could we do with this next

22:14.520 --> 22:16.040
 that was grounded in reality.

22:16.040 --> 22:17.120
 It wasn't like he was, you know,

22:17.120 --> 22:19.200
 just finger painting on the wall

22:19.200 --> 22:20.960
 and wishing somebody would interpret it.

22:20.960 --> 22:23.400
 Like, so he had this interesting connection

22:23.400 --> 22:28.320
 because, you know, he wasn't a computer architecture designer,

22:28.320 --> 22:30.840
 but he had an intuition from the computers we had

22:30.840 --> 22:33.880
 to what could happen and.

22:33.880 --> 22:36.760
 It's interesting to say intuition because it seems

22:36.760 --> 22:41.760
 like he was pissing off a lot of engineers in his intuition

22:41.760 --> 22:43.680
 about what can and can't be done.

22:43.680 --> 22:46.840
 Those, like the, what is all these stories

22:46.840 --> 22:48.960
 about like floppy disks and all that kind of stuff.

22:48.960 --> 22:49.800
 Like.

22:49.800 --> 22:50.640
 Yeah.

22:50.640 --> 22:54.320
 So in Steve, the first round, like he'd go into a lab

22:54.320 --> 22:56.200
 and look at what's going on and hate it

22:56.200 --> 23:00.520
 and fire people or assembly in the elevator,

23:00.520 --> 23:03.800
 what they're doing for Apple and, you know, not be happy.

23:03.800 --> 23:07.960
 When he came back, my impression was is he surrounded himself

23:07.960 --> 23:10.160
 with this relatively small group of people.

23:10.160 --> 23:11.000
 Yes.

23:11.000 --> 23:13.840
 And didn't really interact outside of that as much.

23:13.840 --> 23:16.280
 And then the joke was, you'd see like somebody moving

23:16.280 --> 23:20.760
 up prototype through the quad with a black blanket over it.

23:20.760 --> 23:23.200
 And that was because it was secret, you know,

23:23.200 --> 23:25.120
 partly from Steve because they didn't want Steve

23:25.120 --> 23:26.960
 to see it until it was ready.

23:26.960 --> 23:31.400
 Yeah, the dynamic with Johnny Ive and Steve is interesting.

23:31.400 --> 23:35.960
 It's like you don't want to, he ruins as many ideas

23:35.960 --> 23:37.280
 as he generates.

23:37.280 --> 23:38.280
 Yeah.

23:38.280 --> 23:39.120
 Yeah.

23:39.120 --> 23:42.080
 It's a dangerous kind of line to walk.

23:42.080 --> 23:44.520
 If you have a lot of ideas, like,

23:44.520 --> 23:47.240
 like Gordon Bell was famous for ideas, right?

23:47.240 --> 23:49.120
 And it wasn't that the percentage of good ideas

23:49.120 --> 23:51.440
 was way higher than anybody else.

23:51.440 --> 23:54.360
 It was, he had so many ideas and he was also good

23:54.360 --> 23:58.120
 at talking to people about it and getting the filters, right?

23:58.120 --> 24:00.200
 And, you know, seeing through stuff.

24:00.200 --> 24:03.360
 Whereas Elon was like, hey, I want to build rockets.

24:03.360 --> 24:06.000
 So Steve was hired bunch of rocket guys

24:06.000 --> 24:08.480
 and Elon would go read rocket manuals.

24:08.480 --> 24:11.480
 So Elon is a better engineer, a sense like,

24:11.480 --> 24:16.480
 or like more like a love and passion for the manuals.

24:16.480 --> 24:17.320
 Yeah.

24:17.320 --> 24:18.160
 And the details.

24:18.160 --> 24:19.000
 The details.

24:19.000 --> 24:19.840
 The data and the understanding.

24:19.840 --> 24:20.800
 The craftsmanship too, right?

24:20.800 --> 24:22.720
 Well, I guess Steve had craftsmanship too,

24:22.720 --> 24:24.240
 but of a different kind.

24:24.240 --> 24:26.960
 What do you make of the, just the standard

24:26.960 --> 24:28.600
 for just a little longer, what do you make of like

24:28.600 --> 24:30.640
 the anger and the passion and all that,

24:30.640 --> 24:35.080
 the firing and the mood swings and the madness,

24:35.080 --> 24:38.360
 the, you know, being emotional and all that.

24:38.360 --> 24:40.680
 That's Steve and I guess Elon too.

24:40.680 --> 24:43.720
 So what, is that a bug or a feature?

24:43.720 --> 24:45.040
 It's a feature.

24:45.040 --> 24:49.600
 So there's a graph, which is Y axis productivity.

24:49.600 --> 24:50.440
 Yeah.

24:50.440 --> 24:55.080
 X axis at zero is chaos and infinity is complete order.

24:55.080 --> 24:55.920
 Yeah.

24:55.920 --> 25:00.920
 So as you go from the, you know, the origin,

25:00.920 --> 25:03.560
 as you improve order, you improve productivity.

25:03.560 --> 25:04.400
 Yeah.

25:04.400 --> 25:06.440
 And at some point productivity peaks

25:06.440 --> 25:08.360
 and then it goes back down again.

25:08.360 --> 25:09.800
 Too much order, nothing can happen.

25:09.800 --> 25:10.640
 Yes.

25:10.640 --> 25:12.040
 But the question is, is the,

25:12.040 --> 25:13.720
 how close to the chaos is that?

25:13.720 --> 25:14.560
 No, no, no.

25:14.560 --> 25:15.400
 Here's the thing.

25:15.400 --> 25:16.920
 Is once you start moving the direction of order,

25:16.920 --> 25:21.000
 the force factor to drive you towards order is unstoppable.

25:21.000 --> 25:21.840
 Oh.

25:21.840 --> 25:24.880
 And every organization will move to the place

25:24.880 --> 25:27.120
 where their productivity is stymied by order.

25:27.120 --> 25:28.200
 So you need to...

25:28.200 --> 25:31.240
 So the question is, who's the counter force?

25:31.240 --> 25:33.360
 Like, and cause it also feels really good.

25:33.360 --> 25:36.240
 As you get more organized and productivity goes up,

25:36.240 --> 25:39.720
 the organization feels it, they orient towards it, right?

25:39.720 --> 25:41.080
 To hire more people.

25:41.080 --> 25:42.880
 They got more guys who couldn't run process,

25:42.880 --> 25:44.760
 you get bigger, right?

25:44.760 --> 25:49.120
 And then inevitably, the organization gets captured

25:49.120 --> 25:51.840
 by the bureaucracy that manages all the processes.

25:52.800 --> 25:53.640
 Yeah.

25:53.640 --> 25:55.520
 All right. And then humans really like that.

25:55.520 --> 25:57.920
 And so if you just walk into a room and say,

25:57.920 --> 26:00.960
 guys, love what you're doing,

26:00.960 --> 26:03.340
 but I need you to have less order.

26:04.960 --> 26:06.920
 If you don't have some force behind that,

26:06.920 --> 26:07.880
 nothing will happen.

26:09.080 --> 26:11.760
 I can't tell you on how many levels that's profound.

26:11.760 --> 26:12.600
 So...

26:12.600 --> 26:14.080
 So that's why I'd say it's a feature.

26:14.080 --> 26:17.200
 Now, could you be nicer about it?

26:17.200 --> 26:18.040
 I don't know.

26:18.040 --> 26:21.440
 I don't know any good examples of being nicer about it.

26:21.440 --> 26:23.520
 Well, the funny thing is to get stuff done.

26:23.520 --> 26:25.200
 You need people who can manage stuff

26:25.200 --> 26:26.920
 and manage people because humans are complicated.

26:26.920 --> 26:28.160
 They need lots of care and feeding

26:28.160 --> 26:29.880
 and you need to tell them they look nice

26:29.880 --> 26:33.120
 and they're doing good stuff and pat them on the back, right?

26:33.120 --> 26:33.960
 I don't know.

26:33.960 --> 26:36.040
 Do you tell me, is that needed?

26:36.040 --> 26:37.080
 Do humans need that?

26:37.080 --> 26:39.640
 I had a friend, he started a manager group and he said,

26:39.640 --> 26:40.840
 I figured it out.

26:40.840 --> 26:43.400
 You have to praise them before they do anything.

26:43.400 --> 26:45.240
 I was waiting till they were done

26:45.240 --> 26:46.560
 and they were always mad at me.

26:46.560 --> 26:48.200
 Now we tell them what a great job they're doing

26:48.200 --> 26:49.400
 while they're doing it.

26:49.400 --> 26:51.080
 But then you get stuck in that trap

26:51.080 --> 26:52.240
 because then when they're not doing something,

26:52.240 --> 26:54.080
 how do you confront these people?

26:54.080 --> 26:55.920
 I think a lot of people that had trauma

26:55.920 --> 26:57.560
 in their childhood would disagree with you.

26:57.560 --> 27:00.680
 Successful people that you just first do the rough stuff

27:00.680 --> 27:02.360
 and then be nice later.

27:02.360 --> 27:03.200
 I don't know.

27:03.200 --> 27:05.440
 Okay, but you know, engineering companies are full

27:05.440 --> 27:08.120
 of adults who had all kinds of range of childhoods.

27:08.120 --> 27:11.440
 You know, most people had okay childhoods.

27:11.440 --> 27:12.760
 Well, I don't know if...

27:12.760 --> 27:15.640
 I know lots of people only work for praise, which is weird.

27:15.640 --> 27:16.840
 You mean like everybody.

27:18.720 --> 27:21.160
 I'm not that interested in it, but...

27:21.160 --> 27:22.760
 Well, you're probably looking

27:22.760 --> 27:27.440
 for somebody's approval, even still.

27:27.440 --> 27:28.280
 Yeah, maybe.

27:28.280 --> 27:29.560
 I should think about that.

27:29.560 --> 27:32.160
 Maybe somebody who's no longer with us kind of thing.

27:33.200 --> 27:34.120
 I don't know.

27:34.120 --> 27:36.080
 I used to call up my dad and tell him what I was doing.

27:36.080 --> 27:38.640
 He was very excited about engineering and stuff.

27:38.640 --> 27:40.360
 You got his approval?

27:40.360 --> 27:42.080
 Yeah, a lot.

27:42.080 --> 27:43.360
 I was lucky.

27:43.360 --> 27:47.200
 Like he decided I was smart and unusual as a kid

27:47.200 --> 27:50.200
 and that was okay when I was really young.

27:50.200 --> 27:52.520
 So when I did poorly in school, I was dyslexic.

27:52.520 --> 27:55.240
 I didn't read until I was third or fourth grade.

27:55.240 --> 27:56.080
 They didn't care.

27:56.080 --> 27:58.440
 My parents were like, oh, he'll be fine.

27:59.760 --> 28:01.520
 So I was lucky.

28:01.520 --> 28:02.480
 That was cool.

28:02.480 --> 28:04.000
 Is he still with us?

28:05.160 --> 28:06.000
 You miss him?

28:07.560 --> 28:10.760
 Sure, yeah, he had Parkinson's and then cancer.

28:11.760 --> 28:13.360
 His last 10 years were tough.

28:15.040 --> 28:15.960
 And I killed him.

28:15.960 --> 28:18.240
 Killing a man like that's hard.

28:18.240 --> 28:19.360
 The mind?

28:19.360 --> 28:21.440
 Well, it was pretty good.

28:21.440 --> 28:23.720
 Parkinson's causes slow dementia

28:23.720 --> 28:27.720
 and the chemotherapy, I think, accelerated it.

28:29.040 --> 28:30.960
 But it was like hallucinogenic dementia.

28:30.960 --> 28:34.120
 So he was clever and funny and interesting

28:34.120 --> 28:37.880
 and it was pretty unusual.

28:37.880 --> 28:41.520
 Do you remember conversations from that time?

28:41.520 --> 28:43.920
 Like what, do you have fond memories of the guy?

28:43.920 --> 28:45.200
 Yeah, oh yeah.

28:45.200 --> 28:46.320
 Anything come to mind?

28:46.320 --> 28:50.360
 A friend told me one time I could draw a computer

28:50.360 --> 28:52.520
 on the way forward faster than anybody had ever met.

28:52.520 --> 28:54.960
 And I said, you should meet my dad.

28:54.960 --> 28:56.920
 Like when I was a kid, he'd come home and say,

28:56.920 --> 28:58.840
 I was driving by this bridge and I was thinking about it.

28:58.840 --> 28:59.840
 And he pulled out a piece of paper

28:59.840 --> 29:01.560
 and he'd draw the whole bridge.

29:01.560 --> 29:03.640
 He was a mechanical engineer.

29:03.640 --> 29:05.040
 And he would just draw the whole thing

29:05.040 --> 29:06.320
 and then he would tell me about it

29:06.320 --> 29:08.720
 and then tell me how he would have changed it.

29:08.720 --> 29:11.920
 And he had this idea that he could understand

29:11.920 --> 29:13.440
 and conceive anything.

29:13.440 --> 29:16.480
 And I just grew up with that, so that was natural.

29:16.480 --> 29:18.800
 So if, you know, like when I interview people,

29:18.800 --> 29:20.280
 I ask them to draw a picture of something

29:20.280 --> 29:21.840
 they did on the whiteboard.

29:21.840 --> 29:22.920
 And it's really interesting.

29:22.920 --> 29:25.960
 Like some people will draw a little box, you know,

29:25.960 --> 29:27.840
 and then they'll say, and then this talks to this

29:27.840 --> 29:30.120
 and I'll be like, that's just frustrating.

29:30.120 --> 29:31.760
 And then I had this other guy come in one time.

29:31.760 --> 29:34.520
 He says, well, I designed a floating point in this chip

29:34.520 --> 29:36.360
 but I'd really like to tell you how the whole thing works

29:36.360 --> 29:38.240
 and then tell you how the floating point works inside of it.

29:38.240 --> 29:39.080
 Do you mind if I do that?

29:39.080 --> 29:42.040
 He covered two whiteboards in like 30 minutes.

29:42.040 --> 29:42.880
 And I hired him.

29:42.880 --> 29:44.560
 Like, he was great.

29:44.560 --> 29:45.400
 There's craftsmen.

29:45.400 --> 29:47.040
 I mean, that's the craftsmanship to that.

29:47.040 --> 29:49.480
 Yeah, but also the mental agility

29:49.480 --> 29:51.360
 to understand the whole thing.

29:51.360 --> 29:52.200
 Right.

29:52.200 --> 29:54.760
 Put the pieces in context, you know,

29:54.760 --> 29:57.680
 real view of the balance of how the design worked.

29:58.640 --> 30:01.040
 Because if you don't understand it properly,

30:01.040 --> 30:02.240
 when you start to draw it,

30:02.240 --> 30:03.800
 you'll fill up half the whiteboard

30:03.800 --> 30:06.040
 with like a little piece of it and, you know,

30:06.040 --> 30:09.280
 like your ability to lay it out in an understandable way

30:09.280 --> 30:10.600
 takes a lot of understanding.

30:10.600 --> 30:13.480
 So. And be able to zoom in to the detail

30:13.480 --> 30:15.000
 and then zoom out to the picture.

30:15.000 --> 30:16.400
 Zoom out really fast.

30:16.400 --> 30:17.600
 What about the impossible thing?

30:17.600 --> 30:21.880
 You see, your dad believed that you can do anything.

30:22.960 --> 30:25.520
 That's a weird feature for a craftsman.

30:25.520 --> 30:26.680
 Yeah.

30:26.680 --> 30:30.800
 It seems that that echoes in your own behavior.

30:30.800 --> 30:32.120
 Like, that's the...

30:32.120 --> 30:35.200
 Well, it's not that anybody can do anything right now.

30:36.200 --> 30:37.040
 Right.

30:37.040 --> 30:39.680
 It's that if you work at it, you can get better at it.

30:39.680 --> 30:41.280
 And there might not be a limit.

30:43.080 --> 30:44.600
 And they did funny things like,

30:44.600 --> 30:46.120
 like he always wanted to play piano.

30:46.120 --> 30:48.440
 So at the end of his life, he started playing the piano.

30:48.440 --> 30:51.560
 When he had Parkinson's, I mean, he was terrible.

30:51.560 --> 30:53.520
 But he thought if he really worked out it in this life,

30:53.520 --> 30:56.360
 maybe the next life, he'd be better at it.

30:56.360 --> 30:57.600
 He might be onto something.

30:57.600 --> 30:58.440
 Yeah.

30:58.440 --> 30:59.760
 Good thing.

30:59.760 --> 31:00.920
 He enjoyed doing it.

31:00.920 --> 31:01.760
 Yeah.

31:01.760 --> 31:02.760
 So that's pretty funny.

31:04.120 --> 31:06.160
 Do you think the perfect is the enemy of the good

31:06.160 --> 31:08.160
 in hardware and software engineering?

31:08.160 --> 31:10.480
 It's like we were talking about JavaScript a little bit

31:10.480 --> 31:14.760
 and the messiness of the 10 day building process.

31:14.760 --> 31:15.600
 Yeah.

31:15.600 --> 31:17.240
 It's, you know, creative tension, right?

31:19.080 --> 31:21.440
 So creative tension is you have two different ideas

31:21.440 --> 31:22.720
 that you can't do both.

31:23.880 --> 31:24.720
 Right.

31:24.720 --> 31:28.360
 And the, but the fact that you want to do both causes you

31:28.360 --> 31:29.960
 to go try to solve that problem.

31:29.960 --> 31:31.160
 That's the creative part.

31:32.040 --> 31:35.960
 So if you're building computers, like some people say,

31:35.960 --> 31:38.280
 we have the schedule and anything that doesn't fit

31:38.280 --> 31:40.440
 in the schedule, we can't do, right?

31:40.440 --> 31:43.240
 And so they throw out the perfect cause I have a schedule.

31:44.280 --> 31:45.120
 I hate that.

31:46.600 --> 31:48.200
 Then there's other people to say,

31:48.200 --> 31:50.560
 we need to get this perfectly right.

31:50.560 --> 31:54.520
 And no matter what, you know, more people, more money, right?

31:55.480 --> 31:57.840
 And there's a really clear idea about what you want.

31:57.840 --> 32:00.720
 And some people are really good at articulating it, right?

32:00.720 --> 32:02.000
 So let's call that the perfect.

32:02.000 --> 32:02.840
 Yeah.

32:02.840 --> 32:03.680
 Yeah.

32:03.680 --> 32:04.520
 All right.

32:04.520 --> 32:05.840
 But that's also terrible cause they never ship anything.

32:05.840 --> 32:07.360
 They never hit any goals.

32:07.360 --> 32:09.960
 So now you have the, now you have your framework.

32:09.960 --> 32:10.800
 Yes.

32:10.800 --> 32:12.760
 You can't throw out stuff cause you can't get it done today.

32:12.760 --> 32:15.360
 Cause maybe you get it done tomorrow with the next project.

32:15.360 --> 32:16.200
 Right.

32:16.200 --> 32:19.520
 You can't, so you have to, I work with a guy that I really

32:19.520 --> 32:23.120
 like working with, but he over filters his ideas.

32:23.120 --> 32:24.760
 Over filters.

32:24.760 --> 32:26.600
 He'd start thinking about something.

32:26.600 --> 32:28.000
 And as soon as he figured out what's wrong with it,

32:28.000 --> 32:29.840
 he'd throw it out.

32:29.840 --> 32:31.240
 And then I start thinking about it.

32:31.240 --> 32:33.200
 And I, you know, you come up with an idea and then you find

32:33.200 --> 32:34.960
 out what's wrong with it.

32:34.960 --> 32:36.720
 And then you give it a little time to set.

32:36.720 --> 32:39.200
 Cause sometimes, you know, you figure out how to tweak it

32:39.200 --> 32:41.320
 or maybe that idea helps some other idea.

32:42.560 --> 32:45.040
 So idea generation is really funny.

32:45.040 --> 32:46.880
 So you have to give your idea space.

32:46.880 --> 32:49.720
 Like spaciousness of mind is key,

32:49.720 --> 32:53.360
 but you also have to execute programs and get shit done.

32:53.360 --> 32:55.480
 And then it turns out computer engineering is fun

32:55.480 --> 32:57.200
 because it takes, you know, a hundred people to build a

32:57.200 --> 33:00.560
 computer, 200 to 300, whatever the number is.

33:00.560 --> 33:05.480
 And people are so variable about, you know, temperament and,

33:05.480 --> 33:09.400
 you know, skill sets and stuff that in a big organization,

33:09.400 --> 33:11.960
 you find that the people who love the perfect ideas and the

33:11.960 --> 33:14.920
 people that want to get stuffed on yesterday and people like

33:14.920 --> 33:18.280
 that come up with ideas and people like the, let's say,

33:18.280 --> 33:19.240
 shoot down ideas.

33:19.240 --> 33:22.920
 And it takes the whole, it takes a large group of people.

33:22.920 --> 33:24.600
 So some are good at generating ideas.

33:24.600 --> 33:25.960
 Some are good at filtering ideas.

33:25.960 --> 33:30.960
 And in that giant mess, you're somehow, I guess the goal is

33:32.120 --> 33:36.080
 for that giant mess of people to find the perfect path

33:36.080 --> 33:38.480
 through the tension, the creative tension.

33:38.480 --> 33:42.000
 But like, how do you know when you said there's some people

33:42.000 --> 33:43.760
 good at articulating what perfect looks like,

33:43.760 --> 33:44.760
 what a good design is?

33:44.760 --> 33:49.760
 Like if you're sitting in a room and you have a set of ideas

33:51.040 --> 33:55.360
 about like how to design a better processor,

33:55.360 --> 33:58.840
 how do you know this is something special here?

33:58.840 --> 33:59.920
 This is a good idea.

33:59.920 --> 34:00.760
 Let's try this.

34:00.760 --> 34:03.080
 So if you ever brainstormed idea with a couple of people

34:03.080 --> 34:05.640
 that were really smart and you kind of go into it

34:05.640 --> 34:09.720
 and you don't quite understand it and you're working on it.

34:09.720 --> 34:12.200
 And then you start, you know, talking about it,

34:12.200 --> 34:16.160
 putting it on the whiteboard, maybe it takes days or weeks.

34:16.160 --> 34:18.640
 And then your brain starts to kind of synchronize.

34:18.640 --> 34:19.480
 It's really weird.

34:19.480 --> 34:24.480
 Like you start to see what each other is thinking and it starts to work.

34:28.440 --> 34:30.960
 Like you can see work, like my talent in computer design

34:30.960 --> 34:35.320
 is I can see how computers work in my head like really well.

34:35.320 --> 34:37.360
 And I know other people can do that too.

34:37.360 --> 34:40.440
 And when you're working with people that can do that,

34:40.440 --> 34:44.440
 like it is kind of an amazing experience.

34:45.360 --> 34:48.200
 And then every once in a while you get to that place

34:48.200 --> 34:50.200
 and then you find the flaw and it was just kind of funny

34:50.200 --> 34:52.480
 because you can fool yourself.

34:53.760 --> 34:58.080
 The two of you kind of drifted along in the direction that was useless.

34:58.080 --> 34:59.440
 Yeah, that happens too.

34:59.440 --> 35:04.120
 Like you have to, because, you know, the nice thing about computer design

35:04.120 --> 35:05.600
 is always reduction of practice.

35:05.600 --> 35:08.120
 Like you come up with your good ideas.

35:08.120 --> 35:11.160
 And I've noticed some architects who really love ideas

35:11.160 --> 35:13.080
 and then they work on them and they put it on the shelf

35:13.080 --> 35:14.800
 and they go work on the next idea and put it on the shelf

35:14.800 --> 35:16.840
 and they never reduce it to practice.

35:16.840 --> 35:18.760
 So they find out what's good and bad

35:18.760 --> 35:22.480
 because almost every time I've done something really new,

35:22.480 --> 35:25.640
 by the time it's done, like the good parts are good,

35:25.640 --> 35:27.600
 but I know all the flaws, like...

35:27.600 --> 35:31.560
 Yeah, would you say your career, just your own experience?

35:31.560 --> 35:35.240
 Is your career defined mostly by flaws or by successes?

35:35.240 --> 35:36.080
 Like if...

35:36.080 --> 35:38.000
 Again, there's great attention between those.

35:38.000 --> 35:43.000
 If you haven't tried hard, right, and done something new,

35:43.000 --> 35:45.760
 right, then you're not gonna be facing the challenges

35:45.760 --> 35:49.120
 when you build it, then you find out all the problems with it.

35:49.120 --> 35:49.960
 And...

35:49.960 --> 35:52.600
 But when you look back, do you see problems or...

35:52.600 --> 35:57.480
 Oh, when I look back, I think earlier in my career,

35:57.480 --> 36:00.320
 like EV5 was the second Alpha chip,

36:00.320 --> 36:03.720
 I was so embarrassed about the mistakes,

36:03.720 --> 36:05.720
 I could barely talk about it.

36:05.720 --> 36:07.520
 And it was in the Guinness Book of Rolls records

36:07.520 --> 36:09.840
 and it was the fastest processor on the planet.

36:09.840 --> 36:10.680
 Yeah.

36:10.680 --> 36:13.120
 So it was, and at some point I realized

36:13.120 --> 36:16.400
 that was really a bad mental framework to deal with,

36:16.400 --> 36:18.320
 like doing something new, we did a bunch of new things

36:18.320 --> 36:20.560
 and some of them worked out great and some were bad.

36:20.560 --> 36:24.120
 And we learned a lot from it and then the next one,

36:24.120 --> 36:24.960
 we learned a lot.

36:24.960 --> 36:28.960
 That also, EV6 also had some really cool things in it.

36:28.960 --> 36:31.360
 I think the proportion of good stuff went up,

36:31.360 --> 36:33.840
 but it had a couple of fatal flaws in it

36:33.840 --> 36:35.800
 that were painful.

36:35.800 --> 36:37.360
 And then...

36:37.360 --> 36:40.880
 You learned to channel the pain into like pride.

36:40.880 --> 36:44.680
 Not pride really, just realization

36:44.680 --> 36:48.560
 about how the world works or how that kind of idea set works.

36:48.560 --> 36:50.640
 Life is suffering, that's the reality.

36:50.640 --> 36:51.480
 What...

36:51.480 --> 36:52.800
 No, it's not.

36:52.800 --> 36:53.640
 Well...

36:53.640 --> 36:55.640
 I know the Buddhists have that and a couple of other people

36:55.640 --> 36:56.480
 are stuck on it.

36:56.480 --> 36:59.840
 No, it's, you know, there's just kind of weird combination

36:59.840 --> 37:03.080
 of good and bad and light and darkness

37:03.080 --> 37:04.800
 that you have to deal with.

37:04.800 --> 37:07.600
 Yeah, there's definitely lots of suffering in the world.

37:07.600 --> 37:08.840
 Depends on the perspective.

37:08.840 --> 37:10.640
 It seems like there's way more darkness,

37:10.640 --> 37:13.760
 but that makes the light part really nice.

37:13.760 --> 37:14.600
 What...

37:14.600 --> 37:21.200
 Computing hardware or just any kind of even software design,

37:21.200 --> 37:24.800
 are you defined beautiful from your own work,

37:24.800 --> 37:27.600
 from other people's work?

37:27.600 --> 37:29.200
 That you're just...

37:29.200 --> 37:31.920
 We were just talking about the kind of software

37:31.920 --> 37:37.240
 that you're just... We were just talking about the battleground

37:37.240 --> 37:39.160
 of flaws and mistakes and errors,

37:39.160 --> 37:42.440
 but things that were just beautifully done.

37:42.440 --> 37:44.400
 Is there something that pops to mind?

37:44.400 --> 37:47.840
 Well, when things are beautifully done,

37:47.840 --> 37:53.640
 usually there's a well thought out set of abstraction layers.

37:53.640 --> 37:56.360
 So the whole thing works in unison nicely.

37:56.360 --> 37:57.280
 Yes.

37:57.280 --> 37:59.280
 And when I say abstraction layer,

37:59.280 --> 38:01.080
 that means two different components

38:01.080 --> 38:04.840
 when they work together, they work independently.

38:04.840 --> 38:07.640
 They don't have to know what the other one is doing.

38:07.640 --> 38:08.600
 So that decoupling.

38:08.600 --> 38:11.400
 Yeah, so the famous one was the network stack.

38:11.400 --> 38:13.000
 Like there's a seven layer network stack,

38:13.000 --> 38:16.280
 you know, data transport and protocol and all the layers.

38:16.280 --> 38:19.880
 And the innovation was is when they really got that right.

38:19.880 --> 38:22.840
 Because networks before that didn't define those very well.

38:22.840 --> 38:26.120
 The layers could innovate independently

38:26.120 --> 38:28.640
 and occasionally the layer boundary would...

38:28.640 --> 38:30.920
 The interface would be upgraded.

38:30.920 --> 38:35.640
 And that let, you know, the design space breathe.

38:35.640 --> 38:37.760
 You could do something new in layer seven

38:37.760 --> 38:40.520
 without having to worry about how layer four worked.

38:40.520 --> 38:42.920
 And so good design does that.

38:42.920 --> 38:45.160
 And you see it in processor designs.

38:45.160 --> 38:48.520
 When we did the Zen design at AMD,

38:48.520 --> 38:51.880
 we made several components very modular.

38:51.880 --> 38:54.640
 And, you know, my insistence at the top was

38:54.640 --> 38:56.560
 I wanted all the interfaces defined

38:56.560 --> 38:59.280
 before we wrote the RTL for the pieces.

38:59.280 --> 39:01.000
 One of the verification leads said,

39:01.000 --> 39:03.520
 if we do this right, I can test the pieces

39:03.520 --> 39:04.840
 so well independently.

39:04.840 --> 39:06.400
 When we put it together,

39:06.400 --> 39:08.080
 we won't find all these interaction bugs

39:08.080 --> 39:10.680
 because the floating point knows how the cache works.

39:10.680 --> 39:14.160
 And I was a little skeptical, but he was mostly right.

39:14.160 --> 39:18.920
 That the modularity design greatly improved the quality.

39:18.920 --> 39:20.480
 Is that universally true in general?

39:20.480 --> 39:21.800
 Would you say about good designs,

39:21.800 --> 39:24.080
 the modularity is like usually modularity?

39:24.080 --> 39:25.120
 Well, we talked about this before.

39:25.120 --> 39:26.360
 Humans are only so smart.

39:26.360 --> 39:29.440
 And we're not getting any smarter, right?

39:29.440 --> 39:32.240
 But the complexity of things is going up.

39:32.240 --> 39:35.520
 So, you know, a beautiful design

39:35.520 --> 39:37.960
 can't be bigger than the person doing it.

39:37.960 --> 39:40.000
 It's just, you know, their piece of it.

39:40.000 --> 39:42.440
 Like the odds of you doing a really beautiful design

39:42.440 --> 39:46.560
 of something that's way too hard for you is low, right?

39:46.560 --> 39:48.000
 If it's way too simple for you,

39:48.000 --> 39:49.000
 it's not that interesting.

39:49.000 --> 39:50.600
 It's like, well, anybody could do that.

39:50.600 --> 39:54.720
 But when you get the right match of your expertise

39:54.720 --> 39:58.680
 and, you know, mental power to the right design size,

39:58.680 --> 40:00.400
 that's cool, but that's not big enough

40:00.400 --> 40:02.240
 to make a meaningful impact in the world.

40:02.240 --> 40:04.880
 So now you have to have some framework

40:04.880 --> 40:08.080
 to design the pieces so that the whole thing

40:08.080 --> 40:10.080
 is big and harmonious.

40:10.080 --> 40:13.520
 But, you know, when you put it together,

40:13.520 --> 40:18.520
 it's, you know, sufficiently interesting to be used.

40:18.880 --> 40:21.480
 And, you know, so that's like a beautiful design is.

40:21.480 --> 40:26.480
 Matching the limits of that human cognitive capacity

40:28.000 --> 40:30.360
 to the module you can create

40:30.360 --> 40:33.160
 and creating a nice interface between those modules.

40:33.160 --> 40:34.560
 And thereby, do you think there's a limit

40:34.560 --> 40:37.120
 to the kind of beautiful complex systems

40:37.120 --> 40:41.000
 we can build with this kind of modular design?

40:41.000 --> 40:46.000
 It's like, you know, if we build increasingly more complicated,

40:46.520 --> 40:50.920
 you can think of like the internet, okay, let's scale it down.

40:50.920 --> 40:52.320
 No, you can think of like social network,

40:52.320 --> 40:56.320
 like Twitter as one computing system.

40:57.320 --> 41:00.840
 And, but those are the little modules, right?

41:00.840 --> 41:03.800
 Well, it's built on, it's built on so many components

41:03.800 --> 41:06.000
 nobody at Twitter even understands.

41:06.000 --> 41:06.840
 Right.

41:06.840 --> 41:09.280
 So, so, so if an alien showed up and looked at Twitter,

41:09.280 --> 41:11.160
 he wouldn't just see Twitter as a beautiful,

41:11.160 --> 41:14.400
 simple thing that everybody uses, which is really big.

41:14.400 --> 41:18.160
 You would see the network, it runs on the fiber optics,

41:18.160 --> 41:19.840
 the data is transported to computers.

41:19.840 --> 41:22.040
 The whole thing is so bloody complicated,

41:22.040 --> 41:23.720
 nobody at Twitter understands it.

41:23.720 --> 41:25.760
 And so that's what the alien would see.

41:25.760 --> 41:28.800
 So yeah, if an alien showed up and looked at Twitter

41:28.800 --> 41:32.040
 or looked at the various different network systems

41:32.040 --> 41:33.680
 that you can see on earth.

41:33.680 --> 41:35.000
 So imagine they were really smart

41:35.000 --> 41:36.720
 that could comprehend the whole thing.

41:36.720 --> 41:40.160
 And then they sort of, you know, evaluated the human

41:40.160 --> 41:41.600
 and thought, this is really interesting.

41:41.600 --> 41:45.560
 No human on this planet comprehends the system they built.

41:45.560 --> 41:48.200
 No individual, well, would they even see individual humans?

41:48.200 --> 41:51.120
 That's interesting, like we humans are very human centric,

41:51.120 --> 41:52.760
 entity centric.

41:52.760 --> 41:56.880
 And so we think of us as the central organism

41:56.880 --> 41:59.840
 and the networks as just the connection of organisms,

41:59.840 --> 42:02.520
 but from a perspective of an alien,

42:02.520 --> 42:05.400
 from an outside perspective, it seems like.

42:05.400 --> 42:08.960
 Yeah, I get it, we're the answer to the ant colony.

42:08.960 --> 42:10.480
 The ant colony, yeah.

42:10.480 --> 42:12.760
 Or the result of production of the ant colony,

42:12.760 --> 42:16.280
 which is like cities and it's,

42:16.280 --> 42:19.840
 it's a, in that sense, humans are pretty impressive.

42:19.840 --> 42:23.080
 The modularity that we're able to and the,

42:23.080 --> 42:25.920
 and how robust we are to noise and mutation,

42:25.920 --> 42:26.760
 all that kind of stuff.

42:26.760 --> 42:28.480
 Well, that's cause it's stress tested all the time.

42:28.480 --> 42:29.320
 Yeah.

42:29.320 --> 42:31.040
 You know, you build all these cities with buildings

42:31.040 --> 42:33.000
 and you get earthquakes occasionally and.

42:33.000 --> 42:33.840
 Wars.

42:33.840 --> 42:35.520
 You know, wars, earthquakes.

42:35.520 --> 42:37.960
 Viruses every once in a while.

42:37.960 --> 42:40.320
 Changes in business plans for, you know,

42:40.320 --> 42:41.600
 like shipping or something.

42:41.600 --> 42:44.720
 Like, as long as there's all stress tested,

42:44.720 --> 42:48.520
 then it keeps adapting to the situation.

42:48.520 --> 42:52.480
 So that's a curious phenomena.

42:52.480 --> 42:54.840
 Well, let's go, let's talk about Moore's Law a little bit.

42:54.840 --> 42:59.840
 It's a, at the broad view of Moore's Law,

43:00.160 --> 43:03.760
 where it's just exponential improvement of computing

43:03.760 --> 43:06.760
 capability, like OpenAI, for example,

43:06.760 --> 43:11.760
 recently published this kind of papers looking

43:11.760 --> 43:15.400
 at the exponential improvement in the training efficiency

43:15.400 --> 43:16.760
 of neural networks.

43:16.760 --> 43:18.560
 For like ImageNet and all that kind of stuff,

43:18.560 --> 43:19.920
 we just got better on this,

43:19.920 --> 43:22.280
 this is purely software side,

43:22.280 --> 43:25.600
 just figuring out better tricks and algorithms

43:25.600 --> 43:26.920
 for training neural networks.

43:26.920 --> 43:30.600
 And that seems to be improving significantly faster

43:30.600 --> 43:33.080
 than the Moore's Law prediction, you know?

43:33.080 --> 43:34.920
 So that's in the software space.

43:34.920 --> 43:39.120
 Like, what do you think if Moore's Law continues

43:39.120 --> 43:42.880
 or if the general version of Moore's Law continues,

43:42.880 --> 43:45.320
 do you think that comes mostly from the hardware,

43:45.320 --> 43:47.560
 from the software, some mix of the two,

43:47.560 --> 43:50.000
 some interesting totally,

43:50.000 --> 43:52.800
 so not the reduction of the size of the transistor

43:52.800 --> 43:57.800
 kind of thing, but more in the totally interesting

43:57.800 --> 43:59.840
 kinds of innovations in the hardware space,

43:59.840 --> 44:01.240
 all that kind of stuff.

44:01.240 --> 44:04.480
 Well, there's like half a dozen things going on

44:04.480 --> 44:05.560
 in that graph.

44:05.560 --> 44:08.480
 So one is there's initial innovations

44:08.480 --> 44:11.680
 that had a lot of headroom to be exploited.

44:11.680 --> 44:13.960
 So, you know, the efficiency of the networks

44:13.960 --> 44:15.920
 has improved dramatically.

44:15.920 --> 44:19.600
 And then the decomposability of those and the use,

44:19.600 --> 44:21.400
 you know, they started running on one computer,

44:21.400 --> 44:23.720
 then multiple computers and then multiple GPUs

44:23.720 --> 44:27.080
 and then arrays of GPUs and they're up to thousands.

44:27.080 --> 44:30.640
 And at some point, so it's sort of like,

44:30.640 --> 44:32.280
 they were consumed, they were going from

44:32.280 --> 44:33.880
 like a single computer application

44:33.880 --> 44:36.240
 to a thousand computer application.

44:36.240 --> 44:38.200
 So that's not really a Moore's Law thing.

44:38.200 --> 44:39.560
 That's an independent vector.

44:39.560 --> 44:42.360
 How many computers can I put on this problem?

44:42.360 --> 44:44.240
 Because the computers themselves are getting better

44:44.240 --> 44:46.000
 on like a Moore's Law rate,

44:46.000 --> 44:49.440
 but their ability to go from one to 10 to 100 to a thousand

44:49.440 --> 44:51.200
 you know, was something.

44:51.200 --> 44:53.320
 And then multiplied by, you know,

44:53.320 --> 44:55.320
 the amount of computers it took to resolve

44:55.320 --> 44:58.320
 like AlexNet, to ResNet, to transformers.

44:58.320 --> 45:01.720
 It's been quite, you know, steady improvements.

45:01.720 --> 45:03.320
 But those are like S cores, aren't they?

45:03.320 --> 45:04.960
 That's the exactly kind of S cores

45:04.960 --> 45:07.640
 that are underlying Moore's Law from the very beginning.

45:07.640 --> 45:12.640
 So what's the biggest, what's the most productive,

45:13.400 --> 45:16.760
 rich source of S curves in the future, do you think?

45:16.760 --> 45:18.720
 Is it hardware or is it software?

45:18.720 --> 45:23.600
 So hardware is going to move along relatively slowly.

45:23.600 --> 45:27.040
 Like, you know, double performance every two years.

45:27.040 --> 45:29.600
 There's still, I like how you call that slow.

45:29.600 --> 45:31.400
 You know, it's the slow version.

45:31.400 --> 45:33.160
 The snail's pace of Moore's Law.

45:33.160 --> 45:38.080
 Maybe we should, we should trade mark that one.

45:38.080 --> 45:41.480
 Whereas the scaling by number of computers,

45:41.480 --> 45:43.400
 you know, can go much faster.

45:43.400 --> 45:46.080
 You know, I'm sure at some point, Google had a,

45:46.080 --> 45:47.520
 you know, their initial search engine

45:47.520 --> 45:50.080
 was running on a laptop, you know, like,

45:50.080 --> 45:52.520
 and at some point they really worked on scaling that.

45:52.520 --> 45:55.880
 And then they factored the indexer from, you know,

45:55.880 --> 45:57.440
 this piece and this piece and this piece

45:57.440 --> 45:59.280
 and they spread the data on more and more things.

45:59.280 --> 46:02.760
 And, you know, they did a dozen innovations.

46:02.760 --> 46:05.360
 But as they scaled up the number of computers on that,

46:05.360 --> 46:08.280
 it kept breaking, finding new bottlenecks in their software

46:08.280 --> 46:11.720
 and their schedulers and made them rethink.

46:11.720 --> 46:13.920
 Like, it seems insane to do a scheduler

46:13.920 --> 46:16.720
 across a thousand computers who schedule parts of it

46:16.720 --> 46:19.000
 and then send the results to one computer.

46:19.000 --> 46:21.400
 But if you want to schedule a million searches,

46:21.400 --> 46:23.200
 that makes perfect sense.

46:23.200 --> 46:26.880
 So there's, the scaling by just quantity

46:26.880 --> 46:28.960
 is probably the richest thing.

46:28.960 --> 46:32.880
 But then as you scale quantity, like a network

46:32.880 --> 46:34.680
 that was great on a hundred computers,

46:34.680 --> 46:36.560
 maybe completely the wrong one,

46:36.560 --> 46:39.640
 you may pick a network that's 10 times slower

46:39.640 --> 46:42.520
 on 10,000 computers, like per computer.

46:42.520 --> 46:45.800
 But if you go from a hundred to 10,000, that's a hundred times.

46:45.800 --> 46:47.240
 So that's one of the things that happened

46:47.240 --> 46:48.760
 when we did internet scaling.

46:48.760 --> 46:52.560
 This efficiency went down, not up.

46:52.560 --> 46:55.520
 The future of computing is inefficiency, not efficiency.

46:55.520 --> 46:57.600
 But scales, inefficient scale.

46:57.600 --> 47:01.840
 It's scaling faster than inefficiency by two.

47:01.840 --> 47:03.840
 And as long as there's, you know, dollar value there,

47:03.840 --> 47:06.000
 like scaling costs lots of money.

47:06.000 --> 47:08.200
 But Google showed, Facebook showed, everybody showed

47:08.200 --> 47:10.720
 that the scale was where the money was at.

47:10.720 --> 47:13.800
 It was worth the financial.

47:13.800 --> 47:16.440
 Do you think, is it possible

47:16.440 --> 47:19.640
 that like basically the entirety of Earth

47:19.640 --> 47:21.800
 will be like a computing surface?

47:21.800 --> 47:24.440
 Like this table will be doing computing.

47:24.440 --> 47:26.120
 This hedgehog will be doing computing.

47:26.120 --> 47:28.160
 Like everything really inefficient

47:28.160 --> 47:29.560
 done computing will be lover.

47:29.560 --> 47:31.840
 Science fiction books, they call it computronium.

47:31.840 --> 47:32.680
 Computronium?

47:32.680 --> 47:34.720
 We turn everything into computing.

47:34.720 --> 47:38.000
 Well, most of the elements aren't very good for anything.

47:38.000 --> 47:39.960
 Like you're not gonna make a computer out of iron.

47:39.960 --> 47:42.560
 Like, you know, silicon and carbon

47:42.560 --> 47:44.120
 have like nice structures.

47:45.080 --> 47:48.680
 You know, we'll see what you can do with the rest of it.

47:48.680 --> 47:50.440
 People talk about, well, maybe you can turn the sun

47:50.440 --> 47:53.440
 into a computer, but it's hydrogen.

47:53.440 --> 47:55.800
 And a little bit of helium, so.

47:55.800 --> 47:57.960
 What I mean is more like actually

47:57.960 --> 47:59.920
 just adding computers to everything.

47:59.920 --> 48:00.760
 Oh, okay.

48:00.760 --> 48:02.560
 I thought you were just converting all the mass

48:02.560 --> 48:04.240
 of the universe into computer.

48:04.240 --> 48:05.080
 No, no, no.

48:05.080 --> 48:05.920
 So not using.

48:05.920 --> 48:07.600
 To be ironic from the simulation point of view

48:07.600 --> 48:10.720
 is like the simulator build mass, the simulate.

48:12.000 --> 48:12.840
 Yeah, I mean, yeah.

48:12.840 --> 48:14.960
 So, I mean, ultimately this is all heading

48:14.960 --> 48:15.800
 towards the simulation.

48:15.800 --> 48:18.440
 Yeah, well, I think I might have told you this story.

48:18.440 --> 48:20.280
 At Tesla, they were deciding,

48:20.280 --> 48:22.400
 so they wanna measure the current coming out of the battery

48:22.400 --> 48:25.880
 and they decided between putting a resistor in there

48:25.880 --> 48:29.360
 and putting a computer with a sensor in there.

48:29.360 --> 48:31.480
 And the computer was faster than the computer

48:31.480 --> 48:34.120
 I worked on in 1982.

48:34.120 --> 48:35.520
 And we chose the computer

48:35.520 --> 48:37.520
 because it was cheaper than the resistor.

48:38.640 --> 48:42.320
 So, sure, this hedgehog, you know, costs $13

48:42.320 --> 48:45.120
 and we can put an AI that's as smart as you

48:45.120 --> 48:46.040
 in there for five bucks.

48:46.040 --> 48:46.880
 It'll have one.

48:48.240 --> 48:51.760
 You know, so computers will be everywhere.

48:51.760 --> 48:53.720
 I was hoping it wouldn't be smarter than me

48:53.720 --> 48:54.600
 because...

48:54.600 --> 48:56.640
 Well, everything's gonna be smarter than you.

48:56.640 --> 48:58.000
 But you were saying it's inefficient.

48:58.000 --> 49:00.200
 I thought it was better to have a lot of dumb things.

49:00.200 --> 49:02.720
 Well, Moore's Law will slowly compact that stuff.

49:02.720 --> 49:04.840
 So even the dumb things will be smarter than us.

49:04.840 --> 49:06.000
 The dumb things are gonna be smart

49:06.000 --> 49:08.000
 or they're gonna be smart enough to talk to something

49:08.000 --> 49:09.000
 that's really smart.

49:10.160 --> 49:13.600
 You know, it's like, well, just remember,

49:13.600 --> 49:16.120
 like a big computer chip, you know,

49:16.120 --> 49:18.680
 it's like an inch by an inch and, you know,

49:18.680 --> 49:22.440
 40 microns thick, it doesn't take very much,

49:22.440 --> 49:25.520
 very many atoms to make a high power computer.

49:25.520 --> 49:27.760
 And 10,000 of them can fit in the shoebox.

49:29.040 --> 49:31.440
 But, you know, you have the cooling and power problems,

49:31.440 --> 49:33.480
 but, you know, people are working on that.

49:33.480 --> 49:37.640
 But they still can't write compelling poetry or music

49:37.640 --> 49:40.360
 or understand what love is

49:40.360 --> 49:41.680
 or have a fear of mortality.

49:41.680 --> 49:43.480
 So we're still winning.

49:43.480 --> 49:46.160
 Neither can most of humanity, so...

49:46.160 --> 49:48.240
 Well, they can write books about it.

49:48.240 --> 49:53.240
 So, but speaking about this walk along the path

49:56.080 --> 49:58.760
 of innovation towards the dumb things

49:58.760 --> 50:00.120
 being smarter than humans,

50:00.120 --> 50:05.120
 you are now the CTO of Ten Storent as of two months ago.

50:08.560 --> 50:12.040
 They built hardware for deep learning.

50:13.840 --> 50:16.160
 How do you build scalable and efficient deep learning?

50:16.160 --> 50:17.520
 This is such a fascinating space.

50:17.520 --> 50:18.760
 Yeah, yeah, so it's interesting.

50:18.760 --> 50:20.800
 So up until recently,

50:20.800 --> 50:22.360
 I thought there was two kinds of computers.

50:22.360 --> 50:25.400
 There are serial computers that run like C programs,

50:25.400 --> 50:27.120
 and then there's parallel computers.

50:27.120 --> 50:29.360
 So the way I think about it is, you know,

50:29.360 --> 50:31.920
 parallel computers have given parallelism.

50:31.920 --> 50:34.800
 Like GPUs are great cause you have a million pixels.

50:34.800 --> 50:37.520
 And modern GPUs run a program on every pixel.

50:37.520 --> 50:39.400
 They call it the shader program, right?

50:39.400 --> 50:42.480
 So, or like finite element analysis.

50:42.480 --> 50:43.960
 You built something, you know,

50:43.960 --> 50:45.560
 you make this into little tiny chunks.

50:45.560 --> 50:47.120
 You give each chunk to a computer.

50:47.120 --> 50:50.200
 So you're given all these chunks of parallelism like that.

50:50.200 --> 50:53.560
 But most C programs, you write this linear narrative

50:53.560 --> 50:55.600
 and you have to make it go fast.

50:55.600 --> 50:57.720
 To make it go fast, you predict all the branches,

50:57.720 --> 51:00.280
 all the data fetches and you run that more in parallel,

51:00.280 --> 51:01.920
 but that's found parallelism.

51:04.240 --> 51:08.400
 AI is, I'm still trying to decide how fundamental this is.

51:08.400 --> 51:10.920
 It's a given parallelism problem.

51:10.920 --> 51:14.800
 But the way people describe the neural networks

51:14.800 --> 51:17.920
 and then how they write them in PyTorch, it makes graphs.

51:17.920 --> 51:18.760
 Yeah.

51:18.760 --> 51:21.680
 That might be fundamentally different than the GPU kind of.

51:21.680 --> 51:23.280
 Parallelism, yeah, it might be.

51:23.280 --> 51:27.320
 Because when you run the GPU program on all the pixels,

51:27.320 --> 51:29.880
 you're running, you know, depends, you know,

51:29.880 --> 51:32.520
 this group of pixels say it's background blue

51:32.520 --> 51:34.000
 and it runs a really simple program.

51:34.000 --> 51:36.920
 This pixel is, you know, some patch of your face.

51:36.920 --> 51:39.520
 So you have some really interesting shader program

51:39.520 --> 51:41.720
 to give you the impression of translucency.

51:41.720 --> 51:43.960
 But the pixels themselves don't talk to each other.

51:43.960 --> 51:46.600
 There's no graph, right?

51:46.600 --> 51:48.200
 So you do the image

51:48.200 --> 51:51.280
 and then you do the next image and you do the next image

51:51.280 --> 51:55.600
 and you run 8 million pixels, 8 million programs every time

51:55.600 --> 51:59.600
 and modern GPUs have like 6,000 thread engines in them.

51:59.600 --> 52:02.080
 So, you know, to get 8 million pixels,

52:02.080 --> 52:06.160
 each one runs a program on, you know, 10 or 20 pixels.

52:06.160 --> 52:09.360
 And that's how they work, there's no graph.

52:09.360 --> 52:13.680
 But you think graph might be a totally new way

52:13.680 --> 52:14.840
 to think about hardware.

52:14.840 --> 52:18.080
 So, Roger Gattori and I have been having this good conversation

52:18.080 --> 52:20.560
 about given versus found parallelism.

52:20.560 --> 52:22.480
 And then the kind of walk,

52:22.480 --> 52:24.640
 as we got more transistors, like, you know,

52:24.640 --> 52:27.800
 computers way back when did stuff on scalar data.

52:27.800 --> 52:30.720
 Then we did on vector data, famous vector machines.

52:30.720 --> 52:34.480
 Now we're making computers that operate on matrices, right?

52:34.480 --> 52:38.840
 And then the category we said that was next was spatial.

52:38.840 --> 52:41.640
 Like imagine you have so much data that, you know,

52:41.640 --> 52:43.360
 you want to do the compute on this data.

52:43.360 --> 52:44.920
 And then when it's done,

52:44.920 --> 52:47.520
 it says send the result to this pile of data

52:47.520 --> 52:49.240
 on some software on that.

52:49.240 --> 52:53.040
 And it's better to think about it spatially

52:53.040 --> 52:56.080
 than to move all the data to a central processor

52:56.080 --> 52:57.560
 and do all the work.

52:57.560 --> 53:00.720
 So, spatially, I mean, moving in the space of data

53:00.720 --> 53:02.440
 as opposed to moving the data.

53:02.440 --> 53:05.320
 Yeah, you have a petabyte data space

53:05.320 --> 53:08.600
 spread across some huge array of computers.

53:08.600 --> 53:10.520
 And when you do a computation somewhere,

53:10.520 --> 53:12.240
 you send the result of that computation

53:12.240 --> 53:14.320
 or maybe a pointer to the next program

53:14.320 --> 53:16.600
 to some other piece of data and do it.

53:16.600 --> 53:18.760
 But I think a better word might be graph

53:18.760 --> 53:21.640
 and all the AI neural networks are graphs.

53:21.640 --> 53:24.000
 Do some computations and the result here,

53:24.000 --> 53:26.360
 do another computation, do a data transformation,

53:26.360 --> 53:30.320
 do emerging, do a pooling, do another computation.

53:30.320 --> 53:32.240
 Is it possible to compress and say

53:32.240 --> 53:34.520
 how we make this thing efficient,

53:34.520 --> 53:37.240
 this whole process efficient, this different?

53:37.240 --> 53:40.880
 So first, the fundamental elements in the graphs

53:40.880 --> 53:43.160
 are things like metrics, multiplies, convolutions,

53:43.160 --> 53:46.120
 data manipulations, and data movements.

53:46.120 --> 53:50.200
 So GPUs emulate those things with their little singles,

53:50.200 --> 53:53.080
 basically running a single threaded program.

53:53.080 --> 53:55.560
 And then there's an NVIDIA calls it a warp

53:55.560 --> 53:56.880
 where they group a bunch of programs

53:56.880 --> 53:59.680
 that are similar together for efficiency

53:59.680 --> 54:01.520
 and instruction use.

54:01.520 --> 54:04.920
 And then at a higher level, you take this graph

54:04.920 --> 54:07.200
 and you say this part of the graph is a matrix multiplier

54:07.200 --> 54:09.800
 which runs on these 30 G threads.

54:09.800 --> 54:12.600
 But the model at the bottom was built

54:12.600 --> 54:17.120
 for running programs on pixels, not executing graphs.

54:17.120 --> 54:19.400
 So it's emulation, ultimately.

54:19.400 --> 54:21.080
 So is it possible to build something

54:21.080 --> 54:23.040
 that natively runs graphs?

54:23.040 --> 54:26.240
 Yes, so it's what Ten Storent did.

54:26.240 --> 54:28.200
 So where are we on that?

54:28.200 --> 54:30.920
 How, like in the history of that effort,

54:30.920 --> 54:32.040
 are we in the early days?

54:32.040 --> 54:33.360
 Yeah, I think so.

54:33.360 --> 54:35.720
 Ten Storent started by a friend of mine,

54:35.720 --> 54:39.000
 Labisha Bajek, and I was his first investor.

54:39.000 --> 54:41.600
 So I've been kind of following him

54:41.600 --> 54:43.560
 and talking to him about it for years

54:43.560 --> 54:46.960
 and in the fall when I was considering things to do.

54:47.840 --> 54:51.560
 I decided, you know, we held a conference last year

54:51.560 --> 54:53.720
 with a friend to organize it.

54:53.720 --> 54:56.120
 And we wanted to bring in thinkers

54:56.120 --> 55:00.480
 and two of the people were Andre Carpathi and Chris Latner.

55:00.480 --> 55:03.400
 And Andre gave this talk, it's on YouTube,

55:03.400 --> 55:06.840
 called software 2.0, which I think is great.

55:06.840 --> 55:10.160
 Which is, we went from programmed computers,

55:10.160 --> 55:13.760
 where you write programs to data program computers.

55:13.760 --> 55:16.760
 You know, like the futures of software

55:16.760 --> 55:19.360
 is data programs, the networks.

55:19.360 --> 55:21.360
 And I think that's true.

55:21.360 --> 55:23.960
 And then Chris has been working,

55:23.960 --> 55:26.600
 he worked on LLVM, the low level virtual machine,

55:26.600 --> 55:29.080
 which became the intermediate representation

55:29.080 --> 55:31.320
 for all compilers.

55:31.320 --> 55:33.640
 And now he's working on another project called MLIR,

55:33.640 --> 55:36.400
 which is mid level intermediate representation,

55:36.400 --> 55:39.800
 which is essentially under the graph

55:39.800 --> 55:42.800
 about how do you represent that kind of computation

55:42.800 --> 55:44.320
 and then coordinate large numbers

55:44.320 --> 55:46.640
 of potentially heterogeneous computers.

55:47.840 --> 55:50.600
 And I would say technically 10 storents,

55:51.480 --> 55:54.880
 you know, two pillars of those two ideas,

55:54.880 --> 55:58.280
 software 2.0 and mid level representation.

55:58.280 --> 56:01.840
 But it's in service of executing graph programs.

56:01.840 --> 56:03.760
 The hardware is designed to do that.

56:03.760 --> 56:06.440
 So that's including the hardware piece.

56:06.440 --> 56:08.440
 And then the other cool thing is,

56:08.440 --> 56:10.040
 for a relatively small amount of money,

56:10.040 --> 56:13.320
 they did a test chip and two production chips.

56:13.320 --> 56:15.320
 So it's like a super effective team.

56:15.320 --> 56:18.160
 And unlike some AI startups,

56:18.160 --> 56:20.120
 where if you don't build the hardware

56:20.120 --> 56:22.840
 to run the software that they really want to do,

56:22.840 --> 56:26.040
 then you have to fix it by writing lots more software.

56:26.040 --> 56:28.200
 So the hardware naturally does,

56:28.200 --> 56:31.800
 matrix multiply, convolution, the data manipulations,

56:31.800 --> 56:35.320
 and the data movement between processing elements

56:35.320 --> 56:37.600
 that you can see in the graph,

56:38.760 --> 56:40.320
 which I think is all pretty clever.

56:40.320 --> 56:45.040
 And that's what I'm working on now.

56:45.040 --> 56:49.760
 So I think it's called the grace call processor

56:49.760 --> 56:51.240
 introduced last year.

56:51.240 --> 56:53.160
 It's, you know, there's a bunch of measures

56:53.160 --> 56:55.560
 of performance we're talking about, horses.

56:55.560 --> 56:59.840
 It seems to outperform 368 trillion operations per second.

56:59.840 --> 57:03.240
 It seems to outperform NVIDIA's Tesla T4 system.

57:03.240 --> 57:04.720
 So these are just numbers.

57:04.720 --> 57:07.600
 What do they actually mean in real world performance?

57:07.600 --> 57:11.160
 Like what are the metrics for you that you're chasing

57:11.160 --> 57:12.520
 in your horse racing?

57:12.520 --> 57:13.840
 What do you care about?

57:13.840 --> 57:17.680
 Well, first, so the native language of,

57:17.680 --> 57:20.360
 you know, people who write AI network programs

57:20.360 --> 57:22.520
 is PyTorch now, PyTorch TensorFlow.

57:22.520 --> 57:24.000
 There's a couple others.

57:24.000 --> 57:25.800
 The PyTorch is one over TensorFlow,

57:25.800 --> 57:27.960
 this is just, I'm not an expert on that.

57:27.960 --> 57:30.440
 I know many people have switched from TensorFlow

57:30.440 --> 57:31.280
 to PyTorch.

57:31.280 --> 57:32.120
 Yeah.

57:32.120 --> 57:33.800
 And there's technical reasons for it.

57:33.800 --> 57:35.880
 I use both, both are still awesome.

57:35.880 --> 57:37.120
 Both are still awesome.

57:37.120 --> 57:39.880
 But the deepest love is for PyTorch currently.

57:39.880 --> 57:41.320
 Yeah, there's more love for that.

57:41.320 --> 57:42.560
 And that may change.

57:42.560 --> 57:46.640
 So the first thing is when they write their programs

57:46.640 --> 57:50.400
 can the hardware execute it pretty much as it was written.

57:50.400 --> 57:51.240
 Right.

57:51.240 --> 57:53.280
 So PyTorch turns into a graph.

57:53.280 --> 57:55.520
 We have a graph compiler that makes that graph.

57:55.520 --> 57:57.440
 Then it fractions the graph down.

57:57.440 --> 57:58.800
 So if you have big matrix multiply,

57:58.800 --> 58:00.120
 we turn it into right size chunks

58:00.120 --> 58:02.160
 to run on the processing elements.

58:02.160 --> 58:05.120
 It hooks all the graph up, it lays out all the data.

58:05.120 --> 58:08.000
 There's a couple of mid level representations of it

58:08.000 --> 58:09.400
 that are also simulatable.

58:09.400 --> 58:12.120
 So that if you're writing the code,

58:12.120 --> 58:15.080
 you can see how it's gonna go through the machine,

58:15.080 --> 58:15.920
 which is pretty cool.

58:15.920 --> 58:17.680
 And then at the bottom it's scheduled kernels

58:17.680 --> 58:21.760
 like math, data manipulation, data movement kernels,

58:21.760 --> 58:22.840
 which do this stuff.

58:22.840 --> 58:26.200
 So we don't have to run, write a little program

58:26.200 --> 58:27.320
 to do matrix multiply.

58:27.320 --> 58:29.000
 Because we have a big matrix multiplier.

58:29.000 --> 58:31.280
 Like there's no SIMT program for that.

58:32.360 --> 58:36.000
 But there is scheduling for that, right?

58:36.000 --> 58:40.200
 So one of the goals is if you write a piece of PyTorch code

58:40.200 --> 58:41.240
 that looks pretty reasonable,

58:41.240 --> 58:43.480
 you should be able to compile it, run it on the hardware

58:43.480 --> 58:44.760
 without having to tweak it

58:44.760 --> 58:48.120
 and do all kinds of crazy things to get performance.

58:48.120 --> 58:50.040
 There's not a lot of intermediate steps.

58:50.040 --> 58:51.320
 It's running directly as written.

58:51.320 --> 58:53.960
 Like on a GPU, if you write a large matrix multiply

58:53.960 --> 58:56.000
 naively, you'll get 5% to 10%

58:56.000 --> 58:58.920
 there's a peak performance of the GPU, right?

58:58.920 --> 59:01.600
 And then there's a bunch of people published papers on this

59:01.600 --> 59:04.080
 and I read them about what steps do you have to do?

59:04.080 --> 59:06.760
 And it goes from pretty reasonable,

59:06.760 --> 59:08.480
 well transpose one of the matrices.

59:08.480 --> 59:12.440
 So you do rotor, not column ordered, you know, block it

59:12.440 --> 59:14.520
 so that you can put a block of the matrix

59:14.520 --> 59:17.720
 on different SMs, you know, groups of threads.

59:19.320 --> 59:21.160
 But some of it gets into little details

59:21.160 --> 59:23.000
 like you have to schedule it just so

59:23.000 --> 59:25.040
 so you don't have register conflicts.

59:25.040 --> 59:28.240
 So the, they call them CUDA ninjas.

59:29.080 --> 59:31.120
 CUDA ninjas, I love it.

59:31.120 --> 59:32.360
 To get to the optimal point,

59:32.360 --> 59:36.080
 you either write a pre, use a pre written library

59:36.080 --> 59:37.920
 which is a good strategy for some things

59:37.920 --> 59:39.640
 or you have to be an expert

59:39.640 --> 59:42.240
 in micro architecture to program it.

59:42.240 --> 59:43.520
 Right, so the optimization step

59:43.520 --> 59:45.000
 is way more complicated with the GPU.

59:45.000 --> 59:47.920
 So our goal is, if you write PyTorch

59:47.920 --> 59:49.600
 that's good PyTorch, you can do it.

59:49.600 --> 59:53.120
 Now there's, as the networks are evolving, you know

59:53.120 --> 59:56.360
 they've changed from convolutional to matrix multiply.

59:56.360 --> 59:58.080
 The people are talking about conditional graphs,

59:58.080 --> 59:59.800
 they're talking about very large matrices,

59:59.800 --> 1:00:01.760
 they're talking about sparsity.

1:00:01.760 --> 1:00:03.400
 They're talking about problems

1:00:03.400 --> 1:00:06.160
 that scale across many, many chips.

1:00:06.160 --> 1:00:11.160
 So the native, you know, data item is a packet.

1:00:11.520 --> 1:00:13.360
 Like so you send the packet to a processor,

1:00:13.360 --> 1:00:15.440
 it gets processed, it does a bunch of work

1:00:15.440 --> 1:00:17.680
 and then it may send packets to other processors

1:00:17.680 --> 1:00:22.120
 and they execute like a data flow graph kind of methodology.

1:00:22.120 --> 1:00:24.400
 Got it. We have a big network on chip

1:00:24.400 --> 1:00:27.800
 and then that second chip has 16 ethernet ports

1:00:27.800 --> 1:00:29.600
 to help lots of them together

1:00:29.600 --> 1:00:32.440
 and it's the same graph compiler across multiple chips.

1:00:32.440 --> 1:00:33.600
 So that's where the scale comes in.

1:00:33.600 --> 1:00:35.160
 So it's built to scale naturally.

1:00:35.160 --> 1:00:38.200
 Now, my experience with scaling is as you scale

1:00:38.200 --> 1:00:40.800
 you run into lots of interesting problems.

1:00:40.800 --> 1:00:43.240
 So scaling is the amount of the climb.

1:00:43.240 --> 1:00:45.040
 Yeah. So the hardware is built to do this

1:00:45.040 --> 1:00:47.720
 and then we're in the process of...

1:00:47.720 --> 1:00:49.200
 Is there a software part to this?

1:00:49.200 --> 1:00:51.680
 With ethernet and all that?

1:00:51.680 --> 1:00:55.680
 Well, the protocol at the bottom, you know, we send,

1:00:55.680 --> 1:00:59.800
 it's an ethernet phi, but the protocol basically says

1:00:59.800 --> 1:01:01.480
 send the packet from here to there.

1:01:01.480 --> 1:01:03.160
 It's all point to point.

1:01:03.160 --> 1:01:05.880
 The header bit says which processor to send it to

1:01:05.880 --> 1:01:09.600
 and we basically take a packet off our on chip network,

1:01:09.600 --> 1:01:13.040
 put an ethernet header on it, send it to the other end,

1:01:13.040 --> 1:01:14.920
 strip the header off and send it to the local thing.

1:01:14.920 --> 1:01:16.160
 It's pretty straightforward.

1:01:16.160 --> 1:01:18.200
 Human to human interaction is pretty straightforward too

1:01:18.200 --> 1:01:19.400
 but when you get a million of us

1:01:19.400 --> 1:01:21.680
 we're just some crazy stuff together.

1:01:21.680 --> 1:01:23.400
 Yeah, it can be fun.

1:01:23.400 --> 1:01:25.880
 So is that the goal is scale?

1:01:25.880 --> 1:01:28.960
 So like, for example, I've been recently doing a bunch

1:01:28.960 --> 1:01:31.560
 of robots at home for my own personal pleasure.

1:01:32.400 --> 1:01:35.840
 Am I going to ever use 10 storey or is this more for...

1:01:35.840 --> 1:01:37.240
 There's all kinds of problems.

1:01:37.240 --> 1:01:38.760
 Like there's small inference problems

1:01:38.760 --> 1:01:41.480
 or small training problems or big training problems.

1:01:41.480 --> 1:01:42.720
 What's the big goal?

1:01:42.720 --> 1:01:45.120
 Is it the big training problems

1:01:45.120 --> 1:01:46.320
 or the small training problems?

1:01:46.320 --> 1:01:48.080
 There's one of the goals is to scale

1:01:48.080 --> 1:01:50.720
 from 100 milliwatts to a megawatt.

1:01:51.720 --> 1:01:54.840
 So like really have some range on the problems

1:01:54.840 --> 1:01:57.360
 and the same kind of AI programs work

1:01:57.360 --> 1:01:59.320
 at all different levels.

1:01:59.320 --> 1:02:00.600
 So that's cool.

1:02:00.600 --> 1:02:03.600
 The natural, since the natural data item is a packet

1:02:03.600 --> 1:02:06.600
 that we can move around, it's built to scale

1:02:07.640 --> 1:02:10.800
 but so many people have small problems.

1:02:11.560 --> 1:02:12.400
 Right, right.

1:02:12.400 --> 1:02:16.400
 But like I said, that phone is a small problem to solve.

1:02:16.400 --> 1:02:19.960
 So do you see 10 storey potentially being inside a phone?

1:02:19.960 --> 1:02:22.600
 Well, the power efficiency of local memory,

1:02:22.600 --> 1:02:26.320
 local computation and the way we built it is pretty good.

1:02:26.320 --> 1:02:28.480
 And then there's a lot of efficiency

1:02:28.480 --> 1:02:31.480
 on being able to do conditional graphs in sparsity.

1:02:31.480 --> 1:02:34.480
 I think it's for complicated networks

1:02:34.480 --> 1:02:38.120
 that want to go in a small factor, it's going to be quite good.

1:02:38.120 --> 1:02:40.720
 But we have to prove that that's a fun problem.

1:02:40.720 --> 1:02:42.240
 And that's the early days of the company, right?

1:02:42.240 --> 1:02:44.560
 It's a couple of years, you said.

1:02:44.560 --> 1:02:47.560
 But you think, you invested, you think they're legit

1:02:47.560 --> 1:02:50.000
 as you join, well, that's...

1:02:50.000 --> 1:02:53.240
 Well, it's also, it's a really interesting place to be.

1:02:53.240 --> 1:02:55.720
 Like the AI world is exploding, you know?

1:02:55.720 --> 1:02:58.480
 And I looked at some other opportunities

1:02:58.480 --> 1:03:01.520
 like build a faster processor, which people want.

1:03:01.520 --> 1:03:03.760
 But that's more on an incremental path

1:03:03.760 --> 1:03:07.920
 than what's going to happen in AI in the next 10 years.

1:03:07.920 --> 1:03:12.280
 So this is kind of an exciting place to be part of.

1:03:12.280 --> 1:03:15.200
 The revolutions will be happening in the very space that's...

1:03:15.200 --> 1:03:16.600
 And then lots of people are working on it,

1:03:16.600 --> 1:03:18.840
 but there's lots of technical reasons why some of them,

1:03:18.840 --> 1:03:21.440
 you know, aren't going to work out that well.

1:03:21.440 --> 1:03:23.600
 And that's interesting.

1:03:23.600 --> 1:03:27.520
 And there's also the same problem about getting the basics right.

1:03:27.520 --> 1:03:30.160
 Like we've talked to customers about exciting features.

1:03:30.160 --> 1:03:32.960
 And at some point, we realized that each of the networks,

1:03:32.960 --> 1:03:35.880
 realizing they want to hear first about memory bandwidth,

1:03:35.880 --> 1:03:39.240
 local bandwidth, compute intensity, programmability.

1:03:39.240 --> 1:03:42.000
 They want to know the basics, power management,

1:03:42.000 --> 1:03:43.320
 how the network ports work.

1:03:43.320 --> 1:03:44.160
 What are the basics?

1:03:44.160 --> 1:03:46.120
 Do all the basics work?

1:03:46.120 --> 1:03:47.800
 Because it's easy to say we've got this great idea

1:03:47.800 --> 1:03:49.880
 of the, you know, the crack GPT3.

1:03:51.040 --> 1:03:54.000
 But the people we talk to want to say,

1:03:54.000 --> 1:03:55.360
 if I buy the...

1:03:55.360 --> 1:03:58.640
 So we have a piece of express card with our chip on it.

1:03:58.640 --> 1:04:00.840
 If you buy the card, you plug it in your machine

1:04:00.840 --> 1:04:01.920
 to download the driver.

1:04:01.920 --> 1:04:05.040
 How long does it take me to get my network to run?

1:04:05.040 --> 1:04:05.880
 Right.

1:04:05.880 --> 1:04:06.920
 You know, that's a real question.

1:04:06.920 --> 1:04:08.320
 It's a very basic question.

1:04:08.320 --> 1:04:09.320
 So, yeah.

1:04:09.320 --> 1:04:10.480
 Is there an answer to that yet?

1:04:10.480 --> 1:04:12.120
 Or is it's trying to get to it?

1:04:12.120 --> 1:04:13.400
 Our goal is like an hour.

1:04:13.400 --> 1:04:14.240
 Okay.

1:04:14.240 --> 1:04:15.840
 When can I buy a test for it?

1:04:16.800 --> 1:04:17.640
 Pretty soon.

1:04:17.640 --> 1:04:19.760
 For my, for the small case training.

1:04:19.760 --> 1:04:21.160
 Yeah, pretty soon.

1:04:21.160 --> 1:04:22.000
 Months.

1:04:22.000 --> 1:04:22.820
 Good.

1:04:22.820 --> 1:04:24.800
 I love the idea of you inside a room

1:04:24.800 --> 1:04:29.200
 with the Carpathian, under Carpathian, Chris Ladner.

1:04:32.000 --> 1:04:36.000
 Very, very interesting, very brilliant people,

1:04:36.000 --> 1:04:37.600
 very out of the box thinkers,

1:04:37.600 --> 1:04:40.000
 but also like first principles thinkers.

1:04:40.000 --> 1:04:42.680
 Well, they both get stuff done.

1:04:42.680 --> 1:04:44.960
 They only get stuff done to get their own projects done.

1:04:44.960 --> 1:04:47.040
 They talk about it clearly.

1:04:47.040 --> 1:04:48.760
 They educate large numbers of people

1:04:48.760 --> 1:04:50.560
 and they've created platforms for other people

1:04:50.560 --> 1:04:52.040
 to go do their stuff on.

1:04:52.040 --> 1:04:52.880
 Yeah.

1:04:52.880 --> 1:04:55.560
 The clear thinking that's able to be communicated

1:04:55.560 --> 1:04:57.240
 is kind of impressive.

1:04:57.240 --> 1:05:00.800
 It's kind of remarkable to, yeah, I'm a fan.

1:05:00.800 --> 1:05:02.040
 Well, let me ask,

1:05:02.040 --> 1:05:05.040
 because I talked to Chris actually a lot these days.

1:05:05.040 --> 1:05:08.120
 He's been a, one of the, just to give him a shout out

1:05:08.120 --> 1:05:13.120
 and he's been so supportive as a human being.

1:05:13.720 --> 1:05:16.320
 So everybody's quite different.

1:05:16.320 --> 1:05:17.680
 Like great engineers are different,

1:05:17.680 --> 1:05:20.800
 but he's been like sensitive to the human element

1:05:20.800 --> 1:05:22.280
 in a way that's been fascinating.

1:05:22.280 --> 1:05:23.800
 Like he was one of the early people

1:05:23.800 --> 1:05:27.920
 on this stupid podcast that I do to say like,

1:05:27.920 --> 1:05:32.120
 don't quit this thing and also talk to whoever

1:05:32.120 --> 1:05:34.160
 the hell you want to talk to.

1:05:34.160 --> 1:05:36.360
 That kind of from a legit engineer

1:05:36.360 --> 1:05:39.960
 to get like props and be like, you can do this.

1:05:39.960 --> 1:05:42.280
 That was, I mean, that's what a good leader does, right?

1:05:42.280 --> 1:05:45.120
 It's just kind of let a little kid do his thing.

1:05:45.120 --> 1:05:48.720
 Like go do it, let's see what turns out.

1:05:48.720 --> 1:05:50.520
 That's a pretty powerful thing.

1:05:50.520 --> 1:05:54.480
 But what do you, what's your sense about,

1:05:54.480 --> 1:05:57.680
 he used to be, no, I think stepped away from Google, right?

1:05:58.840 --> 1:06:01.800
 He said, sci fi, I think.

1:06:01.800 --> 1:06:03.840
 What's really impressive to you

1:06:03.840 --> 1:06:05.760
 about the things that Chris has worked on?

1:06:05.760 --> 1:06:08.320
 As we mentioned, the optimization,

1:06:08.320 --> 1:06:10.880
 the compiler design stuff, the LLVM,

1:06:11.960 --> 1:06:15.240
 then there's, he's also at Google work, the TPU stuff.

1:06:16.440 --> 1:06:19.400
 He's obviously worked on Swift,

1:06:19.400 --> 1:06:21.400
 so the programming language side,

1:06:21.400 --> 1:06:24.120
 talking about people that work in the entirety of the stack.

1:06:24.120 --> 1:06:25.320
 Yeah, yeah.

1:06:25.320 --> 1:06:27.960
 What, from your time interacting with Chris

1:06:27.960 --> 1:06:30.800
 and knowing the guy, what's really impressive to you?

1:06:30.800 --> 1:06:32.160
 It just inspires you.

1:06:32.160 --> 1:06:37.160
 Well, like LLVM became the de facto platform

1:06:39.800 --> 1:06:43.840
 for compilers, it's amazing.

1:06:43.840 --> 1:06:46.360
 And it was good code quality, good design choices.

1:06:46.360 --> 1:06:48.840
 He hit the right level of abstraction.

1:06:48.840 --> 1:06:52.040
 There's a little bit of the right time and the right place.

1:06:52.040 --> 1:06:55.440
 And then he built a new programming language called Swift,

1:06:55.440 --> 1:06:59.080
 which after, let's say some adoption resistance

1:06:59.080 --> 1:07:01.160
 became very successful.

1:07:01.160 --> 1:07:03.360
 I don't know that much about his work at Google,

1:07:03.360 --> 1:07:07.120
 although I know that, that was a typical,

1:07:07.120 --> 1:07:09.960
 they started TensorFlow stuff and they,

1:07:09.960 --> 1:07:12.800
 it was new, they wrote a lot of code

1:07:12.800 --> 1:07:16.120
 and then at some point it needed to be refactored to be,

1:07:17.200 --> 1:07:19.120
 because it's development slowed down,

1:07:19.120 --> 1:07:22.320
 why PyTorch started a little later and then passed it.

1:07:22.320 --> 1:07:23.960
 So he did a lot of work on that.

1:07:23.960 --> 1:07:26.000
 And then his idea about MLIR,

1:07:26.000 --> 1:07:28.240
 which is what people started to realize

1:07:28.240 --> 1:07:29.960
 is the complexity of the software stack

1:07:29.960 --> 1:07:33.560
 above the low level IR was getting so high

1:07:33.560 --> 1:07:36.240
 that forcing the features of that into low level

1:07:36.240 --> 1:07:38.760
 was putting too much of a burden on it.

1:07:38.760 --> 1:07:41.640
 So he's splitting that into multiple pieces.

1:07:41.640 --> 1:07:43.880
 And that was one of the inspirations for our software stack

1:07:43.880 --> 1:07:46.720
 where we have several intermediate representations

1:07:46.720 --> 1:07:48.800
 that are all executable.

1:07:48.800 --> 1:07:51.360
 And you can look at them and do transformations on them

1:07:51.360 --> 1:07:54.000
 before you lower the level.

1:07:54.000 --> 1:07:58.200
 So that was, I think we started before MLIR

1:07:58.200 --> 1:08:01.720
 really got far enough along to use,

1:08:01.720 --> 1:08:02.840
 but we're interested in that.

1:08:02.840 --> 1:08:04.880
 He's really excited about MLIR.

1:08:04.880 --> 1:08:06.680
 That's just like little baby.

1:08:06.680 --> 1:08:10.960
 So he, and there seems to be some profound ideas on that

1:08:10.960 --> 1:08:11.840
 that are really useful.

1:08:11.840 --> 1:08:15.000
 So each one of those things has been,

1:08:15.000 --> 1:08:17.840
 as the world of software gets more and more complicated,

1:08:17.840 --> 1:08:20.080
 how do we create the right abstraction levels

1:08:20.080 --> 1:08:23.360
 to simplify it in a way that people can now work independently

1:08:23.360 --> 1:08:25.200
 on different levels of it?

1:08:25.200 --> 1:08:29.040
 So I would say all three of those projects, LLVM, Swift,

1:08:29.040 --> 1:08:31.640
 and MLIR did that successfully.

1:08:31.640 --> 1:08:33.680
 So I'm interested in what he's going to do next

1:08:33.680 --> 1:08:34.840
 in the same kind of way.

1:08:34.840 --> 1:08:36.200
 Yes.

1:08:36.200 --> 1:08:41.840
 On either the TPU or maybe the NVIDIA GPU side,

1:08:41.840 --> 1:08:44.920
 how does TensorFlow, you think, or the ideas

1:08:44.920 --> 1:08:46.920
 underlying it doesn't have to be TensorFlow.

1:08:46.920 --> 1:08:54.840
 Just this kind of graph focused, graph centric hardware

1:08:54.840 --> 1:08:59.080
 deep learning centric hardware beat NVIDIAs.

1:08:59.080 --> 1:09:02.280
 Do you think it's possible for it to basically overtake NVIDIA?

1:09:02.280 --> 1:09:03.520
 Sure.

1:09:03.520 --> 1:09:05.640
 What's that process look like?

1:09:05.640 --> 1:09:08.120
 What's that journey look like, do you think?

1:09:08.120 --> 1:09:11.080
 Well, GPUs were built around shader programs

1:09:11.080 --> 1:09:14.440
 on millions of pixels, not to run graphs.

1:09:14.440 --> 1:09:17.400
 So there's a hypothesis that says,

1:09:17.400 --> 1:09:20.680
 the way the graphs are built is going

1:09:20.680 --> 1:09:24.120
 to be really interesting to be efficient on computing this.

1:09:24.120 --> 1:09:27.560
 And then the primitives is not a SIMD program.

1:09:27.560 --> 1:09:30.120
 It's a matrix multiply convolution.

1:09:30.120 --> 1:09:33.000
 And then the data manipulations are fairly extensive

1:09:33.000 --> 1:09:36.400
 about how do you do a fast transpose with a program.

1:09:36.400 --> 1:09:38.840
 I don't know if you've ever written a transpose program.

1:09:38.840 --> 1:09:42.200
 They're ugly and slow, but in hardware you can do really well.

1:09:42.200 --> 1:09:43.440
 I've got to give you an example.

1:09:43.440 --> 1:09:48.160
 So when GPU accelerators started doing triangles,

1:09:48.160 --> 1:09:51.200
 so you have a triangle which maps on a set of pixels.

1:09:51.200 --> 1:09:53.200
 So it's very easy, straightforward

1:09:53.200 --> 1:09:55.720
 to build a hardware engine that will find all those pixels.

1:09:55.720 --> 1:09:57.680
 And it's kind of weird because you walk along the triangle

1:09:57.680 --> 1:09:59.200
 to get to the edge.

1:09:59.200 --> 1:10:01.280
 And then you have to go back down to the next row

1:10:01.280 --> 1:10:02.080
 and walk along.

1:10:02.080 --> 1:10:04.040
 And then you have to decide on the edge

1:10:04.040 --> 1:10:08.000
 if the line of the triangle is like half on the pixel.

1:10:08.000 --> 1:10:09.120
 What's the pixel color?

1:10:09.120 --> 1:10:11.240
 Because it's half of this pixel and half the next one.

1:10:11.240 --> 1:10:13.000
 That's called rasterization.

1:10:13.000 --> 1:10:15.920
 And you're saying that could be done in hardware?

1:10:15.920 --> 1:10:19.320
 No, that's an example of that operation

1:10:19.320 --> 1:10:22.040
 as a software program is really bad.

1:10:22.040 --> 1:10:24.360
 I've written a program that did rasterization.

1:10:24.360 --> 1:10:26.840
 The hardware that does it has actually less code

1:10:26.840 --> 1:10:28.960
 than the software program that does it.

1:10:28.960 --> 1:10:31.840
 And it's way faster.

1:10:31.840 --> 1:10:35.440
 So there are certain times when the abstraction you have

1:10:35.440 --> 1:10:39.640
 rasterize a triangle, execute a graph,

1:10:39.640 --> 1:10:42.160
 components of a graph, the right thing

1:10:42.160 --> 1:10:43.800
 to do in the hardware software boundary

1:10:43.800 --> 1:10:45.800
 is for the hardware to naturally do it.

1:10:45.800 --> 1:10:47.880
 And so the GPU is really optimized

1:10:47.880 --> 1:10:50.040
 for the rasterization of triangles.

1:10:50.040 --> 1:10:56.960
 Well, like in a modern, that's a small piece of modern GPUs.

1:10:56.960 --> 1:10:59.920
 What they did is that they still rasterized triangles

1:10:59.920 --> 1:11:00.920
 when you're running a game.

1:11:00.920 --> 1:11:03.520
 But for the most part, most of the computation

1:11:03.520 --> 1:11:05.840
 in the area of the GPU is running shader programs.

1:11:05.840 --> 1:11:09.560
 But there's single threaded programs on pixels, not graphs.

1:11:09.560 --> 1:11:11.200
 To be honest, let's say I don't actually

1:11:11.200 --> 1:11:15.000
 know the math behind shader, shading and lighting

1:11:15.000 --> 1:11:16.160
 and all that kind of stuff.

1:11:16.160 --> 1:11:17.720
 I don't know what.

1:11:17.720 --> 1:11:20.080
 They look like little simple floating point programs

1:11:20.080 --> 1:11:21.200
 or complicated ones.

1:11:21.200 --> 1:11:23.680
 You can have 8,000 instructions in a shader program.

1:11:23.680 --> 1:11:25.560
 But I don't have a good intuition

1:11:25.560 --> 1:11:27.920
 why it could be parallelized so easily.

1:11:27.920 --> 1:11:30.640
 No, it's because you have 8 million pixels in every single.

1:11:30.640 --> 1:11:37.280
 So when you have a light that comes down, the amount of light,

1:11:37.280 --> 1:11:40.720
 like say this is a line of pixels across this table,

1:11:40.720 --> 1:11:43.560
 the amount of light on each pixel is subtly different.

1:11:43.560 --> 1:11:46.000
 And each pixel is responsible for figuring out what it is.

1:11:46.000 --> 1:11:48.560
 Figuring it out. So that pixel says, I'm this pixel.

1:11:48.560 --> 1:11:49.920
 I know the angle of the light.

1:11:49.920 --> 1:11:52.360
 I know the occlusion, I know the color I am.

1:11:52.360 --> 1:11:54.400
 Like every single pixel here is a different color.

1:11:54.400 --> 1:11:57.160
 Every single pixel gets a different amount of light.

1:11:57.160 --> 1:12:00.560
 Every single pixel has a subtly different translucency.

1:12:00.560 --> 1:12:02.720
 So to make it look realistic, the solution

1:12:02.720 --> 1:12:05.120
 was you run a separate program on every pixel.

1:12:05.120 --> 1:12:07.760
 See, but I thought there's a reflection from all over the place.

1:12:07.760 --> 1:12:08.520
 Is it every pixel?

1:12:08.520 --> 1:12:09.600
 Yeah, but there is.

1:12:09.600 --> 1:12:12.240
 So you build a reflection map, which also

1:12:12.240 --> 1:12:14.160
 has some pixelated thing.

1:12:14.160 --> 1:12:16.320
 And then when the pixel is looking at the reflection map,

1:12:16.320 --> 1:12:19.240
 it has to calculate what the normal of the surface is.

1:12:19.240 --> 1:12:20.920
 And it does it per pixel.

1:12:20.920 --> 1:12:23.040
 By the way, there's bull loads of hacks on that.

1:12:23.040 --> 1:12:26.640
 You may have a lower resolution light map, reflection map.

1:12:26.640 --> 1:12:29.200
 There's all these tax they do.

1:12:29.200 --> 1:12:32.960
 But at the end of the day, it's per pixel computation.

1:12:32.960 --> 1:12:37.160
 And it's so happening that you can map graph light computation

1:12:37.160 --> 1:12:39.320
 onto this pixel central computation.

1:12:39.320 --> 1:12:41.320
 You can do floating point programs

1:12:41.320 --> 1:12:43.480
 on convolution and matrices.

1:12:43.480 --> 1:12:47.240
 And NVIDIA invested for years in CUDA, first for HPC.

1:12:47.240 --> 1:12:50.120
 And then they got lucky with the AI trend.

1:12:50.120 --> 1:12:52.600
 But do you think they're going to essentially not

1:12:52.600 --> 1:12:55.440
 be able to hardcore pivot out of their hole?

1:12:55.440 --> 1:12:57.440
 We'll see.

1:12:57.440 --> 1:12:59.480
 That's always interesting.

1:12:59.480 --> 1:13:03.840
 How often do big companies hardcore pivot occasionally?

1:13:03.840 --> 1:13:06.320
 How much do you know about NVIDIA, folks?

1:13:06.320 --> 1:13:06.960
 Some.

1:13:06.960 --> 1:13:08.160
 Some.

1:13:08.160 --> 1:13:11.480
 Well, I'm curious as well, who's ultimately as a?

1:13:11.480 --> 1:13:13.360
 Well, they've innovated several times.

1:13:13.360 --> 1:13:15.200
 But they've also worked really hard on mobile.

1:13:15.200 --> 1:13:18.400
 They've worked really hard on radios.

1:13:18.400 --> 1:13:20.680
 They're fundamentally a GPU company.

1:13:20.680 --> 1:13:21.800
 Well, they tried to pivot.

1:13:21.800 --> 1:13:27.640
 It's an interesting little game and play in autonomous vehicles,

1:13:27.640 --> 1:13:31.080
 with or semi autonomous, like playing with Tesla and so on,

1:13:31.080 --> 1:13:35.680
 and seeing that's dipping a toe into that kind of pivot.

1:13:35.680 --> 1:13:37.240
 They came out with this platform, which

1:13:37.240 --> 1:13:39.120
 is interesting technically.

1:13:39.120 --> 1:13:46.040
 But it was like a $3,000 GPU platform.

1:13:46.040 --> 1:13:47.480
 I don't know if it's interesting technically.

1:13:47.480 --> 1:13:48.880
 It's interesting philosophically.

1:13:48.880 --> 1:13:51.040
 I technically, I don't know if it's

1:13:51.040 --> 1:13:53.440
 the execution the craftsmanship is there.

1:13:53.440 --> 1:13:54.600
 I'm not sure.

1:13:54.600 --> 1:13:55.480
 I didn't get a sense.

1:13:55.480 --> 1:13:59.120
 I think they were repurposing GPUs for an automotive solution.

1:13:59.120 --> 1:13:59.360
 Right.

1:13:59.360 --> 1:14:00.320
 It's not a real pivot.

1:14:00.320 --> 1:14:03.800
 They didn't build a ground up solution.

1:14:03.800 --> 1:14:06.360
 Like the chips inside Tesla are pretty cheap.

1:14:06.360 --> 1:14:08.080
 Like Mobileye has been doing this.

1:14:08.080 --> 1:14:11.240
 They're doing the classic work from the simplest thing.

1:14:11.240 --> 1:14:14.240
 They were building 40 square millimeter chips.

1:14:14.240 --> 1:14:17.480
 And NVIDIA, their solution, had 800 millimeter chips

1:14:17.480 --> 1:14:19.160
 and 200 millimeter chips.

1:14:19.160 --> 1:14:23.800
 And like boatloads are really expensive DRAMs.

1:14:23.800 --> 1:14:27.000
 And it's a really different approach.

1:14:27.000 --> 1:14:30.320
 So Mobileye fit the, let's say, automotive cost and form

1:14:30.320 --> 1:14:31.280
 factor.

1:14:31.280 --> 1:14:34.680
 And then they added features as it was economically viable.

1:14:34.680 --> 1:14:36.280
 NVIDIA said, take the biggest thing

1:14:36.280 --> 1:14:39.080
 and we're going to go make it work.

1:14:39.080 --> 1:14:41.400
 And that's also influenced like Waymo.

1:14:41.400 --> 1:14:43.640
 There's a whole bunch of autonomous startups

1:14:43.640 --> 1:14:47.240
 where they have a 5,000 watt server in their trunk.

1:14:47.240 --> 1:14:50.560
 And but that's because they think, well, 5,000 watts

1:14:50.560 --> 1:14:54.720
 and $10,000 is OK because it's replacing a driver.

1:14:54.720 --> 1:14:56.280
 Elon's approach was that port has

1:14:56.280 --> 1:14:59.520
 to be cheap enough to put it in every single Tesla,

1:14:59.520 --> 1:15:02.960
 whether they turn on autonomous driving or not.

1:15:02.960 --> 1:15:06.200
 And Mobileye was like, we need to fit in the bomb

1:15:06.200 --> 1:15:09.440
 and cost structure that car companies do.

1:15:09.440 --> 1:15:12.440
 So they may sell you a GPS for 1,500 bucks.

1:15:12.440 --> 1:15:16.440
 But the bomb for that's like $25.

1:15:16.440 --> 1:15:20.160
 Well, and for Mobileye, it seems like neural networks

1:15:20.160 --> 1:15:22.960
 were not first class citizens, like the computation.

1:15:22.960 --> 1:15:24.640
 They didn't start out as a.

1:15:24.640 --> 1:15:26.120
 Yeah, it was a CB problem.

1:15:26.120 --> 1:15:27.120
 Yeah.

1:15:27.120 --> 1:15:29.880
 And they did classic CB and found stop lights and lines.

1:15:29.880 --> 1:15:31.200
 And they were really good at it.

1:15:31.200 --> 1:15:33.920
 Yeah, and they never, I mean, I don't know what's happening

1:15:33.920 --> 1:15:35.800
 now, but they never fully pivoted.

1:15:35.800 --> 1:15:37.960
 I mean, it's like, it's the NVIDIA thing.

1:15:37.960 --> 1:15:42.000
 And then as opposed to, so if you look at the new Tesla work,

1:15:42.000 --> 1:15:45.560
 it's like neural networks from the ground up, right?

1:15:45.560 --> 1:15:48.080
 Yeah, and even Tesla started with a lot of CB stuff in it.

1:15:48.080 --> 1:15:51.840
 And Andre's basically been eliminating it.

1:15:51.840 --> 1:15:54.360
 Move everything into the network.

1:15:54.360 --> 1:15:57.920
 So without, this isn't like confidential stuff,

1:15:57.920 --> 1:16:01.640
 but you sitting on a porch looking over the world,

1:16:01.640 --> 1:16:03.720
 looking at the work that Andre is doing,

1:16:03.720 --> 1:16:06.440
 that Elon's doing with Tesla autopilot.

1:16:06.440 --> 1:16:08.800
 Do you like the trajectory of where things are going

1:16:08.800 --> 1:16:09.640
 on the hardware side?

1:16:09.640 --> 1:16:10.960
 Well, they're making serious progress.

1:16:10.960 --> 1:16:14.160
 I like the videos of people driving the beta stuff.

1:16:14.160 --> 1:16:16.520
 Like it's taking some pretty complicated intersections

1:16:16.520 --> 1:16:20.800
 and all that, but it's still an intervention per drive.

1:16:20.800 --> 1:16:23.640
 I mean, I have autopilot, the current autopilot, my Tesla.

1:16:23.640 --> 1:16:24.560
 I use it every day.

1:16:24.560 --> 1:16:26.840
 Do you have full stop driving beta or no?

1:16:26.840 --> 1:16:28.720
 So you like where this is going?

1:16:28.720 --> 1:16:29.520
 They're making progress.

1:16:29.520 --> 1:16:32.240
 It's taking longer than anybody thought.

1:16:32.240 --> 1:16:37.240
 You know, my wonder was, you know, hardware three,

1:16:37.360 --> 1:16:39.040
 is it enough computing?

1:16:39.040 --> 1:16:42.320
 Off by two, off by five, off by 10, off by 100.

1:16:42.320 --> 1:16:43.160
 Yeah.

1:16:43.160 --> 1:16:47.160
 And I thought it probably wasn't enough,

1:16:47.160 --> 1:16:49.760
 but they're doing pretty well with it now.

1:16:49.760 --> 1:16:50.600
 Yeah.

1:16:50.600 --> 1:16:53.320
 And one thing is, the data set gets bigger,

1:16:53.320 --> 1:16:55.040
 the training gets better.

1:16:55.040 --> 1:16:57.360
 And then there's this interesting thing is,

1:16:57.360 --> 1:16:59.960
 you sort of train and build an arbitrary size network

1:16:59.960 --> 1:17:01.360
 that solves the problem.

1:17:01.360 --> 1:17:03.680
 And then you refactor the network down to the thing

1:17:03.680 --> 1:17:06.760
 that you can afford to ship, right?

1:17:06.760 --> 1:17:10.720
 So the goal isn't to build a network that fits in the phone.

1:17:10.720 --> 1:17:13.680
 It's to build something that actually works.

1:17:14.800 --> 1:17:17.680
 And then how do you make that most effective

1:17:17.680 --> 1:17:19.840
 on the hardware you have?

1:17:19.840 --> 1:17:21.640
 And they seem to be doing that much better

1:17:21.640 --> 1:17:23.520
 than a couple of years ago.

1:17:23.520 --> 1:17:25.760
 Well, the one really important thing is also

1:17:25.760 --> 1:17:28.640
 what they're doing well is how to iterate that quickly,

1:17:28.640 --> 1:17:31.160
 which means like it's not just about one time

1:17:31.160 --> 1:17:32.800
 deployment, one building, it's constantly

1:17:32.800 --> 1:17:35.080
 iterating the network and trying to automate

1:17:35.080 --> 1:17:37.520
 as many steps as possible, right?

1:17:37.520 --> 1:17:41.680
 And that's actually the principles of the software 2.0

1:17:41.680 --> 1:17:46.680
 I can mention with Andre is it's not just,

1:17:46.920 --> 1:17:48.280
 I mean, I don't know what the actual,

1:17:48.280 --> 1:17:50.880
 his description of software 2.0 is.

1:17:50.880 --> 1:17:53.480
 If it's just high level, full software, there's specifics.

1:17:53.480 --> 1:17:57.040
 But the interesting thing about what that actually looks

1:17:57.040 --> 1:18:01.600
 in the real world is it's that, what I think Andre

1:18:01.600 --> 1:18:02.640
 calls the data engine.

1:18:02.640 --> 1:18:06.600
 It's like, it's the iterative improvement of the thing.

1:18:06.600 --> 1:18:10.480
 You have a neural network that does stuff,

1:18:10.480 --> 1:18:12.760
 fails on a bunch of things and learns from it

1:18:12.760 --> 1:18:13.600
 over and over and over.

1:18:13.600 --> 1:18:15.920
 So you constantly discovering edge cases.

1:18:15.920 --> 1:18:19.920
 So it's very much about like data engineering,

1:18:19.920 --> 1:18:23.040
 like figuring out, it's kind of what you were talking about

1:18:23.040 --> 1:18:25.720
 with TensorFlow is you have the data landscape.

1:18:25.720 --> 1:18:27.920
 You have to walk along that data landscape in a way

1:18:27.920 --> 1:18:32.600
 that it's constantly improving the neural network.

1:18:32.600 --> 1:18:34.920
 And that feels like that's the central piece

1:18:34.920 --> 1:18:35.760
 of itself.

1:18:35.760 --> 1:18:40.360
 And there's two pieces of it, like you find edge cases

1:18:40.360 --> 1:18:42.000
 that don't work and then you define something

1:18:42.000 --> 1:18:44.200
 that goes get you data for that.

1:18:44.200 --> 1:18:45.840
 But then the other constraint is whether you have

1:18:45.840 --> 1:18:46.920
 to label it or not.

1:18:46.920 --> 1:18:49.840
 Like the amazing thing about like the GPT3 stuff

1:18:49.840 --> 1:18:51.560
 is it's unsupervised.

1:18:51.560 --> 1:18:53.280
 So there's essentially infinite amount of data.

1:18:53.280 --> 1:18:56.280
 Now there's obviously infinite amount of data available

1:18:56.280 --> 1:18:59.200
 from cars of people who are successfully driving.

1:18:59.200 --> 1:19:03.040
 But the current pipelines are mostly running on labeled data,

1:19:03.040 --> 1:19:04.680
 which is human limited.

1:19:04.680 --> 1:19:09.800
 So when that becomes unsupervised, it'll

1:19:09.800 --> 1:19:14.240
 create unlimited amount of data, which is on a scale.

1:19:14.240 --> 1:19:16.200
 Now the networks that may use that data

1:19:16.200 --> 1:19:18.720
 might be way too big for cars, but then there'll

1:19:18.720 --> 1:19:20.880
 be the transformation from now we have unlimited data.

1:19:20.880 --> 1:19:22.360
 I know exactly what I want.

1:19:22.360 --> 1:19:25.800
 Now can I turn that into something that fits in the car?

1:19:25.800 --> 1:19:29.200
 And that process is going to happen all over the place.

1:19:29.200 --> 1:19:32.280
 Every time you get to the place where you have unlimited data,

1:19:32.280 --> 1:19:35.360
 that's what software 2.0 is about, unlimited data training

1:19:35.360 --> 1:19:40.680
 networks to do stuff without humans writing code to do it.

1:19:40.680 --> 1:19:42.960
 And ultimately also trying to discover

1:19:42.960 --> 1:19:46.560
 like you're saying the self supervised formulation

1:19:46.560 --> 1:19:48.800
 of the problem, so the unsupervised formulation

1:19:48.800 --> 1:19:49.680
 of the problem.

1:19:49.680 --> 1:19:53.560
 Like in driving, there's this really interesting thing,

1:19:53.560 --> 1:19:58.200
 which is you look at a scene that's before you,

1:19:58.200 --> 1:20:01.920
 and you have data about what a successful human driver did

1:20:01.920 --> 1:20:04.440
 in that scene one second later.

1:20:04.440 --> 1:20:06.840
 It's a little piece of data that you can use just

1:20:06.840 --> 1:20:09.400
 like with GPT3 as training.

1:20:09.400 --> 1:20:12.400
 Currently, even though Tesla says they're using that,

1:20:12.400 --> 1:20:14.480
 it's an open question to me.

1:20:14.480 --> 1:20:17.440
 How far can you, can you saw all of the driving

1:20:17.440 --> 1:20:21.120
 with just that self supervised piece of data?

1:20:21.120 --> 1:20:23.400
 And like I think.

1:20:23.400 --> 1:20:25.560
 Well, that's what Common AI is doing.

1:20:25.560 --> 1:20:27.560
 That's what Common AI is doing, but the question

1:20:27.560 --> 1:20:32.280
 is how much data, so what Common AI doesn't have

1:20:32.280 --> 1:20:35.960
 is as good of a data engine, for example, as Tesla does.

1:20:35.960 --> 1:20:39.840
 That's where the organization of the data.

1:20:39.840 --> 1:20:41.960
 I mean, as far as I know, I haven't talked to George,

1:20:41.960 --> 1:20:44.640
 but they do have the data.

1:20:44.640 --> 1:20:47.880
 The question is how much data is needed?

1:20:47.880 --> 1:20:51.440
 Because we say infinite very loosely here.

1:20:51.440 --> 1:20:54.400
 And then the other question, which you said,

1:20:54.400 --> 1:20:56.520
 I don't know if you think it's still an open question,

1:20:56.520 --> 1:20:59.400
 is are we on the right order of magnitude

1:20:59.400 --> 1:21:02.040
 for the compute necessary?

1:21:02.040 --> 1:21:04.960
 That is this, is it like what Elon said,

1:21:04.960 --> 1:21:07.120
 this chip that's in there now is enough

1:21:07.120 --> 1:21:09.080
 to do full self driving, or do we need

1:21:09.080 --> 1:21:10.840
 another order of magnitude?

1:21:10.840 --> 1:21:13.320
 I think nobody actually knows the answer to that question.

1:21:13.320 --> 1:21:16.280
 I like the confidence that Elon has, but.

1:21:16.280 --> 1:21:17.840
 Yeah, we'll see.

1:21:17.840 --> 1:21:20.200
 There's another funny thing is you don't learn to drive

1:21:20.200 --> 1:21:22.280
 with infinite amounts of data.

1:21:22.280 --> 1:21:24.320
 You learn to drive with an intellectual framework

1:21:24.320 --> 1:21:28.080
 that understands physics and color and horizontal surfaces

1:21:28.080 --> 1:21:34.000
 and laws and roads and all your experience

1:21:34.000 --> 1:21:37.240
 from manipulating your environment.

1:21:37.240 --> 1:21:39.120
 There's so many factors go into that.

1:21:39.120 --> 1:21:41.680
 And then when you learn to drive,

1:21:41.680 --> 1:21:44.320
 driving is a subset of this conceptual framework

1:21:44.320 --> 1:21:46.240
 that you have.

1:21:46.240 --> 1:21:48.120
 And so with self driving cars right now,

1:21:48.120 --> 1:21:51.640
 we're teaching them to drive with driving data.

1:21:51.640 --> 1:21:53.520
 You never teach a human to do that.

1:21:53.520 --> 1:21:55.720
 You teach a human all kinds of interesting things,

1:21:55.720 --> 1:21:59.280
 like language, like don't do that, watch out.

1:21:59.280 --> 1:22:01.000
 There's all kinds of stuff going on.

1:22:01.000 --> 1:22:02.880
 This is where you, I think, previous time

1:22:02.880 --> 1:22:07.240
 we talked about where you poetically disagreed

1:22:07.240 --> 1:22:10.280
 with my naive notion about humans.

1:22:10.280 --> 1:22:13.680
 I just think that humans will make

1:22:13.680 --> 1:22:15.760
 this whole driving thing really difficult.

1:22:15.760 --> 1:22:17.080
 Yeah, all right.

1:22:17.080 --> 1:22:19.480
 I said, humans don't move that slow.

1:22:19.480 --> 1:22:20.800
 It's a ballistics problem.

1:22:20.800 --> 1:22:22.720
 It's a ballistics, humans are a ballistics problem,

1:22:22.720 --> 1:22:24.080
 which is like poetry to me.

1:22:24.080 --> 1:22:26.200
 It's very possible that in driving,

1:22:26.200 --> 1:22:28.480
 they're indeed purely a ballistics problem.

1:22:28.480 --> 1:22:30.880
 And I think that's probably the right way to think about it,

1:22:30.880 --> 1:22:34.400
 but I still, they still continue to surprise me

1:22:34.400 --> 1:22:36.920
 with those damp pedestrians, the cyclists,

1:22:36.920 --> 1:22:39.360
 other humans in other cars and.

1:22:39.360 --> 1:22:41.200
 Yeah, but it's going to be one of these compensating things.

1:22:41.200 --> 1:22:45.480
 So like when you're driving, you have an intuition

1:22:45.480 --> 1:22:46.880
 about what humans are going to do,

1:22:46.880 --> 1:22:49.680
 but you don't have 360 cameras and radars

1:22:49.680 --> 1:22:51.160
 and you have an attention problem.

1:22:51.160 --> 1:22:55.120
 So the self driving car comes in with no attention problems,

1:22:55.120 --> 1:22:58.800
 360 cameras, a bunch of other features.

1:22:58.800 --> 1:23:02.000
 So they'll wipe out a whole class of accidents, right?

1:23:02.000 --> 1:23:05.760
 And emergency braking with radar,

1:23:05.760 --> 1:23:08.000
 and especially as it gets AI enhanced,

1:23:08.000 --> 1:23:10.920
 will eliminate collisions, right?

1:23:10.920 --> 1:23:12.040
 But then you have the other problems

1:23:12.040 --> 1:23:13.880
 of these unexpected things where,

1:23:13.880 --> 1:23:15.600
 you think your human intuition is helping,

1:23:15.600 --> 1:23:19.560
 but then the cars also have a set of hardware features

1:23:19.560 --> 1:23:21.520
 that you're not even close to.

1:23:21.520 --> 1:23:23.920
 And the key thing of course is,

1:23:23.920 --> 1:23:27.040
 if you wipe out a huge number of kind of accidents,

1:23:27.040 --> 1:23:30.240
 then it might be just way safer than a human driver,

1:23:30.240 --> 1:23:33.000
 even though, even if humans are still a problem,

1:23:33.000 --> 1:23:34.720
 that's hard to figure out.

1:23:34.720 --> 1:23:36.160
 Yeah, that's probably what will happen.

1:23:36.160 --> 1:23:38.800
 So autonomous cars will have a small number of accidents,

1:23:38.800 --> 1:23:40.000
 humans would have avoided,

1:23:40.000 --> 1:23:43.800
 but they'll wipe, they'll get rid of the bulk of them.

1:23:43.800 --> 1:23:48.640
 What do you think about like Tesla's dojo efforts,

1:23:48.640 --> 1:23:51.120
 or it can be bigger than Tesla in general?

1:23:51.120 --> 1:23:52.840
 It's kind of like the tense torrent,

1:23:54.280 --> 1:23:56.640
 trying to innovate, like this is the economy,

1:23:56.640 --> 1:23:59.200
 like should a company try to from scratch

1:23:59.200 --> 1:24:03.160
 build its own neural network training hardware?

1:24:03.160 --> 1:24:04.240
 Well, first I think it's great.

1:24:04.240 --> 1:24:06.800
 So we need lots of experiments, right?

1:24:06.800 --> 1:24:09.440
 And there's lots of startups working on this

1:24:09.440 --> 1:24:11.520
 and they're pursuing different things.

1:24:11.520 --> 1:24:13.360
 Now I was there when we started Dojo,

1:24:13.360 --> 1:24:14.560
 and it was sort of like,

1:24:14.560 --> 1:24:17.960
 what's the unconstrained computer solution

1:24:17.960 --> 1:24:21.720
 to go do very large training problems.

1:24:21.720 --> 1:24:23.680
 And then there's fun stuff like,

1:24:23.680 --> 1:24:27.200
 we said, well, we have this 10,000 watt board to cool.

1:24:27.200 --> 1:24:29.120
 Well, you go talk to guys at SpaceX,

1:24:29.120 --> 1:24:31.160
 and they think 10,000 watts is a really small number,

1:24:31.160 --> 1:24:32.720
 not a big number.

1:24:32.720 --> 1:24:35.280
 And there's brilliant people working on it.

1:24:35.280 --> 1:24:37.280
 I'm curious to see how it'll come out.

1:24:37.280 --> 1:24:39.000
 I couldn't tell you,

1:24:39.000 --> 1:24:41.640
 I know it pivoted a few times since I left, so.

1:24:41.640 --> 1:24:44.520
 So the cooling does seem to be a big problem.

1:24:44.520 --> 1:24:46.840
 I do like what Elon said about it,

1:24:46.840 --> 1:24:49.280
 which is like, we don't want to do the thing

1:24:49.280 --> 1:24:51.280
 unless it's way better than the alternative,

1:24:51.280 --> 1:24:52.960
 whatever the alternative is.

1:24:52.960 --> 1:24:57.600
 So it has to be way better than like racks or GPUs.

1:24:57.600 --> 1:24:59.960
 Yeah, and the other thing is just like,

1:24:59.960 --> 1:25:03.840
 you know, the Tesla autonomous driving hardware,

1:25:03.840 --> 1:25:06.520
 it was only serving one software stack.

1:25:06.520 --> 1:25:08.000
 And the hardware team and the software team

1:25:08.000 --> 1:25:09.840
 were tightly coupled.

1:25:09.840 --> 1:25:12.120
 You know, if you're building a general purpose AI solution,

1:25:12.120 --> 1:25:14.240
 then you know, there's so many different customers

1:25:14.240 --> 1:25:16.360
 with so many different needs.

1:25:16.360 --> 1:25:18.360
 Now, something Andre said is,

1:25:18.360 --> 1:25:21.400
 I think this is amazing, 10 years ago,

1:25:21.400 --> 1:25:24.600
 like vision, recommendation, language

1:25:24.600 --> 1:25:27.080
 were completely different disciplines.

1:25:27.080 --> 1:25:29.680
 I said, the people literally couldn't talk to each other.

1:25:29.680 --> 1:25:32.520
 And three years ago, it was all neural networks,

1:25:32.520 --> 1:25:34.800
 but the very different neural networks.

1:25:34.800 --> 1:25:37.680
 And recently it's converging on one set of networks.

1:25:37.680 --> 1:25:39.320
 They vary a lot in size, obviously,

1:25:39.320 --> 1:25:42.040
 they vary in data, vary in outputs,

1:25:42.040 --> 1:25:44.760
 but the technology has converged a good bit.

1:25:44.760 --> 1:25:47.360
 Yeah, these transformers behind GPT3,

1:25:47.360 --> 1:25:48.960
 it seems like they could be applied to video,

1:25:48.960 --> 1:25:50.560
 they could be applied to a lot of,

1:25:50.560 --> 1:25:52.520
 and it's like, and they're all really simple.

1:25:52.520 --> 1:25:56.320
 And it was like to literally replace letters with pixels.

1:25:56.320 --> 1:25:58.760
 It does vision, it's amazing.

1:25:58.760 --> 1:26:02.080
 So, and then size actually improves the thing.

1:26:02.080 --> 1:26:03.200
 So the bigger it gets,

1:26:03.200 --> 1:26:05.680
 the more compute you throw at it, the better it gets.

1:26:05.680 --> 1:26:08.320
 And the more data you have, the better it gets.

1:26:08.320 --> 1:26:11.040
 So, so then you start to wonder,

1:26:11.040 --> 1:26:12.560
 well, is that a fundamental thing,

1:26:12.560 --> 1:26:15.080
 or is this just another step

1:26:15.080 --> 1:26:16.560
 to some fundamental understanding

1:26:16.560 --> 1:26:18.840
 about this kind of computation,

1:26:18.840 --> 1:26:20.280
 which is really interesting.

1:26:20.280 --> 1:26:21.520
 Us humans don't want to believe

1:26:21.520 --> 1:26:22.680
 that that kind of thing will achieve

1:26:22.680 --> 1:26:24.400
 conceptual understandings you were saying,

1:26:24.400 --> 1:26:27.000
 like you'll figure out physics, but maybe it will.

1:26:27.000 --> 1:26:29.400
 Maybe. Probably will.

1:26:29.400 --> 1:26:31.040
 Well, it's worse than that.

1:26:31.040 --> 1:26:33.760
 It'll understand physics in ways that we can't understand.

1:26:33.760 --> 1:26:35.880
 I liked your Stephen Wolfram talk,

1:26:35.880 --> 1:26:36.720
 where he said, you know,

1:26:36.720 --> 1:26:38.000
 there's three generations of physics.

1:26:38.000 --> 1:26:40.120
 There was physics by reasoning,

1:26:40.120 --> 1:26:42.640
 well, big things should fall faster than small things,

1:26:42.640 --> 1:26:44.080
 right, that's reasoning.

1:26:44.080 --> 1:26:48.240
 And then there's physics by equations, like, you know,

1:26:48.240 --> 1:26:50.160
 but the number of programs in a world that are solved

1:26:50.160 --> 1:26:52.000
 with the single equations relatively low,

1:26:52.000 --> 1:26:53.640
 almost all programs have, you know,

1:26:53.640 --> 1:26:54.960
 more than one line of code,

1:26:54.960 --> 1:26:56.840
 maybe a hundred million lines of code.

1:26:56.840 --> 1:26:59.960
 So he said, now we're going to physics by equation,

1:26:59.960 --> 1:27:02.560
 which is his project, which is cool.

1:27:02.560 --> 1:27:07.280
 I might point out that there was two generations of physics

1:27:07.280 --> 1:27:10.240
 before reasoning, habit.

1:27:10.240 --> 1:27:11.440
 Like all animals, you know,

1:27:11.440 --> 1:27:14.520
 no things fall and, you know, birds fly and, you know,

1:27:14.520 --> 1:27:16.080
 predators know how to, you know,

1:27:16.080 --> 1:27:20.000
 solve a differential equation to cut off a accelerating,

1:27:20.000 --> 1:27:23.480
 you know, curving animal path.

1:27:23.480 --> 1:27:28.400
 And then there was, you know, the gods did it, right?

1:27:28.400 --> 1:27:31.640
 So, right, so there was, you know, there's five generations.

1:27:31.640 --> 1:27:35.960
 Now, software 2.0 says programming things

1:27:35.960 --> 1:27:38.880
 is not the last step, data.

1:27:38.880 --> 1:27:41.280
 So there's going to be a physics

1:27:41.280 --> 1:27:44.080
 past Stevens, Wolfram's comp.

1:27:44.080 --> 1:27:46.280
 That's not explainable to us humans.

1:27:47.520 --> 1:27:51.040
 And actually, there's no reason that I can see

1:27:51.040 --> 1:27:53.720
 while that, even that's the limit, like,

1:27:53.720 --> 1:27:55.600
 there's something beyond that.

1:27:55.600 --> 1:27:56.440
 I mean, they're usually, like,

1:27:56.440 --> 1:27:58.800
 usually when you have this hierarchy, it's not like,

1:27:58.800 --> 1:28:00.600
 well, if you have this step and this step and this step

1:28:00.600 --> 1:28:03.160
 and they're all qualitatively different,

1:28:03.160 --> 1:28:05.400
 conceptually different, it's not obvious why, you know,

1:28:05.400 --> 1:28:07.360
 six is the right ant number of hierarchy steps

1:28:07.360 --> 1:28:09.200
 and not seven or eight or...

1:28:09.200 --> 1:28:12.120
 Well, then it's probably impossible for us

1:28:12.120 --> 1:28:15.240
 to comprehend something that's beyond

1:28:15.240 --> 1:28:17.080
 the thing that's not explainable.

1:28:17.080 --> 1:28:20.800
 Yeah, but the thing that, you know,

1:28:20.800 --> 1:28:23.480
 understands the thing that's not explainable to us,

1:28:23.480 --> 1:28:26.640
 Wolfram's conceives the next one and, like,

1:28:26.640 --> 1:28:28.640
 I'm not sure why there's a limit to it.

1:28:30.880 --> 1:28:31.720
 Looks like your brain hurts.

1:28:31.720 --> 1:28:32.720
 That's the sad story.

1:28:33.960 --> 1:28:36.520
 If we look at our own brain,

1:28:36.520 --> 1:28:39.920
 which is an interesting, illustrative example,

1:28:41.160 --> 1:28:42.560
 in your work with TestTorrent

1:28:42.560 --> 1:28:46.120
 and trying to design deep learning architectures,

1:28:46.120 --> 1:28:50.040
 do you think about the brain at all?

1:28:50.040 --> 1:28:53.440
 Maybe from a hardware designer perspective,

1:28:53.440 --> 1:28:56.200
 if you could change something about the brain,

1:28:56.200 --> 1:28:57.520
 what would you change?

1:28:57.520 --> 1:28:58.360
 Or do you...

1:28:58.360 --> 1:28:59.200
 Funny question.

1:28:59.200 --> 1:29:01.040
 Like, how would you do that?

1:29:01.040 --> 1:29:02.360
 So your brain is really weird.

1:29:02.360 --> 1:29:03.960
 Like, you know, your cerebral cortex,

1:29:03.960 --> 1:29:06.040
 where we think we do most of our thinking,

1:29:06.040 --> 1:29:08.640
 is what, like six or seven neurons thick?

1:29:08.640 --> 1:29:09.480
 Yeah.

1:29:09.480 --> 1:29:10.320
 Like, that's weird.

1:29:10.320 --> 1:29:13.200
 Like, all the big networks are way bigger than that.

1:29:13.200 --> 1:29:14.320
 Like, way deeper.

1:29:14.320 --> 1:29:16.160
 So that seems odd.

1:29:16.160 --> 1:29:18.000
 And then, you know, when you're thinking,

1:29:18.000 --> 1:29:21.800
 if the input generates a result you can lose,

1:29:21.800 --> 1:29:22.800
 it goes really fast.

1:29:22.800 --> 1:29:25.240
 But if it can't, that generates an output

1:29:25.240 --> 1:29:27.080
 that's interesting, which turns into an input,

1:29:27.080 --> 1:29:28.360
 and then your brain,

1:29:28.360 --> 1:29:30.680
 to the point where you mull things over for days,

1:29:30.680 --> 1:29:33.400
 and how many trips through your brain is that, right?

1:29:33.400 --> 1:29:36.080
 Like, it's, you know, 300 milliseconds or something,

1:29:36.080 --> 1:29:37.840
 and you get through seven levels of neurons.

1:29:37.840 --> 1:29:39.840
 I forget the number exactly.

1:29:39.840 --> 1:29:43.400
 But then it does it over and over and over as it searches.

1:29:43.400 --> 1:29:46.120
 And the brain clearly looks like some kind of graph

1:29:46.120 --> 1:29:48.160
 because you have a neuron with, you know, connections,

1:29:48.160 --> 1:29:49.200
 and it talks to other ones.

1:29:49.200 --> 1:29:52.360
 And it's locally very computationally intense,

1:29:52.360 --> 1:29:55.480
 but it also does sparse computations

1:29:55.480 --> 1:29:57.800
 across a pretty big area.

1:29:57.800 --> 1:30:00.640
 There's a lot of messy biological type of things,

1:30:00.640 --> 1:30:03.720
 and it's meaning like, first of all,

1:30:03.720 --> 1:30:06.000
 there's mechanical, chemical, and electrical signals.

1:30:06.000 --> 1:30:07.480
 It's all that's going on.

1:30:07.480 --> 1:30:12.480
 Then there's the acynchronicity of signals.

1:30:12.480 --> 1:30:14.720
 And there's like, there's just a lot of variability

1:30:14.720 --> 1:30:16.520
 that seems continuous and messy

1:30:16.520 --> 1:30:18.640
 and just a mess of biology.

1:30:18.640 --> 1:30:22.640
 And it's unclear whether that's a good thing

1:30:22.640 --> 1:30:24.040
 or it's a bad thing.

1:30:24.040 --> 1:30:26.320
 Because if it's a good thing,

1:30:26.320 --> 1:30:29.240
 then we need to run the entirety of the evolution.

1:30:29.240 --> 1:30:31.560
 Well, we're gonna have to start with basic bacteria

1:30:31.560 --> 1:30:32.400
 to create something.

1:30:32.400 --> 1:30:35.640
 Imagine you could build a brain with 10 layers.

1:30:35.640 --> 1:30:37.360
 Would that be better or worse?

1:30:37.360 --> 1:30:39.800
 Or more connections or less connections,

1:30:39.800 --> 1:30:43.400
 or we don't know to what level our brains are optimized.

1:30:44.240 --> 1:30:45.480
 But if I was changing things,

1:30:45.480 --> 1:30:49.320
 like you can only hold like seven numbers in your head.

1:30:49.320 --> 1:30:51.840
 Like why not a hundred or a million?

1:30:51.840 --> 1:30:53.680
 There was a lot of that.

1:30:53.680 --> 1:30:56.800
 And why can't we have like a floating point processor

1:30:56.800 --> 1:30:59.520
 that can compute anything we want

1:30:59.520 --> 1:31:01.240
 and see it all properly?

1:31:01.240 --> 1:31:03.120
 Like that would be kind of fun.

1:31:03.120 --> 1:31:05.760
 And why can't we see in four or eight dimensions?

1:31:05.760 --> 1:31:10.080
 Like three D's kind of a drag.

1:31:10.080 --> 1:31:11.600
 Like all the hard mass transforms

1:31:11.600 --> 1:31:13.960
 are up in multiple dimensions.

1:31:13.960 --> 1:31:16.560
 So there's, you could imagine a brain architecture

1:31:16.560 --> 1:31:21.120
 that you could enhance with a whole bunch of features

1:31:21.120 --> 1:31:24.440
 that would be really useful for thinking about things.

1:31:24.440 --> 1:31:26.880
 It's possible that the limitations you're describing

1:31:26.880 --> 1:31:29.880
 are actually essential for like the constraints

1:31:29.880 --> 1:31:34.000
 are essential for creating like the depth of intelligence.

1:31:34.000 --> 1:31:38.240
 Like that, the ability to reason, you know.

1:31:38.240 --> 1:31:39.080
 Yeah, it's hard to say

1:31:39.080 --> 1:31:43.040
 because like your brain is clearly a parallel processor.

1:31:43.040 --> 1:31:46.200
 You know, 10 billion neurons talking to each other

1:31:46.200 --> 1:31:48.440
 at a relatively low clock rate.

1:31:48.440 --> 1:31:51.080
 But it produces something that looks like

1:31:51.080 --> 1:31:52.640
 a serial thought process.

1:31:52.640 --> 1:31:54.720
 It's a serial narrative in your head.

1:31:54.720 --> 1:31:55.560
 That's true.

1:31:55.560 --> 1:31:59.040
 But then there are people famously who are visual thinkers.

1:31:59.040 --> 1:32:02.320
 Like I think I'm a relatively visual thinker.

1:32:02.320 --> 1:32:05.080
 I can imagine any object and rotate it in my head

1:32:05.080 --> 1:32:06.440
 and look at it.

1:32:06.440 --> 1:32:09.640
 And there are people who say they don't think that way at all.

1:32:09.640 --> 1:32:12.440
 And recently I read an article about people

1:32:12.440 --> 1:32:15.520
 who say they don't have a voice in their head.

1:32:16.760 --> 1:32:19.880
 They can talk, but when they, you know, it's like,

1:32:19.880 --> 1:32:21.520
 well, what are you thinking though?

1:32:21.520 --> 1:32:24.400
 They'll describe something that's visual.

1:32:24.400 --> 1:32:26.480
 So that's curious.

1:32:26.480 --> 1:32:31.480
 Now, if you're saying, if we dedicated more hardware

1:32:33.800 --> 1:32:36.360
 to holding information like, you know, 10 numbers

1:32:36.360 --> 1:32:40.960
 or a million numbers, like would that distract us

1:32:40.960 --> 1:32:44.760
 from our ability to form this kind of singular identity?

1:32:44.760 --> 1:32:46.280
 Like it dissipates somehow.

1:32:46.280 --> 1:32:47.120
 Right.

1:32:47.120 --> 1:32:50.720
 But maybe, you know, future humans will have many identities

1:32:50.720 --> 1:32:53.120
 that have some higher level organization,

1:32:53.120 --> 1:32:55.640
 but can actually do lots more things in parallel.

1:32:55.640 --> 1:32:57.880
 Yeah, there's no reason, if we're thinking modularly,

1:32:57.880 --> 1:33:00.280
 there's no reason we can have multiple consciousnesses

1:33:00.280 --> 1:33:01.480
 in one brain.

1:33:01.480 --> 1:33:02.320
 Yeah.

1:33:02.320 --> 1:33:03.680
 And maybe there's some way to make it faster

1:33:03.680 --> 1:33:07.880
 so that the, you know, the area of the computation

1:33:07.880 --> 1:33:12.880
 could still have a unified feel to it

1:33:13.200 --> 1:33:15.680
 while still having way more ability

1:33:15.680 --> 1:33:17.560
 to do parallel stuff at the same time.

1:33:17.560 --> 1:33:19.040
 Could definitely be improved.

1:33:19.040 --> 1:33:20.000
 Could be improved?

1:33:20.000 --> 1:33:20.840
 Yeah.

1:33:20.840 --> 1:33:22.880
 Well, it's pretty good right now.

1:33:22.880 --> 1:33:24.640
 Actually, people don't give it enough credit.

1:33:24.640 --> 1:33:27.840
 The thing is pretty nice that, you know,

1:33:27.840 --> 1:33:30.240
 the fact that the ride ends seem to be,

1:33:31.840 --> 1:33:36.840
 give a nice like spark of beauty to the whole experience.

1:33:37.880 --> 1:33:40.280
 So I don't know, I don't know if it can be improved easily.

1:33:40.280 --> 1:33:42.480
 It could be more beautiful.

1:33:42.480 --> 1:33:45.240
 I don't know how, what do you mean?

1:33:45.240 --> 1:33:46.280
 What do you mean how?

1:33:46.280 --> 1:33:48.280
 All the ways you can't imagine.

1:33:48.280 --> 1:33:49.480
 No, but that's the whole point.

1:33:49.480 --> 1:33:51.080
 I wouldn't be able to imagine,

1:33:51.080 --> 1:33:52.720
 the fact that I can imagine ways

1:33:52.720 --> 1:33:55.880
 in which it could be more beautiful means.

1:33:55.880 --> 1:33:59.360
 So do you know, you know, Ian Banks, his stories.

1:33:59.360 --> 1:34:03.560
 So the super smart AIs, they're live,

1:34:03.560 --> 1:34:06.080
 mostly live in the world of what they call infinite fun

1:34:07.480 --> 1:34:12.120
 because they can create arbitrary worlds.

1:34:12.120 --> 1:34:14.440
 So they interact and, you know, the story has it.

1:34:14.440 --> 1:34:15.800
 They interact in the normal world

1:34:15.800 --> 1:34:18.520
 and they're very smart and they can do all kinds of stuff.

1:34:18.520 --> 1:34:20.360
 And, you know, given mine can, you know,

1:34:20.360 --> 1:34:21.960
 talk to a million humans at the same time

1:34:21.960 --> 1:34:24.640
 because we're very slow and for reasons,

1:34:24.640 --> 1:34:26.240
 you know, artificial, the story,

1:34:26.240 --> 1:34:28.200
 they're interested in people and doing stuff,

1:34:28.200 --> 1:34:32.960
 but they mostly live in this other land of thinking.

1:34:32.960 --> 1:34:36.440
 My inclination is to think that the ability

1:34:36.440 --> 1:34:40.080
 to create infinite fun will not be so fun.

1:34:41.120 --> 1:34:42.440
 That's sad.

1:34:42.440 --> 1:34:43.280
 Well.

1:34:43.280 --> 1:34:44.120
 There's so many things to do.

1:34:44.120 --> 1:34:47.560
 Imagine being able to make a star, move planets around.

1:34:47.560 --> 1:34:48.560
 Yeah, yeah.

1:34:48.560 --> 1:34:51.320
 But because we can imagine that as wildlife is fun,

1:34:51.320 --> 1:34:55.000
 if we actually were able to do it, it'd be a slippery slope

1:34:55.000 --> 1:34:56.720
 where fun, we'd even have a meeting

1:34:56.720 --> 1:35:00.320
 because we just consistently desensitize ourselves

1:35:00.320 --> 1:35:03.280
 by the infinite amounts of fun we're having.

1:35:04.120 --> 1:35:08.040
 And the sadness, the dark stuff is what makes it fun, I think.

1:35:09.520 --> 1:35:10.440
 That could be the Russian.

1:35:10.440 --> 1:35:12.400
 It could be the fun makes it fun

1:35:12.400 --> 1:35:14.680
 and the sadness makes it bittersweet.

1:35:16.560 --> 1:35:17.400
 Yeah, that's true.

1:35:17.400 --> 1:35:20.520
 Fun could be the thing that makes it fun.

1:35:20.520 --> 1:35:22.520
 So what do you think about the expansion,

1:35:22.520 --> 1:35:23.880
 not through the biology side,

1:35:23.880 --> 1:35:27.200
 but through the BCI, the brain computer interfaces?

1:35:27.200 --> 1:35:30.080
 Yeah, you got a chance to check out the Neuralink stuff.

1:35:30.080 --> 1:35:31.480
 It's super interesting.

1:35:31.480 --> 1:35:36.480
 Like humans, like our thoughts manifest as action.

1:35:38.720 --> 1:35:41.680
 Like as a kid, shooting a rifle was super fun,

1:35:41.680 --> 1:35:44.240
 driving a mini bike, doing things.

1:35:44.240 --> 1:35:46.120
 And then computer games, I think,

1:35:46.120 --> 1:35:49.680
 for a lot of kids became the thing where they can do

1:35:49.680 --> 1:35:51.440
 what they want, they can fly a plane,

1:35:51.440 --> 1:35:53.560
 they can do this, they can do this, right?

1:35:53.560 --> 1:35:55.840
 But you have to have this physical interaction.

1:35:55.840 --> 1:36:00.280
 Now imagine, you know, you could just imagine stuff

1:36:00.280 --> 1:36:03.240
 and it happens, right?

1:36:03.240 --> 1:36:06.600
 Like really richly and interestingly.

1:36:06.600 --> 1:36:08.040
 Like we kind of do that when we dream.

1:36:08.040 --> 1:36:10.480
 Like dreams are funny because like,

1:36:10.480 --> 1:36:14.360
 if you have some control or awareness in your dreams,

1:36:14.360 --> 1:36:16.360
 like it's very realistic looking

1:36:16.360 --> 1:36:19.400
 or not realistic, it depends on the dream.

1:36:19.400 --> 1:36:21.240
 But you can also manipulate that.

1:36:22.440 --> 1:36:26.200
 And you know, what's possible there is odd

1:36:26.200 --> 1:36:29.840
 in the fact that nobody understands it's hilarious, but.

1:36:29.840 --> 1:36:31.720
 Do you think it's possible to expand

1:36:31.720 --> 1:36:34.000
 that capability through computing?

1:36:34.000 --> 1:36:35.320
 Sure.

1:36:35.320 --> 1:36:36.480
 Is there some interesting,

1:36:36.480 --> 1:36:38.400
 so from a hardware designer perspective,

1:36:38.400 --> 1:36:41.600
 is there, do you think it'll present totally new challenges

1:36:41.600 --> 1:36:44.080
 in the kind of hardware that required that like,

1:36:44.080 --> 1:36:47.720
 so this hardware isn't standalone computing.

1:36:47.720 --> 1:36:50.240
 So just take it from this, so today,

1:36:50.240 --> 1:36:53.640
 computer games are rendered by GPUs, right?

1:36:53.640 --> 1:36:56.840
 So, but you've seen the GAN stuff, right?

1:36:56.840 --> 1:37:00.880
 Where train neural networks render realistic images,

1:37:00.880 --> 1:37:03.760
 but there's no pixels, no triangles, no shaders,

1:37:03.760 --> 1:37:05.400
 no light maps, no nothing.

1:37:05.400 --> 1:37:09.960
 So the future of graphics is probably AI, right?

1:37:09.960 --> 1:37:14.840
 Now that AI is heavily trained by lots of real data, right?

1:37:14.840 --> 1:37:19.840
 So if you have an interface with a AI renderer, right?

1:37:20.360 --> 1:37:22.760
 So if you say render a cat,

1:37:22.760 --> 1:37:24.520
 it won't say, well, how tall is the cat

1:37:24.520 --> 1:37:26.280
 and how big, you know, it'll render a cat.

1:37:26.280 --> 1:37:28.200
 You might say, oh, a little bigger, a little smaller,

1:37:28.200 --> 1:37:31.320
 you know, make it a tabby, shorter hair, you know,

1:37:31.320 --> 1:37:32.880
 like you could tweak it.

1:37:32.880 --> 1:37:36.520
 Like the amount of data you'll have to send

1:37:36.520 --> 1:37:41.400
 to interact with a very powerful AI renderer could be low.

1:37:41.400 --> 1:37:44.800
 But the question is, for brain computer interfaces,

1:37:44.800 --> 1:37:47.840
 we need to render not onto a screen,

1:37:47.840 --> 1:37:50.360
 but render onto the brain.

1:37:50.360 --> 1:37:53.040
 And like directly, so there's a bandwidth.

1:37:53.040 --> 1:37:53.880
 Well, we could do it both ways.

1:37:53.880 --> 1:37:56.000
 I mean, our eyes are really good sensors.

1:37:56.000 --> 1:37:58.560
 They could render onto a screen

1:37:58.560 --> 1:38:01.120
 and we could feel like we're participating in it.

1:38:01.120 --> 1:38:03.360
 You know, they're gonna have, you know,

1:38:03.360 --> 1:38:04.880
 like the Oculus kind of stuff.

1:38:04.880 --> 1:38:07.040
 It's gonna be so good when a projection to your eyes,

1:38:07.040 --> 1:38:08.040
 you think it's real.

1:38:08.040 --> 1:38:11.600
 You know, they're slowly solving those problems.

1:38:12.520 --> 1:38:17.240
 And I suspect when the renderer of that information

1:38:17.240 --> 1:38:19.720
 into your head is also AI mediated,

1:38:20.600 --> 1:38:23.480
 they'll be able to give you the cues that, you know,

1:38:23.480 --> 1:38:26.200
 you really want for depth and all kinds of stuff.

1:38:27.280 --> 1:38:30.920
 Like your brain is partly faking your visual field, right?

1:38:30.920 --> 1:38:32.680
 Like your eyes are twitching around,

1:38:32.680 --> 1:38:33.800
 but you don't notice that.

1:38:33.800 --> 1:38:36.520
 Occasionally they blank, you don't notice that.

1:38:36.520 --> 1:38:37.760
 You know, there's all kinds of things.

1:38:37.760 --> 1:38:39.120
 Like you think you see over here,

1:38:39.120 --> 1:38:40.800
 but you don't really see there.

1:38:40.800 --> 1:38:42.160
 It's all fabricated.

1:38:42.160 --> 1:38:45.480
 Yeah, peripheral vision is fascinating.

1:38:45.480 --> 1:38:48.520
 So if you have an AI renderer that's trained

1:38:48.520 --> 1:38:51.640
 to understand exactly how you see

1:38:51.640 --> 1:38:54.760
 and the kind of things that enhance the realism

1:38:54.760 --> 1:38:57.600
 of the experience could be super real, actually.

1:39:01.120 --> 1:39:03.480
 So I don't know what the limits that are,

1:39:03.480 --> 1:39:06.920
 but obviously if we have a brain interface

1:39:06.920 --> 1:39:10.440
 that goes in inside your, you know, visual cortex

1:39:10.440 --> 1:39:13.480
 in a better way than your eyes do, which is possible.

1:39:13.480 --> 1:39:14.640
 It's a lot of neurons.

1:39:17.040 --> 1:39:19.760
 Maybe that'll be even cooler.

1:39:19.760 --> 1:39:21.560
 Well, the really cool thing is it has to do

1:39:21.560 --> 1:39:24.200
 with the infinite fun that you were referring to,

1:39:24.200 --> 1:39:26.600
 which is our brains have to be very limited.

1:39:26.600 --> 1:39:28.160
 And like you said, computations.

1:39:28.160 --> 1:39:29.880
 It's also very plastic.

1:39:29.880 --> 1:39:31.040
 Very plastic, yeah.

1:39:31.040 --> 1:39:33.600
 So it's a interesting combination.

1:39:33.600 --> 1:39:37.480
 The interesting open question is the limits

1:39:37.480 --> 1:39:38.800
 of that neuroplasticity.

1:39:38.800 --> 1:39:42.360
 Like how flexible is that thing?

1:39:42.360 --> 1:39:44.920
 Because we haven't really tested it.

1:39:44.920 --> 1:39:47.000
 We know about that experience where they put

1:39:47.000 --> 1:39:49.160
 like a pressure pad on somebody's head

1:39:49.160 --> 1:39:51.560
 and had a visual transducer pressurize it

1:39:51.560 --> 1:39:53.520
 and somebody slowly learned to see.

1:39:53.520 --> 1:39:54.360
 Yep.

1:39:55.920 --> 1:39:58.760
 Especially at a young age, if you throw a lot at it,

1:39:58.760 --> 1:40:03.760
 like what can it completely,

1:40:03.960 --> 1:40:06.880
 so can you like arbitrarily expand it with computing power?

1:40:06.880 --> 1:40:09.880
 So connected to the internet directly somehow.

1:40:09.880 --> 1:40:11.960
 Yeah, the answer is probably yes.

1:40:11.960 --> 1:40:14.400
 So the problem with biology and ethics is like,

1:40:14.400 --> 1:40:15.560
 there's a mess there.

1:40:15.560 --> 1:40:20.560
 Like us humans are perhaps unwilling to take risks

1:40:21.840 --> 1:40:25.600
 into directions that are full of uncertainty.

1:40:25.600 --> 1:40:26.440
 So it's like.

1:40:26.440 --> 1:40:28.880
 90% of the population's unwilling to take risks.

1:40:28.880 --> 1:40:31.320
 The other 10% is rushing into the risks

1:40:31.320 --> 1:40:34.160
 unaided by any infrastructure whatsoever.

1:40:34.160 --> 1:40:38.920
 And that's where all the fun happens in this society.

1:40:38.920 --> 1:40:41.120
 There's been huge transformations

1:40:41.120 --> 1:40:43.600
 in the last couple of thousand years.

1:40:43.600 --> 1:40:44.560
 Yeah, it's funny.

1:40:44.560 --> 1:40:48.200
 I got the chance to interact with this Matthew Johnson

1:40:48.200 --> 1:40:49.360
 from Johns Hopkins.

1:40:49.360 --> 1:40:52.520
 He's doing this large scale study of psychedelics.

1:40:52.520 --> 1:40:54.240
 It's becoming more and more.

1:40:54.240 --> 1:40:55.240
 I've gotten a chance to interact

1:40:55.240 --> 1:40:57.800
 with that community of scientists working on psychedelics.

1:40:57.800 --> 1:41:00.120
 But because of that, that opened the door to me

1:41:00.120 --> 1:41:02.760
 to all these, what are they called?

1:41:02.760 --> 1:41:06.480
 Psychonauts, the people who, like you said, the 10%.

1:41:06.480 --> 1:41:08.040
 Like I don't care.

1:41:08.040 --> 1:41:09.840
 I don't know if there's a science behind this.

1:41:09.840 --> 1:41:13.640
 I'm taking the spaceship to, if I'm be the first on Mars,

1:41:13.640 --> 1:41:17.480
 I'll be, you know, psychedelics interesting in the sense

1:41:17.480 --> 1:41:21.440
 that in another dimension, like you said,

1:41:21.440 --> 1:41:25.440
 it's a way to explore the limits of the human mind.

1:41:25.440 --> 1:41:28.240
 Like what is this thing capable of doing?

1:41:28.240 --> 1:41:30.560
 Cause you kind of, like when you dream,

1:41:30.560 --> 1:41:32.240
 you detach it, I don't know exactly

1:41:32.240 --> 1:41:33.080
 in your science of it,

1:41:33.080 --> 1:41:38.080
 but you detach your like reality from what your mind,

1:41:39.000 --> 1:41:40.800
 the images your mind is able to conjure up

1:41:40.800 --> 1:41:43.160
 and your mind goes into weird places.

1:41:43.160 --> 1:41:44.960
 And like entities appear,

1:41:44.960 --> 1:41:48.760
 somehow Freudian type of like trauma

1:41:48.760 --> 1:41:50.320
 is probably connected in there somehow.

1:41:50.320 --> 1:41:54.000
 You start to have like these weird, vivid worlds that like.

1:41:54.000 --> 1:41:55.480
 So do you actively dream?

1:41:56.360 --> 1:41:58.160
 Do you, why not?

1:41:59.240 --> 1:42:00.960
 I have like six hours of dreams

1:42:00.960 --> 1:42:03.080
 and it's like real useful time.

1:42:03.080 --> 1:42:03.920
 I know.

1:42:03.920 --> 1:42:07.320
 I don't, I don't for some reason, I just knock out

1:42:07.320 --> 1:42:12.120
 and I have sometimes like anxiety inducing kind of like

1:42:12.120 --> 1:42:16.640
 a very pragmatic like nightmare type of dreams,

1:42:16.640 --> 1:42:18.440
 but not nothing fun, nothing.

1:42:18.440 --> 1:42:19.280
 Nothing fun?

1:42:19.280 --> 1:42:20.600
 Nothing fun.

1:42:20.600 --> 1:42:24.640
 I try, I unfortunately have mostly have fun

1:42:24.640 --> 1:42:26.520
 in the waking world,

1:42:26.520 --> 1:42:30.000
 which is very limited in the amount of fun you can have.

1:42:30.000 --> 1:42:31.200
 It's not that limited either.

1:42:31.200 --> 1:42:33.400
 Yeah, that's why we'll have to talk.

1:42:35.040 --> 1:42:36.840
 Yeah, and your instructions.

1:42:36.840 --> 1:42:37.680
 Yeah.

1:42:37.680 --> 1:42:38.640
 There's like a manual for that.

1:42:38.640 --> 1:42:39.480
 You might wanna.

1:42:41.000 --> 1:42:41.840
 I looked it up.

1:42:41.840 --> 1:42:44.680
 I'll ask you on what, what did you dream?

1:42:44.680 --> 1:42:47.040
 You know, years ago when I read about, you know,

1:42:47.040 --> 1:42:51.360
 like, you know, a book about how to have, you know,

1:42:51.360 --> 1:42:53.080
 become aware in your dreams.

1:42:53.080 --> 1:42:54.320
 I worked on it for a while.

1:42:54.320 --> 1:42:55.960
 Like there's this trick about, you know,

1:42:55.960 --> 1:42:58.240
 imagine you can see your hands and look out

1:42:58.240 --> 1:43:00.640
 and I got somewhat good at it.

1:43:00.640 --> 1:43:04.360
 Like, but my mostly when I'm thinking about things

1:43:04.360 --> 1:43:05.440
 or working on problems,

1:43:05.440 --> 1:43:09.040
 I prep myself before I go to sleep.

1:43:09.040 --> 1:43:13.160
 It's like, I pull into my mind all the things

1:43:13.160 --> 1:43:15.400
 I wanna work on or think about.

1:43:15.400 --> 1:43:19.840
 And then that, let's say, greatly improves the chances

1:43:19.840 --> 1:43:22.160
 that I'll work on that while I'm sleeping.

1:43:23.400 --> 1:43:28.400
 And then I also, you know, basically asked to remember it.

1:43:30.320 --> 1:43:33.200
 And I often remember very detailed.

1:43:33.200 --> 1:43:34.120
 Within the dream.

1:43:34.120 --> 1:43:34.960
 Yeah.

1:43:34.960 --> 1:43:35.800
 Or outside the dream.

1:43:35.800 --> 1:43:37.800
 Well, to bring it up in my dreaming

1:43:37.800 --> 1:43:39.800
 and then to remember it when I wake up.

1:43:41.040 --> 1:43:43.360
 It's just, it's more of a meditative practice.

1:43:43.360 --> 1:43:47.920
 You say, you know, to prepare yourself to do that.

1:43:48.920 --> 1:43:50.560
 Like if you go to, you know, to sleep,

1:43:50.560 --> 1:43:52.960
 still gnashing your teeth about some random thing

1:43:52.960 --> 1:43:55.520
 that happened that you're not that really interested

1:43:55.520 --> 1:43:56.840
 in your dream about it.

1:43:57.960 --> 1:43:58.800
 That's really interesting.

1:43:58.800 --> 1:43:59.640
 Maybe.

1:43:59.640 --> 1:44:03.440
 But you can direct your dreams somewhat by prepping.

1:44:04.440 --> 1:44:05.440
 Yeah, I'm gonna have to try that.

1:44:05.440 --> 1:44:06.400
 It's really interesting.

1:44:06.400 --> 1:44:08.440
 Like the most important, the interesting,

1:44:08.440 --> 1:44:12.240
 not like what did this guy send an email

1:44:12.240 --> 1:44:14.080
 kind of like stupid worry stuff,

1:44:14.080 --> 1:44:16.320
 but like fundamental problems you're actually concerned about.

1:44:16.320 --> 1:44:17.160
 Yeah.

1:44:17.160 --> 1:44:18.000
 Prepping.

1:44:18.000 --> 1:44:18.840
 And interesting things you're worried about.

1:44:18.840 --> 1:44:19.680
 Interesting.

1:44:19.680 --> 1:44:20.520
 Or most of your reading or, you know,

1:44:20.520 --> 1:44:21.360
 some great conversation you had

1:44:21.360 --> 1:44:23.480
 or some adventure you want to have.

1:44:23.480 --> 1:44:28.480
 Like there's a lot of space there and it seems to work

1:44:31.080 --> 1:44:34.400
 that, you know, my percentage of interesting dreams

1:44:34.400 --> 1:44:36.440
 and memories went up.

1:44:36.440 --> 1:44:40.440
 Is there, is that the source of,

1:44:40.440 --> 1:44:42.760
 if you were able to deconstruct like where

1:44:42.760 --> 1:44:44.600
 some of your best ideas came from,

1:44:45.720 --> 1:44:49.440
 is there a process that's at the core of that?

1:44:49.440 --> 1:44:52.440
 Like, so some people, you know, walk and think,

1:44:52.440 --> 1:44:55.200
 some people like in the shower, the best ideas hit them.

1:44:55.200 --> 1:44:56.560
 If you talk about like Newton,

1:44:56.560 --> 1:44:58.600
 Apple hitting them on the head.

1:44:58.600 --> 1:45:01.120
 No, I found out a long time ago,

1:45:01.120 --> 1:45:03.240
 I process things somewhat slowly.

1:45:03.240 --> 1:45:05.760
 So like in college, I had friends that could study it

1:45:05.760 --> 1:45:07.560
 the last minute and get an A next day.

1:45:07.560 --> 1:45:09.080
 I can't do that at all.

1:45:09.080 --> 1:45:10.960
 So I always front loaded all the work.

1:45:10.960 --> 1:45:14.200
 Like I do all the problems early, you know,

1:45:14.200 --> 1:45:15.840
 for finals, like the last three days,

1:45:15.840 --> 1:45:18.840
 I wouldn't look at a book because I want, you know,

1:45:18.840 --> 1:45:22.240
 cause like a new fact day before finals may screw up

1:45:22.240 --> 1:45:23.920
 my understanding of what I thought I knew.

1:45:23.920 --> 1:45:27.240
 So my goal was to always get it in

1:45:27.240 --> 1:45:29.920
 and give it time to soak.

1:45:29.920 --> 1:45:32.080
 And I used to, you know,

1:45:32.080 --> 1:45:33.840
 I remember when we were doing like 3D calculus,

1:45:33.840 --> 1:45:36.320
 I would have these amazing dreams of 3D surfaces

1:45:36.320 --> 1:45:38.600
 with normal, you know, calculating the gradient.

1:45:38.600 --> 1:45:40.160
 And just like all come up.

1:45:40.160 --> 1:45:43.920
 So it was like really fun, like very visual.

1:45:43.920 --> 1:45:47.440
 And if I got cycles of that, that was useful.

1:45:48.480 --> 1:45:50.960
 And the other is just don't over filter your ideas.

1:45:50.960 --> 1:45:54.520
 Like I like that process of brainstorming

1:45:54.520 --> 1:45:55.640
 where lots of ideas can happen.

1:45:55.640 --> 1:45:57.400
 I like people who have lots of ideas.

1:45:57.400 --> 1:45:58.800
 And then you just let them sit.

1:45:58.800 --> 1:46:00.240
 Then there's a, yeah, I'll let them sit

1:46:00.240 --> 1:46:02.560
 and let it breathe a little bit.

1:46:02.560 --> 1:46:05.000
 And then reduce it to practice.

1:46:05.000 --> 1:46:09.920
 Like at some point you really have to, does it really work?

1:46:09.920 --> 1:46:12.080
 Like, you know, is this real or not?

1:46:13.000 --> 1:46:15.040
 Right, but you have to do both.

1:46:15.040 --> 1:46:16.160
 There's creative tension there.

1:46:16.160 --> 1:46:20.480
 Like how do you be both open and, you know, precise?

1:46:20.480 --> 1:46:22.280
 Have you had ideas that you just,

1:46:22.280 --> 1:46:26.120
 that sit in your mind for like years before the?

1:46:26.120 --> 1:46:27.360
 Sure.

1:46:27.360 --> 1:46:28.200
 That's it.

1:46:28.200 --> 1:46:31.760
 It's an interesting way to just generate ideas

1:46:31.760 --> 1:46:33.120
 and just let them sit.

1:46:33.120 --> 1:46:35.160
 Let them sit there for a while.

1:46:35.160 --> 1:46:38.480
 I think I have a few of those ideas.

1:46:38.480 --> 1:46:40.160
 You know, it was so funny.

1:46:40.160 --> 1:46:43.720
 Yeah, I think that's, you know, creativity,

1:46:43.720 --> 1:46:44.800
 this one or something.

1:46:45.760 --> 1:46:49.400
 For the slow thinkers in the, in the room, I suppose.

1:46:49.400 --> 1:46:53.320
 As I, some people, like you said, are just like, like the.

1:46:53.320 --> 1:46:54.880
 Yeah, it's really interesting.

1:46:54.880 --> 1:46:58.080
 There's so much diversity in how people think, you know,

1:46:58.080 --> 1:47:00.400
 how fast or slow they are, how well they remember,

1:47:00.400 --> 1:47:04.080
 don't like, you know, I'm not super good at remembering facts,

1:47:04.080 --> 1:47:06.480
 but processes and methods.

1:47:06.480 --> 1:47:08.080
 Like in our engineering, I went to Penn State

1:47:08.080 --> 1:47:11.880
 and almost all our engineering tests were open book.

1:47:11.880 --> 1:47:14.840
 I could remember the page and not the formula.

1:47:14.840 --> 1:47:15.920
 As soon as I saw the formula,

1:47:15.920 --> 1:47:19.760
 I could remember the whole method if I, if I'd learned it.

1:47:19.760 --> 1:47:20.600
 Yeah.

1:47:20.600 --> 1:47:23.480
 So it's a funny, where some people could, you know,

1:47:23.480 --> 1:47:25.600
 I just watched friends like flipping through the book,

1:47:25.600 --> 1:47:27.480
 trying to find the formula,

1:47:27.480 --> 1:47:30.120
 even knowing that they'd done just as much work.

1:47:30.120 --> 1:47:33.080
 Now we just opened the book and I was on page 27.

1:47:33.080 --> 1:47:35.960
 About a half, I could see the whole thing visually.

1:47:35.960 --> 1:47:36.800
 Yeah.

1:47:36.800 --> 1:47:37.640
 And, you know.

1:47:37.640 --> 1:47:39.040
 And you have to learn that about yourself

1:47:39.040 --> 1:47:41.480
 and figure out what the, what the function optimally.

1:47:41.480 --> 1:47:43.320
 I had a friend who, he was always concerned.

1:47:43.320 --> 1:47:45.760
 He didn't know how he came up with ideas.

1:47:45.760 --> 1:47:49.120
 He had lots of ideas, but he said they just sort of popped up.

1:47:49.120 --> 1:47:51.000
 Like he'd be working on something, he had this idea.

1:47:51.000 --> 1:47:53.320
 Like, where does it come from?

1:47:53.320 --> 1:47:54.840
 But you can have more awareness of it.

1:47:54.840 --> 1:47:58.040
 Like, like, like, like how you,

1:47:58.040 --> 1:48:00.440
 how your brain works as a little murky as you go down

1:48:00.440 --> 1:48:03.880
 from the voice in your head or the obvious visualizations.

1:48:03.880 --> 1:48:06.560
 Like when you visualize something, how does that happen?

1:48:06.560 --> 1:48:07.400
 Yeah, that's right.

1:48:07.400 --> 1:48:09.040
 You know, if I say, you know, visualize a volcano,

1:48:09.040 --> 1:48:09.880
 it's easy to do, right?

1:48:09.880 --> 1:48:12.520
 And what does it actually look like when you visualize it?

1:48:12.520 --> 1:48:14.400
 I can visualize to the point where I don't see

1:48:14.400 --> 1:48:16.240
 very much out of my eyes and I see the colors

1:48:16.240 --> 1:48:18.240
 of the thing I'm visualizing.

1:48:18.240 --> 1:48:20.560
 Yeah, but there's like a, there's a shape, there's a texture,

1:48:20.560 --> 1:48:23.120
 there's a color, but there's also conceptual visualization.

1:48:23.120 --> 1:48:25.680
 Like, what are you actually visualizing

1:48:25.680 --> 1:48:27.200
 when you're visualizing a volcano?

1:48:27.200 --> 1:48:28.480
 Just like with peripheral vision,

1:48:28.480 --> 1:48:29.680
 you think you see the whole thing.

1:48:29.680 --> 1:48:30.520
 Yeah, yeah, yeah.

1:48:30.520 --> 1:48:31.800
 That's a good way to say it.

1:48:31.800 --> 1:48:34.840
 You know, you have this kind of almost peripheral vision

1:48:34.840 --> 1:48:37.400
 of your visualizations, they're like these ghosts.

1:48:38.400 --> 1:48:40.160
 But if, you know, if you, if you work on it,

1:48:40.160 --> 1:48:42.280
 you can get a pretty high level of detail.

1:48:42.280 --> 1:48:44.360
 And somehow you can walk along those visualizations

1:48:44.360 --> 1:48:47.200
 to come up with an idea, which is weird.

1:48:47.200 --> 1:48:50.920
 But when you're thinking about solving problems,

1:48:50.920 --> 1:48:52.960
 like you're putting information in,

1:48:52.960 --> 1:48:55.720
 you're exercising the stuff you do know,

1:48:55.720 --> 1:48:58.120
 you're sort of teasing the area that's,

1:48:58.120 --> 1:49:00.680
 you don't understand and don't know,

1:49:00.680 --> 1:49:03.120
 but you can almost, you know, feel,

1:49:04.600 --> 1:49:06.560
 you know, that process happening.

1:49:06.560 --> 1:49:08.280
 You know, that's, that's how I, like,

1:49:10.040 --> 1:49:12.000
 like, I know sometimes when I'm working really hard

1:49:12.000 --> 1:49:14.880
 on something, like, I get really hot when I'm sleeping.

1:49:14.880 --> 1:49:17.280
 And, you know, it's like, we got the blank throw,

1:49:17.280 --> 1:49:20.040
 I wake up with all the blank throw on the floor.

1:49:20.040 --> 1:49:22.400
 And, you know, every time it's while I wake up

1:49:22.400 --> 1:49:25.360
 and think, wow, that was great, you know.

1:49:25.360 --> 1:49:27.600
 Oh, you're able to reverse engineer

1:49:27.600 --> 1:49:29.000
 what the hell happened there?

1:49:29.000 --> 1:49:30.360
 Oh, sometimes it's vivid dreams.

1:49:30.360 --> 1:49:32.520
 And sometimes it's just kind of like you say,

1:49:32.520 --> 1:49:35.160
 like shadow thinking that you sort of have this feeling

1:49:35.160 --> 1:49:36.960
 you're going through this stuff,

1:49:36.960 --> 1:49:38.760
 but it's not that obvious.

1:49:38.760 --> 1:49:40.960
 Isn't that so amazing that the mind just does

1:49:40.960 --> 1:49:42.880
 all these little experiments?

1:49:42.880 --> 1:49:45.240
 I never, you know, I thought, I always thought,

1:49:45.240 --> 1:49:46.760
 it's like a river that you can't,

1:49:46.760 --> 1:49:48.920
 you're just there for the ride, but you're right.

1:49:48.920 --> 1:49:50.360
 If you prep it.

1:49:50.360 --> 1:49:52.400
 No, it's all understandable.

1:49:52.400 --> 1:49:53.720
 Meditation really helps.

1:49:53.720 --> 1:49:56.240
 You gotta start figuring out, you need to learn language

1:49:56.240 --> 1:49:57.080
 if you're on mind.

1:49:59.280 --> 1:50:02.600
 And there's multiple levels of it, but.

1:50:02.600 --> 1:50:04.000
 The abstractions again, right?

1:50:04.000 --> 1:50:08.400
 It's somewhat comprehensible and observable and feelable

1:50:08.400 --> 1:50:09.960
 or whatever the right word is.

1:50:11.920 --> 1:50:13.640
 You know, you're not alone for the ride.

1:50:13.640 --> 1:50:15.560
 You are the ride.

1:50:15.560 --> 1:50:18.320
 I have to ask you, hardware engineer working

1:50:18.320 --> 1:50:21.400
 on your own networks now, what's consciousness?

1:50:21.400 --> 1:50:22.800
 What the hell is that thing?

1:50:22.800 --> 1:50:25.920
 Is that, is that just some little weird quirk

1:50:25.920 --> 1:50:29.240
 of our particular computing device?

1:50:29.240 --> 1:50:31.240
 Or is it something fundamental that we really need

1:50:31.240 --> 1:50:36.240
 to crack open it for, to build like good computers?

1:50:36.520 --> 1:50:37.920
 Do you ever think about consciousness?

1:50:37.920 --> 1:50:40.000
 Like why it feels like something to be?

1:50:40.000 --> 1:50:41.720
 I know, it's really weird.

1:50:42.600 --> 1:50:43.640
 So.

1:50:43.640 --> 1:50:44.480
 Yeah.

1:50:45.520 --> 1:50:47.960
 I mean, everything about it's weird.

1:50:47.960 --> 1:50:51.320
 First is to half a second behind reality, right?

1:50:51.320 --> 1:50:53.760
 It's a post hoc narrative about what happened.

1:50:53.760 --> 1:50:58.840
 You've already done stuff by the time you're conscious of it.

1:50:58.840 --> 1:51:01.200
 And your consciousness generally is a single threaded thing,

1:51:01.200 --> 1:51:04.120
 but we know your brain is 10 billion neurons running

1:51:04.120 --> 1:51:07.960
 some crazy parallel thing.

1:51:07.960 --> 1:51:11.160
 And there's a really big sorting thing going on there.

1:51:11.160 --> 1:51:13.560
 It also seems to be really reflective in the sense

1:51:13.560 --> 1:51:17.960
 that you create a space in your head, right?

1:51:17.960 --> 1:51:19.600
 Like we don't really see anything, right?

1:51:19.600 --> 1:51:22.800
 Like photons hit your eyes, it gets turned into signals,

1:51:22.800 --> 1:51:25.560
 it goes through multiple layers of neurons.

1:51:25.560 --> 1:51:28.160
 You know, like I'm so curious that, you know,

1:51:28.160 --> 1:51:30.440
 that looks glassy and that looks not glassy.

1:51:30.440 --> 1:51:33.480
 And like, like how the resolution of your vision is so high,

1:51:33.480 --> 1:51:36.040
 you had to go through all this processing.

1:51:36.040 --> 1:51:39.640
 Where for most of it, it looks nothing like vision, right?

1:51:39.640 --> 1:51:43.600
 Like, like there's no theater in your mind, right?

1:51:43.600 --> 1:51:46.800
 So we, we have a world in our heads.

1:51:46.800 --> 1:51:51.080
 We're literally disisolated behind our sensors,

1:51:51.080 --> 1:51:55.160
 but we can look at it, speculate about it,

1:51:55.160 --> 1:51:59.280
 speculate about alternatives, problem solve, what if,

1:51:59.280 --> 1:52:02.240
 you know, there's so many things going on,

1:52:02.240 --> 1:52:05.640
 and that process is lagging reality.

1:52:05.640 --> 1:52:07.040
 And it's single threaded,

1:52:07.040 --> 1:52:09.880
 even though the underlying thing is like massively parallel.

1:52:09.880 --> 1:52:12.240
 Yeah, so it's, it's so curious.

1:52:12.240 --> 1:52:14.000
 So imagine you're building an AI computer,

1:52:14.000 --> 1:52:15.840
 if you wanted to replicate humans,

1:52:15.840 --> 1:52:17.800
 well, you'd have huge arrays of neural networks,

1:52:17.800 --> 1:52:21.840
 and apparently only sixers have in deep, which is hilarious.

1:52:21.840 --> 1:52:23.160
 They don't even remember seven numbers,

1:52:23.160 --> 1:52:25.640
 but I think we can upgrade that a lot, right?

1:52:25.640 --> 1:52:27.680
 And then somewhere in there,

1:52:27.680 --> 1:52:30.040
 you would train the network to create basically

1:52:30.040 --> 1:52:32.360
 the world that you live in, right?

1:52:32.360 --> 1:52:35.280
 So like tell stories to itself about the world

1:52:35.280 --> 1:52:36.240
 that it's perceiving.

1:52:36.240 --> 1:52:40.240
 Well, create the world, tell stories in the world,

1:52:40.240 --> 1:52:45.080
 and then have many dimensions of, you know,

1:52:45.080 --> 1:52:47.640
 like side jokes to it.

1:52:47.640 --> 1:52:49.280
 Like we have an emotional structure,

1:52:49.280 --> 1:52:51.480
 like we have a biological structure,

1:52:51.480 --> 1:52:52.720
 and that seems hierarchical too,

1:52:52.720 --> 1:52:55.600
 like if you're hungry, it dominates your thinking.

1:52:55.600 --> 1:52:57.880
 If you're mad, it dominates your thinking.

1:52:57.880 --> 1:53:00.320
 Like, and we don't know if that's important

1:53:00.320 --> 1:53:03.040
 to consciousness or not, but it certainly disrupts,

1:53:03.040 --> 1:53:05.720
 you know, intrudes in the consciousness.

1:53:05.720 --> 1:53:08.120
 Like so there's lots of structure to that,

1:53:08.120 --> 1:53:09.840
 and we like to dwell on the past,

1:53:09.840 --> 1:53:11.240
 we like to think about the future,

1:53:11.240 --> 1:53:14.680
 we like to imagine, we like to fantasize, right?

1:53:14.680 --> 1:53:18.520
 And the somewhat circular observation of that

1:53:18.520 --> 1:53:20.600
 is the thing we call consciousness.

1:53:21.720 --> 1:53:23.320
 Now, if you created a computer system

1:53:23.320 --> 1:53:24.880
 that did all things, created worldviews,

1:53:24.880 --> 1:53:27.320
 created future alternate histories,

1:53:27.320 --> 1:53:29.040
 you know, dwelled on past events,

1:53:29.040 --> 1:53:33.000
 you know, accurately or semi accurately, you know, it's...

1:53:33.000 --> 1:53:35.320
 Well, consciousness just spring up like naturally.

1:53:35.320 --> 1:53:38.080
 Well, would that feel, look and feel conscious to you?

1:53:38.080 --> 1:53:39.920
 Like you seem conscious to me, but I don't know.

1:53:39.920 --> 1:53:41.760
 Like external observer sense.

1:53:41.760 --> 1:53:44.920
 Do you think a thing that looks conscious is conscious?

1:53:44.920 --> 1:53:47.080
 Like, do you, again,

1:53:47.080 --> 1:53:49.240
 this is like an engineering kind of question, I think,

1:53:49.240 --> 1:53:54.240
 because like, if we want to engineer consciousness,

1:53:56.800 --> 1:53:58.280
 is it okay to engineer something

1:53:58.280 --> 1:53:59.720
 that just looks conscious?

1:54:00.720 --> 1:54:02.680
 Or is there a difference between something that is...

1:54:02.680 --> 1:54:04.040
 Well, we have all consciousness

1:54:04.040 --> 1:54:07.120
 because it's a super effective way to manage our affairs.

1:54:07.120 --> 1:54:09.000
 Yeah, yeah, this is a social element, yeah.

1:54:09.000 --> 1:54:11.520
 Well, it gives us a planning system, you know,

1:54:11.520 --> 1:54:13.280
 we have a huge amount of stuff.

1:54:13.280 --> 1:54:14.720
 Like when we're talking,

1:54:14.720 --> 1:54:16.880
 like the reason we can talk really fast is we're modeling

1:54:16.880 --> 1:54:19.080
 each other a really high level of detail.

1:54:19.080 --> 1:54:21.360
 And consciousness is required for that.

1:54:21.360 --> 1:54:25.200
 Well, all those components together manifest consciousness.

1:54:26.160 --> 1:54:27.000
 Right?

1:54:27.000 --> 1:54:29.600
 So if we make intelligent beings that we want to interact with

1:54:29.600 --> 1:54:32.040
 that we're like, you know, wondering what they're thinking,

1:54:32.040 --> 1:54:34.920
 you know, you know, looking forward to seeing them,

1:54:34.920 --> 1:54:36.480
 you know, when they interact with them,

1:54:36.480 --> 1:54:40.800
 they're interesting, surprising, you know, fascinating,

1:54:40.800 --> 1:54:43.480
 you know, they will probably be feel conscious like we do

1:54:43.480 --> 1:54:45.400
 and we'll perceive them as conscious.

1:54:47.160 --> 1:54:49.000
 I don't know why not, but never know.

1:54:49.960 --> 1:54:51.440
 Another fun question on this,

1:54:51.440 --> 1:54:55.040
 because in, from a computing perspective,

1:54:55.040 --> 1:54:56.680
 we're trying to create something that's human like

1:54:56.680 --> 1:54:57.840
 or super human like.

1:54:59.720 --> 1:55:01.280
 Let me ask you about aliens.

1:55:01.280 --> 1:55:02.120
 Aliens.

1:55:04.400 --> 1:55:08.440
 Do you think there's intelligent alien civilizations

1:55:08.440 --> 1:55:13.160
 out there and do you think their technology,

1:55:13.160 --> 1:55:16.480
 their computing, their AI bots,

1:55:16.480 --> 1:55:21.280
 their chips are of the same nature as ours?

1:55:21.280 --> 1:55:23.120
 Yeah, I got no idea.

1:55:23.120 --> 1:55:24.960
 I mean, if there's lots of aliens out there,

1:55:24.960 --> 1:55:26.320
 they've been awfully quiet.

1:55:27.320 --> 1:55:29.560
 You know, there's your speculation about why

1:55:30.680 --> 1:55:34.960
 there seems to be more than enough planets out there.

1:55:34.960 --> 1:55:35.800
 There's a lot.

1:55:35.800 --> 1:55:37.480
 Yeah.

1:55:37.480 --> 1:55:38.960
 There's intelligent life on this planet

1:55:38.960 --> 1:55:41.720
 that seems quite different, you know, like, you know,

1:55:41.720 --> 1:55:44.600
 dolphins seem like plausibly understandable.

1:55:44.600 --> 1:55:47.640
 Octopuses don't seem understandable at all.

1:55:47.640 --> 1:55:48.800
 If they live longer than a year,

1:55:48.800 --> 1:55:51.000
 maybe they would be running the planet.

1:55:51.000 --> 1:55:52.720
 They seem really smart.

1:55:52.720 --> 1:55:56.560
 And their neuro architecture is completely different than ours.

1:55:56.560 --> 1:55:58.680
 Now, who knows how they perceive things.

1:55:58.680 --> 1:56:01.200
 I mean, that's the question is for us intelligent beings,

1:56:01.200 --> 1:56:03.600
 we might not be able to perceive other kinds of intelligence

1:56:03.600 --> 1:56:05.600
 if they become sufficiently different than us.

1:56:05.600 --> 1:56:08.760
 Yeah, like we live in the current constrained world,

1:56:08.760 --> 1:56:10.600
 you know, it's three dimensional geometry

1:56:10.600 --> 1:56:14.440
 and the geometry defines a certain amount of physics.

1:56:14.440 --> 1:56:18.200
 And, you know, there's like how time work seems to work.

1:56:18.200 --> 1:56:21.680
 Like there's so many things that seem like a whole bunch

1:56:21.680 --> 1:56:23.480
 of the input parameters to the, you know,

1:56:23.480 --> 1:56:25.520
 another conscious being are the same.

1:56:26.440 --> 1:56:27.280
 Yes.

1:56:27.280 --> 1:56:29.960
 Like if it's biological, biological things seem to be

1:56:29.960 --> 1:56:32.920
 in a relatively narrow temperature range, right?

1:56:32.920 --> 1:56:35.600
 Because, you know, organics don't aren't stable,

1:56:35.600 --> 1:56:39.200
 too cold or too hot, you know, so, so there's,

1:56:39.200 --> 1:56:44.200
 if you specify the list of things that input to that,

1:56:45.240 --> 1:56:49.560
 but soon as we make really smart, you know, beings

1:56:49.560 --> 1:56:52.040
 and they go solve about how to think about a billion numbers

1:56:52.040 --> 1:56:56.040
 at the same time and then how to think in n dimensions.

1:56:56.040 --> 1:56:57.320
 There's a funny science fiction book

1:56:57.320 --> 1:57:01.560
 where all the society had uploaded into this matrix.

1:57:01.560 --> 1:57:05.320
 And at some point, some of the beings in the matrix thought,

1:57:05.320 --> 1:57:07.880
 I wonder if there's intelligent life out there.

1:57:07.880 --> 1:57:09.920
 So they had to do a whole bunch of work to figure out

1:57:09.920 --> 1:57:12.360
 like how to make a physical thing

1:57:12.360 --> 1:57:14.960
 because their matrix was self sustaining

1:57:14.960 --> 1:57:16.120
 and they made a little spaceship

1:57:16.120 --> 1:57:17.720
 and they traveled to another planet.

1:57:17.720 --> 1:57:20.600
 When they got there, there was like life running around,

1:57:20.600 --> 1:57:22.640
 but there was no intelligent life.

1:57:22.640 --> 1:57:26.200
 And then they figured out that there was these huge,

1:57:26.200 --> 1:57:29.560
 you know, organic matrix all over the planet inside there

1:57:29.560 --> 1:57:31.760
 where intelligent beings had uploaded themselves

1:57:31.760 --> 1:57:33.740
 and into that matrix.

1:57:34.960 --> 1:57:39.960
 So everywhere intelligent life was, soon as it got smart,

1:57:40.480 --> 1:57:43.640
 it up leveled itself into something way more interesting

1:57:43.640 --> 1:57:45.160
 than 3D geometry and...

1:57:45.160 --> 1:57:47.080
 Yeah, it escaped whatever the...

1:57:47.080 --> 1:57:48.400
 It's not escaped, it's...

1:57:48.400 --> 1:57:49.800
 Upload was better.

1:57:49.800 --> 1:57:53.240
 The essence of what we think of as an intelligent being,

1:57:53.240 --> 1:57:58.120
 I tend to like the thought experiment of the organism,

1:57:58.120 --> 1:58:00.400
 like humans aren't the organisms.

1:58:00.400 --> 1:58:03.760
 I like the notion of like Richard Dawkins and memes

1:58:03.760 --> 1:58:08.040
 that ideas themselves are the organisms,

1:58:08.040 --> 1:58:11.520
 like they're just using our minds to evolve.

1:58:11.520 --> 1:58:15.240
 So like we're just like meat receptacles

1:58:15.240 --> 1:58:18.200
 for ideas to breed and multiply and so on.

1:58:18.200 --> 1:58:20.920
 And maybe those are the aliens.

1:58:20.920 --> 1:58:22.240
 Yeah.

1:58:22.240 --> 1:58:26.720
 So Jordan Peterson has a line that says, you know,

1:58:26.720 --> 1:58:29.200
 you think you have ideas, but ideas have you.

1:58:29.200 --> 1:58:30.040
 Yeah.

1:58:30.040 --> 1:58:30.880
 Right?

1:58:30.880 --> 1:58:31.720
 Good line.

1:58:31.720 --> 1:58:34.280
 And then we know about the phenomenon of groupthink

1:58:34.280 --> 1:58:37.960
 and there's so many things that constrain us.

1:58:37.960 --> 1:58:39.960
 But I think you can examine all that

1:58:39.960 --> 1:58:43.320
 and not be completely owned by the ideas

1:58:43.320 --> 1:58:46.160
 and completely sucked into groupthink.

1:58:46.160 --> 1:58:49.840
 And part of your responsibility as a human

1:58:49.840 --> 1:58:52.800
 is to escape that kind of phenomena, which isn't...

1:58:52.800 --> 1:58:55.920
 You know, it's one of the creative tension things again.

1:58:55.920 --> 1:58:59.520
 You're constructed by it, but you can still observe it

1:58:59.520 --> 1:59:00.760
 and you can think about it

1:59:00.760 --> 1:59:04.040
 and you can make choices about to some level

1:59:04.040 --> 1:59:05.680
 how constrained you are by it.

1:59:06.960 --> 1:59:09.800
 And, you know, it's useful to do that.

1:59:12.000 --> 1:59:13.480
 And...

1:59:13.480 --> 1:59:17.400
 But at the same time, and it could be by doing that,

1:59:17.400 --> 1:59:21.480
 you know, the group and society you're part of

1:59:21.480 --> 1:59:24.200
 becomes collectively even more interesting.

1:59:24.200 --> 1:59:27.040
 So, you know, so that the outside observer will think,

1:59:27.040 --> 1:59:30.080
 wow, you know, all these lexes running around

1:59:30.080 --> 1:59:31.560
 with all these really independent ideas

1:59:31.560 --> 1:59:33.720
 have created something even more interesting

1:59:33.720 --> 1:59:35.720
 and aggregate.

1:59:35.720 --> 1:59:38.440
 So, so I don't know.

1:59:39.760 --> 1:59:41.880
 Those are lenses to look at the situation.

1:59:41.880 --> 1:59:42.720
 But it's all...

1:59:42.720 --> 1:59:43.560
 That'll give you some inspiration,

1:59:43.560 --> 1:59:45.480
 but I don't think they're constrained.

1:59:45.480 --> 1:59:46.720
 Right, you know.

1:59:46.720 --> 1:59:49.360
 As a small little quirk of history,

1:59:49.360 --> 1:59:53.680
 it seems like you're related to Jordan Peterson,

1:59:53.680 --> 1:59:54.920
 like you mentioned.

1:59:54.920 --> 1:59:57.680
 He's going through some rough stuff now.

1:59:57.680 --> 1:59:59.200
 Is there some comment you can make

1:59:59.200 --> 2:00:02.640
 about the roughness of the human journey,

2:00:02.640 --> 2:00:04.320
 ups and downs?

2:00:04.320 --> 2:00:09.320
 Well, I became an expert in Benz withdrawal.

2:00:10.800 --> 2:00:13.640
 Like, which is, you took Benz as the aspenes

2:00:13.640 --> 2:00:18.640
 and at some point they interact with GABA circuits,

2:00:19.040 --> 2:00:21.960
 you know, to reduce anxiety and do a hundred other things.

2:00:21.960 --> 2:00:25.120
 Like, there's actually no known list of everything they do

2:00:25.120 --> 2:00:28.240
 because they interact with so many parts of your body.

2:00:28.240 --> 2:00:30.520
 And then once you're on them, you habituate to them

2:00:30.520 --> 2:00:32.640
 and you have a dependency.

2:00:32.640 --> 2:00:34.200
 It's not like you're a drug dependency.

2:00:34.200 --> 2:00:35.080
 We're trying to get high.

2:00:35.080 --> 2:00:38.880
 It's a metabolic dependency.

2:00:38.880 --> 2:00:41.080
 And then if you discontinue them,

2:00:42.640 --> 2:00:44.440
 there's a funny thing called kindling,

2:00:45.400 --> 2:00:47.600
 which is if you stop them and then go,

2:00:47.600 --> 2:00:49.960
 you know, you'll have a horrible withdrawal symptoms.

2:00:49.960 --> 2:00:51.480
 If you go back on them at the same level,

2:00:51.480 --> 2:00:53.280
 you won't be stable.

2:00:53.280 --> 2:00:55.840
 And that unfortunately happened to him.

2:00:55.840 --> 2:00:57.280
 Because it's so deeply integrated

2:00:57.280 --> 2:00:58.880
 into all the kinds of systems in the body?

2:00:58.880 --> 2:01:00.840
 It literally changes the size and numbers

2:01:00.840 --> 2:01:03.880
 of neurotransmitter sites in your brain.

2:01:03.880 --> 2:01:07.400
 So there's a process called the Ashton protocol

2:01:07.400 --> 2:01:10.360
 where you taper it down slowly over two years.

2:01:10.360 --> 2:01:13.720
 The people go through that, go through unbelievable hell.

2:01:13.720 --> 2:01:15.680
 And what Jordan went through seemed to be worse

2:01:15.680 --> 2:01:18.520
 because on advice of doctors, you know,

2:01:18.520 --> 2:01:20.320
 we'll stop taking these and take this.

2:01:20.320 --> 2:01:23.920
 It was a disaster and he got some.

2:01:23.920 --> 2:01:25.320
 Yeah, it was pretty tough.

2:01:26.680 --> 2:01:29.240
 He seems to be doing quite a bit better intellectually.

2:01:29.240 --> 2:01:32.040
 You can see his brain clicking back together.

2:01:32.040 --> 2:01:32.960
 I spent a lot of time with him.

2:01:32.960 --> 2:01:34.960
 I've never seen anybody suffer so much.

2:01:34.960 --> 2:01:37.720
 Well, his brain is also like this powerhouse, right?

2:01:37.720 --> 2:01:42.480
 So I wonder, does a brain that's able to think deeply

2:01:42.480 --> 2:01:45.280
 about the world suffer more to these kinds of withdrawals?

2:01:45.280 --> 2:01:46.640
 Like, I don't know.

2:01:46.640 --> 2:01:49.440
 I've watched videos of people going through withdrawal.

2:01:49.440 --> 2:01:52.640
 They all seem to suffer unbelievably.

2:01:54.000 --> 2:01:57.520
 And, you know, my heart goes out to everybody.

2:01:57.520 --> 2:01:59.240
 And there's some funny math about this.

2:01:59.240 --> 2:02:01.920
 Some doctors said as best you can tell, you know,

2:02:01.920 --> 2:02:03.520
 there's the standard recommendations

2:02:03.520 --> 2:02:04.720
 don't take them for more than a month

2:02:04.720 --> 2:02:07.120
 and then taper over a couple of weeks.

2:02:07.120 --> 2:02:09.320
 Many doctors prescribe them endlessly,

2:02:09.320 --> 2:02:13.080
 which is against the protocol, but it's common, right?

2:02:13.080 --> 2:02:16.600
 And then something like 75% of people,

2:02:16.600 --> 2:02:18.520
 when they taper it's, you know,

2:02:18.520 --> 2:02:19.840
 half the people have difficulty,

2:02:19.840 --> 2:02:22.080
 but 75% get off okay.

2:02:22.080 --> 2:02:24.000
 20% have severe difficulty

2:02:24.000 --> 2:02:27.280
 and 5% have life threatening difficulty.

2:02:27.280 --> 2:02:29.520
 And if you're one of those, it's really bad.

2:02:29.520 --> 2:02:31.520
 And the stories that people have on this

2:02:31.520 --> 2:02:34.960
 is heartbreaking and tough.

2:02:34.960 --> 2:02:36.800
 So you put some of the fault at the doctors.

2:02:36.800 --> 2:02:38.600
 They just not know what the hell they're doing.

2:02:38.600 --> 2:02:40.520
 Oh, that was hard to say.

2:02:40.520 --> 2:02:43.080
 It's one of those commonly prescribed things.

2:02:43.080 --> 2:02:46.080
 Like one doctor said, what happens is

2:02:46.080 --> 2:02:47.800
 if you're prescribed them for a reason

2:02:47.800 --> 2:02:49.880
 and then you have a hard time getting off,

2:02:49.880 --> 2:02:52.440
 the protocol basically says you're either crazy

2:02:52.440 --> 2:02:55.480
 or dependent and you get kind of pushed

2:02:55.480 --> 2:02:58.360
 into a different treatment regime.

2:02:58.360 --> 2:03:01.800
 You're a drug addict or a psychiatric patient.

2:03:01.800 --> 2:03:04.080
 And so like one doctor said, you know,

2:03:04.080 --> 2:03:05.520
 I prescribed them for 10 years thinking

2:03:05.520 --> 2:03:06.560
 I was helping my patients

2:03:06.560 --> 2:03:08.680
 and I realized I was really harming them.

2:03:09.600 --> 2:03:12.880
 And, you know, the awareness of that is slowly coming up.

2:03:12.880 --> 2:03:17.880
 The fact that they're casually prescribed to people

2:03:18.160 --> 2:03:22.240
 is horrible and it's bloody scary.

2:03:23.800 --> 2:03:25.040
 And some people are stable on them,

2:03:25.040 --> 2:03:26.240
 but they're on them for life.

2:03:26.240 --> 2:03:27.080
 Like once you, you know,

2:03:27.080 --> 2:03:29.240
 it's another one of those drugs that,

2:03:29.240 --> 2:03:31.360
 but Benzo's long range have real impacts

2:03:31.360 --> 2:03:32.560
 on your personality.

2:03:32.560 --> 2:03:34.120
 People talk about the Benzo bubble

2:03:34.120 --> 2:03:36.320
 where you get disassociated from reality

2:03:36.320 --> 2:03:38.200
 and your friends a little bit.

2:03:38.200 --> 2:03:40.360
 It's really terrible.

2:03:40.360 --> 2:03:41.720
 The mind is terrifying.

2:03:41.720 --> 2:03:45.480
 We were talking about how the infinite possibility of fun,

2:03:45.480 --> 2:03:48.640
 but like it's the infinite possibility of suffering too,

2:03:48.640 --> 2:03:52.320
 which is one of the dangers of like expansion

2:03:52.320 --> 2:03:53.480
 of the human mind.

2:03:53.480 --> 2:03:58.200
 It's like, I wonder if all the possible human experiences

2:03:58.200 --> 2:04:01.680
 that intelligent computer can have,

2:04:01.680 --> 2:04:05.840
 is it mostly fun or is it mostly suffering?

2:04:05.840 --> 2:04:10.840
 So like if you brute force expand the set of possibilities

2:04:10.840 --> 2:04:13.960
 like are you going to run into some trouble

2:04:13.960 --> 2:04:16.560
 in terms of like torture and suffering and so on?

2:04:16.560 --> 2:04:18.840
 Maybe our human brain is just protecting us

2:04:18.840 --> 2:04:22.280
 from much more possible pain and suffering.

2:04:22.280 --> 2:04:25.960
 Maybe the space of pain is like much larger

2:04:25.960 --> 2:04:28.120
 than we could possibly imagine and that.

2:04:28.120 --> 2:04:29.520
 The world's in a balance.

2:04:30.720 --> 2:04:34.200
 You know, all the literature on religion and stuff is,

2:04:34.200 --> 2:04:36.280
 you know, the struggle between good and evil

2:04:36.280 --> 2:04:39.360
 is balanced for very finely tuned

2:04:39.360 --> 2:04:41.640
 for reasons that are complicated.

2:04:41.640 --> 2:04:44.840
 But that's a long philosophical conversation.

2:04:44.840 --> 2:04:46.680
 Speaking of balance that's complicated,

2:04:46.680 --> 2:04:48.640
 I wonder because we're living through one

2:04:48.640 --> 2:04:51.600
 of the more important moments in human history

2:04:51.600 --> 2:04:53.760
 with this particular virus,

2:04:53.760 --> 2:04:56.960
 it seems like pandemics have at least the ability

2:04:56.960 --> 2:05:01.960
 to kill off most of the human population at their worst.

2:05:03.040 --> 2:05:04.240
 And there's just fascinating

2:05:04.240 --> 2:05:06.120
 because there's so many viruses in this world.

2:05:06.120 --> 2:05:08.560
 There's so many, I mean viruses basically run the world

2:05:08.560 --> 2:05:12.240
 in the sense that they've been around for a very long time.

2:05:12.240 --> 2:05:13.640
 They're everywhere.

2:05:13.640 --> 2:05:15.320
 They seem to be extremely powerful

2:05:15.320 --> 2:05:17.240
 and they're distributed kind of way,

2:05:17.240 --> 2:05:19.560
 but at the same time they're not intelligent

2:05:19.560 --> 2:05:21.240
 and they're not even living.

2:05:21.240 --> 2:05:23.800
 Do you have like high level thoughts about this virus

2:05:23.800 --> 2:05:27.280
 that like in terms of you being fascinated

2:05:27.280 --> 2:05:30.360
 or terrified or somewhere in between?

2:05:30.360 --> 2:05:32.480
 So I believe in frameworks, right?

2:05:32.480 --> 2:05:35.400
 So like one of them is evolution.

2:05:36.240 --> 2:05:37.840
 Like we're evolved creatures, right?

2:05:37.840 --> 2:05:38.920
 Yes.

2:05:38.920 --> 2:05:40.840
 And one of the things about evolution

2:05:40.840 --> 2:05:42.720
 is it's hyper competitive.

2:05:42.720 --> 2:05:44.840
 And it's not competitive out of a sense of evil.

2:05:44.840 --> 2:05:47.760
 It's competitive in a sense of there's endless variation

2:05:47.760 --> 2:05:50.320
 and variations that work better when.

2:05:50.320 --> 2:05:52.920
 And then over time, there's so many levels

2:05:52.920 --> 2:05:56.760
 of that competition, like multi cellular life

2:05:56.760 --> 2:06:01.120
 partly exists because of the competition

2:06:01.120 --> 2:06:04.200
 between different kinds of life forms.

2:06:04.200 --> 2:06:06.840
 And we know sex partly exists to scramble our genes

2:06:06.840 --> 2:06:09.880
 so that we have genetic variation

2:06:09.880 --> 2:06:14.200
 against the invasion of the bacteria and the viruses.

2:06:14.200 --> 2:06:16.040
 And it's endless.

2:06:16.040 --> 2:06:18.000
 Like I read some funny statistic,

2:06:18.000 --> 2:06:20.760
 like the density of viruses and bacteria in the ocean

2:06:20.760 --> 2:06:22.040
 is really high.

2:06:22.040 --> 2:06:23.880
 And one third of the bacteria die every day

2:06:23.880 --> 2:06:26.200
 because the virus is invading them.

2:06:26.200 --> 2:06:27.960
 Like one third of them.

2:06:27.960 --> 2:06:29.040
 Wow.

2:06:29.040 --> 2:06:31.000
 Like I don't know if that number is true,

2:06:31.000 --> 2:06:34.880
 but it was like there's like the amount of competition

2:06:34.880 --> 2:06:37.320
 and what's going on is stunning.

2:06:37.320 --> 2:06:38.600
 And there's a theory as we age,

2:06:38.600 --> 2:06:41.720
 we slowly accumulate bacterias and viruses

2:06:41.720 --> 2:06:45.520
 and as our immune system kind of goes down,

2:06:45.520 --> 2:06:47.680
 that's what slowly kills us.

2:06:47.680 --> 2:06:50.160
 It just feels so peaceful from a human perspective

2:06:50.160 --> 2:06:52.200
 when we sit back and they're able to have a relaxed

2:06:52.200 --> 2:06:56.720
 conversation and there's wars going on out there.

2:06:56.720 --> 2:07:00.840
 Like right now you're harboring how many bacteria

2:07:00.840 --> 2:07:04.800
 and the ones, many of them are parasites on you.

2:07:04.800 --> 2:07:06.000
 And some of them are helpful.

2:07:06.000 --> 2:07:07.720
 And some of them are modifying your behavior.

2:07:07.720 --> 2:07:12.160
 And some of them are, you know, it's just really wild.

2:07:12.160 --> 2:07:16.160
 But you know, this particular manifestation is unusual.

2:07:16.160 --> 2:07:18.360
 You know, in the demographic, how it hit

2:07:18.360 --> 2:07:21.280
 and the political response that it engendered

2:07:21.280 --> 2:07:23.760
 and the healthcare response it engendered

2:07:23.760 --> 2:07:27.040
 and the technology it engendered, it's kind of wild.

2:07:27.040 --> 2:07:28.520
 Yeah, the communication on Twitter

2:07:28.520 --> 2:07:31.160
 that it led to all that kind of stuff,

2:07:31.160 --> 2:07:32.920
 at every single level, yeah.

2:07:32.920 --> 2:07:34.520
 But what usually kills is life.

2:07:34.520 --> 2:07:39.360
 The big extinctions are caused by meteors and volcanoes.

2:07:39.360 --> 2:07:40.720
 That's the one you're worried about,

2:07:40.720 --> 2:07:44.400
 as opposed to human created bombs that we launch.

2:07:44.400 --> 2:07:46.040
 Solar flares are another good one.

2:07:46.040 --> 2:07:48.520
 You know, occasionally solar flares hit the planet.

2:07:48.520 --> 2:07:49.520
 So it's nature.

2:07:51.080 --> 2:07:52.640
 Yeah, it's all pretty wild.

2:07:53.480 --> 2:07:57.440
 On another historic moment, this is perhaps outside,

2:07:57.440 --> 2:08:02.440
 but perhaps within your space of frameworks

2:08:02.440 --> 2:08:04.600
 so you think about that just happened,

2:08:04.600 --> 2:08:06.680
 I guess a couple of weeks ago is,

2:08:06.680 --> 2:08:08.040
 I don't know if you're paying attention at all,

2:08:08.040 --> 2:08:12.440
 it's the game stop and Wall Street bets.

2:08:12.440 --> 2:08:14.160
 It's a lot of fun.

2:08:14.160 --> 2:08:16.600
 So it's really fascinating.

2:08:16.600 --> 2:08:19.200
 There's kind of a theme to this conversation today

2:08:19.200 --> 2:08:20.800
 because it's like neural networks,

2:08:22.040 --> 2:08:25.040
 it's cool how there's a large number of people

2:08:25.040 --> 2:08:30.040
 in a distributed way, almost having a kind of fun,

2:08:30.040 --> 2:08:35.040
 were able to take on the powerful elite hedge funds,

2:08:35.840 --> 2:08:39.120
 centralized powers and overpower them.

2:08:40.000 --> 2:08:43.360
 Do you have thoughts on this whole saga?

2:08:43.360 --> 2:08:45.000
 I don't know enough about finance,

2:08:45.000 --> 2:08:47.800
 but it was like the Elon, you know,

2:08:47.800 --> 2:08:49.280
 Robin Hood guy when they talked.

2:08:49.280 --> 2:08:51.560
 Yeah, what'd you think about that?

2:08:51.560 --> 2:08:52.680
 Well, Robin Hood guy didn't know

2:08:52.680 --> 2:08:54.280
 how the finance system worked.

2:08:54.280 --> 2:08:55.560
 That was clear, right?

2:08:55.560 --> 2:08:57.760
 He was treating like the people who settled

2:08:57.760 --> 2:09:00.000
 the transactions as a black box.

2:09:00.000 --> 2:09:01.360
 And suddenly somebody called him up

2:09:01.360 --> 2:09:03.600
 and say, hey, black box calling you,

2:09:03.600 --> 2:09:05.240
 your transaction volume means you need

2:09:05.240 --> 2:09:06.920
 to put out $3 billion right now.

2:09:06.920 --> 2:09:08.960
 And he's like, I don't have $3 billion.

2:09:08.960 --> 2:09:10.520
 Like I don't even make any money on these trades.

2:09:10.520 --> 2:09:13.200
 Why do I have $3 billion while you're sponsoring a trade?

2:09:13.200 --> 2:09:16.960
 So there was a set of abstractions that,

2:09:16.960 --> 2:09:19.520
 I don't think either, like now we understand it.

2:09:19.520 --> 2:09:21.120
 Like this happens in chip design.

2:09:21.120 --> 2:09:25.640
 Like you buy wafers from TSMC or Samsung or Intel.

2:09:25.640 --> 2:09:27.440
 And they say it works like this

2:09:27.440 --> 2:09:29.000
 and you do your design based on that.

2:09:29.000 --> 2:09:31.280
 And then chip comes back and it doesn't work.

2:09:31.280 --> 2:09:34.280
 And then suddenly you started having to open the black boxes.

2:09:34.280 --> 2:09:36.400
 The transistors really work like they said,

2:09:36.400 --> 2:09:38.080
 what's the real issue?

2:09:38.080 --> 2:09:43.080
 So there's a whole set of things

2:09:43.240 --> 2:09:46.240
 that created this opportunity and somebody spotted it.

2:09:46.240 --> 2:09:49.880
 Now, people spot these kinds of opportunities all the time.

2:09:49.880 --> 2:09:52.720
 So there's been flash crashes, there's been,

2:09:52.720 --> 2:09:55.360
 there's always short squeezes that are fairly regular.

2:09:55.360 --> 2:09:58.480
 Every CEO I know hates the shorts

2:09:58.480 --> 2:10:00.320
 because they're manipulating,

2:10:00.320 --> 2:10:01.840
 they're trying to manipulate their stock

2:10:01.840 --> 2:10:05.920
 in a way that they make money and deprive value

2:10:05.920 --> 2:10:08.880
 from both the company and the investors.

2:10:08.880 --> 2:10:13.680
 So the fact that some of these stocks were so short,

2:10:13.680 --> 2:10:17.320
 it's hilarious that this hasn't happened before.

2:10:17.320 --> 2:10:18.160
 I don't know why.

2:10:18.160 --> 2:10:21.120
 And I don't actually know why some serious hedge funds

2:10:21.120 --> 2:10:23.440
 didn't do it to other hedge funds.

2:10:23.440 --> 2:10:25.360
 And some of the hedge funds actually made a lot of money

2:10:25.360 --> 2:10:26.200
 on this.

2:10:26.200 --> 2:10:31.200
 Yes, so my guess is we know 5% of what really happened

2:10:32.160 --> 2:10:34.440
 and a lot of the players don't know what happened.

2:10:34.440 --> 2:10:37.440
 And the people who probably made the most money

2:10:37.440 --> 2:10:39.560
 aren't the people that they're talking about.

2:10:39.560 --> 2:10:41.120
 Yeah, that's...

2:10:41.120 --> 2:10:42.720
 Do you think there was something...

2:10:42.720 --> 2:10:46.720
 I mean, this is the cool kind of Elon.

2:10:47.960 --> 2:10:50.720
 You're the same kind of conversationalist,

2:10:50.720 --> 2:10:52.360
 which is like first principles,

2:10:52.360 --> 2:10:56.280
 questions of like what the hell happened.

2:10:56.280 --> 2:10:57.920
 Just very basic questions of like,

2:10:57.920 --> 2:10:59.880
 was there something shady going on?

2:11:00.840 --> 2:11:03.680
 What, you know, who are the parties involved?

2:11:03.680 --> 2:11:06.320
 It's the basic questions that everybody wants to know about.

2:11:06.320 --> 2:11:08.480
 Yeah, so like we're in a very

2:11:08.480 --> 2:11:10.320
 hyper competitive world, right?

2:11:10.320 --> 2:11:12.160
 But transactions like buying and selling stock

2:11:12.160 --> 2:11:13.560
 is a trust event.

2:11:13.560 --> 2:11:14.520
 You know, I trust the company,

2:11:14.520 --> 2:11:16.240
 representing themselves properly.

2:11:16.240 --> 2:11:17.800
 You know, I bought the stock

2:11:17.800 --> 2:11:19.680
 because I think it's gonna go up.

2:11:19.680 --> 2:11:22.680
 I trust that the regulations are solid.

2:11:22.680 --> 2:11:26.120
 Now, inside of that, there's all kinds of places

2:11:26.120 --> 2:11:28.600
 where, you know, humans over trust.

2:11:28.600 --> 2:11:31.520
 And, you know, this, this expose,

2:11:31.520 --> 2:11:34.600
 let's say some weak points in the system.

2:11:34.600 --> 2:11:37.320
 I don't know if it's gonna get corrected.

2:11:37.320 --> 2:11:40.880
 I don't know if we have close to the real story.

2:11:41.760 --> 2:11:44.480
 You know, my suspicion is we don't.

2:11:44.480 --> 2:11:47.280
 And listening to that guy, he was like a little wide eyed

2:11:47.280 --> 2:11:49.080
 about, and then he did this and then he did that.

2:11:49.080 --> 2:11:51.800
 And I was like, I think you should know more

2:11:51.800 --> 2:11:54.160
 about that spit your business than that.

2:11:54.160 --> 2:11:56.480
 But again, there's many businesses when,

2:11:56.480 --> 2:11:58.720
 like this layer is really stable.

2:11:58.720 --> 2:12:00.640
 You stop paying attention to it.

2:12:00.640 --> 2:12:04.320
 You pay attention to the stuff that's bugging you or new.

2:12:04.320 --> 2:12:05.760
 You don't pay attention to the stuff

2:12:05.760 --> 2:12:07.040
 that just seems to work all the time.

2:12:07.040 --> 2:12:11.040
 You just, you know, the sky's blue every day, California.

2:12:11.040 --> 2:12:12.840
 And we're once while, you know, it rains there.

2:12:12.840 --> 2:12:14.200
 It was like, what do we do?

2:12:15.240 --> 2:12:17.240
 Somebody go bring in the lawn furniture.

2:12:17.240 --> 2:12:18.680
 You know, like it's getting wet.

2:12:18.680 --> 2:12:19.960
 You don't know why it's getting wet.

2:12:19.960 --> 2:12:20.800
 Yeah, it doesn't.

2:12:20.800 --> 2:12:24.560
 I was blue for like 100 days and now it's, you know, so.

2:12:24.560 --> 2:12:27.000
 But part of the problem here with Vlad,

2:12:27.000 --> 2:12:29.520
 the CEO of Robinhood is the scaling

2:12:29.520 --> 2:12:30.880
 is that what we've been talking about

2:12:30.880 --> 2:12:34.720
 is there's a lot of unexpected things

2:12:34.720 --> 2:12:36.040
 that happen with the scaling.

2:12:36.040 --> 2:12:39.680
 And you have to be, I think the scaling forces you

2:12:39.680 --> 2:12:41.920
 to then return to the fundamentals.

2:12:41.920 --> 2:12:43.640
 Well, it's interesting because when you buy

2:12:43.640 --> 2:12:45.600
 and sell stocks, the scaling is, you know,

2:12:45.600 --> 2:12:47.280
 the stocks don't only move in a certain range.

2:12:47.280 --> 2:12:50.000
 And if you buy a stock, you can only lose that amount of money.

2:12:50.000 --> 2:12:52.400
 On the short, short market, you can lose a lot more

2:12:52.400 --> 2:12:53.800
 than you can benefit.

2:12:53.800 --> 2:12:56.440
 Like it has a, it has a weird cause, you know,

2:12:56.440 --> 2:12:59.240
 cost function or whatever the right word for that is.

2:12:59.240 --> 2:13:01.080
 So he was trading in a market

2:13:01.080 --> 2:13:04.160
 where he wasn't actually capitalized for the downside.

2:13:04.160 --> 2:13:06.240
 If it got outside a certain range.

2:13:07.320 --> 2:13:09.720
 Now, whether something that various has happened,

2:13:09.720 --> 2:13:14.720
 I have no idea, but at some point the financial risk,

2:13:14.720 --> 2:13:16.920
 both him and his customers was way outside

2:13:16.920 --> 2:13:18.640
 of his financial capacity.

2:13:18.640 --> 2:13:22.840
 And his understanding how the system work was clearly weak

2:13:22.840 --> 2:13:24.640
 or he didn't represent himself.

2:13:24.640 --> 2:13:26.360
 I don't know the person.

2:13:26.360 --> 2:13:28.240
 When I listened to him, Nick,

2:13:28.240 --> 2:13:29.840
 it could have been the surprise question was,

2:13:29.840 --> 2:13:31.920
 like, how many of these guys called him?

2:13:31.920 --> 2:13:34.280
 You know, it sounded like he was treating stuff

2:13:34.280 --> 2:13:37.080
 as a black box, maybe he shouldn't have,

2:13:37.080 --> 2:13:38.360
 but maybe he has a whole pile of experts

2:13:38.360 --> 2:13:39.600
 somewhere else than it was going on.

2:13:39.600 --> 2:13:40.720
 I don't, I don't know.

2:13:40.720 --> 2:13:45.200
 Yeah, I mean, this is, this is one of the qualities

2:13:45.200 --> 2:13:49.080
 of a good leader is under fire, you have to perform.

2:13:49.080 --> 2:13:53.040
 And that means to think clearly and to speak clearly.

2:13:53.040 --> 2:13:55.280
 And he dropped the ball on those things

2:13:55.280 --> 2:13:58.040
 cause and understand the problem quickly,

2:13:58.040 --> 2:14:00.080
 learn and understand the problem at like,

2:14:00.080 --> 2:14:05.080
 at this like basic level, like what the hell happened.

2:14:05.080 --> 2:14:09.360
 And my guess is, you know, at some level it was amateurs

2:14:09.360 --> 2:14:12.320
 trading against, you know, experts slash insiders

2:14:12.320 --> 2:14:14.920
 slash people with, you know, special information.

2:14:14.920 --> 2:14:16.880
 Outsiders versus insiders.

2:14:16.880 --> 2:14:19.320
 Yeah. And the insiders, you know,

2:14:19.320 --> 2:14:21.160
 my guess is the next time this happens,

2:14:21.160 --> 2:14:23.000
 we'll make money on it.

2:14:23.000 --> 2:14:25.080
 The insiders always win.

2:14:25.080 --> 2:14:27.160
 Well, they have more tools and more incentive.

2:14:27.160 --> 2:14:28.480
 I mean, this always happens.

2:14:28.480 --> 2:14:30.800
 Like the outsiders are doing this for fun.

2:14:30.800 --> 2:14:33.320
 The insiders are doing this 24 stop.

2:14:33.320 --> 2:14:35.720
 But there's numbers in the outsiders.

2:14:35.720 --> 2:14:36.800
 This is the interesting thing.

2:14:36.800 --> 2:14:39.360
 Well, there's numbers on the insiders too.

2:14:40.360 --> 2:14:44.040
 Like that's different kind of numbers.

2:14:44.040 --> 2:14:46.080
 But this could be a new era because, I don't know,

2:14:46.080 --> 2:14:49.120
 at least I didn't expect that a bunch of Redditors could,

2:14:49.120 --> 2:14:51.280
 you know, there's, you know, millions of people

2:14:51.280 --> 2:14:54.200
 can get to the next one will be a surprise.

2:14:54.200 --> 2:14:56.480
 But don't you think the crowd,

2:14:56.480 --> 2:14:59.240
 the people are planning the next attack?

2:14:59.240 --> 2:15:00.480
 We'll see.

2:15:00.480 --> 2:15:01.440
 But it has to be a surprise.

2:15:01.440 --> 2:15:02.600
 Can't be the same game.

2:15:02.600 --> 2:15:06.520
 And so the insiders, like it could be,

2:15:06.520 --> 2:15:08.840
 there's a very large number of games to play

2:15:08.840 --> 2:15:10.520
 and they can be agile about it.

2:15:10.520 --> 2:15:12.160
 I don't know, I'm not an expert.

2:15:12.160 --> 2:15:13.720
 Right. That's a good question.

2:15:13.720 --> 2:15:16.480
 The space of games, how restricted is it?

2:15:18.000 --> 2:15:20.200
 Yeah. And the system is so complicated,

2:15:20.200 --> 2:15:22.720
 it could be relatively unrestricted.

2:15:22.720 --> 2:15:24.200
 And also, like, you know,

2:15:24.200 --> 2:15:26.640
 during the last couple of financial crashes,

2:15:26.640 --> 2:15:28.800
 you know, what set it off was, you know,

2:15:28.800 --> 2:15:31.320
 sets of derivative events where, you know,

2:15:31.320 --> 2:15:34.560
 Nesim Talib's, you know, saying is,

2:15:34.560 --> 2:15:39.400
 they're trying to lower volatility in the short run

2:15:39.400 --> 2:15:41.600
 by creating tail events.

2:15:41.600 --> 2:15:43.680
 And systems always evolve towards that.

2:15:43.680 --> 2:15:45.560
 And then they always crash.

2:15:45.560 --> 2:15:47.840
 Like S curve is the, you know,

2:15:47.840 --> 2:15:51.560
 start low, ramp, plateau, crash.

2:15:51.560 --> 2:15:53.080
 It's 100% effective.

2:15:54.520 --> 2:15:58.120
 In the long run, let me ask you some advice

2:15:58.120 --> 2:15:59.760
 to put on your profound hat.

2:15:59.760 --> 2:16:00.600
 Mm hmm.

2:16:01.640 --> 2:16:04.640
 There's a bunch of young folks who listen to this thing

2:16:04.640 --> 2:16:07.480
 for no good reason whatsoever.

2:16:07.480 --> 2:16:10.600
 Undergraduate students, maybe high school students,

2:16:10.600 --> 2:16:13.040
 maybe just young folks, young at heart,

2:16:13.040 --> 2:16:16.840
 looking for the next steps to taking life.

2:16:16.840 --> 2:16:19.320
 What advice would you give to a young person today

2:16:19.320 --> 2:16:23.840
 about life, maybe career, but also life in general?

2:16:23.840 --> 2:16:25.080
 Get good at some stuff.

2:16:26.080 --> 2:16:28.200
 Well, get to know yourself, right?

2:16:28.200 --> 2:16:30.640
 To get good at something that you're actually interested in.

2:16:30.640 --> 2:16:33.480
 You have to love what you're doing to get good at it.

2:16:33.480 --> 2:16:34.440
 You really got to find that.

2:16:34.440 --> 2:16:35.800
 Don't waste all your time doing stuff

2:16:35.800 --> 2:16:40.160
 that's just boring or bland or numbing, right?

2:16:40.160 --> 2:16:41.720
 Don't let old people screw you.

2:16:44.720 --> 2:16:46.760
 Well, people get talked into doing all kinds of shit

2:16:46.760 --> 2:16:49.320
 and racking up huge student, you know, student deaths.

2:16:49.320 --> 2:16:52.600
 And like, there's so much crap going on, you know?

2:16:52.600 --> 2:16:54.760
 And they drains your time and drains your energy.

2:16:54.760 --> 2:16:56.840
 Yeah, they are quite a sign, you know, thesis that,

2:16:56.840 --> 2:16:59.560
 you know, the older generation won't let go.

2:16:59.560 --> 2:17:01.200
 They're trapping all the young people.

2:17:01.200 --> 2:17:02.480
 I think that's some truth to that.

2:17:02.480 --> 2:17:03.320
 Yeah, sure.

2:17:05.000 --> 2:17:06.960
 Just because you're old doesn't mean you stop thinking.

2:17:06.960 --> 2:17:10.400
 I know lots of really original old people.

2:17:10.400 --> 2:17:11.960
 I'm an old person.

2:17:11.960 --> 2:17:15.680
 So, but you have to be conscious about it.

2:17:15.680 --> 2:17:19.000
 You can fall into the ruts and then do that.

2:17:19.000 --> 2:17:22.080
 I mean, when I hear young people spouting opinions,

2:17:22.080 --> 2:17:24.400
 it sounds like they come from Fox News or CNN.

2:17:24.400 --> 2:17:27.280
 I think they've been captured by group thinking, memes,

2:17:27.280 --> 2:17:28.120
 and stuff.

2:17:28.120 --> 2:17:29.520
 I supposed to think on their own.

2:17:29.520 --> 2:17:31.440
 You know, so if you find yourself repeating

2:17:31.440 --> 2:17:33.440
 what everybody else is saying,

2:17:33.440 --> 2:17:35.800
 you're not gonna have a good life.

2:17:35.800 --> 2:17:37.720
 Like, that's not how the world works.

2:17:37.720 --> 2:17:39.320
 It may be, it seems safe,

2:17:39.320 --> 2:17:41.480
 but it puts you at great jeopardy for

2:17:42.680 --> 2:17:45.960
 well being boring or unhappy.

2:17:45.960 --> 2:17:47.800
 How long did it take you to find the thing

2:17:47.800 --> 2:17:50.640
 that you have fun with?

2:17:50.640 --> 2:17:52.160
 I don't know.

2:17:52.160 --> 2:17:54.320
 I've been a fun person since I was pretty little.

2:17:54.320 --> 2:17:55.160
 So, everything.

2:17:55.160 --> 2:17:56.120
 I've gone through a couple of periods

2:17:56.120 --> 2:17:58.080
 of depression in my life.

2:17:58.080 --> 2:18:00.160
 Well, good reason or for a reason

2:18:00.160 --> 2:18:02.600
 that doesn't make any sense.

2:18:02.600 --> 2:18:03.440
 Yeah.

2:18:04.480 --> 2:18:05.960
 Like, some things are hard.

2:18:05.960 --> 2:18:08.880
 Like, you go through mental transitions in high school.

2:18:08.880 --> 2:18:10.680
 I was really depressed for a year.

2:18:10.680 --> 2:18:15.120
 And I think I had my first midlife crisis at 26.

2:18:15.120 --> 2:18:16.600
 I kind of thought, is this all there is?

2:18:16.600 --> 2:18:20.000
 Like, I was working at a job that I loved

2:18:20.000 --> 2:18:23.360
 and but I was going to work and all my time is consumed.

2:18:23.360 --> 2:18:25.760
 What's the escape out of that depression?

2:18:25.760 --> 2:18:29.200
 What's the answer to is, is this all there is?

2:18:29.200 --> 2:18:31.720
 Well, a friend of mine, I asked him

2:18:31.720 --> 2:18:32.840
 because he was working in his ass off.

2:18:32.840 --> 2:18:34.520
 I said, what's your work life balance?

2:18:34.520 --> 2:18:39.520
 Like, there's work, friends, family, personal time.

2:18:40.280 --> 2:18:41.360
 Are you balancing in that?

2:18:41.360 --> 2:18:43.560
 And he said, work 80%, family 20%.

2:18:43.560 --> 2:18:47.520
 And I tried to find some time to sleep.

2:18:47.520 --> 2:18:49.160
 Like, there's no personal time.

2:18:49.160 --> 2:18:51.800
 There's no passionate time.

2:18:51.800 --> 2:18:53.760
 Like, you know, the young people are often passionate

2:18:53.760 --> 2:18:54.600
 about work.

2:18:54.600 --> 2:18:56.920
 So I was sort of like that.

2:18:56.920 --> 2:18:59.920
 But you need to have some space in your life

2:18:59.920 --> 2:19:01.800
 for different things.

2:19:01.800 --> 2:19:05.840
 And that's, that creates, that makes you resistant

2:19:05.840 --> 2:19:10.840
 to the whole, the deep dips into depression kind of thing.

2:19:11.200 --> 2:19:13.040
 Yeah. Well, you have to get to know yourself too.

2:19:13.040 --> 2:19:14.440
 Meditation helps.

2:19:14.440 --> 2:19:18.480
 Some physical, something physically intense helps.

2:19:18.480 --> 2:19:21.920
 Like the weird places your mind goes kind of thing.

2:19:21.920 --> 2:19:23.760
 Like, and why does it happen?

2:19:23.760 --> 2:19:24.800
 Why do you do what you do?

2:19:24.800 --> 2:19:27.640
 Like triggers, like the things that cause your mind

2:19:27.640 --> 2:19:29.440
 to go to different places kind of thing.

2:19:29.440 --> 2:19:32.200
 Or like events, like.

2:19:32.200 --> 2:19:33.680
 You're upbringing for better or worse,

2:19:33.680 --> 2:19:35.640
 whether your parents are great people or not,

2:19:35.640 --> 2:19:40.640
 you come into adulthood with all kinds of emotional burdens.

2:19:42.320 --> 2:19:43.160
 Yeah.

2:19:43.160 --> 2:19:45.040
 And you can see some people are so bloody stiff

2:19:45.040 --> 2:19:46.800
 and restrained and they think, you know,

2:19:46.800 --> 2:19:49.040
 the world's fundamentally negative.

2:19:49.040 --> 2:19:53.000
 Like you maybe, you have unexplored territory.

2:19:53.000 --> 2:19:53.960
 Yeah.

2:19:53.960 --> 2:19:56.280
 Or you're afraid of something.

2:19:56.280 --> 2:19:58.720
 Definitely afraid of quite a few things.

2:19:58.720 --> 2:20:00.280
 Then you got to go face them.

2:20:00.280 --> 2:20:03.480
 Like what's the worst thing that can happen?

2:20:03.480 --> 2:20:05.160
 You're going to die, right?

2:20:05.160 --> 2:20:06.360
 Like that's inevitable.

2:20:06.360 --> 2:20:08.280
 You might as well get over that, like a hundred percent.

2:20:08.280 --> 2:20:09.800
 That's right.

2:20:09.800 --> 2:20:11.120
 Like people are worried about the virus,

2:20:11.120 --> 2:20:14.520
 but you know, the human condition is pretty deadly.

2:20:14.520 --> 2:20:16.360
 There's something about embarrassment.

2:20:16.360 --> 2:20:18.200
 That's, I've competed a lot in my life.

2:20:18.200 --> 2:20:21.960
 And I think the, if I'm too introspected,

2:20:21.960 --> 2:20:26.120
 the thing I'm most afraid of is being like humiliated.

2:20:26.120 --> 2:20:28.040
 I think nobody cares about that.

2:20:28.040 --> 2:20:30.080
 Like you're the only person on the planet

2:20:30.080 --> 2:20:32.320
 that cares about you being humiliated.

2:20:32.320 --> 2:20:34.720
 So it can really useless thought.

2:20:34.720 --> 2:20:35.560
 It is.

2:20:35.560 --> 2:20:39.520
 It's like, you're all humiliated.

2:20:39.520 --> 2:20:41.080
 Something happened in a room full of people

2:20:41.080 --> 2:20:42.640
 and they walk out and they didn't think about it.

2:20:42.640 --> 2:20:43.760
 One more second.

2:20:43.760 --> 2:20:45.360
 Or maybe somebody told a funny story

2:20:45.360 --> 2:20:47.520
 to somebody else and then it dissipated throughout.

2:20:47.520 --> 2:20:48.600
 Yeah.

2:20:48.600 --> 2:20:49.440
 Yeah.

2:20:49.440 --> 2:20:50.280
 No, I know it too.

2:20:50.280 --> 2:20:53.400
 I mean, I've been really embarrassed about shit

2:20:53.400 --> 2:20:55.520
 that nobody cared about myself.

2:20:55.520 --> 2:20:56.360
 Yeah.

2:20:56.360 --> 2:20:57.200
 It's a funny thing.

2:20:57.200 --> 2:20:59.920
 So the worst thing ultimately is just, yeah.

2:20:59.920 --> 2:21:02.600
 But that's a cage and then you have to get out of it.

2:21:02.600 --> 2:21:03.880
 Like once you, here's the thing.

2:21:03.880 --> 2:21:05.720
 Once you find something like that,

2:21:05.720 --> 2:21:08.160
 you have to be determined to break it.

2:21:09.040 --> 2:21:10.240
 Cause otherwise you'll just, you know,

2:21:10.240 --> 2:21:11.760
 so you accumulate that kind of junk

2:21:11.760 --> 2:21:15.440
 and then you die as a, you know, a mess.

2:21:15.440 --> 2:21:18.440
 So the goal, I guess it's like a cage within a cage.

2:21:18.440 --> 2:21:21.960
 I guess the goal is to die in the biggest possible cage.

2:21:21.960 --> 2:21:23.760
 Well, ideally you'd have no cage.

2:21:25.120 --> 2:21:26.480
 You know, people do get enlightened.

2:21:26.480 --> 2:21:27.440
 I've got a few.

2:21:27.440 --> 2:21:28.520
 It's great.

2:21:28.520 --> 2:21:29.360
 You found a few?

2:21:29.360 --> 2:21:30.480
 There's a few out there?

2:21:30.480 --> 2:21:31.320
 I don't know.

2:21:31.320 --> 2:21:32.160
 Of course there are.

2:21:33.360 --> 2:21:34.560
 Either that or they have, you know,

2:21:34.560 --> 2:21:35.520
 it's a great sales pitch.

2:21:35.520 --> 2:21:36.480
 There's like enlightened people,

2:21:36.480 --> 2:21:38.280
 write books and do all kinds of stuff.

2:21:38.280 --> 2:21:39.520
 It's a good way to sell a book.

2:21:39.520 --> 2:21:40.840
 I'll give you that.

2:21:40.840 --> 2:21:42.880
 You've never met somebody you just thought,

2:21:42.880 --> 2:21:43.840
 they just kill me.

2:21:43.840 --> 2:21:47.880
 Like they just, like mental clarity, humor.

2:21:47.880 --> 2:21:50.000
 No, 100%, but I just feel like they're living

2:21:50.000 --> 2:21:50.960
 in a bigger cage.

2:21:50.960 --> 2:21:52.000
 They have their own.

2:21:52.000 --> 2:21:53.320
 You still think there's a cage?

2:21:53.320 --> 2:21:54.360
 There's still a cage.

2:21:54.360 --> 2:21:57.560
 You secretly suspect there's always a cage.

2:21:57.560 --> 2:21:59.920
 There's no, there's nothing outside the universe.

2:21:59.920 --> 2:22:01.520
 There's nothing outside the cage.

2:22:01.520 --> 2:22:06.520
 You work, you work, you work at a bunch of companies.

2:22:06.520 --> 2:22:10.200
 You work, you work at a bunch of companies.

2:22:10.200 --> 2:22:12.680
 You led a lot of amazing teams.

2:22:14.080 --> 2:22:16.880
 I don't, I'm not sure if you've ever been like

2:22:16.880 --> 2:22:19.480
 at the early stages of a startup,

2:22:19.480 --> 2:22:24.480
 but do you have advice for somebody that wants to

2:22:25.880 --> 2:22:28.360
 do a startup or build a company,

2:22:28.360 --> 2:22:31.240
 like build a strong team of engineers that are passionate.

2:22:31.240 --> 2:22:35.040
 Just want to solve a big problem.

2:22:35.040 --> 2:22:38.360
 Like, is there a more specifically on that point?

2:22:39.360 --> 2:22:41.400
 You have to be really good at stuff.

2:22:41.400 --> 2:22:43.040
 If you're going to lead and build a team,

2:22:43.040 --> 2:22:46.160
 you better be really interested in how people work and think.

2:22:47.000 --> 2:22:49.080
 The people or the solution to the problem.

2:22:49.080 --> 2:22:50.200
 So there's two things, right?

2:22:50.200 --> 2:22:53.040
 One is how people work and the other is the fund.

2:22:53.040 --> 2:22:55.680
 Actually, there's quite a few successful startups

2:22:55.680 --> 2:22:56.520
 that's really clear.

2:22:56.520 --> 2:22:58.400
 The founders don't know anything about people.

2:22:58.400 --> 2:23:01.480
 Like the idea was so powerful that it propelled them.

2:23:01.480 --> 2:23:03.760
 But I suspect somewhere early,

2:23:03.760 --> 2:23:06.960
 they hired some people who understood people

2:23:06.960 --> 2:23:08.480
 because people really need a lot of care

2:23:08.480 --> 2:23:10.440
 and feeding the collaborate and work together

2:23:10.440 --> 2:23:12.760
 and feel engaged and work hard.

2:23:13.800 --> 2:23:17.000
 Like startups are all about out producing other people.

2:23:17.000 --> 2:23:19.800
 Like you're nimble because you don't have any legacy.

2:23:19.800 --> 2:23:22.880
 You don't have a bunch of people who are depressed

2:23:22.880 --> 2:23:24.720
 about life just showing up.

2:23:25.760 --> 2:23:28.120
 So startups have a lot of advantages that way.

2:23:28.120 --> 2:23:33.000
 Do you like the, Steve Jobs talked about this idea of A players

2:23:33.000 --> 2:23:33.840
 and B players?

2:23:33.840 --> 2:23:36.120
 I don't know if you know this formulation.

2:23:36.120 --> 2:23:36.960
 Yeah, no.

2:23:38.320 --> 2:23:42.240
 Organizations that get taken over by B player leaders

2:23:43.520 --> 2:23:46.960
 often really underperform their HRC players.

2:23:46.960 --> 2:23:49.240
 That said, in big organizations,

2:23:49.240 --> 2:23:51.200
 there's so much work to do.

2:23:51.200 --> 2:23:53.720
 And there's so many people who are happy to do what,

2:23:53.720 --> 2:23:56.240
 like the leadership or the big idea people

2:23:56.240 --> 2:23:58.640
 would consider menial jobs.

2:23:58.640 --> 2:24:00.480
 And you need a place for them,

2:24:00.480 --> 2:24:04.440
 but you need an organization that both values and rewards them,

2:24:04.440 --> 2:24:07.040
 but doesn't let them take over the leadership of it.

2:24:07.040 --> 2:24:07.880
 Got it.

2:24:07.880 --> 2:24:10.560
 So you need to have an organization that's resistant to that.

2:24:10.560 --> 2:24:12.800
 But in the early days,

2:24:12.800 --> 2:24:17.800
 the notion with Steve was that one B player in a room

2:24:18.440 --> 2:24:21.600
 of A players will be destructive to the whole.

2:24:21.600 --> 2:24:22.960
 I've seen that happen.

2:24:22.960 --> 2:24:25.080
 I don't know if it's always true,

2:24:25.080 --> 2:24:27.840
 like you run into people who are clearly B players,

2:24:27.840 --> 2:24:28.920
 but they think they're A players.

2:24:28.920 --> 2:24:30.560
 And so they have a loud voice at the table

2:24:30.560 --> 2:24:32.680
 and they make lots of demands for that.

2:24:32.680 --> 2:24:34.960
 But there's other people are like, I know I am.

2:24:34.960 --> 2:24:37.160
 I just want to work with cool people on cool shit

2:24:37.160 --> 2:24:39.560
 and just tell me what to do and I'll go get it done.

2:24:39.560 --> 2:24:42.440
 So you have to, again, this is like people skills,

2:24:42.440 --> 2:24:45.080
 like what kind of person is it?

2:24:45.080 --> 2:24:48.600
 I've met some really great people I love working with.

2:24:48.600 --> 2:24:50.280
 That weren't the biggest ID people,

2:24:50.280 --> 2:24:52.280
 the most productive ever, but they show up,

2:24:52.280 --> 2:24:53.120
 they get it done.

2:24:53.120 --> 2:24:55.680
 You know, they create connection and community

2:24:55.680 --> 2:24:57.000
 that people value.

2:24:57.000 --> 2:24:58.880
 It's pretty diverse.

2:24:58.880 --> 2:25:00.880
 I don't think there's a recipe for that.

2:25:01.880 --> 2:25:03.720
 I got to ask you about love.

2:25:03.720 --> 2:25:05.520
 I heard you into this now.

2:25:05.520 --> 2:25:06.360
 Into this love thing?

2:25:06.360 --> 2:25:07.200
 Yeah.

2:25:07.200 --> 2:25:10.120
 Do you think this is your solution to your depression?

2:25:10.120 --> 2:25:11.840
 No, I'm just trying to, like you said,

2:25:11.840 --> 2:25:13.880
 to delight in people on occasion trying to sell a book.

2:25:13.880 --> 2:25:14.960
 I'm writing a book about love.

2:25:14.960 --> 2:25:15.800
 You're writing a book about love.

2:25:15.800 --> 2:25:16.640
 No, I'm not.

2:25:16.640 --> 2:25:17.480
 I'm not.

2:25:17.480 --> 2:25:18.320
 I'm not.

2:25:18.320 --> 2:25:19.160
 I'm not.

2:25:19.160 --> 2:25:20.000
 I'm not.

2:25:20.000 --> 2:25:20.840
 I'm not.

2:25:20.840 --> 2:25:21.680
 I'm not.

2:25:21.680 --> 2:25:23.280
 I'm not.

2:25:23.280 --> 2:25:24.640
 I'm a friend of mine.

2:25:24.640 --> 2:25:25.760
 He's gonna.

2:25:25.760 --> 2:25:27.240
 Somebody said, you should really write a book

2:25:27.240 --> 2:25:29.120
 about your management philosophy.

2:25:29.120 --> 2:25:30.640
 He said, it'd be a short book.

2:25:35.000 --> 2:25:36.880
 Well, that one was all pretty well.

2:25:37.760 --> 2:25:40.440
 What role do you think love, family, friendship,

2:25:40.440 --> 2:25:44.400
 all that kind of human stuff play in a successful life?

2:25:44.400 --> 2:25:46.360
 You've been exceptionally successful in the space

2:25:46.360 --> 2:25:51.160
 of like running teams, building cool shit in this world,

2:25:51.160 --> 2:25:53.160
 creating some amazing things.

2:25:53.160 --> 2:25:54.720
 What, did love get in the way?

2:25:54.720 --> 2:25:57.680
 Did love help the family get in the way?

2:25:57.680 --> 2:25:59.720
 Did family help friendship?

2:25:59.720 --> 2:26:02.120
 You want the engineer's answer?

2:26:02.120 --> 2:26:02.960
 Please.

2:26:02.960 --> 2:26:05.800
 So, but first love is functional, right?

2:26:05.800 --> 2:26:07.280
 It's functional in what way?

2:26:07.280 --> 2:26:10.960
 So we habituate ourselves to the environment.

2:26:10.960 --> 2:26:12.040
 And actually Jordan told me,

2:26:12.040 --> 2:26:13.920
 Jordan Peterson told me this line.

2:26:13.920 --> 2:26:16.440
 So you go through life and you just get used to everything,

2:26:16.440 --> 2:26:17.800
 except for the things you love.

2:26:17.800 --> 2:26:20.120
 They remain new.

2:26:20.120 --> 2:26:22.440
 Like this is really useful for, you know,

2:26:22.440 --> 2:26:26.080
 like other people's children and dogs and trees.

2:26:26.080 --> 2:26:27.720
 You just don't pay that much attention to them.

2:26:27.720 --> 2:26:31.000
 Your own kids, you're monitoring them really closely.

2:26:31.000 --> 2:26:32.720
 Like, and if they go off a little bit,

2:26:32.720 --> 2:26:35.280
 because you love them, if you're smart,

2:26:35.280 --> 2:26:37.480
 if you're gonna be a successful parent,

2:26:37.480 --> 2:26:38.920
 you notice it right away.

2:26:38.920 --> 2:26:43.920
 You don't habituate just things you love.

2:26:44.280 --> 2:26:46.120
 And if you wanna be successful at work,

2:26:46.120 --> 2:26:47.560
 if you don't love it,

2:26:47.560 --> 2:26:50.400
 you're not gonna put the time in somebody else.

2:26:50.400 --> 2:26:51.640
 It's somebody else that loves it.

2:26:51.640 --> 2:26:53.760
 Like, cause it's new and interesting

2:26:53.760 --> 2:26:56.040
 and that lets you go to the next level.

2:26:57.560 --> 2:26:59.120
 So it's a thing, it's just a function

2:26:59.120 --> 2:27:01.680
 that generates newness and novelty

2:27:01.680 --> 2:27:04.680
 and surprises, you know, all those kinds of things.

2:27:04.680 --> 2:27:05.800
 It's really interesting.

2:27:05.800 --> 2:27:08.600
 Like, and there's people figured out lots of, you know,

2:27:08.600 --> 2:27:10.440
 frameworks for this, you know, like,

2:27:10.440 --> 2:27:12.440
 like humans seem to go in partnership,

2:27:12.440 --> 2:27:13.880
 go through, you know, interests.

2:27:13.880 --> 2:27:16.680
 Like somebody, suddenly somebody's interesting

2:27:16.680 --> 2:27:18.200
 and then you're infatuated with them

2:27:18.200 --> 2:27:20.080
 and then you're in love with them.

2:27:20.080 --> 2:27:22.640
 And then you, you know, different people have ideas

2:27:22.640 --> 2:27:24.520
 about parental love or mature love.

2:27:24.520 --> 2:27:26.600
 Like you go through a cycle of that,

2:27:26.600 --> 2:27:28.720
 which keeps us together and it's, you know,

2:27:28.720 --> 2:27:30.600
 super functional for creating families

2:27:30.600 --> 2:27:32.560
 and creating communities

2:27:32.560 --> 2:27:34.560
 and making you support somebody

2:27:34.560 --> 2:27:36.960
 despite the fact that you don't love them.

2:27:36.960 --> 2:27:41.960
 Like, and, and it can be really enriching.

2:27:41.960 --> 2:27:45.040
 You know, no, no, in the work life balance scheme,

2:27:45.040 --> 2:27:47.360
 if all you do is work,

2:27:47.360 --> 2:27:50.040
 you think you may be optimizing your work potential,

2:27:50.040 --> 2:27:51.600
 but if you don't love your work

2:27:51.600 --> 2:27:54.680
 or you don't have family and friends

2:27:54.680 --> 2:27:56.560
 and things you care about,

2:27:56.560 --> 2:27:59.560
 your brain isn't well balanced.

2:27:59.560 --> 2:28:01.040
 Like everybody knows experience

2:28:01.040 --> 2:28:02.360
 of your work's on something all week.

2:28:02.360 --> 2:28:05.360
 You went home and took two days off and you came back in.

2:28:05.360 --> 2:28:07.360
 The odds of you working on the thing,

2:28:07.360 --> 2:28:09.840
 you picking up right where you left off is zero.

2:28:09.840 --> 2:28:14.840
 Your brain refactored it, but being in love is great.

2:28:16.240 --> 2:28:18.840
 It's like change is the color of the light in the room.

2:28:18.840 --> 2:28:22.840
 It creates a spaciousness that's, that's different.

2:28:22.840 --> 2:28:26.840
 It helps you think, it makes you strong.

2:28:26.840 --> 2:28:29.840
 Bukowski had this line about love being a fog

2:28:29.840 --> 2:28:32.840
 that dissipates with the first light of reality

2:28:32.840 --> 2:28:33.840
 in the morning.

2:28:33.840 --> 2:28:34.840
 That's depressing.

2:28:34.840 --> 2:28:36.840
 I think it's the other way around.

2:28:36.840 --> 2:28:39.840
 It lasts, well, like you said, it's just a function.

2:28:39.840 --> 2:28:40.840
 It's a thing that generates.

2:28:40.840 --> 2:28:43.840
 It can be the light that actually enlivens your world

2:28:43.840 --> 2:28:46.840
 and creates the interest and the power and the strengths

2:28:46.840 --> 2:28:48.840
 to go do something.

2:28:48.840 --> 2:28:51.840
 It's like, that sounds like, you know,

2:28:51.840 --> 2:28:53.840
 there's like physical love, emotional love,

2:28:53.840 --> 2:28:55.840
 intellectual love, spiritual love, right?

2:28:55.840 --> 2:28:56.840
 Isn't it all the same thing?

2:28:56.840 --> 2:28:57.840
 Nope.

2:28:57.840 --> 2:28:59.840
 You should differentiate that.

2:28:59.840 --> 2:29:01.840
 Maybe that's your problem.

2:29:01.840 --> 2:29:03.840
 In your book, you should refine that a little bit.

2:29:03.840 --> 2:29:06.840
 The different chapters?

2:29:06.840 --> 2:29:08.840
 Yeah, there's different chapters.

2:29:08.840 --> 2:29:10.840
 What's the, what's, these are, aren't these just different

2:29:10.840 --> 2:29:12.840
 layers of the same thing or the stack?

2:29:12.840 --> 2:29:13.840
 Physical.

2:29:13.840 --> 2:29:16.840
 People, people, some people are addicted to physical love

2:29:16.840 --> 2:29:20.840
 and they have no idea about emotional or intellectual love.

2:29:20.840 --> 2:29:22.840
 I don't know if they're the same things.

2:29:22.840 --> 2:29:23.840
 I think they're different.

2:29:23.840 --> 2:29:24.840
 That's true.

2:29:24.840 --> 2:29:25.840
 They could be different.

2:29:25.840 --> 2:29:27.840
 I'd be, I guess the ultimate goal is for it to be the same.

2:29:27.840 --> 2:29:29.840
 Well, if you want something to be bigger and interesting,

2:29:29.840 --> 2:29:31.840
 you should find all its components and differentiate them,

2:29:31.840 --> 2:29:33.840
 not clown it together.

2:29:33.840 --> 2:29:35.840
 People do this all the time.

2:29:35.840 --> 2:29:36.840
 Yeah.

2:29:36.840 --> 2:29:37.840
 Modularity.

2:29:37.840 --> 2:29:39.840
 Get your abstraction layers right and then you can,

2:29:39.840 --> 2:29:40.840
 you have room to breathe.

2:29:40.840 --> 2:29:43.840
 Well, maybe you can write the forward to my book about love.

2:29:43.840 --> 2:29:45.840
 Or the afterwards.

2:29:45.840 --> 2:29:48.840
 You really tried.

2:29:48.840 --> 2:29:52.840
 I feel like Lex has made a lot of progress with this book.

2:29:52.840 --> 2:29:55.840
 Well, you have things in your life that you love.

2:29:55.840 --> 2:29:56.840
 Yeah.

2:29:56.840 --> 2:29:57.840
 Yeah.

2:29:57.840 --> 2:29:58.840
 And they are, you're right.

2:29:58.840 --> 2:29:59.840
 They're modular.

2:29:59.840 --> 2:30:04.840
 And you can have multiple things with the same person or the same thing.

2:30:04.840 --> 2:30:05.840
 Yeah.

2:30:05.840 --> 2:30:07.840
 But, yeah.

2:30:07.840 --> 2:30:09.840
 Depending on the moment of the day.

2:30:09.840 --> 2:30:10.840
 Yeah.

2:30:10.840 --> 2:30:14.840
 Like what Bacowski described is that moment you go from being in love

2:30:14.840 --> 2:30:16.840
 to having a different kind of love.

2:30:16.840 --> 2:30:17.840
 Yeah.

2:30:17.840 --> 2:30:18.840
 Right.

2:30:18.840 --> 2:30:19.840
 And that's a transition.

2:30:19.840 --> 2:30:22.840
 But when it happens, if you'd read the owner's manual and you believed it,

2:30:22.840 --> 2:30:24.840
 you would have said, oh, this happened.

2:30:24.840 --> 2:30:25.840
 It doesn't mean it's not love.

2:30:25.840 --> 2:30:27.840
 It's a different kind of love.

2:30:27.840 --> 2:30:32.840
 But, but maybe there's something better about that is you grow old.

2:30:32.840 --> 2:30:36.840
 If all you do is regret how you used to be.

2:30:36.840 --> 2:30:37.840
 It's sad.

2:30:37.840 --> 2:30:38.840
 Right.

2:30:38.840 --> 2:30:42.840
 You should have learned a lot of things because like who you can be in your future

2:30:42.840 --> 2:30:47.840
 self is actually more interesting and possibly delightful than, you know,

2:30:47.840 --> 2:30:51.840
 being a mad kid in love with the next person.

2:30:51.840 --> 2:30:54.840
 Like that's super fun when it happens.

2:30:54.840 --> 2:30:59.840
 That's, that's, you know, 5% of the possibility.

2:30:59.840 --> 2:31:00.840
 Yeah.

2:31:00.840 --> 2:31:01.840
 That's right.

2:31:01.840 --> 2:31:04.840
 That there's a lot more fun to be had in the long lasting stuff.

2:31:04.840 --> 2:31:05.840
 Yeah.

2:31:05.840 --> 2:31:07.840
 Or meaning, you know, if that's your thing.

2:31:07.840 --> 2:31:08.840
 Meaning, which is a kind of fun.

2:31:08.840 --> 2:31:10.840
 It's a deeper kind of fun.

2:31:10.840 --> 2:31:14.840
 And it's surprising, you know, that's like, like the thing I like is surprises,

2:31:14.840 --> 2:31:18.840
 you know, and you just never know what's going to happen.

2:31:18.840 --> 2:31:21.840
 But you have to look carefully and you have to work at it.

2:31:21.840 --> 2:31:23.840
 You have to think about it.

2:31:23.840 --> 2:31:24.840
 Yeah.

2:31:24.840 --> 2:31:26.840
 You have to see the surprises when they happen, right?

2:31:26.840 --> 2:31:29.840
 You have to be looking for it from the branching perspective.

2:31:29.840 --> 2:31:32.840
 You mentioned regrets.

2:31:32.840 --> 2:31:35.840
 Do you have regrets about your own trajectory?

2:31:35.840 --> 2:31:36.840
 Oh yeah.

2:31:36.840 --> 2:31:37.840
 Of course.

2:31:37.840 --> 2:31:38.840
 Yeah.

2:31:38.840 --> 2:31:42.840
 Some of it's painful, but you want to hear the painful stuff.

2:31:42.840 --> 2:31:48.840
 I'd say like in terms of working with people, when people did say stuff I didn't like,

2:31:48.840 --> 2:31:51.840
 especially if it was a bit nefarious, I took it personally.

2:31:51.840 --> 2:31:55.840
 I also felt it was personal about them.

2:31:55.840 --> 2:31:59.840
 But a lot of times, like humans are, you know, most humans are a mess, right?

2:31:59.840 --> 2:32:01.840
 And then they act out and they do stuff.

2:32:01.840 --> 2:32:08.840
 And this psychologist I heard a long time ago said, you tend to think somebody does something to you.

2:32:08.840 --> 2:32:12.840
 But really what they're doing is they're doing what they're doing while they're in front of you.

2:32:12.840 --> 2:32:14.840
 It's not that much about you.

2:32:14.840 --> 2:32:15.840
 Yeah.

2:32:15.840 --> 2:32:16.840
 Right.

2:32:16.840 --> 2:32:20.840
 And as I got more interested in, you know, when I work with people,

2:32:20.840 --> 2:32:25.840
 I think about them and probably analyze them and understand them a little bit.

2:32:25.840 --> 2:32:28.840
 And then when they do stuff, I'm way less surprised.

2:32:28.840 --> 2:32:31.840
 And I'm way, you know, and if it's bad, I'm way less hurt.

2:32:31.840 --> 2:32:33.840
 And I react way less.

2:32:33.840 --> 2:32:36.840
 Like I sort of expect everybody's got their shit.

2:32:36.840 --> 2:32:37.840
 Yeah.

2:32:37.840 --> 2:32:38.840
 And it's not about you.

2:32:38.840 --> 2:32:40.840
 It's not about me that much.

2:32:40.840 --> 2:32:44.840
 It's like, you know, you do something and you think you're embarrassed, but nobody cares.

2:32:44.840 --> 2:32:46.840
 Like somebody's really mad at you.

2:32:46.840 --> 2:32:48.840
 The odds of it being about you.

2:32:48.840 --> 2:32:49.840
 Yeah.

2:32:49.840 --> 2:32:52.840
 Because they're getting mad the way they're doing that because of some pattern they learned.

2:32:52.840 --> 2:32:56.840
 And, you know, and maybe you can help them if you care enough about it.

2:32:56.840 --> 2:32:59.840
 Or you could see it coming and step out of the way.

2:32:59.840 --> 2:33:02.840
 Like, I wish I was way better at that.

2:33:02.840 --> 2:33:04.840
 I'm a bit of a hothead.

2:33:04.840 --> 2:33:05.840
 You regret that?

2:33:05.840 --> 2:33:08.840
 You said with Steve, that was a feature, not a bug.

2:33:08.840 --> 2:33:09.840
 Yeah.

2:33:09.840 --> 2:33:13.840
 Well, he was using it as the counter for orderliness that would crush his work.

2:33:13.840 --> 2:33:14.840
 Well, you were doing the same.

2:33:14.840 --> 2:33:15.840
 Yeah.

2:33:15.840 --> 2:33:16.840
 Maybe.

2:33:16.840 --> 2:33:18.840
 I don't think my vision was big enough.

2:33:18.840 --> 2:33:22.840
 It was more like I just got pissed off and did stuff.

2:33:22.840 --> 2:33:24.840
 I'm sure that's the...

2:33:24.840 --> 2:33:25.840
 Yeah.

2:33:25.840 --> 2:33:26.840
 You're telling...

2:33:26.840 --> 2:33:28.840
 I don't know if it had the...

2:33:28.840 --> 2:33:31.840
 It didn't have the amazing effect of creating a trillion dollar company.

2:33:31.840 --> 2:33:34.840
 It was more like I just got pissed off and left.

2:33:34.840 --> 2:33:38.840
 And or made enemies that he shouldn't have.

2:33:38.840 --> 2:33:39.840
 Yeah.

2:33:39.840 --> 2:33:40.840
 It's hard.

2:33:40.840 --> 2:33:44.840
 Like, I didn't really understand politics until I worked at Apple where, you know, Steve

2:33:44.840 --> 2:33:47.840
 was a master player of politics and his staff had to be or they wouldn't survive them.

2:33:47.840 --> 2:33:50.840
 And it was definitely part of the culture.

2:33:50.840 --> 2:33:54.840
 And then I've been in companies where they say it's political, but it's all, you know,

2:33:54.840 --> 2:33:56.840
 fun and games compared to Apple.

2:33:56.840 --> 2:33:59.840
 And it's not that the people at Apple are bad people.

2:33:59.840 --> 2:34:03.840
 It's just they operate politically at a higher level.

2:34:03.840 --> 2:34:08.840
 You know, it's not like, oh, somebody said something bad about somebody, somebody else,

2:34:08.840 --> 2:34:10.840
 which is most politics.

2:34:10.840 --> 2:34:15.840
 It's, you know, they had strategies about accomplishing their goals.

2:34:15.840 --> 2:34:21.840
 Sometimes, you know, over the dead bodies of their enemies, you know, with sophistication.

2:34:21.840 --> 2:34:27.840
 Yeah, more game of thrones and sophistication and like a big time factor rather than a,

2:34:27.840 --> 2:34:28.840
 you know.

2:34:28.840 --> 2:34:34.840
 Well, that requires a lot of control over your emotions, I think, to have a bigger strategy

2:34:34.840 --> 2:34:35.840
 in the way you behave.

2:34:35.840 --> 2:34:36.840
 Yeah.

2:34:36.840 --> 2:34:42.840
 And it's effective in the sense that coordinating thousands of people to do really hard things

2:34:42.840 --> 2:34:47.840
 where many of the people in there don't understand themselves much less how they're participating

2:34:47.840 --> 2:34:54.840
 creates all kinds of, you know, drama and problems that, you know, our solution is political

2:34:54.840 --> 2:34:55.840
 and nature.

2:34:55.840 --> 2:34:56.840
 Like, how do you convince people?

2:34:56.840 --> 2:34:57.840
 How do you leverage them?

2:34:57.840 --> 2:34:58.840
 How do you motivate them?

2:34:58.840 --> 2:34:59.840
 How do you get rid of them?

2:34:59.840 --> 2:35:03.840
 How, you know, like, there's, there's so many layers of that that are interesting.

2:35:03.840 --> 2:35:11.840
 And even though some, some of it, let's say, may be tough, it's not evil.

2:35:11.840 --> 2:35:16.840
 Unless, you know, you use that skill to evil purposes, which some people obviously do.

2:35:16.840 --> 2:35:18.840
 But it's a skill set that operates.

2:35:18.840 --> 2:35:23.840
 You know, and I wish I'd, you know, I was interested in it, but I, you know, it was sort of like,

2:35:23.840 --> 2:35:26.840
 I'm an engineer, I do my thing.

2:35:26.840 --> 2:35:31.840
 And, you know, there's, there's times when I could have had a way bigger impact if I,

2:35:31.840 --> 2:35:35.840
 you know, knew how to, if I paid more attention and knew more about that.

2:35:35.840 --> 2:35:36.840
 Yeah.

2:35:36.840 --> 2:35:38.840
 About the human layer of the stack.

2:35:38.840 --> 2:35:42.840
 Yeah, that, that human political power, you know, expression layer of the stack.

2:35:42.840 --> 2:35:43.840
 It's just complicated.

2:35:43.840 --> 2:35:45.840
 And there's lots to know about it.

2:35:45.840 --> 2:35:47.840
 I mean, people are good at it or just amazing.

2:35:47.840 --> 2:35:55.840
 And when they're good at it and let's say relatively kind and oriented a good direction,

2:35:55.840 --> 2:36:00.840
 you can really feel, you can get lots of stuff done and coordinate things that you never

2:36:00.840 --> 2:36:02.840
 thought possible.

2:36:02.840 --> 2:36:07.840
 But all people like that also have some pretty hard edges because, you know, it's,

2:36:07.840 --> 2:36:08.840
 it's a heavy lift.

2:36:08.840 --> 2:36:13.840
 And I wish I'd spent more time with that when I was younger, but maybe I wasn't ready.

2:36:13.840 --> 2:36:16.840
 You know, I was a wide eyed kid for 30 years.

2:36:16.840 --> 2:36:18.840
 Still a bit of a kid.

2:36:18.840 --> 2:36:19.840
 Yeah, I know.

2:36:19.840 --> 2:36:26.840
 What do you hope your legacy is when there's a, when there's a book like a H Hikers guy

2:36:26.840 --> 2:36:27.840
 to the galaxy.

2:36:27.840 --> 2:36:32.840
 And this is like a, one sentence entry about Jim Miller from like that guy lived at some

2:36:32.840 --> 2:36:33.840
 point.

2:36:33.840 --> 2:36:37.840
 There's not many, you know, not many people would be remembered.

2:36:37.840 --> 2:36:43.840
 You're one of the sparkling little human creatures that had a big impact on the world.

2:36:43.840 --> 2:36:45.840
 How do you hope you'll be remembered?

2:36:45.840 --> 2:36:51.840
 My daughter was trying to get, she edited my Wikipedia page to say that I was a legend

2:36:51.840 --> 2:36:54.840
 in the guru, but they took it out.

2:36:54.840 --> 2:36:56.840
 So she put it back and she's 15.

2:36:56.840 --> 2:37:01.840
 I think, I think that was probably the best part of my legacy.

2:37:01.840 --> 2:37:04.840
 She got her sister and they were all excited.

2:37:04.840 --> 2:37:08.840
 They were like trying to put it in the references because there's articles in that on the top.

2:37:08.840 --> 2:37:11.840
 So in the eyes of your kids, you're a legend.

2:37:11.840 --> 2:37:15.840
 Well, they're pretty skeptical because they don't be better than that.

2:37:15.840 --> 2:37:17.840
 They're like, dad.

2:37:17.840 --> 2:37:22.840
 So yeah, that's, that's super, that kind of stuff is super fun in terms of the big legend

2:37:22.840 --> 2:37:23.840
 stuff.

2:37:23.840 --> 2:37:24.840
 I don't care.

2:37:24.840 --> 2:37:26.840
 I don't really care.

2:37:26.840 --> 2:37:28.840
 You're just an engineer.

2:37:28.840 --> 2:37:31.840
 They've been thinking about building a big pyramid.

2:37:31.840 --> 2:37:36.840
 So I had a debate with a friend about whether pyramids or craters are cooler.

2:37:36.840 --> 2:37:40.840
 And he realized that there's craters everywhere, but you know, they built a couple pyramids

2:37:40.840 --> 2:37:41.840
 5,000 years ago.

2:37:41.840 --> 2:37:42.840
 And they remember you for a while.

2:37:42.840 --> 2:37:44.840
 We're still talking about it.

2:37:44.840 --> 2:37:46.840
 I think that would be cool.

2:37:46.840 --> 2:37:48.840
 Those aren't easy to build.

2:37:48.840 --> 2:37:49.840
 Oh, I know.

2:37:49.840 --> 2:37:53.840
 And they don't actually know how they built them, which is great.

2:37:53.840 --> 2:37:58.840
 It's either a AGI or aliens could be involved.

2:37:58.840 --> 2:38:03.840
 So I think, I think you're going to have to figure out quite a few more things than just

2:38:03.840 --> 2:38:06.840
 the basics of civil engineering.

2:38:06.840 --> 2:38:09.840
 So I guess you hope your legacy is pyramids.

2:38:09.840 --> 2:38:11.840
 That would, that would be cool.

2:38:11.840 --> 2:38:15.840
 And my Wikipedia page, you know, getting updated by my daughter periodically.

2:38:15.840 --> 2:38:18.840
 Like those two things would pretty much make it.

2:38:18.840 --> 2:38:20.840
 Jim, it's a huge honor talking to you again.

2:38:20.840 --> 2:38:22.840
 I hope we talk many more times in the future.

2:38:22.840 --> 2:38:25.840
 I can't wait to see what you do with TimeStorrent.

2:38:25.840 --> 2:38:27.840
 I can't wait to use it.

2:38:27.840 --> 2:38:32.840
 I can't wait for you to revolutionize yet another space in computing.

2:38:32.840 --> 2:38:34.840
 It's a huge honor to talk to you.

2:38:34.840 --> 2:38:35.840
 Thanks for talking today.

2:38:35.840 --> 2:38:36.840
 This was fun.

2:38:36.840 --> 2:38:39.840
 Thanks for listening to this conversation with Jim Keller.

2:38:39.840 --> 2:38:44.840
 And thank you to our sponsors, Athletic Greens, all in one nutrition drink, Brooklyn and

2:38:44.840 --> 2:38:49.840
 Sheets, ExpressVPN, and Bell Campo grass fed meat.

2:38:49.840 --> 2:38:53.840
 Click the sponsor links to get a discount and to support this podcast.

2:38:53.840 --> 2:38:57.840
 And now let me leave you with some words from Alan Turing.

2:38:57.840 --> 2:39:02.840
 Those who can imagine anything can create the impossible.

2:39:02.840 --> 2:39:20.840
 Thank you for listening and hope to see you next time.

