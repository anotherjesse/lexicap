WEBVTT

00:00.000 --> 00:05.440
 The following is a conversation with Nick Bostrom, a philosopher at University of Oxford

00:05.440 --> 00:10.560
 and the director of the Future of Humanity Institute. He has worked on fascinating and

00:10.560 --> 00:16.800
 important ideas in existential risk, simulation hypothesis, human enhancement ethics,

00:16.800 --> 00:22.160
 and the risks of superintelligent AI systems, including in his book, Superintelligence.

00:23.040 --> 00:27.520
 I can see talking to Nick multiple times in this podcast, many hours each time,

00:27.520 --> 00:33.520
 because he has done some incredible work in artificial intelligence, in technology space,

00:33.520 --> 00:37.600
 science, and really philosophy in general. But we'll have to start somewhere.

00:38.640 --> 00:43.360
 This conversation was recorded before the outbreak of the coronavirus pandemic,

00:43.360 --> 00:47.840
 that both Nick and I, I'm sure, will have a lot to say about next time we speak.

00:48.560 --> 00:54.400
 And perhaps that is for the best, because the deepest lessons can be learned only in retrospect

00:54.400 --> 01:00.000
 when the storm has passed. I do recommend you read many of his papers on the topic of existential

01:00.000 --> 01:07.040
 risk, including the technical report titled Global Catastrophic Risks Survey that he coauthored with

01:07.040 --> 01:13.520
 Anders Sandberg. For everyone feeling the medical, psychological, and financial burden of this crisis,

01:13.520 --> 01:18.800
 I'm sending love your way. Stay strong. We're in this together. We'll beat this thing.

01:18.800 --> 01:25.440
 This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,

01:25.440 --> 01:29.760
 review it with five stars on Apple Podcast, support on Patreon, or simply connect with

01:29.760 --> 01:37.360
 me on Twitter at Lex Freedman, spelled F R I D M A N. As usual, I'll do one or two minutes of ads now

01:37.360 --> 01:41.680
 and never any ads in the middle that can break the flow of the conversation. I hope that works

01:41.680 --> 01:48.160
 for you. It doesn't hurt the listening experience. This show is presented by Cash App, the number

01:48.160 --> 01:54.640
 one finance app in the App Store. When you get it, use code LexPodcast. Cash App lets you send money

01:54.640 --> 02:00.800
 to friends by Bitcoin and invest in the stock market with as little as $1. Since Cash App does

02:00.800 --> 02:05.920
 fractional share trading, let me mention that the order execution algorithm that works behind the

02:05.920 --> 02:11.760
 scenes to create the abstraction of fractional orders is an algorithmic marvel. So big props to

02:11.760 --> 02:17.280
 the Cash App engineers for solving a hard problem that in the end provides an easy interface that

02:17.280 --> 02:22.240
 takes a step up to the next layer of abstraction over the stock market, making trading more accessible

02:22.240 --> 02:29.120
 for new investors and diversification much easier. So again, if you get Cash App from the App Store,

02:29.120 --> 02:36.880
 Google Play, and use the code LexPodcast, you get $10 and Cash App will also donate $10 to first,

02:36.880 --> 02:42.080
 an organization that is helping to advance robotics and STEM education for young people around the

02:42.080 --> 02:50.560
 world. And now here's my conversation with Nick Bostrom. At the risk of asking the Beatles to

02:50.560 --> 02:56.480
 play yesterday or the Rolling Stones to play satisfaction, let me ask you the basics. What

02:56.480 --> 03:03.440
 is the simulation hypothesis? That we are living in a computer simulation. What is a computer

03:03.440 --> 03:10.000
 simulation? How are we supposed to even think about that? Well, so the hypothesis is meant to be

03:10.000 --> 03:17.600
 understood in a literal sense, not that we can kind of metaphorically view the universe as an

03:17.600 --> 03:25.200
 information processing physical system, but that there is some advanced civilization who built a

03:25.200 --> 03:33.040
 lot of computers and that what we experience is an effect of what's going on inside one of those

03:33.040 --> 03:39.760
 computers so that the world around us, our own brains, everything we see and perceive and think

03:39.760 --> 03:49.920
 and feel would exist because this computer is running certain programs. So do you think of this

03:49.920 --> 03:56.480
 computer as something similar to the computers of today, these deterministic sort of touring machine

03:56.480 --> 04:01.440
 type things? Is that what we're supposed to imagine or we're supposed to think of something more

04:02.480 --> 04:09.040
 like a quantum mechanical system, something much bigger, something much more complicated,

04:09.040 --> 04:14.000
 something much more mysterious from our current perspective? The ones we have today would do

04:14.000 --> 04:19.120
 find them bigger, certainly. You'd need more memory and more processing power. I don't think

04:19.120 --> 04:24.640
 anything else would be required. Now, it might well be that they do have additional, maybe they

04:24.640 --> 04:29.920
 have quantum computers and other things that would give them even more umph. It seems kind of

04:29.920 --> 04:37.280
 plausible, but I don't think it's a necessary assumption in order to get to the conclusion

04:37.280 --> 04:43.280
 that a technologically mature civilization would be able to create these kinds of computer

04:43.280 --> 04:50.000
 simulations with conscious beings inside them. So do you think the simulation hypothesis is an idea

04:50.000 --> 04:56.400
 that's most useful in philosophy, computer science, physics, sort of where do you see it

04:58.240 --> 05:05.040
 having valuable kind of starting point in terms of the thought experiment of it?

05:05.040 --> 05:11.840
 Is it useful? I guess it's more informative and interesting and maybe important,

05:13.200 --> 05:16.400
 but it's not designed to be useful for something else.

05:16.400 --> 05:22.880
 Okay, interesting, sure. But is it philosophically interesting or is there some kind of implications

05:22.880 --> 05:29.040
 of computer science and physics? I think not so much for computer science or physics per se.

05:29.040 --> 05:37.440
 Certainly it would be of interest in philosophy, I think also to say cosmology or physics in as

05:37.440 --> 05:44.080
 much as you're interested in the fundamental building blocks of the world and the rules that

05:44.080 --> 05:50.160
 govern it. If we are in a simulation, there is then the possibility that say physics at the

05:50.160 --> 05:57.600
 level where the computer running the simulation could be different from the physics governing

05:57.600 --> 06:04.800
 phenomena in the simulation. So I think might be interesting from point of view of religion or

06:04.800 --> 06:13.600
 just for trying to figure out what the heck is going on. So we mentioned the simulation hypothesis

06:13.600 --> 06:20.160
 so far. There is also the simulation argument, which I tend to make a distinction. So simulation

06:20.160 --> 06:24.320
 hypothesis, we are living in a computer simulation argument, this argument that

06:24.320 --> 06:30.640
 tries to show that one of three propositions is true, one of which is the simulation hypothesis,

06:30.640 --> 06:36.400
 but there are two alternatives in the original simulation argument, which we can get to.

06:36.400 --> 06:42.480
 Yeah, let's go there. By the way, confusing terms because people will, I think, probably naturally

06:42.480 --> 06:47.760
 think simulation argument equals simulation hypothesis, just terminology wise. But let's go

06:47.760 --> 06:52.560
 there. So simulation hypothesis means that we are living in a simulation. The hypothesis that we're

06:52.560 --> 06:59.520
 living in a simulation, simulation argument has these three complete possibilities that cover

06:59.520 --> 07:03.760
 all possibilities. So what are they? Yeah, so it's like a disjunction. It says at least one of these

07:03.760 --> 07:13.200
 three is true, although it doesn't on its own tell us which one. So the first one is that almost all

07:14.400 --> 07:17.520
 civilizations at their current stage of technological development

07:17.520 --> 07:25.680
 go extinct before they reach technological maturity. So there is some great filter

07:27.440 --> 07:35.440
 that makes it so that basically none of the civilizations throughout, you know,

07:35.440 --> 07:42.080
 maybe vast cosmos will ever get to realize the full potential of technological development.

07:42.080 --> 07:48.160
 And this could be theoretically speaking, this could be because most civilizations kill themselves

07:48.160 --> 07:54.240
 too eagerly or destroy themselves too eagerly, or it might be super difficult to build a simulation.

07:54.960 --> 08:01.360
 So the span of time. Theoretically, it could be both. Now, I think it looks like we would

08:01.360 --> 08:08.640
 technologically be able to get there in a time span that is short compared to, say, the lifetime of

08:08.640 --> 08:16.160
 planets and other sort of astronomical processes. So your intuition is the build simulation is not.

08:16.720 --> 08:21.600
 Well, so this is interesting concept of technological maturity. It's kind of an

08:21.600 --> 08:27.120
 interesting concept to have other purposes as well. We can see, even based on our current

08:27.120 --> 08:33.200
 limited understanding, what some lower bound would be on the capabilities that you could

08:33.200 --> 08:39.440
 realize by just developing technologies that we already see are possible. So for example,

08:40.000 --> 08:48.240
 one of my research fellows here, Eric Drexler back in the 80s, studied molecular manufacturing.

08:48.240 --> 08:55.440
 That is, you could analyze using theoretical tools and computer modeling the performance

08:55.440 --> 09:01.200
 of various molecularly precise structures that we didn't then and still don't today have the

09:01.200 --> 09:06.480
 ability to actually fabricate. But you could say that, well, if we could put these atoms together

09:06.480 --> 09:12.960
 in this way, then the system would be stable and it would rotate at this speed and have these

09:12.960 --> 09:20.320
 computational characteristics. And he also outlined some pathways that would enable us to get to

09:20.320 --> 09:27.280
 this kind of molecularly manufacturing in the fullness of time. And you could do other studies

09:27.280 --> 09:33.360
 we've done. You could look at the speed at which say it would be possible to colonize the galaxy

09:33.360 --> 09:38.800
 if you had mature technology. We have an upper limit, which is the speed of light. We have

09:38.800 --> 09:43.760
 sort of a lower current limit, which is how fast current rockets go. We know we can go faster than

09:43.760 --> 09:51.440
 that by just making them bigger and have more fuel and stuff. And you can then start to describe

09:51.440 --> 09:57.760
 the technological affordances that would exist once a civilization has had enough time to develop.

09:57.760 --> 10:01.760
 Even at least those technologies we already know are possible. Then maybe they would discover other

10:01.760 --> 10:06.960
 new physical phenomena as well that we haven't realized that would enable them to do even more.

10:06.960 --> 10:11.920
 But at least there is this kind of basic set of capabilities.

10:11.920 --> 10:18.720
 Can you just link on that? How do we jump from molecular manufacturing to deep space exploration

10:18.720 --> 10:27.600
 to mature technology? What's the connection? These would be two examples of technological

10:27.600 --> 10:34.960
 capability sets that we can have a high degree of confidence or physically possible in our

10:34.960 --> 10:41.680
 universe under that civilization that was allowed to continue to develop its science and technology

10:41.680 --> 10:48.560
 would eventually attain. We can kind of see the set of breakthroughs that are likely to

10:48.560 --> 10:52.240
 happen. So you can see, what did you call it, the technological set?

10:52.880 --> 10:59.680
 With computers, maybe it's easier stuff. The one is we could just imagine bigger computers

10:59.680 --> 11:04.160
 using exactly the same parts that we have. So you can kind of scale things that way, right?

11:04.160 --> 11:10.000
 But you could also make processors a bit faster if you had this molecular nanotechnology that

11:10.000 --> 11:16.640
 Director Exeter described. He characterized a kind of crude computer built with these parts

11:16.640 --> 11:22.640
 that would perform at a million times the human brain while being significantly smaller, the

11:22.640 --> 11:28.320
 size of a sugar cube. And he may not claim that that's the optimum computing structure.

11:29.920 --> 11:34.000
 You could build a faster computer that would be more efficient, but at least you could do that

11:34.000 --> 11:39.200
 if you had the ability to do things that were atomically precise. So you can then combine these

11:39.200 --> 11:44.480
 two. You could have this kind of nanomolecular ability to build things atom by atom and then

11:44.480 --> 11:52.080
 say at this as a spatial scale that would be attainable through space colonizing technology.

11:52.800 --> 11:57.360
 You could then start, for example, to characterize a lower bound on the amount of computing power

11:58.080 --> 12:04.560
 that a technologically mature civilization would have if it could grab resources, you know,

12:04.560 --> 12:10.000
 planets and so forth and then use this molecular nanotechnology to optimize them for computing.

12:10.000 --> 12:16.160
 You'd get a very, very high lower bound on the amount of compute.

12:16.160 --> 12:21.280
 So, sorry, I just need to define some terms. So technologically mature civilization is one

12:21.280 --> 12:27.360
 that took that piece of technology to its lower bound. What is it technologically mature civilization?

12:27.360 --> 12:31.040
 Well, okay. So that means it's a stronger concept than we really need for the simulation hypothesis.

12:31.040 --> 12:35.280
 I just think it's interesting in its own right. So it would be the idea that there is

12:35.280 --> 12:41.840
 some stage of technological development where you've basically maxed out that you developed all

12:41.840 --> 12:48.400
 those general purpose widely useful technologies that could be developed or at least kind of come

12:48.400 --> 12:55.040
 very close to the, you know, 99.9% there or something. So that's an independent question.

12:55.040 --> 12:59.280
 You can think either that there is such a ceiling or you might think it just goes,

12:59.280 --> 13:04.720
 the technology tree just goes on forever. Where does your sense fall?

13:04.720 --> 13:09.840
 I would guess that there is a maximum that you would start to asymptote towards.

13:09.840 --> 13:13.760
 So new things won't keep springing up. New ceilings.

13:13.760 --> 13:19.840
 In terms of basic technological capabilities, I think there is like a finite set of those

13:19.840 --> 13:27.920
 that can exist in this universe. Moreover, I mean, I wouldn't be that surprised if we actually

13:27.920 --> 13:33.120
 reached close to that level fairly shortly after we have, say, machine superintelligence.

13:33.120 --> 13:39.120
 So I don't think it would take millions of years for a human originating civilization

13:39.120 --> 13:45.760
 to begin to do this. I think it's like more likely to happen on historical timescales.

13:45.760 --> 13:50.640
 But that's an independent speculation from the simulation argument.

13:50.640 --> 13:54.640
 I mean, for the purpose of the simulation argument, it doesn't really matter whether it

13:54.640 --> 13:59.120
 goes indefinitely far up or whether there is a ceiling, as long as we know we can at least

13:59.120 --> 14:04.560
 get to a certain level. And it also doesn't matter whether that's going to happen in 100 years

14:04.560 --> 14:10.560
 or 5,000 years or 50 million years. Like the timescales really don't make any difference

14:10.560 --> 14:15.360
 for the simulation. Can you look on that a little bit? There's a big difference between 100 years

14:15.360 --> 14:22.000
 and 10 million years. So it doesn't really not matter. Because you just said,

14:22.000 --> 14:30.320
 does it matter if we jump scales to beyond historical scales? So we described that. So

14:31.200 --> 14:40.720
 for the simulation argument, doesn't it matter that if it takes 10 million years,

14:40.720 --> 14:44.640
 it gives us a lot more opportunity to destroy civilization in the meantime?

14:44.640 --> 14:48.880
 Yeah. Well, so it would shift around the probabilities between these three alternatives.

14:48.880 --> 14:54.560
 Right. That is, if we are very, very far away from being able to create these simulations,

14:54.560 --> 14:58.960
 if it's like say the billions of years into the future, then it's more likely that we will fail

14:58.960 --> 15:03.840
 ever to get there. There's more time for us to kind of, you know, go extinct along the way.

15:03.840 --> 15:08.320
 And so it's similarly for other civilizations. So it is important to think about how hard it is

15:08.320 --> 15:15.200
 to build a simulation. In terms of figuring out which of the disjuncts. But for the simulation

15:15.200 --> 15:19.760
 argument itself, which is agnostic as to which of these three alternatives is true.

15:19.760 --> 15:20.320
 Yeah, okay.

15:21.600 --> 15:26.000
 You don't have to, like the simulation argument would be true whether or not,

15:26.000 --> 15:29.840
 we thought this could be done in 500 years or it would take 500 million years.

15:29.840 --> 15:33.520
 No, for sure. The simulation argument stands. I mean, I'm sure there might be some people who

15:33.520 --> 15:40.320
 oppose it, but it doesn't matter. I mean, it's very nice those three cases cover it. But the fun part

15:40.320 --> 15:45.360
 is at least not saying what the probabilities are, but kind of thinking about kind of

15:45.360 --> 15:51.360
 intuitive reasoning about like what's more likely, what are the kind of things that would make

15:51.360 --> 15:56.400
 some of the arguments less and more so like, but let's actually, I don't think we went through them.

15:56.400 --> 16:00.640
 So number one is we destroy ourselves before we ever create simulation.

16:00.640 --> 16:07.200
 Right. So that's kind of sad, but we have to think not just what might destroy us.

16:07.200 --> 16:12.960
 I mean, so there could be some whatever disasters or meteorites slamming the earth

16:13.680 --> 16:18.480
 a few years from now that could destroy us, right? But you'd have to postulate

16:19.520 --> 16:26.720
 in order for this first disjunct to be true that almost all civilizations throughout

16:26.720 --> 16:31.360
 the cosmos also failed to reach technological maturity.

16:31.360 --> 16:37.120
 And the underlying assumption there is that there is likely a very large number of other

16:37.120 --> 16:43.360
 intelligent civilizations. Well, if there are, yeah, then they would virtually all have to

16:43.360 --> 16:48.560
 succumb in the same way. I mean, then that leads off another. I guess there are a lot of little

16:48.560 --> 16:51.920
 digressions that they're interesting. Definitely, let's go there. Let's go there. I'll keep dragging

16:51.920 --> 16:56.720
 us back. Well, there are these, there is a set of basic questions that always come up

16:56.720 --> 17:02.000
 in conversations with interesting people. Yeah. Like the Fermi Paradox. Like there's like,

17:03.120 --> 17:07.680
 you could almost define whether a person is interesting, whether at some point the question

17:07.680 --> 17:15.120
 of the Fermi Paradox comes up. Well, so forward is where it looks to me that the universe is very

17:15.120 --> 17:22.480
 big. I mean, in fact, according to the most popular current cosmological theory is infinitely big.

17:22.480 --> 17:28.560
 And so then it would follow pretty trivially that it would contain a lot of other civilizations.

17:28.560 --> 17:35.040
 In fact, infinite the many. If you have some local stochasticity and infinite the many,

17:35.040 --> 17:38.960
 it's like, you know, infinite the many lumps of matter, one next to another, there's kind

17:38.960 --> 17:43.840
 of random stuff in each one, then you're going to get all possible outcomes with probability

17:43.840 --> 17:50.880
 one, infinitely repeated. So then certainly there would be a lot of random stuff in each one,

17:50.880 --> 17:53.920
 so then certainly there would be a lot of extraterrestrials out there.

17:55.920 --> 18:00.320
 Even short of that, if the universe is very big, there might be a finite but large number.

18:02.320 --> 18:05.280
 If we were literally the only one, yeah, then of course,

18:06.880 --> 18:13.520
 if we went extinct, then all of civilizations at our current stage would have gone extinct before

18:13.520 --> 18:19.360
 becoming technological material. So then it kind of becomes trivially true that a very high fraction

18:19.360 --> 18:24.880
 of those went extinct. But if we think there are many, I mean, it's interesting because there are

18:24.880 --> 18:31.920
 certain things that possibly could kill us, like if you look at existential risks,

18:33.840 --> 18:38.960
 and it might be a different, like that the best answer to what would be most likely to kill us

18:38.960 --> 18:44.720
 might be a different answer than the best answer to the question. If there is something that kills

18:44.720 --> 18:50.320
 almost everyone, what would that be? Because that would have to be some risk factor that was kind of

18:50.320 --> 18:56.160
 uniform overall possible civilization. Yeah. So in this, for the sake of this argument,

18:56.160 --> 19:02.240
 you have to think about not just us, but like every civilization dies out before they create

19:02.240 --> 19:09.760
 the simulation or something very close to everybody. Okay. So what's number two in the

19:09.760 --> 19:15.360
 Well, so number two is the convergence hypothesis that is that maybe like a lot of some of these

19:15.360 --> 19:20.480
 civilizations do make it through to technological maturity. But out of those who do get there,

19:21.200 --> 19:28.160
 they all lose interest in creating these simulations. So they just they have the capability

19:29.040 --> 19:34.640
 of doing it, but they choose not to. Yeah, not just a few of them decide not to, but

19:34.640 --> 19:41.440
 you know, out of a million, maybe not even a single one of them would do it.

19:41.440 --> 19:48.080
 And I think when you say lose interest, that sounds like unlikely because it's like they

19:48.080 --> 19:55.920
 get bored or whatever, but it could be so many possibilities within that. I mean, losing interest

19:55.920 --> 20:05.360
 could be it could be anything from it being exceptionally difficult to do to fundamentally

20:05.360 --> 20:12.240
 changing the sort of the fabric of reality. If you do it is ethical concerns, all those kinds

20:12.240 --> 20:18.320
 of things could be exceptionally strong pressures. Well, certainly, I mean, yeah, ethical concerns.

20:18.320 --> 20:24.080
 I mean, not really too difficult to do. I mean, in a sense, that's the first assumption that you

20:24.080 --> 20:29.360
 get to technological maturity where you would have the ability using only a tiny fraction of your

20:29.360 --> 20:36.960
 resources to create many, many simulations. So it wouldn't be the case that they would need to

20:36.960 --> 20:41.840
 spend half of their GDP forever in order to create one simulation. And they had this like

20:41.840 --> 20:46.160
 difficult debate about whether they should, you know, invest half of their GDP for this.

20:46.160 --> 20:50.800
 It would more be like, well, if any little fraction of the civilization feels like doing this

20:50.800 --> 20:57.360
 at any point during maybe their millions of years of existence, then there would be millions

20:57.920 --> 21:06.160
 of simulations. But certainly, there could be many conceivable reasons for why there

21:06.160 --> 21:10.960
 would be this convert, many possible reasons for not running ancestor simulations or other

21:10.960 --> 21:16.960
 computer simulations, even if you could do so cheaply. By the way, what's an ancestor simulation?

21:16.960 --> 21:24.800
 Well, that would be type of computer simulation that would contain people like those we think

21:24.800 --> 21:30.000
 have lived on our planet in the past and like ourselves in terms of the types of experiences

21:30.000 --> 21:35.760
 they have. And where those simulated people are conscious. So like, not just simulated in the

21:35.760 --> 21:42.560
 same sense that a non player character would be simulated in the current computer game where

21:42.560 --> 21:48.320
 it kind of has like an avatar body and then a very simple mechanism that moves it forward or

21:48.320 --> 21:56.080
 backwards or but something where the simulated being has a brain, let's say that simulated

21:56.080 --> 22:02.800
 at the sufficient level of granularity that it would have the same subjective experiences as we

22:02.800 --> 22:08.640
 have. So where does consciousness fit into this? Do you think simulation, like is there

22:08.640 --> 22:12.880
 different ways to think about how this can be simulated? Just like you're talking about now.

22:14.080 --> 22:22.080
 Do we have to simulate each brain within the larger simulation? Is it enough to simulate

22:22.080 --> 22:27.600
 just the brain, just the minds and not the simulation, not the big universe itself? Like,

22:27.600 --> 22:32.560
 is there different ways to think about this? Yeah, I guess there is a kind of premise

22:32.560 --> 22:40.240
 in the simulation argument, rolled in from philosophy of mind, that is that it would be

22:40.240 --> 22:46.800
 possible to create a conscious mind in a computer. And that what determines whether

22:47.520 --> 22:54.160
 some system is conscious or not is not like whether it's built from organic biological

22:54.160 --> 22:58.480
 neurons, but maybe something like what the structure of the computation is that it implements.

22:58.480 --> 23:05.760
 Right. So we can discuss that if we want, but I think it would be more forward as far as my view

23:05.760 --> 23:15.040
 that it would be sufficient, say, if you had a computation that was identical to the computation

23:15.040 --> 23:19.760
 in the human brain down to the level of neurons. So if you had a simulation with 100 billion neurons

23:19.760 --> 23:25.520
 connected in the same Western human brain, and you then roll that forward with the same kind of

23:25.520 --> 23:30.160
 synaptic weights and so forth, so you actually had the same behavior coming out of this

23:31.280 --> 23:37.120
 as a human with that brain, then I think that would be conscious. Now, it's possible you could

23:37.120 --> 23:46.000
 also generate consciousness without having that detailed assimilation. There, I'm getting more

23:46.000 --> 23:50.160
 uncertain exactly how much you could simplify or abstract away.

23:50.160 --> 23:55.760
 Can you look on that? What do you mean? So I missed where you're placing consciousness in

23:55.760 --> 24:01.920
 the second. Well, so if you are a computationalist, do you think that what creates consciousness is

24:01.920 --> 24:07.200
 the implementation of a computation? So some property, emergent property of the computation

24:07.200 --> 24:13.440
 itself is the idea. Yeah, you could say that. But then the question is, what's the class of

24:13.440 --> 24:19.120
 computations such that when they are run, consciousness emerges? So if you just have

24:19.120 --> 24:24.560
 like something that adds one plus one plus one plus one, like a simple computation, you think maybe

24:24.560 --> 24:31.760
 that's not going to have any consciousness. If on the other hand, the computation is one, like our

24:31.760 --> 24:40.080
 human brains are performing, where as part of the computation, there is like, you know, a global

24:40.080 --> 24:46.400
 workspace, a sophisticated attention mechanism, there is like self representations of other

24:46.400 --> 24:53.040
 cognitive processes, and a whole lot of other things that possibly would be conscious. And in

24:53.040 --> 25:00.480
 fact, if it's exactly like ours, I think definitely it would. But exactly how much less than the full

25:00.480 --> 25:06.000
 computation that the human brain is performing would be required is a little bit, I think,

25:06.000 --> 25:14.400
 of an open question. And you asked another interesting question as well, which is, would it be

25:14.400 --> 25:23.200
 sufficient to just have, say, the brain or would you need the environment in order to generate

25:23.200 --> 25:29.280
 the same kind of experiences that we have? And there is a bunch of stuff we don't know. I mean,

25:29.280 --> 25:36.160
 if you look at, say, current virtual reality environments, one thing that's clear is that

25:36.160 --> 25:42.560
 we don't have to simulate all details of them all the time in order for, say, the human player to

25:42.560 --> 25:47.280
 have the perception that there is a full reality in there. You can have, say, procedurally generated

25:47.280 --> 25:52.480
 virtual reality might only render a scene when it's actually within the view of the player character.

25:54.480 --> 26:05.280
 And so similarly, if this environment that we perceive is simulated, it might be that

26:05.280 --> 26:10.880
 all of the parts that come into our view are rendered at any given time. And a lot of aspects

26:10.880 --> 26:17.120
 that never come into view, say, the details of this microphone I'm talking into exactly what

26:17.120 --> 26:23.680
 each atom is doing at any given point in time might not be part of the simulation, only a more

26:23.680 --> 26:29.600
 coarse grained representation. So that to me is actually from an engineering perspective, why the

26:29.600 --> 26:37.040
 simulation hypothesis is really interesting to think about, is how much, how difficult is it to

26:37.040 --> 26:42.720
 fake sort of in a virtual reality context? I don't know, fake is the right word, but to construct

26:42.720 --> 26:51.120
 a reality that is sufficiently real to us to be immersive in the way that the physical world is,

26:51.120 --> 26:58.320
 I think that's actually probably an answerable question of psychology, of computer science,

26:58.320 --> 27:06.160
 of how, where's the line where it becomes so immersive that you don't want to leave that

27:06.160 --> 27:12.560
 world? Yeah, or that you don't realize while you're in it that it is a virtual world.

27:12.560 --> 27:17.920
 Yeah, those are two actually questions. Yours is the more sort of the good question about the realism.

27:17.920 --> 27:26.320
 But mine, from my perspective, what's interesting is it doesn't have to be real, but how can we

27:26.320 --> 27:31.840
 construct a world that we wouldn't want to leave? Yeah, I mean, I think that might be too low a bar.

27:31.840 --> 27:37.600
 I mean, if you think, say, when people first had Pong or something like that, I'm sure there were

27:37.600 --> 27:42.320
 people who wanted to keep playing it for a long time because it was fun and they wanted to be in

27:42.320 --> 27:48.000
 this little world. I'm not sure we would say it's immersive. I mean, I guess in some sense it is,

27:48.000 --> 27:52.400
 but like an absorbing activity doesn't even have to be. But they left that world though.

27:52.400 --> 28:04.720
 So I think that bar is deceivingly high. So you can play Pong or Starcraft or would have

28:04.720 --> 28:12.080
 more sophisticated games for hours, for months, while the work could be in a big addiction,

28:12.080 --> 28:17.680
 but eventually they escaped that. So I mean, when it's absorbing enough that you would spend your

28:17.680 --> 28:23.040
 entire, it would choose to spend your entire life in there. And then thereby changing the concept

28:23.040 --> 28:31.040
 of what reality is, because your reality becomes the game, not because you're fooled,

28:31.040 --> 28:37.760
 but because you've made that choice. Yeah, I mean, people might have different preferences

28:37.760 --> 28:42.960
 regarding that. Some might, even if you had any perfect virtual reality,

28:42.960 --> 28:50.720
 might still prefer not to spend the rest of their lives there. I mean, in philosophy,

28:50.720 --> 28:55.120
 there's this experience machine thought experiment. Have you come across this?

28:55.680 --> 29:01.360
 So Robert Nozick had this thought experiment where you imagine some crazy,

29:02.400 --> 29:07.520
 super duper neuroscientists of the future have created a machine that could give you any experience

29:07.520 --> 29:12.960
 you want if you step in there. And for the rest of your life, you can kind of pre programmed it

29:12.960 --> 29:22.800
 in different ways. So your fondest dreams could come true. You could whatever you dream, you want

29:22.800 --> 29:29.600
 to be a great artist, a great lover, like have a wonderful life, all of these things. If you step

29:29.600 --> 29:37.360
 into the experience machine will be your experiences constantly happy. But would you kind of disconnect

29:37.360 --> 29:44.080
 from the rest of reality and you would float there in a tank. And so Nozick thought that most

29:44.080 --> 29:50.400
 people would choose not to enter the experience machine. I mean, many might want to go there for

29:50.400 --> 29:55.200
 a holiday, but they wouldn't want us to check out of existence permanently. And so he thought that

29:55.200 --> 30:02.640
 was an argument against certain views of value, according to what we value is a function of what

30:02.640 --> 30:07.360
 we experience. Because in the experience machine, you could have any experience you want. And yet,

30:08.240 --> 30:14.800
 many people would think that would not be much value. So therefore, what we value depends on

30:14.800 --> 30:22.080
 other things than what we experience. So okay, can you take that argument further? What about

30:22.080 --> 30:26.480
 the fact that maybe what we value is the up and down of life. So you could have up and downs in

30:26.480 --> 30:31.680
 the experience machine, right? But what can't you have in the experience machine? Well, I mean,

30:31.680 --> 30:38.000
 I mean, that then becomes an interesting question to explore. But for example, real connection

30:38.000 --> 30:41.600
 with other people, if the experience machine is a solar machine, where it's only you,

30:42.800 --> 30:46.560
 like that's something you wouldn't have there, you would have this objective experience that

30:46.560 --> 30:52.800
 would be like fake people. But when if you gave somebody flowers, that wouldn't be anybody they

30:52.800 --> 30:58.560
 who actually got happy, it would just be a little simulation of somebody smiling. But the

30:58.560 --> 31:02.400
 simulation would not be the kind of simulation I'm talking about in the simulation argument where

31:02.400 --> 31:06.960
 the simulated creature is conscious, it would just be a kind of smiley face that would look

31:06.960 --> 31:13.520
 perfectly real to you. So we're now drawing a distinction between appear to be perfectly real

31:13.520 --> 31:20.320
 and actually being real. Yeah. So that could be one thing. I mean, like a big impact on history,

31:20.320 --> 31:26.000
 maybe it's also something you won't have if you check into this experience machine. So some people

31:26.000 --> 31:33.120
 might actually feel the life I want to have for me is one where I have a big positive impact on

31:34.080 --> 31:42.000
 history unfolds. So if you could kind of explore these different possible explanations for why it

31:42.000 --> 31:47.040
 is, you wouldn't want to go into the experience machine if that's if that's what you feel.

31:47.680 --> 31:53.280
 And one interesting observation regarding this nosic thought experiment and the conclusions

31:53.280 --> 31:59.680
 you wanted to draw from it is how much is a kind of status quo effect. So a lot of people might not

31:59.680 --> 32:09.840
 want to get this on current reality to plug into this dream machine. But if they instead were told,

32:09.840 --> 32:18.400
 well, what you've experienced up to this point was a dream. Now, do you want to disconnect from this

32:18.400 --> 32:22.960
 and enter the real world when you have no idea maybe what the real world is? Or maybe you could

32:22.960 --> 32:30.160
 say, well, you're actually a farmer in Peru growing peanuts and you could live for the rest of your

32:30.160 --> 32:39.360
 life in this. Or would you want to continue your dream life as Alex Friedman going around the

32:39.360 --> 32:47.600
 world, making podcasts and doing research? So if the status quo was that they were actually in

32:47.600 --> 32:53.040
 the experience machine, I think a lot of people might then prefer to live the life that they are

32:53.040 --> 32:59.040
 familiar with rather than sort of bail out into. So essentially the change itself, the leap,

32:59.040 --> 33:03.840
 whatever. Yeah, so it might not be so much the reality itself that we're after, but it's more

33:03.840 --> 33:09.840
 that we are maybe involved in certain projects and relationships. And we have, you know, a self

33:09.840 --> 33:14.480
 identity and these things that's our values are kind of connected with carrying that forward.

33:14.480 --> 33:21.520
 And then whether it's inside a tank or outside a tank in Peru or whether inside a computer,

33:21.520 --> 33:27.520
 outside a computer, that's kind of less important to what we ultimately care about.

33:27.520 --> 33:35.760
 Yeah, so just to linger on it, it is interesting. I find maybe people are different, but I find

33:35.760 --> 33:41.920
 myself quite willing to take the leap to the farmer in Peru, especially as the virtual

33:41.920 --> 33:48.480
 reality system become more realistic. I find that possibility and I think more people would take

33:48.480 --> 33:52.160
 that leap. But so in this thought experiment, just to make sure we are understanding, so in this

33:52.160 --> 33:58.160
 case, the farmer in Peru would not be a virtual reality. That would be the real. The real. The

33:58.160 --> 34:04.640
 real, your life, like before this whole experience machine started. Well, I kind of assumed from

34:04.640 --> 34:09.280
 that description, you're being very specific, but that kind of idea just like,

34:09.280 --> 34:16.000
 washes away the concept of what's real. I mean, I'm still a little hesitant about your kind of

34:16.000 --> 34:25.040
 distinction between real and illusion. Because when you can have an illusion that feels, I mean,

34:25.040 --> 34:31.520
 that looks real, I mean, what, I don't know how you can definitively say something is real or not.

34:31.520 --> 34:35.360
 Like what's a good way to prove that something is real in that context?

34:35.360 --> 34:40.240
 Well, so I guess in this case, it's more a definition. In one case, you're floating in a

34:40.240 --> 34:45.520
 tank with these wires by the super duper neuroscientists, plugging into your head,

34:46.480 --> 34:51.840
 giving you like Friedman experiences. In the other, you're actually tilling the soil in

34:51.840 --> 34:56.640
 Peru, growing peanuts, and then those peanuts are being eaten by other people all around the

34:56.640 --> 35:02.560
 world who buy the exports. And that's two different possible situations in the one and the

35:02.560 --> 35:07.040
 same real world that you could choose to occupy.

35:07.040 --> 35:11.520
 But just to be clear, when you're in a vat with wires and the neuroscientists,

35:12.240 --> 35:19.600
 you can still go farming in Peru, right? No, well, you could, if you wanted to,

35:19.600 --> 35:24.320
 you could have the experience of farming in Peru, but that wouldn't actually be any peanuts grown.

35:24.320 --> 35:36.560
 Well, but what makes a peanut? So a peanut could be grown and you could feed things with that

35:36.560 --> 35:41.520
 peanut. And why can't all of that be done in a simulation?

35:41.520 --> 35:45.920
 I hope, first of all, that they actually have peanut farms in Peru. I guess we'll get a lot

35:45.920 --> 35:52.640
 of comments out of us from angry. I was with you up until the point when you started talking about

35:52.640 --> 35:56.800
 you should know you can't realize you're a lot of these in that climate.

35:57.840 --> 36:05.120
 No, I mean, I think, I mean, I, in the simulation, I think there's a sense, the important sense in

36:05.120 --> 36:11.200
 which it would all be real. Nevertheless, there is a distinction between inside a simulation and

36:11.200 --> 36:17.120
 outside a simulation, or in the case of no six thought experiment, whether you're in the vat

36:17.120 --> 36:23.120
 or outside the vat. And some of those differences may or may not be important. I mean, that comes

36:23.120 --> 36:30.640
 down to your values and preferences. So if the, if the experience machine only gives you the

36:30.640 --> 36:35.440
 experience of growing peanuts, but you're the only one in the experience machines.

36:35.440 --> 36:40.400
 No, but there's other, you can, within the experience machine, others can plug in.

36:41.200 --> 36:45.440
 Well, there are versions of the experience machine. So in fact, you might want to have

36:45.440 --> 36:49.520
 distinguish different thought experiments, different versions of it. So in, like in the

36:49.520 --> 36:53.600
 original thought experiment, maybe it's only you, right? Just you. So, and you think, I wouldn't

36:53.600 --> 36:57.200
 want to go in there. Well, that tells you something interesting about what you value and what you

36:57.200 --> 37:02.080
 care about. Then you could say, well, what if you add the fact that there would be other people in

37:02.080 --> 37:05.360
 there and you would interact with them? Well, it starts to make it more attractive, right?

37:06.640 --> 37:10.800
 Then you could add in, well, what if you could also have important long term effects on human

37:10.800 --> 37:15.120
 history in the world and you could actually do something useful, even though you were in there,

37:15.120 --> 37:21.200
 that makes it maybe even more attractive. Like you could actually have a life that had a purpose

37:21.200 --> 37:30.240
 and consequences. So as you sort of add more into it, it becomes more similar to the baseline

37:30.240 --> 37:35.200
 reality that you were comparing it to. Yeah, but I just think inside the experience machine and

37:36.480 --> 37:43.040
 without taking those steps you just mentioned, you still have an impact on long term history

37:43.040 --> 37:50.480
 of the creatures that live inside that, of the quote unquote, fake creatures that live inside

37:50.480 --> 37:59.280
 that experience machine. And that, like at a certain point, if there's a person waiting for

37:59.280 --> 38:06.640
 you inside that experience machine, maybe you're newly found wife and she dies, she has fear,

38:06.640 --> 38:13.520
 she has hopes and she exists in that machine when you unplug yourself and plug back in,

38:13.520 --> 38:18.000
 she's still there, going on about her life. Well, in that case, yeah, she starts to have

38:18.000 --> 38:22.960
 more of an independent existence. Independent existence. But it depends, I think, on how she's

38:22.960 --> 38:31.120
 implemented in the experience machine. Take one limit case where all she is is a static picture

38:31.120 --> 38:38.480
 on the wall of photograph. So you think, well, I can look at her, but that's it. Then you think,

38:38.480 --> 38:42.800
 well, it doesn't really matter much what happens to that. Any more than a normal photograph,

38:42.800 --> 38:48.560
 if you tear it up, it means you can't see it anymore, but you haven't harmed the person

38:48.560 --> 38:56.320
 whose picture you tore it up. But if she's actually implemented, say, at a neural level

38:56.320 --> 39:04.000
 of detail, so that she's a fully realized digital mind with the same behavioral repertoire as you

39:04.000 --> 39:10.320
 have, then very possibly she would be a conscious person like you are. And then you would, what you

39:10.320 --> 39:15.760
 do in this experience machine would have real consequences for how this other mind felt.

39:17.440 --> 39:20.960
 So you have to specify which of these experience machines you're talking about.

39:20.960 --> 39:28.400
 I think it's not entirely obvious that it would be possible to have an experience machine that gave

39:28.400 --> 39:35.200
 you a normal set of human experiences, which include experiences of interacting with other people,

39:35.200 --> 39:41.760
 without that also generating consciousnesses corresponding to those other people. That is,

39:41.760 --> 39:48.320
 if you create another entity that you perceive and interact with, that to you looks entirely

39:48.320 --> 39:52.720
 realistic. Not just when you say hello, they say hello back, but you have a rich interaction

39:52.720 --> 39:59.760
 many days, deep conversations. Like it might be that the only possible way of implementing that

39:59.760 --> 40:04.400
 would be one that also as a side effect, instantiated this other person in enough detail

40:05.280 --> 40:11.520
 that you would have a second consciousness there. I think that's to some extent an open question.

40:11.520 --> 40:14.800
 So you don't think it's possible to fake consciousness and fake intelligence?

40:14.800 --> 40:19.680
 Well, it might be. I mean, I think you could certainly fake, if you have a very limited

40:19.680 --> 40:25.600
 interaction with somebody, you could certainly fake that. If all you have to go on is somebody

40:25.600 --> 40:30.320
 said hello to you, that's not enough for you to tell whether that was a real person there

40:30.320 --> 40:36.640
 or a prerecorded message or a very superficial simulation that has no consciousness,

40:37.760 --> 40:41.120
 because that's something easy to fake. We could already fake it. Now you can record a voice

40:41.120 --> 40:48.320
 recording. But if you have a richer set of interactions where you're allowed to ask open

40:48.320 --> 40:54.320
 ended questions and probe from different angles that you couldn't give canned answer to all of

40:54.320 --> 41:00.080
 the possible ways that you could probe it, then it starts to become more plausible that the only

41:00.080 --> 41:05.440
 way to realize this thing in such a way that you would get the right answer from any which angle

41:05.440 --> 41:10.560
 you probe it would be a way of instantiating it where you also instantiated a conscious mind.

41:10.560 --> 41:14.720
 Yeah, I'm with you on the intelligence part, but there's something about me that says consciousness

41:14.720 --> 41:20.880
 is easier to fake. I've recently gotten my hands on a lot of rubas. Don't ask me why or how.

41:22.800 --> 41:28.320
 And I've made them, this is just a nice robotic mobile platform for experiments,

41:28.320 --> 41:34.880
 and I made them scream and or moan and pain and so on just to see when they're responding to me.

41:34.880 --> 41:40.960
 And it's just a sort of psychological experiment on myself. And I think they appear conscious to me

41:40.960 --> 41:48.320
 pretty quickly. Like I, to me, at least my brain can be tricked quite easily. So if I introspect,

41:49.760 --> 41:55.120
 it's harder for me to be tricked that something is intelligent. So I just have this feeling that

41:55.120 --> 42:02.400
 inside this experience machine, just saying that you're conscious and having certain qualities

42:02.400 --> 42:08.880
 of the interaction like being able to suffer, like being able to hurt, like being able to wonder

42:08.880 --> 42:15.920
 about the essence of your own existence, not actually, I mean, the creating the illusion

42:15.920 --> 42:19.680
 that you're wondering about it is enough to create the feeling of consciousness and

42:21.520 --> 42:25.920
 the illusion of consciousness. And because of that, create a really immersive experience

42:25.920 --> 42:29.680
 to where you feel like that is the real world. So you think there's a big gap between

42:29.680 --> 42:35.120
 being appearing conscious and being conscious? Or is it that you think it's very easy to be

42:35.120 --> 42:40.400
 conscious? I'm not actually sure what it means to be conscious. All I'm saying is the illusion of

42:40.400 --> 42:49.120
 consciousness is enough for this to create a social interaction that's as good as if the

42:49.120 --> 42:54.240
 thing was conscious, meaning I'm making it about myself. Right. Yeah. I mean, I guess there are a

42:54.240 --> 42:57.840
 few difficulties. One is how good the interaction is, which might, I mean, if you don't really care

42:57.840 --> 43:04.960
 about probing hard for whether the thing is conscious, maybe it would be a satisfactory

43:04.960 --> 43:13.360
 interaction, whether or not you really thought it was conscious. Now, if you really care about it being

43:14.960 --> 43:22.880
 conscious inside this experience machine, how easy would it be to fake it? And you say

43:22.880 --> 43:28.400
 it sounds really easy. But then the question is, would that also mean it's very easy to

43:28.400 --> 43:34.400
 instantiate consciousness? It's much more widely spread in the world than we have thought. It

43:34.400 --> 43:39.440
 doesn't require a big human brain with 100 billion neurons. All you need is some system that exhibits

43:39.440 --> 43:44.160
 basic intentionality and can respond and you already have consciousness. In that case, I guess

43:44.160 --> 43:52.000
 you still have a close coupling. I guess a data case would be where they can come apart,

43:52.000 --> 43:57.520
 where you could create the appearance of there being a conscious mind without actually not being

43:57.520 --> 44:04.480
 another conscious mind. I'm somewhat agnostic exactly where these lines go. I think one

44:04.480 --> 44:10.640
 observation that makes it plausible that you could have very realistic appearances

44:12.080 --> 44:18.640
 relatively simply, which also is relevant for the simulation argument. And in terms of thinking

44:18.640 --> 44:25.040
 about how realistic would a virtual reality model have to be in order for the simulated

44:25.040 --> 44:32.240
 creature not to notice that anything was awry. Well, just think of our own humble brains during

44:32.240 --> 44:38.640
 the wee hours of the night when we are dreaming. Many times, well, dreams are very immersive,

44:38.640 --> 44:46.640
 but often you also don't realize that you're in a dream. And that's produced by simple primitive

44:46.640 --> 44:53.520
 three pound lumps of neural matter effortlessly. So if a simple brain like this can create the

44:53.520 --> 45:01.280
 virtual reality that seems pretty real to us, then how much easier would it be for a super

45:01.280 --> 45:06.800
 intelligent civilization with planetary sized computers optimized over the eons to create

45:06.800 --> 45:13.600
 a realistic environment for you to interact with? Yeah, by the way, behind that intuition

45:13.600 --> 45:20.000
 is that our brain is not that impressive relative to the possibilities of what technology could

45:20.000 --> 45:27.760
 bring. It's also possible that the brain is the epitome is the ceiling. Like just the ceiling.

45:28.640 --> 45:34.640
 How is that possible? Meaning like this is the smartest possible thing that the universe could

45:34.640 --> 45:43.120
 create. So that seems unlikely to me. Yeah, I mean, for some of these reasons we alluded to

45:43.120 --> 45:52.560
 earlier in terms of designs we already have for computers that would be faster by many orders

45:52.560 --> 45:58.480
 of magnitude than the human brain. Yeah, but it could be that the constraints, the cognitive

45:58.480 --> 46:04.640
 constraints in themselves is what enables the intelligence. So the more powerful you make the

46:04.640 --> 46:10.560
 computer, the less likely it is to become super intelligent. This is where I say dumb things

46:10.560 --> 46:16.480
 to push back. Yeah, I'm not sure. I mean, so there are different dimensions of intelligence.

46:17.920 --> 46:23.680
 A simple one is just speed. Like if you can solve the same challenge faster in some sense,

46:23.680 --> 46:30.480
 you're smarter. So there I think we have very strong evidence for thinking that you could have

46:30.480 --> 46:37.520
 a computer in this universe that would be much faster than the human brain and therefore have

46:37.520 --> 46:41.200
 speed superintelligence, like be completely superior, maybe a million times faster.

46:42.560 --> 46:46.960
 Then maybe there are other ways in which you could be smarter as well, maybe more qualitative

46:47.680 --> 46:54.080
 ways, right? And there the concepts are a little bit less clear cut. So it's harder to make a very

46:54.080 --> 47:01.760
 crisp, neat, firmly logical argument for why that could be qualitative superintelligence as

47:01.760 --> 47:04.960
 opposed to just things that were faster. Although I still think it's very plausible.

47:04.960 --> 47:10.880
 And for various reasons that are less than watertight arguments. But for example, if you look at

47:10.880 --> 47:19.200
 animals and even within humans, there seems to be Einstein versus random person. It's not just

47:19.200 --> 47:24.640
 that Einstein was a little bit faster. But how long would it take a normal person to invent

47:24.640 --> 47:30.640
 general relativity? It's not 20% longer than it took Einstein or something like that. I don't

47:30.640 --> 47:34.880
 know whether they would do it at all or it would take millions of years or some totally bizarre.

47:36.800 --> 47:42.640
 But your intuition is that the compute size will get you go. Increasing the size of the computer

47:43.280 --> 47:49.680
 and the speed of the computer might create some much more powerful levels of intelligence that

47:49.680 --> 47:54.560
 would enable some of the things we've been talking about with the simulation, being able to simulate

47:54.560 --> 48:01.200
 an ultra realistic environment, ultra realistic reception of reality.

48:01.200 --> 48:05.840
 Yeah. I mean, strictly speaking, it would not be necessary to have superintelligence in order to

48:05.840 --> 48:11.440
 have, say, the technology to make these simulations, ancestor simulations or other kinds of simulations.

48:14.000 --> 48:21.120
 As a matter of fact, I think if we are in a simulation, it would most likely be one built

48:21.120 --> 48:28.080
 by a civilization that had superintelligence. It certainly would help. I mean, it could build

48:28.080 --> 48:32.400
 more efficient, larger scale structures if you had superintelligence. I also think that if you had

48:32.400 --> 48:36.160
 the technology to build these simulations, that's like a very advanced technology. It seems kind

48:36.160 --> 48:42.720
 of easier to get the technology to superintelligence. So I'd expect by the time they could make these

48:42.720 --> 48:47.760
 fully realistic simulations of human history with human brains in there, before that, they

48:47.760 --> 48:53.200
 got to that stage that would have figured out how to create machine superintelligence or maybe

48:54.000 --> 48:57.920
 biological enhancements of their own brains if they were biological creatures to start with.

48:58.960 --> 49:05.520
 So we talked about the three parts of the simulation argument. One, we destroy ourselves

49:05.520 --> 49:11.440
 before we ever create the simulation. Two, we somehow, everybody somehow loses interest in

49:11.440 --> 49:19.040
 creating simulation. Three, we're living in a simulation. So you've kind of, I don't know if

49:19.040 --> 49:24.880
 your thinking has evolved on this point, but you kind of said that we know so little that these

49:24.880 --> 49:31.120
 three cases might as well be equally probable. So probabilistically speaking, where do you stand

49:31.120 --> 49:40.240
 on this? Yeah, I mean, I don't think equal necessarily would be the most supported probability

49:40.240 --> 49:46.480
 assignment. So how would you, without assigning actual numbers, what's more or less likely in

49:46.480 --> 49:54.320
 your view? Well, I mean, I've historically tended to punt on the question of like as between these

49:54.320 --> 50:01.440
 three. So maybe you asked me another way is which kind of things would make each of these more or

50:01.440 --> 50:07.920
 less likely? What kind of, yeah. I mean, certainly in general terms, if you think anything that say

50:07.920 --> 50:15.760
 increases or reduces the probability of one of these would tend to slash probability around

50:16.320 --> 50:20.400
 on the other. So if one becomes less probable, like the other would have to, because it's going to

50:20.400 --> 50:27.040
 add up to one. Yes. So if we consider the first hypothesis, the first alternative that there's

50:27.040 --> 50:35.040
 this filter that makes it so that virtually no civilization reaches technical maturity.

50:35.040 --> 50:42.160
 In particular, our own civilization, if that's true, then it's like very unlikely that we would

50:42.160 --> 50:47.280
 reach technical maturity, because if almost no civilization at our stage does it, then

50:47.280 --> 50:51.360
 it's unlikely that we do it. So I'm sorry, can you longer on that for a second? Well,

50:51.360 --> 50:56.160
 so if it's the case that almost all civilizations at our current stage of technical maturity fail,

50:57.440 --> 51:01.120
 at our current stage of technical development, fail to reach maturity,

51:01.120 --> 51:07.520
 that would give us very strong reason for thinking we will fail to reach technical maturity.

51:07.520 --> 51:12.240
 Oh, and also sort of the flip side of that is the fact that we've reached it means that many

51:12.240 --> 51:15.760
 other civilizations have reached this point. Yeah. So that means if we get closer and closer to

51:15.760 --> 51:22.480
 actually reaching technical maturity, there's less and less distance left where we could

51:22.480 --> 51:29.200
 go extinct before we are there. And therefore, the probability that we will reach increases

51:29.200 --> 51:34.880
 as we get closer. And that would make it less likely to be true that almost all civilizations

51:34.880 --> 51:40.000
 at our current stage failed to get there. Like we would have this, the one case we had started

51:40.000 --> 51:44.480
 ourselves would be very close to getting there. That would be strong evidence is not so hard to

51:44.480 --> 51:51.040
 get to technical maturity. So to the extent that we feel we are moving nearer to technical

51:51.040 --> 51:57.760
 maturity, that would tend to reduce the probability of the first alternative and increase the probability

51:57.760 --> 52:04.640
 of the other to it doesn't need to be a monotonic change. Like if every once in a while, some new

52:04.640 --> 52:10.560
 threat comes into view, some bad new thing you could do with some novel technology, for example,

52:10.560 --> 52:14.320
 you know, that that could change our probabilities in the other direction.

52:14.880 --> 52:20.800
 But that that technology again, you have to think about as that technology has to be able to

52:20.800 --> 52:26.160
 equally in an even way affect every civilization out there.

52:26.160 --> 52:31.600
 Yeah, pretty much. I mean, that's strictly speaking, it's not true. I mean, I could be two

52:31.600 --> 52:39.920
 different existential risk and every civilization, you know, one or the other, like, but none of them

52:39.920 --> 52:48.000
 kills more than 50%. Like, yeah, but I incidentally, so in some of my the work, I mean, on machine

52:48.000 --> 52:54.080
 superintelligence, like some existential risks related to sort of superintellent AI and how we

52:54.080 --> 53:02.400
 must make sure, you know, to handle that wisely and carefully. It's not the right kind of existential

53:03.040 --> 53:10.960
 catastrophe to make the first alternative true, though, like it might be bad for us.

53:12.000 --> 53:18.400
 If the future lost a lot of value as a result of it being shaped by some process that optimized for

53:18.400 --> 53:25.680
 some completely non human value. But even if we got killed by machine superintelligence is that

53:26.320 --> 53:29.360
 machine superintelligence might still attain technological maturity.

53:30.080 --> 53:35.360
 So I see. So you're not very, you're not human exclusive. This could be any intelligent species

53:35.360 --> 53:40.240
 that achieves like it's all about the technological maturity. It's not that the humans have to

53:42.080 --> 53:43.040
 attain it. Right.

53:43.040 --> 53:47.040
 So like superintelligence because it replaced us. And that's just as well for the simulation

53:47.040 --> 53:52.640
 argument. Yeah, I mean, it could interact with the second by alternative. Like if the thing that

53:52.640 --> 53:58.320
 replaced us was either more likely or less likely, then we would be to have an interest in creating

53:58.320 --> 54:03.920
 ancestor simulations, you know, that that could affect probabilities. But yeah, to a first order,

54:05.600 --> 54:12.160
 like if we all just die, then yeah, we won't produce any simulations because we are dead. But if we

54:12.160 --> 54:17.120
 all die and get replaced by some other intelligent thing that then gets to technological maturity,

54:17.120 --> 54:22.720
 the question remains, of course, if my not that thing, then use some of its resources to do this

54:22.720 --> 54:28.320
 stuff. So can you reason about this stuff? This is given how little we know about the universe.

54:29.280 --> 54:38.880
 Is it reasonable to reason about these probabilities? So like how little, well,

54:38.880 --> 54:46.080
 well, maybe you can disagree. But to me, it's not trivial to figure out how difficult it is to

54:46.080 --> 54:52.960
 build a simulation. We kind of talked about it a little bit. We've also don't know, like as we

54:52.960 --> 54:57.840
 try to start building it, like start creating virtual worlds and so on, how that changes the

54:57.840 --> 55:03.440
 fabric of society. Like there's all these things along the way that can fundamentally change just

55:03.440 --> 55:09.600
 so many aspects of our society about our existence that we don't know anything about. Like the kind

55:09.600 --> 55:19.120
 of things we might discover when we understand to a greater degree the fundamental, the physics,

55:19.120 --> 55:23.600
 like the theory, if we have a breakthrough, have a theory and everything, how that changes,

55:23.600 --> 55:30.640
 how that changes deep space exploration and so on. So like, is it still possible to reason about

55:30.640 --> 55:37.920
 probabilities given how little we know? Yes, I think though, there will be a large residual of

55:37.920 --> 55:45.680
 uncertainty that we'll just have to acknowledge. And I think that's true for most of these big

55:45.680 --> 55:54.400
 picture questions that we might wonder about. It's just we are small, short lived, small brained,

55:54.400 --> 56:01.680
 cognitively very limited humans with little evidence and it's amazing we can figure out as

56:01.680 --> 56:09.680
 much as we can really about the cosmos. But okay, so there's this cognitive trick that seems to happen

56:09.680 --> 56:15.120
 when I look at the simulation argument, which for me, it seems like case one and two feel

56:15.120 --> 56:22.000
 unlikely. I want to say feel unlikely as opposed to sort of like, it's not like I have too much

56:22.000 --> 56:29.040
 scientific evidence to say that either one or two are not true. It just seems unlikely that every

56:29.040 --> 56:36.480
 single civilization destroys itself. And it seems like feels unlikely that the civilizations lose

56:36.480 --> 56:44.160
 interest. So naturally, without necessarily explicitly doing it, but the simulation argument

56:44.160 --> 56:51.520
 basically says it's very likely we're living in a simulation. Like to me, my mind naturally goes

56:51.520 --> 56:56.720
 there. I think the mind goes there for a lot of people. Is that the incorrect place for it to go?

56:57.600 --> 57:06.560
 Well, not necessarily. I think the second alternative which has to do with the motivations

57:06.560 --> 57:14.000
 and interests of technological and mature civilizations. I think there is much we don't

57:14.000 --> 57:19.280
 understand about that. Can you talk about that a little bit? What do you think? I mean, this

57:19.280 --> 57:24.160
 question that pops up when you build an AGI system or build a general intelligence or

57:26.080 --> 57:30.240
 how does that change your motivations? Do you think it will fundamentally transform our motivations?

57:31.360 --> 57:39.680
 Well, it doesn't seem that implausible that once you take this leap to technological maturity.

57:39.680 --> 57:45.680
 I mean, I think it involves creating machine superintelligence possibly that would be sort of

57:45.680 --> 57:51.440
 on the path for basically all civilizations, maybe before they are able to create large

57:51.440 --> 57:58.400
 numbers of ancestor simulations. That possibly could be one of these things that quite radically

57:58.400 --> 58:06.800
 changes the orientation of what a civilization is, in fact, optimizing for. There are other

58:06.800 --> 58:20.000
 things as well. At the moment, we have not perfect control over our own being, our own mental states,

58:20.000 --> 58:29.600
 our own experiences are not under our direct control. For example, if you want to experience

58:29.600 --> 58:37.280
 a pleasure and happiness, you might have to do a whole host of things in the external world to

58:37.280 --> 58:43.040
 try to get into the stage, into the mental state where you experience pleasure. Like when people

58:43.040 --> 58:47.520
 get some pleasure from eating great food, well, they can't just turn that on. They have to kind

58:47.520 --> 58:52.240
 of actually go to a nice restaurant and then they have to make money. So there's like all this kind

58:52.240 --> 59:01.840
 of activity that maybe arises from the fact that we are trying to ultimately produce mental states,

59:01.840 --> 59:06.800
 but the only way to do that is by a whole host of complicated activities in the external world.

59:06.800 --> 59:12.000
 Now, at some level of technological development, I think we'll become auto potent in the sense of

59:12.000 --> 59:19.440
 gaining direct ability to choose our own internal configuration and enough knowledge and insight

59:19.440 --> 59:24.800
 to be able to actually do that in a meaningful way. So then it could turn out that there are a lot

59:24.800 --> 59:31.040
 of instrumental goals that would drop out of the picture and be replaced by other instrumental

59:31.040 --> 59:37.680
 goals because we could now serve some of these final goals in more direct ways. And who knows how

59:37.680 --> 59:45.920
 all of that shakes out after civilizations reflect on that and converge and different

59:45.920 --> 59:54.960
 attractors and so on and so forth. And that could be new instrumental considerations that come into

59:54.960 --> 1:00:03.440
 view as well that we are just oblivious to that would maybe have a strong shaping effect on actions,

1:00:03.440 --> 1:00:07.680
 like very strong reasons to do something or not to do something. And we just don't realize

1:00:07.680 --> 1:00:13.120
 they are there because we are so dumb, bumbling through the universe. But if almost inevitably

1:00:13.120 --> 1:00:18.720
 on route to attaining the ability to create many answers to simulations, you do have this

1:00:18.720 --> 1:00:24.160
 cognitive enhancement or advice from superintelligence or you yourself, then maybe there's like this

1:00:24.160 --> 1:00:28.560
 additional set of considerations coming into view. And yesterday I asked, it's obvious that the thing

1:00:28.560 --> 1:00:33.520
 that makes sense is to do X. Whereas right now it seems you could X, Y or Z and different people

1:00:33.520 --> 1:00:41.040
 will do different things and we are kind of random in that sense. Yeah, because at this time,

1:00:41.040 --> 1:00:46.000
 with our limited technology, the impact of our decisions is minor. I mean, that's starting

1:00:46.000 --> 1:00:53.600
 to change in some ways. Well, I'm not sure how it follows that the impact of our decisions is minor.

1:00:54.240 --> 1:00:59.600
 Well, it's starting to change. I mean, I suppose 100 years ago was minor. It's starting to...

1:01:00.480 --> 1:01:07.440
 Well, it depends on how you view it. So what people did 100 years ago still have effects on

1:01:07.440 --> 1:01:16.160
 the world today. Oh, as I see, as a civilization in the togetherness. Yeah. So it might be that

1:01:16.880 --> 1:01:23.040
 the greatest impact of individuals is not at technological maturity or very far down. It might

1:01:23.040 --> 1:01:28.480
 be earlier on when there are different tracks, civilization could go down. I mean, maybe the

1:01:28.480 --> 1:01:37.920
 population is smaller, things still haven't settled out. If you count indirect effects, that

1:01:37.920 --> 1:01:44.160
 those could be bigger than the direct effects that people have later on. So part three of the

1:01:44.160 --> 1:01:51.520
 argument says that, so that leads us to a place where eventually somebody creates a simulation.

1:01:53.520 --> 1:01:57.680
 I think you had a conversation with Joe Rogan. I think there's some aspect here where you get

1:01:57.680 --> 1:02:06.880
 stuck a little bit. How does that lead to we're likely living in a simulation? So this kind of

1:02:08.080 --> 1:02:13.440
 probability argument, if somebody eventually creates a simulation, why does that mean that

1:02:13.440 --> 1:02:20.240
 we're now in a simulation? What you get to if you accept alternative three first is there would be

1:02:20.240 --> 1:02:30.400
 more simulated people with our kinds of experiences than non simulated ones. If you look at the world

1:02:30.400 --> 1:02:37.680
 as a whole, by the end of time, as it were, you just count it up, that would be more simulated

1:02:37.680 --> 1:02:43.760
 ones than non simulated ones. Then there is an extra step to get from that. If you assume that,

1:02:43.760 --> 1:02:51.280
 suppose for the sake of the argument that that's true, how do you get from that to the statement

1:02:51.280 --> 1:03:00.240
 we are probably in a simulation? So here you're introducing an indexical statement like it's

1:03:01.840 --> 1:03:09.280
 that this person right now is in a simulation. There are all these other people that are in

1:03:09.280 --> 1:03:15.680
 simulations and some that are not in the simulation. But what probability should you have that you

1:03:15.680 --> 1:03:23.280
 yourself is one of the simulated ones in the setup. So I call it the bland principle of

1:03:23.280 --> 1:03:30.720
 indifference, which is that in cases like this, when you have two sets of observers,

1:03:30.720 --> 1:03:38.880
 one of which is much larger than the other, and you can't from any internal evidence you have,

1:03:39.840 --> 1:03:48.320
 tell which set you belong to. You should assign a probability that's proportional to the size

1:03:49.200 --> 1:03:55.040
 of these sets so that if there are 10 times more simulated people with your kinds of experiences,

1:03:55.040 --> 1:04:00.480
 you would be 10 times more likely to be one of those. Is that as intuitive as it sounds?

1:04:00.480 --> 1:04:06.480
 I mean, that seems kind of, if you don't have enough information, you should rationally just

1:04:06.480 --> 1:04:15.600
 assign the same probability as the size of the set. It seems pretty plausible to me.

1:04:15.600 --> 1:04:22.320
 Where are the holes in this? Is it at the very beginning, the assumption that everything stretches

1:04:22.320 --> 1:04:26.720
 sort of you have infinite time essentially? You don't need infinite time.

1:04:26.720 --> 1:04:32.960
 You just need how long does the time you take? However long it takes, I guess, for a universe

1:04:32.960 --> 1:04:37.680
 to produce an intelligent civilization that then attains the technology to run some

1:04:37.680 --> 1:04:44.320
 ancestry simulations. Got you. When the first simulation is created, that stretch of time

1:04:44.320 --> 1:04:49.040
 just a little longer than they'll all start creating simulations, kind of like order of

1:04:49.040 --> 1:04:54.320
 matters. Well, I mean, it might be different. If you think of there being a lot of different

1:04:54.320 --> 1:05:00.160
 planets and some subset of them have life and then some subset of those get intelligent life

1:05:00.800 --> 1:05:06.240
 and some of those maybe eventually start creating simulations, they might get started at quite

1:05:06.240 --> 1:05:10.960
 different times. Like maybe on some planet, it takes a billion years longer before you get

1:05:11.840 --> 1:05:20.240
 like monkeys or before you get even bacteria than on another planet. So this might happen

1:05:20.240 --> 1:05:26.800
 when kind of at different cosmological epochs. Is there a connection here to the doomsday

1:05:26.800 --> 1:05:32.480
 argument and that sampling there? Yeah, there is a connection in that they both

1:05:33.360 --> 1:05:39.040
 involve an application of anthropic reasoning that is reasoning about these kind of indexical

1:05:40.240 --> 1:05:45.520
 propositions. But the assumption you need in the case of the simulation argument

1:05:45.520 --> 1:05:53.520
 is much weaker than the assumption you need to make the doomsday argument go through.

1:05:53.520 --> 1:05:58.160
 What is the doomsday argument? And maybe you can speak to the anthropic reasoning in more

1:05:58.160 --> 1:06:03.200
 general. Yeah, that's a big and interesting topic in its own right, anthropics. But the

1:06:03.200 --> 1:06:11.120
 doomsday argument is this really first discovered by Brandon Carter, who was a theoretical physicist

1:06:11.120 --> 1:06:18.160
 and then developed by philosopher John Leslie. I think it might have been discovered initially

1:06:18.160 --> 1:06:24.240
 in the 70s or 80s. And Leslie wrote this book, I think in 96. And there are some other versions

1:06:24.240 --> 1:06:29.680
 as well by Richard Gott, he's a physicist, but let's focus on the Carter Leslie version where

1:06:32.320 --> 1:06:39.520
 it's an argument that we have systematically underestimated the probability that

1:06:39.520 --> 1:06:46.000
 he might not be able to go extinct soon. Now, I should say most people probably

1:06:46.960 --> 1:06:50.960
 think at the end of the day, there is something wrong with this doomsday argument that it doesn't

1:06:50.960 --> 1:06:56.080
 really hold. It's like there's something wrong with it. But it's proved hard to say exactly what

1:06:56.080 --> 1:07:02.560
 is wrong with it. And different people have different accounts. My own view is it seems

1:07:02.560 --> 1:07:08.880
 inconclusive. But I can say what the argument is. Yeah, that would be good. Yeah, so maybe it's

1:07:08.880 --> 1:07:21.280
 easy to explain via an analogy to sampling from urns. So imagine you have a big, imagine you

1:07:21.280 --> 1:07:27.440
 have two urns in front of you, and they have balls in them that have numbers. The two urns look

1:07:27.440 --> 1:07:32.080
 the same, but inside one, there are 10 balls, ball number one, two, three up to ball number 10.

1:07:32.080 --> 1:07:41.600
 And then in the other urn, you have a million balls numbered one to a million. And now somebody

1:07:41.600 --> 1:07:48.720
 puts one of these urns in front of you and asks you to guess what's the chance it's the 10 ball

1:07:48.720 --> 1:07:54.240
 urn. And you say, well, 50, 50, I can't tell which urn it is. But then you're allowed to

1:07:54.240 --> 1:07:59.600
 reach in and pick a ball at random from the urn. And that's suppose you find that it's ball number

1:07:59.600 --> 1:08:07.440
 seven. So that's strong evidence for the 10 ball hypothesis. Like, it's a lot more likely that

1:08:07.440 --> 1:08:12.480
 you would get such a low numbered ball, if there are only 10 balls in the urn, like it's in fact

1:08:12.480 --> 1:08:18.000
 10% done, right? Then if there are a million balls, it would be very unlikely you would get number

1:08:18.000 --> 1:08:26.800
 seven. So you perform a Bayesian update. And if your prior was 50, 50, that it was the 10 ball

1:08:26.800 --> 1:08:31.920
 urn, you become virtually certain after finding the random sample was seven that it only has 10

1:08:31.920 --> 1:08:36.560
 balls in it. So in the case of the urns, this is uncontroversial, just elementary probability

1:08:36.560 --> 1:08:42.160
 theory. The Doomsday argument says that you should reason in a similar way with respect to

1:08:42.800 --> 1:08:49.280
 different hypotheses about how many balls there will be in the urn of humanity, I said,

1:08:49.280 --> 1:08:55.760
 for how many humans there will ever be by the time we go extinct. So to simplify, let's suppose we

1:08:55.760 --> 1:09:03.520
 only consider two hypotheses, either maybe 200 billion humans in total, or 200 trillion humans

1:09:03.520 --> 1:09:09.360
 in total. You could fill in more hypotheses, but it doesn't change the principle here. So it's

1:09:09.360 --> 1:09:14.080
 easiest to see if we just consider these two. So you start with some prior based on ordinary,

1:09:14.080 --> 1:09:20.880
 empirical ideas about threats to civilization and so forth. And maybe you say it's a 5% chance that

1:09:20.880 --> 1:09:26.400
 we will go extinct. By the time there will have been 200 billion only, you're kind of optimistic,

1:09:26.400 --> 1:09:33.600
 let's say, I think probably we'll make it through colonized universe. But then, according to this

1:09:33.600 --> 1:09:41.120
 Doomsday argument, you should take off your own birth rank as a random sample. So your birth rank

1:09:41.120 --> 1:09:48.320
 is your sequence in the position of all humans that have ever existed. And it turns out you're

1:09:48.320 --> 1:09:53.920
 about a human number of 100 billion, you know, give or take. That's like roughly how many people

1:09:53.920 --> 1:09:58.480
 have been born before you. That's fascinating, because I probably, we each have a number.

1:09:59.520 --> 1:10:04.080
 We would each have a number in this. I mean, obviously, the exact number would depend on

1:10:04.080 --> 1:10:09.840
 where you started counting, like which ancestors was human enough to count as human. But those

1:10:09.840 --> 1:10:16.320
 are not really important. They're relatively few. So yeah, so you're roughly 100 billion. Now,

1:10:16.320 --> 1:10:21.120
 if they're only going to be 200 billion in total, that's a perfectly unremarkable number. You're

1:10:21.120 --> 1:10:27.680
 somewhere in the middle, right? Run of the mill human, completely unsurprising. Now, if they're

1:10:27.680 --> 1:10:34.640
 going to be 200 trillion, you would be remarkably early. Like, what are the chances out of these

1:10:34.640 --> 1:10:41.520
 200 trillion human that you should be human number 100 billion? That seems it would have

1:10:41.520 --> 1:10:48.880
 a much lower conditional probability. And so analogously to how in the urn case, you thought

1:10:48.880 --> 1:10:54.480
 after finding this low number random sample, you updated in favor of the urn having few balls.

1:10:54.480 --> 1:11:00.720
 Similarly, in this case, you should update in favor of the human species having a lower total

1:11:00.720 --> 1:11:08.960
 number of members that is doomed soon. You said doomed soon. That's the hypothesis in this case

1:11:08.960 --> 1:11:14.080
 that it will end 100 billion. I just like that term for the hypothesis. Yeah.

1:11:14.080 --> 1:11:20.480
 So what it kind of crucially relies on, the doomed argument, is the idea that you should reason

1:11:21.520 --> 1:11:26.000
 as if you were a random sample from the set of all humans that will ever have existed.

1:11:27.200 --> 1:11:31.440
 If you have that assumption, then I think the rest kind of follows. The question then is,

1:11:31.440 --> 1:11:37.280
 why should you make that assumption? In fact, you know you're 100 billion, so where do you get this

1:11:37.280 --> 1:11:43.280
 prior? And then there is like a literature on that with different ways of supporting that assumption.

1:11:44.960 --> 1:11:50.000
 There's just one example of anthropocrysine, right? That seems to be kind of convenient when you

1:11:50.000 --> 1:11:56.240
 think about humanity, when you think about sort of even like existential threats and so on,

1:11:56.880 --> 1:12:02.480
 as it seems that quite naturally that you should assume that you're just an average case.

1:12:02.480 --> 1:12:09.520
 Yeah, that you're kind of a typical randomly sample. Now in the case of the doomed argument,

1:12:09.520 --> 1:12:14.320
 it seems to lead to what intuitively we think is the wrong conclusion. Or at least many people

1:12:14.320 --> 1:12:19.680
 have this reaction that there's got to be something fishy about this argument. Because from very,

1:12:19.680 --> 1:12:27.040
 very weak premises, it gets this very striking implication that we have almost no chance of

1:12:27.040 --> 1:12:33.920
 reaching size 200 trillion humans in the future. And how can we possibly get there just by reflecting

1:12:33.920 --> 1:12:38.640
 on when we were born? It seems you would need sophisticated arguments about the impossibility

1:12:38.640 --> 1:12:43.840
 of space colonization, blah, blah. So one might be tempted to reject this key assumption. I call

1:12:43.840 --> 1:12:47.920
 it the self sampling assumption. The idea that you should reason as if you're a random sample from

1:12:47.920 --> 1:12:56.480
 all observers or in your some reference class. However, it turns out that in other domains,

1:12:56.480 --> 1:13:01.120
 it looks like we need something like this self sampling assumption to make sense of

1:13:02.080 --> 1:13:07.760
 bona fide scientific inferences in contemporary cosmology, for example, we have these multiverse

1:13:07.760 --> 1:13:15.040
 theories. And according to a lot of those, all possible human observations are made. I mean,

1:13:15.040 --> 1:13:19.120
 if you have a sufficiently large universe, you will have a lot of people observing all kinds

1:13:19.120 --> 1:13:26.240
 of different things. So if you have two competing theories, say about the value of some constant,

1:13:29.040 --> 1:13:33.600
 it could be true, according to both of these theories, that there will be some observers

1:13:34.400 --> 1:13:42.080
 observing the value that corresponds to the other theory, because there will be some observers that

1:13:42.080 --> 1:13:47.360
 have hallucinations. So there's a local fluctuation or a statistically anomalous measurement,

1:13:47.360 --> 1:13:52.160
 these things will happen. And if enough observers make enough different observations,

1:13:52.160 --> 1:13:57.360
 there will be some that sort of by chance make these different ones. And so what we would want to say

1:13:57.360 --> 1:14:06.160
 is, well, many more observers, a larger proportion of the observers will observe as it were the true

1:14:06.160 --> 1:14:12.560
 value. And a few will observe the wrong value. If we think of ourselves as a random sample,

1:14:12.560 --> 1:14:18.160
 we should expect with our own probability to observe the true value and that will then allow us

1:14:18.160 --> 1:14:24.480
 to conclude that the evidence we actually have is evidence for the theories we think are supported.

1:14:24.480 --> 1:14:31.840
 It kind of done is a way of making sense of these inferences that clearly seem correct,

1:14:31.840 --> 1:14:38.960
 that we can make various observations and infer what the temperature of the cosmic background is

1:14:38.960 --> 1:14:46.640
 and the fine structure constant and all of this. But it seems that without rolling in some assumption

1:14:46.640 --> 1:14:52.000
 similar to the self sampling assumption, this inference doesn't go through. And there are

1:14:52.000 --> 1:14:55.680
 other examples. So there are these scientific contexts where it looks like this kind of

1:14:55.680 --> 1:15:00.880
 anthropic reasoning is needed and makes perfect sense. And yet, in the case of the Dubster argument,

1:15:00.880 --> 1:15:04.320
 it has this weird consequence and people might think there's something wrong with it there.

1:15:04.320 --> 1:15:14.480
 So there's then this project that would consistent try to figure out what are the legitimate ways

1:15:14.480 --> 1:15:20.560
 of reasoning about these indexical facts when observer selection effects are in play. In other

1:15:20.560 --> 1:15:25.760
 words, developing a theory of anthropics. And there are different views of looking at that.

1:15:25.760 --> 1:15:33.200
 And it's a difficult methodological area. But to tie it back to the simulation argument,

1:15:33.200 --> 1:15:40.560
 the key assumption there, this bland principle of indifference, is much weaker than the self

1:15:40.560 --> 1:15:47.760
 sampling assumption. So if you think about in the case of the Dubster argument, it says you

1:15:47.760 --> 1:15:51.760
 should reason as if you are a random sample from all humans that would have lived, even though in

1:15:51.760 --> 1:15:59.520
 fact you know that you are about number 100 billionth human and you're alive in the year 2020,

1:15:59.520 --> 1:16:04.400
 whereas in the case of the simulation argument, it says that, well, if you actually have no way

1:16:04.400 --> 1:16:09.920
 of telling which one you are, then you should assign this kind of uniform probability.

1:16:11.040 --> 1:16:14.800
 Yeah, your role as the observer in the simulation argument is different, it seems like.

1:16:15.840 --> 1:16:19.440
 Who's the observer? I keep assigning the individual consciousness.

1:16:20.960 --> 1:16:25.360
 Well, a lot of observers in the simulation, in the context of the simulation argument,

1:16:25.360 --> 1:16:32.240
 the relevant observers would be A, the people in original histories and B, the people in simulations.

1:16:33.200 --> 1:16:37.280
 So this would be the class of observers that we need. I mean, there are also maybe the simulators,

1:16:37.280 --> 1:16:42.720
 but we can set those aside for this. So the question is, given that class of observers,

1:16:43.920 --> 1:16:48.400
 a small set of original history observers and the large class of simulated observers,

1:16:48.400 --> 1:16:53.440
 which one should you think is you? Where are you amongst this set of observers?

1:16:53.440 --> 1:16:59.600
 I'm maybe having a little bit of trouble wrapping my head around the intricacies of

1:16:59.600 --> 1:17:08.240
 what it means to be an observer in the different instantiations of the anthropic reasoning cases

1:17:08.240 --> 1:17:14.640
 that we mentioned. It's like the observer. No, I mean, it may be an easier way of putting it,

1:17:14.640 --> 1:17:19.680
 it's just like, are you simulated or are you not simulated, given this assumption that these

1:17:19.680 --> 1:17:23.600
 two groups of people exist? Yeah, in the simulation case, it seems pretty straightforward.

1:17:24.480 --> 1:17:31.200
 Yeah, so the key point is the methodological assumption you need to make to get the simulation

1:17:31.200 --> 1:17:38.400
 argument to where it wants to go is much weaker and less problematic than the methodological

1:17:38.400 --> 1:17:43.280
 assumption you need to make to get the doomsday argument to its conclusion. Maybe the doomsday

1:17:43.280 --> 1:17:49.920
 argument is sound or unsound, but you need to make a much stronger and more controversial assumption

1:17:49.920 --> 1:17:54.880
 to make it go through. In the case of the simulation argument, I guess one

1:17:56.080 --> 1:18:00.720
 maybe way intuition popped to support this bland principle of indifference,

1:18:00.720 --> 1:18:07.600
 is to consider a sequence of different cases where the fraction of people who are simulated

1:18:07.600 --> 1:18:16.800
 to non simulated approaches one. In the limiting case where everybody is simulated,

1:18:18.560 --> 1:18:25.440
 obviously you can deduce with certainty that you are simulated. If everybody

1:18:26.640 --> 1:18:30.160
 with your experiences is simulated and you know you've got to be one of those,

1:18:30.720 --> 1:18:33.520
 you don't need a probability at all, you just kind of logically

1:18:33.520 --> 1:18:43.360
 conclude it, right? So then as we move from a case where say 90% of everybody is simulated,

1:18:44.960 --> 1:18:52.960
 99.9%, it should seem possible that the probability assigned should sort of approach

1:18:52.960 --> 1:19:00.480
 one, certainty, as the fraction approaches the case where everybody is in the simulation.

1:19:00.480 --> 1:19:06.480
 And so you wouldn't expect that to be a discrete. Well, if there's one non simulated person,

1:19:06.480 --> 1:19:13.200
 then it's 50, 50, but if we move that, then it's 100%. There are other

1:19:13.200 --> 1:19:16.880
 arguments as well one can use to support this bland principle of indifference, but

1:19:18.000 --> 1:19:23.840
 that might be nice too. But in general, when you start from time equals zero and go into the future,

1:19:24.640 --> 1:19:28.960
 the fraction of simulated, if it's possible to create simulated worlds,

1:19:28.960 --> 1:19:35.120
 the fraction simulated worlds will go to one. Well, I mean, is that an obvious kind of thing?

1:19:35.120 --> 1:19:40.240
 Well, it won't probably go all the way to one. In reality, there would be some

1:19:40.240 --> 1:19:44.640
 ratio, although maybe a technological mature civilization could run a lot of

1:19:46.000 --> 1:19:52.240
 simulations using a small portion of its resources. It probably wouldn't be able to

1:19:52.240 --> 1:19:58.560
 run infinitely many. I mean, if we take say the observed, the physics in the observed universe,

1:19:58.560 --> 1:20:04.880
 if we assume that that's also the physics at the level of the simulators, that would be limits

1:20:04.880 --> 1:20:11.520
 to the amount of information processing that anyone civilization could perform in its future

1:20:11.520 --> 1:20:18.560
 trajectory. Right. Well, first of all, there's limited amount of matter you can get your hands

1:20:18.560 --> 1:20:23.760
 off because with the positive cosmological constant, the universe is accelerating,

1:20:24.320 --> 1:20:28.000
 there's like a finite sphere of stuff, even if you travel with the speed of light that you

1:20:28.000 --> 1:20:34.320
 could ever reach to have a finite amount of stuff. And then if you think there is like a lower limit

1:20:34.320 --> 1:20:40.640
 to the amount of loss you get when you perform an erasure of a computation, or if you think,

1:20:40.640 --> 1:20:46.960
 for example, just matter gradually over cosmological time scales, decay, maybe protons, decay,

1:20:46.960 --> 1:20:52.640
 other things, you radiate out gravitational waves, like there's all kinds of seemingly

1:20:52.640 --> 1:21:01.600
 unavoidable losses that occur. So eventually, we'll have something like a heat death of the

1:21:01.600 --> 1:21:07.920
 universe or a close death or whatever. So it's fine. But of course, we don't know which, if there's

1:21:07.920 --> 1:21:14.080
 many ancestral simulations, we don't know which level we are. So there could be,

1:21:15.600 --> 1:21:20.960
 couldn't there be like an arbitrary number of simulations that spawned ours? And those had

1:21:20.960 --> 1:21:27.680
 more resources in terms of physical universe to work with? Sorry, what do you mean that that could

1:21:27.680 --> 1:21:41.840
 be? Okay, so if simulations spawn other simulations, it seems like each new spawn has fewer resources

1:21:41.840 --> 1:21:49.920
 to work with. But we don't know at which step along the way we are at,

1:21:49.920 --> 1:21:58.320
 any one observer doesn't know whether we're in level 42, or 100, or one, or is that not

1:21:58.320 --> 1:22:05.760
 matter for the resources? I mean, it's true that there would be uncertainty asked,

1:22:05.760 --> 1:22:12.560
 you could have stacked simulations. Yes. And that could then be uncertainty as to which level we

1:22:12.560 --> 1:22:23.840
 are at. As you remarked also, all the computations performed in a simulation within a simulation

1:22:24.640 --> 1:22:30.640
 also have to be expanded at the level of the simulation. So the computer in basement reality

1:22:30.640 --> 1:22:34.240
 where all these simulations with the simulations with the simulations are taking place, like that

1:22:34.240 --> 1:22:40.000
 computer ultimately, it's CPU or whatever it is, like that has to power this whole tower, right? So

1:22:40.000 --> 1:22:46.320
 if there is a finite compute power in basement reality, that would impose a limit to how tall

1:22:46.320 --> 1:22:53.520
 this tower can be. And if each level kind of imposes a large extra overhead, you might think

1:22:53.520 --> 1:22:59.120
 maybe the tower would not be very tall, that most people would be low down in the tower.

1:23:00.560 --> 1:23:07.520
 I love the term basement reality. Let me ask one of the popularizers, you said there's many

1:23:07.520 --> 1:23:13.280
 through this, when you look at sort of the last few years of the simulation hypothesis,

1:23:13.280 --> 1:23:17.680
 just like you said, it comes up every once in a while, some new community discovers it and so on.

1:23:17.680 --> 1:23:23.280
 But I would say one of the biggest popularizers of this idea is Elon Musk. Do you have any kind

1:23:23.280 --> 1:23:28.640
 of intuition about what Elon thinks about when he thinks about simulation? Why is this of such

1:23:28.640 --> 1:23:33.920
 interest? Is it all the things we've talked about, or is there some special kind of intuition about

1:23:33.920 --> 1:23:39.120
 simulation that he has? I mean, you might have a better, I think, I mean, why it's of interest,

1:23:39.120 --> 1:23:44.960
 I think it seems pretty obvious why to the extent that one think the argument is credible,

1:23:44.960 --> 1:23:49.840
 why it would be of interest. If it's correct, tell us something very important about the world,

1:23:49.840 --> 1:23:54.960
 in one way or the other, whichever of the three alternatives for a simulation that seems arguably

1:23:54.960 --> 1:24:00.080
 one of the most fundamental discoveries, right? Now, interestingly, in the case of someone like

1:24:00.080 --> 1:24:04.640
 Elon, so there's like the standard arguments for why you might want to take the simulation hypothesis

1:24:04.640 --> 1:24:09.360
 seriously, the simulation argument, right? In the case that if you are actually Elon Musk, let

1:24:09.360 --> 1:24:15.520
 us say, there's a kind of an additional reason in that what are the chances you would be Elon Musk?

1:24:16.720 --> 1:24:24.240
 Like, it seems like maybe there would be more interest in simulating the lives of very unusual

1:24:24.240 --> 1:24:31.600
 and remarkable people. So, if you consider not just assimilations where all of human history

1:24:31.600 --> 1:24:36.400
 or the whole of human civilization are simulated, but also other kinds of simulations which only

1:24:37.200 --> 1:24:43.600
 include some subset of people. Like, in those simulations that only include a subset,

1:24:43.600 --> 1:24:48.160
 it might be more likely that they would include subsets of people with unusually interesting

1:24:48.160 --> 1:24:52.400
 or consequential lives. If you're Elon Musk, you got to wonder, right? It's more likely that

1:24:52.400 --> 1:24:58.960
 if you are Donald Trump or if you are Bill Gates or you're like some particularly

1:25:00.320 --> 1:25:05.440
 like distinctive character, you might think that that add, I mean, if you just think of yourself

1:25:06.160 --> 1:25:10.560
 into the shoes, right? It's got to be like an extra reason to think. That's kind of...

1:25:11.200 --> 1:25:18.320
 So interesting. So, on a scale of like farmer and Peru to Elon Musk, the more you get towards

1:25:18.320 --> 1:25:23.200
 the Elon Musk, the higher the probability... You'd imagine there would be some extra boost from that.

1:25:25.040 --> 1:25:30.800
 There's an extra boost. So, he also asked the question of what he would ask an AGI saying,

1:25:30.800 --> 1:25:37.600
 the question being what's outside the simulation. Do you think about the answer to this question

1:25:37.600 --> 1:25:44.320
 if we are living a simulation? What is outside the simulation? So, the programmer of the simulation?

1:25:44.320 --> 1:25:48.960
 Yeah. I mean, I think it connects to the question of what's inside the simulation in that

1:25:50.320 --> 1:25:54.880
 if you had views about the creators of the simulation, it might help you

1:25:56.240 --> 1:26:02.720
 make predictions about what kind of simulation it is, what might happen, what happens after

1:26:02.720 --> 1:26:07.680
 the simulation if there is some after, but also like the kind of setup. So, these two questions

1:26:07.680 --> 1:26:16.000
 would be quite closely intertwined. But do you think it would be very surprising to... Is the

1:26:16.000 --> 1:26:20.240
 stuff inside the simulation, is it possible for it to be fundamentally different than the stuff

1:26:20.240 --> 1:26:28.880
 outside? Yeah. Another way to put it, can the creatures inside the simulation be smart enough

1:26:28.880 --> 1:26:33.840
 to even understand or have the cognitive capabilities or any kind of information processing

1:26:33.840 --> 1:26:41.920
 capabilities enough to understand the mechanism that created them? They might understand some

1:26:41.920 --> 1:26:51.120
 aspects of it. I mean, it's a level of explanation, like degrees to which you can understand. So,

1:26:51.120 --> 1:26:56.160
 does your dog understand what it is to be human? Well, it's got some idea, like humans are these

1:26:56.160 --> 1:27:03.520
 physical objects that move around and do things. And a normal human would have a deeper understanding

1:27:03.520 --> 1:27:11.040
 of what it is to be human. And maybe some very experienced psychologists or great novelists

1:27:11.040 --> 1:27:15.840
 might understand a little bit more about what it is to be human. And maybe superintelligence

1:27:15.840 --> 1:27:25.360
 could see right through your soul. So, similarly, I do think that we are quite limited in our

1:27:25.360 --> 1:27:30.960
 ability to understand all of the relevant aspects of the larger context that we exist in.

1:27:30.960 --> 1:27:36.000
 But there might be hope for some. I think we understand some aspects of it. But

1:27:36.880 --> 1:27:43.360
 how much good is that? If there's one key aspect that changes the significance of all

1:27:43.360 --> 1:27:49.280
 the other aspects. So, we understand maybe seven out of ten key insights that you need.

1:27:51.280 --> 1:27:57.840
 But the answer actually varies completely, depending on what number eight, nine, and

1:27:57.840 --> 1:28:06.640
 ten insight is. It's like whether you want to... Suppose that the big task were to guess whether

1:28:06.640 --> 1:28:14.160
 a certain number was odd or even, like a ten digit number. And if it's even, the best thing

1:28:14.160 --> 1:28:18.400
 for you to do in life is to go north. And if it's odd, the best thing for you to go south.

1:28:20.880 --> 1:28:25.120
 Now, we are in a situation where maybe through our science and philosophy, we figured out what

1:28:25.120 --> 1:28:29.520
 the first seven digits are. So, we have a lot of information, right? Most of it we figured out.

1:28:30.720 --> 1:28:36.400
 But we are clueless about what the last three digits are. So, we are still completely clueless

1:28:36.400 --> 1:28:41.040
 about whether the number is odd or even, and therefore whether we should go north or go south.

1:28:41.040 --> 1:28:46.960
 I feel that's an analogy, but I feel we're somewhat in that predicament. We know a lot about the

1:28:46.960 --> 1:28:52.400
 universe. We've come maybe more than half of the way there to kind of fully understanding it,

1:28:52.400 --> 1:28:57.520
 but the parts we are missing are possibly ones that could completely change the overall

1:28:58.480 --> 1:29:04.320
 upshot of the thing. And including change our overall view about what the scheme of

1:29:04.320 --> 1:29:07.680
 priorities should be or which strategic direction would make sense to pursue.

1:29:07.680 --> 1:29:15.200
 Yeah, I think your analogy of us being the dog, trying to understand human beings is an entertaining

1:29:15.200 --> 1:29:22.800
 one and probably correct. The closer the understanding tends from the dog's viewpoint to us human

1:29:22.800 --> 1:29:28.640
 psychologist's viewpoint, the steps along the way there will have completely transformative ideas

1:29:28.640 --> 1:29:34.880
 of what it means to be human. So, the dog has a very shallow understanding. It's interesting to

1:29:34.880 --> 1:29:41.440
 think that to analogize that a dog's understanding of a human being is the same as our current

1:29:41.440 --> 1:29:49.600
 understanding of the fundamental laws of physics in the universe. Oh, man. Okay, we spent an hour

1:29:49.600 --> 1:29:54.640
 or 40 minutes talking about the simulation. I like it. Let's talk about superintelligence,

1:29:54.640 --> 1:30:00.160
 at least for a little bit. And let's start at the basics. What to you is intelligence?

1:30:01.440 --> 1:30:08.080
 Yeah, I tend not to get too stuck with the definitional question. I mean, the common sense

1:30:08.080 --> 1:30:13.280
 understand, like the ability to solve complex problems, to learn from experience, to plan,

1:30:13.280 --> 1:30:21.200
 to reason, some combination of things like that. Is consciousness mixed up into that or no? Is

1:30:21.200 --> 1:30:26.000
 consciousness mixed up into that? Well, I don't think, I think it could be fairly intelligent,

1:30:26.000 --> 1:30:33.520
 at least without being conscious, probably. So, then what is superintelligence? Yeah,

1:30:33.520 --> 1:30:40.080
 that would be like something that was much more of that, had much more general cognitive capacity

1:30:40.080 --> 1:30:47.920
 than we humans have. So, if we talk about general superintelligence, it would be much faster learner

1:30:47.920 --> 1:30:52.960
 be able to reason much better, make plans that are more effective at achieving its goals,

1:30:52.960 --> 1:30:59.200
 say in a wide range of complex, challenging environments. In terms of, as we turn our eye

1:30:59.200 --> 1:31:05.520
 to the idea of existential threats from superintelligence, do you think superintelligence

1:31:05.520 --> 1:31:13.520
 has to exist in the physical world or can it be digital only? We think of our general intelligence

1:31:13.520 --> 1:31:19.680
 as us humans, as an intelligence that's associated with the body that's able to interact with the

1:31:19.680 --> 1:31:25.440
 world, that's able to affect the world directly with physically. I mean, digital only is perfectly

1:31:25.440 --> 1:31:31.440
 fine, I think. I mean, it's physical in the sense that obviously the computers and the memories are

1:31:31.440 --> 1:31:36.800
 physical. But it's capable to affect the world, sort of? Could be very strong, even if it has a

1:31:36.800 --> 1:31:44.000
 limited set of actuators. If it can type text on the screen or something like that, that would be,

1:31:44.000 --> 1:31:52.480
 I think, ample. So, in terms of the concerns of existential threat of AI, how can an AI system

1:31:52.480 --> 1:32:01.600
 that's in the digital world have existential risk? What are the attack vectors for a digital system?

1:32:01.600 --> 1:32:08.080
 Well, I mean, I guess maybe to take one step back. I should emphasize that I also think there's this

1:32:08.080 --> 1:32:15.760
 huge positive potential from machine intelligence, including superintelligence. I want to stress that

1:32:15.760 --> 1:32:22.000
 because some of my writing has focused on what can go wrong. When I wrote the book,

1:32:22.000 --> 1:32:30.560
 Superintelligence, at that point, I felt that there was a kind of neglect of what would happen

1:32:30.560 --> 1:32:36.000
 if AI succeeds. And in particular, a need to get a more granular understanding of where the pitfalls

1:32:36.000 --> 1:32:44.480
 are so we can avoid them. I think that since the book came out in 2014, there has been a much

1:32:44.480 --> 1:32:48.880
 wider recognition of that. And a number of research groups are now actually working on

1:32:48.880 --> 1:32:56.160
 developing, say, AI alignment techniques and so on and so forth. So, yeah, I think now it's

1:32:56.160 --> 1:33:02.400
 important to make sure we bring back onto the table the upside as well.

1:33:02.400 --> 1:33:07.200
 And there's a little bit of a neglect now on the upside, which is, I mean, if you look at,

1:33:08.000 --> 1:33:11.520
 talking to a friend, if you look at the amount of information that is available,

1:33:11.520 --> 1:33:16.160
 or people talking, or people being excited about the positive possibilities of general

1:33:16.160 --> 1:33:24.000
 intelligence, that's not, it's far outnumbered by the negative possibilities in terms of our

1:33:24.000 --> 1:33:30.480
 public discourse. Possibly, yeah, it's hard to measure. What are, can you look at that for a

1:33:30.480 --> 1:33:36.800
 little bit? What are some, to you, possible big positive impacts of general intelligence,

1:33:37.360 --> 1:33:40.320
 superintelligence? Well, I mean, super, because I tend to

1:33:40.320 --> 1:33:46.640
 also want to distinguish these two different contexts of thinking about AI and AI impacts,

1:33:47.520 --> 1:33:53.040
 the kind of near term and long term, if you want, both of which I think are legitimate things to

1:33:53.040 --> 1:33:59.360
 think about. And people should, you know, discuss both of them, but they are different and they

1:33:59.360 --> 1:34:06.240
 often get mixed up. And then, then I get, you get confusion. I think you get simultaneously,

1:34:06.240 --> 1:34:10.960
 like maybe an overhyping of the near term and an underhyping of the long term. And so I think as

1:34:10.960 --> 1:34:16.960
 long as we keep them apart, we can have like two good conversations, but, or we can mix them

1:34:16.960 --> 1:34:21.440
 together and have one bad conversation. Can you clarify just the two things we're talking about,

1:34:21.440 --> 1:34:26.640
 the near term and the long term? Yeah, and what are the distinctions? Well, it's a, it's a blur

1:34:26.640 --> 1:34:32.720
 distinction. But say the things I wrote about in this book, superintelligence, long term,

1:34:32.720 --> 1:34:40.560
 things people are worrying about today with, I don't know, algorithmic discrimination or even

1:34:40.560 --> 1:34:47.760
 things self driving cars and drones and stuff, more near term. And then, of course,

1:34:48.400 --> 1:34:52.880
 you could imagine some medium term where they kind of overlap and one evolves into the other.

1:34:54.880 --> 1:35:00.080
 But anyway, I think both, yeah, the issues look kind of somewhat different depending on

1:35:00.080 --> 1:35:05.280
 which of these contexts. So I think it would be nice if we can talk about the long term

1:35:06.560 --> 1:35:16.080
 and think about a positive impact or a better world because of the existence of the long

1:35:16.080 --> 1:35:21.200
 term superintelligence. Do you have views of such a world? Yeah, I mean, I guess it's a little

1:35:21.200 --> 1:35:27.040
 hard to articulate because it seems obvious that the world has a lot of problems as it currently

1:35:27.040 --> 1:35:34.880
 stands. And it's hard to think of any one of those which it wouldn't be useful to have a

1:35:34.880 --> 1:35:45.120
 like a friendly aligned superintelligence working on. So from health to the economic system to be

1:35:45.120 --> 1:35:50.800
 able to sort of improve the investment and trade and foreign policy decisions, all that kind of

1:35:50.800 --> 1:35:58.800
 stuff. All that kind of stuff and a lot more. I mean, what's the killer app? Well, I don't think

1:35:58.800 --> 1:36:05.520
 there is one. I think AI, especially artificial general intelligence is really the ultimate

1:36:05.520 --> 1:36:10.720
 general purpose technology. So it's not that there is this one problem, this one area where it will

1:36:10.720 --> 1:36:18.320
 have a big impact. But if and when it succeeds, it will really apply across the board in all fields

1:36:18.320 --> 1:36:23.280
 where human creativity and intelligence and problem solving is useful, which is pretty much

1:36:23.280 --> 1:36:29.760
 all fields, right? The thing that it would do is give us a lot more control over nature.

1:36:30.640 --> 1:36:34.960
 It wouldn't automatically solve the problems that arise from conflict between humans,

1:36:36.560 --> 1:36:40.160
 fundamentally political problems. Some subset of those might go away if you just had more

1:36:40.160 --> 1:36:49.600
 resources and cooler tech, but some subset would require coordination that is not automatically

1:36:49.600 --> 1:36:55.040
 achieved just by having more technical capability. But anything that's not of that sort, I think

1:36:55.040 --> 1:37:02.320
 you just get like an enormous boost with this kind of cognitive technology once it goes all

1:37:02.320 --> 1:37:10.400
 the way. Now, again, that doesn't mean I'm like thinking, oh, people don't recognize what's possible

1:37:10.400 --> 1:37:14.960
 with current technology. And like sometimes things get overhyped. But I mean, those are

1:37:14.960 --> 1:37:20.960
 perfectly consistent views to hold the ultimate potential being enormous. And then it's a very

1:37:20.960 --> 1:37:25.200
 different question of how far are we from that? Or what can we do with near term technology?

1:37:25.200 --> 1:37:29.680
 Yeah. So what's your intuition about the idea of intelligence explosion? So there's this,

1:37:29.680 --> 1:37:36.080
 you know, when you start to think about that leap from the near term to the long term,

1:37:36.080 --> 1:37:40.960
 the natural inclination, like for me, sort of building machine learning systems today,

1:37:40.960 --> 1:37:45.840
 it seems like it's a lot of work to get the general intelligence. But there's some intuition

1:37:45.840 --> 1:37:51.200
 of exponential growth of exponential improvement of intelligence explosion. Can you maybe

1:37:51.200 --> 1:38:01.120
 try to elucidate, to try to talk about what's your intuition about the possibility of

1:38:01.120 --> 1:38:06.880
 intelligence explosion, that it won't be this gradual slow process that might be a phase shift?

1:38:08.720 --> 1:38:15.200
 Yeah, I think it's, we don't know how explosive it will be. I think for what it's worth,

1:38:15.200 --> 1:38:20.560
 it seems fairly likely to me that at some point there will be some intelligence

1:38:20.560 --> 1:38:25.440
 explosion, like some period of time where progress in AI becomes extremely rapid.

1:38:26.800 --> 1:38:33.680
 Roughly in the area where you might say it's kind of humanish equivalent in

1:38:34.320 --> 1:38:41.040
 core cognitive faculties, that the concept of human equivalent starts to break down when

1:38:41.040 --> 1:38:46.320
 you look too closely at it. And just how explosive does something have to be for it to

1:38:47.280 --> 1:38:50.800
 be called an intelligence explosion? Like does it have to be like overnight literally,

1:38:50.800 --> 1:38:59.440
 or a few years? But overall, I guess if you plotted the opinions of different people in

1:38:59.440 --> 1:39:04.640
 the world, I guess I would be somewhat more probability towards the intelligence explosion

1:39:04.640 --> 1:39:09.360
 scenario than probably the average AI researcher, I guess.

1:39:09.360 --> 1:39:15.120
 So, and then the other part of the intelligence explosion, or just forget explosion, just progress,

1:39:15.840 --> 1:39:22.000
 is once you achieve that gray area of human level intelligence, is it obvious to you that

1:39:22.000 --> 1:39:26.960
 we should be able to proceed beyond it to get to super intelligence?

1:39:26.960 --> 1:39:34.080
 Yeah, that seems, I mean, as much as any of these things can be obvious, given we've never had one,

1:39:34.800 --> 1:39:38.080
 people have different views, smart people have different views, it's like some

1:39:38.080 --> 1:39:45.360
 some degree of uncertainty that always remains for any big futuristic philosophical grand question

1:39:45.920 --> 1:39:49.360
 that just we realize humans are fallible, especially about these things.

1:39:49.360 --> 1:39:55.680
 But it does seem as far as I'm judging things based on my own impressions that it seems very

1:39:55.680 --> 1:40:03.680
 unlikely that there would be a ceiling at or near human cognitive capacity.

1:40:03.680 --> 1:40:10.240
 But that's such a, I don't know, that's such a special moment. It says both terrifying and

1:40:10.240 --> 1:40:17.680
 exciting to create a system that's beyond our intelligence. So maybe you can step back and

1:40:17.680 --> 1:40:23.200
 say, like, how does that possibility make you feel that we can create something,

1:40:24.400 --> 1:40:30.960
 it feels like there's a line beyond which it steps, it'll be able to all smart you.

1:40:30.960 --> 1:40:34.720
 And therefore, it feels like a step where we lose control.

1:40:35.360 --> 1:40:43.440
 Well, I don't think the latter follows, that is, you could imagine, and in fact, this is what

1:40:43.440 --> 1:40:47.040
 a number of people are working towards, making sure that we could ultimately

1:40:48.080 --> 1:40:54.640
 project higher levels of problem solving ability while still making sure that they are aligned,

1:40:54.640 --> 1:41:02.480
 like they are in the service of human values. I mean, so losing control, I think,

1:41:02.480 --> 1:41:08.640
 is not a given that that would happen. I asked how it makes me feel. I mean, to some extent,

1:41:08.640 --> 1:41:16.320
 I've lived with this for so long, since as long as I can remember being an adult or even a teenager,

1:41:16.320 --> 1:41:22.160
 it seemed to me obvious that at some point, AI will succeed. And so I actually misspoke,

1:41:22.160 --> 1:41:27.840
 I didn't mean control. I meant, because the control problem isn't an interesting thing.

1:41:27.840 --> 1:41:33.840
 And I think the hope is, at least we should be able to maintain control over systems that are

1:41:33.840 --> 1:41:45.680
 smarter than us. But we do lose our specialness. It sort of will lose our place as the smartest,

1:41:45.680 --> 1:41:53.200
 coolest thing on earth. And there's an ego involved with that, that humans are very good at

1:41:54.240 --> 1:42:01.760
 dealing with. I mean, I value my intelligence as a human being. It seems like a big transformative

1:42:01.760 --> 1:42:06.800
 step to realize there's something out there that's more intelligent. I mean, you don't see that

1:42:08.240 --> 1:42:13.280
 as such a fundamentally... Well, yeah, I think, yes, a lot. I think it would be small. I mean,

1:42:13.280 --> 1:42:18.000
 I think there are already a lot of things out there that are... I mean, certainly if you think

1:42:18.000 --> 1:42:22.160
 the universe is big, there's going to be other civilizations that already have super intelligences

1:42:22.720 --> 1:42:29.520
 or that just naturally have brains the size of beach balls and are completely leaving us in the

1:42:29.520 --> 1:42:35.200
 dust. And we haven't come face to face with this. We haven't come face to face. But I mean, that's

1:42:35.200 --> 1:42:44.720
 an open question. What would happen in a kind of post human world? Like how much day to day would

1:42:45.760 --> 1:42:51.840
 these super intelligences be involved in the lives of ordinary... I mean, you could imagine

1:42:51.840 --> 1:42:55.680
 some scenario where it would be more like a background thing that would help protect against

1:42:55.680 --> 1:43:02.240
 some things. But you wouldn't... They wouldn't be this intrusive kind of making you feel bad by

1:43:02.240 --> 1:43:06.480
 like making clever jokes on your experience. But there's like all sorts of things that maybe in

1:43:06.480 --> 1:43:12.000
 the human context would feel awkward about that. You wouldn't want to be the dumbest kid in your

1:43:12.000 --> 1:43:17.840
 class, everybody picks it. Like a lot of those things maybe you need to abstract away from

1:43:17.840 --> 1:43:21.520
 if you're thinking about this context where we have infrastructure that is in some sense

1:43:22.800 --> 1:43:29.280
 beyond any of our old humans. I mean, it's a little bit like, say, the scientific community

1:43:29.280 --> 1:43:33.680
 as a whole. If you think of that as a mind, it's a little bit of metaphor. But I mean,

1:43:33.680 --> 1:43:40.640
 obviously it's got to be like way more capacious than any individual. So in some sense, there is this

1:43:41.600 --> 1:43:49.440
 mind like thing already out there. That's just vastly more intelligent than any individual is.

1:43:49.440 --> 1:43:56.720
 And we think, okay, that's... You just accept that as a fact. That's the basic fabric of our

1:43:56.720 --> 1:44:00.640
 existence. There's a super intelligent thing. Yeah, you get used to a lot of... I mean,

1:44:00.640 --> 1:44:07.600
 there's already Google and Twitter and Facebook, these recommender systems that are the basic

1:44:07.600 --> 1:44:14.800
 fabric of our... I could see them becoming... I mean, do you think of the collective intelligence

1:44:14.800 --> 1:44:20.880
 of these systems as already perhaps reaching superintelligence level? Well, I mean, so here

1:44:20.880 --> 1:44:26.800
 it comes to the concept of intelligence and the scale and what human level means.

1:44:29.200 --> 1:44:33.200
 The kind of vagueness and the determinacy of those concepts starts to

1:44:36.560 --> 1:44:42.240
 dominate how you would answer that question. So, say the Google search engine has a very high

1:44:42.240 --> 1:44:47.600
 capacity of a certain kind, like retrieving... Remembering and retrieving information,

1:44:47.600 --> 1:44:58.000
 particularly like text or images that are... You have a kind of string, a word string key.

1:44:58.800 --> 1:45:06.000
 Obviously, superhuman at that. But a vast set of other things it can't even do at all,

1:45:06.000 --> 1:45:13.520
 not just not do well. So you have these current AI systems that are superhuman in some limited

1:45:13.520 --> 1:45:20.000
 domain and then radically subhuman in all other domains. Same with a chess...

1:45:21.440 --> 1:45:25.360
 A simple computer that can multiply really large numbers, right? So it's going to have this one

1:45:25.360 --> 1:45:31.520
 spike of superintelligence and then a kind of zero level of capability across all other cognitive

1:45:31.520 --> 1:45:36.640
 fields. Yeah, I don't necessarily think the generalness... I mean, I'm not so attached with it,

1:45:36.640 --> 1:45:46.880
 but it's a gray area and it's a feeling, but to me, alpha zero is somehow much more intelligent,

1:45:47.440 --> 1:45:53.440
 much, much more intelligent than D blue. And to say which domain... Well, you could say, well,

1:45:53.440 --> 1:45:57.440
 these are both just board games. They're both just able to play board games. Who cares if they're

1:45:57.440 --> 1:46:03.200
 going to do better or not? But there's something about the learning and the self play that makes

1:46:03.200 --> 1:46:09.520
 it crosses over into that land of intelligence that doesn't necessarily need to be general.

1:46:09.520 --> 1:46:15.600
 And the same way Google is much closer to D blue currently in terms of its search engine than it

1:46:15.600 --> 1:46:21.120
 is to sort of the alpha zero. And the moment it becomes... And the moment these recommender systems

1:46:21.120 --> 1:46:27.920
 really become more like alpha zero, but being able to learn a lot without the constraints of

1:46:27.920 --> 1:46:33.760
 being heavily constrained by human interaction, that seems like a special moment in time.

1:46:34.320 --> 1:46:42.800
 I mean, certainly learning ability seems to be an important facet of general intelligence,

1:46:42.800 --> 1:46:48.160
 that you can take some new domain that you haven't seen before and you weren't specifically

1:46:48.160 --> 1:46:53.280
 preprogrammed for and then figure out what's going on there and eventually become really good at it.

1:46:53.280 --> 1:47:01.760
 So that's something alpha zero has much more of than D blue had. And in fact, I mean, systems

1:47:01.760 --> 1:47:08.800
 like alpha zero can learn not just go, but other, in fact, probably beat D blue in chess and so

1:47:08.800 --> 1:47:15.120
 forth. So you do see this general and it matches the intuition. We feel it's more intelligent.

1:47:15.120 --> 1:47:20.800
 And it also has more of this general purpose learning ability. And if we get systems that

1:47:20.800 --> 1:47:24.640
 have even more general purpose learning ability, it might also trigger an even stronger intuition

1:47:24.640 --> 1:47:30.480
 that they're actually starting to get smart. So if you were to pick a future, what do you think a

1:47:30.480 --> 1:47:39.520
 utopia looks like with AGI systems? Is it the neural link brain computer interface world where

1:47:39.520 --> 1:47:46.640
 we're kind of really closely interlinked with AI systems? Is it possibly where AGI systems replace

1:47:46.640 --> 1:47:54.320
 us completely while maintaining the values and the consciousness? Is it something like it's a

1:47:54.320 --> 1:48:00.000
 completely invisible fabric? Like you mentioned a society where just aids and a lot of stuff that

1:48:00.000 --> 1:48:05.360
 we do like curing diseases and so on. What is utopia if you get to pick? Yeah, I mean, it is a good

1:48:05.360 --> 1:48:12.960
 question and a deep and difficult one. I'm quite interested in it. I don't have all the answers

1:48:12.960 --> 1:48:20.160
 yet, but might never have. But I think there are some different observations one can make. One is if

1:48:20.160 --> 1:48:28.640
 this scenario actually did come to pass, it would open up this vast space of possible modes of

1:48:28.640 --> 1:48:36.080
 being. On one hand, material and resource constraints would just be expanded dramatically.

1:48:36.080 --> 1:48:44.480
 So there would be a lot of a big pie, let's say. Also, it would enable us to do things

1:48:47.360 --> 1:48:53.200
 including to ourselves or like that, it would just open up this much larger design space

1:48:53.200 --> 1:49:00.560
 and option space than we have ever had access to in human history. So I think two things follow

1:49:00.560 --> 1:49:06.960
 from that. One is that we probably would need to make a fairly fundamental rethink of what

1:49:07.600 --> 1:49:12.640
 ultimately we value. Think things through more from first principles. The context would be so

1:49:12.640 --> 1:49:16.640
 different from the familiar that we could have just take what we've always been doing and then

1:49:17.920 --> 1:49:23.760
 oh, well, we have this cleaning robot that cleans the dishes in the sink and a few other small

1:49:23.760 --> 1:49:28.560
 things. I think we would have to go back to first principles. So even from the individual

1:49:28.560 --> 1:49:32.800
 level, go back to the first principles of what is the meaning of life, what is happiness,

1:49:32.800 --> 1:49:40.400
 what is fulfillment. And then also connected to this large space of resources is that it

1:49:41.920 --> 1:49:51.360
 would be possible and I think something we should aim for is to do well by the lights

1:49:51.360 --> 1:50:05.760
 of more than one value system. That is, we wouldn't have to choose only one value criterion and say

1:50:05.760 --> 1:50:13.920
 we're going to do something that scores really high on the metric of, say, hedonism. And then it's

1:50:13.920 --> 1:50:21.760
 like a zero by other criteria, like kind of wireheaded brains in a vat. And it's like a lot

1:50:21.760 --> 1:50:28.800
 of pleasure. That's good. But then like no beauty, no achievement like that. I think to some

1:50:28.800 --> 1:50:33.680
 significant, not unlimited sense, but the significant sense, it would be possible to do

1:50:34.640 --> 1:50:42.400
 very well by many criteria. Like maybe you could get like 98% of the best according to several

1:50:42.400 --> 1:50:50.960
 criteria at the same time, given this great expansion of the option space. And so have

1:50:50.960 --> 1:50:59.440
 competing value systems, competing criteria as sort of forever, just like our Democrat versus

1:50:59.440 --> 1:51:04.720
 Republican, there seems to be this always multiple parties that are useful for our progress in society,

1:51:05.520 --> 1:51:11.040
 even though it might seem dysfunctional inside the moment, but having the multiple value systems

1:51:11.040 --> 1:51:18.960
 seems to be beneficial for, I guess, a balance of power. So that's not exactly what I have in mind,

1:51:18.960 --> 1:51:27.440
 that it's, well, although it may be in an indirect way, it is. But that if you had the chance to do

1:51:27.440 --> 1:51:34.480
 something that scored well on several different metrics, our first instinct should be to do that

1:51:34.480 --> 1:51:40.000
 rather than immediately leap to the thing, which ones of these value systems are we going to screw

1:51:40.000 --> 1:51:46.560
 over? Let's first try to do very well by all of them. Then it might be that you can't get 100%

1:51:46.560 --> 1:51:51.600
 of all, and you would have to then have the hard conversation about which one will only get 97%.

1:51:51.600 --> 1:51:56.480
 There you go. There's my cynicism that all of existence is always a tradeoff. But you say,

1:51:57.120 --> 1:52:02.160
 maybe it's not such a bad tradeoff. Let's first try that. Well, this would be a distinctive context

1:52:02.160 --> 1:52:09.920
 in which at least some of the constraints would be removed. There's probably still be tradeoffs in

1:52:09.920 --> 1:52:15.440
 the end. It's just that we should first make sure we at least take advantage of this abundance.

1:52:17.120 --> 1:52:25.760
 In terms of thinking about this, one should think in this kind of frame of mind of generosity

1:52:25.760 --> 1:52:33.120
 and inclusiveness to different value systems and see how far one can get there first.

1:52:34.560 --> 1:52:40.640
 And I think one could do something that would be very good according to many different criteria.

1:52:41.600 --> 1:52:50.800
 We talked about AGI fundamentally transforming the value system of our existence, the meaning of

1:52:50.800 --> 1:52:57.600
 life. But today, what do you think is the meaning of life? What are you the silliest or perhaps the

1:52:57.600 --> 1:53:03.200
 biggest question? What's the meaning of life? What's the meaning of existence? What gives

1:53:03.200 --> 1:53:11.440
 your life fulfillment, purpose, happiness, meaning? Yeah, I think these are, I guess, a bunch of

1:53:12.720 --> 1:53:18.000
 different related questions in there that one can ask. Happiness, meaning,

1:53:18.000 --> 1:53:22.640
 yeah. You could imagine somebody getting a lot of happiness from something that

1:53:22.640 --> 1:53:30.560
 they didn't think was meaningful. Mindless watching reruns of some television series,

1:53:30.560 --> 1:53:33.920
 waiting junk food, maybe some people that gives pleasure, but they wouldn't think

1:53:34.560 --> 1:53:39.200
 it had a lot of meaning. Whereas, conversely, something that might be quite loaded with meaning

1:53:39.200 --> 1:53:44.240
 might not be very fun always. Some difficult achievement that really helps a lot of people

1:53:44.240 --> 1:53:51.680
 maybe requires self sacrifice and hard work. And so these things can, I think, come apart,

1:53:53.360 --> 1:54:01.040
 which is something to bear in mind also when you're thinking about these utopia questions that

1:54:03.520 --> 1:54:09.120
 you might actually start to do some constructive thinking about that. You might have to isolate

1:54:09.120 --> 1:54:15.040
 it and distinguish these different kinds of things that might be valuable in different ways.

1:54:16.160 --> 1:54:20.160
 Make sure you can sort of clearly perceive each one of them, and then you can think about how

1:54:20.160 --> 1:54:26.720
 you can combine them. And just as you said, hopefully come up with a way to maximize all of

1:54:26.720 --> 1:54:33.040
 them together. Yeah, or at least get, I mean, maximize or get a very high score on a wide

1:54:33.040 --> 1:54:37.120
 range of them, even if not literally all. You can always come up with values that are

1:54:37.120 --> 1:54:43.040
 exactly opposed to one another, right? But I think for many values, they're kind of opposed with,

1:54:44.320 --> 1:54:49.680
 if you place them within a certain dimensionality of your space, like there are shapes that are kind

1:54:49.680 --> 1:54:56.880
 of, you can't untangle like in a given dimensionality. But if you start adding dimensions, then it

1:54:56.880 --> 1:55:03.040
 might in many cases just be that they are easy to pull apart. And you could. So we'll see how

1:55:03.040 --> 1:55:08.240
 much space there is for that. But I think that there could be a lot in this context of radical

1:55:08.240 --> 1:55:15.520
 abundance, if ever we get to that. I don't think there's a better way to end it, Nick. You've

1:55:15.520 --> 1:55:21.440
 influenced a huge number of people to work on what could very well be the most important

1:55:21.440 --> 1:55:24.400
 problems of our time. So it's a huge honor. Thank you so much for talking to me. Well,

1:55:24.400 --> 1:55:29.440
 thank you for coming by, Lex. That was fun. Thank you. Thanks for listening to this conversation

1:55:29.440 --> 1:55:34.720
 with Nick Bostrom. And thank you to our presenting sponsor, Cash App. Please consider supporting

1:55:34.720 --> 1:55:41.120
 the podcast by downloading Cash App and using code Lex podcast. If you enjoy this podcast,

1:55:41.120 --> 1:55:46.240
 subscribe on YouTube, review it with five stars on Apple podcast, support it on Patreon, or simply

1:55:46.240 --> 1:55:53.200
 connect with me on Twitter at Lex Friedman. And now let me leave you with some words from Nick

1:55:53.200 --> 1:56:01.280
 Bostrom. Our approach to existential risks cannot be one of trial and error. There's no opportunity

1:56:01.280 --> 1:56:08.400
 to learn from errors. The reactive approach, see what happens, limit damages and learn from experience

1:56:08.400 --> 1:56:15.040
 is unworkable. Rather, we must take a proactive approach. This requires foresight to anticipate

1:56:15.040 --> 1:56:20.320
 new types of threats and a willingness to take decisive preventative action and to bear the

1:56:20.320 --> 1:56:36.240
 costs, moral and economic of such actions. Thank you for listening and hope to see you next time.

