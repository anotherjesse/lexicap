WEBVTT

00:00.000 --> 00:05.600
 The following is a conversation with Michael I. Jordan, a professor at Berkeley and one

00:05.600 --> 00:10.280
 of the most influential people in the history of machine learning, statistics, and artificial

00:10.280 --> 00:11.280
 intelligence.

00:11.280 --> 00:17.640
 He has been cited over 170,000 times and has mentored many of the world class researchers

00:17.640 --> 00:25.480
 defining the field of AI today, including Andrew Eng, Zubin Garamani, Bantaskar, and

00:25.480 --> 00:27.600
 Yoshio Benjo.

00:27.600 --> 00:34.560
 All this, to me, is as impressive as the over 32,000 points in the six NBA championships

00:34.560 --> 00:38.880
 of the Michael J. Jordan of basketball fame.

00:38.880 --> 00:43.200
 There's a nonzero probability that I'd talk to the other Michael Jordan given my connection

00:43.200 --> 00:48.520
 to and love of the Chicago Bulls of the 90s, but if I had to pick one, I'm going with the

00:48.520 --> 00:54.160
 Michael Jordan of statistics and computer science, or as Jan Lacoon calls him, the Miles

00:54.160 --> 00:56.160
 Davis of machine learning.

00:56.160 --> 01:01.560
 In his blog post titled Artificial Intelligence, The Revolution Hasn't Happened Yet, Michael

01:01.560 --> 01:06.040
 argues for broadening the scope or the artificial intelligence field.

01:06.040 --> 01:12.080
 In many ways, the underlying spirit of this podcast is the same, to see artificial intelligence

01:12.080 --> 01:18.640
 as a deeply human endeavor, to not only engineer algorithms and robots, but to understand and

01:18.640 --> 01:25.080
 empower human beings at all levels of abstraction, from the individual to our civilization as

01:25.080 --> 01:26.760
 a whole.

01:26.760 --> 01:29.440
 This is the Artificial Intelligence Podcast.

01:29.440 --> 01:34.040
 If you enjoy it, subscribe on YouTube, give us five stars at Apple Podcast, support it

01:34.040 --> 01:42.120
 on Patreon, or simply connect with me on Twitter, at Lex Friedman, spelled F R I D M A N.

01:42.120 --> 01:46.640
 As usual, I'll do one or two minutes of ads now and never any ads in the middle that can

01:46.640 --> 01:48.680
 break the flow of the conversation.

01:48.680 --> 01:53.960
 I hope that works for you and doesn't hurt the listening experience.

01:53.960 --> 01:58.320
 This show is presented by Cash App, the number one finance app in the App Store.

01:58.320 --> 02:01.920
 When you get it, use code LEX Podcast.

02:01.920 --> 02:06.480
 Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with

02:06.480 --> 02:08.480
 as little as $1.

02:08.480 --> 02:13.480
 Since Cash App does fractional share trading, let me mention that the order execution algorithm

02:13.480 --> 02:18.440
 that works behind the scenes to create the abstraction of the fractional orders is to

02:18.440 --> 02:20.960
 me an algorithmic marvel.

02:20.960 --> 02:26.400
 So big props for the Cash App engineers for solving a hard problem that in the end provides

02:26.400 --> 02:31.120
 an easy interface that takes a step up to the next layer of abstraction over the stock

02:31.120 --> 02:38.440
 market, making trading more accessible for new investors and diversification much easier.

02:38.440 --> 02:43.120
 So once again, if you get Cash App from the App Store, Google Play, and use the code LEX

02:43.120 --> 02:49.280
 Podcast, you'll get $10 and Cash App will also donate $10 to first, one of my favorite

02:49.280 --> 02:55.120
 organizations that is helping to advance robotics and STEM education for young people around

02:55.120 --> 02:57.120
 the world.

02:57.120 --> 03:02.720
 And now, here's my conversation with Michael I. Jordan.

03:02.720 --> 03:06.560
 Given that you're one of the greats in the field of AI, machine learning, computer science,

03:06.560 --> 03:13.960
 and so on, you're trivially called the Michael Jordan of machine learning, although as you

03:13.960 --> 03:19.760
 know, you were born first, so technically MJ is the Michael I. Jordan of basketball,

03:19.760 --> 03:25.560
 but anyway, my favorite is Yanlacoon calling you the Miles Davis of machine learning, because

03:25.560 --> 03:30.640
 as he says, you reinvent yourself periodically and sometimes leave fans scratching their

03:30.640 --> 03:32.480
 heads after you change direction.

03:32.480 --> 03:39.000
 So can you put at first your historian hat on and give a history of computer science and

03:39.000 --> 03:46.200
 AI as you saw it, as you experienced it, including the four generations of AI successes that

03:46.200 --> 03:48.000
 I've seen you talk about?

03:48.000 --> 03:49.000
 Sure.

03:49.000 --> 03:50.000
 Yeah.

03:50.000 --> 03:54.200
 First of all, I much prefer Yan's metaphor.

03:54.200 --> 04:00.000
 Miles Davis was a real explorer in jazz, and he had a coherent story.

04:00.000 --> 04:03.320
 So I think I have one, but it's not just the one you lived.

04:03.320 --> 04:04.920
 It's the one you think about later.

04:04.920 --> 04:09.920
 What a good historian does is they look back and they revisit.

04:09.920 --> 04:13.360
 I think what's happening right now is not AI.

04:13.360 --> 04:18.660
 That was an intellectual aspiration that's still alive today as an aspiration.

04:18.660 --> 04:22.480
 But I think this is akin to the development of chemical engineering from chemistry or

04:22.480 --> 04:25.920
 electrical engineering from electromagnetism.

04:25.920 --> 04:31.040
 So if you go back to the 30s or 40s, there wasn't yet chemical engineering.

04:31.040 --> 04:32.040
 There was chemistry.

04:32.040 --> 04:33.040
 There was fluid flow.

04:33.040 --> 04:35.600
 There was mechanics and so on.

04:35.600 --> 04:41.280
 But people pretty clearly viewed interesting goals to try to build factories that make

04:41.280 --> 04:48.040
 chemicals products and do it viably, safely, make good ones, do it at scale.

04:48.040 --> 04:51.720
 So people started to try to do that, of course, and some factories worked, some didn't.

04:51.720 --> 04:54.080
 Some were not viable, some exploded.

04:54.080 --> 04:58.200
 But in parallel, developed a whole field called chemical engineering.

04:58.200 --> 04:59.560
 Chemical engineering is a field.

04:59.560 --> 05:00.960
 It's no bones about it.

05:00.960 --> 05:02.560
 It has theoretical aspects to it.

05:02.560 --> 05:04.720
 It has practical aspects.

05:04.720 --> 05:06.720
 It's not just engineering, quote, unquote.

05:06.720 --> 05:09.640
 It's the real thing, real concepts are needed.

05:09.640 --> 05:11.680
 Same thing with electrical engineering.

05:11.680 --> 05:16.600
 There was Maxwell's equations, which in some sense were everything you know about electromagnetism.

05:16.600 --> 05:19.080
 But you needed to figure out how to build circuits, how to build modules, how to put

05:19.080 --> 05:22.840
 them together, how to bring electricity from one point to another safely and so on and

05:22.840 --> 05:23.840
 so forth.

05:23.840 --> 05:26.040
 So a whole field that developed called electrical engineering.

05:26.040 --> 05:32.520
 I think that's what's happening right now is that we have a proto field, which is statistics,

05:32.520 --> 05:36.240
 computer, more of the theoretical side of the algorithmic side of computer science.

05:36.240 --> 05:38.000
 That was enough to start to build things.

05:38.000 --> 05:39.320
 But what things?

05:39.320 --> 05:44.080
 Systems that bring value to human beings and use human data and mix in human decisions.

05:44.080 --> 05:47.600
 The engineering side of that is all ad hoc.

05:47.600 --> 05:48.600
 That's what's emerging.

05:48.600 --> 05:51.520
 In fact, if you want to call machine learning a field, I think that's what it is.

05:51.520 --> 05:55.520
 That's a proto form of engineering based on statistical and computational ideas in previous

05:55.520 --> 05:56.520
 generations.

05:56.520 --> 06:01.240
 But do you think there's something deeper about AI in his dreams and aspirations as

06:01.240 --> 06:03.840
 compared to chemical engineering and electrical engineering?

06:03.840 --> 06:07.960
 Well, the dreams and aspirations may be, but those are 500 years from now.

06:07.960 --> 06:10.480
 I think that that's like the Greek sitting there and saying, it would be neat to get

06:10.480 --> 06:12.880
 to the moon someday.

06:12.880 --> 06:16.160
 I think we have no clue how the brain does computation.

06:16.160 --> 06:17.160
 We're just a clue.

06:17.160 --> 06:23.720
 We're even worse than the Greeks on most anything interesting scientifically of our era.

06:23.720 --> 06:29.080
 Can you linger on that just for a moment because you stand not completely unique, but a little

06:29.080 --> 06:31.640
 bit unique in the clarity of that.

06:31.640 --> 06:37.120
 Can you elaborate your intuition of where we stand in our understanding of the human

06:37.120 --> 06:38.120
 brain?

06:38.120 --> 06:41.240
 A lot of people say, and your scientists say, we're not very far in understanding human

06:41.240 --> 06:44.680
 brain, but you're saying we're in the dark here.

06:44.680 --> 06:45.880
 Well, I know I'm not unique.

06:45.880 --> 06:49.240
 I don't even think in the clarity, but if you talk to real neuroscientists that really

06:49.240 --> 06:52.720
 study real synapses or real neurons, they agree.

06:52.720 --> 06:53.720
 They agree.

06:53.720 --> 06:58.640
 It's 100 years, hundreds of year task, and they're building it up slowly, surely.

06:58.640 --> 07:01.200
 What the signal is there is not clear.

07:01.200 --> 07:02.680
 We have all of our metaphors.

07:02.680 --> 07:06.760
 We think it's electrical, maybe it's chemical, it's a whole soup.

07:06.760 --> 07:10.880
 It's ions and proteins, and it's a cell, and that's even around like a single synapse.

07:10.880 --> 07:16.040
 If you look at an electron micrograph of a single synapse, it's a city of its own.

07:16.040 --> 07:20.760
 That's one little thing on a dendritic tree, which is extremely complicated, electrochemical

07:20.760 --> 07:25.720
 thing, and it's doing these spikes and voltages are flying around, and then proteins are taking

07:25.720 --> 07:29.480
 that and taking it down into the DNA, and who knows what.

07:29.480 --> 07:31.720
 It is the problem of the next few centuries.

07:31.720 --> 07:34.920
 It is fantastic, but we have our metaphors about it.

07:34.920 --> 07:36.120
 Is it an economic device?

07:36.120 --> 07:41.120
 Is it like the immune system, or is it like a layered set of arithmetic computations?

07:41.120 --> 07:48.120
 We have all these metaphors, and they're fun, but that's not real science per se.

07:48.120 --> 07:49.120
 There is neuroscience.

07:49.120 --> 07:51.360
 That's not neuroscience.

07:51.360 --> 07:55.440
 That's like the Greek speculating about how to get to the moon, fun.

07:55.440 --> 07:59.240
 I think that I like to say this fairly strongly, because I think a lot of young people think

07:59.240 --> 08:03.440
 that we're on the verge, because a lot of people who don't talk about it clearly, let

08:03.440 --> 08:08.480
 it be understood that, yes, we kind of, this is brain inspired, we're kind of close, breakthroughs

08:08.480 --> 08:14.480
 are on the horizon, and unscrupulous people sometimes who need money for their labs.

08:14.480 --> 08:17.160
 As I'm saying, unscrupulous, but people will oversell.

08:17.160 --> 08:18.640
 I need money from a lab.

08:18.640 --> 08:22.560
 I'm studying computational neuroscience.

08:22.560 --> 08:25.240
 I'm going to oversell it, and so there's been too much of that.

08:25.240 --> 08:32.080
 So I'll step into the gray area between metaphor and engineering with, I'm not sure if you're

08:32.080 --> 08:35.520
 familiar with brain computer interfaces.

08:35.520 --> 08:42.240
 So a company like Elon Musk has Neuralink that's working on putting electrodes into

08:42.240 --> 08:46.640
 the brain and trying to be able to read both read and send electrical signals, just as

08:46.640 --> 08:55.200
 you said, even the basic mechanism of communication in the brain is not something we understand.

08:55.200 --> 09:00.880
 But do you hope, without understanding the fundamental principles of how the brain works,

09:00.880 --> 09:06.560
 we'll be able to do something interesting at that gray area of metaphor?

09:06.560 --> 09:07.560
 It's not my area.

09:07.560 --> 09:11.400
 So I hope in the sense like anybody else hopes for some interesting things to happen from

09:11.400 --> 09:15.920
 research, I would expect more something like Alzheimer's will get figured out from modern

09:15.920 --> 09:16.920
 neuroscience.

09:16.920 --> 09:22.560
 There's a lot of humans offering based on brain disease, and we throw things like lithium

09:22.560 --> 09:23.560
 at the brain.

09:23.560 --> 09:24.560
 It kind of works.

09:24.560 --> 09:25.880
 It's a blue eye.

09:25.880 --> 09:28.240
 That's not quite true, but mostly we don't know.

09:28.240 --> 09:31.920
 And that's even just about the biochemistry of the brain and how it leads to mood swings

09:31.920 --> 09:33.080
 and so on.

09:33.080 --> 09:34.720
 How thought emerges from that.

09:34.720 --> 09:38.120
 We're really, really completely dim.

09:38.120 --> 09:41.520
 So that you might want to hook up electrodes and try to do some signal processing on that

09:41.520 --> 09:45.600
 and try to find patterns, fine, by all means go for it.

09:45.600 --> 09:48.720
 It's just not scientific at this point.

09:48.720 --> 09:53.200
 So it's like kind of sitting in a satellite and watching the emissions from a city and

09:53.200 --> 09:56.920
 trying to affirm things about the microeconomy, even though you don't have microeconomic concepts.

09:56.920 --> 09:59.160
 I mean, it's really that kind of thing.

09:59.160 --> 10:02.480
 And so yes, can you find some signals that do something interesting or useful?

10:02.480 --> 10:06.640
 Can you control a cursor or a mouse with your brain?

10:06.640 --> 10:07.640
 Yeah.

10:07.640 --> 10:08.640
 Absolutely.

10:08.640 --> 10:13.320
 And then I can imagine business models based on that and even medical applications of

10:13.320 --> 10:14.320
 that.

10:14.320 --> 10:19.520
 But from there to understanding algorithms that allow us to really tie in deeply from

10:19.520 --> 10:22.560
 the brain to computer, I just, no, I don't agree with Elon Musk.

10:22.560 --> 10:26.560
 I don't think that's even, that's not for our generation, it's not even for the century.

10:26.560 --> 10:33.560
 So just in the hopes of getting you to dream, you've mentioned Komogorov and touring might

10:33.560 --> 10:36.200
 pop up.

10:36.200 --> 10:41.240
 Do you think that there might be breakthroughs that will get you to sit back in five, 10 years

10:41.240 --> 10:43.240
 and say, wow.

10:43.240 --> 10:49.200
 Oh, I'm sure there will be, but I don't think that there'll be demos that impress me.

10:49.200 --> 10:55.000
 I don't think that having a computer call a restaurant and pretend to be a human is

10:55.000 --> 10:59.840
 breakthrough and people, you know, some people presented as such.

10:59.840 --> 11:01.640
 It's imitating human intelligence.

11:01.640 --> 11:07.360
 It's even putting coughs in the thing to make a bit of a PR stunt.

11:07.360 --> 11:10.920
 And so fine that the world runs on those things too.

11:10.920 --> 11:14.920
 And I don't want to diminish all the hard work and engineering that goes behind things

11:14.920 --> 11:17.720
 like that and the ultimate value to the human race.

11:17.720 --> 11:20.480
 But that's not scientific understanding.

11:20.480 --> 11:23.680
 And I know the people that work on these things, they are after scientific understanding, you

11:23.680 --> 11:26.320
 know, in the meantime, they've got to kind of, you know, the trains got to run and they

11:26.320 --> 11:28.680
 got mouths to feed and they got things to do.

11:28.680 --> 11:30.440
 And there's nothing wrong with all that.

11:30.440 --> 11:32.560
 I would call that though, just engineering.

11:32.560 --> 11:35.880
 And I want to distinguish that between an engineering field like electrical engineering

11:35.880 --> 11:39.480
 that originally that originally emerged that had real principles and you really know what

11:39.480 --> 11:43.640
 you're doing and you had a little scientific understanding, maybe not even complete.

11:43.640 --> 11:49.000
 So it became more predictable and it was really gave value to human life because it was understood.

11:49.000 --> 11:54.720
 And so we don't want to muddle too much these waters of what we're able to do versus what

11:54.720 --> 11:58.040
 we really can do in a way that's going to impress the next.

11:58.040 --> 12:02.520
 So I don't need to be wowed, but I think that someone comes along in 20 years, a younger

12:02.520 --> 12:08.360
 person who's absorbed all the technology and for them to be wowed, I think they have to

12:08.360 --> 12:09.360
 be more deeply impressed.

12:09.360 --> 12:13.000
 A young Kolmogorov would not be wowed by some of the stunts that you see right now coming

12:13.000 --> 12:14.000
 from the big companies.

12:14.000 --> 12:19.040
 The demos, but do you think the breakthroughs from Kolmogorov would be and give this question

12:19.040 --> 12:20.040
 a chance?

12:20.040 --> 12:24.120
 Do you think they'll be in the scientific fundamental principles arena?

12:24.120 --> 12:28.560
 Or do you think it's possible to have fundamental breakthroughs in engineering?

12:28.560 --> 12:33.200
 Meaning you know, I would say some of the things that Elon Musk is working with SpaceX

12:33.200 --> 12:38.800
 and then others sort of trying to revolutionize the fundamentals of engineering of manufacturing

12:38.800 --> 12:45.120
 of saying, here's a problem, we know how to do a demo of and actually taking it to scale.

12:45.120 --> 12:46.960
 So there's going to be all kinds of breakthroughs.

12:46.960 --> 12:48.280
 I just don't like that terminology.

12:48.280 --> 12:52.000
 I'm a scientist and I work on things day in and day out and things move along and eventually

12:52.000 --> 12:56.400
 say, wow, something happened, but I don't like that language very much.

12:56.400 --> 13:01.000
 Also I don't like to prize theoretical breakthroughs over practical ones.

13:01.000 --> 13:06.760
 I tend to be more of a theoretician and I think there's lots to do in that arena right now.

13:06.760 --> 13:09.640
 And so I wouldn't point to the Kolmogorovs, I might point to the Edison's of the era

13:09.640 --> 13:11.840
 and maybe Musk is a bit more like that.

13:11.840 --> 13:17.440
 But you know, Musk, God bless him also, we'll say things about AI that he knows very little

13:17.440 --> 13:21.680
 about and he doesn't know what he, he is, you know, it leads people astray when he talks

13:21.680 --> 13:23.800
 about things he doesn't know anything about.

13:23.800 --> 13:27.320
 Trying to program a computer to understand natural language, to be involved in a dialogue

13:27.320 --> 13:30.440
 we're having right now, that can happen in our lifetime.

13:30.440 --> 13:35.080
 You could fake it, you can mimic, sort of take old sentences that humans use and retread

13:35.080 --> 13:38.560
 them with the deep understanding of language, no, it's not going to happen.

13:38.560 --> 13:42.920
 And so from that, you know, I hope you can perceive that deeper, yet deeper kind of aspects

13:42.920 --> 13:44.480
 and intelligence are not going to happen.

13:44.480 --> 13:45.480
 Now will there be breakthroughs?

13:45.480 --> 13:49.240
 You know, I think that Google was a breakthrough, I think Amazon is a breakthrough.

13:49.240 --> 13:52.680
 You know, I think Uber is a breakthrough, you know, that bring value to human beings

13:52.680 --> 13:56.840
 at scale in new brand new ways based on data flows and so on.

13:56.840 --> 14:01.240
 A lot of these things are slightly broken because there's not a kind of a engineering

14:01.240 --> 14:06.720
 field that takes economic value in context of data and, you know, planetary scale and

14:06.720 --> 14:09.520
 worries about all the externalities, the privacy.

14:09.520 --> 14:13.160
 You know, we don't have that field, so we don't think these things through very well.

14:13.160 --> 14:16.640
 But I see that as emerging and that will be constant, that will, you know, looking back

14:16.640 --> 14:20.320
 from a hundred years, that will be constantly a breakthrough in this era, just like electrical

14:20.320 --> 14:23.520
 engineering was a breakthrough in the early part of the last century and chemical engineering

14:23.520 --> 14:24.520
 was a breakthrough.

14:24.520 --> 14:30.360
 So the scale, the markets that you talk about and we'll get to will be seen as sort of breakthrough

14:30.360 --> 14:33.960
 and we're in very early days of really doing interesting stuff there.

14:33.960 --> 14:37.440
 And we'll get to that, but it's just taking a quick step back.

14:37.440 --> 14:45.640
 Can you give, kind of throw off the historian hat, I mean, you briefly said that in the

14:45.640 --> 14:50.000
 history of AI kind of mimics the history of chemical engineering, but I keep saying machine

14:50.000 --> 14:53.640
 learning, you keep wanting to say AI, just to let you know, I don't, you know, I resist

14:53.640 --> 14:54.640
 that.

14:54.640 --> 14:59.600
 I don't think this is about AI really was John McCarthy as almost a philosopher.

14:59.600 --> 15:03.560
 Saying, wouldn't it be cool if we could put thought in a computer?

15:03.560 --> 15:08.040
 If we could mimic the human capability to think or put intelligence in in some sense

15:08.040 --> 15:09.960
 into a computer.

15:09.960 --> 15:12.120
 That's an interesting philosophical question.

15:12.120 --> 15:13.560
 And he wanted to make it more than philosophy.

15:13.560 --> 15:17.360
 He wanted to actually write down logical formula and algorithms that would do that.

15:17.360 --> 15:20.160
 And that is a perfectly valid reasonable thing to do.

15:20.160 --> 15:23.080
 That's not what's happening in this era.

15:23.080 --> 15:27.760
 So the reason I keep saying AI actually, and I'd love to hear what you think about it,

15:27.760 --> 15:34.640
 machine learning has a has a very particular set of methods and tools.

15:34.640 --> 15:37.720
 Maybe your version of it is that mine doesn't know it doesn't very, very open.

15:37.720 --> 15:38.720
 It does optimization.

15:38.720 --> 15:39.720
 It does sampling.

15:39.720 --> 15:40.720
 It does.

15:40.720 --> 15:44.600
 So systems that learn is what machine learning is systems that learn and make decisions and

15:44.600 --> 15:45.600
 make decisions.

15:45.600 --> 15:50.040
 So what is pattern recognition and, you know, finding patterns is all about making decisions

15:50.040 --> 15:52.560
 in real worlds and having close feedback loops.

15:52.560 --> 15:57.640
 So something like symbolic AI expert systems, reasoning systems, knowledge based representation

15:57.640 --> 16:00.120
 and all of those kinds of things search.

16:00.120 --> 16:04.760
 Does that neighbor fit into what you think of as machine learning?

16:04.760 --> 16:06.120
 So I don't even like the word machine learning.

16:06.120 --> 16:09.560
 I think that with the field you're talking about is all about making large collections

16:09.560 --> 16:12.640
 of decisions under uncertainty by large collections of entities.

16:12.640 --> 16:13.640
 Yes.

16:13.640 --> 16:14.640
 Right.

16:14.640 --> 16:16.120
 And there are principles for that at that scale.

16:16.120 --> 16:18.960
 You don't have to say the principles are for a single entity that's making decisions,

16:18.960 --> 16:20.560
 single agent or single human.

16:20.560 --> 16:22.880
 It really immediately goes to the network of decisions.

16:22.880 --> 16:23.880
 Is a good word for that?

16:23.880 --> 16:25.400
 No, there's no good words for any of this.

16:25.400 --> 16:27.240
 That's kind of part of the problem.

16:27.240 --> 16:29.920
 So we can continue the conversation, use AI for all that.

16:29.920 --> 16:35.520
 I just want to kind of raise our flag here that this is not about, we don't know what

16:35.520 --> 16:38.120
 intelligence is and real intelligence.

16:38.120 --> 16:41.000
 We don't know much about abstraction and reasoning at the level of humans.

16:41.000 --> 16:42.000
 We don't have a clue.

16:42.000 --> 16:44.680
 We're not trying to build that because we don't have a clue.

16:44.680 --> 16:45.680
 Eventually it may emerge.

16:45.680 --> 16:49.040
 They'll make, I don't know if they'll be breakthroughs, but eventually we'll start to get glimmers

16:49.040 --> 16:50.160
 of that.

16:50.160 --> 16:51.400
 It's not what's happening right now.

16:51.400 --> 16:52.400
 Okay.

16:52.400 --> 16:53.400
 We're taking data.

16:53.400 --> 16:54.400
 We're trying to make good decisions based on that.

16:54.400 --> 16:55.400
 We're trying to do a scale.

16:55.400 --> 16:56.760
 We're trying to do it economically, viably.

16:56.760 --> 16:58.240
 We're trying to build markets.

16:58.240 --> 17:01.040
 We're trying to keep value at that scale.

17:01.040 --> 17:03.720
 And aspects of this will look intelligent.

17:03.720 --> 17:08.120
 They will look, computers were so dumb before, they will see more intelligent.

17:08.120 --> 17:09.840
 We will use that buzzword of intelligence.

17:09.840 --> 17:14.780
 So we can use it in that sense, but you know, so machine learning, you can scope it narrowly

17:14.780 --> 17:17.960
 is just learning from data and pattern recognition.

17:17.960 --> 17:21.960
 But whatever I, when I talk about these topics, I maybe data science is another word you could

17:21.960 --> 17:23.680
 throw in the mix.

17:23.680 --> 17:28.040
 It really is important that the decisions are, as part of it, it's consequential decisions

17:28.040 --> 17:29.040
 in the real world.

17:29.040 --> 17:30.400
 Am I going to have a medical operation?

17:30.400 --> 17:35.240
 Am I going to drive down the street, you know, things that were, they're scarcity, things

17:35.240 --> 17:39.400
 that impact other human beings or other, you know, the environment and so on.

17:39.400 --> 17:40.800
 How do I do that based on data?

17:40.800 --> 17:41.800
 How do I do that?

17:41.800 --> 17:44.160
 I definitely how do I use computers to help those kinds of things go forward, whatever

17:44.160 --> 17:45.640
 you want to call that.

17:45.640 --> 17:46.640
 So let's call it AI.

17:46.640 --> 17:51.640
 Let's agree to call it AI, but it's, let's, let's not say that what the goal of that

17:51.640 --> 17:55.600
 is, is intelligence, the goal of that is really good working systems at planetary scale that

17:55.600 --> 17:56.600
 we've never seen before.

17:56.600 --> 18:00.800
 So reclaim the word AI from the Dartmouth conference from many decades ago of the dream

18:00.800 --> 18:01.800
 of human.

18:01.800 --> 18:02.800
 I don't want to reclaim it.

18:02.800 --> 18:03.800
 I want a new word.

18:03.800 --> 18:04.800
 I think it was a bad choice.

18:04.800 --> 18:08.240
 I mean, I, you know, I, if you read one of my little things, the history was basically

18:08.240 --> 18:12.400
 that McCarthy needed a new name because cybernetics already existed.

18:12.400 --> 18:15.280
 And he didn't like, you know, no one really liked Norbert Wiener.

18:15.280 --> 18:17.400
 Norbert Wiener was kind of an island to himself.

18:17.400 --> 18:21.200
 And he felt that he had encompassed all this and in some sense he did.

18:21.200 --> 18:24.400
 If you look at the language of cybernetics, it was everything we're talking about.

18:24.400 --> 18:28.160
 It was control theory and single processing and some notions of intelligence and close

18:28.160 --> 18:29.360
 feedback loops and data.

18:29.360 --> 18:30.960
 It was all there.

18:30.960 --> 18:34.240
 It's just not a word that lived on partly because of the, maybe the personalities.

18:34.240 --> 18:36.720
 But McCarthy needed a new word to say, I'm different from you.

18:36.720 --> 18:38.360
 I'm not part of your, your show.

18:38.360 --> 18:41.720
 I got my own invented this word.

18:41.720 --> 18:46.160
 And again, as a kind of a thinking forward about the movies that would be made about

18:46.160 --> 18:51.000
 it, it was a great choice, but thinking forward about creating a sober academic and

18:51.000 --> 18:52.000
 real world discipline.

18:52.000 --> 18:56.280
 It was a terrible choice because it led to promises that are not true, that we understand.

18:56.280 --> 18:59.320
 We understand artificial perhaps, but we don't understand intelligence.

18:59.320 --> 19:03.320
 It's a small tangent because you're one of the great personalities of machine learning,

19:03.320 --> 19:05.120
 whatever the heck you call the field.

19:05.120 --> 19:11.800
 The, do you think science progresses by personalities or by the fundamental principles and theories

19:11.800 --> 19:15.040
 and research that's outside of personality?

19:15.040 --> 19:16.040
 Yeah, both.

19:16.040 --> 19:17.520
 And I wouldn't say there should be one kind of personality.

19:17.520 --> 19:23.200
 I have mine and I have my preferences and I have a kind of network around me that feeds

19:23.200 --> 19:26.680
 me and some of them agree with me and some disagree, but you know, all kinds of personalities

19:26.680 --> 19:28.480
 are needed.

19:28.480 --> 19:31.680
 Right now, I think the personality that it's a little too exuberant, a little bit too ready

19:31.680 --> 19:35.840
 to promise the moon is, is a little bit too much in ascendance.

19:35.840 --> 19:38.160
 And I do, I do think that that's, there's some good to that.

19:38.160 --> 19:41.560
 It certainly attracts lots of young people to our field, but a lot of those people come

19:41.560 --> 19:46.040
 in with strong misconceptions and they have to then unlearn those and then find something

19:46.040 --> 19:48.880
 in, you know, to do.

19:48.880 --> 19:52.160
 And so I think there's just got to be some, you know, multiple voices and there's, I didn't,

19:52.160 --> 19:54.840
 I wasn't hearing enough of the more sober voice.

19:54.840 --> 20:02.160
 So as a continuation of a fun tangent and speaking of vibrant personalities, what would

20:02.160 --> 20:07.400
 you say is the most interesting disagreement you have with Jan Lacoon?

20:07.400 --> 20:12.480
 So Jan's an old friend and I just say that I don't think we disagree about very much

20:12.480 --> 20:13.480
 really.

20:13.480 --> 20:18.760
 He and I both kind of have a let's build that kind of mentality and does it work and kind

20:18.760 --> 20:21.360
 of mentality and kind of concrete.

20:21.360 --> 20:24.880
 We both speak French and we speak French more together and we have, we have a lot, a lot

20:24.880 --> 20:27.120
 in common.

20:27.120 --> 20:31.760
 And so, you know, if one wanted to highlight a disagreement, it's not really a fundamental.

20:31.760 --> 20:38.320
 When I think it's just kind of where we're emphasizing, Jan has emphasized pattern recognition

20:38.320 --> 20:40.920
 and has emphasized prediction.

20:40.920 --> 20:41.920
 All right.

20:41.920 --> 20:45.360
 So, you know, and it's interesting to try to take that as far as you can.

20:45.360 --> 20:50.640
 If you could do perfect prediction, what would that give you kind of as a thought experiment?

20:50.640 --> 20:55.220
 And I think that's way too limited.

20:55.220 --> 20:56.680
 We cannot do perfect prediction.

20:56.680 --> 20:59.320
 We will never have the data sets that allow me to figure out what you're about ready to

20:59.320 --> 21:00.760
 do, what question you're going to ask next.

21:00.760 --> 21:01.760
 I have no clue.

21:01.760 --> 21:03.280
 I will never know such things.

21:03.280 --> 21:07.560
 Moreover, most of us find ourselves during the day in all kinds of situations we had

21:07.560 --> 21:13.440
 no anticipation of that are kind of various, their novel in various ways.

21:13.440 --> 21:16.320
 And in that moment, we want to think through what we want.

21:16.320 --> 21:18.920
 And also, there's going to be market forces acting on us.

21:18.920 --> 21:22.280
 I'd like to go down that street, but now it's full because there's a crane in the street.

21:22.280 --> 21:23.280
 I got it.

21:23.280 --> 21:24.280
 I got to think about that.

21:24.280 --> 21:26.240
 I got to think about what I might really want here.

21:26.240 --> 21:29.080
 And I got to sort of think about how much it costs me to do this action versus this

21:29.080 --> 21:30.080
 action.

21:30.080 --> 21:32.280
 I got to think about the risks involved.

21:32.280 --> 21:36.680
 You know, a lot of our current pattern recognition and prediction systems don't do any risk evaluations.

21:36.680 --> 21:38.960
 They have no error bars, right?

21:38.960 --> 21:41.040
 I got to think about other people's decisions around me.

21:41.040 --> 21:45.560
 I got to think about a collection of my decisions, even just thinking about like a medical treatment.

21:45.560 --> 21:50.400
 You know, I'm not going to take the prediction of a neural net about my health, about something

21:50.400 --> 21:51.400
 consequential.

21:51.400 --> 21:54.560
 I'm not about ready to have a heart attack because some number is over.7.

21:54.560 --> 21:58.840
 Even if you had all the data in the world, they've ever been collected about heart attacks

21:58.840 --> 22:00.880
 better than any doctor ever had.

22:00.880 --> 22:03.840
 I'm not going to trust the output of that neural net to predict my heart attack.

22:03.840 --> 22:06.400
 I'm going to want to ask what if questions around that.

22:06.400 --> 22:09.800
 I'm going to want to look at some other possible data I didn't have.

22:09.800 --> 22:10.800
 Causal things.

22:10.800 --> 22:13.680
 I'm going to want to have a dialogue with a doctor about things we didn't think about

22:13.680 --> 22:15.480
 when we gathered the data.

22:15.480 --> 22:16.640
 You know, I could go on and on.

22:16.640 --> 22:17.880
 I hope you can see.

22:17.880 --> 22:21.520
 And I don't, I think that if you say predictions, everything that, that, that you're missing

22:21.520 --> 22:23.480
 all of this stuff.

22:23.480 --> 22:28.240
 And so prediction plus decision making is everything, but both of them are equally important.

22:28.240 --> 22:30.120
 And so the field has emphasized prediction.

22:30.120 --> 22:33.640
 Yon rightly so has seen how powerful that is.

22:33.640 --> 22:37.040
 But at the cost of people not being aware of the decision making is where the rubber

22:37.040 --> 22:41.240
 really hits the road, where human lives are at stake, where risks are being taken, where

22:41.240 --> 22:42.240
 you got to gather more data.

22:42.240 --> 22:43.640
 You got to think about the air bars.

22:43.640 --> 22:45.880
 You got to think about the consequences of your decisions on others.

22:45.880 --> 22:48.960
 You got about the economy around your decisions, blah, blah, blah, blah.

22:48.960 --> 22:52.120
 I'm not the only one working on those, but we're a smaller tribe.

22:52.120 --> 22:56.400
 And right now we're not the one that people talk about the most.

22:56.400 --> 23:00.480
 But you know, if you go out in the real world and industry, you know, at Amazon, I'd say

23:00.480 --> 23:03.720
 half the people there are working on decision making and the other half are doing, you know,

23:03.720 --> 23:04.720
 the pattern recognition.

23:04.720 --> 23:05.720
 It's important.

23:05.720 --> 23:10.200
 And the words of pattern recognition and prediction, I think the distinction there, not to linger

23:10.200 --> 23:16.160
 on words, but the distinction there is more a constrained sort of in the lab data set

23:16.160 --> 23:21.160
 versus decision making is talking about consequential decisions in the real world under the messiness

23:21.160 --> 23:23.680
 and the uncertainty of the real world.

23:23.680 --> 23:27.320
 And just the whole of it, the whole mess of it that actually touches human beings and

23:27.320 --> 23:31.080
 scale, like you said, market forces, that's the distinction.

23:31.080 --> 23:33.800
 It helps add those, that perspective, that broader perspective.

23:33.800 --> 23:34.800
 You're right.

23:34.800 --> 23:35.800
 I totally agree.

23:35.800 --> 23:38.120
 On the other hand, if you're a real prediction person, of course you want it to be in the

23:38.120 --> 23:39.120
 real world.

23:39.120 --> 23:40.120
 You want to predict real world events.

23:40.120 --> 23:44.280
 I'm just saying that's not possible with just data sets, that it has to be in the context

23:44.280 --> 23:48.480
 of, you know, strategic things that someone's doing, data they might gather, things they

23:48.480 --> 23:50.880
 could have gathered, the reasoning process around data.

23:50.880 --> 23:53.580
 It's not just taking data and making predictions based on the data.

23:53.580 --> 23:58.280
 So one of the things that you're working on, I'm sure there's others working on it, but

23:58.280 --> 24:04.960
 I don't hear often it talked about, especially in the clarity that you talk about it.

24:04.960 --> 24:10.000
 And I think it's both the most exciting and the most concerning area of AI in terms of

24:10.000 --> 24:11.000
 decision making.

24:11.000 --> 24:15.280
 So you've talked about AI systems that help make decisions that scale in a distributed

24:15.280 --> 24:19.760
 way, millions, billions of decisions, sort of markets of decisions.

24:19.760 --> 24:24.880
 Can you, as a starting point, sort of give an example of a system that you think about

24:24.880 --> 24:27.040
 when you're thinking about these kinds of systems?

24:27.040 --> 24:28.040
 Yeah.

24:28.040 --> 24:31.400
 So first of all, you're absolutely getting into some territory, which I will be beyond

24:31.400 --> 24:32.400
 my expertise.

24:32.400 --> 24:35.680
 And there are lots of things that are going to be very not obvious to think about.

24:35.680 --> 24:39.880
 Just like, again, I like to think about history a little bit, but think about, put yourself

24:39.880 --> 24:40.880
 back in the 60s.

24:40.880 --> 24:43.360
 There was kind of a banking system that wasn't computerized really.

24:43.360 --> 24:46.160
 There was database theory emerging.

24:46.160 --> 24:49.160
 And database people had to think about, how do I actually not just move data around, but

24:49.160 --> 24:55.400
 actual money, and have it be valid, and have transactions at ATMs happen that are actually

24:55.400 --> 24:57.840
 all valid, and so on and so forth.

24:57.840 --> 25:01.760
 So that's the kind of issues you get into when you start to get serious about things

25:01.760 --> 25:02.760
 like this.

25:02.760 --> 25:07.240
 I like to think about as kind of almost a thought experiment to help me think something

25:07.240 --> 25:15.360
 simpler, which is the music market, because to first order, there is no music market in

25:15.360 --> 25:18.760
 the world right now in our country, for sure.

25:18.760 --> 25:25.200
 There are things called record companies, and they make money, and they prop up a few

25:25.200 --> 25:31.000
 really good musicians, and make them superstars, and they all make huge amounts of money.

25:31.000 --> 25:33.680
 But there's a long tale of huge numbers of people that make lots and lots of really

25:33.680 --> 25:40.560
 good music that is actually listened to by more people than the famous people.

25:40.560 --> 25:41.560
 They are not in a market.

25:41.560 --> 25:42.800
 They cannot have a career.

25:42.800 --> 25:43.880
 They do not make money.

25:43.880 --> 25:44.880
 The creators.

25:44.880 --> 25:45.880
 The creators.

25:45.880 --> 25:46.880
 The creators.

25:46.880 --> 25:47.880
 The so called influencers or whatever.

25:47.880 --> 25:49.320
 The managers who they are, right?

25:49.320 --> 25:53.360
 So there are people who make extremely good music, especially in the hip hop or Latin

25:53.360 --> 25:55.240
 world these days.

25:55.240 --> 25:56.320
 They do it on their laptop.

25:56.320 --> 26:01.040
 That's what they do on the weekend, and they have another job during the week, and they

26:01.040 --> 26:03.360
 put it up on SoundCloud or other sites.

26:03.360 --> 26:04.720
 Eventually, it gets streamed.

26:04.720 --> 26:06.120
 It down gets turned into bits.

26:06.120 --> 26:07.680
 It's not economically valuable.

26:07.680 --> 26:08.960
 The information is lost.

26:08.960 --> 26:09.960
 It gets put up.

26:09.960 --> 26:11.560
 There are people stream it.

26:11.560 --> 26:13.760
 You walk around in a big city.

26:13.760 --> 26:16.360
 You see people with headphones all, you know, especially young kids listening to music all

26:16.360 --> 26:17.360
 the time.

26:17.360 --> 26:21.080
 Especially the data, none of them, very little of the music they listen to is the famous

26:21.080 --> 26:22.080
 people's music.

26:22.080 --> 26:23.080
 And none of it's old music.

26:23.080 --> 26:24.920
 It's all the latest stuff.

26:24.920 --> 26:27.480
 But the people who made that latest stuff are like some 16 year old somewhere who will

26:27.480 --> 26:29.640
 never make a career out of this, who will never make money.

26:29.640 --> 26:31.480
 Of course, there will be a few counter examples.

26:31.480 --> 26:35.360
 The record companies incentivize to pick out a few and highlight them.

26:35.360 --> 26:37.720
 Long story short, there's a missing market there.

26:37.720 --> 26:43.480
 There is not a consumer producer relationship at the level of the actual creative acts.

26:43.480 --> 26:48.120
 The pipelines and spotifies of the world that take this stuff and stream it along, they

26:48.120 --> 26:51.200
 make money off of subscriptions or advertising and those things.

26:51.200 --> 26:52.200
 They're making the money, right?

26:52.200 --> 26:56.080
 And then they will offer bits and pieces of it to a few people again to highlight that,

26:56.080 --> 26:58.560
 you know, they're the simulator market.

26:58.560 --> 27:03.560
 Anyway, a real market would be if you're a creator of music that you actually are somebody

27:03.560 --> 27:06.320
 who's good enough that people want to listen to you.

27:06.320 --> 27:07.840
 You should have the data available to you.

27:07.840 --> 27:11.480
 There should be a dashboard showing a map of the United States.

27:11.480 --> 27:14.680
 So in last week, here's all the places your songs were listened to.

27:14.680 --> 27:20.520
 It should be transparent, vettable so that if someone down in Providence sees that you're

27:20.520 --> 27:24.160
 being listened to 10,000 times in Providence, that they know that's real data.

27:24.160 --> 27:25.280
 You know it's real data.

27:25.280 --> 27:27.280
 They will have you come give a show down there.

27:27.280 --> 27:30.000
 They will broadcast to the people who've been listening to you that you're coming.

27:30.000 --> 27:34.480
 If you do this right, you could, you could, you know, go down there and make $20,000.

27:34.480 --> 27:37.080
 You do that three times a year, you start to have a career.

27:37.080 --> 27:39.560
 So in this sense, AI creates jobs.

27:39.560 --> 27:40.680
 It's not about taking away human jobs.

27:40.680 --> 27:43.480
 It's creating new jobs because it creates a new market.

27:43.480 --> 27:46.560
 Once you've created a market, you've now connected up producers and consumers.

27:46.560 --> 27:49.600
 You know, the person who's making the music can say to someone who comes to their shows

27:49.600 --> 27:53.200
 a lot, hey, I'll play your daughter's wedding for $10,000.

27:53.200 --> 27:54.200
 You'll say $8,000.

27:54.200 --> 27:55.200
 They'll say $9,000.

27:55.200 --> 27:59.000
 Then you, again, you can now get an income up to $100,000.

27:59.000 --> 28:01.920
 You're not going to be a millionaire, all right.

28:01.920 --> 28:07.080
 And now even think about really the value of music is in these personal connections, even

28:07.080 --> 28:13.680
 so much so that a young kid wants to wear a T shirt with their favorite musician's signature

28:13.680 --> 28:14.840
 on it, right?

28:14.840 --> 28:18.080
 So if they listen to the music on the internet, the internet should be able to provide them

28:18.080 --> 28:21.840
 with a button that they push and the merchandise arrives the next day.

28:21.840 --> 28:23.000
 We can do that, right?

28:23.000 --> 28:24.400
 And now why should we do that?

28:24.400 --> 28:27.600
 Well, because the kid who bought the shirt will be happy, but more the person who made

28:27.600 --> 28:29.040
 the music will get the money.

28:29.040 --> 28:32.360
 There's no advertising needed, right?

28:32.360 --> 28:36.480
 So you can create markets between producers and consumers, take 5% cut.

28:36.480 --> 28:41.120
 Your company will be perfectly sound, it'll go forward into the future, and it will create

28:41.120 --> 28:45.080
 new markets and that raises human happiness.

28:45.080 --> 28:48.720
 Now this seems like it was easy, just create this dashboard, kind of create some connections

28:48.720 --> 28:49.720
 and all that.

28:49.720 --> 28:52.920
 But if you think about Uber or whatever, you think about the challenges in the real world

28:52.920 --> 28:56.200
 of it doing things like this, and there are actually new principles going to be needed.

28:56.200 --> 28:59.080
 You're trying to create a new kind of two way market at a different scale that's ever

28:59.080 --> 29:00.080
 been done before.

29:00.080 --> 29:05.760
 There's going to be unwanted aspects of the market, there'll be bad people, there'll

29:05.760 --> 29:11.080
 be the data will get used in the wrong ways, it'll fail in some ways, it won't deliver

29:11.080 --> 29:12.640
 about, you have to think that through.

29:12.640 --> 29:17.200
 Just like anyone who ran a big auction or ran a big matching service in economics will

29:17.200 --> 29:18.840
 think these things through.

29:18.840 --> 29:22.120
 And so that maybe didn't get at all the huge issues that can arise when you start to create

29:22.120 --> 29:27.320
 markets, but it starts for me solidify my thoughts and allow me to move forward in my

29:27.320 --> 29:28.320
 own thinking.

29:28.320 --> 29:32.840
 Yeah, so I talked to, had a research at Spotify actually, I think their long term goal they've

29:32.840 --> 29:41.120
 said is to have at least one million creators make a comfortable living putting on Spotify.

29:41.120 --> 29:52.160
 So I think you articulate a really nice vision of the world and the digital and the cyberspace

29:52.160 --> 29:53.920
 of markets.

29:53.920 --> 30:02.720
 What do you think companies like Spotify or YouTube or Netflix can do to create such

30:02.720 --> 30:04.080
 markets?

30:04.080 --> 30:05.400
 Is it an AI problem?

30:05.400 --> 30:08.560
 Is it an interface problems or interface design?

30:08.560 --> 30:13.400
 Is it some other kind of, is it an economics problem?

30:13.400 --> 30:15.600
 Who should they hire to solve these problems?

30:15.600 --> 30:17.480
 Well, part of it's not just top down.

30:17.480 --> 30:20.000
 So the Silicon Valley has this attitude that they know how to do it.

30:20.000 --> 30:23.360
 They will create the system just like Google did with the search box that will be so good

30:23.360 --> 30:26.960
 that they'll just everyone will adopt that.

30:26.960 --> 30:31.480
 It's not, it's everything you said, but really I think missing the kind of culture.

30:31.480 --> 30:34.800
 So it's literally that 16 year old who's, who's able to create the songs.

30:34.800 --> 30:37.000
 You don't create that as a Silicon Valley entity.

30:37.000 --> 30:39.360
 You don't hire them per se, right?

30:39.360 --> 30:44.360
 You have to create an ecosystem in which they are wanted and that they're belong, right?

30:44.360 --> 30:48.160
 And so you have to have some culture credibility to do things like this, you know, Netflix

30:48.160 --> 30:53.040
 to their credit wanted some of that credibility and they created shows, you know, content.

30:53.040 --> 30:54.040
 They call it content.

30:54.040 --> 30:56.880
 It's such a terrible word, but it's culture, right?

30:56.880 --> 31:01.160
 And so with movies, you can kind of go give a large sum of money to somebody graduate

31:01.160 --> 31:03.480
 from the USC film school.

31:03.480 --> 31:07.360
 It's a whole thing of its own, but it's kind of like rich white people's thing to do,

31:07.360 --> 31:11.800
 you know, and you know, American culture has not been so much about rich white people.

31:11.800 --> 31:15.840
 It's been about all the immigrants, all the, you know, the Africans who came and brought

31:15.840 --> 31:21.600
 that culture and those, those rhythms and that to this world and created this whole

31:21.600 --> 31:24.040
 new thing, you know, American culture.

31:24.040 --> 31:26.840
 And so companies can't artificially create that.

31:26.840 --> 31:28.480
 They can't just say, Hey, we're here.

31:28.480 --> 31:29.480
 We're going to buy it up.

31:29.480 --> 31:31.440
 You got a partner, right?

31:31.440 --> 31:35.720
 And so, but anyway, you know, not to denigrate these companies are all trying and they should

31:35.720 --> 31:39.480
 and they, they are, I'm sure they're asking these questions and some of them are even

31:39.480 --> 31:44.160
 making an effort, but it is partly a respect the culture as you were a, as a technology

31:44.160 --> 31:49.880
 person, you got to blend your technology with cultural, with cultural, you know, meaning.

31:49.880 --> 31:55.040
 How much of a role do you think the algorithm machine learning has in connecting the consumer

31:55.040 --> 31:59.560
 to the creator sort of the recommender system aspect of this?

31:59.560 --> 32:00.560
 Yeah.

32:00.560 --> 32:01.560
 It's a great question.

32:01.560 --> 32:05.280
 I think pretty high recommend, you know, there's no magic in the algorithms, but a good

32:05.280 --> 32:10.360
 recommender system is way better than a bad recommender system and recommender systems

32:10.360 --> 32:15.200
 is a billion dollar industry back even, you know, 10, 20 years ago.

32:15.200 --> 32:17.520
 And it continues to be extremely important going forward.

32:17.520 --> 32:20.680
 What's your favorite recommender system just so we can put something well, just historically

32:20.680 --> 32:23.920
 I was one of the, you know, when I first went to Amazon, you know, I first didn't like

32:23.920 --> 32:27.160
 Amazon because they put the book people are out of business or the library, you know,

32:27.160 --> 32:30.360
 the local booksellers went out of business.

32:30.360 --> 32:33.920
 I've come to accept that there, you know, there probably are more books being sold now

32:33.920 --> 32:36.920
 and poor people reading them than ever before.

32:36.920 --> 32:39.440
 And then local book stores are coming back.

32:39.440 --> 32:41.520
 So you know, that's how economics sometimes work.

32:41.520 --> 32:44.280
 You go up and you go down.

32:44.280 --> 32:48.760
 But anyway, when I finally started going there and I bought a few books, I was really pleased

32:48.760 --> 32:52.400
 to see another few books being recommended to me that I never would have thought of.

32:52.400 --> 32:55.960
 And I bought a bunch of them, so they obviously had a good business model, but I learned things

32:55.960 --> 33:00.960
 and I still to this day kind of browse using that service.

33:00.960 --> 33:05.480
 And I think lots of people get a lot, you know, that is a good aspect of a recommendation

33:05.480 --> 33:06.480
 system.

33:06.480 --> 33:10.480
 I'm learning from my peers in an indirect way.

33:10.480 --> 33:13.880
 And their algorithms are not meant to have them impose what we learn.

33:13.880 --> 33:16.680
 It really is trying to find out what's in the data.

33:16.680 --> 33:20.080
 It doesn't work so well for other kind of entities, but that's just the complexity of human life

33:20.080 --> 33:21.080
 like shirts.

33:21.080 --> 33:26.440
 I'm not going to get recommendations on shirts, but that's interesting.

33:26.440 --> 33:32.160
 If you try to recommend restaurants, it's hard.

33:32.160 --> 33:35.400
 It's hard to do it at scale.

33:35.400 --> 33:42.080
 But a blend of recommendation systems with other economic ideas, matchings and so on

33:42.080 --> 33:46.080
 is really, really still very open, research wise, and there's new companies that are going

33:46.080 --> 33:47.920
 to emerge that do that well.

33:47.920 --> 33:53.840
 What do you think was going to the messy, difficult land of, say, politics and things

33:53.840 --> 33:58.440
 like that that YouTube and Twitter have to deal with in terms of recommendation systems?

33:58.440 --> 34:05.600
 Being able to suggest, I think Facebook just launched Facebook News, so having recommend

34:05.600 --> 34:08.840
 the kind of news that are most likely for you to be interesting.

34:08.840 --> 34:14.000
 Do you think this is AI solvable, again, whatever term you want to use, do you think it's a

34:14.000 --> 34:18.800
 solvable problem for machines or is it a deeply human problem that's unsolvable?

34:18.800 --> 34:20.280
 I don't even think about it at that level.

34:20.280 --> 34:25.400
 I think that what's broken with some of these companies, it's all monetization by advertising.

34:25.400 --> 34:27.000
 They're not at least Facebook.

34:27.000 --> 34:28.160
 I want to critique them.

34:28.160 --> 34:32.720
 They didn't really try to connect a producer and a consumer in an economic way.

34:32.720 --> 34:34.960
 No one wants to pay for anything.

34:34.960 --> 34:37.280
 They all started with Google and Facebook.

34:37.280 --> 34:41.640
 They went back to the playbook of the television companies back in the day.

34:41.640 --> 34:45.520
 Everyone wanted to pay for this signal, they will pay for the TV box, but not for the signal,

34:45.520 --> 34:48.000
 at least back in the day.

34:48.000 --> 34:51.000
 Advertising kind of filled that gap and advertising was new and interesting and it somehow didn't

34:51.000 --> 34:54.400
 take over our lives quite.

34:54.400 --> 35:01.080
 Fast forward, Google provides a service that people don't want to pay for.

35:01.080 --> 35:04.760
 Somewhat surprisingly in the 90s, they ended up making huge amounts to the corner of the

35:04.760 --> 35:05.760
 advertising market.

35:05.760 --> 35:08.400
 It didn't seem like that was going to happen, at least to me.

35:08.400 --> 35:11.600
 These little things on the right hand side of the screen just did not seem all that economically

35:11.600 --> 35:14.360
 interesting, but companies had maybe no other choice.

35:14.360 --> 35:18.280
 The TV market was going away and billboards and so on.

35:18.280 --> 35:19.920
 They got it.

35:19.920 --> 35:24.960
 I think that, sadly, that Google was doing so well with that and making it so much more.

35:24.960 --> 35:28.160
 They didn't think much more about how, wait a minute, is there a producer or consumer

35:28.160 --> 35:32.840
 relationship to be set up here, not just between us and the advertisers market to be created?

35:32.840 --> 35:34.840
 Is there an actual market between the producer and consumer?

35:34.840 --> 35:37.320
 There, the producer is the person who created that video clip.

35:37.320 --> 35:40.880
 The person that made that website, the person who couldn't make more such things, the person

35:40.880 --> 35:45.360
 who could adjust it as a function of demand, the person on the other side who's asking

35:45.360 --> 35:47.720
 for different kinds of things.

35:47.720 --> 35:51.960
 You see glimmers of that now, there's influencers and there's a little glimmering of a market,

35:51.960 --> 35:54.400
 but it should have been done 20 years ago, it should have been thought about.

35:54.400 --> 35:58.440
 It should have been created in parallel with the advertising ecosystem.

35:58.440 --> 36:03.800
 Then Facebook inherited that and I think they also didn't think very much about that.

36:03.800 --> 36:08.200
 Fast forward and now they are making huge amounts of money off of advertising and the

36:08.200 --> 36:11.560
 news thing and all these clicks is just feeding the advertising.

36:11.560 --> 36:13.760
 It's all connected up to the advertising.

36:13.760 --> 36:18.600
 You want more people to click on certain things because that money flows to you, Facebook.

36:18.600 --> 36:22.480
 You're very much incentivized to do that and when you start to find it's breaking, people

36:22.480 --> 36:25.280
 are telling you, well, we're getting into some troubles, you try to adjust it with your

36:25.280 --> 36:29.840
 smart AI algorithms and figure out what are bad clicks though, maybe shouldn't be clicked

36:29.840 --> 36:30.840
 through the radar.

36:30.840 --> 36:35.880
 I find that pretty much hopeless, it does get into all the complexity of human life

36:35.880 --> 36:40.600
 and you can try to fix it, you should, but you could also fix the whole business model

36:40.600 --> 36:44.560
 and the business model is that really, what are, are there some human producers and consumers

36:44.560 --> 36:45.560
 out there?

36:45.560 --> 36:48.720
 Is there some economic value to be liberated by connecting them directly?

36:48.720 --> 36:52.560
 Is it such that it's so valuable that people will be going to pay for it?

36:52.560 --> 36:53.560
 All right.

36:53.560 --> 36:55.160
 Like micro payment, like small payment.

36:55.160 --> 36:59.040
 Micro, but even after you micro, so I like the example, suppose I'm going, next week

36:59.040 --> 37:01.960
 I'm going to India, never been to India before, right?

37:01.960 --> 37:06.880
 I have a couple of days in, in Mumbai, I have no idea what to do there, right?

37:06.880 --> 37:10.040
 And I could go on the web right now and search, it's going to be kind of hopeless.

37:10.040 --> 37:14.840
 I'm not going to find, you know, I'll have lots of advertisers in my face, right?

37:14.840 --> 37:19.280
 What I really want to do is broadcast to the world that I am going to Mumbai and have someone

37:19.280 --> 37:23.960
 on the other side of a market look at me and, and there's a recommendation system there.

37:23.960 --> 37:26.640
 So they're not looking at all possible people coming to Mumbai, they're looking at the people

37:26.640 --> 37:27.640
 who are relevant to them.

37:27.640 --> 37:32.480
 So someone in my age group, someone who kind of knows me in some level, I give up a little

37:32.480 --> 37:33.480
 privacy by that.

37:33.480 --> 37:36.120
 But I'm happy because what I'm going to get back is this person can make a little video

37:36.120 --> 37:39.880
 for me or they're going to write a little two page paper on, here's the cool things

37:39.880 --> 37:43.160
 that you want to do and move by this week, especially, right?

37:43.160 --> 37:44.160
 I'm going to look at that.

37:44.160 --> 37:45.160
 I'm not going to pay a micro payment.

37:45.160 --> 37:47.960
 I'm going to pay, you know, a hundred dollars or whatever for that.

37:47.960 --> 37:48.960
 It's real value.

37:48.960 --> 37:49.960
 It's like journalism.

37:49.960 --> 37:54.880
 And as an odd subscription, it's that I'm going to pay that person in that moment.

37:54.880 --> 37:57.760
 I mean, it's going to take 5% of that and that person has now got it.

37:57.760 --> 38:01.000
 It's a gig economy, if you will, but, you know, done for it, you know, thinking about

38:01.000 --> 38:05.000
 a little bit behind YouTube, there was actually people who could make more of those things.

38:05.000 --> 38:07.960
 If they were connected to a market, they would make more of those things independently.

38:07.960 --> 38:08.960
 You don't have to tell them what to do.

38:08.960 --> 38:12.680
 You don't have to incentivize them in any other way.

38:12.680 --> 38:15.760
 And so yeah, these companies, I don't think have thought long, long and heard about that.

38:15.760 --> 38:19.840
 So I do distinguish on, you know, Facebook on the one side who just not thought about

38:19.840 --> 38:20.840
 these things at all.

38:20.840 --> 38:25.200
 They were thinking that AI will fix everything and Amazon thinks about them all the time

38:25.200 --> 38:26.520
 because they were already out in the real world.

38:26.520 --> 38:28.040
 They were delivering packages to people's doors.

38:28.040 --> 38:29.400
 They were worried about a market.

38:29.400 --> 38:32.600
 They were worried about sellers and, you know, they worry and some things they do are great.

38:32.600 --> 38:36.440
 Some things maybe not so great, but, you know, they're in that business model.

38:36.440 --> 38:38.480
 And then I'd say Google sort of hovers somewhere in between.

38:38.480 --> 38:41.440
 I don't think for a long, long time they got it.

38:41.440 --> 38:46.640
 I think they probably see that YouTube is more pregnant with possibility than they might

38:46.640 --> 38:49.920
 have thought and that they're probably heading that direction.

38:49.920 --> 38:54.000
 But you know, Silicon Valley has been dominated by the Google, Facebook kind of mentality

38:54.000 --> 38:58.800
 and the subscription and advertising and that is, that's the core problem, right?

38:58.800 --> 39:03.640
 The fake news actually rides on top of that because it means that you're monetizing with

39:03.640 --> 39:05.600
 clip through rate and that is the core problem.

39:05.600 --> 39:06.880
 You got to remove that.

39:06.880 --> 39:11.200
 So advertisement, if we're going to linger on that, I mean, that's an interesting thesis.

39:11.200 --> 39:15.080
 I don't know if everyone really deeply thinks about that.

39:15.080 --> 39:16.720
 So you're right.

39:16.720 --> 39:20.720
 The thought is the advertisement model is the only thing we have, the only thing we'll

39:20.720 --> 39:21.720
 ever have.

39:21.720 --> 39:30.200
 So we have to fix, we have to build algorithms that despite that business model, you know,

39:30.200 --> 39:34.680
 find the better angels of our nature and do good by society and by the individual.

39:34.680 --> 39:40.000
 But you think we can slowly, you think, first of all, there's a difference between should

39:40.000 --> 39:41.000
 and could.

39:41.000 --> 39:46.560
 So you're saying we should slowly move away from the advertising model and have a direct

39:46.560 --> 39:49.920
 connection between the consumer and the creator.

39:49.920 --> 39:55.240
 The question I also have is, can we, because the advertising model is so successful now

39:55.240 --> 40:00.400
 in terms of just making a huge amount of money and therefore being able to build a big company

40:00.400 --> 40:04.000
 that provides, has really smart people working that create a good service.

40:04.000 --> 40:05.680
 Do you think it's possible?

40:05.680 --> 40:07.920
 And just to clarify, you think we should move away?

40:07.920 --> 40:08.920
 Well, I think we should.

40:08.920 --> 40:09.920
 Yeah.

40:09.920 --> 40:10.920
 But we is the, you know, me.

40:10.920 --> 40:11.920
 Society.

40:11.920 --> 40:12.920
 Yeah.

40:12.920 --> 40:13.920
 Well, the companies.

40:13.920 --> 40:15.280
 I mean, so first of all, full disclosure.

40:15.280 --> 40:18.320
 I'm doing a day a week at Amazon because I kind of want to learn more about how they

40:18.320 --> 40:19.320
 do things.

40:19.320 --> 40:22.720
 So, you know, I'm not speaking for Amazon in any way, but, you know, I did go there

40:22.720 --> 40:26.200
 because I actually believe they get a little bit of this or trying to create these markets.

40:26.200 --> 40:29.640
 And they don't really use, advertising is not a crucial part of Amazon.

40:29.640 --> 40:30.640
 That's a good question.

40:30.640 --> 40:34.640
 So it has become not crucial, but it's become more and more present if you go to Amazon

40:34.640 --> 40:35.640
 website.

40:35.640 --> 40:38.800
 And, you know, without revealing too many deep secrets about Amazon, I can tell you

40:38.800 --> 40:42.720
 that, you know, a lot of people come to question this and there's a huge questioning going

40:42.720 --> 40:43.720
 on.

40:43.720 --> 40:45.640
 You do not want a world where there's zero advertising.

40:45.640 --> 40:47.200
 That actually is a bad world.

40:47.200 --> 40:48.200
 Okay.

40:48.200 --> 40:49.320
 So here's the way to think about it.

40:49.320 --> 40:55.000
 You're a company that like Amazon is trying to bring products to customers, right?

40:55.000 --> 40:58.440
 And the customer and you get more, you want to buy a vacuum cleaner, say, you want to

40:58.440 --> 40:59.440
 know what's available for me.

40:59.440 --> 41:00.840
 And, you know, it's not going to be that obvious.

41:00.840 --> 41:02.160
 You have to do a little bit of work at it.

41:02.160 --> 41:04.600
 The recommendation system will sort of help, right?

41:04.600 --> 41:08.080
 But now suppose this other person over here has just made the world, you know, they spend

41:08.080 --> 41:09.080
 a huge amount of energy.

41:09.080 --> 41:10.080
 They had a great idea.

41:10.080 --> 41:11.080
 They made a great vacuum cleaner.

41:11.080 --> 41:12.400
 They know they really did it.

41:12.400 --> 41:13.400
 They nailed it.

41:13.400 --> 41:16.680
 They, you know, whiz kid that made a great new vacuum cleaner, right?

41:16.680 --> 41:18.240
 It's not going to be in the recommendation system.

41:18.240 --> 41:19.240
 No one will know about it.

41:19.240 --> 41:22.360
 The algorithms will not find it and AI will not fix that.

41:22.360 --> 41:23.360
 Okay.

41:23.360 --> 41:24.360
 At all.

41:24.360 --> 41:25.360
 Right.

41:25.360 --> 41:28.960
 How do you allow that vacuum cleaner to start to get in front of people?

41:28.960 --> 41:29.960
 Be sold.

41:29.960 --> 41:34.480
 Well, advertising and here what advertising is, it's a signal that you're, you believe in

41:34.480 --> 41:37.480
 your product enough that you're willing to pay some real money for it.

41:37.480 --> 41:41.480
 And to me as a consumer, I look at that signal, I say, well, first of all, I know these are

41:41.480 --> 41:44.960
 not just cheap little ads because we have now right now that I know that, you know, these

41:44.960 --> 41:47.760
 are super cheap, you know, pennies.

41:47.760 --> 41:51.120
 If I see an ad where it's actually, I know the company is only doing a few of these and

41:51.120 --> 41:54.920
 they're making real money is kind of flowing and I see an ad, I may pay more attention

41:54.920 --> 41:55.920
 to it.

41:55.920 --> 42:01.600
 And I actually might want that because I see, Hey, that guy spent money on his vacuum cleaner.

42:01.600 --> 42:02.600
 Maybe there's something good there.

42:02.600 --> 42:03.600
 So I will look at it.

42:03.600 --> 42:06.600
 And so that's part of the overall information flow in a good market.

42:06.600 --> 42:09.440
 So advertising has a role.

42:09.440 --> 42:13.640
 But the problem is, of course, that that signal is now completely gone because it just, you

42:13.640 --> 42:17.400
 know, dominated by these tiny little things that add up to big money for the company.

42:17.400 --> 42:22.280
 You know, so I think it will just, I think it will change because societies just don't,

42:22.280 --> 42:26.000
 you know, stick with things that annoy a lot of people and advertising currently annoys

42:26.000 --> 42:28.480
 people more than it provides information.

42:28.480 --> 42:32.240
 And I think that a Google probably is smart enough to figure out that this is a dead, this

42:32.240 --> 42:35.760
 is a bad model, even though it's a hard huge amount of money and they'll have to figure

42:35.760 --> 42:39.920
 out how to pull it away from it slowly, and I'm sure the CEO there will figure it out.

42:39.920 --> 42:44.200
 But they need to do it and they needed it.

42:44.200 --> 42:47.680
 So if you reduce advertising, not to zero, but you reduce it at the same time you bring

42:47.680 --> 42:51.600
 up producer, consumer, actual real value being delivered.

42:51.600 --> 42:54.720
 So real money is being paid and they take a 5% cut.

42:54.720 --> 42:58.840
 That 5% could start to get big enough to cancel out the lost revenue from the kind of the

42:58.840 --> 42:59.840
 poor kind of advertising.

42:59.840 --> 43:04.760
 And I think that a good company will do that, will realize that.

43:04.760 --> 43:08.440
 And they're, you know, Facebook, you know, again, God bless them.

43:08.440 --> 43:14.320
 They bring, you know, grandmothers, you know, they bring children's pictures into grandmothers

43:14.320 --> 43:15.320
 lives.

43:15.320 --> 43:17.320
 It's fantastic.

43:17.320 --> 43:22.440
 But they need to think of a new business model and that's the core problem there.

43:22.440 --> 43:26.440
 Until they start to connect producer, consumer, I think they will just continue to make money

43:26.440 --> 43:30.040
 and then buy the next social network company and then buy the next one.

43:30.040 --> 43:34.880
 And the innovation level will not be high and the health issues will not go away.

43:34.880 --> 43:37.920
 So I apologize that we kind of returned to words.

43:37.920 --> 43:46.040
 I don't think the exact terms matter, but in sort of defensive advertisement, don't

43:46.040 --> 43:56.120
 you think the kind of direct connection between consumer and creator, producer is the best,

43:56.120 --> 44:00.960
 like the, is what advertisement strives to do, right?

44:00.960 --> 44:06.680
 So it is best advertisement is literally now the Facebook is listening to our conversation

44:06.680 --> 44:11.400
 and heard that you're going to India and we'll be able to actually start automatically for

44:11.400 --> 44:14.520
 you making these connections and start giving this offer.

44:14.520 --> 44:19.800
 So like, I apologize if it's just a matter of terms, but just to draw a distinction,

44:19.800 --> 44:23.000
 is it possible to make advertisements just better and better and better algorithmically

44:23.000 --> 44:26.040
 to where it actually becomes a connection almost a direct connection?

44:26.040 --> 44:27.040
 That's a good question.

44:27.040 --> 44:31.880
 So let's put it on that first of all, what we just talked about, I was defending advertising.

44:31.880 --> 44:32.880
 Okay.

44:32.880 --> 44:36.280
 So I was defending it as a way to get signals into a market that don't come any other way,

44:36.280 --> 44:37.720
 especially algorithmically.

44:37.720 --> 44:41.640
 It's a sign that someone spent money on it is a sign they think it's valuable.

44:41.640 --> 44:45.040
 And if I think that if other things, someone else thinks it's valuable, then if I trust

44:45.040 --> 44:47.360
 other people, I might be willing to listen.

44:47.360 --> 44:51.840
 I don't trust that Facebook though is who's an intermediary between this.

44:51.840 --> 44:54.720
 I don't think they care about me.

44:54.720 --> 44:55.720
 Okay.

44:55.720 --> 44:56.720
 I don't think they do.

44:56.720 --> 45:01.080
 And I find it creepy that they know I'm going to India next week because of our conversation.

45:01.080 --> 45:02.080
 Why do you think that?

45:02.080 --> 45:07.120
 Can we just put your PR hat on?

45:07.120 --> 45:14.200
 Why do you think you find Facebook creepy and not trust them as do majority of the population?

45:14.200 --> 45:16.280
 So they're out of the Silicon Valley companies.

45:16.280 --> 45:21.120
 I saw like, not approval rate, but there's ranking of how much people trust companies

45:21.120 --> 45:23.080
 and Facebook is in the gutter.

45:23.080 --> 45:25.600
 In the gutter, including people inside of Facebook.

45:25.600 --> 45:28.000
 So what do you attribute that to?

45:28.000 --> 45:29.000
 Because when I...

45:29.000 --> 45:30.000
 Come on.

45:30.000 --> 45:32.240
 You don't find it creepy that right now we're talking that I might walk out on the street

45:32.240 --> 45:36.000
 right now that some unknown person who I don't know kind of comes up to me and says,

45:36.000 --> 45:37.500
 I hear you going to India.

45:37.500 --> 45:38.880
 I mean, that's not even Facebook.

45:38.880 --> 45:39.880
 That's just a...

45:39.880 --> 45:42.560
 I want transparency in human society.

45:42.560 --> 45:43.560
 I want to have...

45:43.560 --> 45:47.080
 If you know something about me, there's actually some reason you know something about me.

45:47.080 --> 45:51.560
 It's something that if I look at it later and audit it kind of, I approve.

45:51.560 --> 45:54.560
 You know something about me because you care in some way.

45:54.560 --> 45:58.240
 There's a caring relationship even or an economic one or something.

45:58.240 --> 46:02.000
 Not just that you're someone who could exploit it in ways I don't know about or care about

46:02.000 --> 46:05.240
 or I'm troubled by or whatever.

46:05.240 --> 46:09.640
 And we're in a world right now where that happened way too much and that Facebook knows

46:09.640 --> 46:14.720
 things about a lot of people and could exploit it and does exploit it at times.

46:14.720 --> 46:16.880
 I think most people do find that creepy.

46:16.880 --> 46:17.880
 It's not for them.

46:17.880 --> 46:18.880
 It's not...

46:18.880 --> 46:23.440
 Facebook does not do it because they care about them in any real sense.

46:23.440 --> 46:24.440
 And they shouldn't.

46:24.440 --> 46:26.760
 They should not be a big brother caring about us.

46:26.760 --> 46:28.840
 That is not the role of a company like that.

46:28.840 --> 46:29.840
 Why not?

46:29.840 --> 46:32.200
 Wait, not the big brother part but the caring, the trust thing.

46:32.200 --> 46:34.800
 I mean, don't those companies...

46:34.800 --> 46:38.400
 Just to linger on it because a lot of companies have a lot of information about us.

46:38.400 --> 46:43.160
 I would argue that there's companies like Microsoft that has more information about us

46:43.160 --> 46:46.040
 than Facebook does and yet we trust Microsoft more.

46:46.040 --> 46:47.560
 But Microsoft is pivoting.

46:47.560 --> 46:51.400
 Microsoft has decided this is really important.

46:51.400 --> 46:53.360
 We don't want to do creepy things.

46:53.360 --> 46:56.760
 Really want people to trust us to actually only use information in ways that they really

46:56.760 --> 47:00.400
 would approve of, that we don't decide.

47:00.400 --> 47:06.680
 And I'm just kind of adding that the health of a market is that when I connect to someone

47:06.680 --> 47:10.200
 who produced or consumers, not just a random producer or consumer, it's people who see

47:10.200 --> 47:11.200
 each other.

47:11.200 --> 47:14.440
 They don't like each other but they sense that if they transact, some happiness will

47:14.440 --> 47:16.000
 go up on both sides.

47:16.000 --> 47:22.840
 If a company helps me to do that and moments that I choose of my choosing, then fine.

47:22.840 --> 47:28.560
 So and also think about the difference between browsing versus buying, right?

47:28.560 --> 47:31.720
 There are moments in my life, I just want to buy a gadget or something.

47:31.720 --> 47:33.080
 I need something for that moment.

47:33.080 --> 47:37.360
 I need some ammonia for my house or something because I got a problem in this bill.

47:37.360 --> 47:38.360
 I want to just go in.

47:38.360 --> 47:40.040
 I don't want to be advertised at that moment.

47:40.040 --> 47:42.440
 I don't want to be led down very straight.

47:42.440 --> 47:43.440
 That's annoying.

47:43.440 --> 47:49.040
 Let's just go and have it extremely easy to do what I want.

47:49.040 --> 47:52.440
 Other moments I might say, no, it's like today I'm going to the shopping mall.

47:52.440 --> 47:55.560
 I want to walk around and see things and see people and be exposed to stuff.

47:55.560 --> 47:56.800
 So I want control over that though.

47:56.800 --> 48:00.200
 I don't want the company's algorithms to decide for me, right?

48:00.200 --> 48:01.200
 And I think that's the thing.

48:01.200 --> 48:05.360
 It's a total loss of control if Facebook thinks they should take the control from us of deciding

48:05.360 --> 48:09.200
 when we want to have certain kinds of information, when we don't, what information that is, how

48:09.200 --> 48:12.480
 much it relates to what they know about us that we didn't really want them to know about

48:12.480 --> 48:13.480
 us.

48:13.480 --> 48:15.840
 I don't want them to be helping me in that way.

48:15.840 --> 48:21.640
 I don't want them to be helping them by they decide they have control over what I want

48:21.640 --> 48:22.640
 and when.

48:22.640 --> 48:23.640
 I totally agree.

48:23.640 --> 48:28.560
 So Facebook, by the way, I have this optimistic thing where I think Facebook has the kind

48:28.560 --> 48:32.480
 of personal information about us that could create a beautiful thing.

48:32.480 --> 48:36.200
 So I'm really optimistic of what Facebook could do.

48:36.200 --> 48:38.680
 It's not what it's doing, but what it could do.

48:38.680 --> 48:43.400
 So I don't see that, I think that optimism is misplaced because you have to have a business

48:43.400 --> 48:45.400
 model behind these things.

48:45.400 --> 48:48.480
 Create a beautiful thing is really, let's be clear.

48:48.480 --> 48:51.400
 It's about something that people would value.

48:51.400 --> 48:53.840
 And I don't think they have that business model.

48:53.840 --> 48:58.920
 And I don't think they will suddenly discover it by what, you know, a long hot shower.

48:58.920 --> 48:59.920
 I disagree.

48:59.920 --> 49:04.840
 I disagree in terms of, you can discover a lot of amazing things in a shower.

49:04.840 --> 49:06.240
 So I didn't say that.

49:06.240 --> 49:07.240
 I said they won't come.

49:07.240 --> 49:08.240
 They won't.

49:08.240 --> 49:11.280
 But in the shower, I think a lot of other people will discover it.

49:11.280 --> 49:15.240
 I think that this guy, so I should also full disclosure, there's a company called United

49:15.240 --> 49:19.080
 Masters, which I'm on their board and they've created this music market and they have 100,000

49:19.080 --> 49:23.520
 artists now signed on and they've done things like gone to the NBA and the NBA, the music

49:23.520 --> 49:26.960
 you find behind NBA Eclipse right now is their music, right?

49:26.960 --> 49:31.920
 That's a company that had the right business model in mind from the get go, right, executed

49:31.920 --> 49:32.920
 on that.

49:32.920 --> 49:37.200
 And from day one, there was value brought to, so here you have a kid who made some songs

49:37.200 --> 49:41.240
 who suddenly their songs are on the NBA website, right?

49:41.240 --> 49:43.440
 That's really economic value to people.

49:43.440 --> 49:45.480
 And so, you know.

49:45.480 --> 49:52.520
 So you and I differ on the optimism of being able to sort of change the direction of the

49:52.520 --> 49:54.400
 Titanic, right?

49:54.400 --> 49:55.400
 So I.

49:55.400 --> 49:56.400
 Yeah.

49:56.400 --> 49:57.400
 I'm older than you.

49:57.400 --> 50:00.400
 So I think the Titanic's crash.

50:00.400 --> 50:01.400
 Got it.

50:01.400 --> 50:06.000
 But just to elaborate, because I totally agree with you and I just want to know how difficult

50:06.000 --> 50:12.600
 you think this problem is of, so for example, I want to read some news and I would, there's

50:12.600 --> 50:17.880
 a lot of times in the day where something makes me either smile or think in a way where

50:17.880 --> 50:20.840
 I like consciously think this really gave me value.

50:20.840 --> 50:26.480
 Like I sometimes listen to the daily podcast in the New York Times, way better than the

50:26.480 --> 50:29.320
 New York Times themselves, by the way, for people listening.

50:29.320 --> 50:32.600
 That's like real journalism is happening for some reason in the podcast space.

50:32.600 --> 50:33.800
 It doesn't make sense to me.

50:33.800 --> 50:39.200
 But often I listen to it 20 minutes and I would be willing to pay for that, like $5,

50:39.200 --> 50:44.120
 $10 for that experience and how difficult.

50:44.120 --> 50:48.240
 That's kind of what you're getting at is that little transaction.

50:48.240 --> 50:52.680
 How difficult is it to create a frictionless system like Uber has, for example, for other

50:52.680 --> 50:53.680
 things?

50:53.680 --> 50:55.320
 What's your intuition there?

50:55.320 --> 50:58.720
 So first of all, I pay little bits of money to, you know, to send there's something called

50:58.720 --> 51:00.360
 courts that does financial things.

51:00.360 --> 51:04.520
 I like medium as a site, I don't pay there, but I would.

51:04.520 --> 51:06.360
 You had a great post on medium.

51:06.360 --> 51:10.280
 I would have loved to pay you a dollar and not others.

51:10.280 --> 51:15.560
 I wouldn't have wanted it per se because there should be also sites where that's not actually

51:15.560 --> 51:16.560
 the goal.

51:16.560 --> 51:20.240
 The goal is to actually have a broadcast channel that I monetize in some other way if I chose

51:20.240 --> 51:21.240
 to.

51:21.240 --> 51:22.480
 I mean, I could now.

51:22.480 --> 51:23.480
 People know about it.

51:23.480 --> 51:24.480
 I could.

51:24.480 --> 51:26.360
 I'm not doing it, but that's fine with me.

51:26.360 --> 51:29.760
 Also the musicians who are making all this music, I don't think the right model is that

51:29.760 --> 51:32.880
 you pay a little subscription fee to them, right?

51:32.880 --> 51:35.880
 Because people can copy the bits too easily and it's just not that somewhere the value

51:35.880 --> 51:36.880
 is.

51:36.880 --> 51:39.440
 The value is that a connection was made with real between real human beings, then you can

51:39.440 --> 51:41.440
 follow up on that, right?

51:41.440 --> 51:42.960
 And create yet more value.

51:42.960 --> 51:46.560
 So no, I think there's a lot of open questions here.

51:46.560 --> 51:50.120
 Hot open questions, but also, yeah, I do want good recommendation systems that recommend

51:50.120 --> 51:51.360
 cool stuff to me.

51:51.360 --> 51:52.360
 But it's pretty hard, right?

51:52.360 --> 51:55.840
 I don't like them to recommend stuff just based on my browsing history.

51:55.840 --> 51:59.000
 I don't like that they're based on stuff they know about me, quote unquote.

51:59.000 --> 52:00.840
 That's unknown about me is the most interesting.

52:00.840 --> 52:03.600
 So this is the really interesting question.

52:03.600 --> 52:05.840
 We may disagree, maybe not.

52:05.840 --> 52:12.120
 I think that I love recommender systems and I want to give them everything about me in

52:12.120 --> 52:13.400
 a way that I trust.

52:13.400 --> 52:14.840
 Yeah, but you don't.

52:14.840 --> 52:19.920
 Because so for example, this morning I clicked on, you know, I was pretty sleepy this morning.

52:19.920 --> 52:24.360
 I clicked on a story about the Queen of England, right?

52:24.360 --> 52:26.400
 I do not give a damn about the Queen of England.

52:26.400 --> 52:27.520
 I really do not.

52:27.520 --> 52:28.520
 But it was clickbait.

52:28.520 --> 52:31.520
 It kind of looked funny and I had to say, what the heck are they talking about there?

52:31.520 --> 52:34.000
 I don't want to have my life, you know, heading that direction.

52:34.000 --> 52:36.160
 Now that's in my browsing history.

52:36.160 --> 52:40.280
 The system and any reasonable system will think that I care about Queen of England.

52:40.280 --> 52:41.280
 But that's browsing history.

52:41.280 --> 52:44.640
 Right, but you're saying all the trace, all the digital exhaust or whatever, that's been

52:44.640 --> 52:45.640
 kind of the models.

52:45.640 --> 52:48.520
 If you collect all this stuff, you're going to figure all of us out.

52:48.520 --> 52:51.200
 Well, if you're trying to figure out like kind of one person, like Trump or something,

52:51.200 --> 52:52.680
 maybe you could figure him out.

52:52.680 --> 52:58.000
 But if you're trying to figure out, you know, 500 million people, you know, no way, no way.

52:58.000 --> 52:59.000
 Do you think so?

52:59.000 --> 53:00.000
 No, I do.

53:00.000 --> 53:01.000
 I think so.

53:01.000 --> 53:02.600
 I think we are humans are just amazingly rich and complicated.

53:02.600 --> 53:04.080
 Every one of us has our little quirks.

53:04.080 --> 53:06.880
 Everyone else has our little things that could intrigue us, that we don't even know and will

53:06.880 --> 53:07.880
 intrigue us.

53:07.880 --> 53:10.000
 And there's no sign of it in our past.

53:10.000 --> 53:12.960
 But by God, there it comes and, you know, you fall in love with it.

53:12.960 --> 53:15.840
 And I don't want a company trying to figure that out for me and anticipate that.

53:15.840 --> 53:21.280
 Okay, well, I want them to provide a forum, a market, a place that I kind of go and by

53:21.280 --> 53:23.320
 hook or by crook, this happens.

53:23.320 --> 53:26.440
 You know, I'm walking down the street and I hear some Chilean music being played and

53:26.440 --> 53:28.080
 I never knew I liked Chilean music.

53:28.080 --> 53:29.080
 Wow.

53:29.080 --> 53:33.680
 So there is that side and I want them to provide a limited but, you know, interesting place

53:33.680 --> 53:34.680
 to go.

53:34.680 --> 53:35.680
 Right.

53:35.680 --> 53:39.760
 And so don't try to use your AI to kind of, you know, figure me out and then put me in

53:39.760 --> 53:45.120
 a world where you figured me out, you know, no, create huge spaces for human beings where

53:45.120 --> 53:50.080
 our creativity and our style will be enriched and come forward and it'll be a lot of more

53:50.080 --> 53:51.080
 transparency.

53:51.080 --> 53:55.480
 I won't have people randomly, anonymously putting comments up and especially based on

53:55.480 --> 54:00.080
 stuff they know about me, facts that, you know, we are so broken right now.

54:00.080 --> 54:02.920
 If you're, you know, especially if you're a celebrity, but, you know, it's about anybody

54:02.920 --> 54:06.640
 that anonymous people are hurting lots and lots of people right now.

54:06.640 --> 54:10.080
 And that's part of this thing that Silicon Valley is thinking that, you know, just collect

54:10.080 --> 54:12.520
 all this information and use it in a great way.

54:12.520 --> 54:16.440
 So, you know, I'm not a pessimist, I'm very much an optimist, my nature, but I think that's

54:16.440 --> 54:19.960
 just been the wrong path for the whole technology to take.

54:19.960 --> 54:24.040
 Be more limited, create, let humans rise up.

54:24.040 --> 54:25.760
 Don't try to replace them.

54:25.760 --> 54:26.760
 That's the AI mantra.

54:26.760 --> 54:28.680
 Don't try to anticipate them.

54:28.680 --> 54:32.920
 Don't try to predict them because you're not going to, you're not going to be able to do

54:32.920 --> 54:33.920
 those things.

54:33.920 --> 54:34.920
 You're going to make things worse.

54:34.920 --> 54:35.920
 Okay.

54:35.920 --> 54:38.760
 So, right now, just give this a chance.

54:38.760 --> 54:43.840
 Right now, the recommender systems are the creepy people in the shadow watching your

54:43.840 --> 54:45.520
 every move.

54:45.520 --> 54:47.800
 So they're looking at traces of you.

54:47.800 --> 54:53.000
 They're not directly interacting with you, sort of your close friends and family, the

54:53.000 --> 54:57.400
 way they know you is by having conversation, by actually having interactions back and forth.

54:57.400 --> 55:02.360
 Do you think there's a place for recommender systems, sort of to step, because you just

55:02.360 --> 55:04.520
 emphasize the value of human to human connection.

55:04.520 --> 55:07.840
 But yeah, let's give it a chance, AI human connection.

55:07.840 --> 55:13.160
 Is there a role for an AI system to have conversations with you in terms of, to try

55:13.160 --> 55:16.840
 to figure out what kind of music you like, not by just watching what you listen to, but

55:16.840 --> 55:19.640
 actually having a conversation, natural language or otherwise.

55:19.640 --> 55:20.640
 Yeah.

55:20.640 --> 55:24.160
 So I'm not against this, I just wanted to push back against the, maybe you were saying,

55:24.160 --> 55:25.160
 you have options for Facebook.

55:25.160 --> 55:27.160
 So there I think it's misplaced.

55:27.160 --> 55:28.660
 But I think that distributing...

55:28.660 --> 55:30.160
 I'm the one that's depending on Facebook.

55:30.160 --> 55:31.160
 Yeah.

55:31.160 --> 55:32.160
 No.

55:32.160 --> 55:33.160
 So good for you.

55:33.160 --> 55:34.160
 Go for it.

55:34.160 --> 55:35.160
 That's a hard spot to be.

55:35.160 --> 55:36.160
 Yeah.

55:36.160 --> 55:37.160
 No.

55:37.160 --> 55:38.160
 Good.

55:38.160 --> 55:39.720
 Human interaction, like on our daily, the context around me in my own home is something that

55:39.720 --> 55:41.360
 I don't want some big company to know about at all.

55:41.360 --> 55:44.200
 But I would be more than happy to have technology help me with it.

55:44.200 --> 55:45.200
 Which kind of technology?

55:45.200 --> 55:46.200
 Well, you know, just...

55:46.200 --> 55:47.200
 Alexa.

55:47.200 --> 55:48.200
 Amazon.

55:48.200 --> 55:49.200
 Well, Alexa's done right.

55:49.200 --> 55:52.200
 I think Alexa's a research platform right now more than anything else.

55:52.200 --> 55:56.520
 But Alexa done right, you know, could do things like I leave the water running in my garden

55:56.520 --> 55:59.240
 and I say, hey, Alexa, the water's running in my garden.

55:59.240 --> 56:02.080
 And even have Alexa figure out that that means when my wife comes home that she should be

56:02.080 --> 56:03.080
 told about that.

56:03.080 --> 56:04.800
 That's a little bit of a reasoning.

56:04.800 --> 56:08.440
 I would call that AI and by any kind of stretch, it's a little bit of reasoning.

56:08.440 --> 56:11.040
 And it actually kind of would make my life a little easier and better.

56:11.040 --> 56:15.760
 And you know, I wouldn't call this a wow moment, but I kind of think that overall rice is human

56:15.760 --> 56:18.360
 happiness up to have that kind of thing.

56:18.360 --> 56:20.960
 And not when you're lonely, Alexa knowing loneliness.

56:20.960 --> 56:25.640
 No, no, I don't want Alexa to feel intrusive.

56:25.640 --> 56:28.480
 And I don't want just the designer of the system to kind of work all this out.

56:28.480 --> 56:32.480
 I really want to have a lot of control and I want transparency and control.

56:32.480 --> 56:36.800
 And if the company can stand up and give me that in the context of technology, I think

56:36.800 --> 56:39.360
 they're going to first of all be way more successful than our current generation.

56:39.360 --> 56:43.280
 And like I said, I was measuring Microsoft, you know, I really think they're pivoting

56:43.280 --> 56:45.200
 to kind of be the trusted old uncle.

56:45.200 --> 56:49.280
 You know, I think that they get that this is a way to go, that if you let people find

56:49.280 --> 56:53.920
 technology empowers them to have more control and have control, not just over privacy, but

56:53.920 --> 56:58.120
 over this rich set of interactions, that that people are going to like that a lot more.

56:58.120 --> 57:00.600
 And that's, that's the right business model going forward.

57:00.600 --> 57:02.280
 What does control over privacy look like?

57:02.280 --> 57:04.840
 Do you think you should be able to just view all the data that?

57:04.840 --> 57:06.000
 No, it's much more than that.

57:06.000 --> 57:07.920
 I mean, first of all, it should be an individual decision.

57:07.920 --> 57:09.240
 Some people don't want privacy.

57:09.240 --> 57:10.760
 They want their whole life out there.

57:10.760 --> 57:13.760
 Other people's want it.

57:13.760 --> 57:16.080
 Privacy is not a zero one.

57:16.080 --> 57:17.080
 It's not a legal thing.

57:17.080 --> 57:20.320
 It's not just about which date is available, which is not.

57:20.320 --> 57:24.880
 I like to recall to people that, you know, a couple of hundred years ago, everyone, there

57:24.880 --> 57:26.240
 was not really big cities.

57:26.240 --> 57:30.040
 Everyone lived in on the countryside and villages and in villages, everybody knew everything

57:30.040 --> 57:31.040
 about you.

57:31.040 --> 57:32.760
 Very, you didn't have any privacy.

57:32.760 --> 57:33.760
 Is that bad?

57:33.760 --> 57:34.760
 Are we better off now?

57:34.760 --> 57:38.880
 Well, you know, arguably no, because what did you get for that loss of at least certain

57:38.880 --> 57:40.240
 kinds of privacy?

57:40.240 --> 57:44.120
 Well, people helped each other because they know everything about you.

57:44.120 --> 57:45.440
 They know something bad's happening.

57:45.440 --> 57:46.440
 They will help you with that.

57:46.440 --> 57:47.440
 Right?

57:47.440 --> 57:48.440
 And now you live in a big city, no one knows the amount.

57:48.440 --> 57:50.880
 You get no help.

57:50.880 --> 57:52.720
 So it kind of depends the answer.

57:52.720 --> 57:56.360
 I want certain people who I trust and there should be relationships.

57:56.360 --> 57:59.040
 I should kind of manage all those, but who knows what about me?

57:59.040 --> 58:00.800
 I should have some agency there.

58:00.800 --> 58:04.720
 It shouldn't be a drift in a city of technology where I have no agency.

58:04.720 --> 58:08.600
 I don't want to go reading things and checking boxes.

58:08.600 --> 58:09.920
 So I don't know how to do that.

58:09.920 --> 58:11.880
 And I'm not a privacy researcher per se.

58:11.880 --> 58:14.120
 I recognize the vast complexity of this.

58:14.120 --> 58:15.120
 It's not just technology.

58:15.120 --> 58:18.920
 It's not just legal scholars meeting technologists.

58:18.920 --> 58:20.920
 There's got to be kind of a whole layers around it.

58:20.920 --> 58:25.760
 And so when I alluded to this emerging engineering field, this is a big part of it.

58:25.760 --> 58:29.760
 Well, like an electrical engineering come game, I'm not running around in the time,

58:29.760 --> 58:34.120
 but you just didn't plug electricity into walls and all kind of work.

58:34.120 --> 58:37.480
 You know, I had to have like underwriters laboratory that reassured you that that plug

58:37.480 --> 58:41.640
 is not going to burn up your house and that that machine will do this and that and everything.

58:41.640 --> 58:44.440
 There'll be whole people who can install things.

58:44.440 --> 58:46.240
 There'll be people who can watch the installers.

58:46.240 --> 58:49.720
 There'll be a whole layers, you know, an onion of these kind of things.

58:49.720 --> 58:55.800
 And for things as deep and interesting as privacy, which is as least as interested as electricity,

58:55.800 --> 58:58.920
 that's going to take decades to kind of work out, but it's going to require a lot of new

58:58.920 --> 59:00.200
 structures that we don't have right now.

59:00.200 --> 59:02.080
 So it's kind of hard to talk about it.

59:02.080 --> 59:04.680
 And you're saying there's a lot of money to be made if you get it right.

59:04.680 --> 59:06.920
 So I should look at a lot of money to be made.

59:06.920 --> 59:10.600
 And all these things that provide human services and people recognize them as useful parts

59:10.600 --> 59:12.360
 of their lives.

59:12.360 --> 59:14.240
 So yeah.

59:14.240 --> 59:19.640
 So yeah, the dialogue sometimes goes from the exuberant technologists to the no technology

59:19.640 --> 59:20.760
 is good kind of.

59:20.760 --> 59:24.440
 And that's, you know, in a public discourse, you know, in newsrooms, you see too much of

59:24.440 --> 59:26.240
 this kind of thing.

59:26.240 --> 59:29.320
 And the sober discussions in the middle, which are the challenging ones to have or where

59:29.320 --> 59:31.520
 we need to be having our conversations.

59:31.520 --> 59:36.440
 And you know, it's just not, actually, there's not many forum for those.

59:36.440 --> 59:39.160
 You know, there's, that's, that's kind of what I would look for.

59:39.160 --> 59:42.040
 Maybe I could go and I could read a comment section of something and it would actually

59:42.040 --> 59:44.520
 be this kind of dialogue going back and forth.

59:44.520 --> 59:45.760
 You don't see much of this, right?

59:45.760 --> 59:49.800
 Which is why actually there's a resurgence of podcasts out of all, because people are

59:49.800 --> 59:55.720
 really hungry for conversation, but there's technology is not helping much.

59:55.720 --> 1:00:03.360
 So comment sections of anything, including YouTube is not hurting and not helping.

1:00:03.360 --> 1:00:07.800
 And you think technically speaking is possible to help?

1:00:07.800 --> 1:00:14.400
 I don't know the answers, but it's a less anonymity, a little more locality, you know,

1:00:14.400 --> 1:00:17.840
 worlds that you kind of enter in and you trust the people there in those worlds so that when

1:00:17.840 --> 1:00:20.560
 you start having a discussion, you know, not only is that people are not going to hurt

1:00:20.560 --> 1:00:23.680
 you, but it's not going to be a total waste of your time because there's a lot of wasting

1:00:23.680 --> 1:00:27.240
 of time that, you know, a lot of us, I pulled out of Facebook early on because it was clearly

1:00:27.240 --> 1:00:31.320
 going to waste a lot of my time, even though there was some value.

1:00:31.320 --> 1:00:35.040
 And so yeah, worlds that are somehow you enter in, you know what you're getting, and it's

1:00:35.040 --> 1:00:40.800
 kind of appeals to you, new things might happen, but you kind of have some trust in that world.

1:00:40.800 --> 1:00:46.440
 And there's some deep, interesting, complex psychological aspects around anonymity, how

1:00:46.440 --> 1:00:49.920
 that changes human behavior that's quite dark.

1:00:49.920 --> 1:00:50.920
 Quite dark, yeah.

1:00:50.920 --> 1:00:55.400
 I think a lot of us are, especially those of us who really loved the advent of technology,

1:00:55.400 --> 1:00:59.520
 I loved social networks when they came out, I didn't see any negatives there at all.

1:00:59.520 --> 1:01:03.720
 But then I started seeing comment sections, I think it was maybe, you know, the CNN or

1:01:03.720 --> 1:01:04.720
 something.

1:01:04.720 --> 1:01:10.040
 And I started to go, wow, this, this darkness I just did not know about, and our technology

1:01:10.040 --> 1:01:11.440
 now amplifying it.

1:01:11.440 --> 1:01:15.800
 So sorry for the big philosophical question, but on that topic, do you think human beings,

1:01:15.800 --> 1:01:21.360
 because you've also, out of all things, had a foot in psychology too, do you think human

1:01:21.360 --> 1:01:23.840
 beings are fundamentally good?

1:01:23.840 --> 1:01:32.280
 Like all of us have good intent that could be mined, or is it, depending on context and

1:01:32.280 --> 1:01:34.960
 environment, everybody could be evil?

1:01:34.960 --> 1:01:39.240
 So my answer is fundamentally good, but fundamentally limited.

1:01:39.240 --> 1:01:41.320
 All of us have very, you know, blinkers on.

1:01:41.320 --> 1:01:43.920
 We don't see the other person's pain that easily.

1:01:43.920 --> 1:01:46.680
 We don't see the other person's point of view that easily.

1:01:46.680 --> 1:01:49.880
 We're very much in our own head, in our own world.

1:01:49.880 --> 1:01:54.080
 And on my good days, I think that technology could open us up to more perspectives and

1:01:54.080 --> 1:01:58.560
 more, less blinkered and more understanding, you know, a lot of wars in human history happened

1:01:58.560 --> 1:01:59.800
 because of just ignorance.

1:01:59.800 --> 1:02:02.600
 They didn't, they thought the other person was doing this, well, that person wasn't

1:02:02.600 --> 1:02:05.400
 doing this, and we have a huge amounts of that.

1:02:05.400 --> 1:02:09.200
 But in my lifetime, I've not seen technology really help in that way yet.

1:02:09.200 --> 1:02:13.600
 And I do, I do, I do believe in that, but, you know, no, I think fundamentally humans

1:02:13.600 --> 1:02:14.600
 are good.

1:02:14.600 --> 1:02:15.600
 People suffer.

1:02:15.600 --> 1:02:18.480
 People have grieves, and so you have grudges, and those things cause them to do things they

1:02:18.480 --> 1:02:19.960
 probably wouldn't want.

1:02:19.960 --> 1:02:22.600
 They regret it often.

1:02:22.600 --> 1:02:27.880
 So no, I, I, I think it's a, you know, part of the progress that technology is to indeed

1:02:27.880 --> 1:02:31.360
 allow it to be a little easier to be the real good person you actually are.

1:02:31.360 --> 1:02:39.480
 Well, but do you think individual human life or society can be modeled as an optimization

1:02:39.480 --> 1:02:40.480
 problem?

1:02:40.480 --> 1:02:44.680
 Um, not the way I think typically, I mean, that's, you're talking about one of the most

1:02:44.680 --> 1:02:49.600
 complex phenomena in the whole, you know, in all, which individual human life or society

1:02:49.600 --> 1:02:50.600
 is a whole.

1:02:50.600 --> 1:02:51.600
 Both.

1:02:51.600 --> 1:02:52.600
 Both.

1:02:52.600 --> 1:02:56.880
 I mean, individual human life is, is amazingly complex and, um, so, uh, you know, optimization

1:02:56.880 --> 1:03:00.040
 is kind of just one branch of mathematics that talks about certain kind of things.

1:03:00.040 --> 1:03:04.520
 And, uh, it just feels way too limited for the complexity of, uh, such things.

1:03:04.520 --> 1:03:09.440
 What properties of optimization problems do you think, so do you think most interesting

1:03:09.440 --> 1:03:13.840
 problems that could be solved through optimization, uh, what kind of properties does that surface

1:03:13.840 --> 1:03:19.720
 have nonconvexity, convexity, linearity, all those kinds of things, saddle points.

1:03:19.720 --> 1:03:22.120
 Well, so optimization is just one piece of mathematics.

1:03:22.120 --> 1:03:26.600
 You know, there's like, you just, even in our era, we're aware that say sampling, um,

1:03:26.600 --> 1:03:30.640
 it's coming up examples of something, um, come in with a distribution.

1:03:30.640 --> 1:03:31.800
 What's optimization?

1:03:31.800 --> 1:03:32.800
 What's sampling?

1:03:32.800 --> 1:03:35.920
 Well, you think you can, if you're a kind of a certain kind of math, but you can try

1:03:35.920 --> 1:03:38.680
 to blend them and make them seem to be sort of the same thing.

1:03:38.680 --> 1:03:43.920
 But optimization is roughly speaking, trying to, uh, find a point that, um, a single point

1:03:43.920 --> 1:03:50.240
 that is the optimum of a criterion function of some kind, um, and sampling is trying to,

1:03:50.240 --> 1:03:55.480
 from that same surface, treat that as a distribution or density and find prop points that have

1:03:55.480 --> 1:03:56.480
 high density.

1:03:56.480 --> 1:04:02.360
 So, um, I want the entire distribution and the sampling paradigm and I want the, um,

1:04:02.360 --> 1:04:06.640
 you know, the single point, that's the best point in the, in the sample, in the, uh, optimization

1:04:06.640 --> 1:04:07.640
 paradigm.

1:04:07.640 --> 1:04:11.880
 Now, if you were optimizing in the space of probability measures, the output of that could

1:04:11.880 --> 1:04:13.080
 be a whole probability distribution.

1:04:13.080 --> 1:04:16.840
 So you can start to make these things the same, but in mathematics, if you go too high

1:04:16.840 --> 1:04:21.320
 up that kind of abstraction hierarchy, you start to lose the, uh, you know, the ability

1:04:21.320 --> 1:04:22.880
 to do the interesting theorems.

1:04:22.880 --> 1:04:23.880
 So you kind of don't try that.

1:04:23.880 --> 1:04:26.960
 You don't try to overly over abstract.

1:04:26.960 --> 1:04:31.560
 So as a small tangent, what kind of world do you, do you find more appealing?

1:04:31.560 --> 1:04:35.360
 One that is deterministic or stochastic?

1:04:35.360 --> 1:04:36.880
 Uh, well, that's easy.

1:04:36.880 --> 1:04:40.120
 I mean, I'm a statistician, you know, the world is highly stochastic.

1:04:40.120 --> 1:04:42.160
 Wait, I don't know what's going to happen in the next five minutes.

1:04:42.160 --> 1:04:43.160
 Right.

1:04:43.160 --> 1:04:47.400
 Cause what you're going to ask, what we're going to do, what I'll say, massive uncertainty,

1:04:47.400 --> 1:04:48.800
 you know, massive uncertainty.

1:04:48.800 --> 1:04:53.080
 And so the best I can do is have come rough sense or probability distribution on things

1:04:53.080 --> 1:04:58.280
 and somehow use that in my reasoning about what to do now.

1:04:58.280 --> 1:05:07.080
 So how does the distributed at scale, when you have multi agent systems, uh, look like,

1:05:07.080 --> 1:05:13.800
 so optimization can optimize sort of, it makes a lot more sense sort of, uh, at least mine

1:05:13.800 --> 1:05:17.880
 from a robotics perspective, for a single robot, for a single agent, trying to optimize

1:05:17.880 --> 1:05:23.400
 some objective function, uh, what, when you start to enter the real world, this game theory

1:05:23.400 --> 1:05:30.600
 ready concepts starts popping up and that's how do you see optimization in this?

1:05:30.600 --> 1:05:32.680
 Cause you've talked about markets and a scale.

1:05:32.680 --> 1:05:33.680
 What does that look like?

1:05:33.680 --> 1:05:34.680
 Do you see it as optimization?

1:05:34.680 --> 1:05:36.120
 Do you see it as a sampling?

1:05:36.120 --> 1:05:39.240
 Do you see like how, how should you, yeah, these all blend together.

1:05:39.240 --> 1:05:43.800
 Um, and a system designer thinking about how to build an incentivized system will have

1:05:43.800 --> 1:05:44.880
 a blend of all these things.

1:05:44.880 --> 1:05:49.880
 So you know, a particle in a potential well is optimizing a functional called a Lagrangian.

1:05:49.880 --> 1:05:50.880
 Right.

1:05:50.880 --> 1:05:51.880
 The particle doesn't know that.

1:05:51.880 --> 1:05:54.600
 There's no algorithm running that does that.

1:05:54.600 --> 1:05:55.600
 It just happens.

1:05:55.600 --> 1:05:59.320
 It's, it's, so it's a description mathematically of something that helps us understand as analysts,

1:05:59.320 --> 1:06:00.320
 what's happening.

1:06:00.320 --> 1:06:01.320
 All right.

1:06:01.320 --> 1:06:03.520
 And so the same thing will happen when we talk about, you know, mixtures of humans and

1:06:03.520 --> 1:06:07.080
 computers and markets and so on and so forth, there'll be certain principles that allow

1:06:07.080 --> 1:06:10.320
 us to understand what's happening and whether or not the actual algorithms are being used

1:06:10.320 --> 1:06:12.360
 by any sense is not clear.

1:06:12.360 --> 1:06:19.080
 Um, now at, at some point I may have set up a multi agent or market kind of system and

1:06:19.080 --> 1:06:23.320
 I'm now thinking about an individual agent in that system and they're asked to do some

1:06:23.320 --> 1:06:26.520
 task and they're incentivized and somewhere they get certain signals and they, they have

1:06:26.520 --> 1:06:27.520
 some utility.

1:06:27.520 --> 1:06:30.800
 Maybe what they will do at that point is they just won't know the answer.

1:06:30.800 --> 1:06:32.640
 They may have to optimize to find an answer.

1:06:32.640 --> 1:06:33.640
 Okay.

1:06:33.640 --> 1:06:37.680
 So an autos could be embedded inside of an overall market, uh, you know, and game theory

1:06:37.680 --> 1:06:39.320
 is, is very, very broad.

1:06:39.320 --> 1:06:43.320
 Um, it is often studied very narrowly for certain kinds of problems.

1:06:43.320 --> 1:06:47.920
 Um, but it's roughly speaking, this is just the, I don't know what you're going to do.

1:06:47.920 --> 1:06:51.640
 I kind of anticipate that a little bit and you anticipate what I'm anticipating and we

1:06:51.640 --> 1:06:53.320
 kind of go back and forth in our own minds.

1:06:53.320 --> 1:06:55.480
 We run kind of thought experiments.

1:06:55.480 --> 1:07:00.400
 You've talked about this interesting point in terms of game theory, you know, most optimization

1:07:00.400 --> 1:07:02.840
 problems really hate saddle points.

1:07:02.840 --> 1:07:07.040
 Maybe you can describe what saddle points are, but I had heard you kind of mentioned

1:07:07.040 --> 1:07:12.680
 that there's a, there's a branch of optimization that you could try to explicitly look for

1:07:12.680 --> 1:07:14.680
 saddle points as a good thing.

1:07:14.680 --> 1:07:15.840
 Oh, not optimization.

1:07:15.840 --> 1:07:16.840
 That's just game theory.

1:07:16.840 --> 1:07:21.080
 That's, so, uh, there's all kinds of different equilibrium game theory and some of them are

1:07:21.080 --> 1:07:23.200
 highly explanatory behavior.

1:07:23.200 --> 1:07:24.720
 They're not attempting to be algorithmic.

1:07:24.720 --> 1:07:29.040
 They're just trying to say, if you happen to be at this equilibrium, you would see certain

1:07:29.040 --> 1:07:30.960
 kind of behavior and we see that in real life.

1:07:30.960 --> 1:07:38.120
 That's what an economist wants to do, especially behavioral economists, um, uh, in, in continuous,

1:07:38.120 --> 1:07:43.100
 uh, differential game theory, you're in continuous spaces, a, um, some of the simplest

1:07:43.100 --> 1:07:47.120
 equilibria are saddle points and Nash equilibrium as a saddle point, it's a special kind of

1:07:47.120 --> 1:07:48.120
 saddle point.

1:07:48.120 --> 1:07:53.560
 So, uh, classically in game theory, you were trying to find Nash equilibria and algorithmic

1:07:53.560 --> 1:07:56.840
 game theory, you're trying to find algorithms that would find them, uh, and so you're trying

1:07:56.840 --> 1:07:57.840
 to find saddle points.

1:07:57.840 --> 1:08:00.000
 I mean, so that's, that's literally what you're trying to do.

1:08:00.000 --> 1:08:04.160
 Um, but you know, any economist knows that Nash equilibria, uh, have their limitations.

1:08:04.160 --> 1:08:08.200
 They are definitely not that explanatory in many situations.

1:08:08.200 --> 1:08:09.680
 You're not what you really want.

1:08:09.680 --> 1:08:14.200
 Um, there's other kind of equilibria and there's names associated with these cause they came

1:08:14.200 --> 1:08:18.080
 from history with certain people working on them, but there'll be new ones emerging.

1:08:18.080 --> 1:08:21.200
 So you know, one example is a Stackelberg equilibrium.

1:08:21.200 --> 1:08:25.840
 So you know, Nash, you and I are both playing this game against each other or for each other,

1:08:25.840 --> 1:08:29.000
 maybe it's cooperative and we're both going to think it through and then we're going to

1:08:29.000 --> 1:08:32.520
 decide and we're going to do our thing simultaneously.

1:08:32.520 --> 1:08:34.640
 You know, in a Stackelberg, no, I'm going to be the first mover.

1:08:34.640 --> 1:08:35.880
 I'm going to make a move.

1:08:35.880 --> 1:08:38.000
 You're going to look at my move and then you're going to make yours.

1:08:38.000 --> 1:08:41.760
 Now, since I know you're going to look at my move, I anticipate what you're going to

1:08:41.760 --> 1:08:42.760
 do.

1:08:42.760 --> 1:08:46.360
 And so I don't do something stupid, but, and, but then I know that you were also anticipating

1:08:46.360 --> 1:08:47.360
 me.

1:08:47.360 --> 1:08:48.360
 So we're kind of going back and so forth.

1:08:48.360 --> 1:08:51.800
 Why, but there is then a first mover thing.

1:08:51.800 --> 1:08:54.920
 And so there's a different equilibria, all right.

1:08:54.920 --> 1:08:58.840
 And, uh, so just mathematically, yeah, these things have certain topologies and certain

1:08:58.840 --> 1:09:02.840
 shapes that are like salivates and then algorithmically or dynamically, how do you move towards them?

1:09:02.840 --> 1:09:04.960
 How do you move away from things?

1:09:04.960 --> 1:09:07.600
 Um, you know, so some of these questions have answers.

1:09:07.600 --> 1:09:08.720
 They've been studied.

1:09:08.720 --> 1:09:13.160
 Others do not, and especially if it becomes stochastic, especially if there's large numbers

1:09:13.160 --> 1:09:16.840
 of decentralized things, there's just, uh, you know, young people get in this field who

1:09:16.840 --> 1:09:19.440
 kind of think it's all done because we have, you know, TensorFlow.

1:09:19.440 --> 1:09:23.680
 Well, no, these are all open problems and they're really important and interesting.

1:09:23.680 --> 1:09:25.160
 And it's about strategic settings.

1:09:25.160 --> 1:09:26.160
 How do I collect data?

1:09:26.160 --> 1:09:29.280
 I suppose I don't know what you're going to do because I don't know you very well, right?

1:09:29.280 --> 1:09:31.200
 Well, I got to collect data about you.

1:09:31.200 --> 1:09:34.400
 So maybe I want to push you in a part of the space where I don't know much about you.

1:09:34.400 --> 1:09:35.400
 So I can get data.

1:09:35.400 --> 1:09:39.120
 And then later I'll realize that you'll never, you'll never go there because of the way the

1:09:39.120 --> 1:09:40.120
 game is set up.

1:09:40.120 --> 1:09:43.200
 But, you know, that's part of the overall, you know, data analysis context.

1:09:43.200 --> 1:09:44.200
 Yeah.

1:09:44.200 --> 1:09:45.920
 Even the game of poker is fascinating space.

1:09:45.920 --> 1:09:46.920
 Yeah.

1:09:46.920 --> 1:09:50.320
 Whenever there's any uncertainty or lack of information, it's, it's a super exciting

1:09:50.320 --> 1:09:51.320
 space.

1:09:51.320 --> 1:09:52.320
 Yeah.

1:09:52.320 --> 1:09:55.320
 Uh, just a lingering optimization for a second.

1:09:55.320 --> 1:10:01.560
 So when we look at deep learning, it's essentially a minimization of a complicated loss function.

1:10:01.560 --> 1:10:07.360
 So is there something insightful or hopeful that you see in the kinds of function surface

1:10:07.360 --> 1:10:13.760
 that lost functions that deep learning in, in the real world is trying to optimize over?

1:10:13.760 --> 1:10:20.000
 Is there something interesting as it's just the usual kind of problems of optimization?

1:10:20.000 --> 1:10:25.600
 I think from an optimization point of view, that surface first of all, it's pretty smooth.

1:10:25.600 --> 1:10:29.040
 And secondly, if there's over, if it's over parameterized, there's kind of lots of paths

1:10:29.040 --> 1:10:31.520
 down to reasonable optima.

1:10:31.520 --> 1:10:35.600
 And so kind of the getting downhill to the, to an optimum is, is viewed as not as hard

1:10:35.600 --> 1:10:39.920
 as you might have expected in high dimensions.

1:10:39.920 --> 1:10:43.600
 The fact that some optima tend to be really good ones and others not so good and you tend

1:10:43.600 --> 1:10:48.560
 to, it's not, sometimes you find the good ones is, is sort of still needs explanation.

1:10:48.560 --> 1:10:49.560
 Yes.

1:10:49.560 --> 1:10:53.520
 But, but the particular surface is coming from the particular generation of neural nets.

1:10:53.520 --> 1:10:56.840
 I kind of suspect those will, those will change in 10 years.

1:10:56.840 --> 1:10:58.440
 It will not be exactly those surfaces.

1:10:58.440 --> 1:11:02.480
 There'll be some others that are, an optimization theory will help contribute to why other surfaces

1:11:02.480 --> 1:11:05.800
 or why other algorithms.

1:11:05.800 --> 1:11:09.800
 Layers of arithmetic operations with a little bit of nonlinearity, that's not, that didn't

1:11:09.800 --> 1:11:10.960
 come from neuroscience per se.

1:11:10.960 --> 1:11:14.520
 I mean, maybe in the minds of some of the people working on it, they were thinking about brains,

1:11:14.520 --> 1:11:19.000
 but they were arithmetic circuits in all kinds of fields, you know, computer science control

1:11:19.000 --> 1:11:20.640
 theory and so on.

1:11:20.640 --> 1:11:24.800
 And that layers of these could transform things in certain ways and that if it's smooth, maybe

1:11:24.800 --> 1:11:31.040
 you could, you know, find parameter values, you know, it's a big, is a, is a, is a, is

1:11:31.040 --> 1:11:33.280
 a sort of big discovery that it's, it's working.

1:11:33.280 --> 1:11:39.080
 It's able to work at this scale, but I don't think that we're stuck with that and we're,

1:11:39.080 --> 1:11:42.120
 we're certainly not stuck with that because we're understanding the brain.

1:11:42.120 --> 1:11:46.320
 So in terms of, on the algorithm side, sort of gradient descent, do you think we're stuck

1:11:46.320 --> 1:11:49.320
 with gradient descent as variants of it?

1:11:49.320 --> 1:11:53.560
 What variants do you find interesting or do you think there'll be something else invented

1:11:53.560 --> 1:11:59.720
 that is able to walk all over these optimization spaces in more interesting ways?

1:11:59.720 --> 1:12:04.680
 So there's a co design of the surface and they are the architecture and the algorithm.

1:12:04.680 --> 1:12:08.240
 So if you just ask if we stay with the kind of architectures that we have now, not just

1:12:08.240 --> 1:12:13.080
 neural nets, but, you know, phase retrieval architectures or maybe completion architectures

1:12:13.080 --> 1:12:15.080
 and so on.

1:12:15.080 --> 1:12:19.560
 You know, I think we've kind of come to a place where a stochastic gradient algorithms

1:12:19.560 --> 1:12:25.800
 are dominant and there are versions that, you know, they're a little better than others.

1:12:25.800 --> 1:12:29.840
 They, you know, have more guarantees, they're more robust and so on and there's ongoing

1:12:29.840 --> 1:12:34.240
 research to kind of figure out which is the best time for which situation.

1:12:34.240 --> 1:12:37.640
 But I think that that'll start to co evolve, that that'll put pressure on the actual architecture

1:12:37.640 --> 1:12:40.800
 and so we shouldn't do it in this particular way, we should do it in a different way because

1:12:40.800 --> 1:12:45.320
 this other algorithm is now available if you do it in a different way.

1:12:45.320 --> 1:12:51.560
 So that I can't really anticipate that co evolution process, but you know, gradients

1:12:51.560 --> 1:12:54.480
 are amazing mathematical objects.

1:12:54.480 --> 1:13:01.120
 They have a lot of people who sort of study them more deeply mathematically or kind of

1:13:01.120 --> 1:13:03.600
 shocked about what they are and what they can do.

1:13:03.600 --> 1:13:08.560
 I mean, to think about this way, if I suppose that I tell you, if you move along the x axis,

1:13:08.560 --> 1:13:13.880
 you get, you know, you go uphill in some objective by, you know, three units, whereas if you move

1:13:13.880 --> 1:13:19.080
 along the y axis, you go uphill by seven units, right now, I'm going to only allow you to

1:13:19.080 --> 1:13:22.440
 move a certain, you know, unit distance, right?

1:13:22.440 --> 1:13:23.440
 What are you going to do?

1:13:23.440 --> 1:13:27.240
 Well, the most not people will say, I'm going to go along the y axis, I'm getting the biggest

1:13:27.240 --> 1:13:30.280
 bang for my buck, you know, and my buck is only one unit.

1:13:30.280 --> 1:13:33.920
 So I'm going to put all of it in the y axis, right?

1:13:33.920 --> 1:13:39.360
 And why should I even take any of my strength, my step size and put any of it in the x axis

1:13:39.360 --> 1:13:41.480
 because I'm getting less bang for my buck.

1:13:41.480 --> 1:13:46.880
 That seems like a completely, you know, clear argument and it's wrong because the gradient

1:13:46.880 --> 1:13:51.800
 direction is not to go along the y axis, it's to take a little bit of the x axis.

1:13:51.800 --> 1:13:55.440
 And that, to understand that, you have to, you have to know some math.

1:13:55.440 --> 1:14:00.360
 And so even a, you know, trivial, so called operator like gradient is not trivial and

1:14:00.360 --> 1:14:04.000
 so, you know, exploiting its properties is still very, very important.

1:14:04.000 --> 1:14:06.840
 Now we know that just creating descent has got all kinds of problems that get stuck in

1:14:06.840 --> 1:14:10.960
 many ways and it had never, you know, good dimension dependence and so on.

1:14:10.960 --> 1:14:15.960
 So my own line of work recently has been about what kinds of stochasticity, how can we get

1:14:15.960 --> 1:14:16.960
 dimension dependence?

1:14:16.960 --> 1:14:19.240
 How can we do the theory of that?

1:14:19.240 --> 1:14:22.720
 And we've come up with pretty favorable results with certain kinds of stochasticity.

1:14:22.720 --> 1:14:25.000
 We have sufficient conditions generally.

1:14:25.000 --> 1:14:28.760
 We know if you, if you do this, we will give you a good guarantee.

1:14:28.760 --> 1:14:32.280
 We don't have necessary conditions that it must be done a certain way in general.

1:14:32.280 --> 1:14:38.200
 So stochasticity, how much randomness to inject into the, into the walking along the gradient.

1:14:38.200 --> 1:14:40.000
 And what kind of randomness?

1:14:40.000 --> 1:14:42.240
 Why is randomness good in this process?

1:14:42.240 --> 1:14:44.200
 Why is stochasticity good?

1:14:44.200 --> 1:14:45.200
 Yeah.

1:14:45.200 --> 1:14:49.280
 So I can give you simple answers, but in some sense, again, it's kind of amazing.

1:14:49.280 --> 1:14:55.120
 Stochasticity just, you know, particular features of a surface that could have hurt

1:14:55.120 --> 1:15:00.000
 you if you were doing one thing, you know, deterministically won't hurt you because,

1:15:00.000 --> 1:15:03.560
 you know, by chance, you know, there's very little chance that you would get hurt.

1:15:03.560 --> 1:15:11.200
 And, you know, so here stochasticity, you know, it's just kind of saves you from some

1:15:11.200 --> 1:15:16.720
 of the particular features of surfaces that, you know, in fact, if you think about, you

1:15:16.720 --> 1:15:19.960
 know, surfaces that are discontinuous in a first derivative, like, you know, absolute

1:15:19.960 --> 1:15:24.640
 value function, you will go down and hit that point where there's non differentiability.

1:15:24.640 --> 1:15:25.640
 Right.

1:15:25.640 --> 1:15:28.600
 And if you're running a deterministic argument at that point, you can really do something

1:15:28.600 --> 1:15:29.600
 bad.

1:15:29.600 --> 1:15:30.600
 Right.

1:15:30.600 --> 1:15:32.800
 Where stochasticity just means it's pretty unlikely that's going to happen.

1:15:32.800 --> 1:15:35.720
 You're going to get, you're going to hit that point.

1:15:35.720 --> 1:15:40.960
 So, you know, it's, again, not trivial to analyze, but, especially in higher dimensions,

1:15:40.960 --> 1:15:44.400
 also stochasticity, our intuition isn't very good about it, but it has properties that

1:15:44.400 --> 1:15:49.200
 kind of are very appealing in high dimensions for kind of law of large number of reasons.

1:15:49.200 --> 1:15:52.520
 So it's all part of the mathematics to kind of, that's what's fun to work in the field

1:15:52.520 --> 1:15:57.040
 is that you get to try to understand this mathematics.

1:15:57.040 --> 1:16:01.200
 But long story short, you know, partly empirically, it was discovered stochastic gradient is very

1:16:01.200 --> 1:16:06.640
 effective and theory kind of followed, I'd say, that, but I don't see that we're getting

1:16:06.640 --> 1:16:09.120
 it clearly out of that.

1:16:09.120 --> 1:16:15.560
 What's the most beautiful, mysterious, or profound idea to you in optimization?

1:16:15.560 --> 1:16:20.440
 I don't know the most, but let me just say that, you know, Nesterov's work on Nesterov

1:16:20.440 --> 1:16:26.280
 acceleration to me is pretty surprising and pretty deep.

1:16:26.280 --> 1:16:27.280
 Can you elaborate?

1:16:27.280 --> 1:16:32.240
 Well, Nesterov acceleration is just that, I suppose that we are going to use gradients

1:16:32.240 --> 1:16:35.960
 to move around into space for the reasons I've alluded to, there are nice directions

1:16:35.960 --> 1:16:37.280
 to move.

1:16:37.280 --> 1:16:40.520
 And suppose that I tell you that you're only allowed to use gradients, you're not going

1:16:40.520 --> 1:16:47.400
 to be allowed to use this local person, it can only sense kind of the change in the surface.

1:16:47.400 --> 1:16:50.880
 But I'm going to give you kind of a computer that's able to store all your previous gradients.

1:16:50.880 --> 1:16:55.000
 And so you start to learn something about the surface.

1:16:55.000 --> 1:16:58.600
 And I'm going to restrict you to maybe move in the direction of like a linear span of

1:16:58.600 --> 1:16:59.600
 all the gradients.

1:16:59.600 --> 1:17:02.840
 So you can't kind of just move in some arbitrary direction, right?

1:17:02.840 --> 1:17:05.680
 So now we have a well defined mathematical complexity model.

1:17:05.680 --> 1:17:09.280
 There's a certain classes of algorithms that can do that and others that can't.

1:17:09.280 --> 1:17:13.800
 And we can ask for certain kinds of surfaces, how fast can you get down to the optimum?

1:17:13.800 --> 1:17:14.920
 So there's an answers to these.

1:17:14.920 --> 1:17:20.680
 So for a, you know, a smooth convex function, there's an answer, which is one over the number

1:17:20.680 --> 1:17:29.120
 of steps squared is that you will be within a ball of that size after K steps.

1:17:29.120 --> 1:17:31.520
 Gradient descent in particular has a slower rate.

1:17:31.520 --> 1:17:35.440
 It's one over K. Okay.

1:17:35.440 --> 1:17:38.960
 So you could ask, is gradient descent actually, even though we know it's a good algorithm,

1:17:38.960 --> 1:17:43.760
 is it the best algorithm in the sense of the answer is no, but well, well, not clear yet

1:17:43.760 --> 1:17:47.440
 because one over K score is a lower bound.

1:17:47.440 --> 1:17:49.360
 That's probably the best you can do.

1:17:49.360 --> 1:17:52.720
 Gradient is one over K, but these are something better.

1:17:52.720 --> 1:17:59.240
 And so I think as a surprise to most, the Nesterov discovered a new algorithm that has got two

1:17:59.240 --> 1:18:00.240
 pieces to it.

1:18:00.240 --> 1:18:06.600
 It uses two gradients and puts those together in a certain kind of obscure way.

1:18:06.600 --> 1:18:09.240
 And the thing doesn't even move downhill all the time.

1:18:09.240 --> 1:18:10.720
 It sometimes goes back uphill.

1:18:10.720 --> 1:18:13.160
 And if you're a physicist, that kind of makes some sense.

1:18:13.160 --> 1:18:17.240
 You're building up some momentum and that is kind of the right intuition, but that that

1:18:17.240 --> 1:18:22.440
 intuition is not enough to understand kind of how to do it and why it works.

1:18:22.440 --> 1:18:23.440
 But it does.

1:18:23.440 --> 1:18:27.480
 It achieves one over K squared and it has a mathematical structure and it's still kind

1:18:27.480 --> 1:18:32.520
 of to this day, a lot of us are writing papers and trying to explore that and understand it.

1:18:32.520 --> 1:18:36.600
 So there are lots of cool ideas and optimization, but just kind of using gradients, I think

1:18:36.600 --> 1:18:40.720
 is number one that goes back, you know, 150 years.

1:18:40.720 --> 1:18:43.560
 And then Nesterov, I think has made a major contribution with this idea.

1:18:43.560 --> 1:18:47.320
 So like you said, gradients themselves are in some sense mysterious.

1:18:47.320 --> 1:18:53.240
 Yeah, they're not, they're not as trivial as not as much as coordinate descent is more

1:18:53.240 --> 1:18:54.240
 of a trivial one.

1:18:54.240 --> 1:18:55.240
 You just pick one of the coordinates.

1:18:55.240 --> 1:18:59.880
 That's how we think that's how our human minds think and gradients are not that easy

1:18:59.880 --> 1:19:03.280
 for our human mind to grapple with.

1:19:03.280 --> 1:19:08.600
 An absurd question, but what is statistics?

1:19:08.600 --> 1:19:12.120
 So here it's a little bit, it's somewhere between math and science and technology.

1:19:12.120 --> 1:19:13.400
 It's somewhere in that convex hole.

1:19:13.400 --> 1:19:17.680
 So it's a set of principles that allow you to make inferences that have got some reason

1:19:17.680 --> 1:19:22.600
 to be believed and also principles that allow you to make decisions where you can have some

1:19:22.600 --> 1:19:25.080
 reason to believe you're not going to make errors.

1:19:25.080 --> 1:19:27.640
 So all of that requires some assumptions about what do you mean by an error?

1:19:27.640 --> 1:19:33.320
 What do you mean by, you know, the probabilities and, but, you know, after you start making

1:19:33.320 --> 1:19:39.640
 some assumptions, you're led to conclusions that, yes, I can guarantee that, you know,

1:19:39.640 --> 1:19:43.600
 if you do this in this way, your probability of making an error will be small.

1:19:43.600 --> 1:19:47.880
 Your probability of continuing to not make errors over time will be small.

1:19:47.880 --> 1:19:52.400
 And probability you found something that's real will be small, will be high.

1:19:52.400 --> 1:19:54.680
 So decision making is a big part of that?

1:19:54.680 --> 1:19:55.760
 So decision making is a big part, yeah.

1:19:55.760 --> 1:20:00.120
 So the original, so statistics, you know, short history was that, you know, it's kind

1:20:00.120 --> 1:20:04.960
 of goes back sort of as a formal discipline, you know, 250 years or so.

1:20:04.960 --> 1:20:09.280
 It was called inverse probability because around that era, probability was developed

1:20:09.280 --> 1:20:12.880
 sort of especially to explain gambling situations.

1:20:12.880 --> 1:20:14.000
 Of course.

1:20:14.000 --> 1:20:15.480
 And interesting.

1:20:15.480 --> 1:20:18.880
 So you would say, well, given the state of nature is this, there's a certain roulette

1:20:18.880 --> 1:20:21.120
 board that has a certain mechanism in it.

1:20:21.120 --> 1:20:23.680
 What kind of outcomes do I expect to see?

1:20:23.680 --> 1:20:27.440
 And especially if I do things longer, longer amounts of time, what outcomes will I see

1:20:27.440 --> 1:20:30.640
 and the physicists start to pay attention to this?

1:20:30.640 --> 1:20:33.520
 And then people say, well, given, let's turn the problem around.

1:20:33.520 --> 1:20:37.480
 What if I saw certain outcomes, could I infer what the underlying mechanism was?

1:20:37.480 --> 1:20:38.480
 That's an inverse problem.

1:20:38.480 --> 1:20:41.960
 And in fact, for quite a while, statistics was called inverse probability.

1:20:41.960 --> 1:20:44.040
 That was the name of the field.

1:20:44.040 --> 1:20:49.640
 And I believe that it was Laplace, who was working in Napoleon's government, who was

1:20:49.640 --> 1:20:54.240
 trying, who needed to do a census of France, learn about the people there.

1:20:54.240 --> 1:21:01.240
 So he went and gathered data and he analyzed that data to determine policy and said, let's

1:21:01.240 --> 1:21:07.360
 call this field that does this kind of thing, statistics, because the word state is in there.

1:21:07.360 --> 1:21:12.000
 And in French, that's état, but it's the study of data for the state.

1:21:12.000 --> 1:21:18.600
 So anyway, that caught on and it's been called statistics ever since.

1:21:18.600 --> 1:21:23.240
 But by the time it got formalized, it was sort of in the 30s.

1:21:23.240 --> 1:21:28.560
 And around that time, there was game theory and decision theory developed nearby.

1:21:28.560 --> 1:21:31.600
 People in that era didn't think of themselves as either computer science or statistics or

1:21:31.600 --> 1:21:32.600
 control or econ.

1:21:32.600 --> 1:21:34.520
 They were all, they were all the above.

1:21:34.520 --> 1:21:37.880
 And so, you know, von Neumann is developing game theory, but also thinking of that as

1:21:37.880 --> 1:21:43.000
 decision theory, Wald is an econometrician, developing decision theory, and then, you

1:21:43.000 --> 1:21:45.120
 know, turning that into statistics.

1:21:45.120 --> 1:21:50.120
 And so it's all about, here's not just data and you analyze it, here's a loss function,

1:21:50.120 --> 1:21:53.080
 here's what you care about, here's the question you're trying to ask.

1:21:53.080 --> 1:21:59.440
 Here is a probability model and here's the risk you will face if you make certain decisions.

1:21:59.440 --> 1:22:04.040
 And to this day, in most advanced statistical curricula, you teach decision theory as the

1:22:04.040 --> 1:22:08.480
 starting point, and then it branches out into the two branches of Bayesian and Frequentist.

1:22:08.480 --> 1:22:11.800
 But that's, it's all about decisions.

1:22:11.800 --> 1:22:18.880
 In statistics, what is the most beautiful, mysterious, maybe surprising idea that you've

1:22:18.880 --> 1:22:20.520
 come across?

1:22:20.520 --> 1:22:25.280
 Yeah, good question.

1:22:25.280 --> 1:22:28.960
 I mean, there's a bunch of surprising ones, there's something that's way too technical

1:22:28.960 --> 1:22:33.440
 for this thing, but something called James Stein estimation, which is kind of surprising

1:22:33.440 --> 1:22:36.000
 and really takes time to wrap your head around.

1:22:36.000 --> 1:22:37.000
 Can you try to maybe?

1:22:37.000 --> 1:22:39.160
 I think I don't want to even want to try.

1:22:39.160 --> 1:22:44.200
 Let me just say a colleague at Steven Stigler at University of Chicago wrote a really beautiful

1:22:44.200 --> 1:22:48.840
 paper on James Stein estimation, which is helps to, its views of paradox, it kind of

1:22:48.840 --> 1:22:52.960
 defeats the mind's attempts to understand it, but you can, and Steve has a nice perspective

1:22:52.960 --> 1:22:56.560
 on that.

1:22:56.560 --> 1:23:00.320
 So one of the troubles with statistics is that it's like in physics, that are in quantum

1:23:00.320 --> 1:23:02.520
 physics, you have multiple interpretations.

1:23:02.520 --> 1:23:07.640
 There's a wave and particle duality in physics and you get used to that over time, but it's

1:23:07.640 --> 1:23:11.680
 still kind of haunts you that you don't really, you know, quite understand the relationship.

1:23:11.680 --> 1:23:16.760
 The electrons of wave and electrons of particle, well, well, same thing happens here.

1:23:16.760 --> 1:23:20.440
 There's Bayesian ways of thinking and Frequentist and they are different.

1:23:20.440 --> 1:23:24.560
 They, they often, they sometimes become sort of the same in practice, but they are physically

1:23:24.560 --> 1:23:25.560
 different.

1:23:25.560 --> 1:23:27.640
 And then in some practice, they are not the same at all.

1:23:27.640 --> 1:23:30.480
 They give you rather different answers.

1:23:30.480 --> 1:23:33.880
 And so it is very much like wave and particle duality and that is something that you have

1:23:33.880 --> 1:23:35.840
 to kind of get used to in the field.

1:23:35.840 --> 1:23:37.440
 Can you define Bayesian and Frequentist?

1:23:37.440 --> 1:23:38.440
 Yeah.

1:23:38.440 --> 1:23:41.080
 In decision theory, you can make, I have a, like I have a video that people could see

1:23:41.080 --> 1:23:45.840
 it's called, are you a Bayesian or a Frequentist and kind of help try to, to, to make it really

1:23:45.840 --> 1:23:46.840
 clear.

1:23:46.840 --> 1:23:47.840
 It comes from decision theory.

1:23:47.840 --> 1:23:51.920
 So, you know, decision theory, you're talking about loss functions, which are a function

1:23:51.920 --> 1:23:54.840
 of data X and parameter theta.

1:23:54.840 --> 1:23:58.520
 So there are a function of two arguments, okay?

1:23:58.520 --> 1:23:59.880
 Neither one of those arguments is known.

1:23:59.880 --> 1:24:01.640
 You don't know the data a priori.

1:24:01.640 --> 1:24:04.080
 It's random and the parameters unknown, all right?

1:24:04.080 --> 1:24:07.120
 So you have this function of two things you don't know and you're trying to say, I want

1:24:07.120 --> 1:24:08.200
 that function to be small.

1:24:08.200 --> 1:24:10.880
 I want small loss, right?

1:24:10.880 --> 1:24:13.440
 Well, what are you going to do?

1:24:13.440 --> 1:24:17.280
 So you sort of say, well, I'm going to average over these quantities or maximize over them

1:24:17.280 --> 1:24:23.120
 or something so that, you know, I turn that uncertainty into something certain.

1:24:23.120 --> 1:24:26.200
 So you could look at the first argument and average over it or you could look at the second

1:24:26.200 --> 1:24:27.200
 argument average over it.

1:24:27.200 --> 1:24:28.200
 That's Bayesian Frequentist.

1:24:28.200 --> 1:24:32.960
 The Frequentist says, I'm going to look at the X, the data, and I'm going to take that

1:24:32.960 --> 1:24:35.320
 as random and I'm going to average over the distribution.

1:24:35.320 --> 1:24:40.680
 So I take the expectation loss under X, theta is held fixed, all right?

1:24:40.680 --> 1:24:42.120
 That's called the risk.

1:24:42.120 --> 1:24:46.440
 And so it's looking at all the data sets you could get, all right?

1:24:46.440 --> 1:24:50.160
 And say how well will a certain procedure do under all those data sets?

1:24:50.160 --> 1:24:52.520
 That's called a Frequentist guarantee, all right?

1:24:52.520 --> 1:24:56.400
 So I think it is very appropriate when you're building a piece of software and you're shipping

1:24:56.400 --> 1:24:59.280
 it out there and people are using all kinds of data sets.

1:24:59.280 --> 1:25:02.560
 You want to have a stamp, a guarantee on it that as people run it on many, many data sets

1:25:02.560 --> 1:25:07.720
 that you never even thought about that 95% of the time it will do the right thing.

1:25:07.720 --> 1:25:09.800
 Perfectly reasonable.

1:25:09.800 --> 1:25:13.120
 The Bayesian Perspective says, well, no, I'm going to look at the other argument of the

1:25:13.120 --> 1:25:15.240
 loss function, the theta part, okay?

1:25:15.240 --> 1:25:17.600
 That's unknown and I'm uncertain about it.

1:25:17.600 --> 1:25:21.520
 So I could have my own personal probability for what it is, you know, how many tall people

1:25:21.520 --> 1:25:22.520
 are there out there?

1:25:22.520 --> 1:25:24.040
 I'm trying to infer the average height of the population.

1:25:24.040 --> 1:25:27.440
 Well, I have an idea roughly what the height is.

1:25:27.440 --> 1:25:32.200
 So I'm going to average over the theta.

1:25:32.200 --> 1:25:37.240
 So now that loss function has only now, again, one argument's gone.

1:25:37.240 --> 1:25:38.880
 Now it's a function of X.

1:25:38.880 --> 1:25:41.920
 And that's what a Bayesian does is they say, well, let's just focus on the particular X

1:25:41.920 --> 1:25:45.320
 we got, the data set we got, we condition on that.

1:25:45.320 --> 1:25:48.240
 Condition on the X, I say something about my loss.

1:25:48.240 --> 1:25:50.480
 That's a Bayesian approach to things.

1:25:50.480 --> 1:25:54.320
 And the Bayesian will argue that it's not relevant to look at all the other data sets

1:25:54.320 --> 1:25:58.800
 you could have gotten and average over them, the frequentist approach.

1:25:58.800 --> 1:26:02.080
 It's really only the data sets you got, all right?

1:26:02.080 --> 1:26:06.000
 And I do agree with that, especially in situations where you're working with a scientist, you

1:26:06.000 --> 1:26:09.440
 can learn a lot about the domain and you're really only focused on certain kinds of data

1:26:09.440 --> 1:26:13.320
 and you've gathered your data and you make inferences.

1:26:13.320 --> 1:26:17.600
 I don't agree with it though that, you know, in the sense that there are needs for frequentist

1:26:17.600 --> 1:26:18.600
 guarantees.

1:26:18.600 --> 1:26:20.880
 In the software people are using out there, you want to say something.

1:26:20.880 --> 1:26:24.880
 So these two things have to fight each other a little bit, but they have to blend.

1:26:24.880 --> 1:26:27.600
 So long story short, there's a set of ideas that are right in the middle.

1:26:27.600 --> 1:26:29.880
 They're called empirical bays.

1:26:29.880 --> 1:26:34.600
 And empirical bays sort of starts with the Bayesian framework.

1:26:34.600 --> 1:26:40.960
 It's kind of arguably philosophically more, you know, reasonable and kosher, right down

1:26:40.960 --> 1:26:44.520
 a bunch of the math that kind of flows from that and then realize there's a bunch of things

1:26:44.520 --> 1:26:48.040
 you don't know because it's the real world and you don't know everything.

1:26:48.040 --> 1:26:50.160
 So you're uncertain about certain quantities.

1:26:50.160 --> 1:26:54.520
 At that point, ask, is there a reasonable way to plug in an estimate for those things?

1:26:54.520 --> 1:26:55.520
 Okay.

1:26:55.520 --> 1:26:59.880
 And in some cases, there's quite a reasonable thing to do to plug in.

1:26:59.880 --> 1:27:03.400
 There's a natural thing you can observe in the world that you can plug in and then do

1:27:03.400 --> 1:27:06.040
 a little bit more mathematics and assure yourself it's really good.

1:27:06.040 --> 1:27:10.040
 So based on math or based on human expertise, what are the good things?

1:27:10.040 --> 1:27:11.040
 They're both going in.

1:27:11.040 --> 1:27:15.560
 The Bayesian framework allows you to put a lot of human expertise in.

1:27:15.560 --> 1:27:19.000
 But the math kind of guides you along that path and then kind of reassures at the end,

1:27:19.000 --> 1:27:22.760
 you could put that stamp of approval under certain assumptions, this thing will work.

1:27:22.760 --> 1:27:26.100
 So you asked question, was my favorite, you know, or was the most surprising, nice idea.

1:27:26.100 --> 1:27:31.800
 So one that is more accessible is something called false discovery rate, which is, you

1:27:31.800 --> 1:27:35.520
 know, you're making not just one hypothesis test or making one decision, you're making

1:27:35.520 --> 1:27:37.440
 a whole bag of them.

1:27:37.440 --> 1:27:41.800
 And in that bag of decisions, you look at the ones where you made a discovery, you announced

1:27:41.800 --> 1:27:43.360
 that something interesting had happened.

1:27:43.360 --> 1:27:44.360
 All right.

1:27:44.360 --> 1:27:47.160
 That's going to be some subset of your big bag.

1:27:47.160 --> 1:27:50.840
 In the ones you made a discovery, which subset of those are bad?

1:27:50.840 --> 1:27:53.280
 There are false, false discoveries.

1:27:53.280 --> 1:27:57.640
 You like the fraction of your false discoveries among your discoveries to be small.

1:27:57.640 --> 1:28:02.120
 That's a different criterion than accuracy or precision or recall or sensitivity and

1:28:02.120 --> 1:28:03.120
 specificity.

1:28:03.120 --> 1:28:04.920
 It's a different quantity.

1:28:04.920 --> 1:28:09.920
 Those latter ones are almost all of them have more of a frequentist flavor.

1:28:09.920 --> 1:28:14.760
 They say, given the truth is that the null hypothesis is true, here's what accuracy I

1:28:14.760 --> 1:28:17.200
 would get or given that the alternative is true.

1:28:17.200 --> 1:28:18.200
 Here's what I would get.

1:28:18.200 --> 1:28:22.360
 So it's kind of going forward from the state of nature to the data.

1:28:22.360 --> 1:28:25.880
 The Bayesian goes the other direction from the data back to the state of nature.

1:28:25.880 --> 1:28:28.160
 And that's actually what false discovery rate is.

1:28:28.160 --> 1:28:32.640
 It says, given you made a discovery, okay, that's conditioned on your data.

1:28:32.640 --> 1:28:36.880
 What's the probability of the hypothesis is going the other direction.

1:28:36.880 --> 1:28:40.400
 And so the classical frequents look at that and say, well, I can't know that there's

1:28:40.400 --> 1:28:42.560
 some priors needed in that.

1:28:42.560 --> 1:28:46.920
 And the empirical Bayesian goes ahead and plows forward and starts writing down to these

1:28:46.920 --> 1:28:50.760
 formulas and realizes at some point, some of those things can actually be estimated

1:28:50.760 --> 1:28:52.560
 in a reasonable way.

1:28:52.560 --> 1:28:54.200
 And so it's a beautiful set of ideas.

1:28:54.200 --> 1:28:58.640
 So this kind of line of argument has come out, it's not certainly mine, but it sort

1:28:58.640 --> 1:29:02.320
 of came out from Robbins around 1960.

1:29:02.320 --> 1:29:07.320
 Brad Efron has written beautifully about this in various papers and books.

1:29:07.320 --> 1:29:14.120
 And the FDR is, you know, Ben Yamini in Israel, John Story did this Bayesian interpretation

1:29:14.120 --> 1:29:15.120
 and so on.

1:29:15.120 --> 1:29:18.280
 So I've just absorbed these things over the years and finally did a very healthy way to

1:29:18.280 --> 1:29:21.280
 think about statistics.

1:29:21.280 --> 1:29:28.240
 Let me ask you about intelligence to jump slightly back out into philosophy, perhaps.

1:29:28.240 --> 1:29:33.960
 You said that maybe you can elaborate, but you said that defining just even the question

1:29:33.960 --> 1:29:38.800
 of what is intelligence is a very difficult question.

1:29:38.800 --> 1:29:39.800
 Is it a useful question?

1:29:39.800 --> 1:29:43.920
 Do you think we'll one day understand the fundamentals of human intelligence and what

1:29:43.920 --> 1:29:51.720
 it means, you know, have good benchmarks for general intelligence that we put before

1:29:51.720 --> 1:29:53.560
 our machines?

1:29:53.560 --> 1:29:55.480
 So I don't work on these topics so much.

1:29:55.480 --> 1:29:59.680
 You're really asking the question of for a psychologist, really, and I studied some,

1:29:59.680 --> 1:30:04.480
 but I don't consider myself at least an expert at this point.

1:30:04.480 --> 1:30:07.720
 You know, a psychologist aims to understand human intelligence, right?

1:30:07.720 --> 1:30:11.200
 And I think maybe the psychologists, I know are fairly humble about this.

1:30:11.200 --> 1:30:15.920
 They might try to understand how a baby understands, you know, whether something's a solid or liquid

1:30:15.920 --> 1:30:18.760
 or whether something's hidden or not.

1:30:18.760 --> 1:30:24.440
 And maybe how a child starts to learn the meaning of certain words, what's a verb, what's

1:30:24.440 --> 1:30:30.600
 a noun, and also, you know, slowly but surely trying to figure out things.

1:30:30.600 --> 1:30:35.640
 But humans ability to take a really complicated environment, reason about it, abstract about

1:30:35.640 --> 1:30:41.480
 it, find the right abstractions, communicate about it, interact and so on is just, you

1:30:41.480 --> 1:30:46.920
 know, really staggeringly rich and complicated.

1:30:46.920 --> 1:30:51.320
 And so, you know, I think in all humidly, we don't think we're kind of aiming for that

1:30:51.320 --> 1:30:55.000
 in the near future, a certain like psychologist doing experiments with babies in the lab or

1:30:55.000 --> 1:30:58.920
 with people talking has a much more limited aspiration.

1:30:58.920 --> 1:31:02.520
 And you know, Kahneman Dversky would look at our reasoning patterns and they're not deeply

1:31:02.520 --> 1:31:06.440
 understanding all the how we do our reasoning, but they're sort of saying, here's some oddities

1:31:06.440 --> 1:31:09.480
 about the reasoning and some things you need to think about it.

1:31:09.480 --> 1:31:14.520
 But also, as I emphasize in some things I've been writing about, you know, AI, the revolution

1:31:14.520 --> 1:31:15.520
 hasn't happened yet.

1:31:15.520 --> 1:31:16.520
 Yeah.

1:31:16.520 --> 1:31:17.520
 Great blog post.

1:31:17.520 --> 1:31:22.560
 I've been emphasizing that, you know, if you step back and look at intelligence systems

1:31:22.560 --> 1:31:26.640
 of any kind and whatever you mean by intelligence, it's not just the humans or the animals or

1:31:26.640 --> 1:31:31.640
 you know, the plants or whatever, you know, so a market that brings goods into a city,

1:31:31.640 --> 1:31:35.640
 you know, food to restaurants or something every day is a system.

1:31:35.640 --> 1:31:39.480
 It's a decentralized set of decisions looking at it from far enough away, just like a collection

1:31:39.480 --> 1:31:40.480
 of neurons.

1:31:40.480 --> 1:31:44.560
 Everyone, every neuron is making its own little decisions, presumably in some way.

1:31:44.560 --> 1:31:47.920
 And if you step back enough, every little part of an economic system is making it solid

1:31:47.920 --> 1:31:49.560
 of its decisions.

1:31:49.560 --> 1:31:53.000
 And just like with a brain, who knows what any of the neuron doesn't know what the overall

1:31:53.000 --> 1:31:57.120
 goal is, right, but something happens at some aggregate level.

1:31:57.120 --> 1:31:58.560
 Same thing with the economy.

1:31:58.560 --> 1:32:01.360
 People eat in a city and it's robust.

1:32:01.360 --> 1:32:04.840
 It works at all scales, small villages to big cities.

1:32:04.840 --> 1:32:10.520
 It's been working for thousands of years, it works rain or shine, so it's adaptive.

1:32:10.520 --> 1:32:14.800
 So all the kind of, you know, those are adjectives, one tends to apply to intelligent systems,

1:32:14.800 --> 1:32:19.120
 robust, adaptive, you know, you don't need to keep adjusting it, it's self, self healing,

1:32:19.120 --> 1:32:24.680
 whatever, plus not perfect, you know, intelligences are never perfect and markets are not perfect.

1:32:24.680 --> 1:32:28.160
 But I do not believe in this ear that you cannot, that you can say, well, our computers

1:32:28.160 --> 1:32:31.760
 are humans are smart, but you know, no markets are not more markets are.

1:32:31.760 --> 1:32:34.080
 So they are intelligent.

1:32:34.080 --> 1:32:38.120
 Now we humans didn't evolve to be markets.

1:32:38.120 --> 1:32:43.280
 We've been participating in them, right, but we are not ourselves a market per se.

1:32:43.280 --> 1:32:45.200
 The neurons could be viewed as the market.

1:32:45.200 --> 1:32:46.200
 You can.

1:32:46.200 --> 1:32:49.440
 There's economic, you know, neuroscience kind of perspective, that's interesting to pursue

1:32:49.440 --> 1:32:50.440
 all that.

1:32:50.440 --> 1:32:54.160
 The point though is, is that if you were to study humans and really be a world's best

1:32:54.160 --> 1:32:57.440
 psychologist, studied for thousands of years and come up with the theory of human intelligence,

1:32:57.440 --> 1:33:02.040
 you might have never discovered principles of markets, you know, spy demand curves and

1:33:02.040 --> 1:33:05.000
 you know, matching and auctions and all that.

1:33:05.000 --> 1:33:08.720
 Those are real principles and they lead to an form of intelligence that's not maybe human

1:33:08.720 --> 1:33:09.720
 intelligence.

1:33:09.720 --> 1:33:11.440
 It's arguably another kind of intelligence.

1:33:11.440 --> 1:33:14.880
 There probably are third kinds of intelligence or fourth that none of us are really thinking

1:33:14.880 --> 1:33:16.480
 too much about right now.

1:33:16.480 --> 1:33:20.840
 So if you really, and then all those are relevant to computer systems in the future, certainly

1:33:20.840 --> 1:33:25.640
 the market one is relevant right now, whereas understanding human intelligence is not so

1:33:25.640 --> 1:33:29.320
 clear that it's relevant right now, probably not.

1:33:29.320 --> 1:33:33.160
 So if you want general intelligence, whatever one means by that or, you know, understanding

1:33:33.160 --> 1:33:37.000
 intelligence in a deep sense and all that, it is definitely has to be not just human

1:33:37.000 --> 1:33:38.000
 intelligence.

1:33:38.000 --> 1:33:39.280
 It's got to be this broader thing.

1:33:39.280 --> 1:33:40.480
 And that's not a mystery.

1:33:40.480 --> 1:33:41.480
 Markets are intelligent.

1:33:41.480 --> 1:33:45.440
 So you know, it's definitely not just a philosophical sense to say, we got to move beyond intelligence,

1:33:45.440 --> 1:33:46.440
 human intelligence.

1:33:46.440 --> 1:33:47.440
 That sounds ridiculous.

1:33:47.440 --> 1:33:48.440
 Yeah.

1:33:48.440 --> 1:33:49.440
 But it's not.

1:33:49.440 --> 1:33:52.240
 And in a block, we'll see to find different kinds of like intelligent infrastructure,

1:33:52.240 --> 1:33:54.320
 II, which I really like.

1:33:54.320 --> 1:34:00.440
 Some of the concept you just been describing, do you see ourselves, we see earth, human

1:34:00.440 --> 1:34:02.680
 civilization as a single organism.

1:34:02.680 --> 1:34:07.000
 Do you think the intelligence of that organism, when you think from the perspective of markets

1:34:07.000 --> 1:34:10.880
 and intelligence infrastructure is increasing?

1:34:10.880 --> 1:34:12.360
 Is it increasing linearly?

1:34:12.360 --> 1:34:14.240
 Is it increasing exponentially?

1:34:14.240 --> 1:34:15.960
 What do you think the future of that intelligence?

1:34:15.960 --> 1:34:16.960
 Yeah, I don't know.

1:34:16.960 --> 1:34:20.600
 I don't tend to think, I don't tend to answer questions like that because, you know, that's

1:34:20.600 --> 1:34:21.600
 science.

1:34:21.600 --> 1:34:24.040
 I was hoping to catch your off guard.

1:34:24.040 --> 1:34:30.360
 Well, again, because you said it's so far in the future, it's fun to ask and you'll

1:34:30.360 --> 1:34:36.440
 probably, you know, like you said, predicting the future is really nearly impossible.

1:34:36.440 --> 1:34:43.680
 But say as an axiom, one day we create a human level, a super human level intelligent, not

1:34:43.680 --> 1:34:47.520
 the scale of markets, but the scale of an individual.

1:34:47.520 --> 1:34:51.760
 What do you think is, what do you think it would take to do that?

1:34:51.760 --> 1:34:58.880
 Or maybe to ask another question is how would that system be different than the biological

1:34:58.880 --> 1:35:01.480
 human beings that we see around us today?

1:35:01.480 --> 1:35:05.720
 Is it possible to say anything interesting to that question or is it just a stupid question?

1:35:05.720 --> 1:35:09.120
 It's not a stupid question, but it's science fiction.

1:35:09.120 --> 1:35:12.120
 And so I'm totally happy to read science fiction and think about it from time to time

1:35:12.120 --> 1:35:13.400
 in my own life.

1:35:13.400 --> 1:35:17.080
 I loved that there was this like brain in a vat kind of, you know, little thing that

1:35:17.080 --> 1:35:18.920
 people were talking about when I was a student.

1:35:18.920 --> 1:35:24.400
 I remember, you know, imagine that, you know, between your brain and your body, there's,

1:35:24.400 --> 1:35:26.960
 you know, there's a bunch of wires, right?

1:35:26.960 --> 1:35:32.000
 And suppose that every one of them was replaced with a literal wire and then suppose that

1:35:32.000 --> 1:35:36.000
 wire was turned into actually a little wireless, you know, there's a receiver and sender.

1:35:36.000 --> 1:35:41.720
 So the brain has got all the senders and receiver, you know, on all of its exiting, you know,

1:35:41.720 --> 1:35:45.880
 axons and all the dendrites down in the body are replaced with senders and receivers.

1:35:45.880 --> 1:35:50.040
 Now you could move the body off somewhere and put the brain in a vat, right?

1:35:50.040 --> 1:35:54.560
 And then you could do things like start killing off those senders receivers one by one.

1:35:54.560 --> 1:35:56.920
 And after you've killed off all of them, where is that person?

1:35:56.920 --> 1:35:59.640
 You know, they thought they were out in the body walking around the world and they moved

1:35:59.640 --> 1:36:00.640
 on.

1:36:00.640 --> 1:36:01.640
 So those are science fiction things.

1:36:01.640 --> 1:36:02.640
 Those are fun to think about.

1:36:02.640 --> 1:36:05.720
 It's just intriguing about where's, what is thought, where is it and all that.

1:36:05.720 --> 1:36:10.640
 And I think every 18 year old, it's to take philosophy classes and think about these things.

1:36:10.640 --> 1:36:13.400
 And I think that everyone should think about what could happen in society that's kind of

1:36:13.400 --> 1:36:14.400
 bad and all that.

1:36:14.400 --> 1:36:17.560
 But I really don't think that's the right thing for most of us that are my age group

1:36:17.560 --> 1:36:19.480
 to be doing and thinking about.

1:36:19.480 --> 1:36:26.720
 I really think that we have so many more present, you know, first challenges and dangers and

1:36:26.720 --> 1:36:32.320
 real things to build and all that, such that, you know, spending too much time on science

1:36:32.320 --> 1:36:36.080
 fiction, at least in public fora like this, I think is, is not what we should be doing.

1:36:36.080 --> 1:36:37.600
 Maybe over beers in private.

1:36:37.600 --> 1:36:38.600
 That's right.

1:36:38.600 --> 1:36:43.080
 I'm well, well, I'm not going to broadcast where I have beers because this is going to

1:36:43.080 --> 1:36:44.080
 go on Facebook.

1:36:44.080 --> 1:36:49.440
 And I know a lot of people showing up there, but yeah, I'll, I love Facebook.

1:36:49.440 --> 1:36:56.720
 Twitter, Amazon, YouTube, I have, I'm optimistic and hopeful, but maybe, maybe I don't have

1:36:56.720 --> 1:37:00.160
 grounds for such optimism and hope.

1:37:00.160 --> 1:37:07.160
 Let me ask you, you've mentored some of the brightest sort of some of the seminal figures

1:37:07.160 --> 1:37:08.360
 in the field.

1:37:08.360 --> 1:37:14.120
 Can you give advice to people who are undergraduates today?

1:37:14.120 --> 1:37:17.640
 What does it take to take, you know, advice on their journey if they're interested in

1:37:17.640 --> 1:37:23.720
 machine learning and AI and, and the ideas of markets from economics and psychology and

1:37:23.720 --> 1:37:25.720
 all the kinds of things that you're exploring?

1:37:25.720 --> 1:37:27.960
 What, what, what steps they take on that journey?

1:37:27.960 --> 1:37:30.360
 Well, yeah, first of all, the door is open and second, it's a journey.

1:37:30.360 --> 1:37:33.880
 I like your language there.

1:37:33.880 --> 1:37:37.680
 It is not that you're so brilliant and you have great brilliant ideas and therefore that's,

1:37:37.680 --> 1:37:42.440
 that's just, you know, that's how you have success or that's how you enter into the field.

1:37:42.440 --> 1:37:46.960
 It's that you apprentice yourself, you, you spend a lot of time, you work on hard things,

1:37:46.960 --> 1:37:51.560
 you try and pull back and you be as broad as you can.

1:37:51.560 --> 1:37:56.960
 You talk to lots of people and it's like entering in any kind of a creative community.

1:37:56.960 --> 1:38:01.560
 There's years that are needed and human connections are critical to it.

1:38:01.560 --> 1:38:06.040
 So, you know, I think about, you know, being a musician or being an artist or something,

1:38:06.040 --> 1:38:10.600
 you don't just, you know, immediately from day one, you know, you're a genius and therefore

1:38:10.600 --> 1:38:11.600
 you do it.

1:38:11.600 --> 1:38:19.040
 So, you know, practice really, really hard on basics and you be humble about where you

1:38:19.040 --> 1:38:22.160
 are and then you realize you'll never be an expert on everything.

1:38:22.160 --> 1:38:27.680
 So, you kind of pick and then there's a lot of randomness and a lot of kind of luck.

1:38:27.680 --> 1:38:31.600
 But luck just kind of picks out which branch of the tree you go down, but you'll go down

1:38:31.600 --> 1:38:33.960
 some branch.

1:38:33.960 --> 1:38:35.480
 So yeah, it's a community.

1:38:35.480 --> 1:38:38.800
 So the graduate school is a, I still think is one of the wonderful phenomena that we

1:38:38.800 --> 1:38:40.800
 have in our, in our world.

1:38:40.800 --> 1:38:43.160
 It's very much about apprenticeship with an advisor.

1:38:43.160 --> 1:38:45.800
 It's very much about a group of people you belong to.

1:38:45.800 --> 1:38:47.000
 It's a four or five year process.

1:38:47.000 --> 1:38:51.760
 So it's plenty of time to start from kind of nothing to come up to something, you know,

1:38:51.760 --> 1:38:55.220
 more expertise and then start to have your own creativity start to flower or even surprise

1:38:55.220 --> 1:38:58.240
 into your own self.

1:38:58.240 --> 1:38:59.760
 And it's a very cooperative endeavor.

1:38:59.760 --> 1:39:05.520
 It's I think a lot of people think of science as highly competitive and I think in some

1:39:05.520 --> 1:39:08.120
 other fields it might be more so.

1:39:08.120 --> 1:39:11.880
 Here it's way more cooperative than you might imagine.

1:39:11.880 --> 1:39:14.640
 And people are always teaching each other something and people are always more than

1:39:14.640 --> 1:39:16.800
 happy to be clear that.

1:39:16.800 --> 1:39:20.600
 So I feel I'm an expert on certain kinds of things, but I'm very much not expert on

1:39:20.600 --> 1:39:24.200
 lots of other things and a lot of them are relevant and a lot of them are, I should know,

1:39:24.200 --> 1:39:26.320
 but it should in some sense, you know, you don't.

1:39:26.320 --> 1:39:32.120
 So I'm always willing to reveal my ignorance to people around me so they can teach me things.

1:39:32.120 --> 1:39:34.240
 And I think a lot of us feel that way about our field.

1:39:34.240 --> 1:39:35.400
 So it's very cooperative.

1:39:35.400 --> 1:39:39.120
 I might add it's also very international because it's so cooperative.

1:39:39.120 --> 1:39:44.080
 We see no barriers and so that the nationalism that you see, especially in the current era

1:39:44.080 --> 1:39:47.480
 and everything is just at odds with the way that most of us think about what we're doing

1:39:47.480 --> 1:39:53.080
 here where this is a human endeavor and we cooperate and are very much trying to do it

1:39:53.080 --> 1:39:56.560
 together for the, you know, the benefit of everybody.

1:39:56.560 --> 1:40:02.840
 So last question, where and how and why did you learn French and which language is more

1:40:02.840 --> 1:40:05.680
 beautiful English or French?

1:40:05.680 --> 1:40:06.680
 Great question.

1:40:06.680 --> 1:40:10.120
 So first of all, I think Italian is actually more beautiful than French and English.

1:40:10.120 --> 1:40:11.120
 And I also speak that.

1:40:11.120 --> 1:40:16.000
 So I'm, I'm, I'm married to an Italian and I have kids and we speak Italian.

1:40:16.000 --> 1:40:23.200
 Anyway, no, all kidding aside, every language allows you to express things a bit differently.

1:40:23.200 --> 1:40:26.840
 And it is one of the great fun things to do in life is to explore those things.

1:40:26.840 --> 1:40:33.480
 So in fact, when I kids or, you know, teens or college kids ask me, what is your study?

1:40:33.480 --> 1:40:36.480
 I say, well, you know, do what your heart, where your heart is, certainly do a lot of

1:40:36.480 --> 1:40:41.120
 math, math is good for everybody, but do some poetry and do some history and do some language

1:40:41.120 --> 1:40:42.120
 too.

1:40:42.120 --> 1:40:45.360
 You know, throughout your life, you'll want to be a thinking person, you'll want to have

1:40:45.360 --> 1:40:47.560
 done that.

1:40:47.560 --> 1:40:54.680
 For me, yeah, French, I learned when I was, I'd say a late teen, I was living in the middle

1:40:54.680 --> 1:40:59.880
 of the country in Kansas and not much was going on in Kansas with all due respect to Kansas.

1:40:59.880 --> 1:41:04.400
 But, and so my parents happened to have some French books on the shelf and just in my boredom,

1:41:04.400 --> 1:41:07.160
 I pulled them down and I found this is fun.

1:41:07.160 --> 1:41:09.240
 And I kind of learned the language by reading.

1:41:09.240 --> 1:41:13.600
 And when I first heard it spoken, I had no idea what was being spoken, but I realized

1:41:13.600 --> 1:41:15.600
 I had somehow knew it from some previous life.

1:41:15.600 --> 1:41:18.560
 And so I made the connection.

1:41:18.560 --> 1:41:23.160
 But then, you know, I traveled and just I love to go beyond my own barriers and my own

1:41:23.160 --> 1:41:27.800
 comfort or whatever, and I found myself in, you know, on trains in France next to say

1:41:27.800 --> 1:41:32.280
 older people who had, you know, lived a whole life of their own and the ability to communicate

1:41:32.280 --> 1:41:38.560
 with them was, was, was, you know, special and ability to also see myself in other people's

1:41:38.560 --> 1:41:43.120
 shoes and have empathy and kind of work on that language as part of that.

1:41:43.120 --> 1:41:49.160
 So, so after that kind of experience and also embedding myself in French culture, which

1:41:49.160 --> 1:41:52.840
 is, you know, quite, quite amazing, you know, languages are rich, not just because there

1:41:52.840 --> 1:41:55.720
 is something inherently beautiful about it, but it's all the creativity that went into

1:41:55.720 --> 1:41:56.720
 it.

1:41:56.720 --> 1:41:59.960
 So I learned a lot of songs, read poems, read books.

1:41:59.960 --> 1:42:05.360
 And then I was here actually at MIT where we're doing the podcast today and a young

1:42:05.360 --> 1:42:11.600
 professor, you know, not yet married and, you know, not having a lot of friends in the

1:42:11.600 --> 1:42:12.600
 area.

1:42:12.600 --> 1:42:14.000
 So I just didn't have, I was kind of a bored person.

1:42:14.000 --> 1:42:16.040
 I said, I heard a lot of Italians around.

1:42:16.040 --> 1:42:18.960
 There's happened to be a lot of Italians at MIT, a lot of Italian professors for some

1:42:18.960 --> 1:42:19.960
 reason.

1:42:19.960 --> 1:42:20.960
 Yeah.

1:42:20.960 --> 1:42:21.960
 And so I was kind of vaguely understanding what they were talking about.

1:42:21.960 --> 1:42:23.680
 I said, well, I should learn this language too.

1:42:23.680 --> 1:42:24.680
 So I did.

1:42:24.680 --> 1:42:29.400
 And then later met my spouse and, you know, Italian became a more important part of my

1:42:29.400 --> 1:42:30.400
 life.

1:42:30.400 --> 1:42:32.200
 But, but I go to China a lot these days.

1:42:32.200 --> 1:42:38.200
 I go to Asia, I go to Europe and every time I go, I kind of am amazed by the richness

1:42:38.200 --> 1:42:42.760
 of human experience and the people don't have any idea if you haven't traveled kind

1:42:42.760 --> 1:42:46.880
 of how amazingly rich and I love the diversity.

1:42:46.880 --> 1:42:48.080
 It's not just a buzzword to me.

1:42:48.080 --> 1:42:49.080
 It really means something.

1:42:49.080 --> 1:42:53.200
 I love the, you know, the, you know, embed myself with other people's experiences.

1:42:53.200 --> 1:42:56.400
 And so, yeah, learning language is a big part of that.

1:42:56.400 --> 1:42:59.840
 I think I've said in some interview at some point that if I had, you know, millions of

1:42:59.840 --> 1:43:03.240
 dollars and infinite time whatever, what would you really work on if you really wanted to

1:43:03.240 --> 1:43:04.240
 do AI?

1:43:04.240 --> 1:43:08.000
 And for me, that is natural language and, and really done right, you know, deep understanding

1:43:08.000 --> 1:43:09.000
 of language.

1:43:09.000 --> 1:43:14.440
 Um, that's to me an amazingly interesting scientific challenge and one we're very far

1:43:14.440 --> 1:43:18.480
 away on one we're very far away, but good natural language people are kind of really

1:43:18.480 --> 1:43:19.480
 invested in that.

1:43:19.480 --> 1:43:22.480
 I think a lot of them see that's where the core of AI is that if you understand that

1:43:22.480 --> 1:43:26.080
 you really help human communication, you understand something about the human mind,

1:43:26.080 --> 1:43:30.320
 the semantics that come out of the human mind and I agree, uh, I think that will be such

1:43:30.320 --> 1:43:31.320
 a long time.

1:43:31.320 --> 1:43:34.720
 So I didn't do that in my career just because I kind of, I was behind in the early days.

1:43:34.720 --> 1:43:36.480
 I didn't kind of know enough of that stuff.

1:43:36.480 --> 1:43:37.480
 I was at MIT.

1:43:37.480 --> 1:43:41.880
 I didn't learn much language, uh, and it was too late at some point to kind of spend a

1:43:41.880 --> 1:43:47.640
 whole career doing that, but I admire that field and, um, and so my little way by learning

1:43:47.640 --> 1:43:52.560
 language, um, you know, kind of, uh, that part of my brain is, um, it's been trained

1:43:52.560 --> 1:43:53.560
 up.

1:43:53.560 --> 1:43:54.560
 Yeah.

1:43:54.560 --> 1:43:55.560
 And it was right.

1:43:55.560 --> 1:43:57.440
 You're truly are the Miles Davis and machine learning.

1:43:57.440 --> 1:43:59.480
 I don't think there's a better place than it was.

1:43:59.480 --> 1:44:01.400
 Mike, it was a huge honor talking to you today.

1:44:01.400 --> 1:44:02.400
 Merci beaucoup.

1:44:02.400 --> 1:44:03.400
 All right.

1:44:03.400 --> 1:44:04.400
 It's been my pleasure.

1:44:04.400 --> 1:44:05.400
 Thank you.

1:44:05.400 --> 1:44:09.240
 Thanks for listening to this conversation with Michael, I, Jordan, and thank you to

1:44:09.240 --> 1:44:14.520
 our presenting sponsor, cash app, download it, use code lex podcast.

1:44:14.520 --> 1:44:20.480
 You get $10 and $10 will go to first an organization that inspires and educates young minds to

1:44:20.480 --> 1:44:23.920
 become science and technology innovators of tomorrow.

1:44:23.920 --> 1:44:28.880
 If you enjoy this podcast, subscribe on YouTube, give it five stars on Apple podcast, support

1:44:28.880 --> 1:44:34.720
 on Patreon, or simply connect with me on Twitter at Lex Friedman.

1:44:34.720 --> 1:44:39.360
 And now let me leave you with some words of wisdom from Michael, I, Jordan from his blog

1:44:39.360 --> 1:44:45.560
 post titled artificial intelligence, the revolution hasn't happened yet calling for broadening

1:44:45.560 --> 1:44:48.560
 the scope of the AI field.

1:44:48.560 --> 1:44:52.860
 We should embrace the fact that what we are witnessing is the creation of a new branch

1:44:52.860 --> 1:44:54.360
 of engineering.

1:44:54.360 --> 1:44:59.800
 The term engineering is often invoked in a narrow sense in academia and beyond with

1:44:59.800 --> 1:45:05.560
 overtones of cold, affect less machinery and negative connotations of loss of control

1:45:05.560 --> 1:45:11.440
 by humans, but an engineering discipline can be what we want it to be in the current

1:45:11.440 --> 1:45:17.520
 era with a real opportunity to conceive of something historically new, a human centric

1:45:17.520 --> 1:45:22.800
 engineering discipline, I will resist giving this emerging discipline a name, but if the

1:45:22.800 --> 1:45:28.280
 acronym AI continues to be used, let's be aware of the very real limitations of this

1:45:28.280 --> 1:45:35.120
 placeholder, let's broaden our scope, tone down the hype and recognize the serious challenges

1:45:35.120 --> 1:45:37.320
 ahead.

1:45:37.320 --> 1:46:05.400
 Thank you for listening and hope to see you next time.

