WEBVTT

00:00.000 --> 00:05.840
 The following is a conversation with Rodney Brooks, one of the greatest roboticists in history.

00:06.400 --> 00:10.640
 He led the Computer Science and Artificial Intelligence Laboratory at MIT,

00:10.640 --> 00:16.000
 then cofounded iRobot, which is one of the most successful robotics companies ever.

00:16.560 --> 00:22.560
 Then he cofounded Rethink Robotics that created some amazing collaborative robots like Baxter

00:22.560 --> 00:30.640
 and Sawyer. Finally, he cofounded Robust.ai, whose mission is to teach robots common sense,

00:30.640 --> 00:36.560
 which is a lot harder than it sounds. To support this podcast, please check out our sponsors

00:36.560 --> 00:42.480
 in the description. As a side note, let me say that Rodney is someone I've looked up to for many years,

00:43.040 --> 00:50.720
 in my now over two decade journey in robotics, because, one, he's a legit great engineer of

00:50.720 --> 00:55.840
 real world systems, and two, he's not afraid to state controversial opinions that challenge

00:55.840 --> 01:02.560
 the way we see the AI world. But of course, while I agree with him on some of his critical views of

01:02.560 --> 01:09.680
 AI, I don't agree with some others. And he's fully supportive of such disagreement. Nobody ever built

01:09.680 --> 01:16.640
 anything great by being fully agreeable. There's always respect and love behind our interactions.

01:16.640 --> 01:21.040
 And when a conversation is recorded, like it was for this podcast, I think a little bit of

01:21.040 --> 01:29.520
 disagreement is fun. This is the Lex Friedman podcast. And here is my conversation with Rodney Brooks.

01:31.760 --> 01:37.040
 What is the most amazing or beautiful robot that you've ever had the chance to work with?

01:37.600 --> 01:43.840
 I think it was Domo, which was made by one of my grad students, Aaron Edsinger. It now sits in

01:43.840 --> 01:50.720
 Daniela Roos's office, director of CSAIL. And it was just a beautiful robot. And Aaron was really

01:50.720 --> 01:56.640
 clever. He didn't give me a budget ahead of time. He didn't tell me what he was going to do. He just

01:56.640 --> 02:03.440
 started spending money. He spent a lot of money. He and Jeff Weber, who was a mechanical engineer,

02:03.440 --> 02:08.720
 who Aaron insisted he bring with him when he became a grad student, built this beautiful,

02:08.720 --> 02:17.040
 gorgeous robot Domo, which is an upper torso humanoid, two arms with fingers, three finger

02:17.040 --> 02:26.080
 hands, and face eyeballs. Not the eyeballs, but everything else, serious elastic actuators.

02:26.960 --> 02:33.760
 You can interact with it. Cable driven, all the motors are inside. And it's just gorgeous.

02:33.760 --> 02:37.440
 The eyeballs are actuated too or no? Oh yeah, the eyeballs are actuated with cameras.

02:37.440 --> 02:44.320
 And so it had a visual attention mechanism, looking when people came in and looking in their face

02:44.320 --> 02:50.960
 and talking with them. Why was it amazing? The beauty of it. You said what was the most beauty?

02:50.960 --> 02:56.080
 What is the most beautiful? It's just mechanically gorgeous. As everything Aaron builds has always

02:56.080 --> 03:02.160
 been mechanically gorgeous. It's just exquisite in the detail. We're talking about mechanically,

03:02.160 --> 03:09.040
 literally the amount of actuators. The actuators, the cables. He anodizes different parts,

03:09.040 --> 03:15.760
 different colors, and it just looks like a work of art. What about the face? Do you find the face

03:15.760 --> 03:22.480
 beautiful in robots? When you make a robot, it's making a promise for how well it will be able

03:22.480 --> 03:28.880
 to interact. So I always encourage my students not to overpromise. Even with its essence,

03:28.880 --> 03:34.160
 like the thing it presents, it should not overpromise. Yeah, so the joke I make, which I think

03:34.160 --> 03:39.440
 you'll get, is if your robot looks like Albert Einstein, it should be as smart as Albert Einstein.

03:39.440 --> 03:47.520
 So the only thing in Domo's face is the eyeballs. And because that's all it can do,

03:47.520 --> 03:53.840
 it can look at you and pay attention. And so there is no, it's not like one of those

03:53.840 --> 04:00.000
 Japanese robots that looks exactly like a person at all. But see, the thing is,

04:00.000 --> 04:07.840
 us humans and dogs, too, don't just use eyes as attentional mechanisms. They also use it to

04:07.840 --> 04:12.240
 communicate as part of the communication. Like a dog can look at you, look at another thing,

04:12.240 --> 04:16.160
 and look back at you. And that designates that we're going to be looking at that thing together.

04:16.160 --> 04:23.440
 Yeah, or intent. And on both Baxter and Sawyer at Rethink Robotics, they had a screen

04:23.440 --> 04:28.560
 with, you know, graphic eyes. So it wasn't actually where the cameras were pointing,

04:28.560 --> 04:34.640
 but the eyes would look in the direction it was about to move its arm. So people in the factory

04:34.640 --> 04:41.440
 nearby were not surprised by its motions because it gave that intent away. Before we talk about

04:41.440 --> 04:46.640
 Baxter, which I think is a beautiful robot, let's go back to the beginning. When did you first

04:47.200 --> 04:51.600
 fall in love with robotics? We're talking about beauty and love to open the conversation. This

04:51.600 --> 05:00.560
 is great. I was born in the end of 1954. I grew up in Adelaide, South Australia. And I have these

05:00.560 --> 05:08.640
 two books that are dated 1961. So I'm guessing my mother found them in a store in 62 or 63.

05:09.280 --> 05:15.280
 How and Why Wonder Books? How and Why Wonder Book of Electricity? And How and Why Wonder Book of

05:15.280 --> 05:24.320
 Giant Brains and Robots. And I learned how to build circuits, you know, when I was eight or nine,

05:24.320 --> 05:31.600
 simple circuits. And I read, you know, I learned the binary system and saw all these drawings

05:31.600 --> 05:40.240
 mostly of robots. And then I tried to build them for the rest of my childhood. Wait, 61, you said?

05:40.240 --> 05:44.960
 This was when the two books, I've still got them at home. What does the robot mean in that

05:44.960 --> 05:52.400
 context? No, they were some of the robots that they had were arms, you know, big arms to move

05:52.400 --> 05:58.080
 nuclear material around. But they had pictures of welding robots that look like humans under the

05:58.080 --> 06:05.120
 sea welding stuff underwater. So they weren't real robots. But they were, you know, what people

06:05.120 --> 06:10.000
 were thinking about for robots? What were you thinking about? Were you thinking about humanoids?

06:10.000 --> 06:14.240
 Were you thinking about arms with fingers? Were you thinking about faces or cars? No,

06:14.240 --> 06:20.240
 actually, to be honest, I realized my limitation on building mechanical stuff. So I just built

06:20.800 --> 06:31.040
 the brains mostly out of different technologies as I got older. I built a learning system, which

06:32.000 --> 06:40.000
 was chemical based. And I had this ice cube tray each. Well, was a cell. And by applying

06:40.000 --> 06:45.440
 voltage to the two electrodes, it would build up a copper bridge. So over time, it would learn

06:46.720 --> 06:53.920
 a simple network. So I could teach it stuff. And that was mostly things were driven by my budget.

06:54.480 --> 07:02.480
 And nails as electrodes and ice cube tray was about my budget at that stage. Later,

07:02.480 --> 07:07.520
 I managed to buy a transistors. And then I could build gates and flip flops and stuff.

07:07.520 --> 07:11.280
 So one one of your first robots was an ice cube tray. Yeah.

07:12.880 --> 07:18.160
 And it was very cerebral because it went to add. Very nice. Well,

07:19.440 --> 07:27.280
 just a decade or so before in 1950, Alan Turing wrote the paper that formulated the Turing test.

07:27.280 --> 07:34.160
 And he opened that paper with the question, can machines think? So let me ask you this question.

07:34.160 --> 07:39.120
 Can machines think? Can your ice cube tray one day think?

07:40.720 --> 07:45.040
 Certainly machines can think because I believe you're a machine and I'm a machine and I believe

07:45.040 --> 07:51.520
 we both think. I think any other philosophical position is sort of a little ludicrous. What

07:51.520 --> 07:59.120
 does think mean if it's not something that we do? And we are machines. So yes, machines can,

07:59.120 --> 08:03.520
 but do we have a clue how to build such machines? That's a very different question.

08:03.520 --> 08:08.960
 Are we capable of building such machines? Are we smart enough? We think we're smart enough to do

08:08.960 --> 08:15.120
 anything, but maybe we're not. Maybe we're just not smart enough to build. It's not like us.

08:15.120 --> 08:19.440
 The kind of computer that Alan Turing was thinking about, do you think there is something

08:20.160 --> 08:25.520
 fundamentally or significantly different between the computer between our ears,

08:25.520 --> 08:32.240
 the biological computer that humans use, and the computer that he was thinking about from a

08:32.240 --> 08:38.080
 sort of high level philosophical? Yeah, I believe that it's very wrong. In fact,

08:38.080 --> 08:45.760
 I'm halfway through a, I think it'll be about a 480 page book titled, the working title is

08:45.760 --> 08:53.600
 Not Even Wrong. And if I may, I'll tell you a bit about that book. So there's two three thrusts

08:53.600 --> 08:59.840
 to it. One is the history of computation, what we call computation, goes all the way back to

08:59.840 --> 09:09.040
 some manuscripts in Latin from 1614 and 1620 by Napier and Kepler through Babbage and Lovelace.

09:09.600 --> 09:19.520
 And then Turing's 1936 paper is what we think of as the invention of modern computation.

09:20.320 --> 09:26.880
 And that paper, by the way, did not set out to invent computation. It set out to

09:26.880 --> 09:33.200
 negatively answer one of Hilbert's three later set of problems. He called it

09:35.040 --> 09:42.880
 as an effective way of getting answers. And Hilbert really worked with rewriting rules as did

09:45.680 --> 09:52.800
 a church who also, at the same time, a month earlier than Turing, disproved Hilbert's

09:52.800 --> 09:56.960
 one of these three hypotheses. The other two had already been disproved by Godel.

09:56.960 --> 10:02.000
 So Turing set out to disprove it because it's always easier to disprove these things than to

10:02.000 --> 10:09.840
 prove that there is an answer. And so he needed, and it really came from his

10:11.120 --> 10:16.000
 professor, I was an undergrad at Cambridge who said, who turned it into, is there a mechanical

10:16.000 --> 10:23.680
 process? So he wanted to show a mechanical process that could calculate numbers because

10:23.680 --> 10:29.280
 that was a mechanical process that people used to generate tables. They were called computers,

10:29.280 --> 10:33.600
 the people at the time. And they followed a set of rules where they had paper,

10:33.600 --> 10:38.480
 and they would write numbers down. And based on the numbers, they'd keep writing other numbers.

10:38.480 --> 10:46.800
 And they would produce numbers for these tables, engineering tables, that the more iterations

10:46.800 --> 10:54.720
 they did, the more significant digits came out. And so Turing, in that paper, set out to define

10:54.720 --> 11:00.720
 what sort of machine could do that mechanical machine, where it could produce an arbitrary

11:00.720 --> 11:11.360
 number of digits in the same way a human computer did. And he came up with a very simple set of

11:11.360 --> 11:17.440
 constraints where there was an infinite supply of paper. This is the tape of the Turing machine.

11:18.000 --> 11:25.680
 And each Turing machine had a set, came with a set of instructions that, as a person could do

11:25.680 --> 11:31.120
 with pencil and paper, write down things on the tape and erase them and put new things there. And

11:32.080 --> 11:39.040
 he was able to show that that system was not able to do something that Hilbert hypothesized. So he

11:39.040 --> 11:46.560
 disproved it. But he had to show that this system was good enough to do whatever could be done,

11:46.560 --> 11:51.440
 but couldn't do this other thing. And there he said, and he says in the paper,

11:51.440 --> 11:56.960
 I don't have any real arguments for this, but based on intuition. So that's how he defined

11:56.960 --> 12:03.920
 computation. And then if you look over the next from 1936 up until really around 1975,

12:04.640 --> 12:11.840
 you see people struggling with, is this really what computation is? And so Marvin Minsky,

12:11.840 --> 12:18.480
 very well known in AI, but also a fantastic mathematician in his book, Finite and Infant

12:18.480 --> 12:22.640
 Machines from the mid 60s, which is a beautiful, beautiful mathematical book,

12:24.080 --> 12:28.240
 says at the start of the book, well, what is computation? Turing says this and,

12:28.240 --> 12:31.600
 yeah, I sort of think it's that. It doesn't really matter whether the stuff's made of wood

12:31.600 --> 12:39.280
 or plastic. It's just relatively cheap stuff can do this stuff. And so, yeah, seems like computation.

12:39.280 --> 12:48.240
 And Donald Knuth, in his first volume of his Art of Computer Programming in around 1968,

12:48.240 --> 12:55.280
 says, well, what's computation? It's the stuff like Turing says that a person could do each step

12:55.280 --> 13:01.920
 without too much trouble. And so one of his examples of what would be too much trouble was

13:02.560 --> 13:08.320
 a step which required knowing whether Fermat's last theorem was true or not, because it was not

13:08.320 --> 13:15.680
 known at the time. And that's too much trouble for a person to do as a step. And Hopcroft and

13:15.680 --> 13:23.200
 Oldman sort of said a similar thing later that year. And by 1975, in the Aho Hopcroft and Oldman

13:23.200 --> 13:27.920
 book, they're saying, well, you know, we don't really know what computation is, but intuition

13:27.920 --> 13:34.560
 says this is sort of about right. And this is what it is. That's computation. It says sort of

13:34.560 --> 13:40.960
 agreed upon thing, which happens to be really easy to implement in silicon. And then we had

13:40.960 --> 13:46.000
 Moore's Law, which took off and it's been an incredibly powerful tool. Certainly wouldn't

13:46.000 --> 13:50.160
 argue with that. The version we have of computation, incredibly powerful.

13:50.160 --> 13:55.200
 Can we just take a pause? So what we're talking about is there's an infinite tape with some

13:55.200 --> 13:59.760
 simple rules of how to write on that tape. And that's that's what we're kind of thinking about.

13:59.760 --> 14:03.760
 This is computation. Yeah. And it's modeled after humans, how humans do stuff. And

14:03.760 --> 14:09.840
 I think it's a, Turing says in the 36 paper, one of the critical facts here is that a human

14:09.840 --> 14:15.680
 has a limited amount of memory. So that's what we're going to put onto our mechanical computers.

14:18.320 --> 14:24.720
 So, you know, I'm like mass, I'm like mass or charge or, you know, it's not, it's not given

14:24.720 --> 14:29.680
 by the universe. It was, this is what we're going to call computation. And then it has this really,

14:29.680 --> 14:34.880
 you know, it had this really good implementation, which has completely changed our technological

14:34.880 --> 14:43.600
 work. That's computation. Second part of the book, I, or argument in the book, I have this

14:43.600 --> 14:52.880
 two by two matrix with science in the top row, engineering the bottom row, left column is

14:52.880 --> 14:59.600
 intelligence, right column is life. So in the bottom row, the engineering, there's artificial

14:59.600 --> 15:04.880
 intelligence and there's artificial life. In the top row, there's neuroscience and

15:04.880 --> 15:10.800
 abiogenesis. How does living matter turn in, how does nonliving matter become living matter?

15:10.800 --> 15:18.400
 Yes. Four disciplines. These four disciplines all came into the current form in the period

15:18.400 --> 15:25.440
 1945 to 1965. That's interesting. There was neuroscience before, but it wasn't

15:25.440 --> 15:29.280
 effective neuroscience. It was, you know, there was ganglion and there's electrical charges,

15:29.280 --> 15:34.960
 but no one knows what to do with it. And furthermore, there are a lot of players who are common

15:34.960 --> 15:41.600
 across them. I've identified common players except for artificial intelligence and abiogenesis,

15:41.600 --> 15:46.560
 I don't have, but for any other pair, I can point to people who work in and a whole bunch of them

15:46.560 --> 15:54.880
 by the way. We're at the research lab for electronics at MIT, where Warren McCulloch held

15:54.880 --> 16:04.240
 forth. And in fact, McCulloch, Pitts, Letvin and Maturana wrote the first paper on functional

16:04.240 --> 16:08.720
 neuroscience called What the Frog's Eye Tells the Frog's Brain, where instead of it just being

16:08.720 --> 16:15.280
 this bunch of nerves, they sort of showed what different anatomical components that

16:15.280 --> 16:22.000
 anatomical components were doing and telling other anatomical components and, you know,

16:22.000 --> 16:27.600
 generating behavior in the frog. Would you put them as basically the fathers or one of the early

16:28.240 --> 16:31.440
 pioneers of what are now called artificial neural networks?

16:33.200 --> 16:40.480
 Yeah. I mean, McCulloch and Pitts. Pitts was much younger than him. In 1943,

16:40.480 --> 16:48.400
 had written a paper inspired by Bertrand Russell on a calculus for the ideas imminent in

16:49.120 --> 16:57.200
 neural systems, where they had tried to, without any real proof, they had tried to give a formalism

16:57.200 --> 17:06.400
 for neurons, basically in terms of logic and gates or gates and not gates, with no real evidence

17:06.400 --> 17:11.120
 that that was what was going on. But they talked about it. And that was picked up by Minsky for

17:11.120 --> 17:19.280
 his 1953 dissertation on, which was a neural network, we'll call it today. It was picked up by

17:21.200 --> 17:27.520
 John von Neumann when he was designing the EDVAC computer in 1945. He talked about its

17:27.520 --> 17:32.400
 components being neurons based on, I mean, references. He's only got three references

17:32.400 --> 17:38.560
 and one of them is the McCulloch Pets paper. So all these people and then the AI people and the

17:38.560 --> 17:42.800
 artificial life people, which was John von Neumann originally, is that overlap?

17:43.600 --> 17:47.600
 They're all going around the same time. And three of these four disciplines

17:47.600 --> 17:54.480
 turn to computation as their primary metaphor. So I've got a couple of chapters in the book.

17:54.480 --> 18:00.800
 One is titled, wait, computers are people, because that's where our computers came from.

18:00.800 --> 18:06.400
 Yeah. And, you know, from people who are computing stuff. And then another chapter,

18:07.120 --> 18:11.920
 wait, people are computers, which is about computational neuroscience. So there's this

18:11.920 --> 18:19.680
 whole circle here. And that computation is it. And, you know, I have talked to people about,

18:19.680 --> 18:24.960
 well, maybe it's not computation that goes on in the head. Of course it is. Yeah. Okay. Well,

18:24.960 --> 18:32.800
 well, when Elon Musk's rocket goes up, is it computing? Is that how it gets into orbit?

18:32.800 --> 18:37.520
 By computing? But we've got this idea. If you want to build an AI system, you'll write a computer

18:37.520 --> 18:46.080
 program. Yeah. In a sense, so the word computation very quickly starts doing a lot of work that

18:46.080 --> 18:51.920
 was not initially intended to do. It's just like going to say, if you talk about the universe

18:51.920 --> 18:56.160
 as essentially performing a computation. Yeah, right. Wolfram does this. He turns it into

18:56.160 --> 19:01.040
 computation. You don't turn rockets into computation. Yeah. By the way, when you say

19:01.040 --> 19:05.760
 computation in our conversation, do you tend to think of computation narrowly in the way

19:05.760 --> 19:18.000
 touring thought of computation? It's gotten very, you know, squishy. Yeah. Squishy. But

19:18.000 --> 19:23.840
 computation in the way touring thinks about it and the way most people think about it actually

19:23.840 --> 19:32.320
 fits very well with thinking like a hunter gatherer. There are places and there can be stuff in

19:32.320 --> 19:38.480
 places and the stuff in places can change and it stays there until someone changes it. And it's

19:38.480 --> 19:46.400
 this metaphor of place and container, which, you know, is a combination of our place cells in our

19:46.400 --> 19:53.440
 hippocampus and cortex. But this is how we use metaphors for mostly to think about. And when

19:53.440 --> 19:59.680
 we get outside of our metaphor range, we have to invent tools, which we can sort of switch on to

19:59.680 --> 20:06.000
 use. So calculus is an example of a tool. It can do stuff that our raw reasoning can't do and we've

20:06.000 --> 20:13.680
 got conventions of when you can use it or not. But sometimes, you know, people try to, all the

20:13.680 --> 20:20.320
 time, we always try to get physical metaphors for things, which is why quantum mechanics has

20:20.320 --> 20:24.720
 been such a problem for 100 years because it's a particle. No, it's a wave. It's got to be something

20:24.720 --> 20:29.280
 we understand. And I say, no, it's some weird mathematical logic that's different from those.

20:29.280 --> 20:36.160
 But we want that metaphor. Well, you know, I suspect that, you know, 100 years or 200 years

20:36.160 --> 20:40.480
 from now, neither quantum mechanics nor dark matter will be talked about in the same terms,

20:40.480 --> 20:48.080
 you know, in the same way that Flodgerton's theory eventually went away because it just wasn't an

20:48.080 --> 20:55.360
 adequate explanatory metaphor. You know, that metaphor was the stuff. There is stuff in the

20:55.360 --> 21:00.080
 burning. The burning is in the matter. It turns out the burning was outside the matter. It was

21:00.080 --> 21:06.880
 the oxygen. So our desire for metaphor and combined with our limited cognitive capabilities gets us

21:06.880 --> 21:11.840
 into trouble. That's my argument in this book. Now, and people say, well, what is it then? And I

21:11.840 --> 21:16.880
 say, well, I wish I knew that right about that. But I, you know, I give some ideas. But so, so

21:16.880 --> 21:24.800
 there's the three things. Computation is sort of a particular thing we use. Oh, can I tell you

21:24.800 --> 21:29.600
 one beautiful thing? One beautiful thing. So, you know, I used an example of a thing that's

21:29.600 --> 21:35.280
 different from computation. You hit a drum and it vibrates. And there are some, some stationary

21:35.280 --> 21:38.800
 points on the drum surface, you know, because the wave is going up and down the stationary points.

21:40.720 --> 21:48.720
 Now, you could compute them to arbitrary precision. But the drum just knows them. The

21:48.720 --> 21:53.680
 drum doesn't have to compute. What was the very first computer program ever written by Adel

21:53.680 --> 21:59.120
 Lovelace to compute Bernoulli numbers? And Bernoulli numbers are exactly what you need to find those

21:59.120 --> 22:06.800
 stable points in the drum surface. Wow. Anyway, and there was a bug in the program. The arguments

22:06.800 --> 22:12.720
 to divide were reversed in one place. And it still worked. Well, she's never got to run it. They

22:12.720 --> 22:16.160
 never built the analytical engine. She wrote the program without, without it, you know.

22:18.960 --> 22:25.920
 So, so computation. Computation is sort of, you know, a thing that's become dominant as a metaphor.

22:25.920 --> 22:34.080
 But yeah, is it the right metaphor? All three of these four fields adopted computation. And,

22:34.080 --> 22:39.440
 you know, the, a lot of it swirls around Warren McCulloch and his, all his students. And he

22:40.080 --> 22:48.480
 funded a lot of people. And, and our human metaphors, our limitations to human thinking

22:48.480 --> 22:56.640
 will play into this. The three themes of the book. So I have a little to say about computation. So

22:59.360 --> 23:05.040
 you're saying that there is a gap between the computer or the, the, the machine that performs

23:05.040 --> 23:14.240
 computation and this machine that appears to have consciousness and intelligence. Yeah. Can we

23:14.240 --> 23:19.280
 that piece of meat in your head piece of meat, and maybe it's not just the meat in your head.

23:19.280 --> 23:23.760
 It's the rest of you too. I mean, you have, you have, you actually have a neural system in your

23:23.760 --> 23:31.040
 gut. I tend to also believe, not believe, but we're now dancing around things we don't know. But

23:31.680 --> 23:40.480
 I tend to believe other humans are important. Like, so we're almost like, I just don't think

23:40.480 --> 23:45.280
 we would ever have achieved the level of intelligence we have with other humans. I'm not

23:45.280 --> 23:51.200
 saying so confidently, but I have an intuition that some of the intelligence is in the interaction.

23:51.200 --> 23:56.720
 Yeah. And, and I think, you know, I think it seems to me very likely again, we, you know,

23:56.720 --> 24:03.440
 this speculation, but we, our species, and probably, probably in the end, those to some

24:03.440 --> 24:09.280
 extent, because you can find old bones where they seem to be counting on them by putting notches

24:09.280 --> 24:16.720
 that were near them in the anticles had done. We're able to put some of our stuff outside

24:16.720 --> 24:22.000
 our body into the world, and then other people can share it. And then we get these tools that

24:22.000 --> 24:27.040
 become shared tools. And so there's a whole coupling that would not occur in, you know,

24:27.920 --> 24:32.800
 the single deep learning network, which was fed, you know, all of literature or something.

24:32.800 --> 24:40.320
 Yeah. The neural network can't step outside of itself. But is there, is there some,

24:41.840 --> 24:48.960
 can we explore this dark room a little bit and try to get at something? What is the magic? Where

24:48.960 --> 24:55.760
 does the magic come from in the human brain that creates the mind? What's your sense as scientists

24:55.760 --> 25:04.160
 that try to understand it and try to build it? What are the directions if followed might be

25:04.160 --> 25:11.360
 productive? Is it creative interactive robots? Is it creating large deep neural networks that

25:12.160 --> 25:17.600
 do like self supervised learning? And just like wolf will, will discover that when you

25:17.600 --> 25:21.760
 make something large enough, some interesting things will emerge. Is it through physics and

25:21.760 --> 25:27.360
 chemistry and biology, like artificial life angle, like we'll sneak up in this four quadrant

25:28.000 --> 25:32.480
 matrix that you mentioned? Is there anything you're most, if you had to bet all your money,

25:33.760 --> 25:39.760
 financial? I wouldn't. Okay. So every intelligence we know, who's, you know,

25:39.760 --> 25:46.480
 animal intelligence, dog intelligence, you know, octopus intelligence, which is a very different

25:46.480 --> 25:57.920
 sort of architecture from us. All the intelligences we know perceive the world in some way and then

25:57.920 --> 26:08.080
 have action in the world, but they're able to perceive objects in a way which is actually pretty

26:08.080 --> 26:18.880
 damn phenomenal and surprising. You know, we tend to think, you know, that the box over here

26:18.880 --> 26:26.560
 between us, which is a sound box, I think, is a blue box. But blueness is something that we

26:26.560 --> 26:34.320
 construct with color constancy. It's not a, it's not a, it's not, the blueness is not a direct

26:34.320 --> 26:42.240
 function of the photons we're receiving. It's actually context, you know, which is why you can

26:42.240 --> 26:51.760
 turn, you know, maybe seen the examples where someone turns a stop sign into a, some other sort

26:51.760 --> 26:55.360
 of sign by just putting a couple of marks on them and the deep learning system gets it wrong.

26:55.360 --> 26:59.360
 And everyone says, but the stop sign's red. You know, why is it, why is it thinking it's the

26:59.360 --> 27:03.600
 other sort of sign? Because redness is not intrinsic in just the photons. It's actually

27:03.600 --> 27:07.840
 a construction of an understanding of the whole world and the relationship between objects

27:07.840 --> 27:15.520
 to get color constancy. But our tendency in order that we get an archive paper really quickly is

27:15.520 --> 27:19.680
 to just show a lot of data and give the labels and hope it figures it out. But it's not figuring

27:19.680 --> 27:24.720
 it out in the same way we do. We have a very complex perceptual understanding of the world.

27:24.720 --> 27:29.760
 Dogs have a very different perceptual understanding based on smell. They go smell, smell a post,

27:29.760 --> 27:35.680
 they can tell how many, you know, different dogs have visited it in the last 10 hours and how

27:35.680 --> 27:40.080
 long ago there's all sorts of stuff that we just don't perceive about the world. And just taking

27:40.080 --> 27:45.920
 a single snapshot is not perceiving about the world. It's not perceiving the registration between

27:45.920 --> 27:53.360
 us and the object. And registration is a philosophical concept. Brian Cantrell Smith

27:53.360 --> 28:01.200
 talks about a lot, very difficult, squirmy thing to understand. But I think none of our systems

28:01.200 --> 28:06.000
 do that. We've always talked in AI about the symbol grounding problem, how our symbols that we

28:06.000 --> 28:10.560
 talk about are grounded in the world. And when deep learning came along and started labeling

28:10.560 --> 28:16.160
 images, people said, ah, the grounding problem has been solved. No, the labeling problem was solved

28:16.160 --> 28:24.000
 with some percentage accuracy, which is different from the grounding problem. So you agree with Hans

28:24.000 --> 28:31.280
 Marwick and what's called the Marwick's paradox that highlights this counterintuitive notion that

28:32.080 --> 28:42.960
 reasoning is easy, but perception and mobility are hard. Yeah, we shared an office when I was

28:42.960 --> 28:48.000
 working on computer vision and he was working on his first mobile robot. What were those conversations

28:48.000 --> 28:54.480
 like? That were great. Do you still kind of maybe you can elaborate and do you still believe this

28:54.480 --> 29:02.320
 kind of notion that perception is really hard? Can you make sense of why we humans have this

29:02.320 --> 29:10.560
 poor intuition about what's hard or not? Well, let me give us sort of another story.

29:10.560 --> 29:16.320
 Sure. If you go back to, you know, the original, you know, teams working on AI

29:18.400 --> 29:23.680
 from the late 50s into the 60s, you know, and you go to the AI lab at MIT,

29:25.920 --> 29:30.800
 who was it that was doing that? Was it a bunch of really smart kids who got into MIT

29:31.440 --> 29:36.640
 and they were intelligent? So what's intelligence about? Well, the stuff they were good at, playing

29:36.640 --> 29:43.440
 chess, doing integrals, that was that was hard stuff. But, you know, a baby could see stuff.

29:43.440 --> 29:47.440
 That wasn't that wasn't intelligent. I mean, anyone could do that. It's not intelligence.

29:48.080 --> 29:53.760
 And so it, you know, this, there was this intuition that the hard stuff is the things they were good

29:53.760 --> 29:59.840
 at. And the easy stuff was the stuff that everyone could do. Yeah. And maybe I'm overplaying it a

29:59.840 --> 30:04.160
 little bit. And I think there's an element of that. Yeah. I mean, there, I don't know how much

30:04.160 --> 30:10.240
 truth there is to like chess, for example, has was for the longest time seen as the highest

30:11.840 --> 30:17.520
 level of intellect, right? Until we got computers that were better at it than people. And then

30:18.160 --> 30:22.080
 we realized, you know, if you go back to the 90s, you'll see, you know, the stories and the press

30:22.080 --> 30:28.320
 around when, when Kasparov was beaten by Deep Blue. Oh, this is the end of all sorts of things.

30:28.320 --> 30:32.240
 Computers are going to be able to do anything from now on. And we saw exactly the same stories

30:32.240 --> 30:40.640
 with AlphaZero, the go playing program. Yeah. But still, to me, reasoning is a special thing.

30:41.200 --> 30:46.320
 And perhaps, no, we actually, we're really bad at reasoning. We just use these analogies based

30:46.320 --> 30:51.360
 on our hunter gatherer intuitions. But why is that not, don't you think the ability to construct

30:51.360 --> 30:57.040
 metaphor is a really powerful thing? Oh, yeah, it is stories. It is. It's the construction of the

30:57.040 --> 31:02.320
 metaphor and registering that something constant in our brains. Like, isn't that what we're doing

31:02.320 --> 31:07.600
 with with vision to and what we're telling our stories, we're constructing good models of the

31:07.600 --> 31:16.000
 world? Yeah, yeah. But I think we jumped between what we're capable of and how we're doing it

31:16.000 --> 31:21.120
 right there. There was a little confusion that went on. Sure. As we were telling each other

31:21.120 --> 31:27.360
 stories. Yes, exactly. Trying to delude each other. No, I just think I'm not exactly, so I'm

31:27.360 --> 31:34.160
 trying to pull apart this Morvex paradox. I don't view it as a paradox. What did evolution,

31:34.160 --> 31:38.160
 what did evolution spend its time on? Yes, it spent its time on getting us to perceive and

31:38.160 --> 31:43.680
 move in the world. That was, you know, 600 million years as multi subtle creatures doing that. And

31:43.680 --> 31:50.560
 then it was, you know, relatively recent that we that we, you know, were able to hunt or gather

31:50.560 --> 31:56.560
 or, you know, even, even animals hunting. That's much more recent. And then, and then anything

31:56.560 --> 32:03.280
 that we, you know, speech, language, those things are, you know, a couple of hundred thousand years

32:03.280 --> 32:10.800
 probably, if that long, and then agriculture, 10,000 years, you know, all that stuff was built

32:10.800 --> 32:15.840
 on top of those earlier things, which took a long time to develop. So if you then look at the

32:15.840 --> 32:23.760
 engineering of these things, so building it into robots, what's the hardest part of robotics,

32:23.760 --> 32:31.680
 do you think, as the decades that you worked on robots, in the context of what we're talking

32:31.680 --> 32:37.920
 about vision, like a perception, the actual sort of the, the biomechanics of movement,

32:38.880 --> 32:43.520
 I'm kind of drawing parallels here between humans and machines always, like, what do you think is

32:43.520 --> 32:51.280
 the hardest part of robotics? I sort of think all of them. There are no easy parts to do well.

32:52.720 --> 32:58.080
 We sort of go reductionist and we reduce it. If only we had all the, the location of all the

32:58.080 --> 33:05.120
 points in 3D, things would be great. You know, if only we had labels on the, on the images,

33:05.120 --> 33:11.520
 you know, things would be great. But, you know, as, as we see, that's not good enough. Some deeper

33:11.520 --> 33:19.120
 understanding. But if you, if I came to you and I could solve one category of problems in robotics

33:20.480 --> 33:24.080
 instantly, what would give you the greatest pleasure?

33:28.240 --> 33:33.760
 I mean, is it, you know, you look at robots that manipulate objects.

33:33.760 --> 33:41.360
 What's hard about that? You know, is it the perception? Is it the, the reasoning about the

33:41.360 --> 33:47.360
 world, like common sense reasoning? Is it the actual building a robot that's able to interact

33:47.360 --> 33:53.200
 with the world? Is it like human aspects of a robot that's interacting with humans and that,

33:53.200 --> 33:55.600
 that game theory of how they work well together?

33:55.600 --> 34:00.240
 Well, let's talk about manipulation for a second, because I had this really blinding moment.

34:00.240 --> 34:06.000
 You know, I'm a grandfather, so grandfathers have blinding moments. Just three or four miles from

34:06.000 --> 34:12.640
 here. Last year, my 16 month old grandson was in his new house, first time, right? First time in

34:12.640 --> 34:18.240
 this house. And he'd never been able to get to a window before, but this had some low windows.

34:18.240 --> 34:24.240
 And he goes up to this window with a handle on it that he's never seen before. And he's got one

34:24.240 --> 34:31.680
 hand pushing the window and the other hand turning the handle to open the window. He, he, he knew

34:31.680 --> 34:38.960
 two different hands, two different things he knew how to, how to put together. And he's 16 months

34:38.960 --> 34:40.960
 old. And there you are watching an awe.

34:44.960 --> 34:47.920
 In an environment, environment he'd never seen before.

34:47.920 --> 34:58.960
 How did he do that? How did he do that? Yes, that's a good question. How did he do that?

34:58.960 --> 35:05.600
 That's why it's like, okay, like, you could see the leap of genius from using one hand to perform

35:05.600 --> 35:11.600
 a task to combining doing, I mean, first of all, in manipulation, that's really difficult. It's

35:11.600 --> 35:17.360
 like two hands, both necessary to complete the action and completely different. And he'd never

35:17.360 --> 35:24.880
 seen a window open before. But he inferred somehow a handle open something.

35:25.760 --> 35:31.120
 There may have been a lot of slightly different failure cases that you didn't see.

35:32.080 --> 35:36.000
 Not with a window, but with other objects of turning and twisting and handles.

35:37.520 --> 35:45.120
 There's a great counter to reinforcement learning. We'll just give the robot,

35:45.120 --> 35:52.320
 we'll give the robot plenty of time to try everything. Can I tell a little side story here?

35:53.360 --> 36:03.280
 So I'm in DeepMind in London, four years ago, where there's a big Google building,

36:03.280 --> 36:06.880
 and then you go inside and you go through this more security, and then you get to DeepMind,

36:06.880 --> 36:12.160
 where the other Google employees can't go. And I'm in a conference room,

36:12.160 --> 36:17.120
 bare conference room with some of the people. And they tell me about their reinforcement

36:17.120 --> 36:25.440
 learning experiment with robots, which are just trying stuff out. And they're my robots.

36:25.440 --> 36:31.840
 They're Soyuz that we sold them. And they really like them because Soyuz are compliant

36:31.840 --> 36:37.120
 and can sense forces, so they don't break when they're bashing into walls. They stop and they

36:37.120 --> 36:42.560
 do stuff. So you just let the robot do stuff and eventually it figures stuff out.

36:42.560 --> 36:47.360
 By the way, we're talking about robot manipulation, so robot arms and so on.

36:47.360 --> 36:51.200
 Yeah, Soyuz is a robot arm. Just go, what's Soyuz?

36:51.200 --> 36:55.120
 Soyuz is a robot arm that my company, Rethink Robotics, built.

36:55.120 --> 36:56.400
 Thank you for the context.

36:56.400 --> 36:58.400
 Yeah, sorry. Okay, cool. So we're in DeepMind.

36:59.360 --> 37:04.080
 And it's in the next room, these robots are just bashing around to try and use reinforcement

37:04.080 --> 37:08.640
 learning to learn how to act. Can I go see them? Oh, no, they're secret. They're all

37:08.640 --> 37:12.160
 my robots that were secret. That's hilarious. Okay.

37:12.160 --> 37:17.840
 Anyway, the point is, you know, this idea that you just let reinforcement learning figure

37:17.840 --> 37:25.120
 everything out is so counter to how a kid does stuff. So again, story about my grandson, I gave

37:25.120 --> 37:30.960
 him this box that had lots of different lock mechanisms. He didn't randomly, you know,

37:30.960 --> 37:35.920
 and he was 18 months old. He didn't randomly try to touch every surface or push everything.

37:35.920 --> 37:42.000
 He found, he could see where the mechanism was, and he started exploring the mechanism

37:42.000 --> 37:47.200
 for each of these different lock mechanisms. And there was reinforcement, no doubt of some

37:47.200 --> 37:54.080
 sort going on there. But he applied a prefilter, which cut down the search space dramatically.

37:55.520 --> 37:59.120
 I wonder to what level we're able to introspect what's going on,

37:59.120 --> 38:02.960
 because what's also possible is you have something like reinforcement learning

38:03.520 --> 38:08.240
 going on in the mind in the space of imagination. So like you have a good model of the world you're

38:08.240 --> 38:15.520
 predicting, and you may be running those tens of thousands of like loops, but you're like as a human,

38:15.520 --> 38:20.320
 you're just looking at yourself trying to tell a story of what happened. And it might seem simple,

38:20.320 --> 38:26.960
 but maybe there's a lot of computation going on. Whatever it is, but there's also a mechanism

38:26.960 --> 38:34.320
 that's being built up. It's not just random search. That mechanism prunes it dramatically.

38:34.320 --> 38:41.120
 Yeah, that pruning, that pruning step, but it doesn't, it's possible that that's,

38:41.760 --> 38:46.320
 so you don't think that's akin to a neural network inside a reinforcement learning algorithm?

38:47.600 --> 38:48.480
 Is it possible?

38:48.480 --> 39:02.400
 Yeah, until it's possible. I'll be incredibly surprised if that happens. I'll also be incredibly

39:02.400 --> 39:08.000
 surprised that after all the decades that I've been doing this where every few years someone

39:08.000 --> 39:14.960
 thinks, now we've got it. Now we've got it. Four or five years ago, I was saying, I don't think

39:14.960 --> 39:19.360
 we've got it yet. And everyone was saying, you don't understand how powerful AI is. I had people

39:19.360 --> 39:28.240
 tell me, you don't understand how powerful it is. I sort of had a track record of what the world

39:28.240 --> 39:33.360
 had done to think, well, this is no different from before. Well, we have bigger computers. We had

39:33.360 --> 39:41.200
 bigger computers in the 90s and we could do more stuff. But okay, so let me push back. I'm

39:41.200 --> 39:47.760
 generally sort of optimistic and tried to find the beauty in things. I think there's a lot of

39:49.280 --> 39:54.720
 surprising and beautiful things that neural networks, this new generation of deep learning

39:54.720 --> 40:00.320
 revolution has revealed to me, has continually been very surprising, the kind of things it's

40:00.320 --> 40:05.760
 able to do. Now, generalizing that over saying like this, we've solved intelligence, that's another

40:05.760 --> 40:12.080
 big leap. But is there something surprising and beautiful to you about neural networks that

40:12.080 --> 40:16.000
 where actually you said back and said, I did not expect this?

40:18.080 --> 40:25.440
 Oh, I think their performance on ImageNet was shocking. So computer vision in those early

40:25.440 --> 40:31.520
 days was just very like, wow, okay. That doesn't mean that they're solving everything in computer

40:31.520 --> 40:38.080
 vision. We need to solve in vision for robots. What about AlphaZero and self play mechanisms

40:38.080 --> 40:44.720
 and reinforcement learning? Yeah, that was all in Donald Mickey's 1961 paper. Everything there

40:44.720 --> 40:50.320
 was there, which introduced reinforcement learning. No, but come on. So you're talking

40:50.320 --> 40:55.680
 about the actual techniques. But isn't this surprising to you the level it's able to achieve

40:55.680 --> 41:05.360
 with no human supervision of chess play? Like, to me, there's a big, big difference in deep blue.

41:05.360 --> 41:11.840
 And maybe what that's saying is how overblown our view of ourselves is.

41:11.840 --> 41:25.920
 You know, we had that chess is easy. Yeah, I mean, I came across this 1946 report that,

41:26.560 --> 41:30.320
 and I'd seen this as a kid in one of those books that my mother had given me actually,

41:31.440 --> 41:39.120
 1946 report, which pitted someone with an abacus against an electronic calculator.

41:39.120 --> 41:46.560
 And he beat the electronic calculator. So there at that point was, well, humans are still better

41:46.560 --> 41:53.520
 than machines are calculating. Are you surprised today that a machine can do a billion floating

41:53.520 --> 42:03.040
 point operations a second and you're puzzling for minutes through one? So I don't know,

42:03.040 --> 42:09.360
 but I am certainly surprised. There's something to me different about learning.

42:10.400 --> 42:14.880
 So system that's able to learn learning. Now you see, now you're getting into one of the deadly

42:14.880 --> 42:23.280
 sins because of using terms overly broadly. Yeah, I mean, there's so many different forms of learning.

42:23.280 --> 42:27.040
 Yeah. And so many different forms. You know, I learned my way around the city. I learned to

42:27.040 --> 42:32.960
 play chess. I learned Latin. I learned to ride a bicycle. All of those are, you know,

42:32.960 --> 42:41.680
 a very different capabilities. Yeah. And if someone has a, you know, in the old days,

42:41.680 --> 42:47.040
 people would write a paper about learning something. Now the corporate press office

42:47.040 --> 42:55.680
 puts out a press release about how company X has leading the world because they have a system that

42:55.680 --> 43:01.840
 can. Yeah, but here's the thing. Okay. So what is learning? What I'm referring to learning is many

43:01.840 --> 43:10.960
 things, but I suitcase word. It's a suitcase word. But loosely, there's a dumb system. And over time,

43:11.760 --> 43:17.200
 it becomes smart. Well, it becomes less dumb at the thing that it's doing. Yeah. Smart is a

43:18.160 --> 43:23.520
 is a loaded word. Yes, less, less dumb at the thing. It gets better performance under some measure.

43:23.520 --> 43:29.360
 Yeah. And some set of conditions at that thing. And most of these learning algorithms,

43:31.520 --> 43:37.040
 learning systems fail when you change the conditions just a little bit in a way that

43:37.040 --> 43:45.040
 humans don't. So, right, I was at DeepMind. The AlphaGo had just come out. And I said,

43:45.040 --> 43:50.320
 what would have happened if you'd given it a 21 by 21 board instead of a 19 by 19 board? They said,

43:50.320 --> 43:55.520
 fail totally. But a human player would actually, you know, well, would actually be able to play

43:55.520 --> 44:03.040
 a game. And actually, funny enough, if you look at DeepMind's work, since then, they are presenting

44:03.040 --> 44:09.280
 a lot of algorithms that would do well at the bigger board. So they're slowly expanding this

44:09.280 --> 44:16.080
 generalization. I mean, to me, there's a core element there. It is very surprising to me that

44:16.080 --> 44:22.560
 even in a constrained game of chess or Go that through self play by system playing itself,

44:23.120 --> 44:30.480
 that can, it can achieve super human level performance through learning alone. So like.

44:30.480 --> 44:34.080
 Okay. So, so, you know, you didn't still fund them as you did in a search of that.

44:34.080 --> 44:41.280
 You didn't, you didn't like it when I referred to Donald Mickey's 1961 paper. There in the second

44:41.280 --> 44:47.360
 part of it, which came a year later, they had self play on an electronic computer at tic,

44:47.360 --> 44:52.720
 tac, toe. Okay. It's not as, but it learned to play tic, tac, toe through self play. That's

44:52.720 --> 44:59.840
 not what learned to play optimally. What I'm saying is I, okay, I have a little bit of a bias,

44:59.840 --> 45:07.680
 but I find ideas beautiful, but only when they actually realize the promise that's another level

45:07.680 --> 45:14.560
 of beauty. Like, for example, with Bezos and Elon Musk are doing with rockets, rockets for a

45:14.560 --> 45:21.280
 long time, but doing reusable, cheap rockets, it's very impressive. In the same way, I, okay,

45:21.280 --> 45:28.320
 yeah, I would have not predicted. First of all, when I was started and fell in love with AI,

45:28.320 --> 45:35.520
 the game of Go was seen to be impossible to solve. Okay. So I thought maybe, you know,

45:35.520 --> 45:41.760
 I, maybe it'd be possible to maybe have big leaps in a Moore's law style of way in computation,

45:41.760 --> 45:47.360
 I'll be able to solve it. But I would never have guessed that you could learn your way. However,

45:49.520 --> 45:55.520
 I mean, in the narrow sense of learning, learn your way to, to, to beat the best people in

45:55.520 --> 46:00.160
 the world at the game of Go without human supervision, not studying the game of experts.

46:00.160 --> 46:07.920
 Okay. So, so using a different learning technique, Arthur Samuel in the early 60s,

46:08.480 --> 46:13.840
 and he was the first person to use machine learning, got, had a program that could beat

46:13.840 --> 46:20.480
 the world champion at checkers. Now, so, and that time was considered amazing. By the way,

46:20.480 --> 46:25.680
 Arthur Samuel had some fantastic advantages. Do you want to hear Arthur Samuel's advantages?

46:25.680 --> 46:32.400
 Two things. One, he was at the 1956 AI conference. I knew Arthur later in life.

46:32.400 --> 46:36.320
 He was at Stanford when I was grad student there. He wore a tie and a jacket every day.

46:36.320 --> 46:40.480
 The rest of us didn't. He's a delightful man, delightful man.

46:43.040 --> 46:50.720
 It turns out, Claude Shannon, in a 1950 scientific American article, outlined on chess

46:50.720 --> 46:57.120
 playing, outlined the learning mechanism that Arthur Samuel used and they had met in 1956.

46:57.120 --> 47:01.520
 I assume there was some communication, but I don't know that for sure. But Arthur Samuel

47:02.240 --> 47:07.360
 has been a vacuum tube engineer on getting reliability of vacuum tubes and then had

47:07.360 --> 47:14.160
 overseen the first transistorized computers at IBM. And in those days, before you shipped a

47:14.160 --> 47:20.080
 computer, you ran it for a week to seek to get early failures. So here you had this whole farm

47:20.080 --> 47:29.120
 of computers running random code for hours and hours a week for each computer. He had a whole

47:29.120 --> 47:38.880
 bunch of them. So he ran his chess learning program with self play on IBM's production line.

47:38.880 --> 47:43.920
 He had more computation available to him than anyone else in the world. And then he was able

47:43.920 --> 47:48.560
 to produce a chess playing program. I mean, a checkers playing program that could beat the

47:48.560 --> 47:55.600
 world champion. So that's amazing. The question is, I mean, I'm surprised, I don't just mean

47:55.600 --> 48:02.400
 it's nice to have that accomplishment. Is there is a stepping towards something that feels

48:04.160 --> 48:08.720
 more intelligent than before? Yeah, but that's in your view of the world.

48:08.720 --> 48:11.440
 Okay, okay. Well, I mean, then it doesn't mean I'm wrong.

48:11.440 --> 48:19.200
 No, no, no. So the question is, if we keep taking steps like that, how far that takes us?

48:19.200 --> 48:23.840
 Are we going to build a better recommender systems? Are we going to build a better robot?

48:23.840 --> 48:29.200
 Or will we solve intelligence? So, you know, I'm putting my bet on

48:31.520 --> 48:37.680
 but still missing a whole lot a lot. And why would I say that? Well, in these games, they're all,

48:37.680 --> 48:46.000
 you know, 100% information games. But again, but each of these systems is a very short

48:46.000 --> 48:52.320
 description of the current state, which is different from registering and perception

48:52.320 --> 48:55.600
 in the world, which gets back to Maurevec's paradox.

48:55.600 --> 49:02.400
 I'm definitely not saying that chess is somehow harder than perception,

49:02.400 --> 49:10.400
 or any kind of robotics in the physical world. I definitely think it's way harder than the game

49:10.400 --> 49:15.920
 of chess. So I was always much more impressed by the workings of the human mind. It's incredible.

49:15.920 --> 49:19.840
 The human mind is incredible. I believe that from the very beginning. I wanted to be a psychiatrist

49:19.840 --> 49:23.360
 for the longest time. I always thought that's way more incredible in the game of chess. I think

49:23.360 --> 49:29.680
 the game of chess is, I love the Olympics. It's just another example of us humans picking a task

49:29.680 --> 49:34.320
 and then agreeing that a million humans will dedicate their whole life to that task. And that's

49:34.320 --> 49:40.320
 the cool thing that the human mind is able to focus on one task and then compete against each

49:40.320 --> 49:46.000
 other and achieve like weirdly incredible levels of performance. That's the aspect of chess that's

49:46.000 --> 49:51.600
 super cool. Not that chess in itself is really difficult. It's like the Fermat's last theorem

49:51.600 --> 49:56.400
 is not in itself to me that interesting. The fact that thousands of people have been struggling

49:56.400 --> 50:00.720
 to solve that particular problem is fascinating. So can I tell you my disease in this way? Sure.

50:01.440 --> 50:06.560
 Which actually is closer to what you're saying. So as a child, you know, I was building various,

50:06.560 --> 50:10.720
 I called them computers. They weren't general purpose computers. Ice cube tray. The ice cube

50:10.720 --> 50:15.520
 tray was one. But I built other machines. And what I liked to build was machines that could beat

50:15.520 --> 50:20.560
 adults at a game. And they couldn't, the adults couldn't beat my machine. Yeah. So that was,

50:20.560 --> 50:26.480
 you were like, that's powerful. Like that's a way to rebel. Yeah. By the way,

50:29.280 --> 50:33.840
 when was the first time you built something that outperformed you? Do you remember?

50:34.640 --> 50:38.720
 Well, I knew how it worked. I was probably nine years old. And I built a thing that

50:39.840 --> 50:46.240
 was a game where you take turns in taking matches from a pile. And either the one who

50:46.240 --> 50:50.320
 takes the last one or the one who doesn't take the last one wins, I forget. And so it was pretty

50:50.320 --> 50:56.240
 easy to build that out of wires and nails and little coils that were like plugging in the number

50:56.240 --> 51:04.320
 and a few light bulbs. The one that I was proud of, I was 12 when I built a thing out of old

51:05.040 --> 51:12.000
 telephone switchboard switches that could always win at tic tac toe. And that was a

51:12.000 --> 51:18.320
 much harder circuit to design. But again, it was just, it was no active components. It was just

51:18.320 --> 51:27.760
 three positions, which is empty x zero, and nine of them and light bulb one, which,

51:27.760 --> 51:31.600
 which move it wanted next. And then the human would go and move that.

51:31.600 --> 51:38.160
 See, there's magic in that creation. I tend to, I tend to see magic in robots that

51:39.920 --> 51:45.760
 like I also think that intelligence is a little bit overrated. I think we can have deep

51:45.760 --> 51:53.440
 connections with robots very soon. And we'll come back to connections. Sure. But, but I do want to

51:53.440 --> 52:00.560
 say I don't, I think too many people make the mistake of seeing that magic and thinking,

52:00.560 --> 52:05.920
 well, we'll just continue, you know, but each, each one of those is a hard fought battle for

52:05.920 --> 52:10.640
 the next step, the next step. Yes. The open question here is, and this is why I'm playing

52:10.640 --> 52:15.680
 devil's advocate, but I often do when I read your blog post in my mind, because I have

52:15.680 --> 52:22.080
 like this eternal optimism is it's not clear to me. So I don't do what obviously the journalists do

52:22.080 --> 52:27.280
 or like give into the hype, but it's not obvious to me how many steps away we are from,

52:29.600 --> 52:38.480
 from a truly transformational understanding of what it means to build intelligence systems,

52:38.480 --> 52:43.280
 like, or how to build intelligence systems. I'm also aware of the whole history of artificial

52:43.280 --> 52:48.960
 intelligence, which is where you're deep grounding of this is, is there has been an optimism for

52:48.960 --> 52:55.520
 decades. And that optimism, just like reading old optimism is absurd, because people were like,

52:56.160 --> 53:00.720
 this is, they were saying things are trivial for decades since the sixties. They're saying

53:00.720 --> 53:07.760
 everything is true computer vision is trivial. But I think my mind is working crisply enough to

53:07.760 --> 53:13.440
 where I mean, we can dig into if you want. I'm really surprised by the things deep mind has

53:13.440 --> 53:20.160
 done. I don't think they're so they're yet close to solving intelligence, but I'm not sure it's

53:20.160 --> 53:27.040
 not 10, 10 years away. What I'm referring to is interesting to see when the engineering

53:29.440 --> 53:34.720
 it takes that idea to scale and this and the idea works and no, it fools people.

53:34.720 --> 53:40.560
 Oh, okay. Honestly, Ronnie, if it was you, me and Demis inside a room, forget the press,

53:40.560 --> 53:45.840
 forget all those things. Just as a scientist as a roboticist, you know, that wasn't surprising to

53:45.840 --> 53:51.920
 you that at scale. So we're talking about a very large now. Okay, let's pick one that's the most

53:51.920 --> 53:59.600
 surprising to you. Okay, please don't yell at me. GPT three. Okay. Well, I was gonna bring that out.

53:59.600 --> 54:08.880
 Okay, Alpha zero, Alpha go Alpha go zero, Alpha zero, and then Alpha fold one and two. So aren't

54:08.880 --> 54:14.960
 any of do any of these kind of have this core of not forget usefulness or application and so on,

54:14.960 --> 54:21.280
 which you could argue for Alpha fold, like as a scientist was doors surprising to you that it

54:21.280 --> 54:28.720
 worked as well as it did. Okay, so if we're going to make the distinction between surprise and

54:28.720 --> 54:39.360
 usefulness, and I'll have to explain this, I would say Alpha fold. And one of the problems

54:39.360 --> 54:44.480
 at the moment with Alpha fold is, you know, it gets a lot of them right, which is a surprise to me

54:44.480 --> 54:51.520
 because they're a really complex thing. But you don't know which ones it gets right, which then

54:51.520 --> 54:54.880
 is a bit of a problem. Now they've come out with a reason. You mean the structure of the protein

54:54.880 --> 54:59.840
 gets a lot of those right? Yeah, it's a surprising number of them right. It's been a really hard

54:59.840 --> 55:05.600
 problem. So that was a surprise how many it gets right. So far, the usefulness is limited because

55:05.600 --> 55:10.960
 you don't know which ones are right or not. And now they've come out with a thing in the last few

55:10.960 --> 55:16.080
 weeks, which is trying to get a useful tool out of it. And they may well do it. In that sense,

55:16.080 --> 55:23.120
 the least Alpha fold is different, because your Alpha fold two is different. Because now it's

55:23.120 --> 55:28.000
 producing data sets that are actually, you know, potentially revolutionizing competition

55:28.000 --> 55:33.520
 biology, like they will actually help a lot of people. But you would say potentially

55:33.520 --> 55:38.400
 revolutionizing. We don't know yet. But yeah, that's true. Yeah, but they're, you know, but I

55:38.400 --> 55:43.360
 got your, I mean, this is okay. So you know what, this is going to be so fun. So let's go

55:44.640 --> 55:50.880
 right into it. Speaking of robots that operate in the real world. Let's talk about

55:50.880 --> 55:59.280
 self driving cars. Oh, okay. Because you do you have built robotics companies, you're one of the

55:59.280 --> 56:04.160
 greatest roboticists in history. And that's not in space of just in the space of ideas.

56:05.040 --> 56:11.280
 We'll also probably talk about that. But in the actual building and execution of businesses that

56:11.280 --> 56:16.000
 make robots that are useful for people and that actually work in the real world and make money.

56:16.000 --> 56:24.640
 You also sometimes are critical of Mr. Elon Musk, or let's more specifically focus on this

56:24.640 --> 56:31.440
 particular technology, which is autopilot inside Tesla's. What are your thoughts about Tesla autopilot

56:31.440 --> 56:36.720
 or more generally vision based machine learning approach to semi autonomous driving?

56:38.640 --> 56:43.120
 These are robots that are being used in the real world by hundreds of thousands of people.

56:43.120 --> 56:50.480
 And if you want to go there, I can go there, but that's not too much, which there is, let's say

56:50.480 --> 56:58.480
 they're on par safety wise as humans currently, meaning human alone versus human plus robot.

56:58.480 --> 57:04.320
 Okay. So first, let me say I really like the car I came here in here today, which is

57:04.320 --> 57:19.600
 a 2021 model Mercedes E 450. I am impressed by the machine vision. So now other things,

57:19.600 --> 57:30.000
 I'm impressed by what it can do. I'm really impressed with many aspects of it. And it's able

57:30.000 --> 57:38.880
 to stay in lane. It does the lane stuff. It's looking on either side of me. It's telling me

57:38.880 --> 57:45.760
 about nearby cars, blind spots and so on. Yeah. When I'm going in close to something in the park,

57:45.760 --> 57:50.880
 I get this beautiful, gorgeous, top down view of the world. I am impressed

57:51.680 --> 57:58.320
 up the wazoo of how registered and metrical that is. So it's like multiple cameras and

57:58.320 --> 58:02.960
 it's all very good together to produce a 360 view kind of 360 view, you know, synthesized

58:02.960 --> 58:08.800
 so it's above the car. And it is unbelievable. I got this car in January. It's the longest

58:08.800 --> 58:16.000
 I've ever owned a car without digging it. So it's better than me. Me and it together better.

58:16.000 --> 58:25.280
 So I'm not saying technologies are bad or not useful. But here's my point. Yes.

58:25.280 --> 58:34.880
 It's a replay of the same movie. Okay. So maybe you've seen me ask this question before.

58:35.760 --> 58:51.680
 But when did the first car go over 55 miles an hour for over 10 miles

58:51.680 --> 58:57.200
 on a public freeway with other traffic around driving completely autonomously? When did that

58:57.200 --> 59:04.160
 happen? Was it in the 80s or something? It was a long time ago. It was actually in 1987

59:04.160 --> 59:13.120
 in Munich at the Bundeswehr. So they had it running in 1987. When do you think,

59:13.680 --> 59:17.520
 and Elon has said he's going to do this, when do you think we'll have the first car

59:17.520 --> 59:24.480
 drive coast to coast in the US hands off the wheel, hands off the wheel, feet off the pedals,

59:24.480 --> 59:30.560
 coast to coast? As far as I know, a few people have claimed to do it. 1995. That was comedy.

59:30.560 --> 59:35.520
 I didn't know. But oh, that was the code. Yeah. They didn't claim, did they claim 100%

59:35.520 --> 59:40.880
 they claim? Not 100%. Not 100%. And then there's a few marketing people who have claimed 100%.

59:40.880 --> 59:50.800
 But my point is that what I see happening again is someone sees a demo and they over generalize

59:50.800 --> 59:56.160
 and say we must be almost there. But we've been working on it for 35 years. So that's demos.

59:56.160 --> 1:00:00.880
 But this is going to take us back to the same conversation with AlphaZero. Are you not?

1:00:01.920 --> 1:00:07.600
 Okay. I'll just say what I am. Because when I first started interacting with the

1:00:07.600 --> 1:00:13.520
 with the Mobileye implementation at Tesla Autopilot, I've driven a lot of cars. You know,

1:00:13.520 --> 1:00:21.200
 I've been in Google stuff driving cars since the beginning. I thought there was no way before I sat

1:00:21.200 --> 1:00:25.840
 and use Mobileye, I thought they're just knowing computer vision. I thought there's no way it could

1:00:25.840 --> 1:00:34.640
 work as well as it was working. So my model of the limits of computer vision was way more limited

1:00:34.640 --> 1:00:40.000
 than the actual implementation of Mobileye. So that's one example. I was really surprised.

1:00:40.000 --> 1:00:46.800
 I was like, wow, that was incredible. The second surprise came when Tesla threw away Mobileye

1:00:48.320 --> 1:00:53.920
 and started from scratch. I thought there's no way they can catch up to Mobileye. I thought what

1:00:53.920 --> 1:00:57.440
 Mobileye was doing was kind of incredible, like the amount of work and the annotation.

1:00:57.440 --> 1:01:02.720
 Yeah. Well, Mobileye was started by Amnon Shresher and used a lot of traditional, you know,

1:01:02.720 --> 1:01:07.440
 hard fought computer vision techniques. But they also did a lot of good, sort of,

1:01:08.320 --> 1:01:14.480
 like non research stuff, like actual, like just good, like what you do to make a successful

1:01:14.480 --> 1:01:18.320
 product, right? At scale, all that kind of stuff. And so I was very surprised when they

1:01:18.320 --> 1:01:22.960
 from scratch were able to catch up to that. That's very impressive. And I've talked to a

1:01:22.960 --> 1:01:29.760
 lot of engineers that was involved. That was impressive. And the recent progress, especially

1:01:29.760 --> 1:01:37.760
 under, well, with the involvement under Capati, what they were, what they're doing with the data

1:01:37.760 --> 1:01:42.800
 engine, which is converting into the driving tasks into these multiple tasks, and then doing this

1:01:42.800 --> 1:01:49.200
 edge case discovery when they're pulling back, like the level of engineering made me rethink

1:01:49.200 --> 1:01:55.040
 what's possible. I don't, I still, you know, I don't know to that intensity, but I always thought

1:01:55.040 --> 1:02:00.160
 it was very difficult to solve the time I was driving with all the sensors, with all the computation.

1:02:00.160 --> 1:02:07.040
 I just thought it was a very difficult problem. But I've been continuously surprised how much

1:02:07.040 --> 1:02:11.600
 you can engineer. First of all, the data acquisition problem, because I thought, you know,

1:02:11.600 --> 1:02:18.080
 just because I worked with a lot of car companies, they're, they're so a little,

1:02:18.800 --> 1:02:24.240
 a little bit old school to where I didn't think they could do this at scale, like AWS style,

1:02:24.240 --> 1:02:31.440
 data collection. So when Tesla was able to do that, I started to think, okay, so what are the limits

1:02:31.440 --> 1:02:40.000
 of this? I still believe that a driver like sensing and the interaction with a driver and like

1:02:40.000 --> 1:02:44.960
 studying the human factors psychology problem is essential. It's, it's always going to be there.

1:02:46.240 --> 1:02:51.920
 It's always going to be there, even with fully autonomous driving. But I've been surprised,

1:02:51.920 --> 1:02:59.280
 what is the limit, especially a vision based alone, how far that can take us? So that's my

1:02:59.280 --> 1:03:07.920
 levels of surprise. Now, okay, can you explain in the same way you said, like Alpha zero,

1:03:07.920 --> 1:03:13.280
 that's a homework problem that's scaled large in his chest, like who cares, go with, here's

1:03:13.280 --> 1:03:19.200
 actual people using an actual car and driving, many of them drive more than half their miles

1:03:19.200 --> 1:03:25.600
 using the system. Right. So, yeah, they're doing well with, with pure vision.

1:03:25.600 --> 1:03:32.080
 Pure vision, yeah. And, you know, they, and now no radar, which is, I suspect that can't go all the

1:03:32.080 --> 1:03:38.080
 way. And one reason is without, without new cameras that have a dynamic range closer to the human eye,

1:03:38.080 --> 1:03:43.680
 because human eye has incredible dynamic range. And we make use of that dynamic range in, in,

1:03:43.680 --> 1:03:49.840
 it's a, we have an autism magnitude or some crazy number like that. The cameras don't have that,

1:03:49.840 --> 1:03:56.560
 which is why you see the, the, the bad cases where the sun on a white thing and it blinds it

1:03:56.560 --> 1:04:04.720
 in a way, it wouldn't blind a person. I think there's a bunch of things to think about before

1:04:04.720 --> 1:04:14.160
 you say, this is so good, it's just going to work. Okay. And I'll come at it from multiple angles.

1:04:14.160 --> 1:04:18.640
 And I know you've got a lot of time. Yeah. Okay, let's, let's do this. I have thought about these

1:04:18.640 --> 1:04:24.720
 things. Yeah. I know. You've been writing a lot of great blog posts about it for a while before

1:04:24.720 --> 1:04:30.400
 Tesla had autopilot, right? So you've been thinking about autonomous driving for a while from every

1:04:30.400 --> 1:04:38.160
 angle. So, so a few things, you know, in the US, I think that the death rate from motor vehicle

1:04:38.160 --> 1:04:48.320
 accidents is about 35,000 a year, which is an outrageous number, not outrageous compared to

1:04:48.320 --> 1:04:54.320
 COVID deaths, but you know, there is no rationality. And that's part of the thing people have said,

1:04:54.320 --> 1:04:59.440
 engineers say to me, well, if we cut down the number of deaths by 10% by having autonomous

1:04:59.440 --> 1:05:06.560
 driving, that's going to be great. Everyone will love it. And my prediction is that if

1:05:06.560 --> 1:05:11.520
 autonomous vehicles kill more than 10 people a year, they'll be screaming and hollering,

1:05:11.520 --> 1:05:17.760
 even though 35,000 people a year have been killed by human drivers. It's not rational.

1:05:18.320 --> 1:05:22.160
 It's a different set of expectations. And that will probably continue.

1:05:22.160 --> 1:05:28.400
 So there's that aspect of it. The other aspect of it is that

1:05:31.040 --> 1:05:39.360
 when we introduce new technology, we often change the rules of the game. So when we introduced cars,

1:05:40.960 --> 1:05:47.040
 first, you know, into our daily lives, we completely rebuilt our cities and we changed

1:05:47.040 --> 1:05:53.600
 all the laws. J walking was not an offense. That was pushed by the car companies so that

1:05:53.600 --> 1:05:57.760
 people would stay off the road so there wouldn't be deaths from pedestrians getting hit.

1:05:58.320 --> 1:06:03.440
 We completely changed the structure of our cities and had these foul smelling things,

1:06:03.440 --> 1:06:09.680
 you know, everywhere around us. And now you see pushback in cities like Barcelona is really

1:06:09.680 --> 1:06:19.760
 trying to exclude cars, etc. So I think that to get to self driving, we will

1:06:22.560 --> 1:06:29.360
 large adoption. It's not going to be just take the current situation, take out the driver and

1:06:29.360 --> 1:06:36.640
 put the same car doing the same stuff because the end case is too many. Here's an interesting question.

1:06:36.640 --> 1:06:44.240
 How many fully autonomous train systems do we have in the US?

1:06:46.480 --> 1:06:50.960
 I mean, do you count them as fully autonomous? I don't know because they're usually as a driver,

1:06:50.960 --> 1:06:54.560
 but they're kind of autonomous, right? No, let's get rid of the driver.

1:06:56.080 --> 1:07:00.720
 Okay, I don't know. It's either 15 or 16. Most of them are in airports.

1:07:00.720 --> 1:07:07.040
 Okay. There's a few that go about five, two that go about five kilometers out of airports.

1:07:10.800 --> 1:07:17.520
 When is the first fully autonomous train system for mass transit expected to operate fully

1:07:17.520 --> 1:07:28.000
 autonomously with no driver in the US city? It's expected to operate in 2017 in Honolulu.

1:07:28.000 --> 1:07:32.720
 Oh, wow. It's delayed, but they will get there. But by the way,

1:07:32.720 --> 1:07:35.840
 it was originally going to be autonomous here in the Bay Area.

1:07:35.840 --> 1:07:38.800
 I mean, they're all very close to fully autonomous, right?

1:07:38.800 --> 1:07:45.120
 Yeah, but getting the closest to things. And I have often gone on a fully autonomous train

1:07:45.120 --> 1:07:51.520
 in Japan, one that goes out to that fake island in the middle of Tokyo Bay. I forget the name of

1:07:51.520 --> 1:07:58.400
 that. And what do you see when you look at that? What do you see when you go to a fully autonomous

1:07:58.400 --> 1:08:08.960
 train in an airport? It's not like regular trains. At every station, there's a double

1:08:08.960 --> 1:08:16.320
 set of doors. So there's a door of the train and there's a door off the platform.

1:08:16.320 --> 1:08:23.600
 Yeah. And it's really visible in this Japanese one because it goes out in amongst buildings.

1:08:24.720 --> 1:08:28.240
 The whole track is built so that people can't climb onto it. Yeah.

1:08:28.880 --> 1:08:33.520
 So there's an engineering that then makes the system safe and makes them acceptable.

1:08:33.520 --> 1:08:41.120
 I think we'll see similar sorts of things happen in the US. What surprised me, I thought,

1:08:41.120 --> 1:08:51.280
 wrongly, that we would have special purpose lanes on 101 in the Bay Area, the leftmost lane,

1:08:52.720 --> 1:08:59.680
 so that it would be normal for Teslas or other cars to move into that lane and then say,

1:08:59.680 --> 1:09:06.320
 okay, now it's autonomous and have that dedicated lane. I was expecting movement to that. Five years

1:09:06.320 --> 1:09:11.440
 ago, I was expecting we'd have a lot more movement towards that. We haven't. And it may be because

1:09:11.440 --> 1:09:17.760
 Teslas has been overpromising by calling their system fully self driving. I think they may have

1:09:17.760 --> 1:09:26.480
 been gotten there quicker by collaborating to change the infrastructure. This is one of the

1:09:26.480 --> 1:09:35.520
 problems with long hold trucking being autonomous. I think it makes sense on freeways at night

1:09:35.520 --> 1:09:42.480
 for the trucks to go autonomously. But then how do you get on to and off of the freeway?

1:09:42.480 --> 1:09:48.000
 What sort of infrastructure do you need for that? Do you need to have the human in there to do that?

1:09:49.040 --> 1:09:53.760
 Can you get rid of the human? So I think there's ways to get there, but it's an infrastructure

1:09:54.480 --> 1:10:03.040
 argument because the long tail of cases is very long and the acceptance of it will not be at the

1:10:03.040 --> 1:10:11.440
 same level as human drivers. So I'm with you still and I was with you for a long time, but I am

1:10:11.440 --> 1:10:18.400
 surprised how well, how many edge cases of machine learning and vision based methods can cover.

1:10:19.120 --> 1:10:25.920
 This is what I'm trying to get at. I think there's something fundamentally different

1:10:25.920 --> 1:10:30.960
 with vision based methods and Tesla autopilot and any company that's trying to do the same.

1:10:30.960 --> 1:10:43.680
 I'm not going to argue with it because we're speculating. My gut feeling tells me it's going to

1:10:43.680 --> 1:10:51.440
 be things will speed up when there is engineering of the environment because that's what happened

1:10:51.440 --> 1:10:57.600
 with every other technology. I don't know about you, but I'm a bit cynical that infrastructure

1:10:57.600 --> 1:11:07.040
 which relies on government to help out in these cases. If you just look at infrastructure in all

1:11:07.040 --> 1:11:13.600
 domains, government always drags behind on infrastructure. There's so many just...

1:11:13.600 --> 1:11:20.000
 Well, in this country, in this country, and of course, there's many, many countries that are

1:11:20.000 --> 1:11:23.600
 actually much worse on infrastructure. Oh, yes, many of them are much worse in the

1:11:23.600 --> 1:11:28.800
 somewhat high speed rail that other countries have done much better.

1:11:28.800 --> 1:11:35.760
 I guess my question is at the core of what I was trying to think through here and ask is,

1:11:35.760 --> 1:11:42.720
 how hard is the driving problem as it currently stands? You mentioned we don't want to just take

1:11:42.720 --> 1:11:46.800
 the human out and duplicate whatever the human was doing, but if we were to try to do that,

1:11:46.800 --> 1:11:56.560
 how hard is that problem? Because I used to think it's way harder. I used to think it's,

1:11:57.600 --> 1:12:02.240
 with vision alone, it would be three decades, four decades.

1:12:02.240 --> 1:12:10.960
 Okay, so I don't know the answer to this thing I'm about to pose, but I do notice that on Highway 280

1:12:10.960 --> 1:12:16.800
 here in the Bay Area, which largely has concrete surface rather than blacktop surface,

1:12:17.920 --> 1:12:21.680
 the white lines that are painted there now have black boundaries around them.

1:12:23.520 --> 1:12:30.240
 And my lane drift system in my car would not work without those black boundaries.

1:12:30.240 --> 1:12:34.560
 Interesting. So I don't know whether they started doing it to help the lane drift,

1:12:34.560 --> 1:12:42.720
 whether it is an instance of infrastructure following the technology, but my car would not

1:12:42.720 --> 1:12:45.440
 perform as well without that change in the way they paint the lane.

1:12:45.440 --> 1:12:52.880
 Unfortunately, really good lane keeping is not as valuable. It's orders of magnitude more

1:12:52.880 --> 1:12:59.760
 valuable to have a fully autonomous system. But for me, lane keeping is really helpful because

1:12:59.760 --> 1:13:07.680
 I'm always at it. But you wouldn't pay 10 times. The problem is there's not financial,

1:13:07.680 --> 1:13:14.240
 like it doesn't make sense to revamp the infrastructure to make lane keeping easier.

1:13:14.800 --> 1:13:19.760
 It does make sense to revamp the infrastructure. If you have a large fleet of autonomous vehicles,

1:13:19.760 --> 1:13:23.840
 now you change what it means to own cars, you change the nature of transportation,

1:13:23.840 --> 1:13:32.000
 but for that you need autonomous vehicles. Let me ask you about Waymo then. I've gotten

1:13:32.000 --> 1:13:39.600
 a bunch of chances to ride in a Waymo self driving car. I don't know if you'd call them

1:13:39.600 --> 1:13:45.760
 self driving. Well, I mean, I rode in one before they were called Waymo at X.

1:13:45.760 --> 1:13:51.200
 So there's currently another surprisingly, but I didn't think it would happen,

1:13:51.200 --> 1:13:55.040
 which is they have no driver currently. Yeah, in Chandler.

1:13:55.040 --> 1:13:58.960
 In Chandler, Arizona. And I think they're thinking of doing that in Austin as well,

1:13:58.960 --> 1:14:06.080
 but they're expanding. Although, you know, I do an annual checkup on this.

1:14:06.080 --> 1:14:13.280
 So as of late last year, they were aiming for hundreds of rides a week, not thousands.

1:14:13.280 --> 1:14:22.160
 And there is no one in the car, but there's certainly safety people in the loop.

1:14:22.160 --> 1:14:26.800
 And it's not clear how many, you know, what the ratio of cars to safety people is.

1:14:27.680 --> 1:14:31.600
 It wasn't, obviously, they're not 100% transparent about this.

1:14:31.600 --> 1:14:34.400
 No, none of them are 100% transparent. They're very untransparent.

1:14:34.400 --> 1:14:39.520
 But at least the way they're, I don't want to make definitively, but they're saying

1:14:39.520 --> 1:14:50.800
 there's no teleoperation. And that sort of fits with YouTube videos I've seen of people being

1:14:50.800 --> 1:14:59.360
 trapped in the car by a red cone on the street. And they do have rescue vehicles that come,

1:14:59.360 --> 1:15:05.040
 and then a person gets in and drives it. But isn't it incredible to you,

1:15:05.040 --> 1:15:11.440
 it was to me to get in a car with no driver and watch the steering wheel turn,

1:15:12.080 --> 1:15:17.200
 like for somebody who has been studying, at least certainly the human side of autonomous vehicles

1:15:17.200 --> 1:15:21.760
 for many years, and you've been doing it for way longer. It was incredible to me that this

1:15:21.760 --> 1:15:25.840
 was actually could happen. I don't care if that scales 100 cars. This is not a demo.

1:15:25.840 --> 1:15:31.680
 This is not, this is me as a regular. The argument I have is that people make

1:15:31.680 --> 1:15:35.840
 interpolations from that. Interpolations. That, you know, it's here, it's done.

1:15:37.040 --> 1:15:42.480
 You know, it's just, you know, we've solved it. No, we haven't yet. And that's my argument.

1:15:42.480 --> 1:15:48.400
 Okay, so I'd like to go to, you keep a list of predictions on your amazing blog posts.

1:15:48.400 --> 1:15:52.720
 It'd be fun to go through them. But before that, let me ask you about this. You have,

1:15:52.720 --> 1:16:03.680
 you have a harshness to you sometimes in your criticism of what is perceived as hype.

1:16:06.560 --> 1:16:11.520
 And so like, because people extrapolate, like you said, and they kind of buy into the hype,

1:16:11.520 --> 1:16:19.440
 and then they, they kind of start to think that the technology is way better than it is.

1:16:19.440 --> 1:16:27.600
 But let me ask you maybe a difficult question. Do you think if you look at history of progress,

1:16:28.880 --> 1:16:33.840
 don't you think to achieve the quote impossible, you have to believe that it's possible?

1:16:33.840 --> 1:16:41.200
 Absolutely. Yeah. Look, his, his, his, his, his two great runs. Great. Unbelievable.

1:16:41.200 --> 1:16:49.280
 1903, first human power, human, you know, heavier than air flight.

1:16:49.280 --> 1:16:56.640
 Yeah. 1969, we land on the moon. That's 66 years. I'm 66 years old in my lifetime,

1:16:56.640 --> 1:17:02.880
 that span of my lifetime. Barely get, you know, flying, I don't know what it was, 50 feet or

1:17:02.880 --> 1:17:08.960
 the length of the first flight or something to landing on the moon. Unbelievable. Fantastic.

1:17:08.960 --> 1:17:13.520
 But that requires, by the way, one of the Wright brothers, both of them, but one of them didn't

1:17:13.520 --> 1:17:20.320
 believe it's even possible like a year before, right? So like, not just possible soon, but like

1:17:20.320 --> 1:17:25.520
 ever. So, so, so, you know, how important is it to believe and be optimistic is what I guess?

1:17:25.520 --> 1:17:31.280
 Oh, yeah, it is important. It's when it goes crazy. When, when, when I, you know, you said,

1:17:31.280 --> 1:17:43.200
 what was the word you used for my bad harshness? Harshness. Yes. I just get so frustrated. Yes.

1:17:43.200 --> 1:17:50.400
 When, when people make these leaps and tell me that I'm, that I don't understand. Right. I, you

1:17:50.400 --> 1:17:58.560
 know, yeah. There's just from iRobot, which I was co founder of. Yeah. I don't know the exact

1:17:58.560 --> 1:18:02.640
 numbers now because I haven't, it's 10 years since I stepped off the board. But I believe it's well

1:18:02.640 --> 1:18:08.160
 over 30 million robots cleaning houses from that one company. And now there's lots of other companies.

1:18:10.320 --> 1:18:19.120
 Was that a crazy idea that we had to believe in 2002 when we released it? Yeah. That was, we,

1:18:19.120 --> 1:18:24.800
 we had, we had to, you know, believe that it could be done. Let me ask you about this. So iRobot,

1:18:24.800 --> 1:18:30.640
 one of the greatest robotics companies ever in terms of many, creating a robot that actually

1:18:30.640 --> 1:18:35.600
 works in the real world is probably the greatest robotics company ever. You were the co founder of

1:18:35.600 --> 1:18:45.920
 it. If, if the Rodney Brooks of today talked to the Rodney of back then, what would you tell him?

1:18:45.920 --> 1:18:51.920
 Because I have a sense that would you pet him on the back and say, well, you're doing is going to

1:18:51.920 --> 1:19:00.080
 fail, but go at it anyway. That's what I'm referring to with the harshness. You've accomplished an

1:19:00.080 --> 1:19:04.880
 incredible thing there. One of the several things we'll talk about. Well, like that's what I'm trying

1:19:04.880 --> 1:19:12.240
 to get at that line. No, it's, it's when my harshness is reserved for people who are not doing it,

1:19:13.040 --> 1:19:18.000
 who claim it's just, well, this shows that it's just going to happen. But here, here's the thing.

1:19:18.000 --> 1:19:26.080
 This show, but you have that harshness for Elon too. And no, no, it's a different harshness.

1:19:26.080 --> 1:19:34.800
 No, it's a different argument with Elon. You know, I, I think SpaceX is an amazing company.

1:19:34.800 --> 1:19:41.280
 On the other hand, you know, I, in one of my blog posts, I said, what's easy and what's hard? I said,

1:19:41.280 --> 1:19:48.720
 SpaceX vertical landing rockets, it had been done before grid fins had been done since the 60s.

1:19:48.720 --> 1:19:58.160
 Every Soyuz has them reusable space DCX reuse those rockets that landed vertically.

1:19:59.920 --> 1:20:06.000
 There's a whole insurance industry in place for rocket launches. So all sorts of infrastructure

1:20:06.000 --> 1:20:14.880
 that was doable. It took a great entrepreneur, a great personal expense. He almost drove himself,

1:20:14.880 --> 1:20:25.360
 you know, bankrupt doing it. A great belief to do it. Whereas Hyperloop has a whole bunch

1:20:25.360 --> 1:20:30.160
 more stuff that's never been thought about and never been demonstrated. So my estimation is

1:20:30.160 --> 1:20:37.200
 Hyperloop is a long, long, long further off. And if I've got a criticism of, of, of Elon,

1:20:37.200 --> 1:20:44.720
 it's that he doesn't make distinctions between when the technology's coming along and ready.

1:20:44.720 --> 1:20:50.080
 And then he'll go off and, and mouth off about other things, which then people go and compete

1:20:50.080 --> 1:20:58.480
 about and try and do. And so this is where I, I understand what you're saying. I tend to draw

1:20:58.480 --> 1:21:04.880
 a different distinction. I, I have a similar kind of harshness towards people who are not telling the

1:21:04.880 --> 1:21:11.440
 truth, who are basically fabricating stuff to make money or to... Oh, he believes what he says.

1:21:11.440 --> 1:21:15.200
 I just think he's wrong. To me, that's a very important difference. Yeah, I'm not, I'm not.

1:21:15.200 --> 1:21:19.760
 Because I think in order to fly, in order to get to the moon, you have to believe,

1:21:20.720 --> 1:21:26.320
 even when most people tell you you're wrong and most likely you're wrong, but sometimes you're

1:21:26.320 --> 1:21:31.360
 right. I mean, that's the same thing I have with Tesla autopilot. I think that's an interesting

1:21:31.360 --> 1:21:37.440
 one. I was, especially when I was, you know, at MIT and just the entire human factors in the

1:21:37.440 --> 1:21:42.080
 robotics community were very negative towards Elon. It was very interesting for me to observe

1:21:42.080 --> 1:21:48.880
 colleagues at MIT. I wasn't sure what to make of that. That was very upsetting to me because I

1:21:48.880 --> 1:21:54.160
 understood where that, where that's coming from. And I agreed with them and I kind of almost felt

1:21:54.160 --> 1:22:00.080
 the same thing in the beginning until I kind of opened my eyes and realized there's a lot of

1:22:00.080 --> 1:22:06.080
 interesting ideas here that might be overhyped. You know, if you focus yourself on the idea that

1:22:07.840 --> 1:22:12.320
 you shouldn't call a system full self driving when it's obviously not

1:22:12.880 --> 1:22:18.560
 autonomous, fully autonomous, you're going to miss the magic. Oh, yeah, you are going to miss the

1:22:18.560 --> 1:22:25.680
 magic. But at the same time, there are people who buy it, literally pay money for it and take those

1:22:25.680 --> 1:22:33.920
 words as given. So that's, but I haven't, so that I take words as given as one thing. I haven't

1:22:33.920 --> 1:22:38.880
 actually seen people that use autopilot that believe that the behavior is really important,

1:22:39.440 --> 1:22:45.280
 like the actual action. So like this is like to push back on the very thing that you're frustrated

1:22:45.280 --> 1:22:51.520
 about, which is like journalists and general people buying all the hype and going on in the

1:22:51.520 --> 1:22:57.600
 same way. I think there's a lot of hype about the negatives of this too, that people are buying

1:22:57.600 --> 1:23:03.600
 without using people use the way this is what this was this open my eyes. Actually, the way

1:23:03.600 --> 1:23:08.960
 people use a product is very different than the way they talk about it. This is true with robotics

1:23:08.960 --> 1:23:14.160
 with everything. Everybody has dreams of how a particular product might be used or so on this.

1:23:14.160 --> 1:23:18.080
 And then when it meets reality, there's a lot of fear of robotics, for example,

1:23:18.080 --> 1:23:21.680
 that robots are somehow dangerous and all those kinds of things. But when you actually have

1:23:21.680 --> 1:23:26.160
 robots in your life, whether it's in the factory or in the home, making your life better, that's

1:23:26.160 --> 1:23:30.800
 going to be, that's way different. The your perceptions of it are going to be way different.

1:23:30.800 --> 1:23:35.040
 And so my just tension was like, here's an innovator.

1:23:35.040 --> 1:23:41.600
 What is it? Sorry, super cruise from Cadillac was super interesting too. That's a really

1:23:41.600 --> 1:23:45.120
 interesting system. We should like be excited by those innovations.

1:23:45.120 --> 1:23:49.920
 Okay, so can I tell you something that's really annoyed me recently? It's really annoyed me

1:23:49.920 --> 1:23:57.920
 that the press and friends of mine on Facebook are going, these billionaires and their space games,

1:23:57.920 --> 1:24:00.400
 you know, why are they doing that? Yeah, that's been very frustrating.

1:24:00.400 --> 1:24:08.480
 It really pisses me off. I must say, I applaud that. I applaud it. It's the taking and not

1:24:08.480 --> 1:24:14.240
 necessarily the people who are doing the things, but, you know, that I keep having to push back

1:24:14.240 --> 1:24:19.760
 against unrealistic expectations when these things can become real.

1:24:19.760 --> 1:24:25.360
 Yeah, I, this was interesting on the, because there's been a particular focus for me is

1:24:25.360 --> 1:24:30.160
 autonomous driving, Elon's prediction of when he's going to be able to do that.

1:24:30.160 --> 1:24:31.840
 When certain milestones will be hit.

1:24:35.200 --> 1:24:39.440
 There's several things to be said there that I always, I thought about because

1:24:39.440 --> 1:24:44.240
 whenever you said them, it was obvious that's not going to me as a person that kind of

1:24:45.680 --> 1:24:52.880
 not inside the system is obviously unlikely to hit those. There's two comments I want to make.

1:24:52.880 --> 1:25:04.000
 One, he legitimately believes it. And two, much more importantly, I think that having ambitious

1:25:04.000 --> 1:25:09.280
 deadlines drives people to do the best work of their life, even when the odds of those deadlines

1:25:09.280 --> 1:25:14.400
 are very low. To a point, and I'm not, I'm not talking about anyone here. I'm just saying.

1:25:14.400 --> 1:25:23.680
 So there's a line there, right? You have to have a line because you overextend and it's demoralizing.

1:25:23.680 --> 1:25:32.480
 But I will say that there's an additional thing here that those words also drive the stock market.

1:25:34.160 --> 1:25:40.720
 And, you know, we have because of the way that rich people in the past have manipulated the

1:25:40.720 --> 1:25:49.120
 rubes through investment, we have developed laws about what you're allowed to say.

1:25:51.280 --> 1:25:58.800
 There's an area here which is... I tend to be, maybe I'm naive, but I tend to believe

1:26:00.480 --> 1:26:07.440
 that engineers, innovators, people like that, they're not, they don't think like that,

1:26:07.440 --> 1:26:13.520
 like manipulating the stock price. But it's possible that I'm, I'm certain it's possible

1:26:13.520 --> 1:26:21.360
 that I'm wrong. It's a very cynical view of the world because I think most people that run companies

1:26:21.360 --> 1:26:28.800
 and build, like, especially original founders, they... Yeah, I'm not saying that's the intent.

1:26:28.800 --> 1:26:35.360
 I'm saying it's a... Eventually it's kind of, you fall into that kind of behavior pattern. I don't

1:26:35.360 --> 1:26:42.160
 know. I tend to... I wasn't saying it's falling into that intent. It's just you also have to protect

1:26:42.160 --> 1:26:48.880
 investors in this market. Yeah. Okay, so you have, first of all, you have an amazing blog

1:26:48.880 --> 1:26:54.080
 that people should check out, but you also have this in that blog, a set of predictions.

1:26:54.880 --> 1:26:58.960
 Such a cool idea. I don't know how long ago you started, like three, four years ago. It was

1:26:58.960 --> 1:27:06.240
 January 1st, 2018. Yeah. And I made these predictions and I said that every January

1:27:06.240 --> 1:27:10.240
 1st, I was going to check back on how my predictions... That's such a great thought.

1:27:10.240 --> 1:27:15.680
 For 32 years. Oh, you said 32 years. I said 32 years because I thought that'll be January 1st,

1:27:15.680 --> 1:27:29.840
 2050. I'll be... I will just turn 95. Nice. And so people know that your predictions,

1:27:29.840 --> 1:27:34.000
 at least for now, are in the space of artificial intelligence. Yeah. I didn't say I was going

1:27:34.000 --> 1:27:37.040
 to make new predictions. I was just going to measure this set of predictions that I made

1:27:37.040 --> 1:27:41.520
 because I was sort of annoyed that everyone could make predictions. They didn't come true

1:27:41.520 --> 1:27:46.080
 and everyone forgot. So I should hope myself to a higher standard. Yeah. But also just putting

1:27:46.080 --> 1:27:52.880
 years and date rangers on things, it's a good thought exercise and reasoning your thoughts out.

1:27:52.880 --> 1:28:01.440
 And so the topics are artificial intelligence, autonomous vehicles, and space. I was wondering

1:28:01.440 --> 1:28:06.080
 if we could just go through some that stand out. Maybe from memory, I can just mention to you some,

1:28:06.080 --> 1:28:11.120
 let's talk about self driving cars, some predictions that you're particularly proud of

1:28:11.120 --> 1:28:20.240
 or are particularly interesting from flying cars to the other element here is like how

1:28:20.240 --> 1:28:26.480
 widespread the location where the deployment of the autonomous vehicles is. And there's also

1:28:26.480 --> 1:28:30.240
 just a few fun ones. Is there something that jumps to mind that you remember from the predictions?

1:28:32.000 --> 1:28:38.320
 Well, I think I did put in there that there would be a dedicated self driving lane on 101

1:28:38.320 --> 1:28:44.160
 by some year. And I think I was over optimistic on that one. Yeah, I actually do remember that.

1:28:44.160 --> 1:28:50.880
 But I think you were mentioning like difficulties at different cities. Yeah. So Cambridge,

1:28:50.880 --> 1:28:55.520
 Massachusetts, I think was an example. Yeah, like in Cambridgeport. I lived in

1:28:55.520 --> 1:29:01.760
 Cambridgeport for a number of years and the roads are narrow and getting anywhere as a human

1:29:01.760 --> 1:29:06.880
 driver is incredibly frustrating when you start to put. And people drive the wrong way on one way

1:29:06.880 --> 1:29:14.320
 streets there. It's just. So your prediction was driverless taxi services operating on all

1:29:14.320 --> 1:29:24.960
 streets in Cambridgeport, Massachusetts in 2035. Yeah. And that may have been too optimistic.

1:29:24.960 --> 1:29:30.400
 You think so. You know, I've gotten a little more pessimistic since I made these internally

1:29:30.400 --> 1:29:38.800
 on some of these things. So what can you put a year to a major milestone of deployment of a

1:29:38.800 --> 1:29:47.440
 taxi service in a few major cities? Like something where you feel like autonomous vehicles are here.

1:29:47.440 --> 1:30:03.840
 So let's take the grid streets of San Francisco north of market. Relatively benign environment.

1:30:03.840 --> 1:30:10.720
 The streets are wide. The major problem is delivery trucks stopping everywhere,

1:30:10.720 --> 1:30:21.680
 which has made things more complicated. A taxi system there with somewhat designated pickup

1:30:21.680 --> 1:30:27.840
 and drop offs, unlike with Uber and Lyft, where you can sort of get to any place and the drivers

1:30:27.840 --> 1:30:39.120
 will figure out how to get in there. We're still a few years away. I live in that area. So I see

1:30:39.120 --> 1:30:45.440
 the, you know, the self driving car companies, cars, multiple, multiple ones every day out

1:30:45.440 --> 1:30:56.640
 there by the cruise. Zooks less often, Waymo all the time, different ones come and go.

1:30:56.640 --> 1:31:01.840
 And there's always a driver. There's always a driver at the moment, although I have noticed

1:31:01.840 --> 1:31:09.440
 that sometimes the driver does not have the authority to take over without talking to the

1:31:09.440 --> 1:31:16.800
 home office because they will sit there waiting for a long time. And clearly something's going on

1:31:16.800 --> 1:31:24.080
 where the home office is making a decision. So there, you know, and so you can see whether

1:31:24.080 --> 1:31:29.440
 they've got their hands on the wheel or not. And it's the incident resolution time that

1:31:29.440 --> 1:31:34.960
 tells you gives you some clues. So what year do you think? What's your intuition? What date range

1:31:34.960 --> 1:31:43.520
 are you currently thinking San Francisco would be autonomous taxi service from any point A to

1:31:43.520 --> 1:31:52.160
 any point B without a driver? Are you still thinking 10 years from now, 20 years from now,

1:31:52.160 --> 1:31:57.440
 30 years from now? Certainly not 10 years from now. It's going to be longer. If you're allowed

1:31:57.440 --> 1:32:03.440
 to go south of the market way longer, unless there's reengineering of roads.

1:32:04.240 --> 1:32:10.240
 By the way, what's the biggest challenge? You mentioned a few. Is it the delivery trucks?

1:32:10.240 --> 1:32:16.160
 Is it the edge cases, the computer perception? Well, it is a case that I saw outside my house

1:32:16.720 --> 1:32:21.840
 a few weeks ago, about 8 p.m. on a Friday night. It was getting dark before the solstice.

1:32:21.840 --> 1:32:32.960
 It was a cruise vehicle come down the hill, turned right, and stopped dead covering the

1:32:32.960 --> 1:32:38.560
 crosswalk. Why did it stop dead? Because there was a human just two feet from it.

1:32:39.360 --> 1:32:46.160
 Now, I just glanced. I knew what was happening. The human was a woman was at the door of her car

1:32:46.160 --> 1:32:49.360
 trying to unlock it with one of those things that, you know, when you don't have a key.

1:32:49.360 --> 1:32:54.800
 Yes. The car thought, oh, she could jump out in front of me any second.

1:32:55.520 --> 1:32:59.360
 As a human, I could tell, no, she's not going to jump out. She's busy trying to unlock her.

1:32:59.360 --> 1:33:05.360
 She's lost her keys. She's trying to get in the car. And it stayed there until I got bored.

1:33:08.720 --> 1:33:14.080
 And so the human driver in there did not take over. But here's the kicker to me.

1:33:14.080 --> 1:33:22.880
 A guy comes down the hill with a stroller. I assume there's a baby in there. And now the

1:33:22.880 --> 1:33:29.200
 crosswalk is blocked by this cruise vehicle. What's he going to do? Cleverly, I think he

1:33:29.200 --> 1:33:36.400
 decided not to go in front of the car. But he had to go behind it. He had to get off the crosswalk

1:33:36.400 --> 1:33:41.200
 out into the intersection to push his baby around this car, which was stopped there.

1:33:41.200 --> 1:33:45.760
 And no human driver would have stopped there for that length of time. They would have gotten out

1:33:45.760 --> 1:33:54.400
 of the way. And that's another one of my pet peeves, that safety is being compromised

1:33:55.120 --> 1:33:59.760
 for individuals who didn't sign up for having this happen in their neighborhood.

1:34:00.480 --> 1:34:03.200
 Yeah. But now you can say that's an edge case, but...

1:34:03.200 --> 1:34:12.640
 Yeah. Well, I'm in general not a fan of anecdotal evidence for stuff like this is

1:34:12.640 --> 1:34:17.040
 one of my biggest problems with the discussion of autonomous vehicles and in general,

1:34:17.040 --> 1:34:22.560
 people that criticize them or support them are using anecdotal evidence.

1:34:22.560 --> 1:34:23.920
 So let me... But I got you.

1:34:23.920 --> 1:34:27.760
 You know, your question is when is it going to happen in San Francisco? I say not soon,

1:34:27.760 --> 1:34:35.120
 but it's going to be one of the... But where it is going to happen is in limited domains,

1:34:36.160 --> 1:34:45.120
 campuses of various sorts, gated communities, where the other drivers are not arbitrary people.

1:34:46.000 --> 1:34:52.720
 They're people who know about these things, they've been warned about them, and at velocities

1:34:52.720 --> 1:34:58.720
 where it's always safe to stop dead, you can't do that on the freeway.

1:34:58.720 --> 1:35:03.520
 That, I think, we're going to start to see. And they may not be shaped like

1:35:05.360 --> 1:35:12.400
 current cars, they may be things like main mobility has those things and various companies have

1:35:12.400 --> 1:35:17.200
 these. Yeah, I wonder if that's a compelling experience. To me, it's not just about automations,

1:35:17.200 --> 1:35:22.800
 it's about creating a product that makes your... It's not just cheaper, but makes your... It's fun

1:35:22.800 --> 1:35:29.520
 to ride. One of the most... One of the least fun things is for a car that stops and waits.

1:35:29.520 --> 1:35:33.920
 There's something deeply frustrating for us humans, for the rest of the world to take

1:35:33.920 --> 1:35:43.760
 advantage of us as we wait. But think about not you as the customer, but someone who's in their

1:35:43.760 --> 1:35:51.040
 80s in a retirement village whose kids have said, you're not driving anymore.

1:35:51.920 --> 1:35:56.160
 And this gives you the freedom to go to the market. That's a hugely beneficial thing, but

1:35:56.720 --> 1:36:02.800
 it's a very few orders of magnitude less impact on the world. It's not just a few people in a

1:36:02.800 --> 1:36:09.280
 small community using cars as opposed to the entirety of the world. I like that the first

1:36:09.280 --> 1:36:15.840
 time that a car equipped with some version of a solution to the trolley problem is what's NIML

1:36:15.840 --> 1:36:27.360
 stand for? Not in my life. I define my lifetime as 2050. I ask you, when have you had to decide

1:36:27.360 --> 1:36:31.840
 which person shall I kill? No, you put the brakes on and you brake as hard as you can.

1:36:31.840 --> 1:36:39.840
 You're not making that decision. I do think autonomous vehicles or semi autonomous vehicles

1:36:39.840 --> 1:36:44.400
 do need to solve the whole pedestrian problem that has elements of the trolley problem within

1:36:44.400 --> 1:36:49.920
 it. And I talk about it in one of the articles or blog posts that I wrote.

1:36:53.040 --> 1:36:59.600
 One of my coworkers has told me he does this. He tortures autonomously driven vehicles and

1:36:59.600 --> 1:37:05.920
 pedestrians. We'll torture them. Now, once they realize that putting one foot off the curb

1:37:05.920 --> 1:37:10.320
 makes the car think that they might walk into the road, kids, teenagers will be doing that

1:37:10.320 --> 1:37:17.120
 all the time. By the way, this is a whole other discussion because my main issue with robotics

1:37:17.120 --> 1:37:23.440
 is HRI, human and robot interaction. I believe that robots that interact with humans will have to

1:37:23.440 --> 1:37:31.440
 push back. They can't just be bullied because that creates a very uncompelling experience for

1:37:31.440 --> 1:37:38.480
 the humans. Waymo, before it was called Waymo, discovered that they had to do that at four way

1:37:38.480 --> 1:37:43.920
 intersections. They had to nudge forward to give the cue that they were going to go because otherwise

1:37:43.920 --> 1:37:50.560
 the other drivers would just eat them all the time. You cofounded iRobot, as we mentioned,

1:37:50.560 --> 1:37:56.560
 one of the most successful robotics companies ever. What are you most proud of with that company

1:37:56.560 --> 1:38:04.080
 and the approach you took to robotics? Well, there's something I'm quite proud of there,

1:38:04.720 --> 1:38:12.240
 which may be a surprise, but I was still on the board when this happened. It was March 2011,

1:38:12.240 --> 1:38:25.120
 and we sent robots to Japan and they were used to help shut down the Fukushima Daiichi nuclear

1:38:25.120 --> 1:38:31.840
 power plant, which was everything was up in there since I was there in 2014. Some of the

1:38:31.840 --> 1:38:37.600
 robots were still there. I was proud that we were able to do that. Why were we able to do that?

1:38:37.600 --> 1:38:44.800
 People have said, well, Japan is so good at robotics. It was because we had had

1:38:45.600 --> 1:38:53.120
 about 6,500 robots deployed in Iraq and Afghanistan, teleopped, but with intelligence,

1:38:54.320 --> 1:39:02.880
 dealing with roadside bombs. We had, I think it was at that time, nine years of in field experience

1:39:02.880 --> 1:39:10.000
 with the robots in harsh conditions, whereas the Japanese robots, which goes back to what

1:39:10.720 --> 1:39:16.560
 annoys me so much, getting all the hype, look at that. Look at that Honda robot. It can walk. Well,

1:39:16.560 --> 1:39:22.560
 the future's here. Couldn't do a thing because they weren't deployed, but we had deployed in

1:39:22.560 --> 1:39:28.960
 really harsh conditions for a long time, and so we're able to do something very positive

1:39:28.960 --> 1:39:35.600
 in a very bad situation. What about just the simple, and for people who don't know, one of the

1:39:35.600 --> 1:39:44.400
 things that iRobot has created is the Roomba vacuum cleaner. What about the simple robot

1:39:44.400 --> 1:39:51.760
 that is the Roomba, quote unquote, simple, that's deployed in tens of millions of homes?

1:39:51.760 --> 1:39:59.120
 What do you think about that? Well, I make the joke that I started out life as a pure

1:39:59.120 --> 1:40:05.360
 mathematician and turned into a vacuum cleaner salesman, so if you're going to be an entrepreneur,

1:40:05.360 --> 1:40:16.800
 be ready to do anything. But I was, you know, there was a wacky lawsuit that I got

1:40:16.800 --> 1:40:23.600
 opposed for not too many years ago, and I was the only one who had emailed from the 1990s,

1:40:24.400 --> 1:40:30.240
 and no one in the company had it, so I went and went through my email, and it reminded me of,

1:40:32.320 --> 1:40:38.720
 you know, the joy of what we were doing, and what was I doing? What was I doing at the time we were

1:40:38.720 --> 1:40:47.760
 building the Roomba. One of the things was we had this incredibly tight budget, because we

1:40:47.760 --> 1:40:55.200
 wanted to put it on the shelves at $200. There was another home cleaning robot at the time. It was

1:40:55.200 --> 1:41:04.640
 the Electrolux Trilobite, which sold for 2,000 euros, and to us that was not going to be a

1:41:04.640 --> 1:41:11.920
 consumer product. So we had reason to believe that $200 was a thing that people would buy at,

1:41:12.480 --> 1:41:18.000
 that was our aim. But that meant we had, you know, that's on the shelf making profit.

1:41:18.880 --> 1:41:26.560
 That means the cost of goods has to be minimal. So I found all these emails of me going, you know,

1:41:26.560 --> 1:41:33.040
 I'd be in Taipei for a MIT meeting, and I'd stay a few extra days. I'd go down to Shinshu and talk to

1:41:33.040 --> 1:41:40.000
 these little tiny companies, lots of little tiny companies outside of TSMC, Taiwan Semiconductor,

1:41:40.000 --> 1:41:45.280
 Taiwan Semiconductor Manufacturing Corporation, which let all these little companies be fabulous.

1:41:45.280 --> 1:41:51.760
 They didn't have to have their own fab, so they could innovate. And they were building, their

1:41:51.760 --> 1:41:58.080
 innovations were to build stripped down 6802s. 6802 was what was in an Apple One. Get rid of

1:41:58.080 --> 1:42:04.800
 half the silicon and still have it be viable. And I'd previously got some of those for some

1:42:04.800 --> 1:42:13.360
 earlier failed products of iRobot. And then that was in Hong Kong, going to all these companies

1:42:13.360 --> 1:42:18.320
 that built, you know, they weren't gaming in the current sense. There were these handheld games

1:42:18.320 --> 1:42:25.440
 that you would play, or birthday cards, because we had about a 50 cent budget for computation.

1:42:25.440 --> 1:42:32.640
 And so I'm tracking from place to place, looking at their chips, looking at what they'd removed.

1:42:32.640 --> 1:42:39.840
 Oh, they're interrupt, they're interrupt handling is too weak for a general purpose. So I was going

1:42:39.840 --> 1:42:44.880
 deep technical detail. And then I found this one from a company called Winbond, which had,

1:42:45.920 --> 1:42:51.600
 and I'd forgotten that had this much RAM, it had 512 bytes of RAM. And it was in our budget,

1:42:51.600 --> 1:42:58.640
 and it had all the capabilities we needed. Yeah. So you're excited. Yeah. And I was reading all

1:42:58.640 --> 1:43:05.040
 these emails, Colin, I found this. So did you think, did you ever think that you guys could be

1:43:05.040 --> 1:43:10.480
 so successful? Like, eventually, this company would be so successful. Did you could you possibly

1:43:10.480 --> 1:43:17.040
 have imagined? And no, we never did think that we had 14 failed business models up to 2002.

1:43:17.040 --> 1:43:26.080
 And then we had two winners same year. No, and then, you know, we I remember the board,

1:43:27.680 --> 1:43:34.080
 because by this time we had some venture capital in the board went along with us building

1:43:36.880 --> 1:43:45.120
 some robots for, you know, aiming at the Christmas 2002 market. And we went three times over what

1:43:45.120 --> 1:43:51.200
 they authorized and built 70,000 of them and sold them all in that first because we released on

1:43:51.200 --> 1:44:01.760
 September 18. And I was all sold by Christmas. So it was so we were gutsy. But, but yeah,

1:44:01.760 --> 1:44:04.480
 you didn't think this will take over the world. Well, this is a

1:44:07.040 --> 1:44:12.400
 so a lot of amazing robotics companies have gone under over the past few decades.

1:44:12.400 --> 1:44:19.680
 Why do you think it's so damn hard to run a successful robotics company?

1:44:19.680 --> 1:44:28.240
 Well, there's a few things. One is expectations of capabilities by the

1:44:29.600 --> 1:44:34.480
 founders that are off base. The founders, not the consumer, the founders.

1:44:34.480 --> 1:44:42.240
 Yeah, expectations of what what can be delivered. Sure. Miss pricing. And what a customer thinks

1:44:42.240 --> 1:44:50.480
 is a valid price is not rational necessarily. Yeah. And expectations of customers. And

1:44:53.440 --> 1:45:02.000
 just the sheer hardness of getting people to adopt a new technology. And I've suffered from all

1:45:02.000 --> 1:45:09.440
 three of these. You know, I've had more failures and successes in terms of companies. I've suffered

1:45:09.440 --> 1:45:21.840
 from all three. So do you think one day there will be a robotics company? And by robotics company,

1:45:21.840 --> 1:45:28.240
 I mean, where your primary source of income is from robots, that will be a trillion plus dollar

1:45:28.240 --> 1:45:39.840
 company. And so what would that company do? I can't, you know, because I'm still starting robot

1:45:39.840 --> 1:45:46.880
 companies. Yeah. I'm not making any such predictions in my own mind. I'm not thinking

1:45:46.880 --> 1:45:51.360
 about a trillion dollar company. And by the way, I don't think, you know, in the 90s, anyone was

1:45:51.360 --> 1:45:56.080
 thinking that Apple would ever be a trillion dollar company. So these are these are very hard to

1:45:56.080 --> 1:46:03.360
 to predict. But sorry to interrupt. But don't you because I kind of have a vision in a small way

1:46:03.360 --> 1:46:08.640
 and it's a big vision in a small way that I see that there would be robots in the home

1:46:10.240 --> 1:46:17.600
 at scale like Roomba, but more. And that's trillion dollar. Right. And I think there's a

1:46:17.600 --> 1:46:23.200
 real market pull for them because of the demographic inversion, you know, who's who's

1:46:23.200 --> 1:46:29.680
 going to do the stuff for the older people? There's too many, you know, I'm leading here.

1:46:31.760 --> 1:46:39.600
 There's gonna be too many of us. And but we don't have capable enough robots to

1:46:39.600 --> 1:46:44.800
 to make that economic argument at this point. Do I expect that that will happen? Yes, I expect

1:46:44.800 --> 1:46:50.640
 it will happen. But I got to tell you, we introduced the Roomba in 2002, and I stayed another

1:46:50.640 --> 1:46:55.920
 nine years. We were always trying to find what the next home robot would be. And

1:46:57.280 --> 1:47:02.800
 still today, the primary product of 20 years late, almost 20 years later, 19 years later,

1:47:02.800 --> 1:47:08.400
 the primary product is still the Roomba. So I robot hasn't found the next one. Do you think it's

1:47:08.400 --> 1:47:15.440
 possible for one person in the garage to build it versus like Google launching Google self driving

1:47:15.440 --> 1:47:20.720
 car that turns into Waymo? Do you think this is almost like what it takes to build a successful

1:47:20.720 --> 1:47:24.880
 robotics company? Do you think it's possible to go from the ground up? Or is it just too much

1:47:24.880 --> 1:47:33.120
 capital investment? Yeah, so it's very hard to get there without a lot of capital. And we started

1:47:33.120 --> 1:47:41.360
 to see, you know, fair chunks of capital for some robotics companies, you know, Series Bs.

1:47:41.360 --> 1:47:46.160
 Because I saw one yesterday for $80 million, I think it was for covariant.

1:47:49.120 --> 1:47:54.880
 But it can take real money to get into these things, and you may fail along the way. I

1:47:54.880 --> 1:48:00.880
 certainly failed at Rethink Robotics. And we lost $150 million in capital there.

1:48:00.880 --> 1:48:05.680
 So okay, so Rethink Robotics is another amazing robotics company you co founded.

1:48:05.680 --> 1:48:14.160
 So what was the vision there? What was the dream? And what are you most proud of with

1:48:14.160 --> 1:48:21.600
 Rethink Robotics? I'm most proud of the fact that we got robots out of the cage in factories

1:48:22.160 --> 1:48:26.160
 that were safe, absolutely safe for people and robots to be next to each other.

1:48:26.160 --> 1:48:31.120
 So these are robotic arms? Robotic arms for me to pick up stuff and interact with humans.

1:48:31.120 --> 1:48:37.600
 Yeah, and that the humans could retask them without writing code. And now that's sort of

1:48:37.600 --> 1:48:42.640
 become an expectation for a lot of other little companies and big companies are advertising

1:48:42.640 --> 1:48:49.760
 they're doing. That's both an interface problem and also a safety problem. Yeah. So I'm most proud

1:48:49.760 --> 1:49:01.200
 of that. I completely, I let myself be talked out of what I wanted to do. And you know, you've

1:49:01.200 --> 1:49:05.760
 always got, you know, I can't replay the tape. You know, I can't replay it. Maybe,

1:49:06.960 --> 1:49:12.560
 maybe, you know, if I've been stronger on, and I remember the day, I remember the exact meeting.

1:49:12.560 --> 1:49:22.160
 Can you take me through that meeting? Yeah. So I'd said that I'd set as a target for the company

1:49:22.160 --> 1:49:28.720
 that we were going to build $3,000 robots with force feedback that were safe for people to be

1:49:28.720 --> 1:49:37.520
 around. Wow. That was my goal. And we built, so we started in 2008. And we had prototypes built

1:49:37.520 --> 1:49:48.560
 of plastic, plastic gearboxes. And at a $3,000, you know, lifetime or $3,000, I was saying,

1:49:48.560 --> 1:49:53.200
 we're going to go after not the people who already have robot arms in factories, the people who would

1:49:53.200 --> 1:49:58.080
 never have a robot arm, we're going to go after a different market. So we don't have to meet their

1:49:58.080 --> 1:50:04.880
 expectations. And so we're going to build it out of plastic. It doesn't have to have a 35,000 hour

1:50:04.880 --> 1:50:13.440
 lifetime. It's going to be so cheap that it's OPEX, not CAPEX. And so we had, we had a prototype

1:50:13.440 --> 1:50:21.280
 that worked reasonably well. But the control engineers were complaining about these plastic

1:50:21.280 --> 1:50:28.480
 gearboxes with a beautiful little planetary gearbox. But we could use something called

1:50:28.480 --> 1:50:33.120
 serious elastic actuators, we embedded them in there, we can measure forces, we knew when we

1:50:33.120 --> 1:50:39.360
 hit something, etc. The control engineers were saying, yeah, but this is torque ripple, because

1:50:39.360 --> 1:50:44.400
 these plastic gears, they're not great gears. And there's this ripple and trying to do force

1:50:44.400 --> 1:50:52.640
 control around this ripple is so hard. And I'm not going to name names, but I remember

1:50:52.640 --> 1:50:57.440
 one of the mechanical engineers saying, we'll just build a metal gearbox with spur gears.

1:50:57.440 --> 1:51:04.800
 And it'll take six weeks, we'll be done, problem solved. Two years later, we got the

1:51:04.800 --> 1:51:11.120
 gear, the spur gearbox working. We cost reduced at every possible way we could.

1:51:12.800 --> 1:51:17.520
 But now the price went up to, and then the CEO at the time said, well,

1:51:17.520 --> 1:51:24.000
 we have to have two arms, not one arm. So our first robot product Baxter now cost $25,000.

1:51:24.000 --> 1:51:30.320
 And the only people who were going to look at that were people who had arms in factories,

1:51:30.320 --> 1:51:35.120
 because that was somewhat cheaper for two arms than arms and factories. But they were used to

1:51:35.760 --> 1:51:44.320
 0.1 millimeter reproducibility of motion and certain velocities. And I kept thinking, but

1:51:44.320 --> 1:51:48.720
 that's not what we're giving you. You don't need position repeatability, use force control like

1:51:48.720 --> 1:51:55.360
 a human does. No, but we want that repeatability. We want that repeatability. All the other robots

1:51:55.360 --> 1:52:00.400
 have that repeatability. Why don't you have that repeatability? So can you clarify force controls

1:52:00.400 --> 1:52:04.640
 you can grab the arm and you can move it? Yeah, you can move it around. But suppose you,

1:52:06.160 --> 1:52:10.160
 can you see that? Yes. Suppose you want to... Yes.

1:52:11.120 --> 1:52:15.840
 Suppose this thing is a precise thing that's got a fit here in this right angle.

1:52:15.840 --> 1:52:23.200
 Yeah. Under position control, you have fixed it where this is. You know where this is precisely

1:52:23.200 --> 1:52:27.920
 and you just move it and it goes there. If force control, you would do something like

1:52:28.640 --> 1:52:34.240
 slide it over here till we feel that and slide it in there. And that's how a human gets precision.

1:52:34.800 --> 1:52:40.640
 They use force feedback and get the things to mate rather than just go straight to it.

1:52:40.640 --> 1:52:48.160
 Yeah. Couldn't convince our customers who are in factories and were used to thinking about

1:52:48.160 --> 1:52:54.320
 things a certain way. And they wanted it. So then we said, okay, we're going to build

1:52:54.320 --> 1:52:59.920
 an arm that gives you that. So now we ended up building a $35,000 robot with one arm with...

1:53:02.400 --> 1:53:03.280
 Oh, what are they called?

1:53:03.280 --> 1:53:10.880
 A certain sort of gearbox made by a company whose name I can't remember right now,

1:53:10.880 --> 1:53:19.120
 but it's the name of the gearbox. But it's got torque ripple in it. So now there was an extra

1:53:19.120 --> 1:53:23.280
 two years of solving the problem of doing the force with the torque ripple. So we had to do the

1:53:24.640 --> 1:53:31.280
 thing we had avoided. And for the plastic gearbox, as we ended up having to do,

1:53:31.280 --> 1:53:37.440
 the robot was now overpriced. And that was your intuition from the very beginning,

1:53:37.440 --> 1:53:43.840
 kind of that this is not... You're opening a door to solve a lot of problems that you're

1:53:43.840 --> 1:53:47.920
 eventually going to have to solve this problem anyway. Yeah. And also, I was aiming at a low

1:53:47.920 --> 1:53:52.560
 price to go into a different market that didn't have... $3,000 would be amazing.

1:53:52.560 --> 1:53:58.960
 Yeah. I think we could have done it for five. But you talked about setting the goal a little too

1:53:58.960 --> 1:54:08.400
 far for the engineers. Yeah, exactly. So why would you say that company not failed but went under?

1:54:10.080 --> 1:54:17.120
 We had buyers and there's this thing called the Committee on Foreign Investment in the US,

1:54:17.120 --> 1:54:26.400
 SIFIUS. And that had previously been invoked twice around where the government could stop

1:54:26.400 --> 1:54:36.240
 foreign money coming into a US company based on defense requirements. We went through

1:54:36.240 --> 1:54:42.800
 due diligence multiple times. We were going to get acquired. But every consortium had Chinese money

1:54:42.800 --> 1:54:48.560
 in it. And all the bankers would say at the last minute, you know, this isn't going to get past SIFIUS

1:54:49.200 --> 1:54:55.680
 and the investors would go away. And then we had two buyers. We were about to run out of money.

1:54:55.680 --> 1:55:00.320
 Two buyers. And one used heavy handed legal stuff with the other one.

1:55:03.280 --> 1:55:08.880
 Said they were going to take it and pay more. Dropped out when we were out of cash and then

1:55:08.880 --> 1:55:16.160
 bought the assets at one 30th of the price they had offered a week before. That was a tough week.

1:55:16.160 --> 1:55:26.720
 Do you, does it hurt to think about like an amazing company that didn't, you know,

1:55:26.720 --> 1:55:32.880
 like iRobot didn't find a way? It was tough. I said I was never going to start another company.

1:55:32.880 --> 1:55:39.840
 I was pleased that everyone liked what we did so much that the team was hired by

1:55:41.520 --> 1:55:45.360
 three companies within a week. Everyone had a job in one of these three companies. Some stayed in

1:55:45.360 --> 1:55:52.720
 their same desks because another company came in and rented the space. So I felt good about people

1:55:52.720 --> 1:56:02.720
 not being out on the street. So Baxter has a screen with a face. What, that's a revolutionary idea for

1:56:02.720 --> 1:56:09.760
 a robot manipulation, a robotic arm. How much opposition did you get? Well, first the screen

1:56:09.760 --> 1:56:15.360
 was also used during codeless programming where you taught by demonstration that showed you what

1:56:15.360 --> 1:56:24.720
 its understanding of the task was. So it had two roles. Some customers hated it and so we made it

1:56:24.720 --> 1:56:29.520
 so that when the robot was running it could be showing graphs of what was happening and not show

1:56:29.520 --> 1:56:36.640
 the eyes. Other people and some of them surprised me who they were saying, well this one doesn't

1:56:36.640 --> 1:56:41.840
 look as human as the old one. We like the human looking. So there was a mixed bag.

1:56:43.280 --> 1:56:48.720
 But do you think that's, I don't know, I'm kind of disappointed whenever I talk to

1:56:50.400 --> 1:56:56.000
 roboticists, like the best robotics people in the world, they seem to not want to do the eyes type

1:56:56.000 --> 1:57:01.600
 of thing. They seem to see it as a machine as opposed to a machine that can also have a human

1:57:01.600 --> 1:57:05.920
 connection. I'm not sure what to do with that. It seems like a lost opportunity. I think the

1:57:05.920 --> 1:57:11.120
 trillion dollar company will have to do the human connection very well no matter what it does.

1:57:11.120 --> 1:57:18.320
 Yeah, I agree. Can I ask you a ridiculous question? Sure. Can I give a ridiculous answer?

1:57:19.920 --> 1:57:25.440
 Do you think, well maybe by way of asking the question, let me first mention that

1:57:25.440 --> 1:57:29.040
 you're kind of critical of the idea of the Turing test as a test of intelligence.

1:57:29.040 --> 1:57:39.200
 Let me first ask this question. Do you think we'll be able to build an AI system that humans

1:57:39.200 --> 1:57:44.480
 fall in love with and it falls in love with the human, like romantic love?

1:57:47.520 --> 1:57:52.080
 Well, we've had that with humans falling in love with cars even back in the 50s.

1:57:52.080 --> 1:57:58.320
 It's a different love, right? I think there's a lifelong partnership where you can communicate

1:57:58.320 --> 1:58:09.280
 and grow. I think we're a long way from that. I think Blade Runner had the time scale totally

1:58:09.280 --> 1:58:18.080
 wrong. To me, honestly, the most difficult part is the thing that you said with the Marvel X

1:58:18.080 --> 1:58:23.680
 Paradox is to create a human form that interacts and perceives the world. But if we just look at a

1:58:23.680 --> 1:58:30.720
 voice, like the movie Her or just like an Alexa type voice, I tend to think we're not that far away.

1:58:32.560 --> 1:58:46.080
 Well, for some people, maybe not. But as humans, as we think about the future, we always try,

1:58:46.800 --> 1:58:50.960
 and this is the premise of most science fiction movies, you've got the world just as

1:58:50.960 --> 1:58:57.120
 is today, and you change one thing. But that's the same with the self driving car. You change

1:58:57.120 --> 1:59:05.120
 one thing. Everything changes. Everything grows together. Surprisingly, I might be surprising

1:59:05.120 --> 1:59:10.320
 to you or might not, I think the best movie about this stuff was by Centennial Man.

1:59:11.840 --> 1:59:16.880
 And what was happening there? It was schmaltzy. But what was happening there?

1:59:16.880 --> 1:59:24.400
 As the robot was trying to become more human, the humans were adopting the technology of the robot

1:59:24.400 --> 1:59:30.960
 and changing their bodies. So there was a convergence happening in a sense. So we will not

1:59:30.960 --> 1:59:35.840
 be the same. We're already talking about genetically modifying our babies. There's

1:59:37.600 --> 1:59:43.440
 more and more stuff happening around that. We will want to modify ourselves even more for all

1:59:43.440 --> 1:59:55.200
 sorts of things. We put all sorts of technology in our bodies to improve it. I've got things in

1:59:55.200 --> 2:00:03.360
 my ears so that I can sort of hear you. So we're always modifying our bodies. So I think it's

2:00:03.360 --> 2:00:09.120
 hard to imagine exactly what it will be like in the future. But on the touring test side,

2:00:09.120 --> 2:00:15.360
 do you think, so forget about love for a second. Let's talk about just the

2:00:15.360 --> 2:00:22.080
 elect surprise. Actually, I was invited to be an interviewer for the elect surprise or whatever

2:00:23.120 --> 2:00:31.840
 that's in two days. Their idea is success looks like a person wanting to talk to an AI system

2:00:31.840 --> 2:00:39.680
 for a prolonged period of time, like 20 minutes. How far away are we and why is it difficult to

2:00:39.680 --> 2:00:45.600
 build an AI system with which you'd want to have a beer and talk for an hour or two hours?

2:00:46.800 --> 2:00:53.200
 Not for to check the weather or to check music, but just to talk as friends.

2:00:53.200 --> 2:00:59.280
 Yeah. Well, we saw Weisenbaum back in the 60s with his programmer, Liza,

2:00:59.280 --> 2:01:04.000
 being shocked at how much people would talk to Eliza. And I remember,

2:01:04.720 --> 2:01:08.960
 in the 70s, typing stuff to Eliza to see what it would come back with.

2:01:10.800 --> 2:01:14.800
 I think right now, and this is a thing that

2:01:18.160 --> 2:01:21.520
 Amazon's been trying to improve with elect. So there is no continuity of

2:01:21.520 --> 2:01:30.880
 topic. You can't refer to what we talked about yesterday. It's not the same as talking to a

2:01:30.880 --> 2:01:36.160
 person where there seems to be an ongoing existence, right? Changes. We share moments

2:01:36.160 --> 2:01:40.800
 together and they last in our memory together. Yeah. There's none of that. And there's no

2:01:42.560 --> 2:01:49.040
 sort of intention of these systems that they have any goal in life, even if it's to be happy.

2:01:49.040 --> 2:01:54.960
 You know, they don't even have a semblance of that. Now, I'm not saying this can't be done.

2:01:54.960 --> 2:01:59.040
 I'm just saying, I think this is why we don't feel that way about them.

2:02:00.960 --> 2:02:06.400
 I'm sort of a minimal requirement. If you want the sort of interaction you're talking about,

2:02:06.400 --> 2:02:10.240
 it's a minimal requirement. Whether it's going to be sufficient, I don't know.

2:02:10.800 --> 2:02:14.640
 We haven't seen it yet. We don't know what it feels like.

2:02:14.640 --> 2:02:22.000
 I tend to think it's not as difficult as solving intelligence, for example,

2:02:22.640 --> 2:02:28.560
 and I think it's achievable in the near term. But on the Turing test,

2:02:29.600 --> 2:02:32.880
 why don't you think the Turing test is a good test of intelligence?

2:02:32.880 --> 2:02:39.600
 Oh, because again, the Turing, if you read the paper, Turing wasn't saying this is

2:02:39.600 --> 2:02:46.240
 a good test. He was using this as a rhetorical device to argue that if you can't tell the

2:02:46.240 --> 2:02:52.240
 difference between a computer and a person, you must say that the computer's thinking because

2:02:52.960 --> 2:02:58.240
 you can't tell the difference when it's thinking. You can't say something different.

2:02:59.440 --> 2:03:05.520
 What it has become as this sort of weird game of fooling people. So

2:03:05.520 --> 2:03:15.040
 back at the AI lab in the late 80s, we had this thing that still goes on called the AI Olympics.

2:03:15.040 --> 2:03:22.240
 And one of the events we had one year was the original imitation game as Turing talked about

2:03:22.240 --> 2:03:28.480
 because he starts by saying, can you tell whether it's a man or a woman? So we did that at the lab.

2:03:28.480 --> 2:03:33.440
 We had, you know, you'd go and type and the thing would come back and you had to tell whether it

2:03:33.440 --> 2:03:51.040
 was a man or a woman. And one man came up with a question that he could ask, which was always a

2:03:51.040 --> 2:03:57.520
 dead giveaway of whether the other person was really a man or a woman. He would ask them,

2:03:57.520 --> 2:04:04.960
 did you have green plastic toy soldiers as a kid? Yeah. What do you do with them? And a woman

2:04:04.960 --> 2:04:09.680
 trying to be a man would say, oh, I lined them up. We had wars. We had battles. And the man just

2:04:09.680 --> 2:04:20.080
 being a man. I stomped on them. I burned them. So, you know, that's what the Turing test with

2:04:20.080 --> 2:04:26.880
 computers has become. What's the trick question? That's why it's sort of devolved into this.

2:04:29.520 --> 2:04:32.960
 Nevertheless, conversation not formulated as a test is a pretty,

2:04:33.680 --> 2:04:40.160
 it's a fascinatingly challenging dance. That's a really hard problem. To me, conversation when

2:04:40.160 --> 2:04:47.200
 non poses a test is a more intuitive illustration how far away we are from solving intelligence

2:04:47.200 --> 2:04:53.520
 than my computer vision. It's hard. Computer vision is harder for me to pull apart. But with

2:04:53.520 --> 2:04:57.120
 language, with conversation, you could see... Because language is so human. We don't...

2:04:57.120 --> 2:05:06.880
 It's so human. We can so clearly see it. Shit, you mentioned something I was going to go off on.

2:05:06.880 --> 2:05:16.080
 Okay. I mean, I have to ask you, because you were the head of CSAIL, AI lab for a long time.

2:05:16.800 --> 2:05:23.360
 You're, I don't know, to me, when I came to MIT, you're like one of the greats at MIT. So, what

2:05:23.360 --> 2:05:33.600
 was that time like? And plus, you were friends with, but you knew Minsky and all the folks there,

2:05:33.600 --> 2:05:40.480
 all the legendary AI people of which you were one. So, what was that time like? What are memories

2:05:40.480 --> 2:05:48.640
 that send out to you from that time? From your time at MIT, from the AI lab, from the dreams

2:05:48.640 --> 2:05:52.960
 that the AI lab represented to the actual revolutionary work?

2:05:53.520 --> 2:05:58.080
 Let me tell you first a disappointment in myself. As I've been researching this book,

2:05:58.080 --> 2:06:06.960
 and so many of the players were active in the 50s and 60s, I knew many of them when they were older.

2:06:06.960 --> 2:06:12.640
 And I didn't ask them all the questions. Now, I wish I had asked. I'd sit with them at our

2:06:12.640 --> 2:06:18.320
 Thursday lunches, which we had a faculty lunch. And I didn't ask them so many questions that now

2:06:18.320 --> 2:06:23.200
 I wish I had. Can I ask you that question? Because you wrote that. You wrote that you were

2:06:23.200 --> 2:06:28.320
 fortunate to know and rub shoulders with many of the greats, those who founded AI, robotics,

2:06:28.320 --> 2:06:33.920
 and computer science, and the World Wide Web. And you wrote that your big regret nowadays is that

2:06:33.920 --> 2:06:39.760
 often I have questions for those who have passed on. And I didn't think to ask them any of these

2:06:39.760 --> 2:06:47.120
 questions, even as I saw them and said hello to them on a daily basis. So, maybe also another

2:06:47.120 --> 2:06:53.040
 question I want to ask. If you could talk to them today, what question would you ask? What

2:06:53.040 --> 2:07:01.920
 questions would you ask? I would ask him, you know, he had the vision for humans and computers

2:07:01.920 --> 2:07:08.880
 working together. And he really founded that at DARPA. And he gave the money to MIT, which

2:07:08.880 --> 2:07:16.160
 started Project MAC in 1963. And I would have talked to him about what the successes were,

2:07:16.160 --> 2:07:23.520
 what the failures were, what he saw as progress, etc. I would have asked him more questions about

2:07:23.520 --> 2:07:29.680
 that. Because now I could use it in my book. But I think it's lost. It's lost forever. A lot of

2:07:29.680 --> 2:07:42.000
 the motivations are lost. I should have asked Marvin why he and Seymour Papert came down so hard

2:07:42.000 --> 2:07:48.720
 on neural networks in 1968 in their book Perceptrons. Because Marvin's PhD thesis was on

2:07:48.720 --> 2:07:52.800
 neural networks. How do you make sense of that? That book destroyed the field.

2:07:54.720 --> 2:07:57.280
 Do you think he knew the effect that book would have?

2:07:57.280 --> 2:08:12.960
 All the theorems are negative theorems. That's the way of life. But still,

2:08:12.960 --> 2:08:17.360
 it's kind of tragic that he was both the proponent and the destroyer of neural networks.

2:08:17.360 --> 2:08:25.200
 Yeah. Is there other memory standouts from the robotics and the AI work at MIT?

2:08:28.800 --> 2:08:33.840
 Yeah, but you'll be more specific. Well, I mean, it's such a magical place. To me,

2:08:33.840 --> 2:08:42.080
 it's a little bit also heartbreaking that with Google and Facebook, like DeepMind and so on,

2:08:42.080 --> 2:08:50.400
 so much of the talent doesn't stay necessarily for prolonged periods of time in these universities.

2:08:50.400 --> 2:08:55.040
 Oh, yeah. I mean, some of the companies are more guilty than others of paying

2:08:56.480 --> 2:09:02.800
 fabulous salaries to some of the highest producers. And then just you never hear from them again.

2:09:02.800 --> 2:09:08.000
 They're not allowed to give public talks. It's sort of locked away. And it's sort of like collecting

2:09:08.000 --> 2:09:14.480
 Hollywood stars or something. And they're not allowed to make movies anymore. I heard them.

2:09:15.360 --> 2:09:21.760
 Yeah. That's tragic. I mean, there's an openness to the university setting where you do research

2:09:21.760 --> 2:09:25.520
 to both in the space of ideas and space like publication, all those kinds of things.

2:09:25.520 --> 2:09:31.200
 Yeah. And there's the publication and all that and often, although these places,

2:09:31.200 --> 2:09:41.600
 say they publish this pressure. But I think, for instance, net net, I think

2:09:43.680 --> 2:09:48.160
 Google buying those eight or nine robotics company was bad for the field because it locked

2:09:48.160 --> 2:09:54.720
 those people away. They didn't have to make the company succeed anymore. Locked them away for years

2:09:54.720 --> 2:10:04.080
 and then sort of all fiddled away. Yeah. So do you have hope for MIT?

2:10:05.840 --> 2:10:10.720
 For MIT? Yeah, why shouldn't I? Well, I could be harsh and say that

2:10:12.800 --> 2:10:19.360
 I'm not sure I would say MIT is leading the world in AI or even Stanford or Berkeley.

2:10:19.360 --> 2:10:28.080
 I would say DeepMind, Google AI, Facebook AI. I would take a slightly different approach,

2:10:28.800 --> 2:10:35.040
 a different answer. I'll come back to Facebook in a minute. But I think those other places are

2:10:36.400 --> 2:10:45.280
 following a dream of one of the founders. And I'm not sure that it's well founded

2:10:45.280 --> 2:10:53.120
 the dream. And I'm not sure that it's going to have the impact that he believes it is.

2:10:55.280 --> 2:10:57.120
 You're talking about Facebook and Google and so on.

2:10:57.120 --> 2:10:58.240
 I'm talking about Google.

2:10:58.240 --> 2:11:05.680
 Google. But the thing is, those research labs aren't, there's the big dream. And I'm usually a

2:11:05.680 --> 2:11:10.400
 fan of, no matter what the dream is, a big dream is a unifier. Because what happens is you have a

2:11:10.400 --> 2:11:18.960
 lot of bright minds working together on a dream. What results is a lot of adjacent ideas. I mean,

2:11:18.960 --> 2:11:23.920
 there's so much progress is made. Yeah. So I'm not saying they're actually leading. I'm not

2:11:23.920 --> 2:11:29.040
 saying that the universities are leading. But I don't think those companies are leading in general

2:11:29.040 --> 2:11:38.960
 because they're, you know, we saw this incredible spike in attendees at NeurIPS. And as I said,

2:11:38.960 --> 2:11:47.120
 in my January 1st review this year for 2020, 2020 will not be remembered as a watershed year

2:11:47.120 --> 2:11:53.280
 for machine learning or AI. You know, there was nothing surprising happen. But anyway,

2:11:53.280 --> 2:12:04.320
 unlike when deep learning hit ImageNet, that was a shake. And there's a lot more people writing

2:12:04.320 --> 2:12:10.960
 papers, but the papers are fundamentally boring and uninteresting. And incremental work. Yeah.

2:12:14.800 --> 2:12:18.960
 Is there a particular memories you have with Minsky or somebody else at MIT that stand out?

2:12:19.760 --> 2:12:24.080
 Funny stories. I mean, unfortunately, he's another one that's passed away.

2:12:26.800 --> 2:12:32.160
 You've known some of the biggest minds in AI. Yeah. And, you know, they did amazing things.

2:12:32.160 --> 2:12:39.600
 And sometimes they were grumpy. Well, he was, he was interesting because he was very grumpy.

2:12:39.600 --> 2:12:45.920
 But that, that was, I remember him saying in an interview that the key to success

2:12:46.720 --> 2:12:50.960
 or being, to keep being productive is to hate everything you've ever done in the past.

2:12:52.000 --> 2:12:57.040
 Maybe that explains the Perceptron book. There it was. He told you exactly.

2:12:57.040 --> 2:13:04.720
 But he, meaning like, just like, I mean, maybe that's the way to not treat yourself too seriously,

2:13:04.720 --> 2:13:11.200
 just always be moving forward. That was his idea. I mean, that crinkiness, I mean,

2:13:13.200 --> 2:13:16.800
 that's the character. So let me, let me, let me tell you what really,

2:13:19.600 --> 2:13:26.800
 you know, the joy memories are about having access to technology before anyone else has seen it.

2:13:26.800 --> 2:13:35.200
 And so, so, you know, I got to Stanford in 1977 and we had, you know, we had terminals that could

2:13:35.200 --> 2:13:45.440
 show live video on them, digital, digital sound system. We had a Xerox graphics printer. We could

2:13:45.440 --> 2:13:52.880
 print, it wasn't, you know, it wasn't like a typewriter ball hitting characters. It could print

2:13:52.880 --> 2:13:57.600
 arbitrary things, only in, you know, one bit, you know, black or white, but you could arbitrary

2:13:57.600 --> 2:14:06.960
 pictures. This was science fiction sort of stuff. At MIT, the list machines, which, you know, they

2:14:06.960 --> 2:14:12.960
 were the first personal computers and, you know, they were cost $100,000 each. And I could, you

2:14:12.960 --> 2:14:18.000
 know, I got there early enough in the day, I got one for the day, couldn't stand up, had to keep

2:14:18.000 --> 2:14:27.200
 working. So having that, like, direct glimpse into the future. Yeah. And, you know, I've had email

2:14:27.200 --> 2:14:36.480
 every day since 1977. And, you know, the host field was only eight bits, you know, that many places,

2:14:36.480 --> 2:14:42.800
 but I could send email to other people at a few places. So that was, that was pretty exciting

2:14:42.800 --> 2:14:48.000
 to be in that world so different from what the rest of the world knew. And

2:14:50.320 --> 2:14:53.760
 let me ask you, I probably edited this out, but just in case you have a story.

2:14:56.000 --> 2:15:01.600
 I'm hanging out with Don Knuth for a while tomorrow. Did you ever get a chance at such

2:15:01.600 --> 2:15:07.520
 a different world than yours? He's a very kind of theoretical computer science, the puzzle of

2:15:07.520 --> 2:15:12.640
 computer science and mathematics. And you're so much about the magic of robotics, like the

2:15:12.640 --> 2:15:18.560
 practice of it. Did you mention him earlier for like, not, you know, about computation? Did your

2:15:18.560 --> 2:15:24.240
 worlds cross? They did in a, you know, I know him now, we talked, you know. But let me tell you

2:15:24.240 --> 2:15:31.120
 my Donald Knuth story. So, you know, besides, you know, analysis of algorithms, he's well

2:15:31.120 --> 2:15:36.960
 known for writing tech, which is in latex, which is the academic publishing system.

2:15:36.960 --> 2:15:43.360
 So he did that at the AI lab. And he would do it, he would work overnight at the AI lab.

2:15:44.240 --> 2:15:55.600
 And one, one day, one night, the mainframe computer went down. And a guy named Robert

2:15:55.600 --> 2:16:04.240
 Paul was there. He led his PhD at the Media Lab at MIT. And he was, you know, engineer. And so

2:16:04.240 --> 2:16:09.200
 he and I, you know, tracked down what were the problem was. It was one of this big refrigerator

2:16:09.200 --> 2:16:14.000
 size or washing machine size disk drives had failed. And that's what brought the whole system

2:16:14.000 --> 2:16:20.640
 down. So we got panels pulled off. And we're pulling, you know, circuit cards out. And Donald

2:16:20.640 --> 2:16:25.680
 Knuth, who's a really tall guy, walks in and he's looking down and says, when will it be fixed?

2:16:25.680 --> 2:16:32.320
 Because he wanted to get back to writing his tech system. Well, Donald Knuth. And so we figured

2:16:32.320 --> 2:16:40.000
 out, you know, it was a particular chip, 7400 series chip, which was socketed, we popped it out,

2:16:40.000 --> 2:16:45.040
 we put a replacement in, put it back in, smoke comes out, because we put it in backwards,

2:16:45.040 --> 2:16:50.000
 because we were so nervous that Donald Knuth was standing over us. Anyway, we eventually got

2:16:50.000 --> 2:16:55.200
 it fixed and got the mainframe running again. So that was your little, when was that again?

2:16:55.200 --> 2:16:59.920
 Well, that must have been before October 79, because we moved out of that building then. So

2:16:59.920 --> 2:17:06.800
 sometime, probably 78, sometime or early 79. Yeah, those, all those figures are just fascinating.

2:17:06.800 --> 2:17:14.800
 All the people who've passed through MIT is really fascinating. Is there a, let me ask you to put on

2:17:14.800 --> 2:17:22.080
 your big wise man hat. Is there advice that you can give to young people today, whether in high

2:17:22.080 --> 2:17:29.520
 school or college, who are thinking about their career, or thinking about life? How to live

2:17:29.520 --> 2:17:39.760
 a life they're proud of, a successful life? Yeah, so, so many people ask me for advice and have

2:17:39.760 --> 2:17:45.040
 asked for, and I give, I talk to a lot of people all the time. And there is no one way.

2:17:48.080 --> 2:17:52.640
 You know, there's a lot of pressure to produce papers

2:17:52.640 --> 2:18:03.040
 that will be acceptable and be published. Maybe I was, maybe I come from an age where I

2:18:03.040 --> 2:18:08.320
 would, I could be a rebel against that and still succeed. Maybe it's harder today.

2:18:09.680 --> 2:18:17.280
 But I think it's important not to get too caught up with what everyone else is doing.

2:18:17.280 --> 2:18:26.080
 And if you, well, it depends on what you want of life. If you want to have real impact,

2:18:27.040 --> 2:18:33.440
 you have to be ready to fail a lot of times. So you have to make a lot of unsafe decisions.

2:18:34.240 --> 2:18:39.280
 And the only way to make that work is to make, keep doing it for a long time. And then one of

2:18:39.280 --> 2:18:45.760
 them will be work out. And so that, that will make something successful. Or not. Or not.

2:18:45.760 --> 2:18:49.760
 Yeah. Or you may, or you just may, you know, end up, you know, not having a, you know,

2:18:49.760 --> 2:18:53.440
 having a lousy career. I mean, it's certainly possible. Taking the risk is the thing.

2:18:53.440 --> 2:19:02.000
 Yeah. So, but it, but there's no way to, to make all safe decisions and actually

2:19:03.760 --> 2:19:11.040
 really contribute. Do you think about your death, about your mortality?

2:19:11.040 --> 2:19:17.520
 I got to say, when COVID hit, I did, because we did, you know, in the early days, we didn't

2:19:17.520 --> 2:19:22.800
 know how bad it was going to be. And I, that, that made me work on my book harder for a while.

2:19:22.800 --> 2:19:26.800
 But then I'd started this company and now I'm doing full time, more than full time at the

2:19:26.800 --> 2:19:31.280
 company. So the book's on hold. But I do want to finish this book. When you think about it,

2:19:31.280 --> 2:19:37.200
 are you afraid of it? I'm afraid of dribbling.

2:19:37.200 --> 2:19:42.240
 Yeah. I'm, I'm losing it.

2:19:42.240 --> 2:19:45.600
 The details of, okay. Yeah. Yeah.

2:19:45.600 --> 2:19:47.040
 But the fact that the ride ends.

2:19:49.040 --> 2:19:52.080
 I've known that for a long time. So it's.

2:19:53.280 --> 2:19:58.800
 Yeah. But there's knowing and knowing. It's such a, yeah. And it really sucks.

2:19:58.800 --> 2:20:05.920
 It feels, it feels a lot closer. So my, in, in my, my blog with my predictions, my sort of push

2:20:05.920 --> 2:20:12.320
 back against that was that I said, I'm going to review these every year for 32 years. That puts

2:20:12.320 --> 2:20:18.800
 me into my mid 90s. So, you know, it's my, every, every time you write the blog posts,

2:20:18.800 --> 2:20:23.680
 you're getting closer and closer to your own prediction of your, of your death.

2:20:23.680 --> 2:20:26.320
 Yeah. What do you hope your legacy is?

2:20:28.160 --> 2:20:31.920
 You're one of the greatest roboticist AI researchers of all time.

2:20:31.920 --> 2:20:43.040
 Um, what I hope is that I actually finish writing this book and that there's one person

2:20:43.760 --> 2:20:51.120
 who reads it and sees something about changing the way they're thinking. And that leads to

2:20:52.160 --> 2:21:00.240
 the next big. And then there'll be on a podcast a hundred years from now saying I once read that

2:21:00.240 --> 2:21:07.520
 book and that changed everything. What do you think is the meaning of life?

2:21:08.400 --> 2:21:13.200
 This whole thing, the existence, the, the, the, all the hurried things we do on this

2:21:13.200 --> 2:21:15.440
 planet. What do you think is the meaning of it all?

2:21:15.440 --> 2:21:18.000
 Ah, well, you know, I think we're all really bad at it.

2:21:19.680 --> 2:21:21.440
 Life or finding meaning or both.

2:21:21.440 --> 2:21:27.200
 Yeah. We get caught up in, in the, it's easier to get, easier to do the stuff that's immediate

2:21:27.200 --> 2:21:33.600
 and not through the stuff. It's not immediate. So the big picture or bad. Yeah. Yeah.

2:21:33.600 --> 2:21:38.400
 Do you have a sense of what that big picture is? Like why ever look up to the stars and

2:21:38.400 --> 2:21:40.080
 ask why the hell are we here?

2:21:43.840 --> 2:21:51.920
 You know, my, my, my, my atheism tells me it's just random, but, you know, I want to understand

2:21:51.920 --> 2:21:57.120
 the, the way random in the, in the, that's what I talk about in this book, how order comes from

2:21:57.120 --> 2:22:04.080
 disorder. Yeah. Um, but it kind of sprung up like most of the whole thing is random, but this little

2:22:04.800 --> 2:22:10.240
 pocket of complexity they will call earth that like, why the hell does that happen?

2:22:10.240 --> 2:22:17.360
 And, and what we don't know is how common that those pockets of complexity are or how often,

2:22:17.360 --> 2:22:27.280
 um, because they may not last forever, which is, uh, more exciting slash sad to you if we're alone

2:22:27.280 --> 2:22:34.640
 or if there's infinite number of, oh, I think, I think it's impossible for me to believe that

2:22:34.640 --> 2:22:43.680
 we're alone. Um, that was just too horrible, too cruel. Could be like the sad thing. It could be

2:22:43.680 --> 2:22:49.120
 like a graveyard of intelligent civilizations. Oh, everywhere. Yeah. That might be the most

2:22:49.120 --> 2:22:54.960
 likely outcome. And for us too. Yeah, exactly. Yeah. And all of this will be forgotten. Yeah.

2:22:55.680 --> 2:23:02.400
 Including all the robots you build, everything forgotten. Well, on average,

2:23:03.680 --> 2:23:08.960
 everyone has been forgotten in history. Yeah. Right. Yeah. Most people are not remembered

2:23:08.960 --> 2:23:14.080
 beyond the generational too. Um, I mean, yeah. Well, not just on average,

2:23:14.080 --> 2:23:18.720
 basically very close to a hundred percent of people who've ever lived are forgotten.

2:23:18.720 --> 2:23:24.080
 Yeah. I mean, no long arc of time. I don't know anyone alive who remembers my great grandparents

2:23:24.080 --> 2:23:32.400
 because we didn't meet them. So still this fun, this, uh, this, uh, life is pretty fun somehow.

2:23:32.400 --> 2:23:40.160
 Yeah. Even the immense absurdity and, uh, at times, meaninglessness of it all. It's pretty fun.

2:23:40.160 --> 2:23:45.360
 And one of the, for me, one of the most fun things is robots. And I've looked up to your work. I've

2:23:45.360 --> 2:23:51.920
 looked up to you for a long time. That's right. Rod, it's an honor that you would spend your

2:23:51.920 --> 2:23:55.680
 valuable time with me today talking. It was an amazing conversation. Thank you so much for being

2:23:55.680 --> 2:24:01.040
 here. Well, thanks for, thanks for talking with me. I enjoyed it. Thanks for listening to this

2:24:01.040 --> 2:24:05.680
 conversation with Rodney Brooks. To support this podcast, please check out our sponsors in the

2:24:05.680 --> 2:24:11.600
 description. And now let me leave you with the three laws of robotics from Isaac Asimov.

2:24:12.640 --> 2:24:19.280
 One, a robot may not injure a human being or through inaction allow human being to come to harm.

2:24:20.080 --> 2:24:25.520
 Two, a robot must obey the orders given to it by human beings, except when such orders

2:24:25.520 --> 2:24:32.880
 would conflict with the first law. And three, a robot must protect its own existence as long

2:24:32.880 --> 2:24:39.760
 as such protection does not conflict with the first or the second laws. Thank you for listening.

2:24:39.760 --> 2:24:56.400
 I hope to see you next time.

