WEBVTT

00:00.000 --> 00:04.920
 The following is a conversation with Steve Vaselli, formerly a truck driver and now a

00:04.920 --> 00:10.840
 sociologist at the University of Pennsylvania, who studies freight transportation.

00:10.840 --> 00:16.080
 His first book, The Big Rig, Trucking and the Decline of the American Dream, explains

00:16.080 --> 00:20.800
 how long hauled trucking went from being one of the best blue collar jobs to one of the

00:20.800 --> 00:22.240
 toughest.

00:22.240 --> 00:27.160
 His current ongoing book project, Driverless, Autonomous Trucks and the Future of the American

00:27.160 --> 00:34.520
 Trucker, explores self driving trucks and their potential impacts on labor and on society.

00:34.520 --> 00:36.480
 This is the Lex Friedman Podcast.

00:36.480 --> 00:39.960
 To support it, please check out our sponsors in the description.

00:39.960 --> 00:45.240
 And now, here's my conversation with Steve Vaselli.

00:45.240 --> 00:50.360
 You wrote a book about trucking called The Big Rig Trucking and the Decline of the American

00:50.360 --> 00:56.880
 Dream and you're currently working on a book about autonomous trucking called Driverless,

00:56.880 --> 01:00.680
 Autonomous Trucks and the Future of the American Trucker.

01:00.680 --> 01:03.640
 I have to bring up some Johnny Cash to you because I was just listening to this song.

01:03.640 --> 01:08.920
 He has a ton of songs about trucking, but one of them I was just listening to is called

01:08.920 --> 01:13.600
 All I Do is Drive, where he's talking to an old truck driver.

01:13.600 --> 01:19.480
 It goes, I asked them if those trucking songs tell about a life like his.

01:19.480 --> 01:24.320
 He said, if you want to know the truth about it, here's the way it is.

01:24.320 --> 01:27.320
 All I do is drive, drive, drive.

01:27.320 --> 01:28.320
 Try to stay alive.

01:28.320 --> 01:29.320
 That's the course.

01:29.320 --> 01:32.240
 And keep my mind on my load, keep my eye upon the road.

01:32.240 --> 01:36.480
 I got nothing in common with any man who's home every day at five.

01:36.480 --> 01:42.080
 All I do is drive, drive, drive, drive, drive, drive, drive, drive.

01:42.080 --> 01:45.880
 So I got to ask you, same thing that he asked the trucker.

01:45.880 --> 01:52.600
 You worked as a trucker for six months while working on the previous book.

01:52.600 --> 01:54.960
 What's it like to be a truck driver?

01:54.960 --> 01:58.040
 I think that captures it.

01:58.040 --> 01:59.040
 It really does.

01:59.040 --> 02:04.640
 Can you take me through the whole experience, what it takes to become a trucker, what actual

02:04.640 --> 02:09.640
 day to day life was on day one, week one, and then over time, how that changed?

02:09.640 --> 02:10.640
 Yeah.

02:10.640 --> 02:15.040
 Well, the book is really about how that changed over time.

02:15.040 --> 02:18.080
 So my experience, and I'm an ethnographer, right?

02:18.080 --> 02:20.200
 So I go in.

02:20.200 --> 02:21.200
 I live with people.

02:21.200 --> 02:22.680
 I work with people.

02:22.680 --> 02:23.680
 I talk to them.

02:23.680 --> 02:26.520
 I try to understand their world.

02:26.520 --> 02:29.520
 Ethnographer, by the way, what is that?

02:29.520 --> 02:34.920
 The science and art of capturing the spirit of a people?

02:34.920 --> 02:36.320
 Yeah, life ways.

02:36.320 --> 02:38.720
 I think that would be a good way to capture it.

02:38.720 --> 02:47.800
 Try to understand what makes them unique as a society, maybe as a subculture, right?

02:47.800 --> 02:54.560
 But it makes them tick that might be different than the way you and I are wired.

02:54.560 --> 02:59.320
 And to really sort of thickly describe it would be at least one component of it.

02:59.320 --> 03:01.120
 That's sort of the basic essential.

03:01.120 --> 03:09.560
 And then for me, I want to exercise what C. Wright Mills called the sociological imagination,

03:09.560 --> 03:17.680
 which is to put that individual biography into the long historical sweep of humanity,

03:17.680 --> 03:19.600
 if at all possible.

03:19.600 --> 03:23.200
 My goals are typically more modest than C. Wright Mills's.

03:23.200 --> 03:30.680
 And to then put that biography in the larger social structure to try to understand that

03:30.680 --> 03:36.320
 person's life and the way they see the world, their decisions in light of their interests

03:36.320 --> 03:40.840
 relative to others and conflict and power and all these things that I find interesting.

03:40.840 --> 03:46.200
 In the context of society and in the context of history, and the small tangent, what does

03:46.200 --> 03:55.360
 it take to do that, to capture this particular group, the spirit, the music, the full landscape

03:55.360 --> 04:00.880
 of experiences that a particular group goes through in the context of everything else?

04:00.880 --> 04:05.480
 You only have limited amount of time and you come to the table probably with preconceived

04:05.480 --> 04:08.880
 notions that are then quickly destroyed, all that whole process.

04:08.880 --> 04:14.480
 So I don't know if it's more art or science, but what does it take to be great at this?

04:14.480 --> 04:22.280
 I do think my first book was a success relative to my goals of trying to really get at the

04:22.280 --> 04:28.240
 heart of sort of the central issues and the lives being led by people.

04:28.240 --> 04:35.240
 If I have a resource, a talent, it's that I'm a good listener.

04:35.240 --> 04:40.000
 I can talk with anybody.

04:40.000 --> 04:45.200
 My wife loves to remark on this that I can sort of sit down with anyone.

04:45.200 --> 04:50.720
 I think I learned that from my dad who worked at a factory and actually had a lot of truckers

04:50.720 --> 04:57.280
 go through the gate that he operated and he always had a story, a joke for everybody kind

04:57.280 --> 05:02.960
 of got to know everyone individually and he just taught me that essentially everyone

05:02.960 --> 05:08.200
 has something to teach you and I try to embody that.

05:08.200 --> 05:14.160
 That's the rule for me is every single person I interact with can teach me something.

05:14.160 --> 05:20.160
 I gotta ask you, I'm sorry to interrupt because I'm clearly of the two of us the poorer listener.

05:20.160 --> 05:23.040
 I think you're a great listener.

05:23.040 --> 05:24.040
 Thank you.

05:24.040 --> 05:25.040
 I've been listening to the podcast.

05:25.040 --> 05:26.040
 I think you're a great listener.

05:26.040 --> 05:28.600
 I really appreciate that.

05:28.600 --> 05:33.880
 You've done a large number of interviews, like you said, of truckers for this book.

05:33.880 --> 05:41.280
 I'm just curious, what are some lessons you've learned about what it takes to listen to a

05:41.280 --> 05:48.440
 person enough, maybe guide the conversation enough to get to the core of the person, the

05:48.440 --> 05:54.120
 idea, again, the ethnographer goal to get to the core?

05:54.120 --> 05:55.120
 Yeah.

05:55.120 --> 06:00.560
 I think it doesn't happen in the moment.

06:00.560 --> 06:02.360
 I'm a ruminator.

06:02.360 --> 06:05.760
 I just sit with the data for years.

06:05.760 --> 06:12.640
 I sat with the trucking data for almost 10 full years and just thought about the problems

06:12.640 --> 06:17.400
 and the questions using everything that I possibly could.

06:17.400 --> 06:23.840
 In the moment, my ideal interview is I open up and I say, tell me about your life as a

06:23.840 --> 06:29.600
 trucker and they never shut up and they keep telling me the things that I'm interested

06:29.600 --> 06:30.600
 in.

06:30.600 --> 06:34.760
 Now, it never works out that way because they don't know what you're interested in.

06:34.760 --> 06:41.800
 It's a lot of it is the, as you know, as I think you're a great interviewer, prep.

06:41.800 --> 06:48.560
 You try to get to know a little bit about the person and understand the central questions

06:48.560 --> 06:53.160
 you're interested in that they can help you explore.

06:53.160 --> 06:59.480
 So I've done hundreds of interviews with truck drivers at this point and I should really

06:59.480 --> 07:01.240
 go back and read the original ones.

07:01.240 --> 07:02.240
 They were probably terrible.

07:02.240 --> 07:03.640
 What's the process like?

07:03.640 --> 07:04.640
 You're sitting down.

07:04.640 --> 07:08.080
 Do you have an audio recorder and also taking notes or do you do no audio recording?

07:08.080 --> 07:09.080
 Just notes or?

07:09.080 --> 07:10.080
 Yeah.

07:10.080 --> 07:14.520
 Audio recorder and social scientists always have to struggle with sampling.

07:14.520 --> 07:15.520
 Who do you interview?

07:15.520 --> 07:16.520
 Where do you find them?

07:16.520 --> 07:17.520
 How do you recruit them?

07:17.520 --> 07:23.080
 I just happened to have a sort of natural place to go that gave me essentially the population

07:23.080 --> 07:25.040
 that I was interested in.

07:25.040 --> 07:28.680
 So all these long haul truck drivers that I was interested in, they have to stop and

07:28.680 --> 07:32.080
 get fuel and get services at truck stops.

07:32.080 --> 07:37.520
 So I picked a truck stop at the juncture of a couple major interstates, went into the

07:37.520 --> 07:43.720
 lounge that drivers have to walk through with my clipboard and everybody who came through,

07:43.720 --> 07:48.200
 I said, hey, are you on break?

07:48.200 --> 07:52.080
 That was sort of the first criteria was do you have time, right?

07:52.080 --> 07:57.600
 And if they said yes, I said, I'd say I'm a graduate student at Indiana University,

07:57.600 --> 08:02.040
 I'm doing a study, I'm trying to understand more about truck drivers, will you sit down

08:02.040 --> 08:03.040
 with me?

08:03.040 --> 08:08.960
 And I think the first, I think I probably asked like 104 or 103 people to get the first

08:08.960 --> 08:10.960
 100 interviews.

08:10.960 --> 08:11.960
 It's pretty good odds.

08:11.960 --> 08:12.960
 It's amazing.

08:12.960 --> 08:18.200
 I mean, for any response rate like that, I mean, these are people who sat down and gave

08:18.200 --> 08:23.440
 me an hour or sometimes more of their time just randomly at a truck stop.

08:23.440 --> 08:29.200
 And it just tells you something about like, truckers have something to say, they're alone

08:29.200 --> 08:30.400
 a lot.

08:30.400 --> 08:36.720
 And so I had to figure out how to kind of turn the spigot on, you know, and I got pretty

08:36.720 --> 08:38.640
 good at it, I think.

08:38.640 --> 08:42.160
 So they have good stories to tell and they have an active life of the mind because they

08:42.160 --> 08:45.320
 spend so much time on the road just basically thinking.

08:45.320 --> 08:46.320
 Yeah.

08:46.320 --> 08:53.160
 There's a lot of reflection, a lot of struggles, you know, and it's, they take different forms,

08:53.160 --> 08:57.120
 you know, one of the things that they talk about is the impact on their families.

08:57.120 --> 09:00.680
 They say truckers have the same rate of divorce as everybody else.

09:00.680 --> 09:04.520
 And that's because trucking saves so many marriages because you're not around and ruins

09:04.520 --> 09:05.520
 so many.

09:05.520 --> 09:09.080
 And so it ends up being a wash.

09:09.080 --> 09:15.040
 So, you know, I had this experience, I met another person and he recognized me from a

09:15.040 --> 09:21.520
 podcast and he said, you know, I'm a fan of yours and a fan of Joe Rogan, but you guys

09:21.520 --> 09:26.160
 never talk, you always talk to people with Nobel Prizes, you always talk to these kind

09:26.160 --> 09:31.440
 of people, you never talk to us regular folk.

09:31.440 --> 09:33.720
 And that guy really stuck with me.

09:33.720 --> 09:36.600
 First of all, the idea of regular folk is a silly notion.

09:36.600 --> 09:41.760
 I think people that win Nobel Prizes are often more boring than the people, these regular

09:41.760 --> 09:46.960
 folks in terms of stories, in terms of richness of experience, in terms of the ups and downs

09:46.960 --> 09:48.320
 of life.

09:48.320 --> 09:54.160
 And you know, that really stuck with me because I said that as a goal for myself to make sure

09:54.160 --> 09:58.280
 I talked to regular folk.

09:58.280 --> 10:06.520
 And you did just this talking, again, regular folk, it's human beings, all of them have

10:06.520 --> 10:08.800
 experiences.

10:08.800 --> 10:15.760
 If you were to recommend to talk to, to talk to some of these folks with stories, how would

10:15.760 --> 10:16.760
 you find them?

10:16.760 --> 10:21.080
 Yeah, so I do do this sometimes for journalists who, you know, will come and they want to

10:21.080 --> 10:25.160
 write about sort of what's happening right now in trucking, you know.

10:25.160 --> 10:29.400
 And I send them to truck stops, you know, I say, you know, yeah, there's a town called

10:29.400 --> 10:31.640
 Effingham, Illinois.

10:31.640 --> 10:37.040
 It's just this place where, you know, a bunch of huge truck stops, tons of trucks and really

10:37.040 --> 10:42.120
 nothing else out there, you know, it's in the middle of corn country.

10:42.120 --> 10:46.840
 And you know, again, truckers in this, you know, sadly, I think, you know, the politics

10:46.840 --> 10:50.280
 of the day, it's changing a little bit.

10:50.280 --> 10:56.120
 I think there's a little, the polarization is getting to the trucking industry in ways

10:56.120 --> 11:02.560
 that, you know, maybe we're seeing in other parts of our social world.

11:02.560 --> 11:07.880
 But truckers are generally, you know, real open, sort of friendly folks.

11:07.880 --> 11:11.560
 Some of them ultimately like to work alone and be alone.

11:11.560 --> 11:15.520
 That's a relatively small subset, I think.

11:15.520 --> 11:20.600
 But all of them are generally, you know, kind of open, you know, trusting, willing to have

11:20.600 --> 11:21.600
 a conversation.

11:21.600 --> 11:26.720
 And so, you know, you go to the truck stop and you go in the lounge and there's usually

11:26.720 --> 11:31.160
 a booth down there and somebody's sitting at their laptop around their phone and willing

11:31.160 --> 11:32.160
 to strike up a conversation.

11:32.160 --> 11:33.160
 You should try that.

11:33.160 --> 11:34.160
 You should, you know.

11:34.160 --> 11:36.600
 That 100% will try this.

11:36.600 --> 11:40.840
 Just again, we're just going from tangent to tangent, we'll return to the main question,

11:40.840 --> 11:43.280
 but what do they listen to?

11:43.280 --> 11:45.760
 Do they listen to talk radio?

11:45.760 --> 11:47.760
 Do they listen to podcasts, audio books?

11:47.760 --> 11:48.840
 Do they listen to music?

11:48.840 --> 11:51.240
 Do they listen to silence?

11:51.240 --> 11:52.240
 Everything.

11:52.240 --> 11:53.240
 Everything.

11:53.240 --> 11:58.080
 Some, I mean, and some still listen to the CB, which, you know, it's an ever dwindling

11:58.080 --> 11:59.080
 group.

11:59.080 --> 12:02.320
 They'll call it the original internet citizens band.

12:02.320 --> 12:07.680
 You know, they, back in the 70s, they thought it was going to be the medium of democracy.

12:07.680 --> 12:12.760
 And they love to just get on there and, you know, cruise along one truck after the other

12:12.760 --> 12:13.760
 and chat away.

12:13.760 --> 12:17.520
 Usually, you know, it's guys who know each other from the same company or happen to run

12:17.520 --> 12:22.480
 into each other, but other than that, it's everything under the sun, you know.

12:22.480 --> 12:26.720
 And that's, it's probably one of the stereotypes and it's, I think it was more true in the

12:26.720 --> 12:32.400
 past, you know, about the sort of heterogeneity of truck drivers.

12:32.400 --> 12:37.680
 They're a really diverse group now, you know, there's definitely a large, still a large

12:37.680 --> 12:44.840
 component of rural white guys who work in the industry, but there's a huge growing chunk

12:44.840 --> 12:51.320
 of the industry that's immigrants, people of color and even some women, still huge barriers

12:51.320 --> 12:57.160
 to women entering it, but it's a much more diverse place than most people think.

12:57.160 --> 13:00.600
 So let's return to your journey as a truck driver.

13:00.600 --> 13:03.680
 What did it take to become a truck driver?

13:03.680 --> 13:04.960
 What were the early days like?

13:04.960 --> 13:05.960
 Yeah.

13:05.960 --> 13:10.080
 So this is, I mean, this is a central part of the story, right, that I uncovered.

13:10.080 --> 13:14.400
 And the good part was that I went in without knowing what was going to happen.

13:14.400 --> 13:20.360
 So I was able to experience it as a new truck driver would is one of the important stories

13:20.360 --> 13:27.320
 in the book is how that experience is constructed by employers to sort of, you know, help you

13:27.320 --> 13:30.680
 think the way that they would like you to think about the job and about the industry

13:30.680 --> 13:34.360
 and about the social relations of it.

13:34.360 --> 13:37.280
 It's super intimidating.

13:37.280 --> 13:41.960
 I say in the book, you know, pretty handy guy, you know, familiar with tools, machines,

13:41.960 --> 13:47.480
 like, you know, comfortable operating stuff, like from time I was a kid, the truck was

13:47.480 --> 13:49.960
 just like a whole nother experience.

13:49.960 --> 13:55.560
 I mean, as I think most people think about it, it's this big, huge vehicle, right?

13:55.560 --> 14:01.320
 It's really long, it's 70 feet long, it can weigh 80,000 pounds, you know, it does not

14:01.320 --> 14:06.160
 stop like a car, it does not turn like a car.

14:06.160 --> 14:13.120
 But at least when I started, and this is changed as part of the technology story of trucking,

14:13.120 --> 14:16.320
 the first thing you had to do was learn how to shift it.

14:16.320 --> 14:21.120
 And it doesn't shift like a manual car, the clutch isn't synchronized.

14:21.120 --> 14:23.600
 So you have to do what's called double clutch.

14:23.600 --> 14:29.600
 And it's basically the foundational skill that a truck driver used to have to learn.

14:29.600 --> 14:35.240
 So you would, you know, accelerate, say you're in first gear, you push in the clutch, you

14:35.240 --> 14:41.400
 pull the shifter out of first gear, you let the clutch out, and then you let the RPMs

14:41.400 --> 14:47.040
 of the engine drop an exact amount, then you push the clutch back in and you put it in

14:47.040 --> 14:48.400
 second gear.

14:48.400 --> 14:52.560
 If your timing is off, those gears aren't going to go together.

14:52.560 --> 14:57.160
 And so if you're in an intersection, you're just going to get this horrible grinding sound

14:57.160 --> 15:02.080
 as you coast, you know, to a dead stop in the, you know, underneath the stoplight or

15:02.080 --> 15:03.280
 whatever it is.

15:03.280 --> 15:06.360
 So the first thing you have to do is learn to shift it.

15:06.360 --> 15:12.280
 And so at least for me and a lot of drivers who are going to private companies, CDL schools,

15:12.280 --> 15:14.680
 what happens is it's kind of like a boot camp.

15:14.680 --> 15:20.040
 They ship me three states away from home, send you a bus ticket and say, hey, we'll put

15:20.040 --> 15:21.680
 you up for two weeks.

15:21.680 --> 15:26.480
 You sit in a classroom, you sort of learn the theory of shifting the, you know, theory

15:26.480 --> 15:31.760
 of kind of how you fill out your logbook, rules of the road, you know, you do that maybe

15:31.760 --> 15:36.080
 half the day and then the other half you're in this giant parking lot with one of these

15:36.080 --> 15:41.240
 old trucks and just like, you know, destroy in what's left of the thing.

15:41.240 --> 15:45.880
 And it's lurching and belching smoke and just making horrible noises and like rattling.

15:45.880 --> 15:48.560
 I mean, in these things, like there's a lot of torque.

15:48.560 --> 15:52.920
 And so if you do manage to get it into gear, but the engine's lugging, I mean, it can throw

15:52.920 --> 15:54.560
 you right out of the seat, right?

15:54.560 --> 16:00.640
 So it's this, it's like, you know, this bull you're trying to ride and it's super intimidating.

16:00.640 --> 16:05.800
 And the thing about it is that for everybody there, it's almost everybody there.

16:05.800 --> 16:07.800
 It's super high stakes.

16:07.800 --> 16:12.920
 So trucking has become a job of last resort for a lot of people.

16:12.920 --> 16:17.720
 And so they, you know, they lose a job in manufacturing.

16:17.720 --> 16:20.640
 They get too old to do construction any longer, right?

16:20.640 --> 16:22.800
 The knees can no longer handle it.

16:22.800 --> 16:25.240
 They get replaced by a machine.

16:25.240 --> 16:30.000
 Their job gets, you know, offshored and they end up going to trucking because it's a place

16:30.000 --> 16:31.720
 where they can maintain their income.

16:31.720 --> 16:34.640
 And so it's super high stress.

16:34.640 --> 16:38.280
 Like they've left their family behind, maybe they quit another job.

16:38.280 --> 16:40.280
 They're typically being charged a lot of money.

16:40.280 --> 16:45.240
 So that first couple of weeks, like you might get charged $8,000 by the company that you

16:45.240 --> 16:47.920
 have to pay back if you don't get hired.

16:47.920 --> 16:52.640
 And so the stakes are high and this machine is huge and it's intimidating.

16:52.640 --> 16:53.840
 And so it's super stressful.

16:53.840 --> 16:58.720
 I mean, I watched, you know, men, grown men break down crying about like how they couldn't

16:58.720 --> 17:02.960
 go home and tell their son that they had been telling they were going to, you know, go become

17:02.960 --> 17:05.320
 a long haul truck driver that they'd failed.

17:05.320 --> 17:08.400
 And it's kind of this super high stress system.

17:08.400 --> 17:12.480
 It's designed that way partly because as one of my trainers later told me, it's basically

17:12.480 --> 17:14.560
 a two week job interview.

17:14.560 --> 17:15.560
 Like they're testing you.

17:15.560 --> 17:19.680
 They're seeing like, you know, how's this person going to respond when it's tough, you

17:19.680 --> 17:23.720
 know, when they have to do the right thing and it's slow and, you know, they need to

17:23.720 --> 17:24.720
 learn something.

17:24.720 --> 17:26.760
 Are they going to rush, you know?

17:26.760 --> 17:31.680
 Or are they going to kind of stay calm, figure it out, you know, nose to the grindstone?

17:31.680 --> 17:35.040
 Because when you're in a truck driver, you're unsupervised, you know, and that's what they're

17:35.040 --> 17:40.240
 really looking for is that kind of quality of conscientious work that's going to carry

17:40.240 --> 17:41.240
 through to the job.

17:41.240 --> 17:44.760
 Well, so the truck is such an imposing part of a traffic scenario.

17:44.760 --> 17:45.760
 Yeah.

17:45.760 --> 17:50.080
 So you said like, like turning, it stresses me out every time I look at a truck because

17:50.080 --> 17:54.120
 they, I mean, the geometry of the problem is so tricky.

17:54.120 --> 17:57.760
 And so if you combine the fact that they have to, like everybody, basically all the cars

17:57.760 --> 18:02.400
 in the scene are staring at the truck and they're waiting, often in frustration.

18:02.400 --> 18:08.680
 And in that mode, you have to then shift gears perfectly and move perfectly.

18:08.680 --> 18:13.200
 And if when you're new especially, like you'll probably, for somebody like me, it feels like

18:13.200 --> 18:18.400
 it would take years to become calm and comfortable in that situation as opposed to be exceptionally

18:18.400 --> 18:24.440
 stressed under the eyes of the road, everybody looking, you're waiting for you.

18:24.440 --> 18:27.360
 Is that the psychological pressure of that?

18:27.360 --> 18:29.040
 Is that something that was really difficult?

18:29.040 --> 18:30.040
 Yeah, absolutely.

18:30.040 --> 18:35.360
 Again, just, I saw people freeze up, you know, in that intersection as, you know, horns

18:35.360 --> 18:39.920
 are blaring and the truck's grinding, you know, gears and you just can't, you know,

18:39.920 --> 18:40.920
 and they just shut down.

18:40.920 --> 18:41.920
 They're like, this isn't for me.

18:41.920 --> 18:42.920
 I can't do it.

18:42.920 --> 18:43.920
 You're right.

18:43.920 --> 18:45.420
 It takes years.

18:45.420 --> 18:51.360
 If, you know, trucking is not considered a skilled occupation, but, you know, my six

18:51.360 --> 18:56.760
 months there, and I was a pretty good rookie, but when I finished, I was still a rookie.

18:56.760 --> 19:04.080
 Even shifting, definitely backing tight corners and situations, you know, I could drive competently,

19:04.080 --> 19:08.920
 but the difference between me and someone who had, you know, two, three years of experience

19:08.920 --> 19:16.880
 was, it was a giant gulf between us and between that and the really skilled drivers who've

19:16.880 --> 19:21.280
 been doing it for 20 years, you know, is still another step beyond that.

19:21.280 --> 19:22.800
 So it is highly skilled.

19:22.800 --> 19:28.560
 Would it be fair to break trucking into the task of truck, of driving a truck to two categories?

19:28.560 --> 19:33.080
 One is like the local stuff, getting out of the parking lot, getting into, getting into,

19:33.080 --> 19:39.160
 you know, driving down local streets and then highway driving those two, those two tasks.

19:39.160 --> 19:41.680
 What are the challenges associated with each task?

19:41.680 --> 19:44.160
 You kind of emphasized the first one.

19:44.160 --> 19:47.800
 What about the actual like long haul highway driving?

19:47.800 --> 19:48.800
 Yeah.

19:48.800 --> 19:52.040
 So, I mean, and they are very different, right?

19:52.040 --> 19:59.920
 And the key with the long haul driving is really a set of the way I came to understand

19:59.920 --> 20:03.400
 it was a set of habits, right?

20:03.400 --> 20:09.160
 We have a sense of driving, particularly men, I think, have a sense of driving as like being

20:09.160 --> 20:14.320
 really skilled is like the goal and you can kind of maneuver yourself out of in and out

20:14.320 --> 20:20.360
 of tight spaces with great speed and breaking and acceleration, you know.

20:20.360 --> 20:26.800
 For a really good truck driver, it's about understanding traffic and traffic patterns

20:26.800 --> 20:30.640
 and making good decisions so you never have to use those skills.

20:30.640 --> 20:37.640
 And the really good drivers, you know, the mantra is always leave yourself and out, right?

20:37.640 --> 20:42.880
 So always have that safe place that you can put that truck in case that four wheeler in

20:42.880 --> 20:49.440
 front of you who's texting loses control, you know, what are you going to do in that

20:49.440 --> 20:50.440
 situation?

20:50.440 --> 20:57.080
 And what really good truck drivers do on the highway is they just keep themselves out of

20:57.080 --> 20:59.560
 those situations entirely.

20:59.560 --> 21:04.560
 They see it, they slow down, they, you know, they avoid it.

21:04.560 --> 21:10.640
 And then the local driving is really something that takes just practice and routine to learn.

21:10.640 --> 21:15.760
 You know, this quarter turn, it feels like the back of the truck sometimes is on delay

21:15.760 --> 21:16.760
 when you're backing it up.

21:16.760 --> 21:20.840
 So it's like, all right, I'm going to do a quarter turn of the wheel now to get the

21:20.840 --> 21:25.720
 effect that I want like five seconds from now in where that tail of that trailer is

21:25.720 --> 21:26.960
 going to be.

21:26.960 --> 21:31.440
 And there's just no, I mean, some people have a natural talent for that, you know, spatial

21:31.440 --> 21:35.280
 visualization and kind of calculating those angles and everything.

21:35.280 --> 21:40.440
 But there's really no escaping the fact that you've got to just do it over and over again

21:40.440 --> 21:43.080
 before you're going to learn how to do it well.

21:43.080 --> 21:47.920
 Do you mind sharing how much you were getting paid, how much you were making as a truck

21:47.920 --> 21:50.360
 driver in your time as a truck driver?

21:50.360 --> 21:54.600
 Yeah, I started out at 25 cents a mile.

21:54.600 --> 21:56.920
 And then I got bumped up to 26 cents a mile.

21:56.920 --> 22:04.560
 So we had a minimum pay, which was sort of a new pay scheme that the industry had started

22:04.560 --> 22:10.240
 to introduce to, you know, because there's lots of unpaid work and time.

22:10.240 --> 22:14.840
 And so we had a minimum pay of $500 a week that you would get if you didn't drive enough

22:14.840 --> 22:17.400
 miles to exceed that.

22:17.400 --> 22:24.440
 You get paid when you turn the bills in, which is the paperwork that goes with the load.

22:24.440 --> 22:30.440
 So you have to get that back to your company and then that's how they bill the customer.

22:30.440 --> 22:34.560
 And so you might get a bunch of those bills that kind of bunch up in one week.

22:34.560 --> 22:38.360
 So, you know, I might get a paycheck for $1,200.

22:38.360 --> 22:44.280
 And I mean, I was a poor graduate student, so this was real, real money to me.

22:44.280 --> 22:51.600
 And so I had this sort of natural incentive to earn a lot or to maximize my pay.

22:51.600 --> 22:54.760
 Some weeks were that minimum, $500, very few.

22:54.760 --> 22:58.400
 And then some I'd get $1,200, $1,300.

22:58.400 --> 23:03.560
 Pay has gone up, you know, typical drivers now starting in the 30s, you know, in the

23:03.560 --> 23:05.040
 kind of job that I was in.

23:05.040 --> 23:09.080
 30s, you know, cents per mile, 30 to 35.

23:09.080 --> 23:15.200
 So can we try to reverse engineer that math, how that maps to the actual hours?

23:15.200 --> 23:19.720
 The hours connected to driving are so widely dispersed, as you said.

23:19.720 --> 23:22.960
 Some of them don't count as actual work, some of it does.

23:22.960 --> 23:26.280
 That's a very interesting discussion that we'll then continue when we start talking

23:26.280 --> 23:28.240
 about autonomous trucking.

23:28.240 --> 23:33.800
 But you know, you're saying all these cents per mile kind of thing.

23:33.800 --> 23:37.680
 How does that map to like average hourly wage?

23:37.680 --> 23:43.440
 Yeah, so I mean, and this is kind of the, this is also an interesting technology story

23:43.440 --> 23:44.440
 in the end.

23:44.440 --> 23:47.080
 And it's the technology story that didn't happen.

23:47.080 --> 23:52.520
 So pay per mile was, you know, invented by companies when you couldn't surveil drivers.

23:52.520 --> 23:53.720
 You didn't know what they were doing, right?

23:53.720 --> 23:56.200
 And you wanted them to have some skin in the game.

23:56.200 --> 24:01.040
 And so you'd say, you know, here's the load, it's going from, you know, for me, I might

24:01.040 --> 24:06.400
 start in, you know, the Northeast, maybe in upstate New York with a load of beer and say,

24:06.400 --> 24:09.960
 here's this load of beer, bring it to this address in Michigan, we're going to pay you

24:09.960 --> 24:10.960
 by the mile, right?

24:10.960 --> 24:15.480
 If you're always being paid by the hour, I might just pull over at the diner and have

24:15.480 --> 24:16.560
 breakfast.

24:16.560 --> 24:25.560
 So you're paid by the mile, but increasingly over time, the typical driver is spending

24:25.560 --> 24:28.160
 more and more time doing non driving tasks.

24:28.160 --> 24:30.480
 There's lots of reasons for that.

24:30.480 --> 24:34.800
 One of which is railroads have captured a lot of freight that goes long distances now.

24:34.800 --> 24:37.520
 Another one is traffic congestion.

24:37.520 --> 24:39.800
 And the other one is that drivers are pretty cheap.

24:39.800 --> 24:44.560
 And they're almost always the low people on the totem pole in some segments.

24:44.560 --> 24:48.600
 And so their time is used really inefficiently.

24:48.600 --> 24:55.200
 So I might go to that brewery to pick up that load of Bud Light.

24:55.200 --> 25:01.000
 And their dock staff may be busy loading up five other trucks.

25:01.000 --> 25:04.720
 And they'll say, you know, go over there and sit and wait, we'll call you on the CB when

25:04.720 --> 25:05.920
 the dock's ready.

25:05.920 --> 25:10.240
 So you wait there a couple hours, they bring you in, you know, you never know what's happening

25:10.240 --> 25:11.240
 in the truck.

25:11.240 --> 25:14.960
 Sometimes they're loading it with a forklift, maybe they're throwing 14 pallets on there

25:14.960 --> 25:16.360
 full of kegs.

25:16.360 --> 25:21.080
 But sometimes it'll take them hours, you know, and you're sitting in that truck and you're

25:21.080 --> 25:27.240
 essentially unpaid, you know, then you pull out, you've got control over what you're going

25:27.240 --> 25:29.960
 to get paid based on how you drive that load.

25:29.960 --> 25:33.840
 And then on the other end, you got a similar situation of kind of waiting.

25:33.840 --> 25:40.440
 So if that's the way truck drivers are paid, then there's a low incentive for the optimization

25:40.440 --> 25:47.320
 of the supply chain to make them more efficient, right, to utilize truck labor more efficiently.

25:47.320 --> 25:48.520
 Absolutely.

25:48.520 --> 25:54.160
 So that's the technology problem that one of several technology problems that could

25:54.160 --> 25:55.160
 be addressed.

25:55.160 --> 26:03.800
 I mean, what, so what did, if we just linger on it, what are we talking about in terms

26:03.800 --> 26:06.800
 of dollars per hour?

26:06.800 --> 26:08.320
 Is it close to minimum wage?

26:08.320 --> 26:16.280
 Is it, you know, there's something you talk about, there was a conception or a misconception

26:16.280 --> 26:19.880
 that truckers get paid a lot for their work.

26:19.880 --> 26:22.000
 Do they get paid a lot for their work?

26:22.000 --> 26:23.560
 Some do.

26:23.560 --> 26:26.080
 And I think that's part of the complexity.

26:26.080 --> 26:31.160
 So you know, what interested me as an ethnographer about this was, you know, I'm interested in

26:31.160 --> 26:36.400
 the kind of economic conceptions that people have in their heads and how they lead to certain

26:36.400 --> 26:41.240
 decisions in labor markets, you know, why some people become an entrepreneur and other

26:41.240 --> 26:45.960
 people become a wage laborer or, you know, why some people want to be doctors and other

26:45.960 --> 26:48.000
 people want to be truck drivers.

26:48.000 --> 26:54.160
 That conception is getting shaped in these labor markets as the argument of the book.

26:54.160 --> 27:00.120
 And the fact that drivers can hear or potential drivers can hear about these, you know, workers

27:00.120 --> 27:04.560
 who make $100,000 plus, which happens regularly in the trucking industry.

27:04.560 --> 27:11.240
 There are many truck drivers who make more than $100,000 a year, you know, is an attraction.

27:11.240 --> 27:14.160
 But the industry is highly segmented.

27:14.160 --> 27:19.720
 And so the entry level segment, and we can probably get into this, but, you know, the

27:19.720 --> 27:25.760
 industry is dominated by, you know, a few dozen really large companies that are self

27:25.760 --> 27:28.720
 insured and can train new drivers.

27:28.720 --> 27:33.600
 So if you want those good jobs, you've got to have several years up until recently,

27:33.600 --> 27:37.160
 now the labor market's becoming tighter, but you had to have several years of accident

27:37.160 --> 27:42.800
 free, you know, perfectly clean record driving to get into them.

27:42.800 --> 27:47.960
 The other part of the segment, you know, those drivers often don't make minimum wage.

27:47.960 --> 27:52.480
 But this leads to one of the sort of central issues that has been in the courts and in

27:52.480 --> 27:59.640
 the legislature in some states, is, you know, what should truck drivers get paid for, right?

27:59.640 --> 28:04.960
 The industry, you know, for the last 30 years or so has said, essentially, it's the hours

28:04.960 --> 28:10.600
 that they log for safety reasons for the Department of Transportation, right?

28:10.600 --> 28:16.840
 Now, since the drivers are paid by the mile, they try to minimize those because those hours

28:16.840 --> 28:19.120
 are limited by the federal government.

28:19.120 --> 28:23.000
 So the federal government says you can't drive more than 60 hours in a week as a long

28:23.000 --> 28:24.560
 haul truck driver.

28:24.560 --> 28:27.960
 And so you want to drive as many miles as you can in those 60 hours.

28:27.960 --> 28:31.960
 And so you under report them, right?

28:31.960 --> 28:36.560
 And so what happens is the companies say, well, that guy, you know, he only said he

28:36.560 --> 28:42.280
 logged 45 hours of work that week or 50 hours of work, that's all we have to pay him minimum

28:42.280 --> 28:43.800
 wage for.

28:43.800 --> 28:48.440
 When in fact, typical truck driver in these jobs will work, according to most people would

28:48.440 --> 28:51.720
 sort of define it as like, okay, I'm at the customer location, I'm waiting below and doing

28:51.720 --> 28:57.040
 some paperwork, you know, inspecting the truck, I'm feeling it, just waiting to, you know,

28:57.040 --> 29:02.360
 get put in the dock, 80 to 90 hours would be sort of a typical work week for one of

29:02.360 --> 29:04.800
 these drivers.

29:04.800 --> 29:07.680
 And just when you look at that, does they don't make minimum wage oftentimes?

29:07.680 --> 29:08.680
 Right.

29:08.680 --> 29:12.560
 Just to be clear, what we're dancing around here is that a little bit over, a little bit

29:12.560 --> 29:18.160
 under minimum wage is nevertheless most truck drivers seem to be making close to minimum

29:18.160 --> 29:19.160
 wage.

29:19.160 --> 29:24.160
 Like this is the, so like we maybe haven't made that clear.

29:24.160 --> 29:31.320
 There's a few that make quite a bit of money, but like you're as an entry and for years,

29:31.320 --> 29:37.720
 you're operating essentially minimum wage and potentially far less than minimum wage

29:37.720 --> 29:43.280
 if you actually count the number of hours that are taken out of your life due to your

29:43.280 --> 29:45.920
 dedication to trucking.

29:45.920 --> 29:50.720
 Well if you count like the hours taken out of your life, then you got to go, you know,

29:50.720 --> 29:52.920
 maybe a full 24.

29:52.920 --> 29:53.920
 That's right.

29:53.920 --> 29:54.920
 Yeah.

29:54.920 --> 29:59.200
 From family, from the high quality of life parts of your life.

29:59.200 --> 30:00.200
 Yeah.

30:00.200 --> 30:05.040
 And there's a whole nother set of rules that the Department of Labor has, which basically

30:05.040 --> 30:10.200
 say that a truck driver who's dispatched away from home for more than a day should get

30:10.200 --> 30:13.880
 minimum wage 24 hours a day.

30:13.880 --> 30:16.680
 And that could be a state minimum wage.

30:16.680 --> 30:21.880
 But typically what it would work out to for most drivers is that, you know, the minimum

30:21.880 --> 30:27.760
 wage for a truck driver should be 50s of thousands, you know, 55, $60,000 should be the minimum

30:27.760 --> 30:31.560
 wage of a truck driver, and you've probably heard about the truck driver shortage, like

30:31.560 --> 30:38.200
 if, you know, which I hope we can talk about, if the minimum wage for truck drivers is as

30:38.200 --> 30:43.080
 it should be on the books at, you know, around $60,000, we wouldn't have a shortage of truck

30:43.080 --> 30:44.080
 drivers.

30:44.080 --> 30:46.440
 Oh, wow.

30:46.440 --> 30:54.120
 And to me, 60,000 is not a lot of money for this kind of job because you're, this isn't,

30:54.120 --> 30:59.840
 this is essentially two jobs and two jobs where you don't get to sleep with your wife

30:59.840 --> 31:03.680
 or see your kids at night.

31:03.680 --> 31:06.600
 That's 60,000 is a very little money for that.

31:06.600 --> 31:11.480
 But you're saying if it was 60,000, you wouldn't even have the shortage.

31:11.480 --> 31:12.680
 If that was the minimum.

31:12.680 --> 31:13.680
 If that was the minimum.

31:13.680 --> 31:17.400
 And I think that's what, now we have drivers who start in the 30s.

31:17.400 --> 31:18.400
 Wow.

31:18.400 --> 31:19.400
 But yeah.

31:19.400 --> 31:23.440
 And I mean, so we're talking two, three jobs really, when you look at the total hours that

31:23.440 --> 31:26.800
 people are working at, you know, they can work over a hundred.

31:26.800 --> 31:30.880
 If they're a trainer, you know, training other truck drivers well over a hundred hours

31:30.880 --> 31:31.880
 a week.

31:31.880 --> 31:37.640
 So a job of last resort, maybe you can jump around from tangent to tangent.

31:37.640 --> 31:41.560
 This is such a fascinating and difficult topic.

31:41.560 --> 31:46.560
 I heard that there's a shortage of truck drivers.

31:46.560 --> 31:49.880
 So there's more jobs than truck drivers willing to take on the job.

31:49.880 --> 31:52.680
 Is that the state of affairs currently?

31:52.680 --> 31:58.160
 I mean, I think the way that you just put that is right.

31:58.160 --> 32:03.000
 We don't have a shortage of people who are currently licensed to do the jobs.

32:03.000 --> 32:06.920
 So I'm working on a project for the state of California to look at the shortage of agricultural

32:06.920 --> 32:07.920
 drivers.

32:07.920 --> 32:13.760
 And the first thing that the DMV commissioner of the state wanted to look at was, you know,

32:13.760 --> 32:15.560
 is there actually a shortage of licensed drivers?

32:15.560 --> 32:19.680
 He's like, I've got a database here of all the people who have a commercial driver's

32:19.680 --> 32:24.320
 license who could potentially have the credential to do this.

32:24.320 --> 32:31.680
 There are about 145,000 jobs in California that require of a Class A CDL, which would

32:31.680 --> 32:37.960
 be that commercial driver's license that you need for the big trucks, about 145,000 jobs.

32:37.960 --> 32:43.800
 The industry in their, you know, regular promotion of the idea that there's a shortage

32:43.800 --> 32:48.640
 is always projecting forward and says, you know, we're going to need 165,000 or so in

32:48.640 --> 32:50.640
 the next 10 years.

32:50.640 --> 32:56.200
 They're currently like 435,000 people licensed in the state of California to drive one of

32:56.200 --> 32:57.960
 these big trucks.

32:57.960 --> 33:03.640
 So it is not at all an absence of people who, I mean, and again, going back to what we were

33:03.640 --> 33:08.760
 talking about before, getting that license is not something that you just walk down to

33:08.760 --> 33:10.560
 the DMV and take the test.

33:10.560 --> 33:17.480
 Like, this is somebody who probably quit another job, was unemployed and took months

33:17.480 --> 33:22.480
 to go to a training school, right, paid for that training school oftentimes, left their

33:22.480 --> 33:28.120
 family for months, right, invested in what they thought was going to be a long term career

33:28.120 --> 33:33.680
 and then said, you know what, forget it, I can't, I can't do it, you know.

33:33.680 --> 33:38.000
 So yeah, so it's not just skill, it's like they were psychologically invested potentially

33:38.000 --> 33:43.520
 for months, if not years into this kind of position as perhaps a position that if they

33:43.520 --> 33:45.800
 lose their current job, they could fall too.

33:45.800 --> 33:50.560
 Okay, so that's an indication that there's something deeply wrong with the job.

33:50.560 --> 33:55.240
 If so many licensed people are not willing to take it, what are the biggest problems

33:55.240 --> 33:58.680
 of the job of truck driver currently?

33:58.680 --> 34:04.200
 Yeah, the job, the problems with the job and the labor market, right, but let's start

34:04.200 --> 34:10.920
 with the job, which is, you know, again, just so much time that's not compensated directly

34:10.920 --> 34:17.200
 for the amount of time, and that's just psychologically, and this was a big part of what I, you know,

34:17.200 --> 34:23.800
 I studied for the first book was, you know, that conception of like, what's my time worth,

34:23.800 --> 34:31.840
 right, and like, what truck drivers love is oftentimes is that tangible outcome based

34:31.840 --> 34:33.080
 compensation.

34:33.080 --> 34:38.400
 So they say, you know what, you know, honest days work, I work hard, I get paid for what

34:38.400 --> 34:43.000
 I do, I drive 500 miles today, that's what I'm going to get paid for, and then you get

34:43.000 --> 34:44.200
 to that dock.

34:44.200 --> 34:49.840
 And they tell you, sorry, the load's not ready, go sit over there, and you stew.

34:49.840 --> 34:56.800
 And that weight can break you psychologically because your time every second becomes more

34:56.800 --> 34:57.800
 worthless.

34:57.800 --> 34:58.800
 Yeah.

34:58.800 --> 34:59.800
 Or worthless.

34:59.800 --> 35:05.360
 Yeah, and again, the industry is going to say, for instance, okay, well, you know, they've

35:05.360 --> 35:08.880
 got skin in the game, right, that argument about sort of compensation based on sort of

35:08.880 --> 35:13.080
 output, right, but that's a holdover from when you couldn't observe truckers.

35:13.080 --> 35:17.320
 Now they all have, you know, satellite linked computers in the trucks that tell these large

35:17.320 --> 35:22.840
 companies, this driver was, you know, at this GPS location for four and a half hours, right.

35:22.840 --> 35:27.400
 So if you wanted to compensate them for that time directly, and the trucker can't control

35:27.400 --> 35:31.480
 what's happening on that customer location, you know, they're waiting for that, you know,

35:31.480 --> 35:35.240
 confirmed that customer to tell them, hey, pull in there.

35:35.240 --> 35:41.080
 And so what it becomes is just a way to shift the inefficiencies and the cost of that onto

35:41.080 --> 35:44.040
 that driver.

35:44.040 --> 35:45.720
 It's competitive for customers.

35:45.720 --> 35:50.520
 So if you're Walmart, you might have your choice of a dozen different trucking companies

35:50.520 --> 35:52.240
 that could move your stuff.

35:52.240 --> 35:56.480
 And if one of them tells you, hey, you're not moving our trucks in and out of your docks

35:56.480 --> 36:01.300
 fast enough, we're going to charge you for how long our truck is sitting on your lot.

36:01.300 --> 36:05.160
 If you're Walmart, you're going to say, I'll go see what the other guy says, right.

36:05.160 --> 36:11.400
 And so companies are going to allow that customer to essentially waste that driver's time, you

36:11.400 --> 36:14.840
 know, in order to keep that business.

36:14.840 --> 36:18.520
 Can you try to describe the economics, the labor market of the situation?

36:18.520 --> 36:21.280
 You mentioned freight and railroad.

36:21.280 --> 36:31.280
 What is the sort of the dynamic financials, the economics of this that allow for such

36:31.280 --> 36:36.480
 low salaries to be paid to truckers?

36:36.480 --> 36:37.480
 What's the competition?

36:37.480 --> 36:42.080
 What's the alternative to transporting goods via trucks?

36:42.080 --> 36:44.720
 What seems to be broken here from an economics perspective?

36:44.720 --> 36:45.720
 Yeah.

36:45.720 --> 36:48.200
 So it's, well, nothing.

36:48.200 --> 36:49.400
 It's a perfect market.

36:49.400 --> 36:50.400
 Okay.

36:50.400 --> 36:51.400
 Right?

36:51.400 --> 36:53.480
 I mean, so for economists, this is how it should work, right?

36:53.480 --> 36:59.600
 But the inefficiencies, like you said, are to interrupt or push to the truck driver.

36:59.600 --> 37:04.040
 Doesn't that like spiral, doesn't that lead to a poor performance on the part of the truck

37:04.040 --> 37:09.720
 driver and just like make the whole thing more and more inefficient and it results in

37:09.720 --> 37:12.800
 lower payment to the truck driver and so on.

37:12.800 --> 37:21.200
 It just feels like in capitalism, you should have a competing solution in terms of truck

37:21.200 --> 37:26.560
 drivers like another company that provides transportation via trucks that creates a much

37:26.560 --> 37:32.520
 better experience for truck drivers, making them more efficient, all those kinds of things.

37:32.520 --> 37:34.920
 How is the competition being suppressed here?

37:34.920 --> 37:35.920
 Yeah.

37:35.920 --> 37:39.680
 So the competition is based on who's cheaper.

37:39.680 --> 37:43.120
 And this is the cheapest way to move the freight now.

37:43.120 --> 37:44.840
 They're externalities, right?

37:44.840 --> 37:50.280
 So this is the explanation that I think is obvious for this, right?

37:50.280 --> 37:57.160
 There are lots of costs that, whether it's that driver's time, whether it's the time

37:57.160 --> 38:04.000
 without their family, whether it's the fact that they drive through congestion and spew

38:04.000 --> 38:09.720
 lots of diesel particulates into cities where kids have asthma and make our commutes longer

38:09.720 --> 38:14.720
 rather than more efficiently use their time by sort of routing them around congestion

38:14.720 --> 38:17.560
 and rush hour and things like that.

38:17.560 --> 38:23.640
 This is the cheapest way to move freight and so it's the most competitive.

38:23.640 --> 38:26.880
 The big part of this is public subsidy of training.

38:26.880 --> 38:33.040
 So when those workers are not paying for the training, you and I often are.

38:33.040 --> 38:44.360
 So if you lose your job because of foreign trade or you're a veteran using your GI benefits,

38:44.360 --> 38:50.720
 you may very well be offered training, publicly subsidized training to become a truck driver.

38:50.720 --> 38:56.000
 And so all of these are externalities that the companies don't have to pay for.

38:56.000 --> 38:58.720
 And so this makes it the most profitable way to move freight.

38:58.720 --> 39:03.160
 So trucks is way cheaper than trains?

39:03.160 --> 39:08.760
 Well over the long, so one of the big stories for these companies is that the average length

39:08.760 --> 39:13.680
 of haul, which becomes very important for self driving trucks, the average length of

39:13.680 --> 39:17.400
 haul has been steadily declining.

39:17.400 --> 39:22.360
 Over the last 15 years or so, I love this industry collected data from sort of the big

39:22.360 --> 39:28.840
 firms that report it, but roughly been cut in half from typically about 1,000 miles to

39:28.840 --> 39:30.840
 under 500.

39:30.840 --> 39:35.920
 And under 500 is what a driver can move in a day.

39:35.920 --> 39:44.520
 So you can get loaded, drive and unload around 400 miles or something like that.

39:44.520 --> 39:49.720
 I want to steal a good question from the Penn Gazette interview you did, which people should

39:49.720 --> 39:50.720
 read.

39:50.720 --> 39:51.720
 It's a great interview.

39:51.720 --> 39:55.720
 Was there a golden age for long haul truckers in America?

39:55.720 --> 40:01.240
 And if so, this is just a journalistic question, and if so, what enabled it and what brought

40:01.240 --> 40:02.240
 it to an end?

40:02.240 --> 40:03.240
 Wow.

40:03.240 --> 40:06.120
 I might have to have you read my answer to that.

40:06.120 --> 40:07.640
 That was a few years ago.

40:07.640 --> 40:10.160
 It'd be interesting to compare what I'll say.

40:10.160 --> 40:16.920
 But I mean, one bigger question to ask, I guess, is Johnny Cash wrote a lot of songs

40:16.920 --> 40:18.000
 about truckers.

40:18.000 --> 40:24.400
 There used to be a time when perhaps falsely, perhaps as part of the kind of perception that

40:24.400 --> 40:29.200
 you study with the labor markets and so on, there was a perception of truckers being first

40:29.200 --> 40:33.800
 of all a lucrative job and second of all a job to be desired.

40:33.800 --> 40:34.800
 Yeah.

40:34.800 --> 40:41.520
 So, I mean, the trucking industry to me is fascinating, but I think it should be fascinating

40:41.520 --> 40:43.720
 to a lot of people.

40:43.720 --> 40:51.240
 So the golden age was really two different kinds of markets as well, right?

40:51.240 --> 40:54.600
 Today we have really good jobs and some really bad jobs.

40:54.600 --> 41:01.600
 We have the Teamsters Union that controlled the vast majority of employee jobs and even

41:01.600 --> 41:05.680
 where they had something called the National Master Freight Agreement.

41:05.680 --> 41:14.160
 And this was Jimmy Hoffa who led the union through its sort of critical period by the

41:14.160 --> 41:22.320
 mid 60s had unified essentially the entire nation's trucking labor force under one contract.

41:22.320 --> 41:29.600
 You were either covered by that contract or your employer paid a lot of attention to it.

41:29.600 --> 41:36.480
 And so by the end of the 1970s, the typical truck driver was making well more than $100,000.

41:36.480 --> 41:41.520
 Typical truck driver was making more than $100,000 in today's dollars and was home every

41:41.520 --> 41:43.200
 night.

41:43.200 --> 41:49.960
 That was without a doubt, and even more than unionized auto workers, steel workers, 10,

41:49.960 --> 41:54.120
 20% more than those workers made.

41:54.120 --> 41:58.400
 That was the golden age force of job quality, wages, Teamster power, they were without a

41:58.400 --> 42:03.240
 doubt the most powerful union in the United States at that time.

42:03.240 --> 42:09.440
 At the same time in the 1970s, you had the mythic long haul trucker.

42:09.440 --> 42:15.040
 And these were the guys who were kind of on the margins of the regulated market, which

42:15.040 --> 42:16.680
 is what the Teamsters controlled.

42:16.680 --> 42:19.640
 A lot of them were in agriculture, which was never regulated.

42:19.640 --> 42:23.640
 So in the New Deal, when they decided to regulate trucking, they didn't regulate agriculture

42:23.640 --> 42:27.160
 because they didn't want to drive up food prices, which would hurt workers in urban

42:27.160 --> 42:28.320
 areas.

42:28.320 --> 42:32.360
 So they essentially left agricultural truckers out of it.

42:32.360 --> 42:41.160
 And that's where a lot of the kind of outlaw, asphalt cowboy imagery that we get.

42:41.160 --> 42:48.280
 And I grew up, I know you didn't grow up in the US as a young child, and I'm a bit older

42:48.280 --> 42:55.480
 than you, but in the late 70s, there were movies and TV shows, and CBs were crazed,

42:55.480 --> 43:00.520
 and it was all these kind of outlaw truckers who were out there hauling some unregulated

43:00.520 --> 43:01.520
 freight.

43:01.520 --> 43:05.960
 They weren't supposed to be trying to avoid the bears who are the cops and with all this

43:05.960 --> 43:12.680
 salty language and these terms that only they understood and the partying at diners

43:12.680 --> 43:15.880
 and popping pills, the California turnarounds.

43:15.880 --> 43:21.160
 So asphalt cowboy is truly, it's like another form of cowboy movies.

43:21.160 --> 43:22.160
 Oh, absolutely.

43:22.160 --> 43:23.160
 Yeah.

43:23.160 --> 43:24.160
 Absolutely.

43:24.160 --> 43:29.480
 And I think that sort of masculine ethos of like, you got 40,000 pounds of something

43:29.480 --> 43:31.560
 you care about, I'm your guy.

43:31.560 --> 43:35.320
 You needed to go from New York to California, don't worry about it, I got it.

43:35.320 --> 43:36.320
 That's appealing.

43:36.320 --> 43:37.320
 And it's tangible, right?

43:37.320 --> 43:40.720
 And you think about people who don't want to be paper pusher and sit in the deal with

43:40.720 --> 43:44.200
 office politics like, just give me what you care about and I'll take care of it.

43:44.200 --> 43:47.080
 Just pay me fair, you know, and that appeals.

43:47.080 --> 43:53.720
 You mentioned unions, Teamsters, Jimmy Hoffa, big question, maybe difficult question.

43:53.720 --> 43:58.960
 What are some pros and cons of unions historically and today in the trucking space?

43:58.960 --> 43:59.960
 Yeah.

43:59.960 --> 44:03.840
 Well, if you're a worker, there are a lot of pros.

44:03.840 --> 44:07.360
 And I don't, you know, and this was one of the things I talked to truckers about a lot.

44:07.360 --> 44:08.360
 Yeah.

44:08.360 --> 44:11.400
 What's their perception of Jimmy Hoffa, for example, and of unions?

44:11.400 --> 44:12.400
 Yeah.

44:12.400 --> 44:17.240
 So, and this was probably one of the central hypotheses that I had going in there.

44:17.240 --> 44:21.280
 And it may sound, you know, someone who does hard science, right?

44:21.280 --> 44:25.400
 You may, if you hear a social scientist, you know, sort of use that terminology, even other

44:25.400 --> 44:26.400
 social scientists.

44:26.400 --> 44:27.400
 Hypothesis.

44:27.400 --> 44:28.400
 Yeah.

44:28.400 --> 44:31.400
 You know, they don't like it, but I do like to think that way.

44:31.400 --> 44:37.720
 And my initial hypothesis was that, you know, and it's very simple that, you know, the tenure

44:37.720 --> 44:43.560
 of the driver in the industry would have a strong effect on how they viewed unions.

44:43.560 --> 44:48.680
 That, you know, somebody who had experienced unions would be more favorable and someone

44:48.680 --> 44:52.200
 who had not would not be, right?

44:52.200 --> 44:55.880
 And that turned out to be the case without a doubt.

44:55.880 --> 45:01.200
 But in an interesting way, which was that even the drivers who were not part of the

45:01.200 --> 45:11.240
 union, who in the kind of public debate of deregulation were portrayed as these kind

45:11.240 --> 45:16.520
 of small business truckers who were getting shut out by the big regulated monopolies and

45:16.520 --> 45:21.480
 the Teamsters Union, you know, the corrupt Teamsters Union, even those drivers longed

45:21.480 --> 45:27.920
 for the days of the Teamsters, because they recognized the overall market impact that

45:27.920 --> 45:34.520
 they had that that trucking just naturally tended toward excessive competition.

45:34.520 --> 45:37.600
 That meant that there was no profit to be made.

45:37.600 --> 45:40.280
 And oftentimes you'd be operating at a loss.

45:40.280 --> 45:45.000
 And so even these, you know, the asphalt cowboy owner operators from back in the day

45:45.000 --> 45:51.160
 would tell me when the Teamsters were in power, I made a lot more money.

45:51.160 --> 45:57.320
 And you know, this is, you know, unions, at least those kinds of unions like the Teamsters,

45:57.320 --> 46:02.320
 you know, there's I think a lot of misconceptions today sort of popularly about what unions

46:02.320 --> 46:03.880
 did back then.

46:03.880 --> 46:06.200
 They tied wages to productivity.

46:06.200 --> 46:10.680
 Like that was the central thing that the Teamsters Union did.

46:10.680 --> 46:16.520
 And you know, there were great accounts of sort of Jimmy Hoffa's perspective for all

46:16.520 --> 46:19.600
 his portrayal as sort of corrupt and criminal.

46:19.600 --> 46:24.520
 And there's, you know, I'm not disputing that he broke a lot of laws.

46:24.520 --> 46:29.480
 He was remarkably open about who he was and what he did.

46:29.480 --> 46:34.800
 He actually invited a pair, a husband and wife team of Harvard economists to follow him

46:34.800 --> 46:41.880
 around and like opened up the Teamsters books to them so that they could see how he was,

46:41.880 --> 46:45.640
 you know, thinking about negotiating with the employers.

46:45.640 --> 46:53.280
 And the Teamsters, and this goes back well before Hoffa, back to the, you know, 1800s,

46:53.280 --> 46:57.680
 they understood that workers did better if their employers did better.

46:57.680 --> 47:02.040
 And the only way the employers would do better was if they controlled the market.

47:02.040 --> 47:07.000
 And so oftentimes the corruption and trucking was initiated by employers who wanted to limit

47:07.000 --> 47:11.080
 competition and they knew they couldn't limit competition without the support of labor.

47:11.080 --> 47:15.720
 And so you'd get these collusive arrangements between employers and labor to say, no new

47:15.720 --> 47:20.960
 trucking companies, there are 10 of us, that's enough, we control Seattle, we're going to

47:20.960 --> 47:24.960
 set the price and we're not going to be undercut.

47:24.960 --> 47:29.080
 When there's a shortage of trucks around, it's great rates, rates go up, but you get

47:29.080 --> 47:30.440
 too many trucks.

47:30.440 --> 47:35.560
 It's very often that you end up operating at a loss just to keep the doors open.

47:35.560 --> 47:39.600
 You know, you don't have any choice, you can't, it's what economists call derived demand.

47:39.600 --> 47:43.120
 You can't like make up a bunch of trucking services and store it in a warehouse, right?

47:43.120 --> 47:46.320
 You got to, you got to keep those trucks moving to pay the bills.

47:46.320 --> 47:51.800
 Can we also lay out the kind of jobs that are in trucking, what are the best jobs in

47:51.800 --> 47:55.720
 trucking, what are the worst jobs in trucking, what are we, how many jobs are we talking

47:55.720 --> 48:01.120
 about today and what kind of jobs are there?

48:01.120 --> 48:07.160
 So there are a number of different segments and the first part would be, you know, are

48:07.160 --> 48:11.520
 you offering, the first question would be, are you offering services to the public or

48:11.520 --> 48:13.240
 are you moving your own freight, right?

48:13.240 --> 48:19.200
 So are you a retailer, say Walmart or, you know, a paper company or something like that

48:19.200 --> 48:22.560
 that's operating your own fleet of trucks?

48:22.560 --> 48:26.120
 That's private trucking.

48:26.120 --> 48:30.960
 For hire are the folks who, you know, offer their services out to other customers.

48:30.960 --> 48:33.400
 So you have private and for hire.

48:33.400 --> 48:38.320
 In general, for hire pays less.

48:38.320 --> 48:43.560
 Is that because of the, something you talk about employee versus contractor situation

48:43.560 --> 48:49.160
 or are they all tricked or led to become contractors?

48:49.160 --> 48:54.960
 That can become a part of it as a strategy, but the fundamental reason is competition.

48:54.960 --> 49:00.400
 So those private carriers don't, aren't in competition with other trucking fleets, right,

49:00.400 --> 49:02.280
 for their own in house services.

49:02.280 --> 49:08.120
 So, you know, they tend to, and this, you know, the question of why private versus for hire,

49:08.120 --> 49:10.240
 because for hire is cheaper, right?

49:10.240 --> 49:15.760
 And so if you need that, if that trucking service is central to what you do and you

49:15.760 --> 49:19.880
 cannot afford disruptions or volatility in the price of it, you keep it in house.

49:19.880 --> 49:22.760
 You should be willing to pay more for that because it's more valuable to you and you

49:22.760 --> 49:23.920
 keep it in house than that.

49:23.920 --> 49:25.600
 So that's an interesting distinction.

49:25.600 --> 49:31.960
 What about, and this is kind of moving towards our conversation, what can and can't be automated?

49:31.960 --> 49:36.260
 How else does it divide the different trucking jobs?

49:36.260 --> 49:40.880
 So the next big chunk is kind of how much stuff are you moving, right?

49:40.880 --> 49:43.920
 And so we have what's called truckload.

49:43.920 --> 49:48.680
 And truckload means, you know, you can fill up a trailer either by volume or by weight

49:48.680 --> 49:50.600
 and then less than truckload.

49:50.600 --> 49:54.360
 Less than truckload, the official definition is like less than 10,000 pounds.

49:54.360 --> 49:58.800
 You know, this is going to be a couple pallets of this, a couple pallets of that.

49:58.800 --> 50:01.080
 The process looks really different, right?

50:01.080 --> 50:06.240
 So that truckload is, you know, point A to point B, I'm buying, you know, a truckload

50:06.240 --> 50:11.800
 of, of bounty paper towels, I'm bringing it into, you know, my distribution center.

50:11.800 --> 50:15.720
 Go pick it up at the, at the bounty plant, bring it to my distribution center, right?

50:15.720 --> 50:16.720
 Nowhere in between.

50:16.720 --> 50:17.720
 Do you stop?

50:17.720 --> 50:19.760
 At least process that freight.

50:19.760 --> 50:24.400
 Less than truckload, what you've got is terminal systems, and this is what you had under, under

50:24.400 --> 50:26.240
 regulation too.

50:26.240 --> 50:29.880
 And so these terminal systems, what you do is you do a bunch of local pickup and delivery,

50:29.880 --> 50:32.160
 maybe with smaller trucks.

50:32.160 --> 50:35.160
 And you pick up two pallets of this here, four pallets of this there.

50:35.160 --> 50:38.480
 You bring it to the terminal, you combine it based on the destination.

50:38.480 --> 50:44.480
 You then create a full truckload, you know, trailer, and you send it to another terminal

50:44.480 --> 50:48.080
 where it gets broken back down and then, and then out for local delivery.

50:48.080 --> 50:52.600
 That's going to look a lot like if you send a package by, by UPS, right?

50:52.600 --> 50:56.520
 They pick all these parcels, right, figure out where they're all going, put them on planes

50:56.520 --> 51:00.160
 or, or in trailers going to the same destination, then break them out to put them in what, what

51:00.160 --> 51:03.120
 they call package cars.

51:03.120 --> 51:09.200
 Before I ask you about autonomous trucks, let's just pause for your experience as a

51:09.200 --> 51:10.200
 trucker.

51:10.200 --> 51:12.200
 Did it get lonely?

51:12.200 --> 51:16.840
 Like, can you talk about some of your experiences of what it was actually like?

51:16.840 --> 51:17.840
 Did it get lonely?

51:17.840 --> 51:18.840
 Yeah.

51:18.840 --> 51:21.400
 No, I mean, it was, I didn't have kids at the time.

51:21.400 --> 51:22.400
 Now, now I have kids.

51:22.400 --> 51:24.400
 I can't even imagine it.

51:24.400 --> 51:29.680
 Uh, you know, I've been married for five years at, at the time.

51:29.680 --> 51:30.680
 My wife hated it.

51:30.680 --> 51:31.680
 I hated it.

51:31.680 --> 51:37.520
 Uh, you know, I, I describe in the book the experience of being stuck, if I remember

51:37.520 --> 51:43.640
 correctly, it was like Ohio, uh, at this truck stop in the middle of nowhere and like, you

51:43.640 --> 51:48.800
 know, sitting on this concrete barrier and just watching fireworks in the distance and

51:48.800 --> 51:51.400
 like eating Chinese food on the 4th of July.

51:51.400 --> 51:56.400
 And, you know, my wife calls me from like the family barbecue and our anniversary is

51:56.400 --> 51:58.760
 July 8th and she's like, are you going to be home?

51:58.760 --> 52:08.920
 And I'm like, I don't know, you know, um, I have a, a cousin whose husband drove, drove

52:08.920 --> 52:12.480
 truck as a truck driver would say drove truck for a while.

52:12.480 --> 52:17.600
 Um, and he told me before I went into it, he was like, the, the advantage you have is

52:17.600 --> 52:21.160
 that you know that you're not going to be doing this long term.

52:21.160 --> 52:27.760
 Like, and Lex, I can't even like the emotional content of some of these interviews.

52:27.760 --> 52:32.160
 I mean, I would sit down at a truck stop with somebody I had never met before and you know,

52:32.160 --> 52:39.440
 you open the spicket and the, the, the last question I would ask drivers was that by the

52:39.440 --> 52:42.480
 time I really sort of figured out how to do it, the last question I, I would ask them

52:42.480 --> 52:48.400
 is, you know, what advice would you give to somebody like your nephew, you know, a family

52:48.400 --> 52:52.320
 friend asks you about what it's like to be a driver and should they do it?

52:52.320 --> 52:54.320
 What advice would you give them?

52:54.320 --> 52:59.560
 And this question, some of these, you know, grizzled old drivers, you know, tough, tough

52:59.560 --> 53:05.200
 guys would that question would like some of them would break down and they would say,

53:05.200 --> 53:12.720
 I would say to them, you better have everything that you ever wanted in life already because

53:12.720 --> 53:15.400
 I've had a car that I've had for 10 years.

53:15.400 --> 53:17.040
 It's got 7,000 miles on it.

53:17.040 --> 53:21.920
 I own a boat that hasn't seen the water in five years.

53:21.920 --> 53:27.320
 My kids, I didn't raise them like I, I'd be out for two weeks at a time.

53:27.320 --> 53:32.840
 I'd come home, my, my wife would give me two kids to punish a list of things to do, you

53:32.840 --> 53:37.760
 know, on Saturday night and I might leave out Sunday night or Monday morning, you know,

53:37.760 --> 53:39.520
 I come home dead tired.

53:39.520 --> 53:46.200
 My kids don't know who I am and you know, it was just like, it was heartbreaking to hear

53:46.200 --> 53:47.200
 those stories.

53:47.200 --> 53:53.760
 And then before you know it, you know, life is short and just the years run away.

53:53.760 --> 53:55.080
 Yeah.

53:55.080 --> 54:00.800
 Hard question to ask in that context, but what's the best, what was the best part of

54:00.800 --> 54:04.240
 being a truck driver?

54:04.240 --> 54:08.080
 Was there moments that you truly enjoyed on the road?

54:08.080 --> 54:09.320
 Oh, absolutely.

54:09.320 --> 54:14.560
 There was, there's definitely a pride and mastery of, you know, even basic competence

54:14.560 --> 54:17.200
 of sort of piloting this thing safely.

54:17.200 --> 54:18.720
 There's a lot of responsibility to it.

54:18.720 --> 54:21.440
 That thing's dangerous and you know it.

54:21.440 --> 54:26.080
 So there's, there's some pride there for me personally and I know for a lot of other

54:26.080 --> 54:30.520
 drivers, it's just like seeing these behind the scenes places that you know exist in

54:30.520 --> 54:32.520
 our economy.

54:32.520 --> 54:38.520
 And I think we're all much more aware of them now after COVID and supply chain mess that

54:38.520 --> 54:39.520
 we have.

54:39.520 --> 54:42.920
 I don't know if we'll talk about that, but you know, you get to see those places, you

54:42.920 --> 54:46.720
 know, you get to see those ports, you get to see the, the place where they make the

54:46.720 --> 54:52.200
 cardboard boxes that the Huggie diapers go in, Huggies diapers going or the warehouse

54:52.200 --> 54:53.560
 full of Bud Light.

54:53.560 --> 54:59.560
 I moved Bud Light from like upstate New York and the first load like went to Atlanta,

54:59.560 --> 55:04.600
 you know, and then a couple months later I circled back through that same brewery and

55:04.600 --> 55:10.960
 I brought a load of Bud Light out to Michigan and I was like, holy shit, all the Bud Light,

55:10.960 --> 55:15.600
 like you know, for this whole giant swath of the United States comes from this one plant,

55:15.600 --> 55:20.440
 this cavernous plant with like kegs of beer and you see that part of the economy and it's

55:20.440 --> 55:26.040
 like you're almost like you're an economic tourist and I think all everybody kind of

55:26.040 --> 55:31.200
 appreciates that like kind of, it's almost like a behind the scenes tour that wears off

55:31.200 --> 55:35.720
 after a few months, you know, you start to see new things less and less frequently.

55:35.720 --> 55:40.440
 At first everything's novel and sort of life on the road and then it becomes just endless

55:40.440 --> 55:47.040
 miles of white lines and yellow lines and truck stops and the days just blur together.

55:47.040 --> 55:49.760
 You know, it's one loading dock after another.

55:49.760 --> 55:52.560
 So you lose the magic of being on the road.

55:52.560 --> 55:58.440
 Yeah, it's very rare, the driver that doesn't.

55:58.440 --> 56:05.760
 You mentioned COVID and supply chain while being this for a brief time, this member of

56:05.760 --> 56:12.240
 the supply chain, what have you come to understand about our supply chain, United States and

56:12.240 --> 56:20.240
 global and its resilience against strategies, catastrophes in the world like COVID for example?

56:20.240 --> 56:29.520
 Yeah, I mean, we have built really long, really lean supply chains and just by definition,

56:29.520 --> 56:31.600
 they're fragile.

56:31.600 --> 56:37.360
 You know, the current mess that we have, it's not going to clear by Christmas.

56:37.360 --> 56:40.040
 It will be lucky if it clears by next Christmas.

56:40.040 --> 56:43.080
 Can you describe the current mess and supply chain that you were referring to?

56:43.080 --> 56:51.920
 Yeah, so we've got pile ups of ships off the coast of California, Long Beach and LA in

56:51.920 --> 56:55.240
 particular in bad shape.

56:55.240 --> 57:00.520
 Last I checked it was around 60 ships, all of which are holding thousands of containers

57:00.520 --> 57:06.760
 full of stuff that retailers were hoping was going to be on shelves for the holiday season.

57:06.760 --> 57:12.840
 Meanwhile, the port itself has stacks and stacks of containers that they can't get rid of.

57:12.840 --> 57:18.080
 The truckers aren't showing up to pick up the containers that are there, so they can't

57:18.080 --> 57:22.920
 offload the ships that are waiting.

57:22.920 --> 57:26.480
 And why aren't the truckers picking it up?

57:26.480 --> 57:30.080
 Partly because there's a long history of inefficiency and making them wait, but it's because the

57:30.080 --> 57:33.000
 warehouses are full.

57:33.000 --> 57:38.960
 And so we've had all these perverse outcomes that no one really expected, like in the middle

57:38.960 --> 57:43.480
 of all these shortages, people are stockpiling stuff.

57:43.480 --> 57:51.040
 So there are suppliers who used to keep two months of supply of bottled water on hand,

57:51.040 --> 57:55.680
 and after going through COVID and not having supply to send to their customers, they're

57:55.680 --> 57:57.880
 like, we need three months.

57:57.880 --> 58:04.760
 Well, our system is not designed for major storage of goods to go up 50% in a category.

58:04.760 --> 58:05.760
 It's lean.

58:05.760 --> 58:08.720
 If you're a warehouse operator, you want to be 90% plus.

58:08.720 --> 58:10.800
 You don't want a lot of open bays sitting around.

58:10.800 --> 58:16.320
 So we don't have 10% extra capacity in warehouses.

58:16.320 --> 58:18.360
 We don't have 10% of them.

58:18.360 --> 58:24.040
 Trucking capacity can fluctuate a bit, but you don't have that kind of slack.

58:24.040 --> 58:31.560
 And now, and we saw this when people shifted consumption, and I get a little mad when people

58:31.560 --> 58:38.320
 talk about panic buying as the reason that we had all these shortages.

58:38.320 --> 58:44.280
 It's preventing us from understanding the real problem there, which is that lean supply

58:44.280 --> 58:45.280
 chain.

58:45.280 --> 58:50.840
 Sure, there was some panic buying, no doubt about it, but we had an enormous shift in

58:50.840 --> 58:51.960
 people's behavior.

58:51.960 --> 58:57.800
 So with my sister and brother in law, I own a couple of small businesses and we serve

58:57.800 --> 58:58.800
 food.

58:58.800 --> 59:03.080
 So we get food from Cisco.

59:03.080 --> 59:06.080
 Cisco couldn't get rid of food because nobody's eating out.

59:06.080 --> 59:11.720
 So they've got 50 pound sacks of flour sitting in their warehouse that they can't get rid

59:11.720 --> 59:12.720
 of.

59:12.720 --> 59:17.520
 They've got cases of lettuce and meat and everything else that's just going to go bad.

59:17.520 --> 59:22.080
 So that panic buying certainly exacerbated some things like toilet paper and whatever,

59:22.080 --> 59:25.920
 but we saw just a massive change in demand.

59:25.920 --> 59:28.840
 And our supply chains are based on historical data.

59:28.840 --> 59:34.760
 So that stuff leaves Asia months before you want to have it on the shelves, and you're

59:34.760 --> 59:39.960
 predicting based on last year what you want on that shelf.

59:39.960 --> 59:48.760
 And so it's a, I guess at its best, it's a beautiful symphony of lots of moving parts,

59:48.760 --> 59:52.880
 but now everyone can't get on the same page of music.

59:52.880 --> 59:59.640
 But it's not resilient to changes in en masse human behavior.

59:59.640 --> 1:00:06.560
 So even like I read somewhere, maybe you can tell me if it's true in relation to food,

1:00:06.560 --> 1:00:10.760
 it's just the change of human behavior between going out to restaurants versus eating at

1:00:10.760 --> 1:00:16.000
 home as a species, we consume a lot less food that way.

1:00:16.000 --> 1:00:20.720
 Apparently what I read in restaurants, like there's a lot of food just thrown out.

1:00:20.720 --> 1:00:23.160
 It's part of the business model.

1:00:23.160 --> 1:00:28.200
 And so like you then have to move a lot more food through the whole supply chain.

1:00:28.200 --> 1:00:33.720
 And now because you're consuming, you know, there's leftovers at home, you're consuming

1:00:33.720 --> 1:00:38.880
 much more of the food you're getting when you're eating at home.

1:00:38.880 --> 1:00:42.880
 That's creating these bottleneck situations, problems that you're referring to too much

1:00:42.880 --> 1:00:45.520
 in a certain place, not enough in another place.

1:00:45.520 --> 1:00:52.000
 And it's just the supply chain is not robust, those kind of dynamic shifts in who gets what

1:00:52.000 --> 1:00:53.000
 where.

1:00:53.000 --> 1:00:54.000
 Yeah.

1:00:54.000 --> 1:00:55.000
 Yeah.

1:00:55.000 --> 1:01:01.600
 I mean, so, and I have worked in agriculture a bit on sort of the supply side, you know,

1:01:01.600 --> 1:01:07.560
 and there are product categories right where 30% of the crop raised does not get used, right?

1:01:07.560 --> 1:01:10.080
 Just gets plowed under or wasted.

1:01:10.080 --> 1:01:14.640
 But here's the importance of this and sort of getting this right, you know, like that,

1:01:14.640 --> 1:01:19.320
 not that like panic buying, you know, blame the irrational consumer, you know, look at

1:01:19.320 --> 1:01:24.600
 the hard sort of truth of the way we've set up our economy.

1:01:24.600 --> 1:01:30.840
 And I'll ask you this Lex, I know you're a hopeful, optimistic person.

1:01:30.840 --> 1:01:31.840
 100%.

1:01:31.840 --> 1:01:32.840
 Yes.

1:01:32.840 --> 1:01:33.840
 Yeah.

1:01:33.840 --> 1:01:34.840
 I am too.

1:01:34.840 --> 1:01:35.840
 I mean, I write about problems all the time.

1:01:35.840 --> 1:01:40.120
 And so people think I'm sort of like just a Debbie Downer, you know, pessimist.

1:01:40.120 --> 1:01:43.240
 But I'm a glass half full kind of guy.

1:01:43.240 --> 1:01:47.520
 Like I want to identify problems so we can solve them.

1:01:47.520 --> 1:01:52.160
 So let me ask you this, we've got these long lean supply chains.

1:01:52.160 --> 1:02:02.760
 In the future, do you see more environmental problems that could disrupt them, more geopolitical

1:02:02.760 --> 1:02:11.240
 problems that could disrupt trade from Asia, you know, other institutional failures?

1:02:11.240 --> 1:02:17.000
 Do those things seem, you know, potentially more likely in the future than they have been

1:02:17.000 --> 1:02:18.720
 in say the last 20 years?

1:02:18.720 --> 1:02:19.720
 Yeah.

1:02:19.720 --> 1:02:21.920
 It almost absolutely seems to be the case.

1:02:21.920 --> 1:02:29.760
 So you then have to ask the question of how do we change our supply chains, whether it's

1:02:29.760 --> 1:02:38.000
 making more resilient or make them less densely connected, you know, building, it's like what

1:02:38.000 --> 1:02:44.600
 is it, you know, the Tesla model for in the automotive sector of like trying to build

1:02:44.600 --> 1:02:49.400
 everything, like trying to get the factory to do as much as possible with as little

1:02:49.400 --> 1:02:55.080
 reliance on widely distributed sources of the supply chain as possible.

1:02:55.080 --> 1:03:01.160
 So maybe like rethinking how much we rely on the infrastructure of the supply chain.

1:03:01.160 --> 1:03:02.160
 Yeah.

1:03:02.160 --> 1:03:08.120
 I mean, you know, there's some basic and I assume, right, that there are a lot of folks

1:03:08.120 --> 1:03:13.960
 in corporate boardrooms looking at risk and saying that didn't go well, and maybe it could

1:03:13.960 --> 1:03:16.800
 have even gone worse.

1:03:16.800 --> 1:03:20.760
 Maybe we need to think about reshoring, right?

1:03:20.760 --> 1:03:24.680
 At the very least, one of the things that I'm hearing about anecdotally is that they're

1:03:24.680 --> 1:03:30.120
 storing stuff up, you know, when they can, right, which is that's not, that's probably

1:03:30.120 --> 1:03:31.120
 not sustainable, right?

1:03:31.120 --> 1:03:35.920
 I mean, at some point, somebody in that corporate boardroom is going to say, you know, guys,

1:03:35.920 --> 1:03:38.960
 inventory is getting kind of heavy and the cost of that is like, do we, can we really

1:03:38.960 --> 1:03:41.880
 justify that much longer to the shareholders, right?

1:03:41.880 --> 1:03:44.680
 We should, we can back off and start, you know, back things are back to normal.

1:03:44.680 --> 1:03:45.680
 Let's lean out.

1:03:45.680 --> 1:03:49.640
 But my hope is that there's a technology solution to a lot of aspects of this.

1:03:49.640 --> 1:03:55.600
 So one of them on the supply chain side is collecting a lot more data, like having much

1:03:55.600 --> 1:04:01.320
 more integrated and accurate representation of the inventory all over the place and the

1:04:01.320 --> 1:04:08.160
 available transportation mechanisms, the trucks, the all kinds of freight and how in the different

1:04:08.160 --> 1:04:15.520
 models of the possible catastrophes that can happen, what, like how will the system

1:04:15.520 --> 1:04:16.520
 respond?

1:04:16.520 --> 1:04:20.760
 So having a really solid model that you're operating under as opposed to just kind of

1:04:20.760 --> 1:04:27.240
 being in emergency response mode under poor, incomplete information, which is what seems

1:04:27.240 --> 1:04:34.200
 like is more commonly the case, except for things like you said, Walmart and Amazon,

1:04:34.200 --> 1:04:38.640
 they're trying to internally get their stuff together on that front, but that doesn't help

1:04:38.640 --> 1:04:40.360
 the rest of the economy.

1:04:40.360 --> 1:04:46.840
 So another exciting technological development as you write about, as you think about is

1:04:46.840 --> 1:04:49.360
 autonomous trucks.

1:04:49.360 --> 1:04:55.920
 So these are often brought up in different contexts as the examples of AI and robots taking

1:04:55.920 --> 1:04:57.560
 our jobs.

1:04:57.560 --> 1:04:58.880
 How true is this?

1:04:58.880 --> 1:05:01.640
 Should we be concerned?

1:05:01.640 --> 1:05:05.880
 I think they've really come to epitomize this anxiety over automation, right?

1:05:05.880 --> 1:05:14.200
 It's such a simple idea, right, truck that drives itself, classic blue collar job that

1:05:14.200 --> 1:05:21.440
 pays well, guy maybe with not a lot of other good options, right, to sort of make that

1:05:21.440 --> 1:05:28.760
 same income easily, right, and you build a robot to take his job away, right?

1:05:28.760 --> 1:05:36.880
 So I think 2016 or so, that was the big question out there, and that's actually how I started

1:05:36.880 --> 1:05:38.280
 studying it, right?

1:05:38.280 --> 1:05:43.320
 I just wrapped up the book, just so happened that somebody who was working at Uber, Uber

1:05:43.320 --> 1:05:47.960
 had just bought Auto, saw the book and was like, hey, can you come out and talk to our

1:05:47.960 --> 1:05:53.360
 engineering teams about what life is like for truck drivers and maybe how our technology

1:05:53.360 --> 1:05:54.360
 could make it better?

1:05:54.360 --> 1:06:00.280
 And at that time, there were a lot of different ideas about how they were going to play out,

1:06:00.280 --> 1:06:01.280
 right?

1:06:01.280 --> 1:06:05.480
 So while the press was saying, all truckers are going to lose their jobs, there were a

1:06:05.480 --> 1:06:10.160
 lot of people in these engineering teams who thought, okay, if we've got an individual

1:06:10.160 --> 1:06:18.800
 owner operator and they can only drive eight or 10 hours a day, they hop in the back, they

1:06:18.800 --> 1:06:23.080
 get their rest, and the asset that they own works for them, right?

1:06:23.080 --> 1:06:25.920
 That's sort of perfect, right?

1:06:25.920 --> 1:06:30.480
 And at that time, there were a bunch of reports that came out, and basically what people did

1:06:30.480 --> 1:06:33.280
 was they took the category of truck driver.

1:06:33.280 --> 1:06:38.080
 Some people took a larger category from BLS of sales and delivery workers that was about

1:06:38.080 --> 1:06:43.640
 three and a half million workers, and others took the heavy duty truck driver category,

1:06:43.640 --> 1:06:46.880
 which was at the time about 1.8 million or so.

1:06:46.880 --> 1:06:52.240
 And they picked a start date and a slope and said, let's assume that all these jobs

1:06:52.240 --> 1:06:54.040
 are just going to disappear.

1:06:54.040 --> 1:07:01.040
 And really smart researcher, Annetta Bernhardt at the Labor Center at UC Berkeley, was sort

1:07:01.040 --> 1:07:07.920
 of looking around for people who were sort of deeply into industries to complicate those

1:07:07.920 --> 1:07:10.520
 analyses, right?

1:07:10.520 --> 1:07:12.640
 And reached out to me and was like, what do you think of this?

1:07:12.640 --> 1:07:15.760
 And I said, the industry's super diverse, you know, this is just, I haven't given a

1:07:15.760 --> 1:07:20.120
 ton of thought, but it can't be that, you know, it's not that simple, you know, it never

1:07:20.120 --> 1:07:21.120
 is.

1:07:21.120 --> 1:07:23.880
 And so she was like, will you, you know, will you do this?

1:07:23.880 --> 1:07:27.240
 And I was like ready to move on to another topic, you know, I'd like been in trucking

1:07:27.240 --> 1:07:28.840
 for 10 years.

1:07:28.840 --> 1:07:31.200
 And that that's how I started looking at it.

1:07:31.200 --> 1:07:34.080
 And it is, it's a lot more complicated.

1:07:34.080 --> 1:07:41.120
 And the initial impacts, and here's the challenge, I think, and it's not just a research challenge,

1:07:41.120 --> 1:07:47.840
 it's the fundamental public policy challenge is we look at the existing industry and the

1:07:47.840 --> 1:07:50.200
 impacts, the potential impacts.

1:07:50.200 --> 1:07:53.520
 We're not, you know, nothing.

1:07:53.520 --> 1:07:58.720
 For some communities and some kinds of drivers, they're going to be hard and there are a significant

1:07:58.720 --> 1:07:59.720
 number of them.

1:07:59.720 --> 1:08:04.680
 Nowhere near what people thought, you know, I estimates like around 300,000, but that's

1:08:04.680 --> 1:08:08.160
 a static picture of the existing industry.

1:08:08.160 --> 1:08:15.560
 And here's the key with this is, at least in my, my conclusion is this is a transformative

1:08:15.560 --> 1:08:17.400
 technology.

1:08:17.400 --> 1:08:23.240
 We are not going to swap in self driving trucks for human driven trucks and all else stays

1:08:23.240 --> 1:08:24.360
 the same.

1:08:24.360 --> 1:08:27.600
 This is going to reshape our supply chains.

1:08:27.600 --> 1:08:29.520
 It's going to reshape landscapes.

1:08:29.520 --> 1:08:33.840
 It's going to affect our ability to fight climate change.

1:08:33.840 --> 1:08:37.000
 This is a really important technology in this space.

1:08:37.000 --> 1:08:43.640
 Do you think it's possible to predict the future of the kind of opportunities it will

1:08:43.640 --> 1:08:46.840
 create, how it will change the world?

1:08:46.840 --> 1:08:53.200
 So like when you have the internet, you can start saying like all the kind of ways that

1:08:53.200 --> 1:08:58.840
 office work, all jobs will be lost because it's easy to network and then software engineering

1:08:58.840 --> 1:09:05.160
 allows you to automate a lot of the tasks at Microsoft Excel does, you know, but it

1:09:05.160 --> 1:09:08.280
 opened up so many opportunities.

1:09:08.280 --> 1:09:12.760
 Even with things that are difficult to imagine, like with the internet, I don't know Wikipedia,

1:09:12.760 --> 1:09:16.040
 which is widely making accessible information.

1:09:16.040 --> 1:09:23.040
 And that increased the general education globally by a lot, all those kinds of things.

1:09:23.040 --> 1:09:29.920
 And then the ripple effects of that in terms of your ability to find other jobs is probably

1:09:29.920 --> 1:09:31.080
 immeasurable.

1:09:31.080 --> 1:09:38.840
 So is it, is it just a hopeless pursuit to try to predict if you talk about these six

1:09:38.840 --> 1:09:45.840
 different trajectories that we might take in automating trucks, but like as a result

1:09:45.840 --> 1:09:50.360
 of taking those trajectories, is it a hopeless pursuit to predict what the future will result

1:09:50.360 --> 1:09:51.360
 in?

1:09:51.360 --> 1:09:52.360
 Yeah, it is.

1:09:52.360 --> 1:09:54.880
 It absolutely is.

1:09:54.880 --> 1:09:56.920
 Because it's the wrong question.

1:09:56.920 --> 1:10:02.440
 The question is, what do we want the future to be and let's shape it, right?

1:10:02.440 --> 1:10:07.080
 And I think this is, you know, and this is the only point that I really want to make

1:10:07.080 --> 1:10:14.120
 in my work, you know, for the foreseeable future is that, you know, we have got to get

1:10:14.120 --> 1:10:21.000
 out of this mindset that we're just going to let technology kind of go and it's a natural

1:10:21.000 --> 1:10:25.520
 process and whatever pops out will fix the problems on the backside.

1:10:25.520 --> 1:10:30.240
 And we've got to recognize that one, that's not what we do, right?

1:10:30.240 --> 1:10:35.480
 You know, and self driving vehicles is just such a perfect example, right?

1:10:35.480 --> 1:10:38.560
 We would not be sitting here today if the Defense Department, right?

1:10:38.560 --> 1:10:47.560
 If Congress in 2000 had not written into legislation funding for the DARPA challenges, which followed

1:10:47.560 --> 1:10:51.080
 for, actually, I think the funding came a couple years later, but the priority that

1:10:51.080 --> 1:10:56.800
 they wrote in 2000 was, let's get a third of all ground vehicles in our military forces

1:10:56.800 --> 1:10:57.800
 unmanned.

1:10:57.800 --> 1:10:58.800
 Right?

1:10:58.800 --> 1:11:02.880
 And this was before aerial unmanned vehicles had really sort of proven their worth.

1:11:02.880 --> 1:11:07.240
 They would come to be incredibly like, you know, just blow people out of them, blow people's

1:11:07.240 --> 1:11:12.960
 minds in terms of their additional capabilities, the lower costs, you know, keeping, you know,

1:11:12.960 --> 1:11:13.960
 soldiers out of harm's way.

1:11:13.960 --> 1:11:17.000
 And of course, they raised other problems and considerations that I think we're still

1:11:17.000 --> 1:11:18.000
 wrestling with.

1:11:18.000 --> 1:11:21.240
 But that was even before that they had this priority.

1:11:21.240 --> 1:11:27.480
 We would not be sitting here today if Congress in 2000 had not said, let's bring this about.

1:11:27.480 --> 1:11:30.080
 So they already had that vision, actually, I didn't know about that.

1:11:30.080 --> 1:11:36.320
 So for people who don't know the DARPA challenges is the events that were just kind of like

1:11:36.320 --> 1:11:41.760
 these seemingly small scale challenges that brought together some of the smartest roboticists

1:11:41.760 --> 1:11:42.760
 in the world.

1:11:42.760 --> 1:11:50.000
 And that somehow created enough of a magic where ideas flourished, both engineering and

1:11:50.000 --> 1:11:56.240
 scientific that eventually then was the catalyst for creating all these different companies

1:11:56.240 --> 1:12:00.200
 that took on the challenge, some failed, some succeeded, some are still fighting the good

1:12:00.200 --> 1:12:01.200
 fight.

1:12:01.200 --> 1:12:07.640
 And that somehow just that little bit of challenge was the essential spark of progress that now

1:12:07.640 --> 1:12:14.720
 resulted in this beautiful up and down wave of hype and profit and all this kind of weird

1:12:14.720 --> 1:12:20.520
 dance where the B word, billions of dollars have been thrown around and we still don't

1:12:20.520 --> 1:12:21.520
 know.

1:12:21.520 --> 1:12:25.600
 And the T word, trillions of dollars in terms of transformative effects of autonomous vehicles

1:12:25.600 --> 1:12:30.760
 and all that started from DARPA and that initial vision of I guess is you're saying

1:12:30.760 --> 1:12:34.560
 of automating part of the military supply chain.

1:12:34.560 --> 1:12:35.560
 Yeah.

1:12:35.560 --> 1:12:36.560
 I did not know that.

1:12:36.560 --> 1:12:37.560
 That's interesting.

1:12:37.560 --> 1:12:41.560
 So they had the same kind of vision for the military as we're not talking about a vision

1:12:41.560 --> 1:12:46.240
 for the civilian, whether it's trucking or whether it's autonomous vehicle, sort of a

1:12:46.240 --> 1:12:48.360
 ride sharing kind of application.

1:12:48.360 --> 1:12:49.360
 Yeah.

1:12:49.360 --> 1:12:53.640
 I mean, what an incredible spark, right?

1:12:53.640 --> 1:12:57.640
 And just the story of what it produced, right?

1:12:57.640 --> 1:13:01.240
 I mean, your own work on self driving, right?

1:13:01.240 --> 1:13:04.320
 I mean, you've studied it as an academic, right?

1:13:04.320 --> 1:13:10.680
 How many great researchers and minds have been harnessed by this outcome of that spark,

1:13:10.680 --> 1:13:11.680
 right?

1:13:11.680 --> 1:13:14.360
 And I think this is sort of theoretically about technology, right?

1:13:14.360 --> 1:13:18.480
 This is what makes it so great is that, this is what makes us human in my opinion, right?

1:13:18.480 --> 1:13:23.520
 Is that you conceive of something in your mind and then you bring it into reality, right?

1:13:23.520 --> 1:13:27.600
 I mean, that's what is so great about it.

1:13:27.600 --> 1:13:32.080
 Sometimes you're too dumb to realize how difficult it is, so you take it off, right?

1:13:32.080 --> 1:13:38.760
 And then eventually you're too, you're in too deep, so you might as well solve the problem.

1:13:38.760 --> 1:13:43.360
 Well, and maybe we're in that situation right now with self driving, but you know, and so

1:13:43.360 --> 1:13:44.360
 let me throw this out there.

1:13:44.360 --> 1:13:49.040
 I'd be curious to hear your thoughts on it, but truck drivers always ask me, like, is

1:13:49.040 --> 1:13:50.040
 this for real?

1:13:50.040 --> 1:13:53.880
 Like, is this really, like, it's harder than they think, like, right?

1:13:53.880 --> 1:13:56.160
 They can't really do this.

1:13:56.160 --> 1:14:01.480
 And you know, at first I was like, look, you know, this is like the Defense Department

1:14:01.480 --> 1:14:08.920
 and like basically the top computer science and robotics departments in the world.

1:14:08.920 --> 1:14:15.360
 And now Silicon Valley with billions of dollars in funding and just, you know, some of the

1:14:15.360 --> 1:14:21.760
 smartest, hardest working, most visionary people focused on what is clearly, you know,

1:14:21.760 --> 1:14:25.040
 a gigantic market, right?

1:14:25.040 --> 1:14:31.960
 And what I tell them is like, if self driving vehicles don't happen, I think this will be

1:14:31.960 --> 1:14:35.680
 the biggest technology failure story in human history.

1:14:35.680 --> 1:14:39.880
 I don't know of anything else that is just galvanized.

1:14:39.880 --> 1:14:43.920
 I mean, you've had people in garages or weird inventors work on things their whole lives

1:14:43.920 --> 1:14:48.280
 and come really close and it never happens and it's a great failure story, right?

1:14:48.280 --> 1:14:52.400
 But never have we had like whole, I mean, we're talking about GM, right?

1:14:52.400 --> 1:14:56.080
 And these are not, you know, these are not tech companies, right?

1:14:56.080 --> 1:14:58.720
 These are industrial giants, right?

1:14:58.720 --> 1:15:03.520
 What were in the 20th century, the pinnacle of industrial production in the world in human

1:15:03.520 --> 1:15:05.480
 history, right?

1:15:05.480 --> 1:15:07.400
 And they're focused on it now.

1:15:07.400 --> 1:15:11.480
 So if we don't pull this off, it's like, wow, you know.

1:15:11.480 --> 1:15:12.560
 It's fascinating to think about.

1:15:12.560 --> 1:15:14.600
 I've never thought of it that way.

1:15:14.600 --> 1:15:21.640
 There was a mass hysteria on a level in terms of excitement and hype on a level that's probably

1:15:21.640 --> 1:15:23.800
 unparalleled in technology space.

1:15:23.800 --> 1:15:27.680
 Like I've seen that kind of hysteria just studying history when you talk about military

1:15:27.680 --> 1:15:28.880
 conflict.

1:15:28.880 --> 1:15:33.700
 So we often wage war with a dream of making a better world and then realize it costs

1:15:33.700 --> 1:15:34.920
 trillions of dollars.

1:15:34.920 --> 1:15:40.240
 And then we step back and like, and go, wait a minute, what do we actually get for this?

1:15:40.240 --> 1:15:44.440
 But in the space of technology, it seems like all these kind of large efforts have paid

1:15:44.440 --> 1:15:45.440
 off.

1:15:45.440 --> 1:15:47.200
 You're right.

1:15:47.200 --> 1:15:53.400
 It seems like, it seems like giving GM and Ford and all these companies now are a little

1:15:53.400 --> 1:16:01.960
 bit like, hey, or Toyota and even Tesla, like, are we sure about this?

1:16:01.960 --> 1:16:02.960
 Yeah.

1:16:02.960 --> 1:16:06.720
 And it's fascinating to think about when you tell the story of this, this could be one

1:16:06.720 --> 1:16:14.560
 of the big first, perhaps, but by far the biggest failures of the dream in the space

1:16:14.560 --> 1:16:16.360
 of technology.

1:16:16.360 --> 1:16:17.800
 It's really interesting to think about.

1:16:17.800 --> 1:16:24.240
 I was a skeptic for a long time because of the human factor, because for business to

1:16:24.240 --> 1:16:28.440
 work in the space, you have to work with humans and you have to work with humans at every

1:16:28.440 --> 1:16:29.440
 level.

1:16:29.440 --> 1:16:32.840
 So in the truck driving space, you have to work with the truck driver, but you also

1:16:32.840 --> 1:16:36.600
 have to work with the society that has a certain conception of what driving means.

1:16:36.600 --> 1:16:43.040
 And also you have to have work with businesses that are not used to this extreme level of

1:16:43.040 --> 1:16:48.720
 technology in the basic operation of their business.

1:16:48.720 --> 1:16:53.480
 So I thought it would be really difficult to move to autonomous vehicles in that way.

1:16:53.480 --> 1:16:58.480
 But then I realized that there are certain companies that are just willing to take big

1:16:58.480 --> 1:17:00.280
 risks and really innovate.

1:17:00.280 --> 1:17:05.840
 I think the first impressive company to me was Waymo, or what used to be the Google

1:17:05.840 --> 1:17:08.080
 Self Driving car.

1:17:08.080 --> 1:17:13.760
 And I saw, okay, here's a company that's willing to really think long term and really

1:17:13.760 --> 1:17:18.520
 try to solve this problem, hire great engineers.

1:17:18.520 --> 1:17:22.600
 Then I saw Tesla with Mobileye when they first had.

1:17:22.600 --> 1:17:25.840
 I thought, actually, Mobileye is the thing that impressed me.

1:17:25.840 --> 1:17:28.760
 When I sat down, I thought, because I'm a computer vision person, I thought there's

1:17:28.760 --> 1:17:35.680
 no way a system could keep me in lane long enough for it to be a pleasant experience

1:17:35.680 --> 1:17:36.680
 for me.

1:17:36.680 --> 1:17:39.920
 So from a computer vision perspective, I thought there'd be too many failures, it'd be really

1:17:39.920 --> 1:17:42.360
 annoying, it'd be a gimmick, a toy.

1:17:42.360 --> 1:17:44.960
 It wouldn't actually create a pleasant experience.

1:17:44.960 --> 1:17:49.160
 And when I first was gotten to Tesla with Mobileye, the initial Mobileye system, it

1:17:49.160 --> 1:17:54.160
 actually held to lane for quite a long time to where I could relax a little bit.

1:17:54.160 --> 1:17:56.080
 And it was a really pleasant experience.

1:17:56.080 --> 1:18:00.000
 I couldn't exactly explain why it's pleasant, because it's not like I still have to really

1:18:00.000 --> 1:18:04.080
 pay attention, but I can relax my shoulders a little bit.

1:18:04.080 --> 1:18:07.080
 I can look around a little bit more.

1:18:07.080 --> 1:18:10.840
 And for some reason, I was really reducing in stress.

1:18:10.840 --> 1:18:16.160
 And then over time, Tesla, with a lot of the revolutionary stuff they're doing on the machine

1:18:16.160 --> 1:18:22.360
 learning space, made me believe that there's opportunities here to innovate, to come up

1:18:22.360 --> 1:18:24.120
 with totally new ideas.

1:18:24.120 --> 1:18:29.200
 Another very sad story that I was really excited about is Cadillac's super cruise system.

1:18:29.200 --> 1:18:34.960
 It is a sad story because I think I vaguely read in the news they just said they're discontinuing

1:18:34.960 --> 1:18:41.680
 super cruise, but it's a nice, innovative way of doing driver attention monitoring and

1:18:41.680 --> 1:18:43.960
 also doing lane keeping.

1:18:43.960 --> 1:18:47.400
 Just innovation could solve this in ways we don't predict.

1:18:47.400 --> 1:18:51.860
 And same with the, in the trucking space, it might not be as simple as like journalists

1:18:51.860 --> 1:18:55.800
 envisioned a few years ago, where everything's just automated.

1:18:55.800 --> 1:19:02.840
 It might be gradually helping out the truck driver in some ways that make their life more

1:19:02.840 --> 1:19:09.200
 efficient, more effective, more pleasant, make the, like, remove some of the inefficiencies

1:19:09.200 --> 1:19:12.680
 that we've been talking about in totally innovative ways.

1:19:12.680 --> 1:19:18.640
 And that, I still have that dream that I believe to solve the fully autonomous driving problem

1:19:18.640 --> 1:19:23.480
 is we're still many years away, but on the way to solving that problem, it feels like

1:19:23.480 --> 1:19:30.200
 there could be, if there's bold risk takers and innovators in this space, there's an opportunity

1:19:30.200 --> 1:19:36.720
 to come up with like subtle technologies that make all the difference.

1:19:36.720 --> 1:19:43.060
 That's actually just what I realized is sometimes little design decisions make all the difference.

1:19:43.060 --> 1:19:45.760
 It's the Blackberry versus the iPhone.

1:19:45.760 --> 1:19:51.360
 You know, why is it that you have a glass and you're using your finger for all of the

1:19:51.360 --> 1:19:54.400
 work versus the buttons makes all the difference?

1:19:54.400 --> 1:20:00.320
 This idea that now that you have a giant screen, so that every part of the experience is now

1:20:00.320 --> 1:20:05.160
 a digital experience, so you can have things like apps that change everything.

1:20:05.160 --> 1:20:10.240
 You can't, you know, when you first think about do I want a keyboard or not on a smartphone,

1:20:10.240 --> 1:20:13.480
 you think it's just the keyboard decision.

1:20:13.480 --> 1:20:20.880
 But then you later realize by removing the keyboard, you're enabling a whole ecosystem

1:20:20.880 --> 1:20:24.560
 of technologies that are inside the phone, and now you're making the smartphone into

1:20:24.560 --> 1:20:25.640
 a computer.

1:20:25.640 --> 1:20:30.600
 And that same way, who knows how you can transform trucks, right?

1:20:30.600 --> 1:20:37.960
 By like automating some parts of it, maybe adding some displays, maybe allows you to maybe

1:20:37.960 --> 1:20:43.400
 giving the truck driver some control in the supply chain to make decisions, all those

1:20:43.400 --> 1:20:45.920
 kinds of things.

1:20:45.920 --> 1:20:56.960
 So I don't know, so where are you on the spectrum of hope for the role of automation in trucking?

1:20:56.960 --> 1:20:59.760
 I think automation is inevitable.

1:20:59.760 --> 1:21:04.800
 And again, I think the, this is really going to be transformative.

1:21:04.800 --> 1:21:10.680
 And it's going to be, I've studied the history of trucking technology as much as I can.

1:21:10.680 --> 1:21:15.240
 There's not a lot of great stuff written, then you kind of have to, you know, there's

1:21:15.240 --> 1:21:19.400
 not a lot of data and places to know volumes of stuff and how they're changing, et cetera.

1:21:19.400 --> 1:21:28.320
 But the big revolutionary changes in trucking are because of constellations of factors.

1:21:28.320 --> 1:21:29.720
 It's not just one thing, right?

1:21:29.720 --> 1:21:35.760
 So Daimler builds a motorized truck, and I think it's 1896, right?

1:21:35.760 --> 1:21:40.960
 Intercity's trucking, so basically what they use that truck for is just to swap out horses,

1:21:40.960 --> 1:21:41.960
 right?

1:21:41.960 --> 1:21:42.960
 They basically do the same thing.

1:21:42.960 --> 1:21:48.760
 The service doesn't really change, you know, and then World War I really spurs the development

1:21:48.760 --> 1:21:54.760
 of bigger, larger trucks, like it spreads, you know, air filled tires.

1:21:54.760 --> 1:21:58.120
 And then we start paving roads, right?

1:21:58.120 --> 1:22:04.720
 And paved roads, right, air filled tires, and the internal combustion engine, now you've

1:22:04.720 --> 1:22:05.840
 got a winning mix.

1:22:05.840 --> 1:22:10.600
 Now it met with demand for people who wanted to get out from under the thumb of the rail

1:22:10.600 --> 1:22:12.280
 roads, right?

1:22:12.280 --> 1:22:18.600
 So there was all of this pent up demand to get cheaper freight from the countryside into

1:22:18.600 --> 1:22:23.000
 cities and between cities that typically had to go by rail.

1:22:23.000 --> 1:22:29.440
 And so now, you know, 40 years after that internal combustion engine, it becomes this

1:22:29.440 --> 1:22:34.440
 absolutely essential, right, this necessary but not sufficient piece of technology to

1:22:34.440 --> 1:22:39.040
 create the modern trucking industry in the 1930s.

1:22:39.040 --> 1:22:43.800
 And I think self driving is going to be self driving trucks are going to be part of that.

1:22:43.800 --> 1:22:51.280
 And the idea, I don't know, I guess we credit Jeff Bezos, the idea is, you know, okay,

1:22:51.280 --> 1:22:55.800
 so Sam Walton, if we can do it like a slight tangent on sort of the importance of trucking

1:22:55.800 --> 1:23:00.960
 to business strategy and sort of how it has transformed our world.

1:23:00.960 --> 1:23:06.920
 The central insight that Sam Walton had that made him the giant that he was in influencing

1:23:06.920 --> 1:23:12.720
 the way that so many people get stuff was a trucking insight.

1:23:12.720 --> 1:23:19.600
 And so if you look at the way that he developed his system, you build a distribution center

1:23:19.600 --> 1:23:25.800
 and then you ring it with stores, those stores are never further out from that distribution

1:23:25.800 --> 1:23:31.920
 center than a human driven truck can drive back and forth in one day.

1:23:31.920 --> 1:23:37.400
 And so rather than the way all of his competitors were doing it with sending trucks all over

1:23:37.400 --> 1:23:43.200
 the place and having people sleep overnight and sort of making the trucking service fit

1:23:43.200 --> 1:23:50.480
 where they had stores, he designed the layout of the stores, right, to fit what trucks could

1:23:50.480 --> 1:23:51.520
 do.

1:23:51.520 --> 1:23:57.400
 And so transportation and logistics, right, become Walmart's, you know, edge, right, and

1:23:57.400 --> 1:23:59.560
 allows them to dominate the space.

1:23:59.560 --> 1:24:05.840
 That's the challenge that Amazon has now, they've mastered the digital part of it, right,

1:24:05.840 --> 1:24:11.000
 and now they got to figure out like, how do we, you know, dominate the actual physical

1:24:11.000 --> 1:24:16.800
 movement that complements that others are obviously going to follow.

1:24:16.800 --> 1:24:22.840
 But the capabilities of these trucks is completely different than the capability of a human driven

1:24:22.840 --> 1:24:23.840
 truck.

1:24:23.840 --> 1:24:30.240
 So if you're Smith packing, right, and you've got, you know, a bunch of meat in a warehouse

1:24:30.240 --> 1:24:35.480
 and it's going to grocery distribution centers, you know, you have that trucker probably come

1:24:35.480 --> 1:24:41.000
 in the night before and you make him wait so that he has, you know, a full 10 hour

1:24:41.000 --> 1:24:45.920
 break, which is what the law requires so that he can get to the furthest reaches that he

1:24:45.920 --> 1:24:51.560
 can of one of those stores, right, so he can drive his full 11 hours and bring that meat

1:24:51.560 --> 1:24:55.840
 so it doesn't have to sit overnight in that refrigerated trailer, right, and so their

1:24:55.840 --> 1:24:57.520
 system is based on that.

1:24:57.520 --> 1:25:04.800
 Now, what happens when that truck can now travel two times as far, right, three times

1:25:04.800 --> 1:25:10.720
 as far, now you don't need the warehouses where they were, now you can go super lean

1:25:10.720 --> 1:25:14.760
 with your inventory instead of having meat here, meat there, meat there, you can put

1:25:14.760 --> 1:25:20.440
 it all right here, and if it's cheap enough, substitute those transportation costs for

1:25:20.440 --> 1:25:25.240
 all that warehousing costs, right, so this is going to remake landscapes in the same

1:25:25.240 --> 1:25:31.000
 way that big box supply chains did, right, and then of course the further compliment

1:25:31.000 --> 1:25:37.960
 of that is, you know, how do you then get it to two people at their door, right, and

1:25:37.960 --> 1:25:44.280
 you know, the big box supply chain, it moves very few items in really large quantities

1:25:44.280 --> 1:25:52.600
 to very few locations pretty slowly, right, eCommerce aspires, you know, to do something

1:25:52.600 --> 1:25:58.760
 completely different, right, move huge varieties of things in small quantities virtually everywhere

1:25:58.760 --> 1:26:06.240
 as fast as possible, right, and so that is like that intercity trucking under the, you

1:26:06.240 --> 1:26:12.960
 know, in the era of railroad monopolies, right, the demand for that is potentially

1:26:12.960 --> 1:26:20.320
 enormous, right, and so there's such a, so right now I think a lot of the business plans

1:26:20.320 --> 1:26:24.280
 for sort of automated trucks, right, and sort of the way that the journalistic accounts

1:26:24.280 --> 1:26:29.640
 portray it is like, okay, if we swap out a human for a computer, what are the labor costs

1:26:29.640 --> 1:26:34.640
 per mile, and like, oh, here's the profitability of self driving trucks, uh uh, like this is

1:26:34.640 --> 1:26:38.600
 transformative technology, we're going to change the way we get stuff.

1:26:38.600 --> 1:26:43.400
 So we'll actually get a lot more trucks, period, with like, with autonomous trucks, because

1:26:43.400 --> 1:26:47.400
 they would enable a very different kind of transportation networks, you think.

1:26:47.400 --> 1:26:54.720
 Yeah, here's, and this is where it's like, uh oh, like, yeah, so we really thought we

1:26:54.720 --> 1:26:57.520
 were going to be electrifying trucks.

1:26:57.520 --> 1:27:01.840
 If they're going twice as far, if they're moving three times as much, if they're going

1:27:01.840 --> 1:27:06.560
 three times as far, right, what does that mean for how far we are behind on batteries,

1:27:06.560 --> 1:27:09.880
 right, we've got sort of these, you know, ideas about like, man, we, you know, here's

1:27:09.880 --> 1:27:12.960
 how far, how it's how close we could get to meet this demand.

1:27:12.960 --> 1:27:17.080
 That demand is going to radically change, right, these trucks are, you know, so then

1:27:17.080 --> 1:27:21.400
 we've got to think about, all right, if it's not batteries, you know, how are we, how are

1:27:21.400 --> 1:27:25.840
 we powering these things, and how many of them are they're going to be, like, right

1:27:25.840 --> 1:27:34.440
 now we've got 5 million containers that move from LA and Long Beach to Chicago on rail.

1:27:34.440 --> 1:27:40.880
 Rail is three or four times at least more efficient than trucks in terms of greenhouse

1:27:40.880 --> 1:27:42.800
 gas emissions.

1:27:42.800 --> 1:27:48.440
 And on that lane, it varies a lot depending on demand, but maybe rail has a 20% advantage

1:27:48.440 --> 1:27:52.240
 in cost, maybe 25%, but it's a couple of days slower.

1:27:52.240 --> 1:27:58.920
 So now you cut the cost of that truck transportation per mile by 30%, now it's cheaper than rail

1:27:58.920 --> 1:28:02.640
 and it gets the stuff there five days faster than rail.

1:28:02.640 --> 1:28:07.240
 How many millions of containers are going to leave LA and Long Beach on self driving

1:28:07.240 --> 1:28:08.960
 trucks and go to Chicago?

1:28:08.960 --> 1:28:17.160
 And it might look very much like a train if we go with a platooning solution, these rows

1:28:17.160 --> 1:28:23.120
 of like, imagine like rows of like 10, like dozens of trucks or like hundreds of trucks,

1:28:23.120 --> 1:28:30.120
 like some absurd situation, just going from LA to Chicago, just this train but taking

1:28:30.120 --> 1:28:31.760
 up a highway.

1:28:31.760 --> 1:28:37.240
 I mean, this is probably a good place to talk about various scenarios.

1:28:37.240 --> 1:28:43.320
 Well, before we get there, can I just make one interesting observation that I made as

1:28:43.320 --> 1:28:48.080
 a driver, when you're in a truck you're up higher, so you can see further and you can

1:28:48.080 --> 1:28:54.120
 see the traffic patterns and cars move in packs.

1:28:54.120 --> 1:28:57.600
 I'm sure there's academic research on this, right, but they move in packs, they kind of

1:28:57.600 --> 1:29:02.840
 bunch up behind a slower car and then a bunch of them break free and this is sort of almost

1:29:02.840 --> 1:29:06.680
 free flowing highways, they kind of move in packs and you can kind of see them in the

1:29:06.680 --> 1:29:07.680
 truck.

1:29:07.680 --> 1:29:11.960
 So, you know, rather than platoons, we might have like hives, you know, of trucks, right?

1:29:11.960 --> 1:29:16.040
 So you have like 20 trucks moving in some coordinated fashion, right?

1:29:16.040 --> 1:29:19.720
 And then maybe the self driving cars are, you know, because people don't like to be around

1:29:19.720 --> 1:29:21.400
 them or whatever it is, right?

1:29:21.400 --> 1:29:25.480
 You might have a pod of, you know, 20 self driving cars sort of moving in a packet behind

1:29:25.480 --> 1:29:26.640
 them, you know?

1:29:26.640 --> 1:29:33.160
 This is what, if the aliens came down or were just observing cars, which is one of the sort

1:29:33.160 --> 1:29:37.800
 of prevalent characteristics of human civilization is there seems to be these cars like moving

1:29:37.800 --> 1:29:42.440
 around that would do this kind of analysis of like, huh, what's the interesting clustering

1:29:42.440 --> 1:29:48.960
 of situations here, especially with autonomous vehicles, I like this.

1:29:48.960 --> 1:29:56.120
 Okay, so what technologically speaking do you see are the different scenarios of increasing

1:29:56.120 --> 1:29:58.280
 automation in trucks?

1:29:58.280 --> 1:30:00.960
 What are some ideas that you think about?

1:30:00.960 --> 1:30:06.440
 For the most part, I have no influence on sort of what these ideas were.

1:30:06.440 --> 1:30:13.400
 So what the project was that I did was I said, technology is created by people, they solve

1:30:13.400 --> 1:30:17.800
 for X and they have some conception of what they want to do.

1:30:17.800 --> 1:30:21.840
 And that's where we should start in sort of thinking about what the, you know, impacts

1:30:21.840 --> 1:30:22.840
 might be.

1:30:22.840 --> 1:30:27.680
 And then I talked to everybody I could find who was, you know, thinking about developing

1:30:27.680 --> 1:30:28.880
 a self driving truck.

1:30:28.880 --> 1:30:32.800
 And the question was essentially, you know, what are you, what are you trying to build?

1:30:32.800 --> 1:30:35.800
 Like, what do you envision this thing doing?

1:30:35.800 --> 1:30:40.520
 It turned out that that for a lot of them was an afterthought.

1:30:40.520 --> 1:30:45.400
 They knew the, the sort of technological capabilities that a self driving vehicle would have.

1:30:45.400 --> 1:30:49.040
 And those were the problems that they were tackling, you know, they were engineers and

1:30:49.040 --> 1:30:50.040
 computer scientists.

1:30:50.040 --> 1:30:53.680
 Oh, robotics people, I love you so much.

1:30:53.680 --> 1:30:58.320
 This is the, I could talk forever about this, but yes, there's a technology problem.

1:30:58.320 --> 1:31:02.920
 Let's focus on that and we'll figure out the actual impact on society, how it's actually

1:31:02.920 --> 1:31:07.200
 going to be applied, how it's actually going to be integrated from a policy and from a

1:31:07.200 --> 1:31:10.040
 human perspective, from a business perspective later.

1:31:10.040 --> 1:31:11.640
 First let's solve the technology problem.

1:31:11.640 --> 1:31:14.400
 That's not how life works, friends, but okay, I'm sorry.

1:31:14.400 --> 1:31:15.400
 Yeah, yeah.

1:31:15.400 --> 1:31:19.000
 So I mean, you know, I'm sure you know the division of labor in these companies, right?

1:31:19.000 --> 1:31:22.240
 They're sort of a business development side, you know, and then there's the engineering

1:31:22.240 --> 1:31:23.240
 side, right?

1:31:23.240 --> 1:31:26.040
 And the engineers are like, oh my God, what are these business development people, you

1:31:26.040 --> 1:31:30.160
 know, why are they involved, you know, in this process?

1:31:30.160 --> 1:31:36.080
 So I ended up sort of coming up with a few different ideas that people seem to be batting

1:31:36.080 --> 1:31:42.520
 around and then really tried to zero in on a layman's understanding of the limitations,

1:31:42.520 --> 1:31:43.520
 right?

1:31:43.520 --> 1:31:47.760
 And it turns out that's really obvious and quite simple.

1:31:47.760 --> 1:31:50.040
 Highway driving is a lot simpler, right?

1:31:50.040 --> 1:31:53.920
 So you know, the plan is simplify the problem, right?

1:31:53.920 --> 1:31:59.960
 And focus on highways because city driving is so much more complicated.

1:31:59.960 --> 1:32:07.120
 So from that, I came up with basically six scenarios, actually I came up with five that

1:32:07.120 --> 1:32:08.720
 the developers were talking about.

1:32:08.720 --> 1:32:15.120
 And then one that I thought was a good idea that I had read about, I think in like 2013

1:32:15.120 --> 1:32:20.480
 or 2014, which was actually something that the US military was looking at.

1:32:20.480 --> 1:32:26.680
 I actually first heard about the idea of this kind of automation, at least in sketched out

1:32:26.680 --> 1:32:33.920
 form in like 2011, I guess it was with Peloton, which was this sort of early technology entrant

1:32:33.920 --> 1:32:39.800
 into the trucking industry, which was working on platooning trucks.

1:32:39.800 --> 1:32:43.600
 And all they were doing was, you know, a cooperative adaptive cruise control, as they

1:32:43.600 --> 1:32:46.120
 came to call it.

1:32:46.120 --> 1:32:48.480
 And we ended up on a panel together.

1:32:48.480 --> 1:32:54.280
 And it's kind of interesting because I was on that panel because I was thinking about

1:32:54.280 --> 1:32:58.720
 how we got the best return on investment for fuel efficient technologies.

1:32:58.720 --> 1:33:03.480
 And if it's cool, I'll sort of set this up because it comes into sort of the story of

1:33:03.480 --> 1:33:05.840
 some of these scenarios.

1:33:05.840 --> 1:33:15.160
 So when I studied the drivers, you had this like complete difference in the driving tasks

1:33:15.160 --> 1:33:19.240
 like we were talking about before with Long Hall and Citi, right?

1:33:19.240 --> 1:33:25.960
 And you're not paid in the city, you've got congestion, the turns are tight, there's lots

1:33:25.960 --> 1:33:30.120
 of, you know, pedestrians, you know, all the things that self driving trucks don't like,

1:33:30.120 --> 1:33:32.320
 truckers don't like, right?

1:33:32.320 --> 1:33:35.800
 And they're not paid, there's lots of waiting time.

1:33:35.800 --> 1:33:39.840
 And then in the highway, they get to cruise, they're getting paid, they have control, they

1:33:39.840 --> 1:33:42.960
 go at their own pace, they're making money, they're happy.

1:33:42.960 --> 1:33:47.320
 Well, it turned out, I guess it was around 2010, this is still when we were thinking

1:33:47.320 --> 1:33:53.440
 about regenerative braking, you know, and hybrid trucks being sort of like the solution.

1:33:53.440 --> 1:33:59.240
 The problems with them, sort of, and the advantages, you know, also split on what I was thinking

1:33:59.240 --> 1:34:02.440
 of is kind of the rural urban divide at that time, right?

1:34:02.440 --> 1:34:05.200
 Like you got the regenerative braking, right?

1:34:05.200 --> 1:34:08.480
 You can make the truck lighter, you can keep it local, right?

1:34:08.480 --> 1:34:15.800
 You don't get any benefit from that, you know, hybrid electric in the rural highway, you

1:34:15.800 --> 1:34:18.160
 want aerodynamics, right?

1:34:18.160 --> 1:34:22.720
 There you want low rolling resistance tires and these super aerodynamics sleek trucks,

1:34:22.720 --> 1:34:23.720
 right?

1:34:23.720 --> 1:34:28.000
 Where we know with off the shelf technology today, we could double the fuel economy more

1:34:28.000 --> 1:34:33.160
 than double the fuel economy of the typical truck in that highway segment, if we segmented

1:34:33.160 --> 1:34:34.880
 the duty cycle, right?

1:34:34.880 --> 1:34:38.400
 And so in the urban environment, you want a clean burning truck, so you're not giving

1:34:38.400 --> 1:34:44.600
 kids asthma, you want it lighter, so it's not destroying those less strong pavements,

1:34:44.600 --> 1:34:45.600
 right?

1:34:45.600 --> 1:34:49.120
 You're not, you can make tighter turns, you don't need a sleeper cab because the driver,

1:34:49.120 --> 1:34:51.320
 you know, hopefully is getting home at night, right?

1:34:51.320 --> 1:34:55.320
 In the long haul, you want that super aerodynamic stuff, now that doesn't get you anything in

1:34:55.320 --> 1:34:58.920
 the city and in fact it causes all kinds of problems because you turn too tight, you crunch

1:34:58.920 --> 1:35:03.040
 up all the aerodynamics that connect the tractor and the trailer.

1:35:03.040 --> 1:35:08.200
 So the idea that I had was like, okay, what if we deliberately segmented it?

1:35:08.200 --> 1:35:15.000
 Like what if we created these drop lots outside cities where, you know, a local city driver

1:35:15.000 --> 1:35:19.240
 who's paid by the hour kind of runs these trailers out once they're loaded, you know,

1:35:19.240 --> 1:35:22.200
 doesn't sit there and wait while it's being loaded, they drop off a trailer, they go pick

1:35:22.200 --> 1:35:25.440
 up one that's loaded, they run it out, when it's loaded, they call them and they just

1:35:25.440 --> 1:35:26.840
 run them out there and stage them.

1:35:26.840 --> 1:35:29.720
 It's like an Uber driver, but for truckloads.

1:35:29.720 --> 1:35:34.120
 Yeah, and we have like intermodal, we have like, we have basically this would be the

1:35:34.120 --> 1:35:37.240
 equivalent of like rail to truck intermodal, right?

1:35:37.240 --> 1:35:40.560
 So you put it on the rail and then, you know, a trucker picks it up and delivers it, right?

1:35:40.560 --> 1:35:44.840
 So instead of having the rail, you'd have these super aerodynamic hopefully platoons

1:35:44.840 --> 1:35:50.040
 or what was at the time was called long combination vehicles, which is basically two trailers

1:35:50.040 --> 1:35:54.200
 connected together, right, because this is like a huge productivity gain, right?

1:35:54.200 --> 1:35:57.720
 And then instead of that driver like me, I would pick up something in upstate New York,

1:35:57.720 --> 1:36:01.680
 drive to Michigan, drive to Alabama, you know, drive to Wisconsin, drive to Florida, you

1:36:01.680 --> 1:36:03.280
 know, I get home every two weeks.

1:36:03.280 --> 1:36:09.440
 If I'm just running that, you know, that double trailer, I might be able to go back and forth

1:36:09.440 --> 1:36:11.840
 from Chicago to Detroit, right?

1:36:11.840 --> 1:36:15.840
 Take two trailers there, pick up two trailers going back, right, and be home every night.

1:36:15.840 --> 1:36:20.480
 Now, the problem with that at the time or one of them was, you know, bridge weights.

1:36:20.480 --> 1:36:25.840
 So you can't, not all bridges can handle that, that much weight on them, they can't handle

1:36:25.840 --> 1:36:26.840
 these doubles, right?

1:36:26.840 --> 1:36:28.680
 And some places can, some places can't.

1:36:28.680 --> 1:36:31.800
 So this platooning idea was happening at the same time.

1:36:31.800 --> 1:36:36.600
 And we ended up on the same panel and the founders were like, hey, so what's it like

1:36:36.600 --> 1:36:39.120
 to follow really close behind another truck?

1:36:39.120 --> 1:36:42.440
 Which was kind of the stage that they were at was like, you know, what's that experience

1:36:42.440 --> 1:36:43.440
 going to be like?

1:36:43.440 --> 1:36:46.000
 And I was like, truckers aren't going to like it.

1:36:46.000 --> 1:36:50.440
 You know, I mean, that's just like the cardinal rule is following distance, like that's the

1:36:50.440 --> 1:36:53.240
 one you really shouldn't violate, right?

1:36:53.240 --> 1:36:56.320
 And when you're out on the road, like you have that trucker, like right on your ass,

1:36:56.320 --> 1:37:02.000
 you know, people remember that they don't remember the 99.9% of truckers that are not

1:37:02.000 --> 1:37:05.400
 on their ass, you know, like they, they're very careful about that.

1:37:05.400 --> 1:37:08.640
 But when you're, when the trucks are really close together, there's benefits in terms

1:37:08.640 --> 1:37:10.200
 of aerodynamics.

1:37:10.200 --> 1:37:16.920
 So that's the idea, so like if you want to get some benefits of a platoon, you want them

1:37:16.920 --> 1:37:20.000
 to be close together, but you're saying that's very uncomfortable for truckers.

1:37:20.000 --> 1:37:21.000
 Yeah.

1:37:21.000 --> 1:37:24.240
 So, I mean, I think that ended up at the, I mean, Peloton, I think is sort of winding

1:37:24.240 --> 1:37:27.920
 down their, their work on this.

1:37:27.920 --> 1:37:32.640
 And I think that ended up being still an open question, like, and I had a chance to interview

1:37:32.640 --> 1:37:37.040
 a couple of drivers who at least one, maybe two of which had actually driven in their

1:37:37.040 --> 1:37:41.440
 platoons, and I got completely different experiences.

1:37:41.440 --> 1:37:44.800
 Some of them were like, it's really cool, you know, I'm like in communication with that

1:37:44.800 --> 1:37:50.240
 other driver, you know, I can see on a screen what, what's out, out, you know, the front

1:37:50.240 --> 1:37:54.760
 of his truck, and then somewhere like it's too close, and it might be one of those things

1:37:54.760 --> 1:37:57.560
 that's just, you know, takes an adjustment to, to sort of get there.

1:37:57.560 --> 1:38:02.040
 So you get the aerodynamic advantage, which, which, you know, saves fuel.

1:38:02.040 --> 1:38:03.480
 There's some problems though, right?

1:38:03.480 --> 1:38:08.360
 So, you know, you're getting that aerodynamic advantage because there's a low pressure system

1:38:08.360 --> 1:38:10.560
 in front of that following truck.

1:38:10.560 --> 1:38:16.000
 But the, the engine is designed with higher pressure feeding that engine, right?

1:38:16.000 --> 1:38:19.520
 So there's, there are sort of adjustments that you need to make and, you know, still

1:38:19.520 --> 1:38:22.000
 the benefits are, are there.

1:38:22.000 --> 1:38:23.000
 That's one scenario.

1:38:23.000 --> 1:38:27.000
 And that's just the automation of that acceleration and braking.

1:38:27.000 --> 1:38:33.680
 Starsky, which, you know, probably a lot of your listeners heard, heard about, was working

1:38:33.680 --> 1:38:38.640
 on another scenario, which was, you know, to solve that local problem was going to do

1:38:38.640 --> 1:38:41.680
 teleoperation, right, sort of remote piloting.

1:38:41.680 --> 1:38:46.280
 I had the chance to, you know, sort of watch, watch them do that.

1:38:46.280 --> 1:38:53.080
 It was, you know, they drove a truck in Florida from, from San Francisco in one of their offices.

1:38:53.080 --> 1:38:54.880
 That was, that was really interesting.

1:38:54.880 --> 1:38:59.560
 And then in case it's not clear, teleoperation means you're controlling the truck remotely,

1:38:59.560 --> 1:39:01.960
 like it's a video game.

1:39:01.960 --> 1:39:06.600
 So you've gotten the chance to witness it, does it actually work?

1:39:06.600 --> 1:39:07.600
 Yeah.

1:39:07.600 --> 1:39:10.200
 I mean, so it's, what are the pros and cons?

1:39:10.200 --> 1:39:13.920
 You know, one of the problems with, with doing research like this, with, with all these,

1:39:13.920 --> 1:39:15.880
 with all these Silicon Valley folks to the NDAs.

1:39:15.880 --> 1:39:16.880
 Oh, right.

1:39:16.880 --> 1:39:17.880
 Right.

1:39:17.880 --> 1:39:22.120
 So, so I don't, you know, I don't know what I'm able to say about sort of watching it.

1:39:22.120 --> 1:39:25.560
 But obviously the, their public statements about sort of what the challenges are, right?

1:39:25.560 --> 1:39:31.280
 And it's about the, the latency and the ability to sort of in real time.

1:39:31.280 --> 1:39:32.280
 There's challenges.

1:39:32.280 --> 1:39:33.720
 Let me say one thing.

1:39:33.720 --> 1:39:39.520
 So I'm talking to the, you know, I've talked to the Waymo CTO, I'm in conversations with

1:39:39.520 --> 1:39:40.520
 them.

1:39:40.520 --> 1:39:45.920
 I'm talking to the, the head of trucking Boris Softman in next month, actually, I'm a huge

1:39:45.920 --> 1:39:52.360
 fan of his because he was, I think the founder of Anki, which is a toy robotics company.

1:39:52.360 --> 1:39:58.920
 So I love cute, I love human robot interaction and he created one of the most effective and

1:39:58.920 --> 1:40:02.400
 beautiful toy robots.

1:40:02.400 --> 1:40:10.720
 Anyway, I keep complaining to them on email privately that there's way too much marketing

1:40:10.720 --> 1:40:15.400
 in these conversations and not enough showing off the, both the challenge and the beauty

1:40:15.400 --> 1:40:17.120
 of the engineering efforts.

1:40:17.120 --> 1:40:21.720
 And that seems to be the case for a lot of these Silicon Valley tech companies.

1:40:21.720 --> 1:40:30.000
 They put up this, you're talking about NDAs, they've, for some reason, rightfully wrongfully,

1:40:30.000 --> 1:40:37.480
 because there's been so much hype and so much money being made, they don't see the upside

1:40:37.480 --> 1:40:43.520
 in being transparent and educating the public about how difficult the problem is.

1:40:43.520 --> 1:40:46.840
 It's much more effective for them to say, we have everything solved.

1:40:46.840 --> 1:40:47.960
 This will change everything.

1:40:47.960 --> 1:40:52.240
 This will change society, as we know, and just kind of wave their hands as opposed to exploring

1:40:52.240 --> 1:40:56.000
 together like these different scenarios, what are the pros and cons?

1:40:56.000 --> 1:40:57.840
 Why is it really difficult?

1:40:57.840 --> 1:41:02.080
 You know, what are the, what are the gray areas of where it works and doesn't?

1:41:02.080 --> 1:41:06.640
 What's the role of the human in this picture of the, both the sort of the operators and

1:41:06.640 --> 1:41:12.040
 the other humans on the road, all of that, which are fascinating human problems, fascinating

1:41:12.040 --> 1:41:17.280
 engineering problems that I wish we could have a conversation about, as opposed to always

1:41:17.280 --> 1:41:23.000
 feeling like it's just marketing talk, because a lot of what we're talking about now, even

1:41:23.000 --> 1:41:28.800
 you with having private conversations under NDA, you still don't have the full picture

1:41:28.800 --> 1:41:31.520
 of everything, of how difficult this problem is.

1:41:31.520 --> 1:41:37.840
 One of the big questions I've had, still have is how difficult is driving, of disagree

1:41:37.840 --> 1:41:40.960
 with Elon Musk and Jim Keller on this point.

1:41:40.960 --> 1:41:45.520
 I have a sense that driving is really difficult.

1:41:45.520 --> 1:41:50.600
 You know, the task of driving, just broadly, this is like philosophy talk, how, how much

1:41:50.600 --> 1:41:55.520
 intelligence is required to drive a car?

1:41:55.520 --> 1:42:01.200
 So from a, like a Jim Keller, who used to be the head of autopilot, the idea is that

1:42:01.200 --> 1:42:05.400
 it's just a collision avoidance problem, it's like billiard balls.

1:42:05.400 --> 1:42:08.960
 It's like you have to convert the drive, you have to do some basic perception, a computer

1:42:08.960 --> 1:42:14.160
 vision to convert driving into a game of pool, and then you just have to get everything

1:42:14.160 --> 1:42:15.960
 into a pocket.

1:42:15.960 --> 1:42:20.560
 To me, there seems to be some game theoretic dance, combined with the fact that people's

1:42:20.560 --> 1:42:25.200
 life is at stake, and then when people die at the hands of a robot, the reaction is going

1:42:25.200 --> 1:42:26.600
 to be much more complicated.

1:42:26.600 --> 1:42:29.040
 So all of that, but that's still an open question.

1:42:29.040 --> 1:42:35.680
 And the cool thing is all of these companies are struggling with this question of how difficult

1:42:35.680 --> 1:42:39.880
 is it to solve this problem sufficiently such that we can build the business on top of it

1:42:39.880 --> 1:42:44.200
 and have a product that's going to make a huge amount of money and compete with the manually

1:42:44.200 --> 1:42:46.840
 driven vehicles.

1:42:46.840 --> 1:42:50.920
 And so their teleoperation from Starsky's is really interesting idea.

1:42:50.920 --> 1:42:57.000
 How much can, I mean, there's a few autonomous vehicle companies that tried to integrate

1:42:57.000 --> 1:42:59.160
 teleoperation in the picture.

1:42:59.160 --> 1:43:08.840
 Can we reduce some of the costs while still having reliability, like catch when the vehicle

1:43:08.840 --> 1:43:11.760
 fails by having teleoperation?

1:43:11.760 --> 1:43:13.920
 It's an open question.

1:43:13.920 --> 1:43:20.080
 So that's for you scenario number two, is to use teleoperation as part of the picture.

1:43:20.080 --> 1:43:21.080
 Yeah.

1:43:21.080 --> 1:43:25.800
 Let me follow up on that question of how hard driving is, because this becomes a big question

1:43:25.800 --> 1:43:31.440
 for researchers who are thinking about labor market impacts, because we start from a perspective

1:43:31.440 --> 1:43:35.240
 of what's hard or easy for humans.

1:43:35.240 --> 1:43:40.560
 And so if you were to look at truck driving prior to a lot, I mean, there's been a lot

1:43:40.560 --> 1:43:47.800
 of thinking and debate in academic research circles around how you estimate labor impacts,

1:43:47.800 --> 1:43:49.280
 what these models look like.

1:43:49.280 --> 1:43:52.440
 And a lot of it is about how automatable is a job.

1:43:52.440 --> 1:43:56.520
 Technic recognition, really easy for people, really hard for computers.

1:43:56.520 --> 1:44:03.800
 And so there's a whole bunch of things that truck drivers do that we see as super easy

1:44:03.800 --> 1:44:07.960
 and as it would have been characterized 10 years ago, routine.

1:44:07.960 --> 1:44:11.400
 And it's not for a computer.

1:44:11.400 --> 1:44:18.000
 It turns out to be something that we do naturally that is cutting edge, computer science.

1:44:18.000 --> 1:44:25.000
 So on the teleoperation question, I think this is a more interesting one than people

1:44:25.000 --> 1:44:29.960
 would like to sort of let on, I think, publicly.

1:44:29.960 --> 1:44:32.240
 There are going to be problems, right?

1:44:32.240 --> 1:44:35.520
 And this is one of the complexities of sort of putting these things out in the world.

1:44:35.520 --> 1:44:40.960
 And if you see the real world of trucking, you realize, wow, it's rough.

1:44:40.960 --> 1:44:41.960
 There are dirt lots.

1:44:41.960 --> 1:44:42.960
 There's gravel.

1:44:42.960 --> 1:44:44.720
 There's salt and ice and cold weather.

1:44:44.720 --> 1:44:48.400
 And there's equipment that just gets left out in the middle of nowhere and the brakes

1:44:48.400 --> 1:44:53.440
 don't get maintained and somebody was supposed to service something and they didn't.

1:44:53.440 --> 1:44:57.840
 And so you imagine, OK, we've got this vehicle that can drive itself, which is going to require

1:44:57.840 --> 1:45:02.680
 a whole lot of sensors to tell it that the doors are still closed and the trailer is

1:45:02.680 --> 1:45:07.960
 still hooked up and each of the tires has adequate pressure and any number of probably

1:45:07.960 --> 1:45:11.960
 hundreds of sensors that are going to be sort of relaying information.

1:45:11.960 --> 1:45:20.240
 And one of them, after 500,000 miles or whatever goes out, now, do we have some fleet of technicians

1:45:20.240 --> 1:45:24.360
 sort of continually cruising the highways and sort of servicing these things as they

1:45:24.360 --> 1:45:25.360
 do what?

1:45:25.360 --> 1:45:28.960
 Pull themselves off to the side of the road and say, I've got a sensor fault, I'm pulling

1:45:28.960 --> 1:45:36.440
 over, or maybe there's some level of safety critical faults or whatever it might be.

1:45:36.440 --> 1:45:44.000
 So that suggests that there might be a role for teleoperation, even with self driving.

1:45:44.000 --> 1:45:50.200
 And when I push people on it in the conversations, they all are like, yeah, we kind of have that

1:45:50.200 --> 1:45:57.120
 on the bottom of the list, figure out how to rescue truck on the to do list, right?

1:45:57.120 --> 1:46:02.280
 After solving the self driving question is like, yeah, what do we do with the problems,

1:46:02.280 --> 1:46:03.280
 right?

1:46:03.280 --> 1:46:07.160
 We could we can imagine like, all right, we have some, you know, protocol that the truck

1:46:07.160 --> 1:46:12.680
 is not, you know, realizes the system says, not safe for operation, pull to the side.

1:46:12.680 --> 1:46:17.160
 Good, you have a crash, but now you got a truck stranded on the side of the road, you're

1:46:17.160 --> 1:46:21.720
 going to send out somebody to like calibrate things and check out what's going on or that

1:46:21.720 --> 1:46:26.160
 sounds like expensive labor, it sounds like downtime, it sounds like the kind of things

1:46:26.160 --> 1:46:30.280
 that shippers don't like to happen to their freight, you know, in a in a just in time

1:46:30.280 --> 1:46:34.760
 world, and so wouldn't it be great if you could just sort of, you know, loop your way

1:46:34.760 --> 1:46:39.560
 into the controls of that truck and say, all right, we've got a sensor out says me that

1:46:39.560 --> 1:46:43.840
 says that the tire is bad, but I can see visually from the camera looks fine, I'm going to drive

1:46:43.840 --> 1:46:48.640
 it to our next depot, you know, maybe the next rider or Penske location, right, sort

1:46:48.640 --> 1:46:52.760
 of all these service locations around and have a technician take a look at it.

1:46:52.760 --> 1:47:01.440
 So teleoperation often gets this, you know, so dismissive, you know, commentary from from

1:47:01.440 --> 1:47:04.200
 other folks working on other other scenarios.

1:47:04.200 --> 1:47:10.040
 But I think it's it's potentially more relevant than than than we we hear publicly, but it's

1:47:10.040 --> 1:47:18.600
 a hard problem and you know, for me, I've gotten a chance to interact with people that

1:47:18.600 --> 1:47:25.680
 take on hard problems and solve them and they're rare, what Tesla has done with their data engine.

1:47:25.680 --> 1:47:31.400
 So I thought autonomous driving cannot be solved without collecting a huge amount of data and

1:47:31.400 --> 1:47:35.200
 organizing well, not just collecting but organizing it.

1:47:35.200 --> 1:47:39.560
 And exactly what Tesla is doing now is what I thought it'll be like I couldn't see car

1:47:39.560 --> 1:47:42.280
 companies doing that, including Tesla.

1:47:42.280 --> 1:47:46.320
 And now that they're doing that, it's like, oh, okay, so it's possible to take on this

1:47:46.320 --> 1:47:52.040
 huge effort seriously, to me, teleoperation is another huge effort like that.

1:47:52.040 --> 1:47:57.160
 It's like taking seriously what happens when it fails.

1:47:57.160 --> 1:48:02.800
 What's the in the case of Waymo for for the consumer, like ride sharing, what's the customer

1:48:02.800 --> 1:48:03.800
 experience like?

1:48:03.800 --> 1:48:08.920
 There's a bunch of videos online now where people are like, the car fails and it pulls

1:48:08.920 --> 1:48:12.600
 off to the side and you call that customer service and you're basically sitting there

1:48:12.600 --> 1:48:17.120
 for a long time and there's confusion and then there's a rescue that comes and they

1:48:17.120 --> 1:48:18.120
 start to drive.

1:48:18.120 --> 1:48:23.800
 I mean, just the whole experience is a mess that has a ripple effect to how you trust

1:48:23.800 --> 1:48:28.280
 in the entirety of the experience, but like actually taking on the problem of that failure

1:48:28.280 --> 1:48:34.600
 case and revolutionizing that experience, both for trucking and for ride sharing.

1:48:34.600 --> 1:48:40.160
 That's an amazing opportunity there because that feels like it would change everything.

1:48:40.160 --> 1:48:44.560
 If you can reliably know when the failures happen, which they will, you have a clear

1:48:44.560 --> 1:48:49.840
 plan that doesn't significantly affect the efficiency of the whole process.

1:48:49.840 --> 1:48:51.920
 That could be the game changer.

1:48:51.920 --> 1:48:56.320
 If teleoperation is part of that, it could be, just like you're saying, it could be teleoperation

1:48:56.320 --> 1:49:01.480
 or it could be like a fleet of rescuers that can come in, which is a similar idea, but

1:49:01.480 --> 1:49:08.600
 teleoperation obviously, that allows you to just have a network of monitors, people monitoring

1:49:08.600 --> 1:49:12.880
 this giant fleet of trucks and taking over when needed.

1:49:12.880 --> 1:49:19.720
 It's a beautiful vision of the future where there's millions of robots and only thousands

1:49:19.720 --> 1:49:22.840
 of humans monitoring those millions of robots.

1:49:22.840 --> 1:49:29.920
 That seems like a perfect dance of allowing humans to do what they do best and allowing

1:49:29.920 --> 1:49:31.840
 robots to do what they do best.

1:49:31.840 --> 1:49:32.840
 Yeah.

1:49:32.840 --> 1:49:36.520
 I mean, I think there are, and we just applied for an NSF.

1:49:36.520 --> 1:49:44.400
 We didn't get anybody's watching, but with some folks from Wisconsin who do teleoperation.

1:49:44.400 --> 1:49:50.840
 Some of this is used for rovers and really high stakes, difficult problems, but one of

1:49:50.840 --> 1:49:56.480
 the things we wanted to study were these mines and these Rio Tinto mines in Australia where

1:49:56.480 --> 1:49:59.800
 they remotely pilot the trucks.

1:49:59.800 --> 1:50:06.400
 There's some autonomy, I guess, but it's overseen by a remote operator.

1:50:06.400 --> 1:50:11.840
 It's near Perth and it's quite remote.

1:50:11.840 --> 1:50:15.880
 They retrained the truck drivers to be the remote operators.

1:50:15.880 --> 1:50:20.400
 There's autonomy in the port of Rotterdam and places like that where there are jobs

1:50:20.400 --> 1:50:21.400
 there.

1:50:21.400 --> 1:50:27.280
 I think maybe we'll get to this later, but there's a real policy question about who's

1:50:27.280 --> 1:50:31.160
 going to lose and what we do about it and whether or not there are opportunities there

1:50:31.160 --> 1:50:37.200
 that maybe we need to put our thumb on the scale a little bit to make sure that there's

1:50:37.200 --> 1:50:42.240
 some give back to the community that's taking the hit.

1:50:42.240 --> 1:50:46.800
 For instance, if there were teleoperation centers, maybe they go in these communities

1:50:46.800 --> 1:50:51.240
 that we disproportionately source truck drivers from today.

1:50:51.240 --> 1:50:52.240
 What does that mean?

1:50:52.240 --> 1:50:55.280
 It may not be the cheapest place to do it if they don't have great connectivity and

1:50:55.280 --> 1:50:59.760
 it may not be where the upper lever managers want to be at places like that.

1:50:59.760 --> 1:51:02.240
 The issues like that.

1:51:02.240 --> 1:51:09.880
 I do think it's an interesting question, both from a practical scenario situation of how

1:51:09.880 --> 1:51:14.160
 it's going to work, but also from a policy perspective.

1:51:14.160 --> 1:51:19.320
 There's platoons, there's teleoperation, and this is taking care of some of the highway

1:51:19.320 --> 1:51:21.240
 driving that we're talking about.

1:51:21.240 --> 1:51:27.880
 Is there other ideas, like, are there other ideas, scenarios that you have for autonomous

1:51:27.880 --> 1:51:28.880
 trucks?

1:51:28.880 --> 1:51:29.880
 Yeah.

1:51:29.880 --> 1:51:36.440
 The most obvious one, actually, is just a facility to facility.

1:51:36.440 --> 1:51:42.800
 It can't go everywhere, but a lot of logistics facilities are very close to interstates and

1:51:42.800 --> 1:51:49.720
 they're on big commercial roads without bikes and parked cars and all that stuff.

1:51:49.720 --> 1:51:54.840
 Some of the jobs that I think are really first on the chopping block are these LTL that less

1:51:54.840 --> 1:51:57.480
 than truckload what's called line haul.

1:51:57.480 --> 1:52:02.640
 These are the drivers who go from terminal to terminal with those full trailers.

1:52:02.640 --> 1:52:09.400
 Those facilities are often located strategically to avoid congestion and to be in big industrial

1:52:09.400 --> 1:52:11.000
 facilities.

1:52:11.000 --> 1:52:17.560
 You could imagine that being the first place you see a Waymo self driving truck rollout

1:52:17.560 --> 1:52:25.720
 might be direct facility to facility for UPS or FedEx or less than truckload care.

1:52:25.720 --> 1:52:30.480
 An idea there is fully driverless, so potentially not even a driver in the truck, it's just

1:52:30.480 --> 1:52:35.600
 going from facility to facility empty, zero occupancy.

1:52:35.600 --> 1:52:39.600
 Because that labor is expensive, they don't keep those drivers out overnight.

1:52:39.600 --> 1:52:47.440
 Those drivers do a run back and forth, typically, or in a team go back and forth in one day.

1:52:47.440 --> 1:52:50.680
 From the people you've spoken with so far, what's your sense?

1:52:50.680 --> 1:52:56.000
 How far are we away from, which scenario is closest and how far away are we from that

1:52:56.000 --> 1:53:02.480
 scenario of autonomy being a big part of our trucking fleet?

1:53:02.480 --> 1:53:08.280
 Most folks are focused on another scenario, which is the exit to exit, which looks like

1:53:08.280 --> 1:53:12.920
 that urban truck boards thing that I laid out earlier.

1:53:12.920 --> 1:53:17.080
 You have a human driven truck that comes out to a drop lot.

1:53:17.080 --> 1:53:20.000
 It meets up with an autonomous truck.

1:53:20.000 --> 1:53:27.120
 That truck then drives it on the interstate to another lot, and then a human driver picks

1:53:27.120 --> 1:53:28.320
 it up.

1:53:28.320 --> 1:53:34.080
 There are a couple variations maybe on that.

1:53:34.080 --> 1:53:38.120
 Let me just run through the last two scenarios.

1:53:38.120 --> 1:53:44.000
 The other thing you could do is to say, all right, I've got a truck that can drive itself,

1:53:44.000 --> 1:53:47.880
 and I refer to this one as autopilot.

1:53:47.880 --> 1:53:53.280
 You have a human drive it out to the interstate, but rather than have that transaction where

1:53:53.280 --> 1:53:59.520
 the human driven truck detaches the trailer and it gets coupled up to a self driving truck,

1:53:59.520 --> 1:54:04.360
 that human driver just hops on the interstate with that truck and goes and back and goes

1:54:04.360 --> 1:54:07.920
 off duty while the truck drives itself.

1:54:07.920 --> 1:54:11.680
 You have a self driving truck that's not driverless.

1:54:11.680 --> 1:54:16.080
 Just to clarify, because Tesla uses the term autopilot and so do aeroplanes, and so everybody

1:54:16.080 --> 1:54:21.240
 uses the word autopilot, we're referring to essentially full autonomy, but because it's

1:54:21.240 --> 1:54:25.680
 exit to exit, the truck driver is on board the truck, but they're sleeping in the back

1:54:25.680 --> 1:54:26.680
 or whatever.

1:54:26.680 --> 1:54:31.200
 Yeah, and this gets to the really weedy policy questions, right?

1:54:31.200 --> 1:54:36.400
 So basically for the Department of Transportation for you to be off duty for safety reasons,

1:54:36.400 --> 1:54:39.480
 you have to be completely relieved of all responsibility.

1:54:39.480 --> 1:54:47.000
 So that truck has to not encounter a construction site or inclement weather or whatever it might

1:54:47.000 --> 1:54:53.000
 be and call to you and say, hey, or I mean, obviously, we're imagining connected vehicles

1:54:53.000 --> 1:54:54.000
 as well, right?

1:54:54.000 --> 1:54:59.840
 So you're in a self driving truck, you're in the back and trucks 20 miles ahead experience

1:54:59.840 --> 1:55:04.720
 some problem that may require teleoperation or whatever it is, right?

1:55:04.720 --> 1:55:09.360
 And it signals to your truck, hey, tell the driver 20 miles ahead.

1:55:09.360 --> 1:55:11.120
 He's got to hop in the seat.

1:55:11.120 --> 1:55:14.480
 That would mean that they're on duty according to the way that the current rules are written.

1:55:14.480 --> 1:55:19.760
 They have some responsibility and part of that is, we need them to get rest, right?

1:55:19.760 --> 1:55:22.840
 They need to have uninterrupted sleep.

1:55:22.840 --> 1:55:26.000
 So that's what I call autopilot.

1:55:26.000 --> 1:55:31.920
 The final scenario is one that I thought was actually the one scenario that was good

1:55:31.920 --> 1:55:39.080
 for labor, which I proposed, because I was like, well, here's an idea that would be like

1:55:39.080 --> 1:55:41.840
 actually good for workers.

1:55:41.840 --> 1:55:47.120
 And just another brief aside here.

1:55:47.120 --> 1:55:53.200
 The history of trucking over the last 40 years, there's been a lot of technological change.

1:55:53.200 --> 1:55:58.000
 So when I learned to drive the truck, I had to learn to manually shift it like I was describing.

1:55:58.000 --> 1:56:03.080
 You had to read these fairly complicated, big sets of laminated maps to figure out where

1:56:03.080 --> 1:56:06.360
 the truck can go and where it couldn't, which is a big deal.

1:56:06.360 --> 1:56:09.800
 You take these trucks on the wrong road and you're destroying a bridge or you're doing

1:56:09.800 --> 1:56:12.920
 a can opener, which is where you try to drive it under a bridge that's too low.

1:56:12.920 --> 1:56:14.520
 You've probably seen that on YouTube.

1:56:14.520 --> 1:56:19.120
 If not, check it out, a truck can opener.

1:56:19.120 --> 1:56:23.080
 There's some bridges that are famous for it, and there's one I think called the can opener

1:56:23.080 --> 1:56:27.040
 that you can find on YouTube.

1:56:27.040 --> 1:56:34.400
 And you had to law those hours manually and do the math and plan your work routine.

1:56:34.400 --> 1:56:35.400
 And I would do this every day.

1:56:35.400 --> 1:56:38.960
 I'd say, okay, I'm going to get up at five, I've got to think about Buffalo and there's

1:56:38.960 --> 1:56:40.160
 traffic there.

1:56:40.160 --> 1:56:43.560
 So I want to be through Buffalo by 630.

1:56:43.560 --> 1:56:51.960
 And then that'll put me in Cleveland at 930, which means I'll miss that rush hour, which

1:56:51.960 --> 1:56:54.040
 is going to put me in Chicago.

1:56:54.040 --> 1:56:55.200
 And so you do this.

1:56:55.200 --> 1:57:02.320
 And now today, 15 years later, truck drivers don't have to do any of that.

1:57:02.320 --> 1:57:03.960
 You don't have to shift the truck.

1:57:03.960 --> 1:57:06.320
 You don't have to map.

1:57:06.320 --> 1:57:12.800
 You can figure out the least congested route to go on, and your hours of service are recorded,

1:57:12.800 --> 1:57:17.280
 or a good portion of them are reported automatically.

1:57:17.280 --> 1:57:23.480
 All of that has been a substantial de skilling that has put downward pressure on wages and

1:57:23.480 --> 1:57:27.240
 allowed companies to speed up, monitor, and direct.

1:57:27.240 --> 1:57:32.640
 I mean, the key technology that I did work under is satellite linked computers.

1:57:32.640 --> 1:57:36.080
 So before you could kind of go out and plan your own work, and the boss really couldn't

1:57:36.080 --> 1:57:40.440
 see what you were doing and push you and say, you've been on break for 10 hours, why aren't

1:57:40.440 --> 1:57:41.440
 you moving?

1:57:41.440 --> 1:57:46.320
 And you might tell them, because I'm tired, like I didn't sleep well, I've got to get

1:57:46.320 --> 1:57:50.760
 a couple more hours, they're only going to accept that so many times, or at least some

1:57:50.760 --> 1:57:52.120
 of those dispatchers are.

1:57:52.120 --> 1:57:58.840
 So all this technology has made the job sort of de skilled the job, hurt drivers in the

1:57:58.840 --> 1:58:01.880
 labor market, made the work worse.

1:58:01.880 --> 1:58:09.240
 So I think the burden is really on the technologists who are like, oh, this will make truck driver

1:58:09.240 --> 1:58:11.640
 jobs better and sort of envision ways that it would.

1:58:11.640 --> 1:58:16.520
 It's like, the burden is really a proof is really on you to sort of really clearly lay

1:58:16.520 --> 1:58:18.920
 out what that is going to look like.

1:58:18.920 --> 1:58:25.320
 Because it's 30 or 40 years of history suggests that that technology into labor markets where

1:58:25.320 --> 1:58:31.680
 workers are really weak and cheap is what wins, that new technology doesn't help workers

1:58:31.680 --> 1:58:34.040
 or raise their wages.

1:58:34.040 --> 1:58:44.360
 So lowers the bar of entry into a skill, that's really interesting, that's tough, that's tough

1:58:44.360 --> 1:58:48.240
 to know what to do with, because yeah, from a technology perspective, you want to make

1:58:48.240 --> 1:58:51.720
 the life of the people doing the job today easier.

1:58:51.720 --> 1:58:52.840
 Is it?

1:58:52.840 --> 1:58:53.840
 Is that what you want?

1:58:53.840 --> 1:59:00.600
 No, but that like, when you think about like what exactly, because the reality is you will

1:59:00.600 --> 1:59:06.120
 make the their life potentially a little bit easier, but that will allow the companies

1:59:06.120 --> 1:59:10.720
 to then hire people that are less skilled, it'll get those people that are previously

1:59:10.720 --> 1:59:13.840
 working there fired or lower wages.

1:59:13.840 --> 1:59:21.360
 And so the result of this easier is a lower quality of life as dark, actually.

1:59:21.360 --> 1:59:22.360
 I know.

1:59:22.360 --> 1:59:23.360
 I'm sorry.

1:59:23.360 --> 1:59:26.440
 But you were saying that was for you initially the hopeful.

1:59:26.440 --> 1:59:28.960
 Oh, no, so I'll get to that.

1:59:28.960 --> 1:59:31.360
 But one more thing, because this is not stopping, right?

1:59:31.360 --> 1:59:34.120
 And this is another interesting question about this sort of automation.

1:59:34.120 --> 1:59:38.560
 And I think Uber is an interesting example here, right?

1:59:38.560 --> 1:59:42.760
 Where it's like, okay, if we had self driving trucks or self driving cars, right, we could

1:59:42.760 --> 1:59:46.280
 automate what used to be taxi service.

1:59:46.280 --> 1:59:49.680
 There's a whole bunch of stuff that's already been automated, like the dispatching.

1:59:49.680 --> 1:59:54.280
 So the dispatchers are already out of work in in in rideshare, and the payment is already

1:59:54.280 --> 1:59:55.280
 automated, right?

1:59:55.280 --> 1:59:59.400
 So so you have to automate steps like this, so you have to have, you know, that initial

1:59:59.400 --> 2:00:04.240
 link to dispatch the truck, you have to have the, you know, the automated mapping and all.

2:00:04.240 --> 2:00:08.120
 So we're sort of done all this, you know, incremental automation, right, that could

2:00:08.120 --> 2:00:11.720
 make the truck completely driverless.

2:00:11.720 --> 2:00:15.400
 There's some important things happening right now with the remaining good jobs.

2:00:15.400 --> 2:00:20.120
 So what you're really paying for when you get a good truck driver is, you know, like

2:00:20.120 --> 2:00:26.600
 I said, you get those kind of local skills of like backing and congested traffic.

2:00:26.600 --> 2:00:30.880
 Those it's really impressive to watch, and there's some value on it, certainly, but it's

2:00:30.880 --> 2:00:36.080
 relatively low value in the actual driving technique, right?

2:00:36.080 --> 2:00:40.600
 So you bump something, you know, back into the dock, it's, you know, it might be a couple

2:00:40.600 --> 2:00:45.640
 thousand dollars because you ruin a canopy or something over a dock or tear up a trailer.

2:00:45.640 --> 2:00:51.000
 What you really want those, those highly skilled, conscientious drivers, and that's really

2:00:51.000 --> 2:00:52.320
 what it what it is.

2:00:52.320 --> 2:00:55.920
 And that's what computers are really good at is about being conscientious, right, in

2:00:55.920 --> 2:01:00.960
 the sense of like, they pay attention continually, right, and and how I was describing those

2:01:00.960 --> 2:01:06.520
 those long haul segments where the driver, you know, just keeps out of the situations

2:01:06.520 --> 2:01:11.440
 that could become problematic and just they don't look at their phone and they take the

2:01:11.440 --> 2:01:13.560
 job seriously and they're safe.

2:01:13.560 --> 2:01:18.520
 And you can give somebody a skills test, right, in, in, you know, as a CDL examiner, you could

2:01:18.520 --> 2:01:21.440
 take them out and say, all right, I need you to go around these cones and like drive safely

2:01:21.440 --> 2:01:24.400
 through this school zone.

2:01:24.400 --> 2:01:29.840
 But what really proves that you're a safe driver is two years without an accident, right?

2:01:29.840 --> 2:01:34.880
 Because that means that day after day, hour after hour, mile after mile, you did the right

2:01:34.880 --> 2:01:36.920
 thing, right?

2:01:36.920 --> 2:01:41.560
 And not when it was like, oh, some situations emerging, but just consistently over time,

2:01:41.560 --> 2:01:44.000
 kept yourself out of accident situations.

2:01:44.000 --> 2:01:48.120
 And you can see this with drivers who are, you know, a million or two million safe miles.

2:01:48.120 --> 2:01:52.800
 The value of those drivers for Walmart is they don't run over minivans.

2:01:52.800 --> 2:01:57.280
 The company I ran, I worked for, they ran over minivans on a regular basis.

2:01:57.280 --> 2:02:03.160
 So you know, when I, when I was trained, they said, we kill 20 people a year.

2:02:03.160 --> 2:02:04.880
 We send someone to the funeral.

2:02:04.880 --> 2:02:07.520
 There's a big check involved.

2:02:07.520 --> 2:02:08.520
 Don't be that.

2:02:08.520 --> 2:02:13.400
 You know, we don't want to go to your funeral and you don't want to be the person who, who

2:02:13.400 --> 2:02:14.800
 caused that funeral.

2:02:14.800 --> 2:02:15.800
 Okay.

2:02:15.800 --> 2:02:17.760
 So they, they just write that off.

2:02:17.760 --> 2:02:18.760
 Okay.

2:02:18.760 --> 2:02:20.880
 That's just part of the business model.

2:02:20.880 --> 2:02:29.480
 Now forward collision avoidance can, you know, basically eliminate the vast majority

2:02:29.480 --> 2:02:31.200
 of those accidents.

2:02:31.200 --> 2:02:35.080
 That's what the value of a really expensive conscientious driver is based on.

2:02:35.080 --> 2:02:37.160
 They don't run over minivans.

2:02:37.160 --> 2:02:41.760
 So as soon as you have that forward collision avoidance, what's going to happen to the wages

2:02:41.760 --> 2:02:43.880
 of those drivers?

2:02:43.880 --> 2:02:53.440
 By way of a therapy session, help me understand is a collision avoidance, automated collision

2:02:53.440 --> 2:02:57.240
 avoidance systems, are they good or bad for society?

2:02:57.240 --> 2:02:58.240
 Yeah.

2:02:58.240 --> 2:03:01.160
 I mean, you know, this, this is, they're good.

2:03:01.160 --> 2:03:02.160
 They're good.

2:03:02.160 --> 2:03:03.160
 Right.

2:03:03.160 --> 2:03:04.160
 They're good.

2:03:04.160 --> 2:03:12.560
 But in, what do we do about the pain of a workforce in the short term because their, their wages

2:03:12.560 --> 2:03:18.680
 are going to go down because the job starts requiring less and less skill?

2:03:18.680 --> 2:03:23.080
 Is it, is there a hopeful message here where other jobs are created?

2:03:23.080 --> 2:03:25.080
 So I'm, you know, I'm a sociologist, right?

2:03:25.080 --> 2:03:28.800
 So, you know, so I'm going to think about what's, what's the structure behind that that

2:03:28.800 --> 2:03:29.800
 creates that pain?

2:03:29.800 --> 2:03:30.800
 Right.

2:03:30.800 --> 2:03:31.800
 That's entrepreneurship.

2:03:31.800 --> 2:03:32.800
 Right.

2:03:32.800 --> 2:03:35.080
 You know, we don't call it capitalism for nothing.

2:03:35.080 --> 2:03:40.640
 You know, what capitalists do is they figure out cheaper, more efficient ways to do stuff

2:03:40.640 --> 2:03:43.400
 and they use technology to do that oftentimes, right?

2:03:43.400 --> 2:03:48.920
 This is the remarkable history of the last couple of centuries and, and all the productivity

2:03:48.920 --> 2:03:56.520
 gains is, you know, people who are in a competitive market saying, if I have to do it, right?

2:03:56.520 --> 2:04:01.000
 I don't have a choice because like my competitor over there is going to eat my lunch if I'm

2:04:01.000 --> 2:04:03.600
 not on my game.

2:04:03.600 --> 2:04:04.600
 I don't have a choice.

2:04:04.600 --> 2:04:09.480
 I've, I've got to invest in this technology to, you know, make it more, more efficient

2:04:09.480 --> 2:04:11.320
 to make it cheaper.

2:04:11.320 --> 2:04:12.480
 And what do you look for?

2:04:12.480 --> 2:04:16.160
 You look for oftentimes, you look for labor costs, right?

2:04:16.160 --> 2:04:17.520
 You look for high value labor.

2:04:17.520 --> 2:04:21.320
 If I can take a hundred and, you know, these, a lot of these truck drivers make good money,

2:04:21.320 --> 2:04:25.440
 hundred thousand dollars, good benefits, vacation, you know, retirement.

2:04:25.440 --> 2:04:31.520
 If I can replace them with a $35,000 worker when I'm competing with maybe a low wage retail

2:04:31.520 --> 2:04:36.600
 employer rather than some other more expensive employers for, you know, skilled blue collar

2:04:36.600 --> 2:04:39.520
 workers, I'm going to do that.

2:04:39.520 --> 2:04:41.880
 And that's just, that's what we do.

2:04:41.880 --> 2:04:46.880
 And so I think those, those are the bigger questions around this technology, right?

2:04:46.880 --> 2:04:50.360
 It's like, you know, are workers going to get screwed by this?

2:04:50.360 --> 2:04:53.560
 Like, yeah, most likely, like that's, that's what we do.

2:04:53.560 --> 2:04:57.960
 So one of the things you say is, I mean, first of all, the numbers of workers that will be,

2:04:57.960 --> 2:05:02.920
 that will feel as pain is not perhaps as large as the journalists kind of articulate.

2:05:02.920 --> 2:05:05.720
 But nevertheless, the pain is real.

2:05:05.720 --> 2:05:13.920
 And I guess my question here is, do you have an optimistic vision about the transformative

2:05:13.920 --> 2:05:17.560
 effects of autonomous trucks on society?

2:05:17.560 --> 2:05:25.720
 Like if you look 20 years from now, and perhaps see maybe 30 years from now, perhaps see these

2:05:25.720 --> 2:05:31.200
 autonomous trucks doing the various parts of the scenarios you listed, and there's just

2:05:31.200 --> 2:05:38.360
 hundreds of thousands of them, just like, like veins, like blood flowing through veins

2:05:38.360 --> 2:05:43.840
 on the interstate system.

2:05:43.840 --> 2:05:48.560
 What kind of world do you see that's a better world than today that involves such trucks?

2:05:48.560 --> 2:05:49.760
 Yeah.

2:05:49.760 --> 2:05:51.080
 Can I defend myself first?

2:05:51.080 --> 2:05:55.520
 Because I can, I'm reading the comments right now of people, you know, of the economists

2:05:55.520 --> 2:05:56.520
 who are telling me.

2:05:56.520 --> 2:05:59.560
 Dear commenters, dear PhD economics.

2:05:59.560 --> 2:06:00.560
 Yes.

2:06:00.560 --> 2:06:01.560
 Yes.

2:06:01.560 --> 2:06:07.440
 Dear PhD in economics, I know that, that higher skilled jobs are created, you know, by, by

2:06:07.440 --> 2:06:09.000
 technological advancement, right?

2:06:09.000 --> 2:06:11.960
 I mean, there are big questions about how many of them, right?

2:06:11.960 --> 2:06:19.640
 So the idea that we would create more, you know, expensive labor positions, right, with

2:06:19.640 --> 2:06:23.920
 a new technology, right, you better check your business plan, if your idea is to take,

2:06:23.920 --> 2:06:28.480
 you know, a bunch of low, low wage labor and replace it with the same amount of high wage

2:06:28.480 --> 2:06:29.480
 labor, right?

2:06:29.480 --> 2:06:33.560
 So we, there's a question about how many of those jobs, and there's the really important

2:06:33.560 --> 2:06:38.920
 social and political question of, are they the same people, right, and do they live in

2:06:38.920 --> 2:06:44.560
 the same places, and I think that kind of, you know, geography is a huge issue here with

2:06:44.560 --> 2:06:45.800
 the impacts, right?

2:06:45.800 --> 2:06:50.240
 Lots of rural workers, interesting politically, lots of red state workers, right?

2:06:50.240 --> 2:06:54.440
 Lots of blue state, maybe union folks who are going to try to slow autonomy and lots of

2:06:54.440 --> 2:06:58.880
 red state, you know, representatives in the house, maybe who want to, you know, stand

2:06:58.880 --> 2:07:01.600
 up for their, for their trucker constituents.

2:07:01.600 --> 2:07:03.360
 So just, just to defend myself.

2:07:03.360 --> 2:07:04.360
 Yeah.

2:07:04.360 --> 2:07:08.360
 And to elaborate, I think economics as a field is not good at measuring the landscape of

2:07:08.360 --> 2:07:10.440
 human pain and suffering.

2:07:10.440 --> 2:07:15.560
 So, you know, sometimes you can forget in the numbers as real lives that are at stake.

2:07:15.560 --> 2:07:18.560
 That's what I suppose sociology is better at doing.

2:07:18.560 --> 2:07:19.560
 So you...

2:07:19.560 --> 2:07:20.560
 We try sometimes, sometimes.

2:07:20.560 --> 2:07:26.640
 Well, the problem with, I mean, I'm somebody who loves psychology and psychiatry, and a

2:07:26.640 --> 2:07:32.960
 little bit, I guess, of sociology, I realized how little, how tragically flawed the field

2:07:32.960 --> 2:07:37.480
 is, not because of lack of trying, but just how difficult the problems are.

2:07:37.480 --> 2:07:43.200
 To do really thorough studies that understand the fundamentals of human behavior and this,

2:07:43.200 --> 2:07:48.280
 yes, landscape of human suffering, it's just, it's almost an impossible task without the

2:07:48.280 --> 2:07:54.000
 data and we currently don't, you know, not everybody's richly integrated to where they're

2:07:54.000 --> 2:07:59.720
 fully connected and all their information is being, like, recorded for sociologists

2:07:59.720 --> 2:08:00.720
 to study.

2:08:00.720 --> 2:08:02.560
 So you have to make a lot of inferences.

2:08:02.560 --> 2:08:03.560
 You have to talk to people.

2:08:03.560 --> 2:08:05.440
 You have to do the interviews that you're doing.

2:08:05.440 --> 2:08:11.880
 And through that, like, really difficult work, try to understand, like, hear the music that

2:08:11.880 --> 2:08:13.520
 nobody else is hearing.

2:08:13.520 --> 2:08:18.640
 The music of, like, what people are feeling, their hopes, their dreams, and the crushing

2:08:18.640 --> 2:08:22.160
 of their dreams due to some kind of economic forces.

2:08:22.160 --> 2:08:23.160
 Yeah.

2:08:23.160 --> 2:08:28.720
 I mean, we've just lived that for four and a half years of probably, you know, elites.

2:08:28.720 --> 2:08:34.480
 Let me just go out on a limb and say, not understanding the sort of emotional and psychological

2:08:34.480 --> 2:08:38.120
 currents of a large portion of the population, right?

2:08:38.120 --> 2:08:42.680
 And just being stunned by it and confused, right?

2:08:42.680 --> 2:08:47.920
 Wasn't confusing for me after having talked to truckers, again, who, trucking is a job

2:08:47.920 --> 2:08:48.920
 of last resort.

2:08:48.920 --> 2:08:53.000
 These are people who've already lost that manufacturing job oftentimes, already lost

2:08:53.000 --> 2:08:57.480
 that construction job to just aging, right?

2:08:57.480 --> 2:08:59.640
 So what, you know, what can we do, right?

2:08:59.640 --> 2:09:04.240
 What's sort of the positive vision because, like, we've got tons of highway deaths.

2:09:04.240 --> 2:09:10.360
 We've got, and just to, you know, the big picture is, and this is the opportunity, I

2:09:10.360 --> 2:09:15.760
 guess, for investors, it's a hugely inefficient system.

2:09:15.760 --> 2:09:17.480
 So we buy this truck.

2:09:17.480 --> 2:09:21.240
 There's this low wage worker, and it oftentimes, and again, I'm setting aside those really

2:09:21.240 --> 2:09:26.440
 good line hall jobs and LTL, those are a different case.

2:09:26.440 --> 2:09:31.760
 That low wage worker is driving a truck that they might, the wheels might roll seven to

2:09:31.760 --> 2:09:32.760
 eight hours a day.

2:09:32.760 --> 2:09:36.360
 That's what the truck is designed to do, and that's what makes the money for the company.

2:09:36.360 --> 2:09:40.880
 In other seven, eight hours a day, the driver's doing other kinds of work that, you know,

2:09:40.880 --> 2:09:42.080
 is not driving.

2:09:42.080 --> 2:09:45.640
 And then the rest of the day, they're basically living out of the truck.

2:09:45.640 --> 2:09:51.200
 You really can't find a more inefficient use of an asset than that, right?

2:09:51.200 --> 2:09:55.200
 Now a big part of that is we pay for the roads and we pay for the rest areas and all this

2:09:55.200 --> 2:09:56.200
 other stuff.

2:09:56.200 --> 2:09:59.960
 So, the way that I work and the way that, you know, I think about these problems is

2:09:59.960 --> 2:10:04.800
 I try to find analogies, right, sort of labor processes and things that make economic sense,

2:10:04.800 --> 2:10:12.960
 you know, that seem, you know, in the same area of the economy, but have some different

2:10:12.960 --> 2:10:17.880
 characteristics for workers, right, and sort of try to figure out why does the economics

2:10:17.880 --> 2:10:19.680
 work there, right?

2:10:19.680 --> 2:10:28.480
 And so, if you look at those really good jobs, the most likely way that you as a passenger

2:10:28.480 --> 2:10:32.400
 car driver would know that it's one of those drivers is that they're multiple trailers,

2:10:32.400 --> 2:10:33.400
 right?

2:10:33.400 --> 2:10:37.040
 So, you see these, like, maybe it's three small trailers, maybe it's two sort of medium

2:10:37.040 --> 2:10:42.400
 size trailers, some places you might even see two really big trailers together.

2:10:42.400 --> 2:10:45.920
 You do that because labor is expensive, right, and it's highly skilled.

2:10:45.920 --> 2:10:50.080
 And so, you use it efficiently and you say, all right, you know, rather than having you,

2:10:50.080 --> 2:10:53.200
 you know, haul that little trailer out of the ports, you know, that sort of half size

2:10:53.200 --> 2:10:56.520
 container, we're going to wait until we get three or we're going to coordinate the movement

2:10:56.520 --> 2:11:01.400
 so that they're three ready, you go do what truckers call make a set, put them together,

2:11:01.400 --> 2:11:04.200
 right, and you go.

2:11:04.200 --> 2:11:08.360
 That's a massive productivity gain, right, because, you know, you're hauling two, three

2:11:08.360 --> 2:11:10.000
 times as much freight.

2:11:10.000 --> 2:11:18.760
 So the positive scenario that I threw out in 2018 was why not have a human driven truck

2:11:18.760 --> 2:11:23.640
 with a self driving truck that follows it, right, just a drone unit?

2:11:23.640 --> 2:11:30.520
 And it was, you know, to me, this seemed as a, you know, non computer scientists, a sociologist,

2:11:30.520 --> 2:11:34.920
 right, this made a lot of sense because when I got done talking to the, you know, the computer

2:11:34.920 --> 2:11:38.960
 scientists and the engineers, they were like, well, you know, it's like object recognition,

2:11:38.960 --> 2:11:40.800
 decision making algorithm, all this stuff.

2:11:40.800 --> 2:11:46.960
 It's like, all right, so why don't you leave the human brain in the lead vehicle, right,

2:11:46.960 --> 2:11:51.440
 you got all that processing and then all that following, now, again, this is sort of me

2:11:51.440 --> 2:11:57.200
 being a layperson, you know, I said, why don't, you know, then that following truck, right,

2:11:57.200 --> 2:12:01.120
 takes direction from the front, it uses the rear of the trailer as a reference point.

2:12:01.120 --> 2:12:06.800
 It maintains the lane, you've got cooperative adaptive cruise control and that you double

2:12:06.800 --> 2:12:08.680
 the productivity of that driver.

2:12:08.680 --> 2:12:14.480
 You solve that problem that I hated in my, you know, urban truck ports thing about the

2:12:14.480 --> 2:12:19.240
 bridge weight, because when you get to the bridges, you know, the two trucks can just

2:12:19.240 --> 2:12:22.360
 spread out just enough to make the bridge weight, right, and you can just program that

2:12:22.360 --> 2:12:28.480
 in and, you know, they're 50 feet further apart, 100 feet further, further apart.

2:12:28.480 --> 2:12:33.800
 So interesting sort of, I think, story about this that, that leads to kind of, I think

2:12:33.800 --> 2:12:43.360
 the policy questions in, I guess, 2017, Jack Reed and Susan Collins and, you know, requested

2:12:43.360 --> 2:12:47.920
 from the Senate, the Senate requested research on what the impacts of self driving trucks

2:12:47.920 --> 2:12:56.080
 would be. And the first stage of that was for the GAO to do a report, sort of looking

2:12:56.080 --> 2:12:59.880
 at the lay of the land, talking to some experts.

2:12:59.880 --> 2:13:07.480
 And I was working on my 2018 report, helped contribute to that GAO report. And, you know,

2:13:07.480 --> 2:13:11.200
 I had the six scenarios, right, I'm like, okay, you know, here's, here's what Starsky's

2:13:11.200 --> 2:13:17.240
 doing, you know, here's what in Bark and Uber doing, you know, here's what Waymo might

2:13:17.240 --> 2:13:23.480
 be doing, you know, nobody really knows, right? Here's what Peloton's doing, you know, here's

2:13:23.480 --> 2:13:29.360
 the autopilot scenario. And then here's this one that I think actually could be good for

2:13:29.360 --> 2:13:34.560
 drivers. So now you've got that driver who's got two, you know, two times the freight,

2:13:34.560 --> 2:13:37.960
 their decisions are more important, they're managing a more complex system, right, they're

2:13:37.960 --> 2:13:41.240
 probably going to have to have some global understanding of how to, you know, the environments

2:13:41.240 --> 2:13:47.240
 at which it can operate safely. Right now we're talking upskilling, right. And so, you

2:13:47.240 --> 2:13:53.360
 know, that the GAO, you know, sort of writes up these different scenarios. And the idea

2:13:53.360 --> 2:13:57.920
 is that it's going to prepare for this Department of Transportation Department of Labor set

2:13:57.920 --> 2:14:05.240
 of processes to engage stakeholders and, and sort of get, you know, get industry perspectives

2:14:05.240 --> 2:14:12.280
 and then do a study on the labor impacts. So, you know, that DOT, DOL process starts

2:14:12.280 --> 2:14:19.280
 to happen. And, you know, I get to the workshop, and a friend was sitting at the table next

2:14:19.280 --> 2:14:24.800
 to me, and he holds up the scenarios that, that they're going to have us discuss at this

2:14:24.800 --> 2:14:29.160
 workshop. And he's like, Hey, these look really familiar, right? Because they were the, you

2:14:29.160 --> 2:14:35.240
 know, scenarios from, from the report, but there were only five instead of six. Interesting.

2:14:35.240 --> 2:14:41.240
 The sixth scenario, which was the upskilling labor, good for, good for workers scenario,

2:14:41.240 --> 2:14:45.920
 wasn't, wasn't discussed. So to clarify, that's the, the integral piece of technology there's

2:14:45.920 --> 2:14:52.400
 platooning. Yeah, I mean, in a sense, it's, it's platooning, but, and I, and in fairness,

2:14:52.400 --> 2:14:58.600
 right, the, as I pitched that idea or sort of ran that idea by the computer scientists

2:14:58.600 --> 2:15:02.840
 and engineers that I would, and product managers that I would talk to, they would say, you

2:15:02.840 --> 2:15:09.400
 know, you know, we thought about that, but that following truck, it's not that simple.

2:15:09.400 --> 2:15:15.240
 You know, that thing, basically, we had to engineer that to be capable of independent

2:15:15.240 --> 2:15:19.600
 self driving, because what if there was a cut in, or, you know, any number of scenarios

2:15:19.600 --> 2:15:26.040
 in which it lost that connection to the lead truck for whatever reason. Now, I mean, I

2:15:26.040 --> 2:15:32.320
 don't know who platooning is hard. There's edge cases. I guarantee the number of edge

2:15:32.320 --> 2:15:38.000
 cases and platooning is orders of magnitude lower than the number of edge cases in the

2:15:38.000 --> 2:15:43.880
 general solo full self drive. You do not need to solve the full self driving problem. I

2:15:43.880 --> 2:15:50.840
 mean, if you're talking about a probability of dangerous events, it just seems with platooning

2:15:50.840 --> 2:15:56.960
 then like, you can deal with cut ends. Yeah. So this is, you know, this is beyond, this

2:15:56.960 --> 2:16:00.960
 is one of the challenges, obviously, of being a researcher who, you know, doesn't doesn't

2:16:00.960 --> 2:16:05.960
 really have any background in the technology, right? So I can, I can dream this up. I don't

2:16:05.960 --> 2:16:10.560
 have no idea if it's feasible. Well, let me speak, you spoke to the PhDs in economics,

2:16:10.560 --> 2:16:14.920
 let me speak to the PhDs in computer science. If you think platooning is as hard as the full

2:16:14.920 --> 2:16:20.800
 self driving problem, we need to talk, because I think that's ridiculous. I think platooning

2:16:20.800 --> 2:16:27.280
 is, in fact, I think platooning is an interesting idea for ride sharing as well for the general

2:16:27.280 --> 2:16:32.080
 autonomous driving problem, not just trucking, but obviously trucking is the big, big benefit

2:16:32.080 --> 2:16:37.160
 because the number of A to B points in trucking is much, much lower than the general ride

2:16:37.160 --> 2:16:42.320
 sharing problem. But anyway, I think that's a great idea, but you're saying it was removed.

2:16:42.320 --> 2:16:46.520
 Yeah. And so you, you know, you can go, you know, and, you know, listeners could go to

2:16:46.520 --> 2:16:50.320
 these reports, they're, they're, they're publicly available. And they explain why in the, in

2:16:50.320 --> 2:16:55.480
 the footnote. And, you know, they, they note that there was this other scenario suggested

2:16:55.480 --> 2:17:00.360
 by at least me and I can remember if they said someone else did too. But they said,

2:17:00.360 --> 2:17:05.160
 you know, we didn't include it because no developers were working on it.

2:17:05.160 --> 2:17:08.800
 Interesting. Full disclosure, that was the approach that

2:17:08.800 --> 2:17:13.960
 I took in my research, right, which was to go to the developers and say, what's your

2:17:13.960 --> 2:17:19.480
 vision, right? What are you trying to develop? That's what I was trying to do. And maybe,

2:17:19.480 --> 2:17:23.400
 you know, and then I tried to think outside the box at the end by adding that one, right?

2:17:23.400 --> 2:17:26.120
 Like here's one that I have, you know, people aren't talking about that could be cool. Now,

2:17:26.120 --> 2:17:31.840
 again, it had been proposed in like 2014 for like fuel convoys. So you could just have

2:17:31.840 --> 2:17:37.760
 like one super armored lead fuel truck, right? In a, you know, bringing fuel to forward operating

2:17:37.760 --> 2:17:42.160
 bases in Afghanistan. And then you wouldn't need, you know, the super heavy, you know,

2:17:42.160 --> 2:17:44.080
 you wouldn't have to protect the human life in the following truck.

2:17:44.080 --> 2:17:47.040
 So that's interesting. You're saying like, when you talk to Waymo, when you talk to

2:17:47.040 --> 2:17:52.200
 these kinds of companies, they weren't at least openly saying they're working on this.

2:17:52.200 --> 2:17:56.040
 So then that doesn't make sense to include in the list.

2:17:56.040 --> 2:18:00.280
 Yeah. And so, but here's the thing, right? This is the Department of Transportation,

2:18:00.280 --> 2:18:05.440
 right? And the Department of Labor. Maybe they could consider some scenarios. Like maybe

2:18:05.440 --> 2:18:09.320
 we could say, you know, this, we, this technology has got a lot of potential. Here's what we'd

2:18:09.320 --> 2:18:14.240
 like it to do. You know, we'd like it to reduce highway deaths, help us fight climate change,

2:18:14.240 --> 2:18:18.440
 reduce congestion, you know, all these other, other things, but that's not how our policy

2:18:18.440 --> 2:18:23.520
 conversation or own technology is happening. We're not, and people don't think that we

2:18:23.520 --> 2:18:27.760
 should. And I think that's the fundamental shift that we need to have, right?

2:18:27.760 --> 2:18:32.760
 I've been involved with this a little bit like NHTSA and DOT. The approach they took

2:18:32.760 --> 2:18:37.360
 is saying, we don't know what the heck we're doing. So we're going to just let the innovators

2:18:37.360 --> 2:18:43.760
 do their thing and not regulate it for a while to just to see. You don't, you think that's,

2:18:43.760 --> 2:18:46.040
 you think DOT should provide ideas themselves?

2:18:46.040 --> 2:18:54.840
 Well, so this is the, this is the great trick in policy of, of private actors is you, you

2:18:54.840 --> 2:19:01.240
 get narrow mandates for government agencies, right? So, you know, this, the safety case

2:19:01.240 --> 2:19:07.000
 will be handled by organizations whose mandate is safety. So the Federal Motor Carrier Safety

2:19:07.000 --> 2:19:13.120
 Administration, who is, you know, going to be a key player, I argue in an article that

2:19:13.120 --> 2:19:16.960
 I wrote, you know, they're going to be a key player in actually determining which scenario

2:19:16.960 --> 2:19:22.440
 is most profitable by setting the rules for truck drivers. Their mandate is safety, right?

2:19:22.440 --> 2:19:26.400
 Now they have lots of good people there who want, you know, who care about truck drivers

2:19:26.400 --> 2:19:32.720
 and who wish truck drivers jobs were better, but they don't have the authority to say,

2:19:32.720 --> 2:19:37.360
 hey, we're going to write this rule because it's good for truck drivers, right? And so when you,

2:19:37.360 --> 2:19:44.560
 you know, we need to say, you know, as a society, we need to not restrict technology, not stand

2:19:44.560 --> 2:19:49.920
 in the way of things. We need to harness it towards the goals that matter, right? Not whatever

2:19:49.920 --> 2:19:54.960
 comes out the end of the pipeline because it's the easiest thing to develop or whatever is most

2:19:54.960 --> 2:20:00.320
 profitable for the first actor or whatever. But, you know, and we do, the thing is we do that,

2:20:00.320 --> 2:20:06.640
 right? I mean, like when we sent people to the moon, you know, we did that. And there were

2:20:06.640 --> 2:20:11.520
 tremendous benefits that followed from it, right? And we do this all the time in, you know, trying

2:20:11.520 --> 2:20:18.240
 to cure cancer or whatever it is, right? I mean, we can do this, right? Now the interesting sort of

2:20:19.760 --> 2:20:25.120
 epilogue to that story is, you know, of six months or so, I don't know how long it was,

2:20:25.120 --> 2:20:32.480
 after those meetings in which that sixth scenario was not considered a company called Locomation,

2:20:33.680 --> 2:20:40.800
 you know, ends up using that, essentially that basic scenario with a slight variation. So they

2:20:40.800 --> 2:20:46.480
 leave the human driver in both trucks. And then that following driver goes off duty. And then,

2:20:48.720 --> 2:20:52.000
 you know, I've been trying to think of what the term is, they kind of, I think of it as like

2:20:52.000 --> 2:20:55.680
 slingshotting. They sort of, when one runs out of hours, you know, the one who's off duty goes

2:20:55.680 --> 2:21:03.920
 in front. And, you know, and so, you know, if only they had been, you know, around six months

2:21:03.920 --> 2:21:08.560
 earlier, that might have been considered by DOT. But it just says, you know, who has the authority

2:21:08.560 --> 2:21:13.520
 to propose what these visions of the future are? Well, some of it is also just the company stepping

2:21:13.520 --> 2:21:18.800
 up and just doing it, screw the authority and showing that it's possible. And then the authority

2:21:18.800 --> 2:21:26.960
 follows. So that's why I really love innovators in the space. The, the criticism I have, the very

2:21:26.960 --> 2:21:33.520
 sort of real, I don't know, harsh criticism I have towards autonomous vehicle companies in the space

2:21:34.400 --> 2:21:41.120
 is they've gotten culturally, they've, it's been, it's become acceptable somehow

2:21:41.120 --> 2:21:49.920
 to do demos and videos as opposed to the old school American way of solving problems.

2:21:50.480 --> 2:21:58.240
 There's, there's a culture in Silicon Valley where you're talking to VCs that have lost that kind of

2:21:58.880 --> 2:22:05.520
 love of solving problems. They kind of like envision, if the story you told me in your

2:22:05.520 --> 2:22:10.640
 PowerPoint presentation is true, how many trillions of dollars might I be able to make?

2:22:10.640 --> 2:22:15.360
 There's something lost in that conversation where you're not really taking on like

2:22:16.160 --> 2:22:20.880
 the problem in a real way. So these autonomous vehicle companies realize we don't need to,

2:22:20.880 --> 2:22:26.400
 we just need to make nice PowerPoint presentations and not actually deliver products that like

2:22:27.280 --> 2:22:32.560
 everybody looks outside and says, holy shit, this is, this is life changing. This is where I have to

2:22:32.560 --> 2:22:40.640
 give props to Waymo is they put driverless cars on the road and like forget PowerPoint slide

2:22:40.640 --> 2:22:45.520
 presentations, actual cars on the road. Then you can criticize like, is that actually going to work?

2:22:45.520 --> 2:22:49.360
 Who knows? But the thing is they have cars on the road and that's why I have to give props to

2:22:49.360 --> 2:22:54.800
 Tesla. They have whatever you want to say about risk and all those kinds of things. They have cars

2:22:54.800 --> 2:23:00.240
 on the road that have some level of automation and soon they have trucks on the road as well.

2:23:00.240 --> 2:23:06.880
 And that kind of, that component, I think is important part of the policy conversation because

2:23:06.880 --> 2:23:11.360
 you get, you start getting data of these, from these companies that are willing to take the big

2:23:11.360 --> 2:23:17.200
 risks as opposed to making slide decks. They're actually putting cars on the road and like

2:23:17.920 --> 2:23:21.920
 real lives are at stake that could be lost and they could bankrupt the company

2:23:21.920 --> 2:23:25.920
 if they make the wrong decisions. And that, that's deeply admirable to me.

2:23:25.920 --> 2:23:30.720
 Speaking of which, I have to ask Waymo Trucks. I think it's called Waymo Via.

2:23:31.840 --> 2:23:36.560
 So I'm talking to the head of trucking at Waymo. I don't know if you've gotten a chance to interact

2:23:36.560 --> 2:23:41.920
 with them. What's a good question to ask the guy? What's a good question of Waymo? Because they seem

2:23:41.920 --> 2:23:49.440
 to be one of the leaders in the space. They have the zen like calm of like being willing to stick

2:23:50.000 --> 2:23:52.720
 with it for the long term in order to solve the problem.

2:23:52.720 --> 2:24:00.960
 Yeah. And I guess they have that luxury, right? Which I don't think I, if I had another life

2:24:00.960 --> 2:24:07.600
 as a researcher, I would love to just study the business strategies of startups and Silicon Valley

2:24:08.720 --> 2:24:12.160
 sort of structure. Would you consider Waymo startup?

2:24:12.160 --> 2:24:18.560
 No. No. No, right? I mean, it's at least not in the things that seem to matter in the self driving

2:24:18.560 --> 2:24:23.600
 space. So you mentioned the demos, you know, and I don't, I don't have enough data as a

2:24:23.600 --> 2:24:29.040
 sociologist to really say like, oh, this is why they do what they do. But, you know, my hypothesis

2:24:29.040 --> 2:24:34.400
 is, you know, there's a real scarcity of talent and money for this. And there certainly was a

2:24:34.400 --> 2:24:40.560
 scarcity of like partnerships with OEMs and, you know, the big trucking companies and there was a

2:24:40.560 --> 2:24:48.880
 race for it, right? And the way that if you don't have, you know, the backing of Alphabet, you do

2:24:48.880 --> 2:24:53.440
 a demo, right? And you get a few more good engineers who say, hey, look, they did that cool thing.

2:24:53.440 --> 2:24:59.200
 Yeah. Like Anthony Lewandowski did with Auto and that resulted in the Uber purchase of that,

2:24:59.200 --> 2:25:06.480
 that program. So what would I, what would I ask? I mean, I think I would ask a lot of questions,

2:25:06.480 --> 2:25:12.240
 but I think there's also on record and off record conversations. Fortunately, I'm asking for an

2:25:12.240 --> 2:25:19.520
 on record conversation. And that I don't know if, if these companies are willing to have

2:25:19.520 --> 2:25:25.040
 interesting on record conversations. Yeah. I mean, I assume that like there are questions that I

2:25:25.040 --> 2:25:28.560
 don't think you'd have to ask. Like I assume they're going to be actually driverless, right?

2:25:28.560 --> 2:25:33.440
 They're not going to like keep the driver in there. Yeah. So, I mean, for the industry,

2:25:33.440 --> 2:25:39.200
 I think it would be interesting to know where they, where they see that first adopter, right?

2:25:39.200 --> 2:25:44.080
 Oh, you mean from like the scenarios that laid out, which one are they going to take on?

2:25:44.080 --> 2:25:48.640
 Yeah. I mean, because that's going to, again, it's those really expensive good jobs, right? So

2:25:48.640 --> 2:25:54.000
 those LTL jobs, the like UPS jobs, now that's going to be, that's where labor is too, right?

2:25:54.000 --> 2:25:58.080
 That's where the Teamsters are. That's the only place they are left, right? So that's the, that's

2:25:58.080 --> 2:26:03.680
 going to be the big fight on the hill and public if, if labor can muster it, right? I don't know.

2:26:04.880 --> 2:26:10.240
 There's a really cool, one thing I would recommend to you and your ear listeners,

2:26:10.240 --> 2:26:15.760
 if you really want to see some like a remarkable page in sort of the history of labor and automation,

2:26:15.760 --> 2:26:23.760
 there's a report that Harry Bridges, who was the socialist leader of the Longshoremen on the

2:26:23.760 --> 2:26:28.320
 West Coast and just, you know, galvanized that union and they still control the ports today

2:26:28.320 --> 2:26:35.760
 because of the sort of vision that he laid down. In the 1960s, he put out a photojournal report

2:26:35.760 --> 2:26:41.360
 called Men and Machines. And basically what it was, was it was an internal education campaign

2:26:42.000 --> 2:26:45.600
 to convince the membership that they had to go along with automation.

2:26:46.560 --> 2:26:51.120
 Machines were coming for their jobs. And what the photojournal, it's almost like a hundred pages

2:26:51.120 --> 2:26:56.080
 or something like that is like, here's how we used to do it. Some of you old timers remember it,

2:26:56.080 --> 2:27:00.800
 like we used to take the barrels of olive oil and we'd stack them in the hold and we'd roll them

2:27:00.800 --> 2:27:05.760
 by hand and we'd put the timber in and we, you know, stack the crates tight, you know, and,

2:27:05.760 --> 2:27:11.120
 and that was the pride of the Longshoremen was a, was a tight stow. And now you all know,

2:27:11.120 --> 2:27:15.840
 you know, their cranes that come down and there's no longer any, you know, rope slings and we're

2:27:15.840 --> 2:27:21.040
 loading bulldozers into the hold to push the ore up into piles and then clamshells are coming down

2:27:21.040 --> 2:27:26.880
 and, and he, he made this case to them and he said, this is why we're signing this agreement

2:27:27.680 --> 2:27:35.600
 to basically allow the employer to automate and we're going to lose jobs, but we're going to

2:27:35.600 --> 2:27:40.400
 get a share of the benefits. And so our wages are going to go up, we're going to continue to control

2:27:40.400 --> 2:27:45.360
 the hiring and training of workers. Our numbers are going to go down, but, you know, basically

2:27:45.360 --> 2:27:48.480
 that last son of a bitch who's working at the ports is going to be one,

2:27:48.480 --> 2:27:54.240
 one really well paid son of a bitch, you know, may just be one standard, but he's,

2:27:54.240 --> 2:27:57.600
 he's going to love his job. You should check out that report.

2:27:57.600 --> 2:28:02.880
 That's an interesting vision of a future that probably still holds. That is, I mean,

2:28:02.880 --> 2:28:05.920
 there is some level to which you have to embrace the automation.

2:28:07.040 --> 2:28:09.840
 Yeah. I mean, and who gets, you know, it's the benefits, right? It's like,

2:28:10.480 --> 2:28:14.080
 I mean, think of the public dollars that went into developing self driving vehicles in the

2:28:14.080 --> 2:28:18.800
 early days, right? Not just the vision of it, right? Which was a public vision to, you know,

2:28:18.800 --> 2:28:24.720
 take soldiers out of harm's way. But, you know, a lot of money.

2:28:24.720 --> 2:28:30.400
 And there's some way, if you are a business that's leveraging the technology from a broad

2:28:31.440 --> 2:28:37.120
 historical ethical perspective, you do owe it to the bigger community

2:28:37.120 --> 2:28:47.120
 to pay back, like for all the investment that was paid to make that technology a reality.

2:28:47.120 --> 2:28:51.440
 In some sense, I don't, I don't know how to make that right, right? On one,

2:28:52.160 --> 2:28:56.800
 there's this pure capitalism and then there's communism and I'm not sure,

2:29:00.240 --> 2:29:03.680
 I'm not sure how to get that balance right.

2:29:03.680 --> 2:29:07.040
 Right. You know, I don't, I don't have all the answers in here, you know, and I don't,

2:29:07.040 --> 2:29:12.480
 I wouldn't expect, you know, individual private companies to kind of kick back, right? That's,

2:29:12.480 --> 2:29:16.640
 capitalism doesn't allow that, right? Unless you have a huge monopoly, right? And then you can,

2:29:16.640 --> 2:29:19.840
 on the backside, create music halls and libraries and things like that.

2:29:21.200 --> 2:29:25.760
 But, you know, here's what I think, you know, the basic obligation is,

2:29:25.760 --> 2:29:33.360
 is, you know, come to the table, like, and have an honest conversation with the policymakers,

2:29:33.360 --> 2:29:38.560
 with the truck drivers, you know, with the communities that are at risk. Like,

2:29:38.560 --> 2:29:43.360
 at least let's talk about these things, you know, in a way that doesn't look like

2:29:43.360 --> 2:29:49.520
 the way lobbying works right now, where you send a well paid lobbyist to the hill to,

2:29:49.520 --> 2:29:54.400
 you know, convince some representative or senator to stick a sentence or two in that

2:29:54.400 --> 2:29:57.680
 favors you into the, like, let's have a real conversation.

2:29:57.680 --> 2:29:59.360
 Real human conversation. Can we just do that?

2:29:59.360 --> 2:30:03.760
 Yeah. Don't play games. Real, real human conversation. Let me ask you,

2:30:05.040 --> 2:30:11.040
 mentioned autopilot, gotta ask you about Tesla, this renegade little company that seems to be,

2:30:11.040 --> 2:30:14.880
 if from my perspective, revolutionizing autonomous driving or semi autonomous driving,

2:30:14.880 --> 2:30:21.280
 or at least the problem of perception and control. They've got a semi on the way.

2:30:21.280 --> 2:30:26.000
 They got a truck on the way. What are your thoughts about Tesla semi?

2:30:26.000 --> 2:30:31.200
 You know, I, and I did have some very preliminary conversations with,

2:30:32.480 --> 2:30:38.400
 with, you know, policy folks there, you know, nothing really in the tech or business side of

2:30:38.400 --> 2:30:44.400
 it too much. And here's why. I think because electrification and autonomy run in opposite

2:30:44.400 --> 2:30:52.000
 directions. And I just, you know, I don't see the application, the value in self driving for

2:30:52.000 --> 2:30:56.240
 the truck that Tesla is going to produce in the near term. You know, they're just,

2:30:56.240 --> 2:31:01.280
 you're not going to have the battery. Now you could have wonderful safety systems and,

2:31:01.280 --> 2:31:07.360
 you know, reinforcing, you know, the auto, you know, self driving features supporting a skilled

2:31:07.360 --> 2:31:12.480
 driver, but you're not going to be able to pull that driver out for long stretches the way that

2:31:12.480 --> 2:31:20.640
 you are with driverless trucks. So do you think, I mean, the reason, so the electrification

2:31:22.240 --> 2:31:24.320
 is not obviously coupled with the automation.

2:31:27.680 --> 2:31:34.160
 They have a very interesting approach to semi autonomous pushing towards autonomous driving.

2:31:34.160 --> 2:31:44.080
 All right. It's very unique. No LiDAR. Now no radar. It's computer vision alone. From a large,

2:31:44.080 --> 2:31:48.720
 they're collecting huge amounts of data from a large fleet. It's an interesting unique approach.

2:31:49.520 --> 2:31:55.440
 Bold and fearless in this direction. If I were to guess whether this approach would work, I would

2:31:55.440 --> 2:32:04.000
 say no, it started. One, you would need a lot of data and two, because you have actual cars

2:32:04.000 --> 2:32:11.200
 deployed on the road using a beta version of this product, you're going to have a system that's far

2:32:11.200 --> 2:32:16.480
 less safe and you're going to run into trouble. It's horrible PR. Like it just seems like a nightmare,

2:32:17.440 --> 2:32:23.280
 but it seems to not be the case, at least up to this point. It seems to be not, you know,

2:32:24.080 --> 2:32:29.200
 on par, if not safer, and it seems to work really well. And the human factor somehow

2:32:29.200 --> 2:32:37.120
 manages like drivers still pay attention. Now there's a selection of who is inside the Tesla

2:32:37.120 --> 2:32:43.120
 autopilot user base, right? There could be a self selection mechanism there. But however it works,

2:32:43.680 --> 2:32:49.760
 these things are not running off the road all the time. So it's very interesting whether that can

2:32:49.760 --> 2:32:57.840
 sort of creep into the trucking space. Yes. At first, the long haul problem is not solved.

2:32:57.840 --> 2:33:03.920
 They need to charge. But maybe you can solve, you know, a lot of your scenarios involved small

2:33:05.360 --> 2:33:12.240
 distances. And, you know, that last mile aspect, which is exactly what Tesla is trying to solve for

2:33:15.280 --> 2:33:21.920
 the regular passenger vehicle space is the city driving. It's possible that you have these trucks.

2:33:21.920 --> 2:33:29.600
 It's almost like, yeah, you solve the last mile delivery part of some of the scenarios that you

2:33:29.600 --> 2:33:35.920
 mentioned in autonomous driving space. Do you think that's from the people you've spoken with

2:33:35.920 --> 2:33:42.560
 too difficult of a problem? The thing that keeps me so interested in this space and thinking that

2:33:42.560 --> 2:33:49.680
 it's so important is, again, that efficiency question, that safety question, and the way

2:33:49.680 --> 2:33:55.680
 that these economics can push us potentially toward a more efficient system. So I want to see

2:33:55.680 --> 2:34:00.880
 those Tesla electric trucks running out to those truck ports where you've got those two

2:34:02.400 --> 2:34:08.480
 trucks with a human driver in front, right? I think that's now what's powering those is

2:34:08.480 --> 2:34:13.920
 in hydrogen. Again, it's very interesting as a researcher who just thought of a background

2:34:13.920 --> 2:34:20.560
 in technology and doesn't have a horse in this race. I mean, for all I know,

2:34:20.560 --> 2:34:26.400
 self driving trucks will ultimately be achieved by some biomechanical sensor that uses echolocation

2:34:26.400 --> 2:34:35.680
 because we took stem cells of bats. I have a completely unable to assess who's the

2:34:35.680 --> 2:34:40.400
 head or who's behind or who makes sense. But I think one key component there, and this is what

2:34:40.400 --> 2:34:46.320
 I see with Tesla often. And it's quite sad to me that other companies don't do this enough.

2:34:47.280 --> 2:34:51.680
 Is that first principles thinking? Like, wait, wait, wait, wait. Okay. It's looking at the

2:34:51.680 --> 2:34:58.240
 inefficiencies as opposed to, I've worked with quite a few car companies and they basically have

2:34:58.960 --> 2:35:04.320
 a lot of meetings. There's a lot of meetings. And the discussion is like, how can we make this

2:35:04.320 --> 2:35:08.320
 cheaper, this cheaper, this cheaper, this component cheaper, this cheaper, the cheapification of

2:35:08.320 --> 2:35:13.520
 everything, just like you said, as opposed to saying, wait a minute, let's step back. Let's

2:35:13.520 --> 2:35:18.720
 look at the entirety of the inefficiencies in the system. Like, why have we been doing this

2:35:18.720 --> 2:35:25.360
 like this for the last few decades? Like, start from scratch, can this be 10x, 100x cheaper?

2:35:25.360 --> 2:35:32.400
 Like, if we not just decrease the cost of one component here or this component here or this

2:35:32.400 --> 2:35:42.160
 component here, but let's redesign everything. Let's infrastructure. Let's have special lanes.

2:35:42.800 --> 2:35:47.440
 Or in terms of truck ports, as opposed to having regular human control truck ports,

2:35:47.440 --> 2:35:54.080
 have some kind of weird sensors where everything about the truck

2:35:55.360 --> 2:36:00.640
 connecting at that final destination is automated fully from the ground up. You build the facility

2:36:00.640 --> 2:36:06.640
 from the ground up for the autonomous truck. All those kinds of sort of questions are platooning.

2:36:06.640 --> 2:36:12.880
 Let's say, wait a minute, okay, I know we think platooning is hard, but can we think through

2:36:12.880 --> 2:36:18.000
 exactly why it's hard and can we actually solve it? Like, if we collect a huge amount of data,

2:36:18.000 --> 2:36:25.280
 can we solve it? And then teleoperation. Like, okay, yeah, yeah, it's difficult to have good signal.

2:36:25.280 --> 2:36:31.520
 But can we actually, can we have, can we consider the probability of those edge cases and what to

2:36:31.520 --> 2:36:36.160
 do in the edge cases when the teleoperation fails? Like, how difficult is this? What are the costs?

2:36:36.160 --> 2:36:41.840
 How do we actually construct a teleoperation center full of humans that are able to pay attention to

2:36:42.480 --> 2:36:46.400
 a large fleet where the average number of vehicles per human is like 10 or 100?

2:36:46.880 --> 2:36:53.040
 You know, like, having that conversation as opposed to kind of having, you know, you show up to work

2:36:53.040 --> 2:36:59.760
 and say, all right, it seems like, you know, because of COVID, we, you know, are not making

2:36:59.760 --> 2:37:05.040
 as much money. Can we have a cheaper, can we give less salary to the trucker? And can we

2:37:06.880 --> 2:37:13.840
 build, like, decrease the cost or decrease the frequency at which we buy new trucks.

2:37:14.480 --> 2:37:20.000
 And when we do buy new trucks, make them cheaper by making them crappier, like this kind of discussion.

2:37:20.000 --> 2:37:25.520
 This is why, to me, it's like Tesla is like rare on this. And there's some sectors in which

2:37:25.520 --> 2:37:30.000
 innovation is part of the culture. In the automotive sector, for some reason, it's not as much.

2:37:31.040 --> 2:37:35.120
 This is obviously the problem that Ford and GM are struggling with. It's like,

2:37:35.120 --> 2:37:42.400
 they're really good at making cars at scale cheap. And they're like legit good, like Toyota at this.

2:37:42.400 --> 2:37:44.960
 They're some of the greatest manufacturing people in the world, right?

2:37:44.960 --> 2:37:45.920
 That's incredible.

2:37:45.920 --> 2:37:52.880
 But then when it comes to hiring software people, they're horrible. So it's culture.

2:37:52.880 --> 2:37:58.640
 And then it's such a difficult thing for them to sort of embrace. But greatness requires that

2:37:58.640 --> 2:38:04.880
 they embrace this, embrace whatever is required to remove the inefficiency from the system.

2:38:04.880 --> 2:38:08.480
 And that may require you to do things very differently than you've done in the past.

2:38:09.280 --> 2:38:13.760
 Yeah. I mean, there are certain things that the market can do well in my, you know,

2:38:13.760 --> 2:38:21.440
 this is how I see the world, right? And that's the best way to organize certain kinds of activities

2:38:21.440 --> 2:38:27.920
 is the market and private interest. But I think we go too far in some areas.

2:38:28.800 --> 2:38:37.360
 Transportation is, if we can't have a public debate about the roads that we all pay for,

2:38:39.040 --> 2:38:43.040
 forget about it, private factories and all these other healthcare and other places,

2:38:43.040 --> 2:38:50.320
 it's going to be way harder there. Healthcare, I guess, has some direct contact with the consumer

2:38:50.320 --> 2:38:55.920
 where we're probably going to have lots of sort of hands on public policy about concerns around

2:38:55.920 --> 2:39:01.920
 patient rights and things like that. But if we can't figure out how to have a public policy

2:39:01.920 --> 2:39:08.080
 conversation around how technology is going to reform our public roadways and our transportation

2:39:08.080 --> 2:39:16.400
 system, we're really leaving way too much to private companies. And it's just, it's not in

2:39:16.400 --> 2:39:20.720
 there. I get asked this question, like, what should companies do? And I'm like, just go about

2:39:20.720 --> 2:39:25.920
 doing what you're doing. I mean, please come to the table and talk about it. But it's not their

2:39:25.920 --> 2:39:36.320
 role. I mean, I appreciate Elon's attempts to have species level goals. We're going to go to Mars.

2:39:36.320 --> 2:39:43.600
 I mean, that's amazing. And that's incredible that someone can realize, have a chance at

2:39:43.600 --> 2:39:49.120
 realizing that vision. It's amazing, right? But when it comes to so many areas of our economy,

2:39:50.080 --> 2:39:55.680
 we can't wait for a hero. We have to have, and there are way too many interests involved.

2:39:56.480 --> 2:40:02.880
 It's who builds the roads. I mean, the money that sloshes around on Capitol Hill to decide what

2:40:02.880 --> 2:40:08.960
 happens in these infrastructure bills and the transportation bill is just obscene, right?

2:40:08.960 --> 2:40:15.360
 See, I think it's just an interesting view of markets. Correct me if I'm wrong. Let me propose

2:40:15.360 --> 2:40:23.040
 a theory to you that progress in the world is made by heroes and the markets remove the

2:40:23.040 --> 2:40:29.440
 inefficiencies from the work the heroes did. So going to Mars from the perspective of markets

2:40:29.440 --> 2:40:35.040
 probably has no value. Maybe you can argue it's good for hiring to have a vision or something

2:40:35.040 --> 2:40:43.680
 like that. But those big projects don't seem to have an obvious value. But our world progresses

2:40:44.240 --> 2:40:51.440
 by those big leaps. And then as after the leaps are taken, then the markets are very good at

2:40:51.440 --> 2:40:57.520
 removing inefficiencies. But it just feels like the autonomous vehicle space and the autonomous

2:40:57.520 --> 2:41:04.720
 trucking space requires leaps. It doesn't feel like we can sneak up into a good solution that

2:41:04.720 --> 2:41:09.280
 is ultimately good for labor, like for human beings in the system. It feels like some,

2:41:10.240 --> 2:41:16.640
 like probably a bad example, but like a Henry Ford type of character steps in and says like,

2:41:16.640 --> 2:41:24.000
 we need to do stuff completely differently. Yeah. And you said we can't hope for a hero.

2:41:24.000 --> 2:41:30.160
 But it's like, no, but we can say we need a hero. We need more heroes. So if you're a young kid

2:41:30.160 --> 2:41:34.800
 right now listening to this, we need you to be a hero. It's not like we need you to start a company

2:41:34.800 --> 2:41:39.360
 that makes a lot of money. No, you need to start a company that makes a lot of money so that you

2:41:39.360 --> 2:41:44.480
 can feed your family as you become a hero and take huge risks and potentially go bankrupt.

2:41:45.040 --> 2:41:51.200
 Those risks is how we move society forward. I think maybe there's a romantic view. I don't know.

2:41:51.200 --> 2:41:57.840
 I totally disagree. You disagree. God damn it. And out of the two of us, you're the knowledgeable

2:41:57.840 --> 2:42:04.880
 one. No, I think it's a matter of like, do we need those heroes? Absolutely. I mean,

2:42:05.600 --> 2:42:16.880
 I saw the boosters come down from SpaceX's rockets and land nearly simultaneously with my kids

2:42:16.880 --> 2:42:25.760
 after school one day. And I thought, oh my god, like science fiction has been made real. It's

2:42:25.760 --> 2:42:33.440
 incredible. And it's a pinnacle of human achievement. It's like this is what we're capable of. But we

2:42:33.440 --> 2:42:43.120
 need to have that those heroes oriented. We need to allow them to orient toward the goals. We've

2:42:43.120 --> 2:42:50.960
 got climate change. You know, I mean, all the heroes out there, right? I mean, it's time. The

2:42:50.960 --> 2:42:57.760
 clock is ticking. It's past time. I've been working on climate change issues since the mid 90s.

2:42:58.560 --> 2:43:09.040
 Like, I still remember the first time in 2010 when I got a grant that was completely focused on

2:43:09.040 --> 2:43:17.120
 adaptation rather than prevention. And just when it hit me, that like, wow.

2:43:18.400 --> 2:43:25.280
 So adaptation versus prevention is like acceptance that there's going to be catastrophic impact.

2:43:25.280 --> 2:43:29.600
 We just need, we need to figure out how we at least live with that. Yeah. And you know,

2:43:29.600 --> 2:43:33.760
 the grant was like, okay, our agriculture system is going to move, our bread basket is no longer

2:43:33.760 --> 2:43:38.320
 going to be California. It's going to be Illinois. What does that mean for truck transportation?

2:43:38.320 --> 2:43:43.920
 So it's like, so in terms of a big philosophical societal level, that's kind of like giving up

2:43:44.880 --> 2:43:48.880
 in terms of the big heroic actions. You know, failures in human history.

2:43:50.560 --> 2:43:58.080
 Yeah, that's going to be, let's hope not the biggest, but could be. So let me say why I

2:43:58.080 --> 2:44:04.880
 disagree, right? Henry Ford, amazing, right, to sort of mass produce cars, right? Daimler,

2:44:04.880 --> 2:44:10.880
 to put that first truck on the road without the roads, right? So there's like, we need that

2:44:10.880 --> 2:44:15.840
 innovation. There's no doubt about it. And there's, there are roles for that, but there's big public

2:44:15.840 --> 2:44:22.320
 stuff that, that, that sets the stage that's critical. And, you know, and what it really is,

2:44:22.320 --> 2:44:26.960
 it's a, it's a sociological problem, right? It's a political problem. It's a social problem. We

2:44:26.960 --> 2:44:31.920
 have to say, and we have these screwed up ideas, right? So we have this politics right now where

2:44:31.920 --> 2:44:37.840
 like everybody feels like they're getting screwed and someone undeserving is, you know, is benefiting

2:44:37.840 --> 2:44:42.160
 when in fact, like, you know, at least in the middle, right? They're huge. I used to teach this

2:44:42.160 --> 2:44:46.960
 course in rich and poor, you know, in economic inequality. And I would go through, you know,

2:44:46.960 --> 2:44:53.360
 public housing subsidies in Philadelphia, you know, section eight subsidies, you know,

2:44:53.920 --> 2:44:59.520
 and then I would go through my housing subsidies for my mortgage interest deduction.

2:44:59.520 --> 2:45:05.520
 And it worked out to basically the average payment for a section eight housing voucher

2:45:05.520 --> 2:45:10.800
 in my neighborhood. I'm not a welfare recipient, according to the dominant discourse. And so we

2:45:10.800 --> 2:45:15.920
 have this completely screwed up sense of like where our dollars go and, you know, where the,

2:45:15.920 --> 2:45:21.360
 who benefits from the investment. And, you know, we need to, you know, we, I don't know that we

2:45:21.360 --> 2:45:28.000
 can do it, but, you know, if we're going to survive, we need to figure out how to have honest

2:45:28.000 --> 2:45:34.320
 conversations where private interest is where we need it to be in fostering innovation and,

2:45:34.320 --> 2:45:39.280
 you know, and rewarding the people who do incredible things, please, you know, we don't

2:45:39.280 --> 2:45:45.280
 want to squash that, but we need to harness that power to solve what I think are some pretty big,

2:45:45.280 --> 2:45:51.440
 you know, existential problems. So you think there's like government level, national level

2:45:51.440 --> 2:46:00.480
 collaboration required for infrastructure project, like there's, we should really have large moonshot

2:46:00.480 --> 2:46:06.880
 projects that are funded by our governments. At least guided by, I mean, I think there are

2:46:06.880 --> 2:46:10.800
 ways to finance them and, you know, other things, but we, we gotta be careful, right?

2:46:10.800 --> 2:46:14.800
 Because that's where you get all these sort of perverse, you know, unintended consequences

2:46:14.800 --> 2:46:20.480
 and whatnot. But if you look at transportation in the United States, and it is the foundation of

2:46:20.480 --> 2:46:27.040
 the, you know, manifest destiny, economic growth, right, that built the United States into the

2:46:27.840 --> 2:46:32.240
 world superpower that it became and the industrial power that it became, it rested on

2:46:32.240 --> 2:46:38.000
 transportation, right? It was like, you know, the Erie Canal, I grew up a few miles from where they

2:46:38.000 --> 2:46:43.120
 dug the first shovel full of the Erie Canal and everyone thought it was, you know, crazy, right?

2:46:44.000 --> 2:46:49.200
 But those public infrastructure projects, the canals, right, the railroads, yeah, they were

2:46:49.200 --> 2:46:53.520
 privately built, but they wouldn't have been privately built without, you know, Lincoln

2:46:53.520 --> 2:46:59.600
 funding them, essentially, and giving, you know, the railroads, you know, land in exchange for

2:46:59.600 --> 2:47:06.640
 building them. The highway system, the Eisenhower, the, the, the payback that the U.S. economy got

2:47:06.640 --> 2:47:13.200
 from the Dwight D. Eisenhower interstate system is phenomenal, right? No private entity was gonna

2:47:13.200 --> 2:47:21.440
 do that, electrification, dams, water, you know, we, we need to do these infrastructure, infrastructure.

2:47:21.440 --> 2:47:25.440
 And now more than ever, it's been really upsetting to me on the COVID front.

2:47:27.120 --> 2:47:32.400
 There's one of the solutions to COVID, which seems obvious to me, from the very beginning,

2:47:32.400 --> 2:47:39.680
 that nobody is opposed to. It's one of the only bipartisan things is at home testing,

2:47:39.680 --> 2:47:46.240
 rapid at home testing. There's no reason why at the government level, we couldn't manufacture

2:47:46.240 --> 2:47:52.480
 hundreds of millions of tests a month. There's no reason starting in May, 2020. And that gives

2:47:52.480 --> 2:47:57.120
 power to a country that values freedom, that gives power information to each individual to

2:47:57.120 --> 2:48:03.440
 know whether they have COVID or not. So it's possible to manufacture them for under a dollar.

2:48:04.080 --> 2:48:08.880
 It's like an obvious thing. It's kind of like the roads. It's like everybody's invested.

2:48:08.880 --> 2:48:13.360
 Let's put countless tests in the hands of every single American citizen,

2:48:13.360 --> 2:48:20.240
 maybe every citizen of the world. The fact that we haven't done that today, and there's some

2:48:20.240 --> 2:48:25.280
 regulation stuff with the FDA, all the kind of dragon of feet, but there's not actually a good

2:48:25.280 --> 2:48:36.160
 explanation, except our leaders and culturally, we've lost the sort of, not lost, but it's a

2:48:36.160 --> 2:48:44.400
 little bit dormant, the will to do these big projects that better the world. I still have the

2:48:44.400 --> 2:48:52.960
 hope that when faced with catastrophic events, the more dramatic, the more damaging, the more

2:48:52.960 --> 2:48:58.000
 painful they are, the higher we will rise to meet those. And that's where the infrastructure

2:48:58.000 --> 2:49:04.240
 style projects are really important. But it's certainly a little bit challenging to remain

2:49:04.240 --> 2:49:09.280
 an optimist in the times of COVID, because the response of our leaders has not been

2:49:09.280 --> 2:49:17.440
 as great and as historic as I would have hoped. I would hope that the actions of leaders in the

2:49:17.440 --> 2:49:22.960
 past few years in response to COVID would be ones that are written in the history books.

2:49:22.960 --> 2:49:28.480
 And we talk about it as we talk about FDR, but sadly, I don't know. I think the history books

2:49:28.480 --> 2:49:39.280
 will forget the actions of our leaders. Let me just, to wrap up autonomy,

2:49:42.000 --> 2:49:50.960
 when you look into the future, are you excited about automation in the space of

2:49:50.960 --> 2:50:00.960
 trucking? When you go to bed at night, do you see a beautiful world in your vision

2:50:01.600 --> 2:50:07.680
 that involves autonomous trucks? All the truckers you've become close with, you've talked to,

2:50:08.560 --> 2:50:11.360
 do you see a better world for them because of autonomous trucks?

2:50:11.360 --> 2:50:20.800
 Damn you, Alex. Because I want to be an optimist. And I want to think of myself,

2:50:20.800 --> 2:50:26.480
 I guess, as a half glass bowl kind of person. But when you ask it like that, and I think about,

2:50:30.480 --> 2:50:40.560
 when I look at the challenges to harnessing that for just labor and climate, right?

2:50:40.560 --> 2:50:44.560
 There are other issues, congestion, et cetera, infrastructure that are going to be affected

2:50:44.560 --> 2:50:52.480
 by this, again, those big transformational issues. I think it's going to take the best of us.

2:50:53.120 --> 2:51:00.320
 Like it's going to take the best of our policy approaches. It's going to take, we need to start

2:51:00.320 --> 2:51:06.800
 investing in rebuilding those institutions. I mean, that's what we've seen in the last four

2:51:06.800 --> 2:51:14.000
 years, right? And the erosion of that was so clear among these truck drivers. When Trump

2:51:14.960 --> 2:51:24.240
 came in and said, free trade's good for workers, yeah, right. I grew up in the Rust Belt. I watched

2:51:24.240 --> 2:51:29.280
 factory after factory close. All of my ancestors worked at the same factory. It's still holding

2:51:29.280 --> 2:51:37.120
 on by a thread. Like, the Democratic Party told blue collar workers for years, I don't worry

2:51:37.120 --> 2:51:41.200
 about free trade. It's not bad for you. And I know the economists will probably get in the

2:51:41.200 --> 2:51:46.640
 comment box now. We'll look forward to your comments. We'll look forward to your comments

2:51:46.640 --> 2:51:55.680
 about how free trade benefits everybody. But immigration, you go, and I think immigration

2:51:55.680 --> 2:52:01.840
 is great. The United States benefits from it tremendously, right? But there are costs, right?

2:52:01.840 --> 2:52:08.160
 Go down to South Philadelphia and find a drywaller and tell him that immigration hasn't hurt him,

2:52:08.160 --> 2:52:15.600
 right? Go to these places where there's competition, right? And yes, we benefit overall,

2:52:15.600 --> 2:52:23.280
 but we have a system that allows some people to pay really high costs. And Trump tapped

2:52:23.280 --> 2:52:30.880
 into that. And there was more than that, too, obviously. And there's lots of really dark stuff

2:52:30.880 --> 2:52:37.040
 that goes along with it, the sort of racialization of others and things like that. But he hit on

2:52:37.040 --> 2:52:43.600
 those core issues that if you were to go back over my trucking interviews for 15 years,

2:52:43.600 --> 2:52:47.680
 you would have heard those stories over and over and over again, that sense of voicelessness,

2:52:47.680 --> 2:52:51.760
 that sense of powerlessness, that sense that there's no difference between the Democrats and

2:52:51.760 --> 2:52:57.040
 the Republicans, because they're all going to screw us over. And that was there. And you just

2:52:57.040 --> 2:53:00.640
 ignore it as long as you want and tell people, don't worry, trade's good for you. Don't worry,

2:53:00.640 --> 2:53:05.120
 immigration's good for you, as their communities lose factories. And I mean, a lot of them were

2:53:05.120 --> 2:53:12.000
 lost to the South before they were lost overseas, whatever, but tapped into that. And there's a

2:53:12.000 --> 2:53:17.600
 fundamental distrust of, you look at these like cue polls on whether people trust the media,

2:53:17.600 --> 2:53:23.280
 but whether or not they trust higher education, these institutions that I find magical.

2:53:24.080 --> 2:53:32.640
 I mean, you look at the vaccine research and stuff, just brilliant people doing incredible

2:53:32.640 --> 2:53:40.800
 things for humanity. The idea that we can take these viruses that used to ravage through the

2:53:40.800 --> 2:53:49.520
 human population and that we had to be terrified of. And we've suffered, but we have such power now

2:53:50.560 --> 2:53:57.520
 to defend ourselves behind these programs. And to see those people be like, I'm not sure if higher

2:53:57.520 --> 2:54:03.360
 education is good for the country or not. It's like, where are we? So we need to rebuild the

2:54:03.360 --> 2:54:07.280
 faith and trust in those institutions and have these, but we need to have honest conversations

2:54:07.280 --> 2:54:12.720
 before people are going to buy it. Do you have ideas for rebuilding the trust and giving a voice

2:54:12.720 --> 2:54:20.640
 to the voices? So many of the things we've been talking about is so deeply integrated.

2:54:21.280 --> 2:54:26.880
 You think like, this is the trouble I have with people that work on AI and autonomous vehicles

2:54:26.880 --> 2:54:38.080
 and so on. It's not just a technology problem. It's this human pain problem. It's the robot

2:54:38.080 --> 2:54:43.280
 essentially silencing the voice of a human being because it's lowering their wage, making

2:54:43.280 --> 2:54:50.000
 them suffer more and giving them no tools of how to escape that suffering. Is there something,

2:54:50.000 --> 2:54:56.720
 I mean, it even gets into the question of meaning. So money is one thing,

2:54:58.320 --> 2:55:07.680
 but it's also what makes us happy in life. A lot of those truckers, the set of jobs they've had in

2:55:07.680 --> 2:55:15.600
 their life were defining to them as human beings. And so, and the question with automation is not

2:55:15.600 --> 2:55:23.920
 just how do we have a job that gives you money to feed your family, but also a job that gives

2:55:23.920 --> 2:55:37.840
 you meaning, that gives you pride. And for me, the hope is that AI and automation will provide

2:55:37.840 --> 2:55:46.880
 other jobs that will be a source of meaning. But coupled with that hope is that there will not

2:55:46.880 --> 2:55:52.400
 be too much suffering in the transition. And that's not obvious from the people you've spoken with.

2:55:53.280 --> 2:55:57.440
 I mean, I think we need to differentiate between the effects of technology and the effects of

2:55:57.440 --> 2:56:06.560
 capitalism. And the fact that workers don't have a lot of power in the system matters.

2:56:06.560 --> 2:56:12.960
 Now, we had a system, and that's why I would say, go to that Harry Bridges report. And

2:56:13.760 --> 2:56:19.200
 those were workers who had a sense of power. They said, we can demand some of the benefits,

2:56:19.200 --> 2:56:26.640
 like, yeah, automate our jobs away, but kick a little down to us. And we had in the golden era

2:56:26.640 --> 2:56:33.520
 of American industrialism in post World War II, that was the contract. The contract was,

2:56:33.520 --> 2:56:37.760
 employers can do what they want in automation and all these things. Yeah, sure, there's some

2:56:37.760 --> 2:56:44.320
 union rules that make things less efficient in places. But the key compromise is tie wages to

2:56:44.320 --> 2:56:49.440
 productivity. That's what we did. That's what unions did. They tied wages to productivity,

2:56:49.440 --> 2:56:53.040
 kept them and up, right? It was good for the economy, some economists think, right?

2:56:54.080 --> 2:57:01.040
 And that's what we need to, I think we need to acknowledge that. We need to acknowledge the

2:57:01.040 --> 2:57:09.440
 the fact that it's not just technology, it's technology in a social context in which some

2:57:09.440 --> 2:57:14.240
 people have a lot of power to determine what happens. For me, I don't have all the answers,

2:57:14.240 --> 2:57:20.800
 but I know what my answer is. And my answer is, and I think I started with this, I can learn

2:57:20.800 --> 2:57:29.120
 from every single person. Did I have to talk to the 200th truck driver? In my opinion, yes,

2:57:29.120 --> 2:57:36.240
 because I was going to learn something from that 200th truck driver. Now, people with more power

2:57:37.200 --> 2:57:47.360
 might talk to none or they might talk to five and say, okay, I got it. People are amazing and

2:57:47.360 --> 2:57:54.400
 every one of them has a life experience and concerns and can teach us something. And they're

2:57:54.400 --> 2:58:01.920
 not in the conversation. And I know this because I'm the expert. So I get pulled in to these

2:58:01.920 --> 2:58:09.840
 conversations and people want to know what's going to happen to labor. So I try to be a sounding

2:58:09.840 --> 2:58:20.080
 board and I feel a tremendous weight of responsibility for that. But I'm not those workers

2:58:20.080 --> 2:58:26.960
 and they may listen to this or walk in the door sometime. It's about to be like,

2:58:26.960 --> 2:58:34.000
 that guy's full of shit. That's not what I think at all. And they don't get heard over and over

2:58:34.000 --> 2:58:37.680
 and over. But in a small way, you are providing a voice to them. And that's kind of the,

2:58:39.280 --> 2:58:46.000
 if at scale, we apply that empathy and listening that we could provide the voice to the voices

2:58:46.000 --> 2:58:51.280
 through our voice, through our money, through, I mean, that's one way to make capitalism work at

2:58:51.280 --> 2:58:59.120
 not making the powerless more powerless is by all of us being a community that listens to the

2:58:59.120 --> 2:59:03.920
 pain of others and tries to minimize that to try to give a voice to the voices to give power to

2:59:03.920 --> 2:59:11.200
 the powerless. I have to ask you on by way of advice, young people, high school students,

2:59:11.200 --> 2:59:21.920
 college students entering this world full of automation, full of these complex labor markets

2:59:21.920 --> 2:59:27.200
 and markets period, what would you, what kind of advice would you give to that person about how

2:59:27.200 --> 2:59:34.400
 to have a career? How do I have a life that can be proud of? Yeah, I think, you know, this is such

2:59:34.400 --> 2:59:46.960
 a great question. I don't, it's okay to quote Steve Jobs, right? Always. Yeah, I mean, so,

2:59:47.920 --> 2:59:51.920
 and I just heard this recently, it was a commencement speech that he gave, and I can't

2:59:51.920 --> 2:59:56.080
 remember where it was. And he was talking about, you know, he, you know, he had famously dropped

2:59:56.080 --> 3:00:01.920
 out of school, but continued to take classes, right? And, and he took a calligraphy class,

3:00:01.920 --> 3:00:07.040
 and it influenced the design of the Mac and, and sort of fonts and, you know, just was, was

3:00:07.040 --> 3:00:12.560
 something that he had no, you know, sense of what it was going to be useful for. And his, his lesson

3:00:12.560 --> 3:00:17.600
 was, you know, you, you can't connect the dots looking forward, you know, looking back, you

3:00:17.600 --> 3:00:24.000
 can see all the pieces that sort of led you to where you ended up. And for me, studying truck

3:00:24.000 --> 3:00:28.720
 driving, like, I mean, I literally went to graduate school because I was worried about climate change

3:00:28.720 --> 3:00:32.960
 and like, you know, I had a whole other dissertation plan and then was like driving home and like I

3:00:32.960 --> 3:00:37.520
 had read about all this management literature and sort of like how you get workers to work hard

3:00:37.520 --> 3:00:44.160
 for my qualifying exams. And then read a popular article on, on satellite linked computers. And

3:00:44.160 --> 3:00:48.640
 the story in the literature was sort of sense of autonomy. And, and I was like, well, that

3:00:49.200 --> 3:00:53.040
 monitoring must affect the sense of autonomy. And it's just this question that I found

3:00:53.040 --> 3:00:57.040
 interesting. And it never in a million years that I ever thought I was going to like, you know,

3:00:57.040 --> 3:01:06.560
 spend 15 years of my life studying truck driving. And it was like, if you were to map out a career

3:01:06.560 --> 3:01:14.000
 path in academia or research, like, you know, you would, you would do none of the things that I did

3:01:14.000 --> 3:01:18.240
 that many people advise me against where like, you can't like go spend a year working as a

3:01:18.240 --> 3:01:22.880
 truck driver, you know, like, that's crazy. Or, you know, you can't, you know, spend all this time

3:01:22.880 --> 3:01:28.080
 trying to write like one huge book. And, you know, so I mean, by the way, if I could just interrupt,

3:01:28.080 --> 3:01:34.560
 what, what, what was the fire that got you to take the leap and go and work as a truck driver

3:01:34.560 --> 3:01:40.240
 and go interview truck drivers? This is what a lot of people would be incapable of doing. Just took,

3:01:40.240 --> 3:01:45.440
 took that leap. What the heck, what the heck is up with your mind that allowed you to take that big

3:01:45.440 --> 3:01:53.680
 leap? So I think it's probably like, Tolkien and Lord of the Rings, you know, I mean, as a teenager,

3:01:53.680 --> 3:02:00.480
 you know, I sort of adopted some sense of needing to, you know, heroically go out in the world and,

3:02:00.480 --> 3:02:05.360
 and, you know, which I've done at various points in my life and like looking back in absolutely

3:02:05.360 --> 3:02:10.240
 stupid ways that, you know, where I could have completely ended up dead and traumatized my,

3:02:10.240 --> 3:02:16.400
 my family, including like a couple of week trip in the Pacific, like solo trip on a kayak. And

3:02:16.400 --> 3:02:21.120
 basically my kayak experience up till that, you know, point had been, you know, on a fairly calm

3:02:21.120 --> 3:02:27.520
 lake and like class one solo trip on a kayak in the Pacific. Yeah. So I was working on forestry

3:02:27.520 --> 3:02:34.240
 issues. And we were starting a campaign up in really remote British Columbia. And I was like,

3:02:34.240 --> 3:02:37.760
 okay, if I'm going to work on this, I've got to actually go there myself and see what this is all

3:02:37.760 --> 3:02:42.720
 about and see whether it's worth like devoting my sort of, you know, life right now too. And just

3:02:43.200 --> 3:02:49.680
 drove up there with this kayak and, you know, put into the Pacific. And it was insane, you know,

3:02:49.680 --> 3:02:56.320
 like the tides are huge. And, you know, there was one point in which I was going down a fjord.

3:02:56.320 --> 3:03:02.720
 And two fjords kind of came up and there was a cross channel. And I had hit the timing completely

3:03:02.720 --> 3:03:08.480
 wrong. And the tide was sort of rushing up like, you know, rivers in these, you know, two fjords.

3:03:08.480 --> 3:03:13.520
 And then coming through this cross channel and met and created this giant standing wave

3:03:14.240 --> 3:03:19.680
 that I had to paddle through. And now actually very recently, I've gone out on whitewater with

3:03:19.680 --> 3:03:25.440
 some people who know what the hell they're doing. And I realized like, just how absolutely stupid

3:03:26.160 --> 3:03:31.040
 and, you know, ill fit I was, but that's just that I think I've always had that.

3:03:31.040 --> 3:03:34.800
 Were you afraid when you had that wave before you scared the shit out of me?

3:03:34.800 --> 3:03:38.160
 Yeah. Okay. What about taking a leap and becoming a trucker?

3:03:39.040 --> 3:03:44.160
 Yeah, there was some nervousness for sure. I mean, and, you know, I guess my advantage

3:03:44.160 --> 3:03:50.000
 as an ethnographer is I grew up in a blue collar environment, you know, again, all my

3:03:51.200 --> 3:03:57.920
 ancestor for factory workers. So I can move through spaces. I'm really,

3:03:57.920 --> 3:04:03.840
 I feel comfortable, I can become comfortable in lots and lots of places, you know, not everywhere,

3:04:03.840 --> 3:04:09.120
 but, you know, along class lines for sort of white, you know, even white ethnic workers,

3:04:09.120 --> 3:04:14.320
 like that's, you know, I can move in those spaces fairly easily. I mean, not entirely,

3:04:14.320 --> 3:04:18.320
 there was one, there was one time where I was like, okay, you know, and I grew up around people

3:04:18.320 --> 3:04:23.840
 worked on cars, I've been to drag races and NASCAR and, and I've been to, you know, Colgate

3:04:23.840 --> 3:04:28.240
 University and so I'd, and I think that was probably my initial training was, you know,

3:04:28.240 --> 3:04:33.840
 being this just working class kid who ends up in this, you know, sort of blue blood,

3:04:33.840 --> 3:04:41.120
 small liberal arts college and just feeling like, you know, both having the entire world

3:04:41.120 --> 3:04:46.720
 opened up to me like philosophy and Buddhism and things that I had never heard of, you know,

3:04:46.720 --> 3:04:52.160
 and just became totally obsessed with and just like, you know, just following my interests.

3:04:52.160 --> 3:04:57.680
 But also culturally, perhaps didn't feel like you fit in feeling like just a fish out of water.

3:04:57.680 --> 3:05:02.160
 And just, you know, but, and at the same time that, you know, sort of drove me in the sense that

3:05:02.720 --> 3:05:07.040
 it drove an opening of my mind because I couldn't understand it. You know, I was like,

3:05:07.040 --> 3:05:13.360
 I didn't know that this world existed. I don't understand. And I think maybe that's where my

3:05:13.360 --> 3:05:18.400
 real first step in trying to understand other people, because they were my friends, you know,

3:05:18.400 --> 3:05:22.640
 I mean, they were my teammates. I played lacrosse in college. Like, you know, I was close to people

3:05:22.640 --> 3:05:29.200
 who came from such different backgrounds than I did. And I just, I was so confused, you know.

3:05:29.200 --> 3:05:34.000
 And so I think I learned to learn. And then, you know, sort of went from there.

3:05:34.000 --> 3:05:36.880
 And then develop your fascination with people. And the funny thing is,

3:05:37.600 --> 3:05:42.080
 you went from trucking now to autonomous trucks. I mean, this speaking of not being able to connect

3:05:42.080 --> 3:05:48.800
 to Datsun, you know, your life in the next 10 years could take very interesting directions.

3:05:48.800 --> 3:05:55.200
 That's very difficult. First of all, us meeting is a funny little thing, given the things I'm

3:05:55.200 --> 3:06:00.960
 working on with robots currently. But, you know, it may not relate to trucks at all. There's,

3:06:02.080 --> 3:06:08.560
 at a certain point, autonomous trucks are just robots. And then it starts getting into a conversation

3:06:08.560 --> 3:06:16.400
 about the roles of robots in society. Yeah. And the roles of humans and robots. And that interplay

3:06:16.960 --> 3:06:22.800
 is right up your alley. Yeah. That somebody who deeply cares about humans and have somehow

3:06:22.800 --> 3:06:28.560
 found themselves studying robots. Yeah, no, it's crazy. I mean, even for five years ago,

3:06:29.680 --> 3:06:32.800
 if you had asked me if I was going to be studying trucking still, I would have said no.

3:06:32.800 --> 3:06:39.360
 And so my advice is, I think if I was going to give advice, you know, you can't connect the

3:06:39.360 --> 3:06:44.720
 dots looking forward. You just got to follow what interests you, you know? And I think we

3:06:45.840 --> 3:06:52.160
 downplay that when we talk to kids, especially, you know, if you have some bright gifted kid

3:06:52.160 --> 3:06:55.760
 that gets identified as like, oh, you could go somewhere, then we're like, we feed them stuff.

3:06:55.760 --> 3:06:59.280
 You're like, well, learn the piano and learn another language, right? Learn robotics.

3:06:59.280 --> 3:07:04.880
 And then we tell other kids, like, oh, learn a trade, you know, like, figure out what's going

3:07:04.880 --> 3:07:09.120
 to pay. And not that there's anything against trades. I think everyone should learn, like,

3:07:09.120 --> 3:07:14.000
 manual skills to make things. I think it's incredibly satisfying and wonderful. And we need

3:07:14.000 --> 3:07:19.840
 more of that. But also, you know, tell, you know, all kids, it's okay to, like, take a class in

3:07:19.840 --> 3:07:24.560
 something random that you don't think you're going to get any economic return on. Well, because maybe

3:07:24.560 --> 3:07:30.880
 you will end up going into a trade, but that class that you took in studio art is going to mean that,

3:07:30.880 --> 3:07:36.320
 you know, you look at buildings differently, right? Or you end up sort of putting your own stamp on,

3:07:36.320 --> 3:07:42.240
 you know, woodworking, you know, just, I think that's the key is like, follow, you know,

3:07:42.240 --> 3:07:47.440
 it's cheesy because everybody says follow your passion. But, you know, we say that and then we

3:07:47.440 --> 3:07:53.440
 just, you know, the 90% of what people hear is, you know, what's the return on investment for that,

3:07:53.440 --> 3:07:58.320
 you know, it's like, you're a human being, like, things interest you, music interests you,

3:07:58.320 --> 3:08:02.240
 literature interests you, video games interest you, like, follow it, you know.

3:08:02.240 --> 3:08:08.880
 Yeah, go grab a kayak and go do something real. No, don't do that. Don't go do something stupid

3:08:08.880 --> 3:08:13.600
 and something you'll regret a lot later. My poor mother. Thank God she didn't know.

3:08:13.600 --> 3:08:19.600
 Well, let me ask, because for a lot of people, work, for me, it is quote unquote, work is a source

3:08:19.600 --> 3:08:27.040
 of meaning. And at the core of something we've been talking about with jobs is, is meaning.

3:08:27.040 --> 3:08:32.240
 So the big ridiculous question, what do you think is the meaning of life? Do you think work

3:08:34.160 --> 3:08:40.320
 for us humans, a modern society is as core to that meaning? Is that and is that something you

3:08:40.320 --> 3:08:45.360
 think about in your work? So the deeper question of meaning, not just financial well being and the

3:08:45.360 --> 3:08:52.640
 quality of life, but the deeper search for meaning. Yeah, the meaning of life is love.

3:08:53.600 --> 3:09:00.800
 And you can find love in your work. Now, and I, I don't think everybody can, there are a lot of

3:09:00.800 --> 3:09:05.440
 jobs out there that just, you know, you do it for a paycheck. And I think we do have to be,

3:09:06.960 --> 3:09:11.120
 you know, honest about that. There are a lot of people who, you know, don't love their jobs.

3:09:11.120 --> 3:09:17.120
 And, you know, we don't have jobs that they're going to love. You know, and maybe that's not a

3:09:17.120 --> 3:09:23.440
 sort of realistic, you know, that's a utopia, right? But for those of us that have the luxury,

3:09:23.440 --> 3:09:32.560
 I mean, I think you, you love what you do that people say that. I think the key, you know, for

3:09:32.560 --> 3:09:39.920
 real happiness is to love what you're trying to achieve. And maybe you love trying to build a

3:09:39.920 --> 3:09:44.080
 company and make a lot of money just for the sake of doing that. But I think the people who,

3:09:44.080 --> 3:09:48.880
 you know, are really happy and have great impacts, you know, they, they love what they do because

3:09:48.880 --> 3:09:54.640
 it has an impact on the world that they think is, it expresses that love, right? And that could be,

3:09:54.640 --> 3:10:01.280
 you know, at a counseling center, that that could be, you know, in your community, that could be

3:10:01.280 --> 3:10:06.560
 sending people to Mars, you know. Well, I also think it doesn't necessarily, the expression of

3:10:06.560 --> 3:10:11.680
 love isn't necessary about helping other people directly. There's something about craftsmanship

3:10:11.680 --> 3:10:16.800
 and skill, as we've talked about, that's almost like you're celebrating humanity by

3:10:19.520 --> 3:10:25.120
 like searching for mastery in the task, in the simple, like especially tasks that people

3:10:25.120 --> 3:10:33.040
 outside may see as menial, as it's not important. Nevertheless, searching for mastery

3:10:33.040 --> 3:10:38.240
 for excellence in that task. There's something deeply human to that and also fulfilling that

3:10:38.240 --> 3:10:45.360
 just like driving a truck and getting damn good at it. Like, you know, the best who's ever lived

3:10:45.360 --> 3:10:51.600
 and driving the truck and taking pride in that, that, that's deeply meaningful and also like

3:10:52.160 --> 3:10:58.480
 a real celebration of humanity and a real show of love, I think, for, for humanity.

3:10:58.480 --> 3:11:04.000
 Yeah. Yeah, I just had my floors redone and the guy who did it was an, he was an artist,

3:11:04.000 --> 3:11:07.440
 you know, he sanded these old hundred year old floors and made them look gorgeous and

3:11:07.440 --> 3:11:12.320
 this is craft. That's love right there. Yeah. I mean, he showed us some love,

3:11:13.200 --> 3:11:16.400
 you know, the product was just like, is enriching our lives.

3:11:17.200 --> 3:11:21.760
 Steve, this was an amazing conversation. We've covered a lot of ground, your work,

3:11:21.760 --> 3:11:27.600
 just like you said, impossible to connect the dots, but I'm, I'm glad you did all the amazing

3:11:27.600 --> 3:11:33.520
 work you did. You're, you're exploring human nature at the core of what, of what America is,

3:11:33.520 --> 3:11:37.520
 the, the blue collar America. So thank you for your work. Thank you for the care and the love

3:11:37.520 --> 3:11:41.200
 you put in your work. And thank you so much for spending your valuable time with me.

3:11:42.160 --> 3:11:45.760
 I appreciate it, Lexi. I'm a big fan. So it's just been great to be on.

3:11:46.960 --> 3:11:51.840
 Thanks for listening to this conversation with Steve Vaselli. To support this podcast,

3:11:51.840 --> 3:11:56.720
 please check out our sponsors in the description. And now let me leave you with some words from

3:11:56.720 --> 3:12:03.360
 Napoleon Hill. If you cannot do great things, do small things in a great way.

3:12:03.360 --> 3:12:27.440
 Thank you for listening and hope to see you next time.

