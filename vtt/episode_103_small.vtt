WEBVTT

00:00.000 --> 00:03.000
 The following is a conversation with Ben Gertzel,

00:03.000 --> 00:04.560
 one of the most interesting minds

00:04.560 --> 00:06.720
 in the artificial intelligence community.

00:06.720 --> 00:08.920
 He's the founder of SingularityNet,

00:08.920 --> 00:11.520
 designer of OpenCog AI framework,

00:11.520 --> 00:13.240
 formerly a director of research

00:13.240 --> 00:15.720
 at the Machine Intelligence Research Institute,

00:15.720 --> 00:18.440
 and chief scientist of Hanson Robotics,

00:18.440 --> 00:21.040
 the company that created the Sophia robot.

00:21.040 --> 00:22.520
 He has been a central figure

00:22.520 --> 00:24.920
 in the AGI community for many years,

00:24.920 --> 00:28.720
 including in his organizing and contributing to the conference

00:28.720 --> 00:30.920
 and artificial general intelligence,

00:30.920 --> 00:34.440
 the 2020 version of which is actually happening this week,

00:34.440 --> 00:36.480
 Wednesday, Thursday and Friday.

00:36.480 --> 00:38.480
 It's virtual and free.

00:38.480 --> 00:40.040
 I encourage you to check out the talks

00:40.040 --> 00:45.040
 including by Yosha Bach from episode 101 of this podcast.

00:45.160 --> 00:47.800
 Quick summary of the ads, two sponsors,

00:47.800 --> 00:51.040
 The Jordan Harbinger Show and Masterclass.

00:51.040 --> 00:52.800
 Please consider supporting this podcast

00:52.800 --> 00:56.520
 by going to jordanharbinger.com slash lex

00:56.520 --> 01:00.400
 and signing up at masterclass.com slash lex.

01:00.400 --> 01:02.840
 Click the links, buy all the stuff.

01:02.840 --> 01:04.640
 It's the best way to support this podcast

01:04.640 --> 01:08.880
 and the journey I'm on in my research and startup.

01:08.880 --> 01:11.480
 This is the artificial intelligence podcast.

01:11.480 --> 01:13.720
 If you enjoy it, subscribe on YouTube,

01:13.720 --> 01:15.960
 review it with five stars on an Apple podcast,

01:15.960 --> 01:18.920
 support it on Patreon or connect with me on Twitter

01:18.920 --> 01:23.920
 and Lex Friedman, spelled without the E, just F R I D M A N.

01:23.920 --> 01:26.240
 As usual, I'll do a few minutes of ads now

01:26.240 --> 01:27.720
 and never any ads in the middle

01:27.720 --> 01:30.360
 that can break the flow of the conversation.

01:30.360 --> 01:33.720
 This episode is supported by The Jordan Harbinger Show.

01:33.720 --> 01:36.200
 Go to Jordanharbinger.com slash Lex.

01:36.200 --> 01:38.000
 It's how he knows I sent you.

01:38.000 --> 01:40.480
 On that page, there's links to subscribe to it

01:40.480 --> 01:43.560
 on Apple podcast, Spotify and everywhere else.

01:43.560 --> 01:45.520
 I've been binging on his podcast.

01:45.520 --> 01:46.520
 Jordan is great.

01:46.520 --> 01:48.200
 He gets the best out of his guests,

01:48.200 --> 01:50.360
 dives deep, calls them out when it's needed

01:50.360 --> 01:52.720
 and makes the whole thing fun to listen to.

01:52.720 --> 01:55.840
 He's interviewed Kobe Bryant, Mark Cuban,

01:55.840 --> 01:59.600
 Neil deGrasse Tyson, Karek Asparov, and many more.

01:59.600 --> 02:02.360
 This conversation with Kobe is a reminder

02:02.360 --> 02:06.640
 how much focus and hard work is acquired for greatness

02:06.640 --> 02:10.000
 in sport, business and life.

02:10.000 --> 02:13.000
 I highly recommend the episode if you want to be inspired.

02:13.000 --> 02:16.520
 Again, go to Jordanharbinger.com slash Lex.

02:16.520 --> 02:18.600
 It's how Jordan knows I sent you.

02:19.600 --> 02:21.920
 This show sponsored by a masterclass

02:21.920 --> 02:25.040
 sign up at masterclass.com slash Lex

02:25.040 --> 02:28.400
 to get a discount and to support this podcast.

02:28.400 --> 02:30.200
 When I first heard about masterclass,

02:30.200 --> 02:32.040
 I thought it was too good to be true.

02:32.040 --> 02:35.200
 For 180 bucks a year, you get an all access pass

02:35.200 --> 02:38.520
 to watch courses from to list some of my favorites.

02:38.520 --> 02:40.560
 Chris Hadfield on Space Exploration,

02:40.560 --> 02:43.760
 Neil deGrasse Tyson on Scientific Thinking and Communication,

02:43.760 --> 02:48.200
 Will Wright, creator of the greatest city building game ever,

02:48.200 --> 02:51.960
 SimCity and Sims on Game Design,

02:51.960 --> 02:55.560
 Carlos Santana on Guitar, Karek Asparov,

02:55.560 --> 02:58.560
 the greatest chess player ever on chess,

02:58.560 --> 03:01.080
 Daniel Negrano on poker, and many more.

03:01.080 --> 03:03.800
 Chris Hadfield explaining how rockets work

03:03.800 --> 03:06.480
 and the experience of being launched into space alone

03:06.480 --> 03:07.840
 is worth the money.

03:07.840 --> 03:11.160
 Once again, sign up at masterclass.com slash Lex

03:11.160 --> 03:15.000
 to get a discount and to support this podcast.

03:15.000 --> 03:20.320
 And now, here's my conversation with Ben Gretzel.

03:20.320 --> 03:24.520
 What books, authors, ideas had a lot of impact on you

03:24.520 --> 03:27.400
 in your life in the early days?

03:27.400 --> 03:31.720
 You know, what got me into AI and science fiction

03:31.720 --> 03:34.080
 and such in the first place wasn't a book,

03:34.080 --> 03:36.480
 but the original Star Trek TV show,

03:36.480 --> 03:39.320
 which my dad watched with me like in its first run.

03:39.320 --> 03:42.200
 It would have been 1968, 69 or something.

03:42.200 --> 03:45.440
 And that was incredible because every show they visited

03:45.440 --> 03:49.560
 a different alien civilization with different culture

03:49.560 --> 03:50.760
 and weird mechanisms.

03:50.760 --> 03:54.440
 But that got me into science fiction

03:54.440 --> 03:57.560
 and there wasn't that much science fiction to watch on TV

03:57.560 --> 03:58.400
 at that stage.

03:58.400 --> 04:00.880
 So that got me into reading the whole literature

04:00.880 --> 04:02.840
 of science fiction, you know,

04:02.840 --> 04:07.000
 from the beginning of the previous century until that time.

04:07.000 --> 04:10.360
 And I mean, there was so many science fiction writers

04:10.360 --> 04:12.360
 who were inspirational to me.

04:12.360 --> 04:14.720
 I'd say if I had to pick two,

04:14.720 --> 04:18.120
 it would have been Stanislaw Lem, the Polish writer.

04:18.120 --> 04:20.880
 Yeah, Solaris.

04:20.880 --> 04:23.360
 And then he had a bunch of more obscure writings

04:23.360 --> 04:26.600
 on superhuman AIs that were engineered.

04:26.600 --> 04:29.840
 Solaris was sort of a superhuman naturally occurring

04:29.840 --> 04:34.840
 intelligence, then Philip K. Dick, who, you know,

04:34.840 --> 04:37.280
 ultimately my fandom for Philip K. Dick

04:37.280 --> 04:39.080
 is one of the things that brought me together

04:39.080 --> 04:43.720
 with David Hansen, my collaborator on robotics projects.

04:43.720 --> 04:46.880
 So, you know, Stanislaw Lem was very much

04:46.880 --> 04:47.880
 an intellectual, right?

04:47.880 --> 04:51.000
 So he had a very broad view of intelligence

04:51.000 --> 04:55.160
 going beyond the human and into what I would call, you know,

04:55.160 --> 04:56.880
 open ended superintelligence.

04:56.880 --> 05:01.880
 The Solaris superintelligent ocean was intelligent

05:01.880 --> 05:04.400
 in some ways more generally intelligent than people,

05:04.400 --> 05:07.320
 but in a complex and confusing way

05:07.320 --> 05:10.120
 so that human beings could never quite connect to it,

05:10.120 --> 05:13.200
 but it was still probably very, very smart.

05:13.200 --> 05:16.520
 And then the Golem IV supercomputer

05:16.520 --> 05:20.360
 in one of Lem's books, this was engineered by people,

05:20.360 --> 05:24.360
 but eventually it became very intelligent

05:24.360 --> 05:25.960
 in a different direction than humans

05:25.960 --> 05:29.160
 and decided that humans were kind of trivial

05:29.160 --> 05:30.200
 and not that interesting.

05:30.200 --> 05:35.200
 So it put some impenetrable shield around itself,

05:35.200 --> 05:37.360
 shut itself off from humanity and then issued

05:37.360 --> 05:41.760
 some philosophical screed about the pathetic

05:41.760 --> 05:46.760
 and hopeless nature of humanity and all human thought

05:46.760 --> 05:48.320
 and then disappeared.

05:48.320 --> 05:51.080
 Now, Philip K. Dick, he was a bit different,

05:51.080 --> 05:52.400
 he was human focused, right?

05:52.400 --> 05:55.800
 His main thing was, you know, human compassion

05:55.800 --> 05:59.480
 and the human heart and soul are going to be the constant

05:59.480 --> 06:03.560
 that will keep us going through whatever aliens we discover

06:03.560 --> 06:08.560
 or telepathy machines or super AIs or whatever it might be.

06:09.080 --> 06:11.120
 So he didn't believe in reality,

06:11.120 --> 06:14.240
 like the reality that we see may be a simulation

06:14.240 --> 06:17.560
 or a dream or something else we can't even comprehend,

06:17.560 --> 06:19.600
 but he believed in love and compassion

06:19.600 --> 06:22.920
 as something persistent through the various simulated realities.

06:22.920 --> 06:25.480
 So those two science fiction writers

06:25.480 --> 06:27.240
 had a huge impact on me.

06:27.240 --> 06:28.800
 Then a little older than that,

06:28.800 --> 06:33.040
 I got into Dostoevsky and Friedrich Nietzsche

06:33.040 --> 06:37.440
 and Rambo, then a bunch of more literary type writing.

06:37.440 --> 06:38.640
 Can we talk about some of those things?

06:38.640 --> 06:41.720
 So on the Solaris side, Stanislaus Lem,

06:43.200 --> 06:47.040
 this kind of idea of there being intelligences out there

06:47.040 --> 06:49.560
 that are different than our own,

06:49.560 --> 06:53.040
 do you think their intelligences may be all around us

06:53.040 --> 06:56.440
 that we're not able to even detect?

06:56.440 --> 07:00.000
 So this kind of idea of maybe you can comment also

07:00.000 --> 07:03.020
 on Stephen Wolfram thinking that there's computations

07:03.020 --> 07:05.880
 all around us and we're just not smart enough

07:05.880 --> 07:09.080
 to kind of detect their intelligence

07:09.080 --> 07:10.440
 or appreciate their intelligence.

07:10.440 --> 07:13.600
 Yeah, so my friend Hugo DeGaris

07:13.600 --> 07:15.880
 who I've been talking to about these things

07:15.880 --> 07:19.360
 for many decades since the early 90s,

07:19.360 --> 07:21.800
 he had an idea he called SIPI,

07:21.800 --> 07:25.160
 the search for intra particulate intelligence.

07:25.160 --> 07:28.200
 So the concept there was as AIs get smarter

07:28.200 --> 07:29.520
 and smarter and smarter,

07:29.520 --> 07:32.160
 you know, assuming the laws of physics

07:32.160 --> 07:37.160
 as we know them now are still what these super intelligences

07:37.520 --> 07:39.280
 perceived hold and are bound by,

07:39.280 --> 07:40.480
 as they get smarter and smarter,

07:40.480 --> 07:43.080
 they're gonna shrink themselves littler and littler

07:43.080 --> 07:47.320
 because special relativity makes it so that it can communicate

07:47.320 --> 07:49.380
 between two spatially distant points.

07:49.380 --> 07:50.880
 So they're gonna get smaller and smaller,

07:50.880 --> 07:53.320
 but then ultimately what does that mean?

07:53.320 --> 07:56.600
 The minds of the super, super, super intelligences,

07:56.600 --> 07:59.120
 they're gonna be packed into the interaction

07:59.120 --> 08:02.040
 of elementary particles or quarks

08:02.040 --> 08:04.680
 or the partons inside quarks or whatever it is.

08:04.680 --> 08:07.720
 So what we perceive as random fluctuations

08:07.720 --> 08:09.820
 on the quantum or subquantum level

08:09.820 --> 08:11.560
 may actually be the thoughts

08:11.560 --> 08:16.360
 of the micro, micro, micro, miniaturized super intelligences

08:16.360 --> 08:20.120
 because there's no way we can tell random from structured

08:20.120 --> 08:21.720
 but with an algorithmic information

08:21.720 --> 08:23.200
 more complex than our brains, right?

08:23.200 --> 08:24.400
 We can't tell the difference.

08:24.400 --> 08:25.800
 So what we think is random

08:25.800 --> 08:27.080
 could be the thought processes

08:27.080 --> 08:30.080
 of some really tiny super minds.

08:30.080 --> 08:34.120
 And if so, there's not a damn thing we can do about it,

08:34.120 --> 08:37.240
 except try to upgrade our intelligences

08:37.240 --> 08:38.560
 and expand our minds

08:38.560 --> 08:41.400
 so that we can perceive more of what's around us.

08:41.400 --> 08:44.080
 But if those random fluctuations,

08:44.080 --> 08:46.640
 like even if we go to like quantum mechanics,

08:46.640 --> 08:51.320
 if that's actually super intelligent systems,

08:51.320 --> 08:54.400
 aren't we then part of the soup of super intelligence?

08:54.400 --> 08:58.400
 Or aren't we just like a finger of the entirety

08:58.400 --> 09:01.360
 of the body of the super intelligent system?

09:01.360 --> 09:05.960
 It could be, I mean, a finger is a strange metaphor.

09:05.960 --> 09:08.040
 I mean, we...

09:08.040 --> 09:10.560
 A finger is dumb is what I mean.

09:10.560 --> 09:12.280
 But the finger is also useful

09:12.280 --> 09:14.840
 and is controlled with intent by the brain

09:14.840 --> 09:16.720
 whereas we may be much less than that, right?

09:16.720 --> 09:21.400
 I mean, yeah, we may be just some random epiphenomenon

09:21.400 --> 09:23.320
 that they don't care about too much.

09:23.320 --> 09:25.960
 Like think about the shape of the crowd

09:25.960 --> 09:28.680
 emanating from a sports stadium or something, right?

09:28.680 --> 09:31.560
 There's some emergent shape to the crowd, it's there.

09:31.560 --> 09:33.680
 You could take a picture of it, it's kind of cool.

09:33.680 --> 09:36.280
 It's irrelevant to the main point of the sports event

09:36.280 --> 09:37.840
 or where the people are going

09:37.840 --> 09:40.200
 or what's on the minds of the people

09:40.200 --> 09:41.840
 making that shape in the crowd, right?

09:41.840 --> 09:46.840
 So we may just be some semi arbitrary, higher level pattern

09:47.640 --> 09:52.200
 popping out of a lower level hyper intelligent self organization.

09:52.200 --> 09:55.800
 And I mean, so be it, right?

09:55.800 --> 09:57.040
 I mean, that's one thing that...

09:57.040 --> 09:58.040
 Still a fun ride.

09:58.040 --> 09:59.440
 Yeah, I mean, the older I've gotten,

09:59.440 --> 10:01.680
 the more respect I've achieved

10:01.680 --> 10:04.160
 for our fundamental ignorance.

10:04.160 --> 10:06.200
 I mean, mine and everybody else's.

10:06.200 --> 10:08.760
 I mean, I look at my two dogs,

10:08.760 --> 10:10.840
 two beautiful little toy poodles

10:10.840 --> 10:14.680
 and they watch me sitting at the computer typing.

10:14.680 --> 10:16.920
 They just think I'm sitting there wiggling my fingers

10:16.920 --> 10:19.920
 to exercise and maybe or guarding the monitor on the desk

10:19.920 --> 10:22.320
 that they have no idea that I'm communicating

10:22.320 --> 10:24.400
 with other people halfway around the world,

10:24.400 --> 10:27.640
 let alone, you know, creating complex algorithms

10:27.640 --> 10:30.240
 running in RAM on some computer server

10:30.240 --> 10:32.560
 in St. Petersburg or something, right?

10:32.560 --> 10:33.640
 Although they're right there,

10:33.640 --> 10:35.080
 they're right there in the room with me.

10:35.080 --> 10:37.800
 So what things are there right around us

10:37.800 --> 10:40.760
 that were just too stupid or closed minded to comprehend?

10:40.760 --> 10:42.120
 Probably quite a lot.

10:42.120 --> 10:46.200
 Your very poodle could also be communicating

10:46.200 --> 10:47.840
 across multiple dimensions

10:47.840 --> 10:52.360
 with other beings and you're too unintelligent

10:52.360 --> 10:54.760
 to understand the kind of communication mechanism

10:54.760 --> 10:55.680
 they're going through.

10:55.680 --> 10:58.440
 There have been various TV shows

10:58.440 --> 10:59.800
 and science fiction novels,

10:59.800 --> 11:03.200
 Poisoning Cats, Dolphins, Mice and whatnot

11:03.200 --> 11:07.240
 are actually super intelligences here to observe that.

11:07.240 --> 11:12.240
 I would guess as one or the other quantum physics founders

11:12.560 --> 11:15.520
 said those theories are not crazy enough to be true.

11:15.520 --> 11:17.640
 The reality is probably crazier than that.

11:17.640 --> 11:18.480
 Beautifully put.

11:18.480 --> 11:22.000
 So on the human side with Philip K. Dick

11:22.000 --> 11:27.000
 and in general, where do you fall on this idea

11:27.240 --> 11:30.560
 that love and just the basic spirit of human nature

11:30.560 --> 11:34.960
 persists throughout these multiple realities?

11:34.960 --> 11:36.400
 Are you on the side?

11:36.400 --> 11:38.400
 Like the thing that inspires you

11:38.400 --> 11:43.400
 about artificial intelligence is it the human side

11:43.400 --> 11:48.400
 of somehow persisting through all of the different systems

11:48.960 --> 11:53.320
 we engineer or is AI inspire you to create something

11:53.320 --> 11:55.440
 that's greater than human, that's beyond human,

11:55.440 --> 11:57.440
 that's almost non human?

11:59.120 --> 12:02.800
 I would say my motivation to create AGI

12:02.800 --> 12:05.200
 comes from both of those directions actually.

12:05.200 --> 12:08.600
 So when I first became passionate about AGI

12:08.600 --> 12:11.400
 when I was, it would have been two or three years old

12:11.400 --> 12:14.680
 after watching robots on Star Trek.

12:14.680 --> 12:18.200
 I mean, then it was really a combination

12:18.200 --> 12:19.840
 of intellectual curiosity.

12:19.840 --> 12:22.900
 Like can a machine really think, how would you do that?

12:22.900 --> 12:27.200
 And yeah, just ambition to create something much better

12:27.200 --> 12:28.720
 than all the clearly limited

12:28.720 --> 12:31.960
 and fundamentally defective humans I saw around me.

12:31.960 --> 12:36.840
 Then as I got older and got more enmeshed in the human world

12:36.840 --> 12:38.800
 and you know, got married, had children.

12:38.800 --> 12:40.800
 So my parents began to age.

12:40.800 --> 12:44.120
 I started to realize, well, not only will AGI

12:44.120 --> 12:46.880
 let you go far beyond the limitations of the human,

12:46.880 --> 12:50.920
 but it could also like stop us from dying and suffering

12:50.920 --> 12:55.000
 and feeling pain and tormenting ourselves mentally.

12:55.000 --> 12:59.560
 So you can see AGI has amazing capability to do good

12:59.560 --> 13:03.480
 for humans as humans alongside with its capability

13:03.480 --> 13:06.640
 to go far, far beyond the human level.

13:06.640 --> 13:10.000
 So I mean, both aspects are there

13:10.000 --> 13:13.240
 which makes it even more exciting and important.

13:13.240 --> 13:15.480
 So you mentioned Dostoevsky and Nietzsche.

13:15.480 --> 13:17.080
 Where did you pick up from those guys?

13:17.080 --> 13:17.920
 I mean.

13:17.920 --> 13:21.520
 That would probably go beyond the scope

13:21.520 --> 13:23.000
 of a brief interview.

13:23.000 --> 13:24.320
 Sure.

13:24.320 --> 13:26.760
 I mean, both of those are amazing thinkers

13:26.760 --> 13:31.760
 who one will necessarily have a complex relationship with.

13:31.960 --> 13:36.480
 So I mean, Dostoevsky on the minus side,

13:36.480 --> 13:38.480
 he's kind of a religious fanatic

13:38.480 --> 13:42.040
 and he's sort of helped squash the Russian nihilist movement

13:42.040 --> 13:43.160
 which was very interesting.

13:43.160 --> 13:45.840
 Because what nihilism meant originally

13:45.840 --> 13:48.640
 in that period of the mid late 1800s in Russia

13:48.640 --> 13:52.160
 was not taking anything fully 100% for grand.

13:52.160 --> 13:54.400
 It was really more like what we'd call Bayesianism now

13:54.400 --> 13:56.880
 where you don't wanna adopt anything

13:56.880 --> 14:01.040
 as a dogmatic certitude and always leave your mind open

14:01.040 --> 14:04.440
 and how Dostoevsky parodied nihilism

14:04.440 --> 14:06.680
 was a bit different, right?

14:06.680 --> 14:10.360
 Dostoevsky parodied is people who believe absolutely nothing.

14:10.360 --> 14:13.000
 So they must assign an equal probability weight

14:13.000 --> 14:17.760
 to every proposition, which doesn't really work.

14:17.760 --> 14:22.560
 So on the one hand, I didn't really agree with Dostoevsky

14:22.560 --> 14:25.240
 on his sort of religious point of view.

14:25.240 --> 14:29.680
 On the other hand, if you look at his understanding

14:29.680 --> 14:32.680
 of human nature and sort of the human mind

14:32.680 --> 14:36.640
 and heart and soul, it's really unparalleled.

14:36.640 --> 14:40.840
 And he had an amazing view of how human beings

14:40.840 --> 14:44.920
 construct a world for themselves based on their own

14:44.920 --> 14:47.720
 understanding and their own mental predisposition.

14:47.720 --> 14:50.360
 And I think if you look in the brothers Karamazov

14:50.360 --> 14:54.840
 in particular, the Russian literary theorist,

14:54.840 --> 14:57.040
 Mikhail Bakhtin, wrote about this

14:57.040 --> 14:59.840
 as a polyphonic mode of fiction,

14:59.840 --> 15:02.560
 which means it's not third person,

15:02.560 --> 15:05.280
 but it's not first person from any one person really.

15:05.280 --> 15:07.240
 There are many different characters in the novel.

15:07.240 --> 15:09.560
 And each of them is sort of telling

15:09.560 --> 15:11.840
 part of the story from their own point of view.

15:11.840 --> 15:13.760
 So the reality of the whole story

15:13.760 --> 15:17.480
 is an intersection, like synergetically,

15:17.480 --> 15:19.880
 of the many different characters, world views.

15:19.880 --> 15:23.480
 And that really, it's a beautiful metaphor

15:23.480 --> 15:26.960
 and even a reflection, I think, of how all of us socially

15:26.960 --> 15:27.960
 create our reality.

15:27.960 --> 15:31.320
 Like each of us sees the world in a certain way.

15:31.320 --> 15:35.000
 Each of us, in a sense, is making the world as we see it

15:35.000 --> 15:37.840
 based on our own minds and understanding.

15:37.840 --> 15:41.240
 But it's polyphony, like in music,

15:41.240 --> 15:43.560
 where multiple instruments are coming together

15:43.560 --> 15:44.920
 to create the sound.

15:44.920 --> 15:46.960
 The ultimate reality that's created

15:46.960 --> 15:50.520
 comes out of each of our subjective understandings,

15:50.520 --> 15:51.600
 intersecting with each other.

15:51.600 --> 15:53.960
 And that was one of the many beautiful things

15:53.960 --> 15:55.960
 in Dostoevsky.

15:55.960 --> 15:58.240
 So maybe a little bit to mention,

15:58.240 --> 16:02.560
 you have a connection to Russia and the Soviet culture.

16:02.560 --> 16:04.720
 I mean, I'm not sure exactly what the nature of the connection

16:04.720 --> 16:07.600
 is, but at least the spirit of your thinking is in there.

16:07.600 --> 16:07.600
 Oh, yeah.

16:07.600 --> 16:13.040
 Well, my ancestry is 3 quarters Eastern European Jewish.

16:13.040 --> 16:17.040
 So I'm in my three of my great grandparents

16:17.040 --> 16:20.600
 emigrated to New York from Lithuania

16:20.600 --> 16:23.520
 and sort of border regions of Poland, which

16:23.520 --> 16:28.320
 are in and out of Poland, around the time of World War I.

16:28.320 --> 16:34.000
 And they were socialists and communists as well as Jews,

16:34.000 --> 16:36.280
 mostly Menshevik, not Bolshevik.

16:36.280 --> 16:39.960
 And they fled at just the right time to the US

16:39.960 --> 16:41.280
 for their own personal reasons.

16:41.280 --> 16:45.640
 And then almost all or maybe all of my extended family

16:45.640 --> 16:48.320
 that remained in Eastern Europe was killed either by Hitlens

16:48.320 --> 16:50.400
 or Stalin's minions at some point.

16:50.400 --> 16:53.640
 So the branch of the family then emigrated to the US

16:53.640 --> 16:56.800
 was pretty much the only one.

16:56.800 --> 17:00.960
 So how much of the spirit of the people is in your blood still?

17:00.960 --> 17:05.760
 When you look in the mirror, do you see what do you see?

17:05.760 --> 17:08.440
 I see a bag of meat that I want to transcend

17:08.440 --> 17:12.080
 by uploading into some sort of superior reality.

17:12.080 --> 17:21.120
 But very clearly, I mean, I'm not religious

17:21.120 --> 17:25.840
 in a traditional sense, but clearly the Eastern European

17:25.840 --> 17:28.800
 Jewish tradition was what I was raised in.

17:28.800 --> 17:32.720
 I mean, my grandfather Leo as well

17:32.720 --> 17:35.400
 was a physical chemist to work with Linus Pauling

17:35.400 --> 17:38.160
 and a bunch of the other early greats in quantum mechanics.

17:38.160 --> 17:41.240
 I mean, he was into x ray diffraction.

17:41.240 --> 17:44.080
 He was on the material science side, experimentalist

17:44.080 --> 17:45.480
 rather than a theorist.

17:45.480 --> 17:47.720
 His sister was also a physicist.

17:47.720 --> 17:51.160
 And my father's father, Victor Gertzel,

17:51.160 --> 17:57.120
 was a PhD in psychology who had the unenviable job

17:57.120 --> 18:00.920
 of giving soccer therapy to the Japanese in internment camps

18:00.920 --> 18:04.680
 in the US in World War II, like to counsel them why they

18:04.680 --> 18:07.560
 shouldn't kill themselves, even though they had all their stuff

18:07.560 --> 18:10.320
 taken away and been imprisoned for no good reason.

18:10.320 --> 18:16.160
 So I mean, there is a lot of Eastern European Jewish

18:16.160 --> 18:18.080
 tradition in my background.

18:18.080 --> 18:20.160
 One of my great uncles was, I guess,

18:20.160 --> 18:22.440
 conductor of San Francisco Orchestra.

18:22.440 --> 18:27.000
 So there was a lot of Mickey Salkind, a bunch of music

18:27.000 --> 18:27.600
 in there also.

18:27.600 --> 18:31.480
 And clearly, this culture was all about learning

18:31.480 --> 18:37.160
 and understanding the world and also not quite

18:37.160 --> 18:39.840
 taking yourself too seriously while you do it, right?

18:39.840 --> 18:42.000
 There's a lot of Yiddish humor in there.

18:42.000 --> 18:46.160
 So I do appreciate that culture, although the whole idea

18:46.160 --> 18:49.320
 that the Jews are the chosen people of God never

18:49.320 --> 18:51.720
 resonated with me too much.

18:51.720 --> 18:55.320
 The graph of the Gertzel family, I mean,

18:55.320 --> 18:58.240
 just the people I've encountered just doing some research

18:58.240 --> 19:01.560
 and just knowing your work through the decades,

19:01.560 --> 19:02.920
 it's kind of fascinating.

19:02.920 --> 19:06.360
 And just the number of PhDs.

19:06.360 --> 19:06.960
 Yeah, yeah.

19:06.960 --> 19:10.760
 I mean, my dad is a sociology professor

19:10.760 --> 19:15.040
 who recently retired from Rutgers University.

19:15.040 --> 19:18.560
 But clearly, that gave me a head start in life.

19:18.560 --> 19:21.600
 I mean, my grandfather gave me all his quantum mechanics books.

19:21.600 --> 19:24.240
 And I was like seven or eight years old.

19:24.240 --> 19:26.080
 I remember going through them.

19:26.080 --> 19:28.040
 And it was all the old quantum mechanics,

19:28.040 --> 19:30.440
 like Rutherford Adams and stuff.

19:30.440 --> 19:32.880
 So I got to the part of wave functions,

19:32.880 --> 19:36.160
 which I didn't understand, although I was a very bright kid.

19:36.160 --> 19:38.680
 And I realized he didn't quite understand it either.

19:38.680 --> 19:41.960
 But at least he pointed me to some professor

19:41.960 --> 19:45.360
 he knew at U Penn nearby who understood these things, right?

19:45.360 --> 19:49.600
 So that's an unusual opportunity for a kid to have, right?

19:49.600 --> 19:53.400
 My dad, he was programming FORTRAM when I was 10 or 11

19:53.400 --> 19:56.200
 years old on like HP 3000, the mainframes

19:56.200 --> 19:57.680
 at Rutgers University.

19:57.680 --> 20:00.680
 So I got to do linear regression and FORTRAM

20:00.680 --> 20:04.200
 on punch cards when I was in middle school, right?

20:04.200 --> 20:06.640
 Because he was doing, I guess, analysis

20:06.640 --> 20:09.640
 of demographic and sociology data.

20:09.640 --> 20:14.800
 So yes, certainly that gave me a head start

20:14.800 --> 20:17.240
 and a push towards science beyond what would have been

20:17.240 --> 20:19.720
 the case with many, many different situations.

20:19.720 --> 20:22.200
 When did you first fall in love with AI?

20:22.200 --> 20:24.680
 Is it the programming side of FORTRAM?

20:24.680 --> 20:27.240
 Is it maybe the sociology psychology

20:27.240 --> 20:29.080
 that you picked up from your dad or is it the quantum physics?

20:29.080 --> 20:30.640
 I fell in love with AI when I was probably three years old

20:30.640 --> 20:32.600
 when I saw a robot on Star Trek.

20:32.600 --> 20:34.600
 It was turning around in a circle going,

20:34.600 --> 20:38.880
 error, error, error, error, because Spock and Kirk

20:38.880 --> 20:40.720
 had tricked into a mechanical breakdown

20:40.720 --> 20:42.920
 by presenting it with a logical paradox.

20:42.920 --> 20:45.640
 And I was just like, well, this makes no sense.

20:45.640 --> 20:47.520
 This AI is very, very smart.

20:47.520 --> 20:49.600
 It's been traveling all around the universe.

20:49.600 --> 20:52.680
 But these people could trick it with a simple logical paradox.

20:52.680 --> 20:57.040
 Like, if the human brain can get beyond that paradox,

20:57.040 --> 20:59.480
 why can't this AI?

20:59.480 --> 21:03.160
 So I felt the screenwriters of Star Trek

21:03.160 --> 21:06.080
 had misunderstood the nature of intelligence.

21:06.080 --> 21:07.600
 And I complained to my dad about it,

21:07.600 --> 21:12.240
 and he wasn't going to say anything one way or the other.

21:12.240 --> 21:18.480
 But before I was born, when my dad was at Antioch College

21:18.480 --> 21:25.360
 in the middle of the US, he led a protest movement called

21:25.360 --> 21:27.440
 SLAM, Student League Against Mortality.

21:27.440 --> 21:30.320
 They were protesting against death, wandering

21:30.320 --> 21:31.480
 across the campus.

21:31.480 --> 21:35.880
 So he was into some futuristic things even back then.

21:35.880 --> 21:40.200
 But whether AI could confront logical paradoxes or not,

21:40.200 --> 21:41.200
 he didn't know.

21:41.200 --> 21:44.760
 But when I, 10 years after that, if something,

21:44.760 --> 21:48.480
 I discovered Douglas Hofstadter's book, Gordel Escher Bach.

21:48.480 --> 21:51.880
 And that was sort of to the same point of AI and paradox

21:51.880 --> 21:55.080
 and logic, because he was over and over with Gordel's

21:55.080 --> 21:56.160
 incompleteness theorem.

21:56.160 --> 22:00.520
 And can an AI really fully model itself reflexively,

22:00.520 --> 22:02.840
 or does that lead you into some paradox?

22:02.840 --> 22:05.280
 Can the human mind truly model itself reflexively,

22:05.280 --> 22:07.520
 or does that lead you into some paradox?

22:07.520 --> 22:11.200
 So I think that book, Gordel Escher Bach, which I think

22:11.200 --> 22:13.480
 I read when it first came out.

22:13.480 --> 22:15.040
 I would have been 12 years old or something.

22:15.040 --> 22:17.160
 I remember it was like 16 hour day.

22:17.160 --> 22:19.840
 I read it cover to cover, and then reread it.

22:19.840 --> 22:21.520
 I reread it after that, because there

22:21.520 --> 22:23.720
 was a lot of weird things with little formal systems

22:23.720 --> 22:25.680
 in there that were hard for me at the time.

22:25.680 --> 22:28.000
 But that was the first book I read

22:28.000 --> 22:34.480
 that gave me a feeling for AI as like a practical academic

22:34.480 --> 22:37.400
 or engineering discipline that people were working in.

22:37.400 --> 22:40.120
 Because before I read Gordel Escher Bach,

22:40.120 --> 22:44.000
 I was into AI from the point of view of a science fiction fan.

22:44.000 --> 22:47.440
 And I had the idea, well, it may be a long time

22:47.440 --> 22:50.440
 before we can achieve immortality in superhuman AGI.

22:50.440 --> 22:54.680
 So I should figure out how to build a spacecraft traveling

22:54.680 --> 22:57.040
 close to the speed of light, go far away,

22:57.040 --> 22:58.760
 then come back to the Earth in a million years

22:58.760 --> 23:00.400
 when technology is more advanced, and we

23:00.400 --> 23:01.720
 can build these things.

23:01.720 --> 23:04.320
 Reading Gordel Escher Bach, well, it didn't all

23:04.320 --> 23:05.280
 ring true to me.

23:05.280 --> 23:09.200
 A lot of it did, but I could see like there are smart people.

23:09.200 --> 23:11.560
 Right now at various universities around me

23:11.560 --> 23:16.000
 who are actually trying to work on building what I would now

23:16.000 --> 23:19.000
 call AGI, although Hofstadter didn't call that.

23:19.000 --> 23:22.320
 So really, it was when I read that book, which would have been

23:22.320 --> 23:24.800
 probably middle school, that then I started to think,

23:24.800 --> 23:29.000
 well, this is something that I could practically work on.

23:29.000 --> 23:31.640
 Yeah, it's supposed to be flying away and waiting it out.

23:31.640 --> 23:33.480
 You can actually be one of the people

23:33.480 --> 23:34.560
 that actually builds this system.

23:34.560 --> 23:35.320
 Yeah, exactly.

23:35.320 --> 23:36.800
 And if you think about, I mean, I

23:36.800 --> 23:40.680
 was interested in what we'd now call nanotechnology

23:40.680 --> 23:44.840
 and in human immortality and time travel,

23:44.840 --> 23:47.880
 all the same cool things as every other science fiction

23:47.880 --> 23:49.240
 loving kid.

23:49.240 --> 23:52.520
 But AI seemed like if Hofstadter was right,

23:52.520 --> 23:54.160
 you'd just figure out the right program,

23:54.160 --> 23:55.080
 sit there and type it.

23:55.080 --> 23:59.640
 Like you don't need to spend stars into weird configurations

23:59.640 --> 24:02.640
 or get government approval to cut people up

24:02.640 --> 24:05.040
 and fiddle with their DNA or something, right?

24:05.040 --> 24:06.240
 It's just programming.

24:06.240 --> 24:10.680
 And then, of course, that can achieve anything else.

24:10.680 --> 24:12.200
 There's another book from back then,

24:12.200 --> 24:21.600
 which was by Gerald Feinbaum, who was a physicist at Princeton.

24:21.600 --> 24:24.600
 And that was the Prometheus Project.

24:24.600 --> 24:26.720
 And this book was written in the late 1960s,

24:26.720 --> 24:28.760
 though I encountered it in the mid-'70s.

24:28.760 --> 24:30.920
 But what this book said is in the next few decades,

24:30.920 --> 24:33.440
 humanity is going to create superhuman thinking

24:33.440 --> 24:37.520
 machines, molecular nanotechnology and human immortality.

24:37.520 --> 24:41.120
 And then the challenge we'll have is what to do with it.

24:41.120 --> 24:43.000
 Do we use it to expand human consciousness

24:43.000 --> 24:44.520
 in a positive direction?

24:44.520 --> 24:49.880
 Or do we use it just to further vapid consumerism?

24:49.880 --> 24:51.800
 And what he proposed was that the UN

24:51.800 --> 24:53.480
 should do a survey on this.

24:53.480 --> 24:56.440
 And the UN should send people out to every little village

24:56.440 --> 24:58.960
 in remotest Africa or South America

24:58.960 --> 25:01.520
 and explain to everyone what technology was going

25:01.520 --> 25:03.000
 to bring the next few decades.

25:03.000 --> 25:05.000
 And the choice that we had about how to use it.

25:05.000 --> 25:07.320
 And let everyone on the whole planet

25:07.320 --> 25:09.680
 vote about whether we should develop

25:09.680 --> 25:14.320
 super AI nanotechnology and immortality

25:14.320 --> 25:18.200
 for expanded consciousness or for rampant consumerism.

25:18.200 --> 25:22.040
 And needless to say, that didn't quite happen.

25:22.040 --> 25:24.120
 And I think this guy died in the mid-'80s,

25:24.120 --> 25:25.840
 so he didn't even see his ideas start

25:25.840 --> 25:28.160
 to become more mainstream.

25:28.160 --> 25:30.560
 But it's interesting, many of the themes

25:30.560 --> 25:33.320
 I'm engaged with now, from AGI and immortality,

25:33.320 --> 25:36.120
 even to trying to democratize technology,

25:36.120 --> 25:37.920
 as I've been pushing forward singularity

25:37.920 --> 25:40.000
 in my work in the blockchain world,

25:40.000 --> 25:43.560
 many of these themes were there in Feinbaum's book

25:43.560 --> 25:47.880
 in the late 60s even.

25:47.880 --> 25:52.880
 And of course, Valentin Turchin, a Russian writer

25:52.880 --> 25:54.880
 and a great Russian physicist, who

25:54.880 --> 25:57.640
 I got to know when we both lived in New York

25:57.640 --> 25:59.840
 in the late 90s and early aughts.

25:59.840 --> 26:03.600
 I mean, he had a book in the late 60s in Russia, which

26:03.600 --> 26:06.920
 was The Phenomenon of Science, which laid out

26:06.920 --> 26:10.160
 all these same things as well.

26:10.160 --> 26:13.760
 And Val died in, I don't remember, 2004, 2005,

26:13.760 --> 26:15.360
 or something of Parkinson'sism.

26:15.360 --> 26:21.440
 So yeah, it's easy for people to lose track now of the fact

26:21.440 --> 26:26.320
 that the futurist and the singularitarian advanced

26:26.320 --> 26:29.680
 technology ideas that are now almost mainstream

26:29.680 --> 26:30.880
 are on TV all the time.

26:30.880 --> 26:34.040
 I mean, these are not that new, right?

26:34.040 --> 26:37.040
 They're sort of new in the history of the human species.

26:37.040 --> 26:41.080
 But I mean, these were all around in fairly mature form

26:41.080 --> 26:43.600
 in the middle of the last century,

26:43.600 --> 26:45.440
 were written about quite articulately

26:45.440 --> 26:48.160
 by fairly mainstream people who were professors

26:48.160 --> 26:50.120
 at top universities.

26:50.120 --> 26:52.880
 It's just until the enabling technologies

26:52.880 --> 26:58.280
 got to a certain point, then you couldn't make it real.

26:58.280 --> 27:02.760
 So even in the 70s, I was sort of seeing that

27:02.760 --> 27:04.680
 and living through it, right?

27:04.680 --> 27:07.880
 From Star Trek to Douglas Hofstadter,

27:07.880 --> 27:09.600
 things were getting very, very practical

27:09.600 --> 27:11.920
 from the late 60s to the late 70s.

27:11.920 --> 27:15.120
 And the first computer I bought, you

27:15.120 --> 27:17.560
 could only program with hexadecimal machine code,

27:17.560 --> 27:19.320
 and you had to solder it together.

27:19.320 --> 27:23.400
 And then a few years later, there's punch cards.

27:23.400 --> 27:27.160
 And a few years later, you could get Atari 400

27:27.160 --> 27:28.520
 and Commodore VIC 20.

27:28.520 --> 27:31.240
 And you could type on a keyboard and program

27:31.240 --> 27:34.600
 in higher level languages alongside the assembly language.

27:34.600 --> 27:38.640
 So these ideas have been building up a while.

27:38.640 --> 27:42.920
 And I guess my generation got to feel them build up,

27:42.920 --> 27:46.320
 which is different than people coming into the field now,

27:46.320 --> 27:49.040
 for whom these things have just been part

27:49.040 --> 27:52.480
 of the ambiance of culture for their whole career,

27:52.480 --> 27:54.080
 or even their whole life.

27:54.080 --> 27:56.920
 Well, it's fascinating to think about there being

27:56.920 --> 28:01.520
 all of these ideas kind of swimming almost with a noise

28:01.520 --> 28:04.480
 all around the world, all the different generations,

28:04.480 --> 28:07.440
 and then some kind of nonlinear thing

28:07.440 --> 28:10.680
 happens where they percolate up and capture

28:10.680 --> 28:12.480
 the imagination of the mainstream.

28:12.480 --> 28:14.760
 And that seems to be what's happening with AI now.

28:14.760 --> 28:16.120
 I mean, Nietzsche, who you mentioned

28:16.120 --> 28:18.200
 had the idea of the Superman, right?

28:18.200 --> 28:21.520
 But he didn't understand enough about technology

28:21.520 --> 28:24.840
 to think you could physically engineer a Superman

28:24.840 --> 28:28.160
 by piecing together molecules in a certain way.

28:28.160 --> 28:33.560
 He was a bit vague about how the Superman would appear,

28:33.560 --> 28:35.800
 but he was quite deep at thinking

28:35.800 --> 28:37.760
 about what the state of consciousness

28:37.760 --> 28:42.400
 and the mode of cognition of a Superman would be.

28:42.400 --> 28:48.320
 He was a very astute analyst of how the human mind constructs

28:48.320 --> 28:50.800
 the illusion of a self, how it constructs the illusion

28:50.800 --> 28:56.800
 of free will, how it constructs values like good and evil out

28:56.800 --> 29:01.280
 of its own desire to maintain and advance its own organism.

29:01.280 --> 29:03.920
 He understood a lot about how human minds work.

29:03.920 --> 29:07.520
 Then he understood a lot about how post human minds would work.

29:07.520 --> 29:09.440
 I mean, this Superman was supposed

29:09.440 --> 29:12.720
 to be a mind that would basically have complete root

29:12.720 --> 29:15.960
 access to its own brain and consciousness

29:15.960 --> 29:20.320
 and be able to architect its own value system and inspect

29:20.320 --> 29:24.280
 and fine tune all of its own biases.

29:24.280 --> 29:27.800
 So that's a lot of powerful thinking there, which then

29:27.800 --> 29:32.160
 fed in and seeded all of postmodern continental philosophy

29:32.160 --> 29:35.960
 and all sorts of things have been very valuable in development

29:35.960 --> 29:39.640
 of culture and indirectly even of technology.

29:39.640 --> 29:42.080
 But of course, without the technology there,

29:42.080 --> 29:44.800
 it was all some quite abstract thinking.

29:44.800 --> 29:47.960
 So now we're at a time in history when

29:47.960 --> 29:51.920
 a lot of these ideas can be made real, which

29:51.920 --> 29:54.320
 is amazing and scary, right?

29:54.320 --> 29:56.000
 It's kind of interesting to think,

29:56.000 --> 29:57.120
 what do you think Nietzsche would,

29:57.120 --> 30:00.880
 if he was born a century later or transported through time?

30:00.880 --> 30:02.880
 What do you think he would say about AI?

30:02.880 --> 30:04.160
 I mean, those are quite different.

30:04.160 --> 30:07.240
 If he's born a century later or transported through time.

30:07.240 --> 30:09.600
 Well, he'd be on, like, TikTok and Instagram,

30:09.600 --> 30:11.920
 and he would never write the great works he's written.

30:11.920 --> 30:13.520
 So let's transport him through time.

30:13.520 --> 30:17.360
 Maybe also Sprach Zarathustra would be a music video, right?

30:17.360 --> 30:19.640
 I mean, who knows?

30:19.640 --> 30:21.640
 Yeah, but if he was transported through time,

30:21.640 --> 30:24.800
 do you think that that'd be interesting, actually,

30:24.800 --> 30:26.240
 to go back?

30:26.240 --> 30:28.200
 You just made me realize that it's

30:28.200 --> 30:31.200
 possible to go back and read Nietzsche with an eye of,

30:31.200 --> 30:34.720
 is there some thinking about artificial beings?

30:34.720 --> 30:37.760
 I'm sure that he had inklings.

30:37.760 --> 30:40.480
 I mean, with Frankenstein before him,

30:40.480 --> 30:42.880
 I'm sure he had inklings of artificial beings

30:42.880 --> 30:44.080
 somewhere in the text.

30:44.080 --> 30:46.880
 It'd be interesting to see, to try to read his work,

30:46.880 --> 30:55.800
 to see if Superman was actually an AGI system,

30:55.800 --> 30:57.920
 like if he had inklings of that kind of thinking.

30:57.920 --> 30:58.720
 He didn't.

30:58.720 --> 30:59.480
 He didn't.

30:59.480 --> 31:01.120
 No, I would say not.

31:01.120 --> 31:05.960
 I mean, he had a lot of inklings of modern cognitive

31:05.960 --> 31:07.440
 science, which are very interesting.

31:07.440 --> 31:12.040
 If you look in the third part of the collection that's

31:12.040 --> 31:15.680
 been titled The Will to Power, I mean, in book three there,

31:15.680 --> 31:20.600
 there's very deep analysis of thinking processes.

31:20.600 --> 31:27.120
 But he wasn't so much of a physical tinkerer type guy,

31:27.120 --> 31:27.880
 right?

31:27.880 --> 31:29.640
 He was very abstract.

31:29.640 --> 31:32.800
 Do you think, what do you think about The Will to Power?

31:32.800 --> 31:36.120
 Do you think human, what do you think drives humans?

31:36.120 --> 31:37.440
 Is it?

31:37.440 --> 31:40.160
 Oh, an unholy mix of things.

31:40.160 --> 31:43.560
 I don't think there's one pure, simple, and elegant

31:43.560 --> 31:47.400
 objective function driving humans by any means.

31:47.400 --> 31:49.880
 What do you think?

31:49.880 --> 31:52.480
 If we look at, I know it's hard to look at humans

31:52.480 --> 31:57.520
 in an aggregate, but do you think overall humans are good?

31:57.520 --> 32:01.560
 Or do we have both good and evil within us

32:01.560 --> 32:03.520
 that depending on the circumstances,

32:03.520 --> 32:08.200
 depending on whatever can percolate to the top?

32:08.200 --> 32:13.920
 Good and evil are very ambiguous, complicated,

32:13.920 --> 32:15.920
 and in some ways silly concepts.

32:15.920 --> 32:18.560
 But if we could dig into your question

32:18.560 --> 32:19.680
 from a couple directions.

32:19.680 --> 32:23.440
 So I think if you look in evolution,

32:23.440 --> 32:28.200
 humanity is shaped both by individual selection

32:28.200 --> 32:30.920
 and what biologists would call group selection,

32:30.920 --> 32:32.760
 like tribe level selection, right?

32:32.760 --> 32:36.520
 So individual selection has driven us

32:36.520 --> 32:40.800
 in a selfish DNA sort of way so that each of us

32:40.800 --> 32:43.480
 does to a certain approximation what

32:43.480 --> 32:47.400
 will help us propagate our DNA to future generations.

32:47.400 --> 32:50.680
 I mean, that's why I've got to have four kids so far.

32:50.680 --> 32:53.920
 And probably that's not the last one.

32:53.920 --> 32:55.040
 On the other hand.

32:55.040 --> 32:56.760
 I like the ambition.

32:56.760 --> 33:00.720
 Tribal, like group selection, means humans in a way

33:00.720 --> 33:04.360
 will do what will advocate for the persistence of the DNA

33:04.360 --> 33:08.080
 of their whole tribe or their social group.

33:08.080 --> 33:11.760
 And in biology, you have both of these, right?

33:11.760 --> 33:14.440
 And you can see, say, an ant colony or a beehive.

33:14.440 --> 33:17.320
 There's a lot of group selection in the evolution

33:17.320 --> 33:18.960
 of those social animals.

33:18.960 --> 33:23.240
 On the other hand, say a big cat or some very solitary animal.

33:23.240 --> 33:26.560
 It's a lot more biased toward individual selection.

33:26.560 --> 33:28.720
 Humans are an interesting balance.

33:28.720 --> 33:31.880
 And I think this reflects itself in what

33:31.880 --> 33:36.760
 we would view as selfishness versus altruism to some extent.

33:36.760 --> 33:40.560
 So we just have both of those objective functions

33:40.560 --> 33:43.760
 contributing to the makeup of our brains.

33:43.760 --> 33:47.280
 And then as Nietzsche analyzed in his own way

33:47.280 --> 33:49.040
 and others have analyzed in different ways,

33:49.040 --> 33:51.600
 I mean, we abstract this as, well, we

33:51.600 --> 33:55.320
 have both good and evil within us, right?

33:55.320 --> 33:57.800
 Because a lot of what we view as evil

33:57.800 --> 34:00.440
 is really just selfishness.

34:00.440 --> 34:03.680
 A lot of what we view as good is altruism,

34:03.680 --> 34:07.240
 which means doing what's good for the tribe.

34:07.240 --> 34:11.360
 And on that level, we have both of those just baked into us.

34:11.360 --> 34:13.160
 And that's how it is.

34:13.160 --> 34:16.960
 Of course, there are psychopaths and sociopaths

34:16.960 --> 34:21.320
 and people who get gratified by the suffering of others.

34:21.320 --> 34:25.240
 And that's a different thing.

34:25.240 --> 34:27.480
 Yeah, those are exceptions on the whole.

34:27.480 --> 34:31.560
 But I think at core, we're not purely selfish.

34:31.560 --> 34:33.200
 We're not purely altruistic.

34:33.200 --> 34:35.200
 We are a mix.

34:35.200 --> 34:38.000
 And that's the nature of it.

34:38.000 --> 34:43.360
 And we also have a complex constellation of values

34:43.360 --> 34:49.160
 that are just very specific to our evolutionary history.

34:49.160 --> 34:52.480
 Like, we love waterways and mountains.

34:52.480 --> 34:55.040
 And the ideal place to put a house is in a mountain

34:55.040 --> 34:56.320
 overlooking the water, right?

34:56.320 --> 35:00.560
 And we care a lot about our kids.

35:00.560 --> 35:02.800
 And we care a little less about our cousins

35:02.800 --> 35:04.400
 and even less about our fifth cousins.

35:04.400 --> 35:09.440
 I mean, there are many particularities to human values,

35:09.440 --> 35:11.880
 which whether they're good or evil

35:11.880 --> 35:15.360
 depends on your perspective.

35:15.360 --> 35:19.600
 Really, I spend a lot of time in Ethiopia in Addis Ababa,

35:19.600 --> 35:22.440
 where we have one of our AI development offices

35:22.440 --> 35:24.400
 for my singularity net project.

35:24.400 --> 35:28.560
 And when I walk through the streets in Addis,

35:28.560 --> 35:32.200
 there's people lying by the side of the road,

35:32.200 --> 35:33.840
 just living there by the side of the road,

35:33.840 --> 35:35.720
 dying probably of curable diseases

35:35.720 --> 35:37.880
 without enough food or medicine.

35:37.880 --> 35:39.920
 And when I walk by them, I feel terrible.

35:39.920 --> 35:41.400
 I give them money.

35:41.400 --> 35:45.040
 When I come back home to the developed world,

35:45.040 --> 35:46.560
 they're not on my mind that much.

35:46.560 --> 35:47.960
 I do donate some.

35:47.960 --> 35:51.760
 But I also spend some of the limited money

35:51.760 --> 35:54.640
 I have enjoying myself in frivolous ways

35:54.640 --> 35:58.040
 rather than donating it to those people who are right now,

35:58.040 --> 36:00.960
 like starving, dying, and suffering on the roadside.

36:00.960 --> 36:03.120
 So does that make me evil?

36:03.120 --> 36:06.680
 I mean, it makes me somewhat selfish and somewhat altruistic.

36:06.680 --> 36:10.880
 And we each balance that in our own way, right?

36:10.880 --> 36:17.000
 So whether that will be true of all possible AGI's

36:17.000 --> 36:20.040
 is a subtler question.

36:20.040 --> 36:21.280
 That's how humans are.

36:21.280 --> 36:23.040
 So you have a sense, you kind of mentioned

36:23.040 --> 36:26.520
 that there's a selfish, I'm not going to bring up

36:26.520 --> 36:29.880
 the whole Ayn Rand idea of selfishness

36:29.880 --> 36:31.080
 being the core virtue.

36:31.080 --> 36:33.920
 That's a whole interesting kind of tangent

36:33.920 --> 36:36.320
 that I think we'll just distract ourselves on.

36:36.320 --> 36:39.480
 I have to make one amusing comment or comment

36:39.480 --> 36:41.200
 that has amused me anyway.

36:41.200 --> 36:46.280
 So the, yeah, I have extraordinary negative respect

36:46.280 --> 36:47.760
 for Ayn Rand.

36:47.760 --> 36:50.120
 Negative, what's a negative respect?

36:50.120 --> 36:54.720
 But when I worked with a company called Geneshant,

36:54.720 --> 36:59.120
 which was evolving flies to have extraordinary long lives

36:59.120 --> 37:01.160
 in Southern California.

37:01.160 --> 37:04.920
 So we had flies that were evolved by artificial selection

37:04.920 --> 37:07.600
 to have five times the lifespan of normal fruit flies.

37:07.600 --> 37:11.720
 But the population of super long lived flies

37:11.720 --> 37:14.000
 was physically sitting in a spare room

37:14.000 --> 37:18.080
 at an Ayn Rand elementary school in Southern California.

37:18.080 --> 37:21.280
 So that was just like, well, if I saw this in a movie,

37:21.280 --> 37:22.600
 I wouldn't believe it.

37:23.960 --> 37:26.000
 Well, yeah, the universe has a sense of humor

37:26.000 --> 37:26.840
 in that kind of way.

37:26.840 --> 37:28.880
 That fits in, humor fits in somehow

37:28.880 --> 37:30.600
 into this whole absurd existence.

37:30.600 --> 37:33.880
 But you mentioned the balance between selfishness

37:33.880 --> 37:37.200
 and altruism as kind of being innate.

37:37.200 --> 37:39.800
 Do you think it's possible that's kind of an emergent

37:40.560 --> 37:45.400
 phenomenon, those peculiarities of our value system?

37:45.400 --> 37:47.160
 How much of it is innate?

37:47.160 --> 37:49.760
 How much of it is something we collectively,

37:49.760 --> 37:51.480
 kind of like a Dostoevsky novel,

37:52.280 --> 37:54.520
 bring to life together as a civilization?

37:54.520 --> 37:58.840
 I mean, the answer to nature versus nurture is usually both.

37:58.840 --> 38:01.800
 And of course it's nature versus nurture

38:01.800 --> 38:04.760
 versus self organization, as you mentioned.

38:04.760 --> 38:08.440
 So clearly there are evolutionary roots

38:08.440 --> 38:11.480
 to individual and group selection

38:11.480 --> 38:13.880
 leading to a mix of selfishness and altruism.

38:13.880 --> 38:16.720
 On the other hand, different cultures

38:16.720 --> 38:19.720
 manifest that in different ways.

38:19.720 --> 38:22.480
 Well, we all have basically the same biology.

38:22.480 --> 38:26.640
 And if you look at sort of precivilized cultures,

38:26.640 --> 38:29.280
 you have tribes like the Yanomamo in Venezuela,

38:29.280 --> 38:34.280
 which their culture is focused on killing other tribes.

38:35.320 --> 38:37.600
 And you have other Stone Age tribes

38:37.600 --> 38:41.360
 that are mostly peaceable and have big taboos against violence.

38:41.360 --> 38:43.880
 So you can certainly have a big difference

38:43.880 --> 38:48.600
 in how culture manifests these innate

38:48.600 --> 38:50.800
 biological characteristics.

38:50.800 --> 38:54.680
 But still, you know, there's probably limits

38:54.680 --> 38:56.720
 that are given by our biology.

38:56.720 --> 39:00.040
 I used to argue this with my great grandparents

39:00.040 --> 39:01.480
 who were Marxists actually,

39:01.480 --> 39:04.520
 because they believed in the withering away of the state.

39:04.520 --> 39:06.880
 Like they believe that, you know,

39:06.880 --> 39:10.640
 as you move from capitalism to socialism to communism,

39:10.640 --> 39:13.400
 people would just become more social minded

39:13.400 --> 39:15.960
 so that a state would be unnecessary

39:15.960 --> 39:17.840
 and people would just give,

39:17.840 --> 39:20.960
 everyone would give everyone else what they needed.

39:20.960 --> 39:23.680
 Now, setting aside that that's not

39:23.680 --> 39:26.640
 what the various Marxist experiments on the planet

39:26.640 --> 39:29.880
 seemed to be heading toward in practice,

39:29.880 --> 39:32.720
 just as a theoretical point,

39:32.720 --> 39:37.520
 I was very dubious that human nature could go there.

39:37.520 --> 39:39.880
 Like at that time when my great grandparents are alive,

39:39.880 --> 39:43.280
 I was just like, you know, I'm a cynical teenager.

39:43.280 --> 39:46.040
 I think humans are just jerks.

39:46.040 --> 39:48.040
 The state is not gonna wither away.

39:48.040 --> 39:50.720
 If you don't have some structure keeping people

39:50.720 --> 39:52.960
 from screwing each other over, they're gonna do it.

39:52.960 --> 39:56.240
 So now I actually don't quite see things that way.

39:56.240 --> 39:59.920
 I mean, I think my feeling now subjectively

39:59.920 --> 40:02.640
 is the culture aspect is more significant

40:02.640 --> 40:04.640
 than I thought it was when I was a teenager.

40:04.640 --> 40:08.280
 And I think you could have a human society

40:08.280 --> 40:11.440
 that was dialed dramatically further toward,

40:11.440 --> 40:13.720
 you know, self awareness, other awareness,

40:13.720 --> 40:17.000
 compassion and sharing than our current society.

40:17.000 --> 40:20.600
 And of course, greater material abundance helps.

40:20.600 --> 40:23.520
 But to some extent, material abundance

40:23.520 --> 40:25.400
 is a subjective perception also

40:25.400 --> 40:27.440
 because many Stone Age cultures

40:27.440 --> 40:30.560
 perceive themselves as living in great material abundance

40:30.560 --> 40:32.200
 that they had all the food and water they wanted.

40:32.200 --> 40:33.560
 They lived in a beautiful place,

40:33.560 --> 40:37.520
 that they had sex lives, that they had children.

40:37.520 --> 40:42.520
 I mean, they had abundance without any factories, right?

40:42.960 --> 40:46.520
 So I think humanity probably would be capable

40:46.520 --> 40:48.640
 of fundamentally more positive

40:48.640 --> 40:53.120
 and joy filled mode of social existence

40:53.120 --> 40:55.600
 than what we have now.

40:57.360 --> 40:59.520
 Clearly, Marx didn't quite have the right idea

40:59.520 --> 41:01.840
 about how to get there.

41:01.840 --> 41:05.680
 I mean, he missed a number of key aspects

41:05.680 --> 41:09.560
 of human society and its evolution.

41:09.560 --> 41:12.000
 And if we look at where we are in society now,

41:13.200 --> 41:15.800
 how to get there is a quite different question

41:15.800 --> 41:18.960
 because they're very powerful forces pushing people

41:18.960 --> 41:21.120
 in different directions

41:21.120 --> 41:26.120
 than a positive, joyous, compassionate existence, right?

41:26.440 --> 41:29.000
 So if we were tried to, you know,

41:29.000 --> 41:32.880
 Elon Musk is dreams of colonizing Mars at the moment.

41:32.880 --> 41:35.320
 So maybe you'll have a chance to start a new

41:35.320 --> 41:38.400
 civilization with a new governmental system.

41:38.400 --> 41:41.600
 And certainly there's quite a bit of chaos.

41:41.600 --> 41:44.320
 We're sitting now, I don't know what the date is,

41:44.320 --> 41:46.880
 but this is June.

41:46.880 --> 41:48.320
 There's quite a bit of chaos

41:48.320 --> 41:50.760
 in all different forms going on in the United States

41:50.760 --> 41:52.080
 and all over the world.

41:52.080 --> 41:55.560
 So there's a hunger for new types of governments,

41:55.560 --> 41:58.240
 new types of leadership, new types of systems.

41:59.840 --> 42:02.000
 And so what are the forces at play

42:02.000 --> 42:04.120
 and how do we move forward?

42:04.120 --> 42:06.560
 Yeah, I mean, colonizing Mars, first of all,

42:06.560 --> 42:08.560
 it's a super cool thing to do.

42:08.560 --> 42:10.080
 We should be doing it.

42:10.080 --> 42:11.560
 So you love the idea.

42:11.560 --> 42:14.760
 Yeah, I mean, it's more important than making

42:14.760 --> 42:18.520
 chocolatey or chocolates and sexier lingerie

42:18.520 --> 42:22.720
 and many of the things that we spend a lot more resources on

42:22.720 --> 42:24.120
 as a species, right?

42:24.120 --> 42:26.480
 So I mean, we certainly should do it.

42:26.480 --> 42:31.480
 I think the possible future is in which

42:31.480 --> 42:36.000
 a Mars colony makes a critical difference for humanity

42:36.000 --> 42:38.040
 are very few.

42:38.040 --> 42:40.760
 I mean, I think, I mean,

42:40.760 --> 42:42.200
 assuming we make a Mars colony

42:42.200 --> 42:44.000
 and people go live there in a couple of decades,

42:44.000 --> 42:46.360
 I mean, their supplies are gonna come from Earth,

42:46.360 --> 42:48.840
 the money to make the colony came from Earth

42:48.840 --> 42:53.760
 and whatever powers are supplying the goods there

42:53.760 --> 42:55.600
 from Earth are gonna, in effect,

42:55.600 --> 42:58.680
 be in control of that Mars colony.

42:58.680 --> 43:02.040
 Of course, there are outlier situations

43:02.040 --> 43:06.440
 where Earth gets nuked into oblivion

43:06.440 --> 43:09.680
 and somehow Mars has been made self sustaining

43:09.680 --> 43:10.760
 by that point.

43:10.760 --> 43:14.200
 And then Mars is what allows humanity to persist.

43:14.200 --> 43:19.200
 But I think that those are very, very, very unlikely.

43:19.720 --> 43:20.560
 Do you don't think it could be

43:20.560 --> 43:22.920
 a first step on a long journey?

43:22.920 --> 43:24.680
 Of course, it's a first step on a long journey,

43:24.680 --> 43:27.080
 which is awesome.

43:27.080 --> 43:30.160
 I'm guessing the colonization

43:30.160 --> 43:32.000
 of the rest of the physical universe

43:32.000 --> 43:36.080
 will probably be done by AGI's

43:36.080 --> 43:38.120
 that are better designed to live in space

43:38.120 --> 43:41.800
 than by the meat machines that we are.

43:41.800 --> 43:44.680
 But I mean, who knows, we may cry or preserve ourselves

43:44.680 --> 43:46.680
 in some superior way to what we know now

43:46.680 --> 43:50.680
 and shoot ourselves out to Alpha Centaurium beyond.

43:50.680 --> 43:52.640
 I mean, that's all cool.

43:52.640 --> 43:55.120
 It's very interesting and it's much more valuable

43:55.120 --> 43:58.800
 than most things that humanity is spending its resources on.

43:58.800 --> 44:01.120
 On the other hand, with AGI,

44:01.120 --> 44:03.480
 we can get to a singularity

44:03.480 --> 44:07.720
 before the Mars colony becomes sustaining for sure,

44:07.720 --> 44:09.720
 possibly before it's even operational.

44:09.720 --> 44:12.360
 So your intuition is that that's the problem

44:12.360 --> 44:13.800
 if we really invest resources

44:13.800 --> 44:16.400
 and we can get to faster than a legitimate,

44:16.400 --> 44:19.640
 full like self sustaining colonization of Mars.

44:19.640 --> 44:23.120
 Yeah, and it's very clear that we will to me

44:23.120 --> 44:26.000
 because there's so much economic value

44:26.000 --> 44:29.440
 in getting from their AI toward AGI,

44:29.440 --> 44:31.160
 whereas the Mars colony,

44:31.160 --> 44:33.320
 there's less economic value

44:33.320 --> 44:37.320
 until you get quite far out into the future.

44:37.320 --> 44:40.240
 So I think that's very interesting.

44:40.240 --> 44:44.320
 I just think it's somewhat off to the side.

44:44.320 --> 44:47.040
 I mean, just as I think say,

44:47.040 --> 44:49.600
 art and music are very, very interesting

44:49.600 --> 44:53.920
 and I wanna see resources go into amazing art and music

44:53.920 --> 44:58.560
 being created and I'd rather see that

44:58.560 --> 45:01.680
 than a lot of the garbage that society spends their money on.

45:01.680 --> 45:04.600
 On the other hand, I don't think Mars colonization

45:04.600 --> 45:07.760
 or inventing amazing new genres of music

45:07.760 --> 45:10.960
 is not one of the things that is most likely

45:10.960 --> 45:13.880
 to make a critical difference in the evolution

45:13.880 --> 45:18.320
 of human or non human life in this part of the universe

45:18.320 --> 45:19.840
 over the next decade.

45:19.840 --> 45:21.480
 Do you think AGI is really?

45:21.480 --> 45:25.800
 AGI is by far the most important thing

45:25.800 --> 45:27.520
 that's on the horizon.

45:27.520 --> 45:31.640
 And then technologies that have direct ability

45:31.640 --> 45:36.000
 to enable AGI or to accelerate AGI

45:36.000 --> 45:37.240
 are also very important.

45:37.240 --> 45:40.520
 For example, say quantum computing.

45:40.520 --> 45:42.720
 I don't think that's critical to achieve AGI,

45:42.720 --> 45:44.360
 but certainly you could see how

45:44.360 --> 45:46.720
 the right quantum computing architecture

45:46.720 --> 45:49.320
 could massively accelerate AGI,

45:49.320 --> 45:52.520
 similar other types of nanotechnology, right?

45:52.520 --> 45:57.520
 Now, the quest to cure aging and end disease

45:57.920 --> 46:02.120
 while not in the big picture as important as AGI,

46:02.120 --> 46:04.360
 of course, it's important to all of us

46:04.360 --> 46:07.400
 as individual humans.

46:07.400 --> 46:11.640
 And if someone made a super longevity pill

46:11.640 --> 46:14.280
 and distributed it tomorrow, I mean,

46:14.280 --> 46:17.240
 that would be huge and a much larger impact

46:17.240 --> 46:20.480
 than a Mars colony is gonna have for quite some time.

46:20.480 --> 46:23.320
 But perhaps not as much as an AGI system.

46:23.320 --> 46:27.120
 No, because if you can make a benevolent AGI,

46:27.120 --> 46:28.760
 then all the other problems are solved.

46:28.760 --> 46:31.960
 I mean, then the AGI can be,

46:31.960 --> 46:34.280
 once it's as generally intelligent as humans,

46:34.280 --> 46:37.440
 it can rapidly become massively more generally intelligent

46:37.440 --> 46:38.640
 than humans.

46:38.640 --> 46:42.200
 And then that AGI should be able to solve

46:42.200 --> 46:44.480
 science and engineering problems much better

46:44.480 --> 46:48.600
 than human beings, as long as it is, in fact,

46:48.600 --> 46:49.720
 motivated to do so.

46:49.720 --> 46:52.760
 That's why I said a benevolent AGI.

46:52.760 --> 46:54.080
 There could be other kinds.

46:54.080 --> 46:56.040
 Maybe it's good to step back a little bit.

46:56.040 --> 46:58.880
 I mean, we've been using the term AGI.

46:58.880 --> 47:00.880
 People often cite you as the creator,

47:00.880 --> 47:03.120
 at least the popularizer of the term AGI,

47:03.120 --> 47:05.720
 artificial general intelligence.

47:05.720 --> 47:08.600
 Can you tell the origin story of the term?

47:08.600 --> 47:09.440
 Sure, sure.

47:09.440 --> 47:14.440
 I would say I launched the term AGI upon the world

47:14.920 --> 47:16.640
 for what it's worth,

47:16.640 --> 47:21.680
 without ever fully being in love with the term.

47:21.680 --> 47:25.400
 What happened is I was editing a book,

47:25.400 --> 47:27.880
 and this process started around 2001 or two.

47:27.880 --> 47:30.520
 I think the book came out 2005, finally.

47:30.520 --> 47:32.040
 I was editing a book,

47:32.040 --> 47:35.040
 which I provisionally was titling real AI.

47:35.880 --> 47:38.840
 And I mean, the goal was to gather together

47:38.840 --> 47:41.680
 fairly serious academicish papers

47:41.680 --> 47:43.920
 on the topic of making thinking machines

47:43.920 --> 47:46.800
 that could really think in the sense like people can,

47:46.800 --> 47:49.240
 or even more broadly than people can, right?

47:49.240 --> 47:52.760
 So then I was reaching out to other folks

47:52.760 --> 47:54.080
 that I had encountered here or there

47:54.080 --> 47:57.360
 who were interested in that,

47:57.360 --> 48:01.360
 which included some other folks out of the,

48:01.360 --> 48:04.360
 who I knew from the transhumist and singularitarian world,

48:04.360 --> 48:06.440
 like Peter Vos, who has a company,

48:06.440 --> 48:09.760
 AGI Incorporated still in California,

48:09.760 --> 48:13.080
 and included Shane Legg,

48:13.080 --> 48:15.720
 who had worked for me at my company WebMind

48:15.720 --> 48:17.600
 in New York in the late 90s,

48:17.600 --> 48:20.480
 who by now has become rich and famous.

48:20.480 --> 48:22.800
 He was one of the cofounders of Google DeepMind,

48:22.800 --> 48:25.280
 but at that time Shane was,

48:27.200 --> 48:29.920
 I think he may have been,

48:29.920 --> 48:34.040
 have just started doing his PhD with Marcus Hooter,

48:34.040 --> 48:38.680
 who at that time hadn't yet published his book Universal AI,

48:38.680 --> 48:41.040
 which sort of gives a mathematical foundation

48:41.040 --> 48:43.400
 for artificial general intelligence.

48:43.400 --> 48:46.120
 So I reached out to Shane and Marcus and Peter Vos

48:46.120 --> 48:49.480
 and Pei Wang, who was another former employee of mine,

48:49.480 --> 48:51.880
 who had been Douglas Hofstadter's PhD student,

48:51.880 --> 48:53.280
 who had his own approach to AGI.

48:53.280 --> 48:55.720
 And a bunch of some Russian folks

48:56.600 --> 48:58.040
 reached out to these guys

48:58.040 --> 49:01.360
 and they contributed papers for the book.

49:01.360 --> 49:03.440
 But that was my provisional title,

49:03.440 --> 49:06.120
 but I never loved it because in the end,

49:07.000 --> 49:11.360
 I was doing some, what we would now call narrow AI,

49:11.360 --> 49:14.640
 as well like applying machine learning to genomics data

49:14.640 --> 49:17.920
 or chat data for sentiment analysis.

49:17.920 --> 49:19.240
 I mean, that work is real.

49:19.240 --> 49:22.760
 And in a sense, it's really AI,

49:22.760 --> 49:26.000
 it's just a different kind of AI.

49:26.000 --> 49:31.000
 Ray Kurzweil wrote about narrow AI versus strong AI.

49:31.160 --> 49:32.960
 But that seemed weird to me

49:32.960 --> 49:36.720
 because first of all, narrow and strong are not antennas.

49:36.720 --> 49:38.680
 That's right.

49:38.680 --> 49:41.920
 But secondly, strong AI was used

49:41.920 --> 49:43.320
 in the cognitive science literature

49:43.320 --> 49:46.600
 to mean the hypothesis that digital computer AIs

49:46.600 --> 49:50.120
 could have true consciousness like human beings.

49:50.120 --> 49:52.480
 So there was already a meaning to strong AI,

49:52.480 --> 49:56.440
 which was complexly different but related, right?

49:56.440 --> 50:00.480
 So we were tossing around on an email list

50:00.480 --> 50:03.160
 whether what title it should be.

50:03.160 --> 50:07.520
 And so we talked about narrow AI, broad AI, wide AI,

50:07.520 --> 50:09.720
 narrow AI, general AI.

50:09.720 --> 50:14.720
 And I think it was either Shane Leg or Peter Vos

50:15.800 --> 50:18.080
 on the private email discussion we had.

50:18.080 --> 50:20.160
 He said, well, why don't we go with AGI,

50:20.160 --> 50:21.720
 artificial general intelligence?

50:21.720 --> 50:24.200
 And say, Pei Wang wanted to do GAI,

50:24.200 --> 50:25.680
 general artificial intelligence,

50:25.680 --> 50:27.800
 because in Chinese it goes in that order, right?

50:27.800 --> 50:31.960
 But we figured GAI wouldn't work in US culture

50:31.960 --> 50:33.160
 at that time, right?

50:33.160 --> 50:37.280
 So we went with the AGI,

50:37.280 --> 50:39.440
 we used it for the title of that book.

50:39.440 --> 50:43.400
 And part of Peter and Shane's reasoning

50:43.400 --> 50:45.400
 was you have the G factor in psychology,

50:45.400 --> 50:47.440
 which is IQ, general intelligence, right?

50:47.440 --> 50:49.400
 So you have a meaning of GI,

50:49.400 --> 50:52.120
 general intelligence in psychology.

50:52.120 --> 50:55.360
 So then you're looking like artificial GI.

50:55.360 --> 51:00.360
 So then we use that for the title of the book.

51:00.400 --> 51:04.040
 And so I think I may be both Shane and Peter

51:04.040 --> 51:05.200
 think they invented the term,

51:05.200 --> 51:08.320
 but then later after the book was published,

51:08.320 --> 51:11.120
 this guy, Mark Gubrid came up to me

51:11.120 --> 51:13.640
 and he's like, well, I published an essay

51:13.640 --> 51:17.120
 with the term AGI in like 1997 or something.

51:17.120 --> 51:20.520
 And so I'm just waiting for some Russian to come out

51:20.520 --> 51:23.400
 and say they published that in 1953, right?

51:23.400 --> 51:27.800
 I mean, that term is not dramatically innovative

51:27.800 --> 51:28.640
 or anything.

51:28.640 --> 51:31.600
 It's one of these obvious in hindsight things,

51:31.600 --> 51:34.920
 which is also annoying in a way

51:34.920 --> 51:39.520
 because, you know, Josh Abak, who you interviewed

51:39.520 --> 51:40.440
 is a close friend of mine.

51:40.440 --> 51:43.280
 He likes the term synthetic intelligence,

51:43.280 --> 51:44.320
 which I like much better,

51:44.320 --> 51:47.120
 but it hasn't actually caught on, right?

51:47.120 --> 51:51.840
 Because I mean, artificial is a bit off to me

51:51.840 --> 51:54.680
 because artificial is like a tool or something,

51:54.680 --> 51:57.800
 but not all AGIs are gonna be tools.

51:57.800 --> 51:58.720
 I mean, they may be now,

51:58.720 --> 52:02.840
 but we're aiming toward making them agents rather than tools.

52:02.840 --> 52:04.880
 And in a way, I don't like the distinction

52:04.880 --> 52:07.240
 between artificial and natural

52:07.240 --> 52:09.400
 because I mean, we're part of nature also

52:09.400 --> 52:12.160
 and machines are part of nature.

52:12.160 --> 52:14.880
 I mean, you can look at evolved versus engineered,

52:14.880 --> 52:17.200
 but that's a different distinction.

52:17.200 --> 52:20.040
 Then it should be engineered general intelligence, right?

52:20.040 --> 52:21.960
 And then general, well,

52:21.960 --> 52:24.680
 if you look at Marcus Hooter's book,

52:24.680 --> 52:28.240
 universally, what he argues there is, you know,

52:28.240 --> 52:30.560
 within the domain of computation theory,

52:30.560 --> 52:31.960
 which has limited been interesting.

52:31.960 --> 52:33.720
 So if you assume computable environments

52:33.720 --> 52:35.640
 and computable reward functions,

52:35.640 --> 52:37.600
 then he articulates what would be

52:37.600 --> 52:40.040
 a truly general intelligence,

52:40.040 --> 52:43.240
 a system called AIXI, which is quite beautiful.

52:43.240 --> 52:44.080
 IXI.

52:44.080 --> 52:46.040
 IXI, and that's the middle name

52:46.040 --> 52:49.360
 of my latest child, actually, is it?

52:49.360 --> 52:50.200
 What's the first name?

52:50.200 --> 52:52.400
 First name is Quarkxi, QORXI,

52:52.400 --> 52:53.800
 which my wife came up with,

52:53.800 --> 52:55.200
 but that's an acronym

52:55.200 --> 52:58.880
 for quantum organized rational expanding intelligence.

52:58.880 --> 53:03.680
 And his middle name is Xiphanes, actually,

53:03.680 --> 53:08.360
 which means the former principal underlying AIXI.

53:08.360 --> 53:09.480
 But in any case...

53:09.480 --> 53:12.120
 You're giving Elon Musk a new child to run for...

53:12.120 --> 53:13.760
 Well, I did it first.

53:13.760 --> 53:17.320
 He copied me with this new freakish name.

53:17.320 --> 53:18.560
 But now if I have another baby,

53:18.560 --> 53:19.880
 I'm gonna have to outdo him.

53:19.880 --> 53:20.720
 Outdo him.

53:20.720 --> 53:24.520
 It's becoming an arms race of weird geeky baby names.

53:24.520 --> 53:26.280
 We'll see what the babies think about it, right?

53:26.280 --> 53:27.120
 Yeah.

53:27.120 --> 53:30.160
 But, I mean, my oldest son, Zarathustra, loves his name,

53:30.160 --> 53:33.200
 and my daughter, Sharazad, loves her name.

53:33.200 --> 53:36.920
 So far, basically, if you give your kids weird names...

53:36.920 --> 53:37.760
 They live up to it.

53:37.760 --> 53:39.720
 Well, you're obliged to make the kids weird enough

53:39.720 --> 53:41.640
 that they like the names, right?

53:41.640 --> 53:43.840
 It directs their upbringing in a certain way.

53:43.840 --> 53:46.200
 But, yeah, anyway, I mean,

53:46.200 --> 53:47.600
 what Mark has shown in that book

53:47.600 --> 53:50.480
 is that a truly general intelligence,

53:50.480 --> 53:51.720
 theoretically as possible,

53:51.720 --> 53:53.760
 but would take infinite computing power.

53:53.760 --> 53:56.280
 So then the artificial is a little off.

53:56.280 --> 53:59.720
 The general is not really achievable within physics

53:59.720 --> 54:01.200
 as we know it.

54:01.200 --> 54:03.440
 And, I mean, physics as we know it may be limited,

54:03.440 --> 54:05.240
 but that's what we have to work with now.

54:05.240 --> 54:06.080
 Intelligence...

54:06.080 --> 54:07.280
 Infinitely general, you mean?

54:07.280 --> 54:10.400
 Like, information processing perspective, yeah.

54:10.400 --> 54:14.680
 Yeah, intelligence is not very well defined either, right?

54:14.680 --> 54:16.680
 I mean, what does it mean?

54:16.680 --> 54:19.480
 I mean, in AI now, it's fashionable to look at it

54:19.480 --> 54:23.240
 as maximizing an expected reward over the future.

54:23.240 --> 54:27.720
 But that sort of definition is pathological in various ways.

54:27.720 --> 54:31.200
 And my friend David Weinbaum, a.k.a. Weaver,

54:31.200 --> 54:34.760
 he had a beautiful PhD thesis on open ended intelligence,

54:34.760 --> 54:36.800
 trying to conceive intelligence in a...

54:36.800 --> 54:38.120
 Without a reward.

54:38.120 --> 54:40.040
 Yeah, he's just looking at it differently.

54:40.040 --> 54:42.600
 He's looking at complex self organizing systems

54:42.600 --> 54:44.560
 and looking at an intelligence system

54:44.560 --> 54:48.800
 as being one that revises and grows and improves itself

54:48.800 --> 54:51.640
 in conjunction with its environment

54:51.640 --> 54:54.800
 without necessarily there being one objective function

54:54.800 --> 54:56.000
 that's trying to maximize.

54:56.000 --> 54:58.440
 Although over certain intervals of time,

54:58.440 --> 55:01.280
 it may act as if it's optimizing a certain objective function.

55:01.280 --> 55:04.480
 Very much Solaris from Stanislav Lem's novels, right?

55:04.480 --> 55:07.760
 So, yeah, the point is artificial, general, and intelligence.

55:07.760 --> 55:08.600
 Don't work.

55:08.600 --> 55:09.440
 They're all bad.

55:09.440 --> 55:11.960
 On the other hand, everyone knows what AI is.

55:11.960 --> 55:15.840
 And AGI seems immediately comprehensible

55:15.840 --> 55:17.480
 to people with a technical background.

55:17.480 --> 55:20.640
 So I think that the term has served as sociological function.

55:20.640 --> 55:24.720
 Now it's out there everywhere, which baffles me.

55:24.720 --> 55:25.760
 It's like KFC.

55:25.760 --> 55:26.800
 I mean, that's it.

55:26.800 --> 55:30.160
 We're stuck with AGI probably for a very long time

55:30.160 --> 55:33.600
 until AGI systems take over and rename themselves.

55:33.600 --> 55:34.160
 Yeah.

55:34.160 --> 55:36.120
 And then we'll be biological.

55:36.120 --> 55:37.520
 We're stuck with GPUs too,

55:37.520 --> 55:40.480
 which mostly have nothing to do with graphics anymore.

55:40.480 --> 55:43.240
 I wonder what the AGI system will call us humans.

55:43.240 --> 55:44.240
 That was maybe.

55:44.240 --> 55:45.240
 Grandpa.

55:45.240 --> 55:46.560
 Yeah.

55:46.560 --> 55:48.320
 GPs.

55:48.320 --> 55:50.320
 Grandpa Processing Unit.

55:50.320 --> 55:54.240
 Biological Grandpa Processing Unit.

55:54.240 --> 55:59.200
 OK, so maybe also just a comment on AGI

55:59.200 --> 56:02.120
 representing, before even the term existed,

56:02.120 --> 56:04.560
 representing a kind of community.

56:04.560 --> 56:06.200
 You've talked about this in the past.

56:06.200 --> 56:08.280
 Sort of, AI is coming in waves.

56:08.280 --> 56:09.960
 But there's always been this community

56:09.960 --> 56:13.200
 of people who dream about creating

56:13.200 --> 56:19.000
 general human level superintelligence systems.

56:19.000 --> 56:23.280
 Can you maybe give your sense of the history of this community

56:23.280 --> 56:26.080
 as it exists today, as it existed before this deep learning

56:26.080 --> 56:29.480
 revolution, all throughout the winters and the summers of AI?

56:29.480 --> 56:30.280
 Sure.

56:30.280 --> 56:33.520
 First, I would say, as a side point,

56:33.520 --> 56:36.960
 the winters and summers of AI are greatly

56:36.960 --> 56:39.920
 exaggerated by Americans.

56:39.920 --> 56:43.560
 And if you look at the publication record

56:43.560 --> 56:46.400
 of the artificial intelligence community,

56:46.400 --> 56:51.360
 since, say, the 1950s, you would find a pretty steady growth

56:51.360 --> 56:54.000
 in advance of ideas and papers.

56:54.000 --> 56:57.720
 And what's thought of as an AI winter or summer

56:57.720 --> 57:00.840
 was sort of how much money is the US military pumping

57:00.840 --> 57:04.640
 into AI, which was meaningful.

57:04.640 --> 57:07.480
 On the other hand, there was AI going on in Germany, UK,

57:07.480 --> 57:10.920
 and in Japan, and in Russia, all over the place,

57:10.920 --> 57:16.280
 while US military got more and less enthused about AI.

57:16.280 --> 57:17.520
 So I mean.

57:17.520 --> 57:20.200
 That happened to be, just for people who don't know,

57:20.200 --> 57:23.640
 the US military happened to be the main source of funding

57:23.640 --> 57:24.480
 for AI research.

57:24.480 --> 57:26.800
 So another way to phrase that is it's

57:26.800 --> 57:30.480
 up and down of funding for artificial intelligence

57:30.480 --> 57:31.080
 research.

57:31.080 --> 57:34.560
 And I would say the correlation between funding

57:34.560 --> 57:38.080
 and intellectual advance was not 100%, right?

57:38.080 --> 57:42.080
 Because I mean, in Russia, as an example, or in Germany,

57:42.080 --> 57:44.760
 there was less dollar funding than in the US.

57:44.760 --> 57:48.120
 But many foundational ideas were laid out.

57:48.120 --> 57:50.840
 But it was more theory than implementation, right?

57:50.840 --> 57:54.560
 And US really excelled at sort of breaking through

57:54.560 --> 58:01.360
 from theoretical papers to working implementations, which

58:01.360 --> 58:04.280
 did go up and down somewhat with US military funding.

58:04.280 --> 58:07.400
 But still, I mean, you can look in the 1980s,

58:07.400 --> 58:10.360
 Dietrich Dörner in Germany had self driving cars

58:10.360 --> 58:11.400
 on the Autobahn, right?

58:11.400 --> 58:16.000
 And I mean, this was a little early with regard

58:16.000 --> 58:16.880
 to the car industry.

58:16.880 --> 58:20.160
 So it didn't catch on, such as has happened now.

58:20.160 --> 58:24.000
 But I mean, that whole advancement of self driving car

58:24.000 --> 58:26.440
 technology in Germany was pretty much

58:26.440 --> 58:31.000
 independent of AI military summers and winters in the US.

58:31.000 --> 58:35.560
 So there's been more going on in AI globally than not only

58:35.560 --> 58:37.120
 most people on the planet realize,

58:37.120 --> 58:40.080
 but then most new AI PhDs realize,

58:40.080 --> 58:44.640
 because they've come up within a certain subfield of AI

58:44.640 --> 58:47.680
 and haven't had to look so much beyond that.

58:47.680 --> 58:54.320
 But I would say when I got my PhD in 1989 in mathematics,

58:54.320 --> 58:56.040
 I was interested in AI already.

58:56.040 --> 58:56.800
 In Philadelphia.

58:56.800 --> 59:00.960
 Yeah, I started at NYU, then I transferred to Philadelphia

59:00.960 --> 59:03.960
 to Temple University, good old North Philly.

59:03.960 --> 59:04.920
 North Philly, yeah.

59:04.920 --> 59:09.280
 Yeah, the pearl of the US.

59:09.280 --> 59:10.880
 You never stopped at a red light, then,

59:10.880 --> 59:12.800
 because you were afraid if you stopped at a red light,

59:12.800 --> 59:13.760
 some more car jacking.

59:13.760 --> 59:17.600
 So you just drive through every red light.

59:17.600 --> 59:20.960
 Every day driving or bicycling to Temple from my house

59:20.960 --> 59:24.280
 was like a new adventure.

59:24.280 --> 59:27.560
 But yeah, the reason I didn't do a PhD in AI

59:27.560 --> 59:30.880
 was what people were doing in the academic AI field then,

59:30.880 --> 59:34.880
 was just astoundingly boring and seemed wrongheaded to me.

59:34.880 --> 59:38.720
 It was really like rule based expert systems and production

59:38.720 --> 59:39.320
 systems.

59:39.320 --> 59:42.080
 And actually, I loved mathematical logic.

59:42.080 --> 59:45.800
 I had nothing against logic as the cognitive engine for an AI.

59:45.800 --> 59:48.920
 But the idea that you could type in the knowledge

59:48.920 --> 59:52.720
 that AI would need to think seemed just completely stupid

59:52.720 --> 59:55.360
 and wrongheaded to me.

59:55.360 --> 59:57.400
 I mean, you can use logic if you want,

59:57.400 --> 1:00:00.720
 but somehow the system has got to be automated.

1:00:00.720 --> 1:00:01.560
 Learning, right?

1:00:01.560 --> 1:00:03.800
 It should be learning from experience.

1:00:03.800 --> 1:00:06.120
 And the AI field then was not interested

1:00:06.120 --> 1:00:08.320
 in learning from experience.

1:00:08.320 --> 1:00:11.040
 I mean, some researchers certainly were.

1:00:11.040 --> 1:00:15.200
 I mean, I remember in mid 80s, I discovered a book

1:00:15.200 --> 1:00:20.720
 by John Andreas, which was it was about a reinforcement

1:00:20.720 --> 1:00:26.160
 learning system called Purpose, PURR dash PUSS,

1:00:26.160 --> 1:00:28.080
 which was an acronym that I can't even remember what

1:00:28.080 --> 1:00:30.360
 it was for, Purpose anyway.

1:00:30.360 --> 1:00:34.280
 But that was a system that was supposed to be an AGI.

1:00:34.280 --> 1:00:40.560
 And basically, by some sort of fancy Markov decision process

1:00:40.560 --> 1:00:43.640
 learning, it was supposed to learn everything just

1:00:43.640 --> 1:00:46.520
 from the bits coming into it and learn to maximize its reward

1:00:46.520 --> 1:00:49.040
 and become intelligent, right?

1:00:49.040 --> 1:00:51.720
 So that was there in academia back then.

1:00:51.720 --> 1:00:55.160
 But it was isolated, scattered, weird people.

1:00:55.160 --> 1:00:57.400
 But all these isolated, scattered, weird people

1:00:57.400 --> 1:01:00.200
 in that period, I mean, they laid

1:01:00.200 --> 1:01:02.120
 intellectual grounds for what happened later.

1:01:02.120 --> 1:01:05.280
 So you look at John Andreas at University of Canterbury

1:01:05.280 --> 1:01:09.720
 with his Purpose Reinforcement Learning Markov system.

1:01:09.720 --> 1:01:14.040
 He was the PhD supervisor for John Cleary in New Zealand.

1:01:14.040 --> 1:01:17.240
 Now, John Cleary worked with me when

1:01:17.240 --> 1:01:21.640
 I was at Waikato University in 1993 in New Zealand.

1:01:21.640 --> 1:01:23.880
 And he worked with Ian Whitten there.

1:01:23.880 --> 1:01:28.640
 And they launched WECA, which was the first open source machine

1:01:28.640 --> 1:01:32.360
 learning toolkit, which was launched in, I guess,

1:01:32.360 --> 1:01:35.160
 93 or 94 when I was at Waikato University.

1:01:35.160 --> 1:01:36.440
 Written in Java, unfortunately.

1:01:36.440 --> 1:01:39.600
 Written in Java, which was a cool language back then.

1:01:39.600 --> 1:01:41.720
 I guess it's still, well, it's not cool anymore,

1:01:41.720 --> 1:01:43.240
 but it's powerful.

1:01:43.240 --> 1:01:45.760
 I find, like most programmers now,

1:01:45.760 --> 1:01:48.800
 I find Java unnecessarily bloated.

1:01:48.800 --> 1:01:52.000
 But back then, it was like Java or C++, basically.

1:01:52.000 --> 1:01:53.400
 Object oriented, so it's nice.

1:01:53.400 --> 1:01:55.800
 Java was easier for students.

1:01:55.800 --> 1:01:57.760
 Amusingly, a lot of the work on WECA

1:01:57.760 --> 1:02:01.560
 when we were in New Zealand was funded by a US, sorry,

1:02:01.560 --> 1:02:05.440
 a New Zealand government grant to use machine learning

1:02:05.440 --> 1:02:08.240
 to predict the menstrual cycles of cows.

1:02:08.240 --> 1:02:10.440
 So in the US, all the grant funding for AI

1:02:10.440 --> 1:02:13.640
 was about how to kill people or spy on people.

1:02:13.640 --> 1:02:16.400
 In New Zealand, it's all about cows or kiwi fruits, right?

1:02:16.400 --> 1:02:17.560
 Yeah.

1:02:17.560 --> 1:02:20.560
 So yeah, anyway, I mean, John Andreas

1:02:20.560 --> 1:02:24.320
 had his probability theory based reinforcement learning

1:02:24.320 --> 1:02:28.480
 proto AGI, John Cleary was trying to do much more

1:02:28.480 --> 1:02:31.840
 ambitious probabilistic AGI systems.

1:02:31.840 --> 1:02:36.360
 Now, John Cleary helped do WECA, which

1:02:36.360 --> 1:02:39.240
 is the first open source machine learning tool,

1:02:39.240 --> 1:02:41.520
 gets it a predecessor for TensorFlow and Torch

1:02:41.520 --> 1:02:43.080
 and all these things.

1:02:43.080 --> 1:02:49.200
 Also, Shane Legg was at Waikato working with John Cleary

1:02:49.200 --> 1:02:51.520
 and Ian Whitten and this whole group,

1:02:51.520 --> 1:02:55.800
 and then working with my own company,

1:02:55.800 --> 1:02:59.840
 my company WebMind, an AI company I had in the late 90s

1:02:59.840 --> 1:03:02.480
 with a team there at Waikato University, which

1:03:02.480 --> 1:03:06.440
 is how Shane got his head full of AGI, which led him to go on

1:03:06.440 --> 1:03:08.680
 and with Demosis Abbas found DeepMind.

1:03:08.680 --> 1:03:11.840
 So what you can see through that lineage is in the 80s

1:03:11.840 --> 1:03:14.800
 and 70s, John Andreas was trying to build probabilistic

1:03:14.800 --> 1:03:17.200
 reinforcement learning AGI systems.

1:03:17.200 --> 1:03:19.680
 The technology, the computers just weren't there to support it.

1:03:19.680 --> 1:03:23.880
 His ideas were very similar to what people are doing now.

1:03:23.880 --> 1:03:27.680
 But although he's long since passed away

1:03:27.680 --> 1:03:30.920
 and didn't become that famous outside of Canterbury,

1:03:30.920 --> 1:03:33.680
 I mean, the lineage of ideas passed on from him

1:03:33.680 --> 1:03:35.120
 to his students to their students,

1:03:35.120 --> 1:03:37.880
 you can go trace directly from there to me

1:03:37.880 --> 1:03:39.440
 and to DeepMind.

1:03:39.440 --> 1:03:45.120
 So there was a lot going on in AGI that did ultimately

1:03:45.120 --> 1:03:46.960
 lay the groundwork for what we have today,

1:03:46.960 --> 1:03:48.520
 but there wasn't a community.

1:03:48.520 --> 1:03:55.800
 And so when I started trying to pull together an AGI community,

1:03:55.800 --> 1:03:58.240
 it was in, I guess, the early arts

1:03:58.240 --> 1:04:00.400
 when I was living in Washington DC

1:04:00.400 --> 1:04:04.560
 and making a living doing AI consulting for various US

1:04:04.560 --> 1:04:07.120
 government agencies.

1:04:07.120 --> 1:04:13.200
 And I organized the first AGI workshop in 2006.

1:04:13.200 --> 1:04:15.760
 And I mean, it wasn't like it was literally

1:04:15.760 --> 1:04:17.000
 in my basement or something.

1:04:17.000 --> 1:04:20.360
 I mean, it was in the conference room at a Marriott in Bethesda.

1:04:20.360 --> 1:04:23.880
 It's not that edgy or underground, unfortunately.

1:04:23.880 --> 1:04:27.520
 But still, that's 60 or something.

1:04:27.520 --> 1:04:28.400
 That's not bad.

1:04:28.400 --> 1:04:30.680
 I mean, DC has a lot of AI going on.

1:04:30.680 --> 1:04:34.120
 Probably until the last five or 10 years,

1:04:34.120 --> 1:04:35.640
 much more than Silicon Valley.

1:04:35.640 --> 1:04:38.680
 Although it's just quiet because of the nature

1:04:38.680 --> 1:04:41.240
 of what happens in DC.

1:04:41.240 --> 1:04:43.520
 Their business isn't driven by PR.

1:04:43.520 --> 1:04:46.080
 Mostly when something starts to work really well,

1:04:46.080 --> 1:04:49.600
 it's taken black and becomes even more quiet.

1:04:49.600 --> 1:04:51.680
 But yeah, the thing is that really

1:04:51.680 --> 1:04:57.000
 had the feeling of a group of starry eyed mavericks

1:04:57.000 --> 1:04:59.520
 huddled in a basement plotting how

1:04:59.520 --> 1:05:02.520
 to overthrow the narrow AI establishment.

1:05:02.520 --> 1:05:05.760
 And for the first time, in some cases,

1:05:05.760 --> 1:05:09.640
 coming together with others who shared their passion for AGI

1:05:09.640 --> 1:05:13.200
 and the technical seriousness about working on it.

1:05:13.200 --> 1:05:19.160
 And that's very, very different than what we have today.

1:05:19.160 --> 1:05:22.280
 I mean, now it's a little bit different.

1:05:22.280 --> 1:05:24.600
 We have AGI conference every year,

1:05:24.600 --> 1:05:29.280
 and there's several hundred people rather than 50.

1:05:29.280 --> 1:05:33.120
 Now it's more like this is the main gathering of people

1:05:33.120 --> 1:05:38.240
 who want to achieve AGI and think that large scale, nonlinear

1:05:38.240 --> 1:05:42.440
 regression is not the golden path to AGI.

1:05:42.440 --> 1:05:44.000
 So I mean, it's a annual network.

1:05:44.000 --> 1:05:44.920
 Yeah, yeah, yeah.

1:05:44.920 --> 1:05:51.360
 Well, certain architecture is for learning using neural

1:05:51.360 --> 1:05:51.880
 networks.

1:05:51.880 --> 1:05:54.400
 So yeah, the AGI conferences are now

1:05:54.400 --> 1:05:57.920
 the main concentration of people not obsessed

1:05:57.920 --> 1:06:00.800
 with deep neural nets and deep reinforcement learning,

1:06:00.800 --> 1:06:06.400
 but still interested in AGI, not the only ones.

1:06:06.400 --> 1:06:10.160
 I mean, there's other little conferences and groupings

1:06:10.160 --> 1:06:15.440
 interested in human level AI and cognitive architectures

1:06:15.440 --> 1:06:15.960
 and so forth.

1:06:15.960 --> 1:06:17.840
 But yeah, it's been a big shift.

1:06:17.840 --> 1:06:21.920
 Like back then, you couldn't really,

1:06:21.920 --> 1:06:25.120
 it'll be very, very edgy then to give a university department

1:06:25.120 --> 1:06:28.400
 seminar that mentioned AGI or human level AI.

1:06:28.400 --> 1:06:31.080
 It was more like you had to talk about something more

1:06:31.080 --> 1:06:34.320
 short term and immediately practical.

1:06:34.320 --> 1:06:36.520
 Then in the bar after the seminar,

1:06:36.520 --> 1:06:39.480
 you could bullshit about AGI in the same breath

1:06:39.480 --> 1:06:43.080
 as time travel or the simulation hypothesis

1:06:43.080 --> 1:06:46.800
 or something where it's now AGI is not only

1:06:46.800 --> 1:06:49.200
 in the academic seminar room, like you

1:06:49.200 --> 1:06:51.880
 have Vladimir Putin knows what AGI is.

1:06:51.880 --> 1:06:55.440
 And he's like, Roshan needs to become the leader in AGI.

1:06:55.440 --> 1:07:01.040
 So national leaders and CEOs of large corporations.

1:07:01.040 --> 1:07:04.200
 I mean, the CTO of Intel, Justin Ratner,

1:07:04.200 --> 1:07:06.800
 this was years ago, Singularity Summit conference,

1:07:06.800 --> 1:07:07.760
 2008 or something.

1:07:07.760 --> 1:07:10.080
 He's like, we believe Ray Kurzweil,

1:07:10.080 --> 1:07:12.000
 the Singularity will happen in 2045.

1:07:12.000 --> 1:07:14.640
 And it will have Intel inside.

1:07:14.640 --> 1:07:18.840
 I mean, so it's gone from being something

1:07:18.840 --> 1:07:22.400
 which is the pursuit of crazed mavericks, crackpots,

1:07:22.400 --> 1:07:28.280
 and science fiction fanatics to being a marketing term

1:07:28.280 --> 1:07:32.000
 for large corporations and the national leaders, which

1:07:32.000 --> 1:07:35.120
 is an astounding transition.

1:07:35.120 --> 1:07:40.200
 But yeah, in the course of this transition,

1:07:40.200 --> 1:07:42.240
 I think a bunch of subcommittees have formed.

1:07:42.240 --> 1:07:45.840
 And the community around the AGI conference series

1:07:45.840 --> 1:07:47.800
 is certainly one of them.

1:07:47.800 --> 1:07:51.920
 It hasn't grown as big as I might have liked it to.

1:07:51.920 --> 1:07:56.320
 On the other hand, sometimes a modest sized community

1:07:56.320 --> 1:07:59.480
 can be better for making intellectual progress also.

1:07:59.480 --> 1:08:02.160
 You get it with Society for Neuroscience Conference.

1:08:02.160 --> 1:08:05.400
 You have 35,000 or 40,000 neuroscientists.

1:08:05.400 --> 1:08:07.480
 On the one hand, it's amazing.

1:08:07.480 --> 1:08:09.760
 On the other hand, you're not going

1:08:09.760 --> 1:08:12.320
 to talk to the leaders of the field there

1:08:12.320 --> 1:08:14.120
 if you're an outsider.

1:08:14.120 --> 1:08:17.880
 Yeah, in the same sense, the AAAI,

1:08:17.880 --> 1:08:20.880
 the artificial intelligence, the main kind

1:08:20.880 --> 1:08:25.440
 of generic artificial intelligence conference,

1:08:25.440 --> 1:08:26.920
 it's too big.

1:08:26.920 --> 1:08:28.240
 It's too amorphous.

1:08:28.240 --> 1:08:30.240
 Like, it doesn't make.

1:08:30.240 --> 1:08:34.920
 Well, yeah, and NIPP has become a company advertising

1:08:34.920 --> 1:08:36.960
 outlet now.

1:08:36.960 --> 1:08:40.240
 So I mean, to comment on the role of AGI

1:08:40.240 --> 1:08:42.680
 in the research community, I'd still,

1:08:42.680 --> 1:08:45.160
 if you look at NeurIPS, if you look at CVPR,

1:08:45.160 --> 1:08:51.840
 if you look at these iClear, AGI is still seen as the outcast.

1:08:51.840 --> 1:08:55.000
 I would say in these main machine learning,

1:08:55.000 --> 1:08:59.000
 in these main artificial intelligence conferences

1:08:59.000 --> 1:09:01.200
 amongst the researchers, I don't know

1:09:01.200 --> 1:09:03.880
 if it's an accepted term yet.

1:09:03.880 --> 1:09:07.960
 What I've seen bravely, you mentioned Shane Leg,

1:09:07.960 --> 1:09:13.000
 is DeepMind and then OpenAI are the two places that are,

1:09:13.000 --> 1:09:15.600
 I would say, unapologetically so far.

1:09:15.600 --> 1:09:17.440
 I think it's actually changing, unfortunately.

1:09:17.440 --> 1:09:19.640
 But so far, they've been pushing the idea

1:09:19.640 --> 1:09:22.760
 that the goal is to create an AGI.

1:09:22.760 --> 1:09:24.840
 Well, they have billions of dollars behind them.

1:09:24.840 --> 1:09:29.800
 And in the public mind, that certainly carries some oomph,

1:09:29.800 --> 1:09:30.800
 right?

1:09:30.800 --> 1:09:33.040
 But they also have really strong researchers, right?

1:09:33.040 --> 1:09:33.540
 They do.

1:09:33.540 --> 1:09:34.560
 They're great teams.

1:09:34.560 --> 1:09:36.680
 DeepMind in particular.

1:09:36.680 --> 1:09:40.160
 And DeepMind has Marcus Hodder walking around.

1:09:40.160 --> 1:09:43.520
 I mean, there's all these folks who, basically,

1:09:43.520 --> 1:09:47.800
 their full time position involves dreaming about creating AGI.

1:09:47.800 --> 1:09:52.840
 I mean, Google Brain has a lot of amazing AGI oriented people

1:09:52.840 --> 1:09:59.440
 also, and I mean, so I'd say from a public marketing

1:09:59.440 --> 1:10:03.800
 view, DeepMind and OpenAI are the two large, well funded

1:10:03.800 --> 1:10:08.840
 organizations that have put the term and concept AGI out there

1:10:08.840 --> 1:10:12.720
 sort of as part of their public image.

1:10:12.720 --> 1:10:16.160
 But I mean, there are other groups

1:10:16.160 --> 1:10:20.640
 that are doing research that seems just as AGI as to me.

1:10:20.640 --> 1:10:24.320
 I mean, including a bunch of groups in Google's main mountain

1:10:24.320 --> 1:10:26.000
 view office.

1:10:26.000 --> 1:10:27.960
 So yeah, it's true.

1:10:27.960 --> 1:10:33.840
 AGI is somewhat away from the mainstream now.

1:10:33.840 --> 1:10:39.920
 But if you compare to where it was 15 years ago,

1:10:39.920 --> 1:10:41.960
 there's been an amazing mainstreaming.

1:10:41.960 --> 1:10:45.520
 You could say the same thing about super longevity research,

1:10:45.520 --> 1:10:49.080
 which is one of my application areas that I'm excited about.

1:10:49.080 --> 1:10:52.840
 I mean, I've been talking about this since the 90s,

1:10:52.840 --> 1:10:54.520
 but working on this since 2001.

1:10:54.520 --> 1:10:57.440
 And back then, really, to say you're

1:10:57.440 --> 1:10:59.400
 trying to create therapies to allow people

1:10:59.400 --> 1:11:02.360
 to live hundreds or thousands of years,

1:11:02.360 --> 1:11:05.480
 you were way, way, way, way out of the industry

1:11:05.480 --> 1:11:06.640
 academic mainstream.

1:11:06.640 --> 1:11:12.040
 But now Google had Project Calico, Craig Venture,

1:11:12.040 --> 1:11:14.000
 at Human Longevity Incorporated.

1:11:14.000 --> 1:11:17.080
 And then once the suits come marching in, right?

1:11:17.080 --> 1:11:20.200
 I mean, once there's big money in it,

1:11:20.200 --> 1:11:22.680
 then people are forced to take it seriously,

1:11:22.680 --> 1:11:24.840
 because that's the way modern society works.

1:11:24.840 --> 1:11:28.400
 So it's still not as mainstream as cancer research,

1:11:28.400 --> 1:11:32.080
 just as AGI is not as mainstream as automated driving

1:11:32.080 --> 1:11:32.920
 or something.

1:11:32.920 --> 1:11:34.840
 But the degree of mainstreaming that's

1:11:34.840 --> 1:11:40.440
 happened in the last 10 to 15 years is astounding to those

1:11:40.440 --> 1:11:42.080
 of us who've been at it for a while.

1:11:42.080 --> 1:11:45.320
 Yeah, but there's a marketing aspect to the term.

1:11:45.320 --> 1:11:48.960
 But in terms of actual full force research that's

1:11:48.960 --> 1:11:53.560
 going on under the header of AGI, it's currently, I would say,

1:11:53.560 --> 1:11:54.280
 dominated.

1:11:54.280 --> 1:11:57.200
 Maybe you can disagree, dominated by neural networks

1:11:57.200 --> 1:12:02.920
 research, that the nonlinear regression, as you mentioned.

1:12:02.920 --> 1:12:08.920
 What's your sense with OpenCog, with your work in general?

1:12:08.920 --> 1:12:11.960
 I was a logic based systems and expert systems.

1:12:11.960 --> 1:12:17.240
 For me, it always seemed to capture

1:12:17.240 --> 1:12:21.560
 a deep element of intelligence that needs to be there.

1:12:21.560 --> 1:12:22.960
 Like you said, it needs to learn,

1:12:22.960 --> 1:12:24.840
 it needs to be automated somehow.

1:12:24.840 --> 1:12:31.360
 But that seems to be missing from a lot of research currently.

1:12:31.360 --> 1:12:34.360
 So what's your sense?

1:12:34.360 --> 1:12:36.280
 I guess one way to ask this question,

1:12:36.280 --> 1:12:40.560
 what's your sense of what kind of things will an AGI system

1:12:40.560 --> 1:12:43.440
 need to have?

1:12:43.440 --> 1:12:45.920
 Yeah, that's a very interesting topic

1:12:45.920 --> 1:12:47.880
 that I thought about for a long time.

1:12:47.880 --> 1:12:53.800
 And I think there are many, many different approaches

1:12:53.800 --> 1:12:56.880
 that can work for getting to human level AI.

1:12:56.880 --> 1:13:03.760
 So I don't think there's one golden algorithm, one golden

1:13:03.760 --> 1:13:05.800
 design that can work.

1:13:05.800 --> 1:13:11.320
 And I mean, flying machines is the much more analogy here,

1:13:11.320 --> 1:13:11.840
 right?

1:13:11.840 --> 1:13:13.720
 I mean, you have airplanes, you have helicopters,

1:13:13.720 --> 1:13:17.080
 you have balloons, you have stealth bombers

1:13:17.080 --> 1:13:18.720
 that don't look like regular airplanes.

1:13:18.720 --> 1:13:20.960
 You've got all blimps.

1:13:20.960 --> 1:13:21.800
 Birds, too.

1:13:21.800 --> 1:13:24.200
 Birds, yeah, and bugs, right?

1:13:24.200 --> 1:13:24.920
 Yeah.

1:13:24.920 --> 1:13:29.840
 And I mean, there are certainly many kinds of flying machines.

1:13:29.840 --> 1:13:33.040
 And there's a catapult that you can just launch.

1:13:33.040 --> 1:13:36.080
 There's bicycle powered flying machines, right?

1:13:36.080 --> 1:13:36.840
 Nice, yeah.

1:13:36.840 --> 1:13:40.760
 Yeah, so now these are all analogs

1:13:40.760 --> 1:13:43.760
 will by a basic theory of aerodynamics, right?

1:13:43.760 --> 1:13:47.440
 Now, so one issue with AGI is we don't yet

1:13:47.440 --> 1:13:50.760
 have the analog of the theory of aerodynamics.

1:13:50.760 --> 1:13:54.200
 And that's what Marcus Hoeder was trying

1:13:54.200 --> 1:13:57.000
 to make with the AXE and his general theory

1:13:57.000 --> 1:13:58.720
 of general intelligence.

1:13:58.720 --> 1:14:03.320
 But that theory in its most clearly articulated parts

1:14:03.320 --> 1:14:07.120
 really only works for either infinitely powerful machines

1:14:07.120 --> 1:14:11.880
 or almost or insanely impractically powerful machines.

1:14:11.880 --> 1:14:14.880
 So I mean, if you were going to take a theory based approach

1:14:14.880 --> 1:14:18.680
 to AGI, what you would do is say, well, let's

1:14:18.680 --> 1:14:24.760
 take what's called, say, AXE TL, which is Hoeder's AXE

1:14:24.760 --> 1:14:28.640
 machine that can work on merely insanely much processing

1:14:28.640 --> 1:14:30.200
 power rather than infinitely much processing.

1:14:30.200 --> 1:14:32.240
 What does TL stand for?

1:14:32.240 --> 1:14:33.840
 Time and length.

1:14:33.840 --> 1:14:42.400
 So you're basically how AXE works basically is each action

1:14:42.400 --> 1:14:45.000
 that it wants to take before taking that action,

1:14:45.000 --> 1:14:47.080
 it looks at all its history.

1:14:47.080 --> 1:14:49.840
 And then it looks at all possible programs

1:14:49.840 --> 1:14:51.800
 that it could use to make a decision.

1:14:51.800 --> 1:14:54.800
 And it decides which decision program would have let

1:14:54.800 --> 1:14:56.960
 it make the best decisions according to its reward

1:14:56.960 --> 1:14:58.520
 function over its history.

1:14:58.520 --> 1:14:59.960
 And it uses that decision program

1:14:59.960 --> 1:15:02.080
 to make the next decision.

1:15:02.080 --> 1:15:04.720
 It's not afraid of infinite resources.

1:15:04.720 --> 1:15:07.360
 It's searching through the space of all possible computer

1:15:07.360 --> 1:15:10.680
 programs in between each action and each next action.

1:15:10.680 --> 1:15:15.280
 Now, AXE TL searches through all possible computer programs

1:15:15.280 --> 1:15:19.640
 that have runtime less than T and length less than L,

1:15:19.640 --> 1:15:22.880
 which is still an impractically humongous space.

1:15:22.880 --> 1:15:28.000
 So what you would like to do to make an AGI

1:15:28.000 --> 1:15:29.880
 and what will probably be done 50 years from now

1:15:29.880 --> 1:15:34.880
 to make an AGI is say, OK, well, we have some constraints.

1:15:34.880 --> 1:15:37.520
 We have these processing power constraints.

1:15:37.520 --> 1:15:42.720
 And we have space and time constraints on the program.

1:15:42.720 --> 1:15:45.400
 We have energy utilization constraints.

1:15:45.400 --> 1:15:49.200
 And we have this particular class of environments

1:15:49.200 --> 1:15:52.720
 that we care about, which may be, say,

1:15:52.720 --> 1:15:55.440
 manipulating physical objects on the surface of the earth,

1:15:55.440 --> 1:15:58.520
 communicating in human language, whatever

1:15:58.520 --> 1:16:02.240
 our particular, not annihilating humanity,

1:16:02.240 --> 1:16:05.480
 whatever our particular requirements happen to be.

1:16:05.480 --> 1:16:07.280
 If you formalize those requirements

1:16:07.280 --> 1:16:10.320
 in some formal specification language,

1:16:10.320 --> 1:16:14.400
 you should then be able to run automated program

1:16:14.400 --> 1:16:17.920
 specializer on AXE TL, specialize it

1:16:17.920 --> 1:16:21.440
 to the computing resource constraints

1:16:21.440 --> 1:16:23.760
 and the particular environment and goal.

1:16:23.760 --> 1:16:29.240
 And then it will spit out the specialized version of AXE TL

1:16:29.240 --> 1:16:31.600
 to your resource restrictions in your environment, which

1:16:31.600 --> 1:16:32.720
 will be your AGI.

1:16:32.720 --> 1:16:36.360
 And that, I think, is how our super AGI will

1:16:36.360 --> 1:16:39.200
 create new AGI systems.

1:16:39.200 --> 1:16:40.520
 But that's a very rush.

1:16:40.520 --> 1:16:41.600
 It just seems really inefficient.

1:16:41.600 --> 1:16:43.320
 That's a very Russian approach, by the way.

1:16:43.320 --> 1:16:45.240
 The whole field of program specialization

1:16:45.240 --> 1:16:47.240
 came out of Russia.

1:16:47.240 --> 1:16:48.040
 Can you backtrack?

1:16:48.040 --> 1:16:49.680
 So what is program specialization?

1:16:49.680 --> 1:16:51.120
 So it's basically.

1:16:51.120 --> 1:16:53.760
 Well, take sorting, for example.

1:16:53.760 --> 1:16:56.640
 You can have a generic program for sorting lists.

1:16:56.640 --> 1:16:58.720
 But what if all your lists, you care about a length,

1:16:58.720 --> 1:16:59.920
 10,000 or less?

1:16:59.920 --> 1:17:00.320
 Got it.

1:17:00.320 --> 1:17:02.560
 You can run an automated program specializer

1:17:02.560 --> 1:17:03.960
 on your sorting algorithm.

1:17:03.960 --> 1:17:05.360
 And it will come up with the algorithm

1:17:05.360 --> 1:17:08.400
 that's optimal for sorting lists of length 1,000 or less,

1:17:08.400 --> 1:17:09.800
 or 10,000 or less, right?

1:17:09.800 --> 1:17:12.200
 It's kind of like, isn't that the kind of the process

1:17:12.200 --> 1:17:17.480
 of evolution is a program specializer to the environment?

1:17:17.480 --> 1:17:20.560
 So you're kind of evolving human beings or living?

1:17:20.560 --> 1:17:21.280
 Oh, exactly.

1:17:21.280 --> 1:17:24.400
 I mean, your Russian heritage is showing that.

1:17:24.400 --> 1:17:28.520
 So with Alexander Vityaev and Peter Anokin and so on.

1:17:28.520 --> 1:17:35.200
 I mean, there's a long history of thinking about evolution

1:17:35.200 --> 1:17:36.760
 that way also, right?

1:17:36.760 --> 1:17:40.160
 So my point is that what we're thinking of

1:17:40.160 --> 1:17:44.200
 is a human level general intelligence.

1:17:44.200 --> 1:17:48.120
 If you start from narrow AIs, like are being used

1:17:48.120 --> 1:17:50.540
 in the commercial AI field now, then

1:17:50.540 --> 1:17:53.400
 you're thinking, OK, how do we make it more and more general?

1:17:53.400 --> 1:17:57.160
 On the other hand, if you start from AXI or Schmidhuber's

1:17:57.160 --> 1:18:02.600
 Gertl machine or these infinitely powerful but practically

1:18:02.600 --> 1:18:06.440
 infeasible AIs, then getting to a human level AGI

1:18:06.440 --> 1:18:08.240
 is a matter of specialization.

1:18:08.240 --> 1:18:10.880
 It's like, how do you take these maximally

1:18:10.880 --> 1:18:12.920
 general learning processes?

1:18:12.920 --> 1:18:16.920
 And how do you specialize them so that they

1:18:16.920 --> 1:18:20.560
 can operate within the resource constraints that you have,

1:18:20.560 --> 1:18:24.320
 but will achieve the particular things that you care about?

1:18:24.320 --> 1:18:28.200
 Because we humans are not maximally general intelligence.

1:18:28.200 --> 1:18:31.440
 If I ask you to run a maze in 750 dimensions,

1:18:31.440 --> 1:18:33.080
 you'll probably be very slow.

1:18:33.080 --> 1:18:37.120
 Whereas at two dimensions, you're probably way better.

1:18:37.120 --> 1:18:40.840
 So I mean, because our hippocampus

1:18:40.840 --> 1:18:43.080
 has a two dimensional map in it, right?

1:18:43.080 --> 1:18:46.720
 And it does not have a 750 dimensional map in it.

1:18:46.720 --> 1:18:55.840
 I mean, we are a peculiar mix of generality and specialization.

1:18:55.840 --> 1:18:59.200
 We'll probably start quite general at birth.

1:18:59.200 --> 1:19:02.200
 Not obviously still narrow, but more general than we

1:19:02.200 --> 1:19:07.480
 are at age 20, and 30, and 40, and 50, and 60.

1:19:07.480 --> 1:19:08.560
 I don't think that.

1:19:08.560 --> 1:19:10.200
 I think it's more complex than that.

1:19:10.200 --> 1:19:17.520
 Because in some sense, a young child is less biased,

1:19:17.520 --> 1:19:20.000
 and the brain has yet to crystallize

1:19:20.000 --> 1:19:23.040
 into appropriate structures for processing

1:19:23.040 --> 1:19:25.400
 aspects of the physical and social world.

1:19:25.400 --> 1:19:28.560
 On the other hand, a young child is very tied

1:19:28.560 --> 1:19:31.360
 to their sensorium, whereas we can

1:19:31.360 --> 1:19:35.520
 deal with abstract mathematics, like 750 dimensions.

1:19:35.520 --> 1:19:38.840
 And the young child cannot, because they

1:19:38.840 --> 1:19:44.000
 haven't grown what Piaget called the formal capabilities.

1:19:44.000 --> 1:19:46.280
 They haven't learned to abstract yet, right?

1:19:46.280 --> 1:19:48.440
 And the ability to abstract gives you

1:19:48.440 --> 1:19:51.680
 a different kind of generality than what a baby has.

1:19:51.680 --> 1:19:55.400
 So there's both more specialization

1:19:55.400 --> 1:19:58.240
 and more generalization that comes with the development

1:19:58.240 --> 1:19:59.760
 process, actually.

1:19:59.760 --> 1:20:03.880
 I mean, I guess just the trajectories of the specialization

1:20:03.880 --> 1:20:08.640
 are most controllable at the young age, I guess,

1:20:08.640 --> 1:20:09.680
 is one way to put it.

1:20:09.680 --> 1:20:10.680
 Do you have kids?

1:20:10.680 --> 1:20:11.640
 No.

1:20:11.640 --> 1:20:13.560
 They're not as controllable as you think.

1:20:13.560 --> 1:20:15.800
 So you think it's interesting.

1:20:15.800 --> 1:20:19.000
 I think, honestly, I think a human adult

1:20:19.000 --> 1:20:23.200
 is much more generally intelligent than a human baby.

1:20:23.200 --> 1:20:25.800
 Babies are very stupid.

1:20:25.800 --> 1:20:27.240
 I mean, they're cute.

1:20:27.240 --> 1:20:29.440
 They're cute, which is why we put up

1:20:29.440 --> 1:20:33.000
 with their repetiveness and stupidity.

1:20:33.000 --> 1:20:34.760
 And they have what the Zen guys would

1:20:34.760 --> 1:20:38.120
 call a beginner's mind, which is a beautiful thing.

1:20:38.120 --> 1:20:40.760
 But that doesn't necessarily correlate

1:20:40.760 --> 1:20:43.280
 with a high level of intelligence.

1:20:43.280 --> 1:20:46.800
 On the plot of cuteness and stupidity,

1:20:46.800 --> 1:20:48.680
 there's a process that allows us to put up

1:20:48.680 --> 1:20:50.640
 with their stupidity as they become more intelligent.

1:20:50.640 --> 1:20:52.400
 So by the time you're an ugly old man like me,

1:20:52.400 --> 1:20:54.520
 you've got to get really, really smart to compensate.

1:20:54.520 --> 1:20:55.360
 To compensate.

1:20:55.360 --> 1:20:56.120
 OK, cool.

1:20:56.120 --> 1:20:59.120
 But yeah, going back to your original question,

1:20:59.120 --> 1:21:05.720
 so the way I look at human level AGI is, yeah,

1:21:05.720 --> 1:21:09.960
 how do you specialize unrealistically

1:21:09.960 --> 1:21:14.600
 inefficient superhuman brute force learning processes

1:21:14.600 --> 1:21:18.320
 to the specific goals that humans need to achieve

1:21:18.320 --> 1:21:21.920
 and the specific resources that we have?

1:21:21.920 --> 1:21:24.560
 And both of these, the goals and the resources

1:21:24.560 --> 1:21:27.120
 and the environments, I mean, all this is important.

1:21:27.120 --> 1:21:31.280
 And on the resources side, it's important

1:21:31.280 --> 1:21:35.600
 that the hardware resources we're bringing to bear

1:21:35.600 --> 1:21:38.240
 are very different than the human brain.

1:21:38.240 --> 1:21:45.320
 So the way I would want to implement AGI on a bunch of neurons

1:21:45.320 --> 1:21:48.040
 in a vat that I could rewire arbitrarily

1:21:48.040 --> 1:21:49.480
 is quite different than the way I

1:21:49.480 --> 1:21:53.480
 would want to create AGI on, say, a modern server

1:21:53.480 --> 1:21:57.440
 form of CPUs and GPUs, which in turn may be quite different

1:21:57.440 --> 1:22:01.320
 than the way I would want to implement AGI on whatever

1:22:01.320 --> 1:22:03.760
 quantum computer we'll have in 10 years,

1:22:03.760 --> 1:22:06.720
 supposing someone makes a robust quantum turing machine

1:22:06.720 --> 1:22:08.240
 or something, right?

1:22:08.240 --> 1:22:12.680
 So I think there's been co evolution

1:22:12.680 --> 1:22:16.960
 of the patterns of organization in the human brain

1:22:16.960 --> 1:22:21.880
 and the physiological particulars of the human brain

1:22:21.880 --> 1:22:23.240
 over time.

1:22:23.240 --> 1:22:25.240
 And when you look at neural networks,

1:22:25.240 --> 1:22:28.000
 that is one powerful class of learning algorithms.

1:22:28.000 --> 1:22:30.040
 But it's also a class of learning algorithms

1:22:30.040 --> 1:22:33.440
 that evolve to exploit the particulars of the human brain

1:22:33.440 --> 1:22:36.320
 as a computational substrate.

1:22:36.320 --> 1:22:38.880
 If you're looking at the computational substrate

1:22:38.880 --> 1:22:41.960
 of a modern server farm, you won't necessarily

1:22:41.960 --> 1:22:45.760
 want the same algorithms that you want on the human brain.

1:22:45.760 --> 1:22:48.920
 And from the right level of abstraction,

1:22:48.920 --> 1:22:51.760
 you could look at maybe the best algorithms on the brain

1:22:51.760 --> 1:22:54.480
 and the best algorithms on a modern computer network

1:22:54.480 --> 1:22:56.480
 as implementing the same abstract learning

1:22:56.480 --> 1:22:59.040
 and representation processes.

1:22:59.040 --> 1:23:01.680
 But finding that level of abstraction

1:23:01.680 --> 1:23:04.960
 is its own AGI research project then, right?

1:23:04.960 --> 1:23:09.080
 So that's about the hardware side and the software

1:23:09.080 --> 1:23:10.880
 side, which follows from that.

1:23:10.880 --> 1:23:14.200
 Then regarding one of the requirements,

1:23:14.200 --> 1:23:16.720
 I wrote the paper years ago on what

1:23:16.720 --> 1:23:19.560
 I called the embodied communication

1:23:19.560 --> 1:23:24.320
 prior, which was quite similar in intent to Yoshua Benjio's

1:23:24.320 --> 1:23:26.800
 recent paper on the consciousness prior,

1:23:26.800 --> 1:23:30.400
 except I didn't want to wrap up consciousness in it,

1:23:30.400 --> 1:23:32.120
 because to me, the quell your problem

1:23:32.120 --> 1:23:35.920
 and subjective experience is a very interesting issue also,

1:23:35.920 --> 1:23:37.880
 which we can chat about.

1:23:37.880 --> 1:23:43.200
 But I would rather keep that philosophical debate distinct

1:23:43.200 --> 1:23:45.240
 from the debate of what kind of biases

1:23:45.240 --> 1:23:47.080
 do you want to put in the general intelligence

1:23:47.080 --> 1:23:49.800
 to give it human like general intelligence.

1:23:49.800 --> 1:23:52.040
 And I'm not sure Yoshua Benjio is really

1:23:52.040 --> 1:23:55.080
 addressing that kind of consciousness.

1:23:55.080 --> 1:23:56.280
 He's just using the term.

1:23:56.280 --> 1:23:59.720
 I love Yoshua to pieces like he's by far

1:23:59.720 --> 1:24:03.160
 my favorite of the lines of deep learning.

1:24:03.160 --> 1:24:05.760
 But he's such a good, hearted guy

1:24:05.760 --> 1:24:06.960
 and a great thinker.

1:24:06.960 --> 1:24:07.600
 Yeah, for sure.

1:24:07.600 --> 1:24:11.160
 I am not sure he has plumbed to the depths

1:24:11.160 --> 1:24:13.480
 of the philosophy of consciousness.

1:24:13.480 --> 1:24:14.920
 No, he's using it as a sexy time.

1:24:14.920 --> 1:24:15.880
 Yeah, yeah, yeah.

1:24:15.880 --> 1:24:21.120
 So when I called it was the embodied communication prior.

1:24:21.120 --> 1:24:22.520
 Can you maybe explain it a little bit?

1:24:22.520 --> 1:24:23.120
 Yeah, yeah.

1:24:23.120 --> 1:24:26.640
 What I meant was, what are we humans of all for?

1:24:26.640 --> 1:24:29.680
 You can say being human, but that's very abstract, right?

1:24:29.680 --> 1:24:32.960
 I mean, our minds control individual bodies,

1:24:32.960 --> 1:24:35.840
 which are autonomous agents, moving around

1:24:35.840 --> 1:24:41.280
 in a world that's composed largely of solid objects, right?

1:24:41.280 --> 1:24:46.240
 And we've also evolved to communicate via language

1:24:46.240 --> 1:24:49.360
 with other solid object agents that

1:24:49.360 --> 1:24:51.200
 are going around doing things collectively

1:24:51.200 --> 1:24:54.400
 with us in a world of solid objects.

1:24:54.400 --> 1:24:56.880
 And these things are very obvious.

1:24:56.880 --> 1:24:58.400
 But if you compare them to the scope

1:24:58.400 --> 1:25:01.800
 of all possible intelligences, or even

1:25:01.800 --> 1:25:03.960
 all possible intelligences that are physically

1:25:03.960 --> 1:25:07.440
 realizable, that actually constrains things a lot.

1:25:07.440 --> 1:25:11.600
 So if you start to look at how would you

1:25:11.600 --> 1:25:15.280
 realize some specialized or constrained

1:25:15.280 --> 1:25:18.400
 version of universal general intelligence

1:25:18.400 --> 1:25:22.000
 in a system that has limited memory and limited speed

1:25:22.000 --> 1:25:25.400
 of processing, but whose general intelligence will

1:25:25.400 --> 1:25:29.040
 be biased toward controlling a solid object agent, which

1:25:29.040 --> 1:25:31.320
 is mobile in a solid object world,

1:25:31.320 --> 1:25:36.520
 for manipulating solid objects and communicating via language

1:25:36.520 --> 1:25:39.880
 with other similar agents in that same world, right?

1:25:39.880 --> 1:25:41.960
 Then starting from that, you're starting

1:25:41.960 --> 1:25:46.720
 to get a requirements analysis for human level

1:25:46.720 --> 1:25:48.080
 general intelligence.

1:25:48.080 --> 1:25:50.880
 And then that leads you into cognitive science.

1:25:50.880 --> 1:25:53.040
 And you can look at, say, what are the different types

1:25:53.040 --> 1:25:56.920
 of memory that the human mind and brain has?

1:25:56.920 --> 1:26:00.800
 And this has matured over the last decades.

1:26:00.800 --> 1:26:02.880
 And I got into this a lot.

1:26:02.880 --> 1:26:04.640
 So after getting my PhD in math, I

1:26:04.640 --> 1:26:06.040
 was an academic for eight years.

1:26:06.040 --> 1:26:09.120
 I was in departments of mathematics, computer

1:26:09.120 --> 1:26:11.120
 science, and psychology.

1:26:11.120 --> 1:26:12.760
 When I was in the psychology department,

1:26:12.760 --> 1:26:14.240
 University of Western Australia, I

1:26:14.240 --> 1:26:18.680
 was focused on cognitive science of memory and perception.

1:26:18.680 --> 1:26:21.280
 Actually, I was teaching neural nets and deep neural nets.

1:26:21.280 --> 1:26:23.560
 And it was multilayer perceptrons, right?

1:26:23.560 --> 1:26:24.560
 Psychology?

1:26:24.560 --> 1:26:25.720
 Yeah.

1:26:25.720 --> 1:26:26.480
 Cognitive science.

1:26:26.480 --> 1:26:28.960
 It was crossdisclerary among engineering, math,

1:26:28.960 --> 1:26:33.280
 psychology, philosophy, linguistics, computer science.

1:26:33.280 --> 1:26:35.960
 But yeah, we were teaching psychology students

1:26:35.960 --> 1:26:39.360
 to try to model the data from human cognition

1:26:39.360 --> 1:26:42.280
 experiments using multilayer perceptrons, which

1:26:42.280 --> 1:26:45.040
 was the early version of a deep neural network.

1:26:45.040 --> 1:26:49.200
 Very, very, recurrent backprop was very, very slow

1:26:49.200 --> 1:26:51.200
 to train back then, right?

1:26:51.200 --> 1:26:53.920
 So this is the study of these constraint systems

1:26:53.920 --> 1:26:55.760
 that are supposed to deal with physical objects.

1:26:55.760 --> 1:27:01.480
 So if you look at cognitive psychology,

1:27:01.480 --> 1:27:04.720
 you can see there's multiple types of memory, which

1:27:04.720 --> 1:27:07.680
 are to some extent represented by different subsystems

1:27:07.680 --> 1:27:08.480
 in the human brain.

1:27:08.480 --> 1:27:11.480
 So we have episodic memory, which shakes into account

1:27:11.480 --> 1:27:15.240
 our life history and everything that's happened to us.

1:27:15.240 --> 1:27:17.280
 We have declarative or semantic memory,

1:27:17.280 --> 1:27:20.040
 which is like facts and beliefs abstracted

1:27:20.040 --> 1:27:22.840
 from the particular situations that they occurred in.

1:27:22.840 --> 1:27:26.080
 There's sensory memory, which to some extent

1:27:26.080 --> 1:27:27.600
 is sense modality specific.

1:27:27.600 --> 1:27:33.360
 And then to some extent is unified across sense modalities.

1:27:33.360 --> 1:27:36.080
 There's procedural memory, memory of how to do stuff,

1:27:36.080 --> 1:27:38.120
 like how to swing the tennis racket, right?

1:27:38.120 --> 1:27:40.440
 Which is there's motor memory, but it's also

1:27:40.440 --> 1:27:43.640
 a little more abstract than motor memory.

1:27:43.640 --> 1:27:47.520
 It involves cerebellum and cortex working together.

1:27:47.520 --> 1:27:51.560
 And then there's memory linkage with emotion,

1:27:51.560 --> 1:27:55.920
 which has to do with linkages of cortex and limbic system.

1:27:55.920 --> 1:27:59.160
 There's specifics of spatial and temporal modeling

1:27:59.160 --> 1:28:02.760
 connected with memory, which has to do with hippocampus

1:28:02.760 --> 1:28:05.360
 and thalamus connecting to cortex.

1:28:05.360 --> 1:28:08.160
 And the basal ganglia, which influences goals.

1:28:08.160 --> 1:28:11.400
 So we have specific memory of what goals, subgoals,

1:28:11.400 --> 1:28:13.160
 and subsubgoals we wanted to perceive

1:28:13.160 --> 1:28:15.080
 in which context in the past.

1:28:15.080 --> 1:28:18.240
 Human brain has substantially different subsystems

1:28:18.240 --> 1:28:21.040
 for these different types of memory

1:28:21.040 --> 1:28:24.240
 and substantially differently tuned learning,

1:28:24.240 --> 1:28:27.280
 like differently tuned modes of long term potentiation

1:28:27.280 --> 1:28:29.720
 to do with the types of neurons and neurotransmitters

1:28:29.720 --> 1:28:31.280
 and the different parts of the brain

1:28:31.280 --> 1:28:33.040
 correspond to these different types of knowledge.

1:28:33.040 --> 1:28:35.880
 And these different types of memory and learning

1:28:35.880 --> 1:28:38.520
 in the human brain, I mean, you can back these all

1:28:38.520 --> 1:28:41.880
 into embodied communication for controlling agents

1:28:41.880 --> 1:28:44.680
 and worlds of solid objects.

1:28:44.680 --> 1:28:47.680
 So if you look at building an AGI system,

1:28:47.680 --> 1:28:50.400
 one way to do it, which starts more from cognitive science

1:28:50.400 --> 1:28:52.640
 than neuroscience is to say,

1:28:52.640 --> 1:28:55.200
 okay, what are the types of memory

1:28:55.200 --> 1:28:57.320
 that are necessary for this kind of world?

1:28:57.320 --> 1:29:00.680
 Yeah, yeah, necessary for this sort of intelligence.

1:29:00.680 --> 1:29:02.720
 What types of learning work well

1:29:02.720 --> 1:29:04.560
 with these different types of memory?

1:29:04.560 --> 1:29:07.760
 And then how do you connect all these things together, right?

1:29:07.760 --> 1:29:10.760
 And of course the human brain did it incrementally

1:29:10.760 --> 1:29:14.320
 through evolution because each of the subnetworks

1:29:14.320 --> 1:29:16.640
 of the brain, and when it's not really the lobes

1:29:16.640 --> 1:29:18.160
 of the brain, it's the subnetworks,

1:29:18.160 --> 1:29:20.760
 each of which is widely distributed,

1:29:20.760 --> 1:29:23.640
 which of each of the subnetworks of the brain

1:29:23.640 --> 1:29:27.120
 co evolved with the other subnetworks of the brain,

1:29:27.120 --> 1:29:29.440
 both in terms of its patterns of organization

1:29:29.440 --> 1:29:31.800
 and the particulars of the neurophysiology.

1:29:31.800 --> 1:29:34.400
 So they all grew up communicating and adapting to each other.

1:29:34.400 --> 1:29:36.720
 It's not like they were separate black boxes

1:29:36.720 --> 1:29:40.160
 that were then glommed together, right?

1:29:40.160 --> 1:29:43.320
 Whereas as engineers, we would tend to say,

1:29:43.320 --> 1:29:46.680
 let's make the declarative memory box here

1:29:46.680 --> 1:29:48.440
 and the procedural memory box here

1:29:48.440 --> 1:29:51.400
 and the perception box here and wire them together.

1:29:51.400 --> 1:29:54.120
 And when you can do that, it's interesting.

1:29:54.120 --> 1:29:55.680
 I mean, that's how a car is built, right?

1:29:55.680 --> 1:29:57.760
 But on the other hand,

1:29:57.760 --> 1:30:01.400
 that's clearly not how biological systems are made.

1:30:01.400 --> 1:30:04.880
 The parts co evolved so as to adapt and work together.

1:30:04.880 --> 1:30:05.720
 So this...

1:30:05.720 --> 1:30:09.240
 That's by the way how every human engineered system

1:30:09.240 --> 1:30:11.640
 that flies that were using that analogy

1:30:11.640 --> 1:30:12.960
 before it's built as well.

1:30:12.960 --> 1:30:14.400
 So do you find this at all appealing?

1:30:14.400 --> 1:30:16.640
 Like, there's been a lot of really exciting,

1:30:16.640 --> 1:30:19.760
 which I find strange that it's ignored

1:30:19.760 --> 1:30:21.840
 work in cognitive architectures, for example,

1:30:21.840 --> 1:30:23.280
 throughout the last few decades.

1:30:23.280 --> 1:30:24.280
 Do you find that...

1:30:24.280 --> 1:30:27.920
 Yeah, I mean, I had a lot to do with that community.

1:30:27.920 --> 1:30:31.000
 And Paul Rosenblum, who was one of the...

1:30:31.000 --> 1:30:33.440
 And John Laird who built the SOAR architecture

1:30:33.440 --> 1:30:34.600
 are friends of mine.

1:30:34.600 --> 1:30:37.760
 And I learned SOAR quite well in ACTR

1:30:37.760 --> 1:30:39.400
 and these different cognitive architectures.

1:30:39.400 --> 1:30:44.400
 How I was looking in the AI world about 10 years ago

1:30:44.480 --> 1:30:47.800
 before this whole commercial deep learning explosion was,

1:30:47.800 --> 1:30:51.520
 on the one hand, you had these cognitive architecture guys

1:30:51.520 --> 1:30:53.400
 who were working closely with psychologists

1:30:53.400 --> 1:30:55.920
 and cognitive scientists who had thought a lot about

1:30:55.920 --> 1:30:58.520
 how the different parts of a human like mine

1:30:58.520 --> 1:31:00.320
 should work together.

1:31:00.320 --> 1:31:03.560
 On the other hand, you had these learning theory guys

1:31:03.560 --> 1:31:06.000
 who didn't care at all about the architecture,

1:31:06.000 --> 1:31:07.320
 but were just thinking about like,

1:31:07.320 --> 1:31:10.280
 how do you recognize patterns and large amounts of data?

1:31:10.280 --> 1:31:14.200
 And in some sense, what you needed to do

1:31:14.200 --> 1:31:18.440
 was to get the learning that the learning theory guys

1:31:18.440 --> 1:31:21.400
 were doing and put it together with the architecture

1:31:21.400 --> 1:31:24.240
 that the cognitive architecture guys were doing

1:31:24.240 --> 1:31:25.920
 and then you would have what you needed.

1:31:25.920 --> 1:31:30.760
 Now, unfortunately, when you look at the details,

1:31:31.600 --> 1:31:34.960
 you can't just do that without totally rebuilding

1:31:34.960 --> 1:31:37.840
 what is happening on both the cognitive architecture

1:31:37.840 --> 1:31:38.760
 and the learning side.

1:31:38.760 --> 1:31:41.760
 So, I mean, they tried to do that in SOAR,

1:31:41.760 --> 1:31:45.320
 but what they ultimately did is like taking deep neural net

1:31:45.320 --> 1:31:46.560
 or something for perception

1:31:46.560 --> 1:31:50.120
 and you include it as one of the black boxes.

1:31:50.120 --> 1:31:51.960
 Yeah, it becomes one of the boxes.

1:31:51.960 --> 1:31:53.800
 The learning mechanism becomes one of the boxes

1:31:53.800 --> 1:31:55.040
 as opposed to a fundamental.

1:31:55.040 --> 1:31:57.200
 Yeah, that doesn't quite work.

1:31:57.200 --> 1:32:00.400
 Now, you could look at some of the stuff deep mind has done,

1:32:00.400 --> 1:32:03.240
 like the differential neural computer or something.

1:32:03.240 --> 1:32:07.080
 That sort of has a neural net for deep learning perception.

1:32:07.080 --> 1:32:08.880
 It has another neural net,

1:32:08.880 --> 1:32:10.640
 which is like a memory matrix.

1:32:10.640 --> 1:32:13.080
 It stores, say, the map of the London subway or something.

1:32:13.080 --> 1:32:16.280
 So, probably Demis or Sabus were thinking about

1:32:16.280 --> 1:32:18.480
 as like part of Cortex and part of hippocampus

1:32:18.480 --> 1:32:20.400
 because hippocampus has a spatial map

1:32:20.400 --> 1:32:21.720
 and when he was a neuroscientist,

1:32:21.720 --> 1:32:24.560
 he was doing a bunch on Cortex hippocampus interconnection.

1:32:24.560 --> 1:32:27.280
 So there, the DNC would be an example of folks

1:32:27.280 --> 1:32:29.160
 from the deep neural net world

1:32:29.160 --> 1:32:32.200
 trying to take a step in the cognitive architecture direction

1:32:32.200 --> 1:32:35.000
 by having two neural modules that correspond roughly

1:32:35.000 --> 1:32:36.720
 to two different parts of the human brain

1:32:36.720 --> 1:32:38.920
 that deal with different kinds of memory and learning.

1:32:38.920 --> 1:32:39.880
 But on the other hand,

1:32:39.880 --> 1:32:42.000
 it's super, super, super crude

1:32:42.000 --> 1:32:44.280
 from the cognitive architecture view, right?

1:32:44.280 --> 1:32:48.080
 Just as what John Laird and Soar did with neural nets

1:32:48.080 --> 1:32:51.200
 was super, super crude from a learning point of view

1:32:51.200 --> 1:32:53.360
 because the learning was like off to the side

1:32:53.360 --> 1:32:55.880
 not affecting the core representations, right?

1:32:55.880 --> 1:32:57.880
 And when you weren't learning the representation,

1:32:57.880 --> 1:33:00.040
 you were learning the data that feeds into the rep.

1:33:00.040 --> 1:33:02.680
 You were learning abstractions of perceptual data

1:33:02.680 --> 1:33:06.640
 to feed into the representation that was not learned, right?

1:33:06.640 --> 1:33:11.080
 So yeah, this was clear to me a while ago

1:33:11.080 --> 1:33:14.320
 and one of my hopes with the AGI community

1:33:14.320 --> 1:33:16.080
 was to sort of bring people

1:33:16.080 --> 1:33:18.560
 from those two directions together.

1:33:19.400 --> 1:33:22.000
 That didn't happen much in terms of...

1:33:22.000 --> 1:33:22.840
 Not yet.

1:33:22.840 --> 1:33:24.640
 Or what I was gonna say is it didn't happen

1:33:24.640 --> 1:33:26.440
 in terms of bringing like the lines

1:33:26.440 --> 1:33:28.640
 of cognitive architecture together

1:33:28.640 --> 1:33:30.560
 with the lines of deep learning.

1:33:30.560 --> 1:33:33.840
 It did work in the sense that a bunch of younger researchers

1:33:33.840 --> 1:33:35.840
 have had their heads filled with both of those ideas.

1:33:35.840 --> 1:33:38.920
 This comes back to a saying my dad

1:33:38.920 --> 1:33:41.440
 who was a university professor often quoted to me

1:33:41.440 --> 1:33:45.040
 which was a science advances one funeral at a time.

1:33:45.920 --> 1:33:47.920
 Which I'm trying to avoid.

1:33:47.920 --> 1:33:49.920
 Like I'm 53 years old

1:33:49.920 --> 1:33:53.560
 and I'm trying to invent amazing weird ass new things

1:33:53.560 --> 1:33:56.280
 that nobody ever thought about

1:33:56.280 --> 1:33:59.280
 which we'll talk about in a few minutes.

1:33:59.280 --> 1:34:02.320
 But there is that aspect, right?

1:34:02.320 --> 1:34:05.720
 Like the people who've been at AI a long time

1:34:05.720 --> 1:34:08.760
 and have made their career at developing one aspect

1:34:08.760 --> 1:34:12.920
 like a cognitive architecture or a deep learning approach.

1:34:12.920 --> 1:34:14.800
 It can be hard once you're old

1:34:14.800 --> 1:34:17.320
 and have made your career doing one thing.

1:34:17.320 --> 1:34:19.680
 It can be hard to mentally shift gears.

1:34:19.680 --> 1:34:23.640
 I mean, I try quite hard to remain flexible minded.

1:34:23.640 --> 1:34:26.520
 You've been successful somewhat in changing,

1:34:26.520 --> 1:34:29.680
 maybe have you changed your mind on some aspects

1:34:29.680 --> 1:34:31.520
 of what it takes to build an AGI?

1:34:31.520 --> 1:34:33.000
 Like tech technical things.

1:34:33.000 --> 1:34:35.840
 The hard part is that the world doesn't want you to.

1:34:35.840 --> 1:34:37.400
 The world or your own brain?

1:34:37.400 --> 1:34:39.600
 The world, well, that one point

1:34:39.600 --> 1:34:41.080
 is that your brain doesn't want to.

1:34:41.080 --> 1:34:43.560
 The other part is that the world doesn't want you to.

1:34:43.560 --> 1:34:46.560
 Like the people who have followed your ideas

1:34:46.560 --> 1:34:49.320
 get mad at you if you change your mind.

1:34:49.320 --> 1:34:52.760
 And the media wants to pigeonhole you

1:34:52.760 --> 1:34:57.120
 as an avatar of a certain idea.

1:34:57.120 --> 1:35:01.440
 But yeah, I've changed my mind on a bunch of things.

1:35:01.440 --> 1:35:03.800
 I mean, when I started my career,

1:35:03.800 --> 1:35:06.080
 I really thought quantum computing would be necessary

1:35:06.080 --> 1:35:10.760
 for AGI and I doubt it's necessary now,

1:35:10.760 --> 1:35:14.640
 although I think it will be a super major enhancement.

1:35:14.640 --> 1:35:18.320
 But I mean, I'm also, I'm now in the middle

1:35:18.320 --> 1:35:22.080
 of embarking on a complete rethink

1:35:22.080 --> 1:35:26.200
 and rewrite from scratch of our OpenCog AGI system,

1:35:26.200 --> 1:35:29.880
 together with Alexei Podopov and his team in St. Petersburg

1:35:29.880 --> 1:35:31.640
 who's working with me in SingularityNet.

1:35:31.640 --> 1:35:35.720
 So now we're trying to like go back to basics,

1:35:35.720 --> 1:35:37.840
 take everything we learned from working

1:35:37.840 --> 1:35:39.640
 with the current OpenCog system,

1:35:39.640 --> 1:35:41.920
 take everything everybody else has learned

1:35:41.920 --> 1:35:45.720
 from working with their Proto AGI systems

1:35:45.720 --> 1:35:50.080
 and design the best framework for the next stage.

1:35:50.080 --> 1:35:53.360
 And I do think there's a lot to be learned

1:35:53.360 --> 1:35:56.880
 from the recent successes with deep neural nets

1:35:56.880 --> 1:35:59.080
 and deep reinforcement systems.

1:35:59.080 --> 1:36:02.720
 I mean, people made these essentially trivial systems

1:36:02.720 --> 1:36:04.880
 work much better than I thought they would.

1:36:04.880 --> 1:36:07.120
 And there's a lot to be learned from that.

1:36:07.120 --> 1:36:10.760
 And I want to incorporate that knowledge appropriately

1:36:10.760 --> 1:36:13.560
 in our OpenCog 2.0 system.

1:36:13.560 --> 1:36:18.560
 On the other hand, I also think current deep neural net

1:36:18.560 --> 1:36:22.280
 architectures as such will never get you anywhere near AGI.

1:36:22.280 --> 1:36:25.120
 So I think you want to avoid the pathology

1:36:25.120 --> 1:36:28.400
 of throwing the baby out with the bathwater

1:36:28.400 --> 1:36:30.920
 and like saying, well, these things are garbage

1:36:30.920 --> 1:36:33.880
 because foolish journalists overblow them

1:36:33.880 --> 1:36:37.080
 as being the path to AGI

1:36:37.080 --> 1:36:40.800
 and a few researchers overblow them as well.

1:36:42.200 --> 1:36:45.440
 There's a lot of interesting stuff to be learned there

1:36:45.440 --> 1:36:48.040
 even though those are not the golden path.

1:36:48.040 --> 1:36:50.160
 So maybe this is a good chance to step back.

1:36:50.160 --> 1:36:52.960
 You mentioned OpenCog 2.0, but...

1:36:52.960 --> 1:36:56.120
 Go back to OpenCog 0.0, which exists now.

1:36:56.120 --> 1:36:58.440
 Alpha, yeah.

1:36:58.440 --> 1:37:01.920
 Yeah, maybe talk to the history of OpenCog

1:37:01.920 --> 1:37:03.960
 and you're thinking about these ideas.

1:37:03.960 --> 1:37:08.760
 I would say OpenCog 2.0 is a term

1:37:08.760 --> 1:37:11.440
 we're throwing around sort of tongue in cheek

1:37:11.440 --> 1:37:16.040
 because the existing OpenCog system that we're working on now

1:37:16.040 --> 1:37:18.040
 is not remotely close to what we'd consider,

1:37:18.040 --> 1:37:20.000
 we'd consider a 1.0, right?

1:37:20.000 --> 1:37:25.000
 I mean, it's been around what, 13 years or something,

1:37:27.400 --> 1:37:29.800
 but it's still an early stage research system, right?

1:37:29.800 --> 1:37:34.800
 And actually we're going back to the beginning

1:37:37.360 --> 1:37:40.680
 in terms of theory and implementation

1:37:40.680 --> 1:37:42.840
 because we feel like that's the right thing to do,

1:37:42.840 --> 1:37:44.320
 but I'm sure what we end up with

1:37:44.320 --> 1:37:46.760
 is going to have a huge amount in common

1:37:46.760 --> 1:37:48.560
 with the current system.

1:37:48.560 --> 1:37:51.280
 I mean, we all still like the general approach.

1:37:51.280 --> 1:37:52.120
 So there's...

1:37:52.120 --> 1:37:54.400
 So first of all, what is OpenCog?

1:37:54.400 --> 1:37:59.400
 Sure, OpenCog is an open source software project

1:37:59.760 --> 1:38:04.400
 that I launched together with several others in 2008

1:38:04.400 --> 1:38:08.280
 and probably the first code written toward that

1:38:08.280 --> 1:38:11.160
 was written in 2001 or two or something

1:38:11.160 --> 1:38:15.320
 that was developed as a proprietary code base

1:38:15.320 --> 1:38:18.280
 within my AI company, NovaMente LLC.

1:38:18.280 --> 1:38:22.000
 Then we decided to open source it in 2008,

1:38:22.000 --> 1:38:23.840
 cleaned up the code throughout some things,

1:38:23.840 --> 1:38:26.920
 added some new things and...

1:38:26.920 --> 1:38:28.240
 What language is it written in?

1:38:28.240 --> 1:38:29.440
 It's C++.

1:38:29.440 --> 1:38:31.400
 Primarily there's a bunch of scheme as well,

1:38:31.400 --> 1:38:33.040
 but most of it's C++.

1:38:33.040 --> 1:38:35.600
 And it's separate from that something

1:38:35.600 --> 1:38:37.480
 we'll also talk about is SingularityNet.

1:38:37.480 --> 1:38:41.320
 So it was born as a non networked thing.

1:38:41.320 --> 1:38:42.400
 Correct, correct.

1:38:42.400 --> 1:38:45.040
 Well, there are many levels of networks

1:38:45.040 --> 1:38:47.080
 involved here, right?

1:38:47.080 --> 1:38:49.400
 No connectivity to the internet.

1:38:49.400 --> 1:38:50.240
 Or no.

1:38:50.240 --> 1:38:52.400
 At birth.

1:38:52.400 --> 1:38:57.240
 Yeah, I mean, SingularityNet is a separate project

1:38:57.240 --> 1:38:59.400
 and a separate body of code.

1:38:59.400 --> 1:39:02.600
 And you can use SingularityNet as part of the infrastructure

1:39:02.600 --> 1:39:04.440
 for a distributed OpenCog system,

1:39:04.440 --> 1:39:07.520
 but there are different layers.

1:39:07.520 --> 1:39:08.360
 Yeah.

1:39:08.360 --> 1:39:09.200
 Got it.

1:39:09.200 --> 1:39:14.200
 So OpenCog on the one hand as a software framework

1:39:14.840 --> 1:39:17.000
 could be used to implement a variety

1:39:17.000 --> 1:39:21.840
 of different AI architectures and algorithms.

1:39:21.840 --> 1:39:26.440
 But in practice, there's been a group of developers

1:39:26.440 --> 1:39:27.920
 which I've been leading together

1:39:27.920 --> 1:39:31.680
 with Linus Vepstas, Neil Geisweiler and a few others

1:39:31.680 --> 1:39:35.080
 which have been using the OpenCog platform

1:39:35.080 --> 1:39:39.440
 and infrastructure to implement certain ideas

1:39:39.440 --> 1:39:41.280
 about how to make an AGI.

1:39:41.280 --> 1:39:43.480
 So there's been a little bit of ambiguity

1:39:43.480 --> 1:39:46.120
 about OpenCog, the software platform

1:39:46.120 --> 1:39:49.360
 versus OpenCog, the AGI design.

1:39:49.360 --> 1:39:51.800
 Because in theory, you could use that software

1:39:51.800 --> 1:39:53.440
 to do, you could use it to make a neural net.

1:39:53.440 --> 1:39:55.880
 You could use it to make a lot of different AGI.

1:39:55.880 --> 1:39:58.640
 What kind of stuff does the software platform provide?

1:39:58.640 --> 1:40:00.760
 Like in terms of utilities, those like what?

1:40:00.760 --> 1:40:03.880
 Yeah, let me first tell about OpenCog

1:40:03.880 --> 1:40:05.560
 as a software platform.

1:40:05.560 --> 1:40:08.720
 And then I'll tell you the specific AGI R&D

1:40:08.720 --> 1:40:10.760
 we've been building on top of it.

1:40:12.280 --> 1:40:16.200
 So the core component of OpenCog is a software platform

1:40:16.200 --> 1:40:17.960
 is what we call the Atom Space,

1:40:18.840 --> 1:40:21.280
 which is a weighted labeled hypergraph.

1:40:21.280 --> 1:40:22.920
 A T O M Atom Space.

1:40:22.920 --> 1:40:25.040
 Atom Space, yeah, yeah, not Atom,

1:40:25.040 --> 1:40:28.120
 like Adam and Eve, although that would be cool too.

1:40:28.120 --> 1:40:32.160
 Yeah, so you have a hypergraph, which is like,

1:40:32.160 --> 1:40:35.400
 so a graph in this sense is a bunch of nodes

1:40:35.400 --> 1:40:37.160
 with links between them.

1:40:37.160 --> 1:40:40.960
 A hypergraph is like a graph,

1:40:40.960 --> 1:40:44.000
 but links can go between more than two nodes.

1:40:44.000 --> 1:40:45.560
 So you have a link between three nodes.

1:40:45.560 --> 1:40:49.600
 And in fact, OpenCog's Atom Space

1:40:49.600 --> 1:40:51.800
 would properly be called a metagraph

1:40:51.800 --> 1:40:54.120
 because you can have links pointing to links

1:40:54.120 --> 1:40:56.880
 or you could have links pointing to whole subgraphs, right?

1:40:56.880 --> 1:41:00.960
 So it's an extended hypergraph or a metagraph.

1:41:00.960 --> 1:41:02.320
 Is metagraph a technical term?

1:41:02.320 --> 1:41:03.680
 It is now a technical term.

1:41:03.680 --> 1:41:04.520
 Interesting.

1:41:04.520 --> 1:41:06.400
 But I don't think it was yet a technical term

1:41:06.400 --> 1:41:10.120
 when we started calling this a generalized hypergraph.

1:41:10.120 --> 1:41:13.400
 But in any case, it's a weighted labeled

1:41:13.400 --> 1:41:16.960
 generalized hypergraph or weighted labeled metagraph.

1:41:16.960 --> 1:41:19.240
 The weights and labels mean that the nodes and links

1:41:19.240 --> 1:41:22.400
 can have numbers and symbols attached to them.

1:41:22.400 --> 1:41:24.960
 So they can have types on them.

1:41:24.960 --> 1:41:27.440
 They can have numbers on the represent,

1:41:27.440 --> 1:41:30.760
 say a truth value or an importance value

1:41:30.760 --> 1:41:32.000
 for a certain purpose.

1:41:32.000 --> 1:41:33.240
 And of course, like with all things,

1:41:33.240 --> 1:41:35.080
 you can reduce that to a hypergraph

1:41:35.080 --> 1:41:37.680
 and then the hypergraph can be reduced to a graph.

1:41:37.680 --> 1:41:39.840
 And you could reduce a graph to an adjacency matrix.

1:41:39.840 --> 1:41:42.680
 So I mean, there's always multiple representations.

1:41:42.680 --> 1:41:44.040
 But there's a layer of representation

1:41:44.040 --> 1:41:45.120
 that seems to work well here.

1:41:45.120 --> 1:41:45.960
 Got it.

1:41:45.960 --> 1:41:46.800
 Right, right, right.

1:41:46.800 --> 1:41:52.080
 And so similarly, you could have a link to a whole graph

1:41:52.080 --> 1:41:53.600
 because a whole graph could represent, say,

1:41:53.600 --> 1:41:55.040
 a body of information.

1:41:55.040 --> 1:41:58.640
 And I could say, I reject this body of information.

1:41:58.640 --> 1:42:00.320
 Then one way to do that is make that link

1:42:00.320 --> 1:42:02.000
 go to that whole subgraph representing

1:42:02.000 --> 1:42:04.040
 the body of information.

1:42:04.040 --> 1:42:07.200
 I mean, there are many alternate representations.

1:42:07.200 --> 1:42:10.720
 But that's anyway, what we have an open cog.

1:42:10.720 --> 1:42:13.160
 We have an atom space, which is this weighted labeled

1:42:13.160 --> 1:42:16.400
 generalized hypergraph, knowledge store.

1:42:16.400 --> 1:42:17.840
 It lives in RAM.

1:42:17.840 --> 1:42:20.120
 There's also a way to back it up to disk.

1:42:20.120 --> 1:42:24.160
 There are ways to spread it among multiple different machines.

1:42:24.160 --> 1:42:28.080
 Then there are various utilities for dealing with that.

1:42:28.080 --> 1:42:30.000
 So there's a pattern matcher, which

1:42:30.000 --> 1:42:33.920
 lets you specify a sort of abstract pattern

1:42:33.920 --> 1:42:36.200
 and then search through a whole atom space

1:42:36.200 --> 1:42:40.000
 with labeled hypergraph to see what subhypergraphs may

1:42:40.000 --> 1:42:42.920
 match that pattern, for an example.

1:42:42.920 --> 1:42:47.560
 So then there's something called the cog server

1:42:47.560 --> 1:42:52.560
 in open cog, which lets you run a bunch of different agents

1:42:52.560 --> 1:42:55.880
 or processes in a scheduler.

1:42:55.880 --> 1:42:58.200
 And each of these agents, basically,

1:42:58.200 --> 1:42:59.960
 it reads stuff from the atom space

1:42:59.960 --> 1:43:01.880
 and it writes stuff to the atom space.

1:43:01.880 --> 1:43:05.600
 So this is sort of the basic operational model.

1:43:05.600 --> 1:43:07.840
 That's the software framework.

1:43:07.840 --> 1:43:08.340
 Right.

1:43:08.340 --> 1:43:10.560
 And of course, there's a lot there just

1:43:10.560 --> 1:43:13.200
 from a scalable software engineering standpoint.

1:43:13.200 --> 1:43:15.080
 So you could use this, I don't know if you've,

1:43:15.080 --> 1:43:18.000
 have you looked into the Steven Wolframs physics project

1:43:18.000 --> 1:43:20.120
 recently with the hypergraphs and stuff?

1:43:20.120 --> 1:43:23.000
 Could you theoretically use the software framework to play?

1:43:23.000 --> 1:43:25.760
 You certainly could, although Wolfram would rather

1:43:25.760 --> 1:43:29.600
 die than use anything but Mathematica for his work.

1:43:29.600 --> 1:43:32.120
 Yeah, but there's a big community of people

1:43:32.120 --> 1:43:36.080
 who would love integration.

1:43:36.080 --> 1:43:37.720
 And like you said, the young minds

1:43:37.720 --> 1:43:40.360
 love the idea of integrating, of connecting things.

1:43:40.360 --> 1:43:40.860
 That's right.

1:43:40.860 --> 1:43:43.200
 And I would add on that note, the idea

1:43:43.200 --> 1:43:47.680
 of using hypergraph type models in physics is not very new.

1:43:47.680 --> 1:43:49.120
 Like if you look at.

1:43:49.120 --> 1:43:50.320
 The Russians did it first.

1:43:50.320 --> 1:43:52.160
 Well, I'm sure they did.

1:43:52.160 --> 1:43:55.840
 And a guy named Ben Dribis, who's a mathematician,

1:43:55.840 --> 1:43:58.200
 a professor in Louisiana or somewhere,

1:43:58.200 --> 1:44:01.920
 had a beautiful book on quantum sets and hypergraphs

1:44:01.920 --> 1:44:05.480
 and algebraic topology for discrete models of physics

1:44:05.480 --> 1:44:09.040
 and carried it much farther than Wolfram has.

1:44:09.040 --> 1:44:10.880
 But he's not rich and famous.

1:44:10.880 --> 1:44:13.240
 So it didn't get in the headlines.

1:44:13.240 --> 1:44:16.360
 But yeah, Wolfram aside, yeah, certainly, that's

1:44:16.360 --> 1:44:17.160
 a good way to put it.

1:44:17.160 --> 1:44:19.600
 The whole OpenCog framework, you could

1:44:19.600 --> 1:44:22.240
 use it to model biological networks

1:44:22.240 --> 1:44:24.240
 and simulate biology processes.

1:44:24.240 --> 1:44:28.600
 You could use it to model physics on discrete graph

1:44:28.600 --> 1:44:30.200
 models of physics.

1:44:30.200 --> 1:44:36.840
 So you could use it to do, say, biologically realistic

1:44:36.840 --> 1:44:39.280
 neural networks, for example.

1:44:39.280 --> 1:44:42.400
 And so that's a framework.

1:44:42.400 --> 1:44:44.280
 What do agents and processes do?

1:44:44.280 --> 1:44:45.400
 Do they grow the graph?

1:44:45.400 --> 1:44:47.040
 Do they, what kind of computations

1:44:47.040 --> 1:44:48.680
 just to get a sense of this?

1:44:48.680 --> 1:44:51.240
 So in theory, they could do anything they want to do.

1:44:51.240 --> 1:44:53.200
 They're just C++ processes.

1:44:53.200 --> 1:44:56.920
 On the other hand, the computation framework

1:44:56.920 --> 1:45:01.120
 is sort of designed for agents where most of their processing

1:45:01.120 --> 1:45:05.480
 time is taken up with reads and writes to the atmosphere.

1:45:05.480 --> 1:45:09.080
 And so that's a very different processing model

1:45:09.080 --> 1:45:12.480
 than, say, the matrix multiplication based model

1:45:12.480 --> 1:45:15.760
 as underlies most deep learning systems.

1:45:15.760 --> 1:45:21.000
 So you could create an agent that just

1:45:21.000 --> 1:45:22.760
 factored numbers for a billion years.

1:45:22.760 --> 1:45:25.000
 It would run within the OpenCog platform,

1:45:25.000 --> 1:45:26.720
 but it would be pointless.

1:45:26.720 --> 1:45:29.200
 I mean, the point of doing OpenCog is

1:45:29.200 --> 1:45:32.160
 because you want to make agents that are cooperating

1:45:32.160 --> 1:45:35.560
 via reading and writing into this weighted labeled

1:45:35.560 --> 1:45:40.600
 hypergraph, and that has both cognitive architecture

1:45:40.600 --> 1:45:43.400
 importance, because then this hypergraph is being used

1:45:43.400 --> 1:45:47.160
 as a sort of shared memory among different cognitive

1:45:47.160 --> 1:45:51.000
 processes, but it also has software and hardware

1:45:51.000 --> 1:45:52.840
 implementation implications.

1:45:52.840 --> 1:45:54.880
 Because current GPU architectures

1:45:54.880 --> 1:45:59.800
 are not so useful for OpenCog, whereas a graph chip would

1:45:59.800 --> 1:46:01.240
 be incredibly useful.

1:46:01.240 --> 1:46:03.640
 And I think GraphCore has those now,

1:46:03.640 --> 1:46:05.240
 but they're not ideally suited for this.

1:46:05.240 --> 1:46:10.680
 But I think in the next, let's say, three to five years,

1:46:10.680 --> 1:46:14.720
 we're going to see new chips where a graph is put on the chip

1:46:14.720 --> 1:46:19.840
 and the back and forth between multiple processes acting

1:46:19.840 --> 1:46:23.640
 SIMD and MIMD on that graph is going to be fast.

1:46:23.640 --> 1:46:26.520
 And then that may do for OpenCog type architectures

1:46:26.520 --> 1:46:29.840
 what GPUs did for deep neural architecture.

1:46:29.840 --> 1:46:31.360
 It's a small tangent.

1:46:31.360 --> 1:46:34.640
 Can you comment on thoughts about neuromorphic computing?

1:46:34.640 --> 1:46:38.480
 So hardware implementations of all these different kind of,

1:46:38.480 --> 1:46:40.920
 are you excited by that possibility?

1:46:40.920 --> 1:46:43.080
 I'm excited by graph processors, because I

1:46:43.080 --> 1:46:46.640
 think they can massively speed up OpenCog, which

1:46:46.640 --> 1:46:50.680
 is a class of architectures that I'm working on.

1:46:50.680 --> 1:46:57.200
 I think if, in principle, neuromorphic computing

1:46:57.200 --> 1:47:00.000
 should be amazing, I haven't yet been fully

1:47:00.000 --> 1:47:03.280
 sold on any of the systems that are out.

1:47:03.280 --> 1:47:06.360
 They're like, memristors should be amazing too, right?

1:47:06.360 --> 1:47:09.360
 So a lot of these things have obvious potential,

1:47:09.360 --> 1:47:12.200
 but I haven't yet put my hands on a system that

1:47:12.200 --> 1:47:13.200
 seemed to manifest that.

1:47:13.200 --> 1:47:14.840
 Yeah, Mark's system should be amazing,

1:47:14.840 --> 1:47:18.960
 but the current system has not been great.

1:47:18.960 --> 1:47:21.720
 For example, if you wanted to make

1:47:21.720 --> 1:47:25.680
 a biologically realistic hardware neural network,

1:47:25.680 --> 1:47:31.520
 like taking, making a circuit in hardware

1:47:31.520 --> 1:47:34.320
 that emulated like the Hodgkin–Huxley equation,

1:47:34.320 --> 1:47:38.200
 or the Izhekevich equation, like differential equations

1:47:38.200 --> 1:47:40.680
 for biologically realistic neuron,

1:47:40.680 --> 1:47:43.800
 and putting that in hardware on the chip,

1:47:43.800 --> 1:47:46.640
 that would seem that it would make more feasible to make

1:47:46.640 --> 1:47:51.040
 a large scale, truly biologically realistic neural network.

1:47:51.040 --> 1:47:54.400
 Now, what's been done so far is not like that.

1:47:54.400 --> 1:47:57.120
 So I guess, personally, as a researcher,

1:47:57.120 --> 1:47:59.760
 I mean, I've done a bunch of work in cognitive neural,

1:47:59.760 --> 1:48:02.400
 sorry, in computational neuroscience,

1:48:02.400 --> 1:48:06.400
 where I did some work with IRPA in DC Intelligence Advanced

1:48:06.400 --> 1:48:08.160
 Research Project Agency.

1:48:08.160 --> 1:48:10.840
 We were looking at, how do you make

1:48:10.840 --> 1:48:13.400
 a biologically realistic simulation of seven

1:48:13.400 --> 1:48:17.000
 different parts of the brain cooperating with each other,

1:48:17.000 --> 1:48:20.360
 using realistic nonlinear dynamical models of neurons?

1:48:20.360 --> 1:48:21.840
 And how do you get that to simulate

1:48:21.840 --> 1:48:24.240
 what's going on in the mind of a geoint intelligence

1:48:24.240 --> 1:48:26.480
 analyst while they're trying to find terrorists on a map,

1:48:26.480 --> 1:48:27.080
 right?

1:48:27.080 --> 1:48:29.800
 So if you want to do something like that,

1:48:29.800 --> 1:48:34.000
 having neuromuffer hardware that really let you simulate

1:48:34.000 --> 1:48:38.760
 like a realistic model of the neuron would be amazing.

1:48:38.760 --> 1:48:42.200
 But that's sort of with my computational neuroscience

1:48:42.200 --> 1:48:43.000
 had on, right?

1:48:43.000 --> 1:48:47.080
 With an AGI had on, I'm just more interested

1:48:47.080 --> 1:48:50.120
 in these hypergraph knowledge representation

1:48:50.120 --> 1:48:54.400
 based architectures, which would benefit more

1:48:54.400 --> 1:48:57.640
 from various types of graph processors.

1:48:57.640 --> 1:49:00.400
 Because the main processing bottleneck

1:49:00.400 --> 1:49:01.960
 is reading, writing to RAM.

1:49:01.960 --> 1:49:03.880
 It's reading, writing to the graph in RAM.

1:49:03.880 --> 1:49:06.840
 The main processing bottleneck for this kind of proto AGI

1:49:06.840 --> 1:49:09.760
 architecture is not multiplying matrices.

1:49:09.760 --> 1:49:13.840
 And for that reason, GPUs, which are really good at multiplying

1:49:13.840 --> 1:49:17.400
 matrices, don't apply as well.

1:49:17.400 --> 1:49:20.120
 There are frameworks like Gunrock and others

1:49:20.120 --> 1:49:23.200
 that try to boil down graph processing to matrix operations.

1:49:23.200 --> 1:49:25.160
 And they're cool, but you're still

1:49:25.160 --> 1:49:28.760
 putting a square peg into a round hole in a certain way.

1:49:28.760 --> 1:49:30.920
 The same is true.

1:49:30.920 --> 1:49:34.160
 I mean, current quantum machine learning, which is very cool.

1:49:34.160 --> 1:49:37.240
 It's also all about how to get matrix and vector operations

1:49:37.240 --> 1:49:38.760
 in quantum mechanics.

1:49:38.760 --> 1:49:41.280
 And I see why that's natural to do.

1:49:41.280 --> 1:49:44.200
 I mean, quantum mechanics is all unitary matrices

1:49:44.200 --> 1:49:45.800
 and vectors, right?

1:49:45.800 --> 1:49:47.840
 On the other hand, you could also

1:49:47.840 --> 1:49:51.000
 try to make graph centric quantum computers, which

1:49:51.000 --> 1:49:54.400
 I think is where things will go.

1:49:54.400 --> 1:50:00.120
 And then we can make take the OpenCog implementation layer,

1:50:00.120 --> 1:50:02.680
 implement it in a uncollapsed state

1:50:02.680 --> 1:50:04.000
 inside a quantum computer.

1:50:04.000 --> 1:50:08.240
 But that may be the singularity squared, right?

1:50:08.240 --> 1:50:11.040
 I'm not sure we need that to get to human level.

1:50:11.040 --> 1:50:11.800
 Singularity squared.

1:50:11.800 --> 1:50:12.360
 Human level.

1:50:12.360 --> 1:50:15.840
 That's already beyond the first singularity.

1:50:15.840 --> 1:50:17.120
 Yeah, let's go back to OpenCog.

1:50:17.120 --> 1:50:17.600
 No, no.

1:50:17.600 --> 1:50:20.040
 Yeah, and the hypergraph and OpenCog.

1:50:20.040 --> 1:50:21.600
 That's the software framework, right?

1:50:21.600 --> 1:50:25.440
 So the next thing is our cognitive architecture

1:50:25.440 --> 1:50:27.960
 tells us particular algorithms to put there.

1:50:27.960 --> 1:50:28.600
 Got it.

1:50:28.600 --> 1:50:33.720
 Can we backtrack on the kind of, is this graph designed?

1:50:33.720 --> 1:50:37.680
 Is it, in general, supposed to be sparse?

1:50:37.680 --> 1:50:40.600
 And the operations constantly grow and change the graph?

1:50:40.600 --> 1:50:42.400
 Yeah, the graph is sparse.

1:50:42.400 --> 1:50:45.000
 But is it constantly adding links and so on?

1:50:45.000 --> 1:50:47.200
 It is a self modifying hypergraph.

1:50:47.200 --> 1:50:50.440
 So it's not, so the write and read operations you're referring

1:50:50.440 --> 1:50:54.320
 to, this isn't just a fixed graph to which you change the way.

1:50:54.320 --> 1:50:55.840
 It's a constantly growing graph.

1:50:55.840 --> 1:50:58.000
 Yeah, that's true.

1:50:58.000 --> 1:51:03.440
 So it is different model than, say,

1:51:03.440 --> 1:51:06.840
 current deep neural nets and have a fixed neural architecture.

1:51:06.840 --> 1:51:08.600
 And you're updating the weights.

1:51:08.600 --> 1:51:11.280
 Although there have been cascade correlational neural net

1:51:11.280 --> 1:51:13.960
 architectures that grow new nodes and links.

1:51:13.960 --> 1:51:16.640
 But the most common neural architectures now

1:51:16.640 --> 1:51:17.960
 have a fixed neural architecture.

1:51:17.960 --> 1:51:19.080
 You're updating the weights.

1:51:19.080 --> 1:51:22.520
 And in OpenCog, you can update the weights.

1:51:22.520 --> 1:51:24.760
 And that certainly happens a lot.

1:51:24.760 --> 1:51:28.200
 But adding new nodes, adding new links,

1:51:28.200 --> 1:51:30.720
 removing nodes and links is an equally critical part

1:51:30.720 --> 1:51:32.160
 of the system's operations.

1:51:32.160 --> 1:51:32.680
 Got it.

1:51:32.680 --> 1:51:37.040
 So now when you start to add these cognitive algorithms

1:51:37.040 --> 1:51:39.840
 on top of this OpenCog architecture,

1:51:39.840 --> 1:51:40.920
 what does that look like?

1:51:40.920 --> 1:51:41.320
 So what?

1:51:41.320 --> 1:51:44.800
 Yeah, so within this framework, then,

1:51:44.800 --> 1:51:48.040
 creating a cognitive architecture is basically two things.

1:51:48.040 --> 1:51:51.640
 It's choosing what type system you

1:51:51.640 --> 1:51:53.800
 want to put on the nodes and links in the hypergraph,

1:51:53.800 --> 1:51:56.120
 what types of nodes and links you want.

1:51:56.120 --> 1:52:01.000
 And then it's choosing what collection of agents,

1:52:01.000 --> 1:52:04.640
 what collection of AI algorithms or processes

1:52:04.640 --> 1:52:08.000
 are going to run to operate on this hypergraph.

1:52:08.000 --> 1:52:10.520
 And of course, those two decisions

1:52:10.520 --> 1:52:14.000
 are closely connected to each other.

1:52:14.000 --> 1:52:17.640
 So in terms of the type system, there

1:52:17.640 --> 1:52:19.880
 are some links that are more neural net like.

1:52:19.880 --> 1:52:23.760
 They just have weights to get updated by heavy and learning

1:52:23.760 --> 1:52:25.960
 and activation spreads along them.

1:52:25.960 --> 1:52:29.400
 There are other links that are more logic like and nodes

1:52:29.400 --> 1:52:30.480
 that are more logic like.

1:52:30.480 --> 1:52:32.120
 So you could have a variable node.

1:52:32.120 --> 1:52:33.600
 And you can have a node representing

1:52:33.600 --> 1:52:36.120
 a universal or existential quantifier

1:52:36.120 --> 1:52:39.080
 as in predicate logic or term logic.

1:52:39.080 --> 1:52:42.000
 So you can have logic like nodes and links.

1:52:42.000 --> 1:52:44.360
 Or you can have neural like nodes and links.

1:52:44.360 --> 1:52:47.360
 You can also have procedure like nodes and links

1:52:47.360 --> 1:52:51.120
 as in, say, combinator logic or lambda

1:52:51.120 --> 1:52:53.600
 calculus representing programs.

1:52:53.600 --> 1:52:56.480
 So you can have nodes and links representing

1:52:56.480 --> 1:52:58.800
 many different types of semantics, which

1:52:58.800 --> 1:53:00.760
 means you could make a horrible, ugly mess.

1:53:00.760 --> 1:53:04.280
 Or you could make a system where these different types of knowledge

1:53:04.280 --> 1:53:08.040
 all interpenetrate and synergize with each other

1:53:08.040 --> 1:53:09.560
 beautifully, right?

1:53:09.560 --> 1:53:12.840
 So the hypergraph can contain programs.

1:53:12.840 --> 1:53:14.480
 Yeah, it can contain programs.

1:53:14.480 --> 1:53:18.120
 Although in the current version, it

1:53:18.120 --> 1:53:22.080
 is a very inefficient way to guide the execution of programs,

1:53:22.080 --> 1:53:24.200
 which is one thing that we are aiming

1:53:24.200 --> 1:53:28.640
 to resolve with our rewrite of the system now.

1:53:28.640 --> 1:53:33.600
 So what do you use the most beautiful aspect of OpenCog?

1:53:33.600 --> 1:53:36.520
 Just to you personally, some aspect

1:53:36.520 --> 1:53:42.840
 that captivates your imagination from beauty or power.

1:53:42.840 --> 1:53:49.480
 What fascinates me is finding a common representation that

1:53:49.480 --> 1:53:55.760
 underlies abstract declarative knowledge

1:53:55.760 --> 1:53:59.520
 and sensory knowledge and movement knowledge

1:53:59.520 --> 1:54:03.000
 and procedural knowledge and episodic knowledge.

1:54:03.000 --> 1:54:06.160
 Finding the right level of representation

1:54:06.160 --> 1:54:07.880
 where all these types of knowledge

1:54:07.880 --> 1:54:12.840
 are stored in a sort of universal and interconvertible,

1:54:12.840 --> 1:54:15.760
 yet practically manipulable way, right?

1:54:15.760 --> 1:54:19.080
 So to me, that's the core.

1:54:19.080 --> 1:54:21.920
 Because once you've done that, then the different learning

1:54:21.920 --> 1:54:24.440
 algorithms can help each other out.

1:54:24.440 --> 1:54:27.440
 Like what you want is if you have a logic engine that

1:54:27.440 --> 1:54:30.320
 helps with declarative knowledge and you have a deep neural net

1:54:30.320 --> 1:54:32.560
 that gathers perceptual knowledge and you

1:54:32.560 --> 1:54:34.800
 have, say, an evolutionary learning system that

1:54:34.800 --> 1:54:38.920
 learns procedures, you want these to not only interact

1:54:38.920 --> 1:54:42.000
 on the level of sharing results and passing inputs

1:54:42.000 --> 1:54:44.760
 and outputs to each other, you want the logic engine, when

1:54:44.760 --> 1:54:48.560
 it gets stuck, to be able to share its intermediate state

1:54:48.560 --> 1:54:51.360
 with the neural net and with the evolutionary learning

1:54:51.360 --> 1:54:55.440
 algorithm so that they can help each other out of bottlenecks

1:54:55.440 --> 1:54:58.280
 and help each other solve combinatorial explosions

1:54:58.280 --> 1:55:02.000
 by intervening inside each other's cognitive processes.

1:55:02.000 --> 1:55:04.880
 But that can only be done if the intermediate state

1:55:04.880 --> 1:55:07.360
 of a logic engine, evolutionary learning engine,

1:55:07.360 --> 1:55:11.120
 and a deep neural net are represented in the same form.

1:55:11.120 --> 1:55:12.360
 And that's what we figured out how

1:55:12.360 --> 1:55:15.760
 to do by putting the right type system on top of this weighted

1:55:15.760 --> 1:55:17.000
 labeled hypergraph.

1:55:17.000 --> 1:55:19.600
 So is there, can you maybe elaborate

1:55:19.600 --> 1:55:22.240
 on what are the different characteristics of a type

1:55:22.240 --> 1:55:27.800
 system that can coexist amongst all these different kinds

1:55:27.800 --> 1:55:30.040
 of knowledge that needs to be represented?

1:55:30.040 --> 1:55:34.240
 And is, I mean, like, is it hierarchical?

1:55:34.240 --> 1:55:37.600
 Just any kind of insights you can give on that kind of type?

1:55:37.600 --> 1:55:38.080
 Yeah, yeah.

1:55:38.080 --> 1:55:43.920
 So this gets very nitty gritty and mathematical, of course.

1:55:43.920 --> 1:55:49.080
 But one key part is switching from predicate logic

1:55:49.080 --> 1:55:50.440
 to term logic.

1:55:50.440 --> 1:55:51.640
 What is predicate logic?

1:55:51.640 --> 1:55:53.200
 What is term logic?

1:55:53.200 --> 1:55:56.080
 So term logic wasn't done by Aristotle,

1:55:56.080 --> 1:55:59.160
 or at least that's the oldest recollection

1:55:59.160 --> 1:56:01.280
 we have of it.

1:56:01.280 --> 1:56:05.280
 But term logic breaks down basic logic

1:56:05.280 --> 1:56:07.480
 into basically simple links between nodes,

1:56:07.480 --> 1:56:13.000
 like an inheritance link between node A and node B.

1:56:13.000 --> 1:56:16.800
 So in term logic, the basic deduction operation

1:56:16.800 --> 1:56:21.560
 is A implies B, B implies C, therefore A implies C.

1:56:21.560 --> 1:56:24.080
 Whereas in predicate logic, the basic operation

1:56:24.080 --> 1:56:28.120
 is modus ponens, like A, A implies B, therefore B.

1:56:28.120 --> 1:56:31.880
 So it's a slightly different way of breaking down logic.

1:56:31.880 --> 1:56:35.760
 But by breaking down logic into term logic,

1:56:35.760 --> 1:56:40.560
 you get a nice way of breaking logic down into nodes and links.

1:56:40.560 --> 1:56:42.960
 So your concepts can become nodes.

1:56:42.960 --> 1:56:45.160
 The logical relations become links.

1:56:45.160 --> 1:56:48.720
 And so then inference is like, so if this link is A implies B,

1:56:48.720 --> 1:56:51.560
 this link is B implies C, then deduction

1:56:51.560 --> 1:56:54.880
 builds a link A implies C. And your probabilistic algorithm

1:56:54.880 --> 1:56:57.400
 can assign a certain weight there.

1:56:57.400 --> 1:57:01.240
 Now, you may also have a Hebbian neural link from A to C,

1:57:01.240 --> 1:57:06.640
 which is the degree to which A being the focus of attention

1:57:06.640 --> 1:57:09.040
 should make B the focus of attention.

1:57:09.040 --> 1:57:10.800
 So you could have then a neural link,

1:57:10.800 --> 1:57:15.040
 and you could have a symbolic logical inheritance

1:57:15.040 --> 1:57:16.920
 link in your term logic.

1:57:16.920 --> 1:57:20.160
 And they have separate meaning, but they

1:57:20.160 --> 1:57:22.960
 could be used to guide each other as well.

1:57:22.960 --> 1:57:26.760
 Like, if there is a large amount of neural weight

1:57:26.760 --> 1:57:29.560
 on the link between A and B, that may direct your logic

1:57:29.560 --> 1:57:31.360
 engine to think about, well, what is the relation?

1:57:31.360 --> 1:57:32.040
 Are they similar?

1:57:32.040 --> 1:57:34.720
 Is there an inheritance relation?

1:57:34.720 --> 1:57:37.440
 Are they similar in some context?

1:57:37.440 --> 1:57:41.280
 On the other hand, if there's a logical relation between A and B,

1:57:41.280 --> 1:57:43.800
 that may direct your neural component to think, well,

1:57:43.800 --> 1:57:45.960
 when I'm thinking about A, should I

1:57:45.960 --> 1:57:48.240
 be directing some attention to B also?

1:57:48.240 --> 1:57:50.200
 Because there's a logical relation.

1:57:50.200 --> 1:57:53.800
 So in terms of logic, there's a lot of thought

1:57:53.800 --> 1:57:58.280
 that went into how do you break down logic relations,

1:57:58.280 --> 1:58:02.320
 including basic sort of propositional logic relations

1:58:02.320 --> 1:58:04.160
 as Aristotle in term logic deals with,

1:58:04.160 --> 1:58:07.080
 and then quantifier logic relations also.

1:58:07.080 --> 1:58:10.880
 How do you break those down elegantly into a hypergraph?

1:58:10.880 --> 1:58:13.200
 Because I mean, you can boil logic expression

1:58:13.200 --> 1:58:14.800
 and do a graph in many different ways.

1:58:14.800 --> 1:58:16.640
 Many of them are very ugly, right?

1:58:16.640 --> 1:58:21.680
 We tried to find elegant ways of sort of hierarchically breaking

1:58:21.680 --> 1:58:26.840
 down complex logic expression into nodes and links

1:58:26.840 --> 1:58:31.360
 so that if you have, say, different nodes representing

1:58:31.360 --> 1:58:34.160
 Ben, AI, Lex, interview, or whatever,

1:58:34.160 --> 1:58:36.760
 the logic relations between those things

1:58:36.760 --> 1:58:40.480
 are compact in the node and link representation

1:58:40.480 --> 1:58:42.880
 so that when you have a neural net acting on those same nodes

1:58:42.880 --> 1:58:45.680
 and links, the neural net and the logic engine

1:58:45.680 --> 1:58:48.200
 can sort of interoperate with each other.

1:58:48.200 --> 1:58:49.880
 And also interpretable by humans.

1:58:49.880 --> 1:58:51.320
 Is that an important?

1:58:51.320 --> 1:58:52.400
 That's tough.

1:58:52.400 --> 1:58:54.560
 In simple cases, it's interpretable by humans.

1:58:54.560 --> 1:59:01.640
 But honestly, I would say logic systems

1:59:01.640 --> 1:59:08.040
 give more potential for transparency

1:59:08.040 --> 1:59:11.600
 and comprehensibility than neural net systems,

1:59:11.600 --> 1:59:12.840
 but you still have to work at it.

1:59:12.840 --> 1:59:16.680
 Because I mean, if I show you a predicate logic proposition

1:59:16.680 --> 1:59:20.080
 with like 500 nested universal and existential quantifiers

1:59:20.080 --> 1:59:24.080
 and 217 variables, that's no more comprehensible than the weight

1:59:24.080 --> 1:59:26.560
 metrics of a neural network, right?

1:59:26.560 --> 1:59:29.000
 So I'd say the logic expressions and AI

1:59:29.000 --> 1:59:32.360
 learns from its experience are mostly totally opaque

1:59:32.360 --> 1:59:35.120
 to human beings and maybe even harder to understand

1:59:35.120 --> 1:59:36.120
 than a neural net.

1:59:36.120 --> 1:59:38.440
 Because I mean, when you have multiple nested quantifier

1:59:38.440 --> 1:59:41.480
 bindings, it's a very high level of abstraction.

1:59:41.480 --> 1:59:44.680
 There is a difference, though, in that within logic,

1:59:44.680 --> 1:59:46.840
 it's a little more straightforward

1:59:46.840 --> 1:59:49.080
 to pose the problem of normalize this

1:59:49.080 --> 1:59:51.040
 and boil this down to a certain form.

1:59:51.040 --> 1:59:52.680
 I mean, you can do that in neural nets, too.

1:59:52.680 --> 1:59:55.200
 Like you can distill a neural net to a simpler form,

1:59:55.200 --> 1:59:57.440
 but that's more often done to make a neural net that'll

1:59:57.440 --> 1:59:59.280
 run on an embedded device or something.

1:59:59.280 --> 2:00:03.880
 It's harder to distill a net to a comprehensible form than is

2:00:03.880 --> 2:00:06.840
 to simplify a logic expression to a comprehensible form,

2:00:06.840 --> 2:00:08.560
 but it doesn't come for free.

2:00:08.560 --> 2:00:14.360
 Like what's in the AI's mind is incomprehensible to a human

2:00:14.360 --> 2:00:16.880
 unless you do some special work to make it comprehensible.

2:00:16.880 --> 2:00:19.720
 So on the procedural side, there's

2:00:19.720 --> 2:00:22.960
 some different and interesting voodoo there.

2:00:22.960 --> 2:00:25.480
 I mean, if you're familiar in computer science,

2:00:25.480 --> 2:00:27.040
 there's something called the Curry Howard

2:00:27.040 --> 2:00:29.840
 correspondence, which is a one to one mapping between proofs

2:00:29.840 --> 2:00:30.920
 and programs.

2:00:30.920 --> 2:00:33.520
 So every program can be mapped into a proof.

2:00:33.520 --> 2:00:35.960
 Every proof can be mapped into a program.

2:00:35.960 --> 2:00:37.760
 You can model this using category theory

2:00:37.760 --> 2:00:40.960
 in a bunch of nice math.

2:00:40.960 --> 2:00:43.240
 But we want to make that practical, right?

2:00:43.240 --> 2:00:46.520
 So that if you have an executable program

2:00:46.520 --> 2:00:49.960
 that moves a robot's arm or figures out

2:00:49.960 --> 2:00:52.040
 in what order to say things in a dialogue, that's

2:00:52.040 --> 2:00:55.800
 a procedure represented in OpenCog's hypergraph.

2:00:55.800 --> 2:01:00.080
 But if you want to reason on how to improve that procedure,

2:01:00.080 --> 2:01:04.280
 you need to map that procedure into logic using Curry Howard

2:01:04.280 --> 2:01:09.320
 isomorphism so that then the logic engine can reason

2:01:09.320 --> 2:01:11.080
 about how to improve that procedure

2:01:11.080 --> 2:01:14.240
 and then map that back into the procedural representation that

2:01:14.240 --> 2:01:16.120
 is efficient for execution.

2:01:16.120 --> 2:01:18.760
 So again, that comes down to not just

2:01:18.760 --> 2:01:21.400
 can you make your procedure into a bunch of nodes and links?

2:01:21.400 --> 2:01:23.200
 Because I mean, that can be done trivially.

2:01:23.200 --> 2:01:26.400
 A C++ compiler has nodes and links inside it.

2:01:26.400 --> 2:01:28.840
 Can you boil down your procedure into a bunch of nodes

2:01:28.840 --> 2:01:32.480
 and links in a way that's like hierarchically decomposed

2:01:32.480 --> 2:01:33.640
 and simple enough?

2:01:33.640 --> 2:01:34.400
 They can reason about it.

2:01:34.400 --> 2:01:34.920
 Yeah, yeah.

2:01:34.920 --> 2:01:37.000
 Given the resource constraints at hand,

2:01:37.000 --> 2:01:42.040
 you can map it back and forth to your term logic fast enough

2:01:42.040 --> 2:01:45.160
 and without having a bloated logic expression, right?

2:01:45.160 --> 2:01:50.320
 So there's just a lot of nitty gritty particulars there.

2:01:50.320 --> 2:01:54.520
 But by the same token, if you ask a chip designer,

2:01:54.520 --> 2:01:58.520
 like, how do you make the Intel i7 chip so good?

2:01:58.520 --> 2:02:02.560
 There's a long list of technical answers there,

2:02:02.560 --> 2:02:04.760
 which will take a while to go through, right?

2:02:04.760 --> 2:02:06.640
 And this has been decades of work.

2:02:06.640 --> 2:02:10.200
 I mean, the first AI system of this nature

2:02:10.200 --> 2:02:13.440
 I tried to build was called WebMind in the mid 1990s.

2:02:13.440 --> 2:02:17.200
 And we had a big graph, a big graph operating in RAM

2:02:17.200 --> 2:02:20.280
 implemented with Java 1.1, which is a terrible, terrible

2:02:20.280 --> 2:02:21.840
 implementation idea.

2:02:21.840 --> 2:02:25.960
 And then each node had its own processing.

2:02:25.960 --> 2:02:29.000
 So like there, the core loop loop through all nodes

2:02:29.000 --> 2:02:31.040
 in the network and let each node enact what

2:02:31.040 --> 2:02:32.920
 its little thing was doing.

2:02:32.920 --> 2:02:35.840
 And we had logic and neural nets in there

2:02:35.840 --> 2:02:38.400
 but an evolutionary learning.

2:02:38.400 --> 2:02:40.760
 But we hadn't done enough of the math

2:02:40.760 --> 2:02:43.320
 to get them to operate together very cleanly.

2:02:43.320 --> 2:02:46.200
 So it was really, it was quite a horrible mess.

2:02:46.200 --> 2:02:49.360
 So as well as shifting and implementation,

2:02:49.360 --> 2:02:51.840
 where the graph is its own object

2:02:51.840 --> 2:02:54.680
 and the agents are separately scheduled,

2:02:54.680 --> 2:02:57.160
 we've also done a lot of work on how

2:02:57.160 --> 2:03:01.160
 do you represent programs, how do you represent procedures,

2:03:01.160 --> 2:03:04.480
 how do you represent genotypes for evolution in a way

2:03:04.480 --> 2:03:07.920
 that the interoperability between the different types

2:03:07.920 --> 2:03:11.720
 of learning associated with these different types of knowledge

2:03:11.720 --> 2:03:13.040
 actually works.

2:03:13.040 --> 2:03:14.960
 And that's been quite difficult.

2:03:14.960 --> 2:03:18.560
 It's taken decades and it's totally off to the side

2:03:18.560 --> 2:03:22.320
 of what the commercial mainstream of the AI field

2:03:22.320 --> 2:03:27.200
 is doing, which isn't thinking about representation at all,

2:03:27.200 --> 2:03:30.760
 really, although you could see, like in the DNC,

2:03:30.760 --> 2:03:32.400
 they had to think a little bit about how

2:03:32.400 --> 2:03:35.560
 do you make representation of a map in this memory matrix

2:03:35.560 --> 2:03:38.400
 work together with the representation needed for,

2:03:38.400 --> 2:03:41.280
 say, visual pattern recognition in the hierarchical neural

2:03:41.280 --> 2:03:42.040
 network.

2:03:42.040 --> 2:03:45.040
 But I would say we have taken that direction

2:03:45.040 --> 2:03:47.600
 of taking the types of knowledge you

2:03:47.600 --> 2:03:49.080
 need for different types of learning,

2:03:49.080 --> 2:03:51.960
 like declarative, procedural, attentional,

2:03:51.960 --> 2:03:54.800
 and how do you make these types of knowledge

2:03:54.800 --> 2:03:58.160
 represent in a way that allows cross learning

2:03:58.160 --> 2:04:00.120
 across these different types of memory.

2:04:00.120 --> 2:04:02.600
 We've been prototyping and experimenting

2:04:02.600 --> 2:04:07.560
 with this within OpenCog and before that webmind

2:04:07.560 --> 2:04:10.560
 since the mid 1990s.

2:04:10.560 --> 2:04:13.800
 Now, disappointingly, to all of us,

2:04:13.800 --> 2:04:18.480
 this has not yet been cashed out in an AGI system.

2:04:18.480 --> 2:04:22.480
 I mean, we've used this system within our consulting business.

2:04:22.480 --> 2:04:25.760
 So we've built natural language processing and robot control

2:04:25.760 --> 2:04:27.720
 and financial analysis.

2:04:27.720 --> 2:04:32.280
 We've built a bunch of vertical market specific proprietary AI

2:04:32.280 --> 2:04:36.680
 projects that use OpenCog on the back end.

2:04:36.680 --> 2:04:37.640
 But we haven't.

2:04:37.640 --> 2:04:40.520
 That's not the AGI goal.

2:04:40.520 --> 2:04:42.680
 It's interesting, but it's not the AGI goal.

2:04:42.680 --> 2:04:48.520
 So now what we're looking at with our rebuild of the system.

2:04:48.520 --> 2:04:49.360
 2.0.

2:04:49.360 --> 2:04:51.400
 Yeah, we're also calling it True AGI.

2:04:51.400 --> 2:04:54.800
 So we're not quite sure what the name is yet.

2:04:54.800 --> 2:04:57.480
 We made a website for TrueAGI.io,

2:04:57.480 --> 2:04:59.800
 but we haven't put anything on there yet.

2:04:59.800 --> 2:05:02.160
 We may come up with an even better name.

2:05:02.160 --> 2:05:05.640
 It's kind of like the real AI starting point for your AGI

2:05:05.640 --> 2:05:05.640
 book.

2:05:05.640 --> 2:05:08.520
 Yeah, but I like True better, because True has like,

2:05:08.520 --> 2:05:09.760
 you can be true hearted, right?

2:05:09.760 --> 2:05:11.080
 You can be true to your girlfriend.

2:05:11.080 --> 2:05:14.160
 So True has a number.

2:05:14.160 --> 2:05:15.720
 And it also has logic in it, right?

2:05:15.720 --> 2:05:17.160
 Because logic is a key point.

2:05:17.160 --> 2:05:18.280
 I like it, yeah.

2:05:18.280 --> 2:05:22.400
 So yeah, with the True AGI system,

2:05:22.400 --> 2:05:25.400
 we're sticking with the same basic architecture,

2:05:25.400 --> 2:05:29.640
 but we're trying to build on what we've learned.

2:05:29.640 --> 2:05:34.440
 And one thing we've learned is that we need type checking

2:05:34.440 --> 2:05:38.040
 among dependent types to be much faster

2:05:38.040 --> 2:05:41.120
 and among probabilistic dependent types to be much faster.

2:05:41.120 --> 2:05:45.720
 So as it is now, you can have complex types

2:05:45.720 --> 2:05:47.080
 on the nodes and links.

2:05:47.080 --> 2:05:48.680
 But if you want to put, like if you

2:05:48.680 --> 2:05:51.280
 want types to be first class citizens,

2:05:51.280 --> 2:05:53.760
 so that you can have the types can be variables,

2:05:53.760 --> 2:05:56.960
 and then you do type checking among complex, higher order

2:05:56.960 --> 2:06:00.960
 types, you can do that in the system now, but it's very slow.

2:06:00.960 --> 2:06:04.080
 This is stuff like it's done in cutting edge program languages

2:06:04.080 --> 2:06:05.360
 like Agda or something.

2:06:05.360 --> 2:06:07.400
 These obscure research languages.

2:06:07.400 --> 2:06:09.720
 On the other hand, we've been doing a lot of time

2:06:09.720 --> 2:06:12.320
 together deep neural nets with symbolic learning.

2:06:12.320 --> 2:06:15.200
 So we did a project for Cisco, for example,

2:06:15.200 --> 2:06:17.360
 which was on this was street scene analysis,

2:06:17.360 --> 2:06:19.480
 but they had deep neural models for a bunch of cameras

2:06:19.480 --> 2:06:20.960
 watching street scenes.

2:06:20.960 --> 2:06:23.200
 But they trained a different model for each camera,

2:06:23.200 --> 2:06:24.600
 because they couldn't get the transfer

2:06:24.600 --> 2:06:27.000
 learning to work between camera A and camera B.

2:06:27.000 --> 2:06:29.000
 So we took what came out of all the deep neural models

2:06:29.000 --> 2:06:30.360
 for the different cameras.

2:06:30.360 --> 2:06:33.400
 We fed it into an open cog symbolic representation.

2:06:33.400 --> 2:06:36.240
 Then we did some pattern mining and some reasoning

2:06:36.240 --> 2:06:38.120
 on what came out of all the different cameras

2:06:38.120 --> 2:06:39.440
 within the symbolic graph.

2:06:39.440 --> 2:06:42.000
 And that worked well for that application.

2:06:42.000 --> 2:06:45.840
 I mean, Hugo Latapie from Cisco gave a talk touching on that

2:06:45.840 --> 2:06:47.360
 at last year's AGR conference.

2:06:47.360 --> 2:06:48.760
 It was in Shenzhen.

2:06:48.760 --> 2:06:51.000
 On the other hand, we learned from there,

2:06:51.000 --> 2:06:53.600
 it was kind of clunky to get the deep neural models to work

2:06:53.600 --> 2:06:58.560
 well with the symbolic system, because we were using Torch.

2:06:58.560 --> 2:07:03.520
 And Torch keeps a sort of computation graph,

2:07:03.520 --> 2:07:06.440
 but you needed real time access to that computation graph

2:07:06.440 --> 2:07:07.600
 within our hypergraph.

2:07:07.600 --> 2:07:10.600
 And we certainly did it.

2:07:10.600 --> 2:07:13.040
 Alexei Podapov, who leads our St. Petersburg team,

2:07:13.040 --> 2:07:16.440
 wrote a great paper on cognitive modules in open cog,

2:07:16.440 --> 2:07:18.560
 explaining sort of how do you deal with the Torch compute

2:07:18.560 --> 2:07:19.880
 graph inside open cog.

2:07:19.880 --> 2:07:23.880
 But in the end, we realized that just hadn't been one

2:07:23.880 --> 2:07:27.160
 of our design thoughts when we built open cog, right?

2:07:27.160 --> 2:07:30.640
 So between wanting really fast dependent type checking

2:07:30.640 --> 2:07:33.600
 and wanting much more efficient interoperation

2:07:33.600 --> 2:07:36.240
 between the computation graphs of deep neural net frameworks

2:07:36.240 --> 2:07:37.680
 and open cogs hypergraph.

2:07:37.680 --> 2:07:41.080
 And adding on top of that, wanting to more effectively

2:07:41.080 --> 2:07:43.000
 run an open cog hypergraph distributed

2:07:43.000 --> 2:07:45.400
 across RAM in 10,000 machines, which

2:07:45.400 --> 2:07:47.240
 is we're doing dozens of machines now,

2:07:47.240 --> 2:07:50.680
 but it's just not, we didn't architect it

2:07:50.680 --> 2:07:53.000
 with that sort of modern scalability in mind.

2:07:53.000 --> 2:07:55.360
 So these performance requirements

2:07:55.360 --> 2:08:00.440
 are what have driven us to want to rearchitect the base.

2:08:00.440 --> 2:08:05.280
 But the core AGI paradigm doesn't really change.

2:08:05.280 --> 2:08:07.760
 Like the mathematics is the same.

2:08:07.760 --> 2:08:10.800
 It's just we can't scale to the level

2:08:10.800 --> 2:08:13.840
 that we want in terms of distributed processing

2:08:13.840 --> 2:08:16.240
 or speed of various kinds of processing

2:08:16.240 --> 2:08:19.360
 with the current infrastructure that

2:08:19.360 --> 2:08:26.080
 was built in the phase 2001 to 2008, which is hardly shocking.

2:08:26.080 --> 2:08:28.760
 Well, the three things you mentioned are really interesting.

2:08:28.760 --> 2:08:32.280
 So what do you think about, in terms of interoperability,

2:08:32.280 --> 2:08:36.280
 communicating with computational graph of neural networks?

2:08:36.280 --> 2:08:38.440
 What do you think about the representations

2:08:38.440 --> 2:08:40.640
 that neural networks form?

2:08:40.640 --> 2:08:42.880
 They're bad, but there's many ways

2:08:42.880 --> 2:08:44.320
 that you could deal with that.

2:08:44.320 --> 2:08:47.480
 So I've been wrestling with this a lot in some work

2:08:47.480 --> 2:08:49.880
 on supervised grammar induction.

2:08:49.880 --> 2:08:52.080
 And I have a simple paper on that.

2:08:52.080 --> 2:08:56.160
 They'll give it the next AGI conference, the online portion

2:08:56.160 --> 2:08:58.160
 of which is next week, actually.

2:08:58.160 --> 2:09:00.360
 What is grammar induction?

2:09:00.360 --> 2:09:05.160
 So this isn't AGI either, but it's sort of on the verge

2:09:05.160 --> 2:09:08.280
 between NERAI and AGI or something.

2:09:08.280 --> 2:09:11.320
 Unsupervised grammar induction is the problem.

2:09:11.320 --> 2:09:15.360
 Throw your AI system a huge body of text

2:09:15.360 --> 2:09:18.240
 and have it learn the grammar of the language that

2:09:18.240 --> 2:09:20.280
 produced that text.

2:09:20.280 --> 2:09:22.480
 So you're not giving it labeled examples.

2:09:22.480 --> 2:09:24.400
 So you're not giving it like 1,000 sentences

2:09:24.400 --> 2:09:27.120
 where the parses were marked up by graduate students.

2:09:27.120 --> 2:09:30.240
 So it's just got to infer the grammar from the text.

2:09:30.240 --> 2:09:33.800
 It's like the Rosetta Stone, but worse, because you only

2:09:33.800 --> 2:09:35.280
 have the one language.

2:09:35.280 --> 2:09:37.120
 And you have to figure out what is the grammar.

2:09:37.120 --> 2:09:43.040
 So that's not really AGI because the way a human learns

2:09:43.040 --> 2:09:44.320
 language is not that.

2:09:44.320 --> 2:09:47.800
 I mean, we learn from language that's used in context.

2:09:47.800 --> 2:09:49.280
 So it's a social embodied thing.

2:09:49.280 --> 2:09:53.480
 We see how a given sentence is grounded in observation.

2:09:53.480 --> 2:09:55.160
 There's an interactive element, I guess.

2:09:55.160 --> 2:09:56.480
 Yeah, yeah, yeah.

2:09:56.480 --> 2:10:00.320
 On the other hand, so I'm more interested in that.

2:10:00.320 --> 2:10:02.920
 I'm more interested in making an AGI system learn language

2:10:02.920 --> 2:10:05.520
 from its social and embodied experience.

2:10:05.520 --> 2:10:08.240
 On the other hand, that's also more of a pain to do.

2:10:08.240 --> 2:10:10.600
 And that would lead us into Hanson Robotics

2:10:10.600 --> 2:10:12.800
 and their robotics work, I've known which we'll talk about

2:10:12.800 --> 2:10:14.520
 in a few minutes.

2:10:14.520 --> 2:10:17.080
 But just as an intellectual exercise,

2:10:17.080 --> 2:10:20.520
 as a learning exercise, trying to learn grammar

2:10:20.520 --> 2:10:24.520
 from a corpus is very, very interesting.

2:10:24.520 --> 2:10:27.520
 And that's been a field in AI for a long time.

2:10:27.520 --> 2:10:29.240
 No one can do it very well.

2:10:29.240 --> 2:10:32.080
 So we've been looking at transformer neural networks

2:10:32.080 --> 2:10:35.720
 and tree transformers, which are amazing.

2:10:35.720 --> 2:10:39.120
 These came out of Google Brain, actually.

2:10:39.120 --> 2:10:41.920
 And actually, on that team was Lucas Kaiser,

2:10:41.920 --> 2:10:46.560
 who used to work for me in the period 2005 through 2008

2:10:46.560 --> 2:10:47.080
 or something.

2:10:47.080 --> 2:10:52.160
 So it's been fun to see my former AGI employees disperse

2:10:52.160 --> 2:10:54.080
 and do all these amazing things.

2:10:54.080 --> 2:10:57.520
 Way too many sucked into Google, actually.

2:10:57.520 --> 2:10:58.960
 We'll talk about that, too.

2:10:58.960 --> 2:11:00.640
 Lucas Kaiser and a bunch of these guys,

2:11:00.640 --> 2:11:03.200
 they create transformer networks.

2:11:03.200 --> 2:11:05.480
 That classic paper like, attention is all you need.

2:11:05.480 --> 2:11:08.200
 And all these things following on from that.

2:11:08.200 --> 2:11:10.160
 So we're looking at transformer networks.

2:11:10.160 --> 2:11:13.880
 And these are able to, I mean, this

2:11:13.880 --> 2:11:16.480
 is what underlies GPT2 and GPT3 and so on,

2:11:16.480 --> 2:11:19.120
 which are very, very cool and have absolutely

2:11:19.120 --> 2:11:21.480
 no cognitive understanding of any of the texts we're looking

2:11:21.480 --> 2:11:25.000
 at, like they're very intelligent idiots, right?

2:11:25.000 --> 2:11:28.080
 So sorry to take, but I'll bring us back.

2:11:28.080 --> 2:11:31.800
 But do you think GPT3 understands language?

2:11:31.800 --> 2:11:34.080
 No, no, no, it understands nothing.

2:11:34.080 --> 2:11:35.320
 It's a complete idiot.

2:11:35.320 --> 2:11:36.720
 But it's a brilliant idiot.

2:11:36.720 --> 2:11:40.480
 You don't think GPT20 will understand language?

2:11:40.480 --> 2:11:42.200
 No, no, no, no.

2:11:42.200 --> 2:11:45.160
 So size is not going to buy you understanding.

2:11:45.160 --> 2:11:48.840
 And any more than a faster car is going to get you to Mars.

2:11:48.840 --> 2:11:50.920
 It's a completely different kind of thing.

2:11:50.920 --> 2:11:54.280
 I mean, these networks are very cool.

2:11:54.280 --> 2:11:57.440
 And as an entrepreneur, I can see many highly valuable uses

2:11:57.440 --> 2:11:57.940
 for them.

2:11:57.940 --> 2:12:01.080
 And as an artist, I love them, right?

2:12:01.080 --> 2:12:05.440
 So I mean, we're using our own neural model, which

2:12:05.440 --> 2:12:09.040
 is along those lines to control the Philip K. Dick robot now.

2:12:09.040 --> 2:12:12.800
 And it's amazing to train a neural model on the robot

2:12:12.800 --> 2:12:15.880
 Philip K. Dick and see it come up with like crazed,

2:12:15.880 --> 2:12:19.120
 stoned philosopher pronouncements very much

2:12:19.120 --> 2:12:22.600
 like what Philip K. Dick might have said, right?

2:12:22.600 --> 2:12:24.880
 These models are super cool.

2:12:24.880 --> 2:12:27.720
 And I'm working with Hanson Robotics now

2:12:27.720 --> 2:12:30.600
 on using a similar but more sophisticated one for Sophia,

2:12:30.600 --> 2:12:34.040
 which we haven't launched yet.

2:12:34.040 --> 2:12:36.040
 But so I think it's cool.

2:12:36.040 --> 2:12:39.440
 But no, these are recognizing a large number

2:12:39.440 --> 2:12:43.440
 of shallow patterns that they're not forming

2:12:43.440 --> 2:12:44.840
 an abstract representation.

2:12:44.840 --> 2:12:47.360
 And that's the point I was coming to when

2:12:47.360 --> 2:12:50.680
 we're looking at grammar induction.

2:12:50.680 --> 2:12:53.520
 We tried to mine patterns out of the structure,

2:12:53.520 --> 2:12:55.880
 the transformer network.

2:12:55.880 --> 2:12:59.600
 And you can, but the patterns aren't what you want.

2:12:59.600 --> 2:13:00.600
 They're nasty.

2:13:00.600 --> 2:13:03.200
 So I mean, if you do supervised learning,

2:13:03.200 --> 2:13:05.240
 if you look at sentences where you know the correct parts

2:13:05.240 --> 2:13:08.640
 of a sentence, you can learn a matrix that

2:13:08.640 --> 2:13:12.280
 maps between the internal representation of the transformer

2:13:12.280 --> 2:13:14.160
 and the parts of the sentence.

2:13:14.160 --> 2:13:16.160
 And so then you can actually train something

2:13:16.160 --> 2:13:18.480
 that will output the sentence parts

2:13:18.480 --> 2:13:20.680
 from the transformer network's internal state.

2:13:20.680 --> 2:13:25.440
 And we did this, I think, CRISPR manning.

2:13:25.440 --> 2:13:28.080
 Some others have not done this also.

2:13:28.080 --> 2:13:30.720
 But I mean, what you get is that the representation is

2:13:30.720 --> 2:13:33.240
 hardly ugly and is scattered all over the network

2:13:33.240 --> 2:13:34.960
 and doesn't look like the rules of grammar

2:13:34.960 --> 2:13:37.280
 that you know are the right rules of grammar, right?

2:13:37.280 --> 2:13:38.280
 It's kind of ugly.

2:13:38.280 --> 2:13:41.160
 So what we're actually doing is we're

2:13:41.160 --> 2:13:44.280
 using a symbolic grammar learning algorithm,

2:13:44.280 --> 2:13:46.760
 but we're using the transformer neural network

2:13:46.760 --> 2:13:48.920
 as a sentence probability oracle.

2:13:48.920 --> 2:13:52.120
 So like, if you have a rule of grammar

2:13:52.120 --> 2:13:54.840
 and you aren't sure if it's a correct rule of grammar or not,

2:13:54.840 --> 2:13:58.040
 you can generate a bunch of senses using that rule of grammar

2:13:58.040 --> 2:14:00.880
 and a bunch of senses violating that rule of grammar.

2:14:00.880 --> 2:14:04.480
 And you can see the transformer model

2:14:04.480 --> 2:14:06.720
 doesn't think the senses obeying the rule of grammar

2:14:06.720 --> 2:14:08.240
 are more probable than the senses

2:14:08.240 --> 2:14:10.080
 disobeying the rule of grammar.

2:14:10.080 --> 2:14:11.840
 So in that way, you can use the neural model

2:14:11.840 --> 2:14:18.040
 as a sense probability oracle to guide a symbolic grammar

2:14:18.040 --> 2:14:19.960
 learning process.

2:14:19.960 --> 2:14:22.640
 And that seems to work better than trying

2:14:22.640 --> 2:14:25.920
 to milk the grammar out of the neural network that

2:14:25.920 --> 2:14:26.880
 doesn't have an end there.

2:14:26.880 --> 2:14:29.480
 So I think the thing is these neural nets

2:14:29.480 --> 2:14:32.880
 are not getting a semantically meaningful representation

2:14:32.880 --> 2:14:35.480
 internally by and large.

2:14:35.480 --> 2:14:38.120
 So one line of research is to try to get them to do that.

2:14:38.120 --> 2:14:40.080
 And Infogam was trying to do that.

2:14:40.080 --> 2:14:43.040
 So if you look back two years ago,

2:14:43.040 --> 2:14:45.280
 there was all these papers on Edward,

2:14:45.280 --> 2:14:47.400
 this probabilistic programming neural net framework

2:14:47.400 --> 2:14:49.680
 that Google had, which came out of Infogam.

2:14:49.680 --> 2:14:54.920
 So the idea there was you could train an Infogam neural net

2:14:54.920 --> 2:14:57.200
 model, which is a generative associative network,

2:14:57.200 --> 2:14:59.240
 to recognize and generate faces.

2:14:59.240 --> 2:15:00.680
 And the model would automatically

2:15:00.680 --> 2:15:03.320
 learn a variable for how long the nose is,

2:15:03.320 --> 2:15:05.800
 and automatically learn a variable for how wide the eyes are

2:15:05.800 --> 2:15:08.080
 or how big the lips are or something, right?

2:15:08.080 --> 2:15:11.080
 So it automatically learned these variables,

2:15:11.080 --> 2:15:12.560
 which have a semantic meaning.

2:15:12.560 --> 2:15:15.360
 So that was a rare case where a neural net

2:15:15.360 --> 2:15:18.120
 trained with a fairly standard GAN method

2:15:18.120 --> 2:15:20.920
 was able to actually learn a semantic representation.

2:15:20.920 --> 2:15:22.800
 So for many years, many of us tried

2:15:22.800 --> 2:15:25.320
 to take that the next step and get

2:15:25.320 --> 2:15:28.800
 a GAN type neural network that would have not just

2:15:28.800 --> 2:15:31.720
 a list of semantic latent variables,

2:15:31.720 --> 2:15:34.000
 but would have, say, a Bayes net of semantic latent variables

2:15:34.000 --> 2:15:35.520
 with dependencies between them.

2:15:35.520 --> 2:15:38.880
 The whole programming framework, Edward, was made for that.

2:15:38.880 --> 2:15:40.760
 I mean, no one got it to work, right?

2:15:40.760 --> 2:15:41.600
 And it couldn't be.

2:15:41.600 --> 2:15:43.040
 You think it's possible?

2:15:43.040 --> 2:15:43.680
 Yeah, do you think so?

2:15:43.680 --> 2:15:44.800
 I don't know.

2:15:44.800 --> 2:15:47.320
 It might be that back propagation just won't work for it,

2:15:47.320 --> 2:15:49.760
 because the gradients are too screwed up.

2:15:49.760 --> 2:15:52.040
 Maybe you could get to work using CMAES,

2:15:52.040 --> 2:15:54.880
 or some floating point evolutionary algorithm.

2:15:54.880 --> 2:15:55.400
 We tried.

2:15:55.400 --> 2:15:57.080
 We didn't get it to work.

2:15:57.080 --> 2:16:01.400
 Eventually, we just paused that rather than gave it up.

2:16:01.400 --> 2:16:03.840
 We paused that and said, well, OK, let's

2:16:03.840 --> 2:16:09.360
 try more innovative ways to learn what

2:16:09.360 --> 2:16:12.200
 are the representations implicit in that network

2:16:12.200 --> 2:16:14.760
 without trying to make it grow inside that network.

2:16:14.760 --> 2:16:19.680
 And I described how we're doing that in language.

2:16:19.680 --> 2:16:21.480
 You can do similar things in vision, right?

2:16:21.480 --> 2:16:22.080
 So what?

2:16:22.080 --> 2:16:23.440
 Use it as an oracle.

2:16:23.440 --> 2:16:24.160
 Yeah, yeah, yeah.

2:16:24.160 --> 2:16:27.160
 So that's one way, is that you use a structure learning

2:16:27.160 --> 2:16:29.160
 algorithm, which is symbolic.

2:16:29.160 --> 2:16:32.480
 And then you use the deep neural net as an oracle

2:16:32.480 --> 2:16:34.240
 to guide the structure learning algorithm.

2:16:34.240 --> 2:16:37.880
 The other way to do it is like InfoGam was trying to do

2:16:37.880 --> 2:16:41.440
 and try to tweak the neural network to have

2:16:41.440 --> 2:16:44.160
 this symbolic representation inside it.

2:16:44.160 --> 2:16:46.440
 I tend to think what the brain is doing

2:16:46.440 --> 2:16:51.680
 is more like using the deep neural net type thing

2:16:51.680 --> 2:16:52.520
 as an oracle.

2:16:52.520 --> 2:16:56.680
 I think the visual cortex, or the cerebellum,

2:16:56.680 --> 2:17:00.280
 are probably learning a non semantically meaningful,

2:17:00.280 --> 2:17:02.480
 opaque, tangled representation.

2:17:02.480 --> 2:17:04.600
 And then when they interface with the more cognitive parts

2:17:04.600 --> 2:17:09.200
 of the cortex, the cortex is sort of using those as an oracle

2:17:09.200 --> 2:17:10.880
 and learning the abstract representation.

2:17:10.880 --> 2:17:13.240
 So if you do sports, say, take, for example,

2:17:13.240 --> 2:17:15.280
 serving in tennis, right?

2:17:15.280 --> 2:17:17.680
 I mean, my tennis serve is OK, not great,

2:17:17.680 --> 2:17:19.800
 but I learned it by trial and error, right?

2:17:19.800 --> 2:17:22.160
 And I mean, I learned music by trial and error, too.

2:17:22.160 --> 2:17:23.960
 I just sit down and play.

2:17:23.960 --> 2:17:27.080
 But then if you're an athlete, which I'm not a good athlete,

2:17:27.080 --> 2:17:30.400
 I mean, then you'll watch videos of yourself serving,

2:17:30.400 --> 2:17:32.840
 and your coach will help you think about what you're doing,

2:17:32.840 --> 2:17:35.040
 and you'll then form a declarative representation.

2:17:35.040 --> 2:17:37.720
 But your cerebellum maybe didn't have a declarative

2:17:37.720 --> 2:17:38.560
 representation.

2:17:38.560 --> 2:17:43.600
 Same way with music, like, I will hear something in my head.

2:17:43.600 --> 2:17:46.960
 I'll sit down and play the thing like I heard it.

2:17:46.960 --> 2:17:51.320
 And then I will try to study what my fingers did to see,

2:17:51.320 --> 2:17:52.800
 like, what did you just play?

2:17:52.800 --> 2:17:55.640
 Like, how did you do that, right?

2:17:55.640 --> 2:17:57.960
 Because if you're composing, you may

2:17:57.960 --> 2:18:02.200
 want to see how you did it, and then declaratively morph that

2:18:02.200 --> 2:18:05.240
 in some way that your fingers wouldn't think of, right?

2:18:05.240 --> 2:18:09.360
 But the physiological movement may come out

2:18:09.360 --> 2:18:13.400
 of some opaque, like, cerebellum reinforcement

2:18:13.400 --> 2:18:14.440
 learned thing, right?

2:18:14.440 --> 2:18:18.080
 And so that's, I think, trying to milk the structure of a neural

2:18:18.080 --> 2:18:21.040
 net by treating it as an oracle, maybe more like how

2:18:21.040 --> 2:18:23.960
 your declarative mind post processes,

2:18:23.960 --> 2:18:27.560
 what your visual or motor cortex.

2:18:27.560 --> 2:18:29.400
 I mean, in vision, it's the same way,

2:18:29.400 --> 2:18:35.280
 like, you can recognize beautiful art much better

2:18:35.280 --> 2:18:38.480
 than you can say why you think that piece of art is beautiful.

2:18:38.480 --> 2:18:40.680
 But if you're trained as an art critic,

2:18:40.680 --> 2:18:41.680
 you do learn to say why.

2:18:41.680 --> 2:18:44.040
 And some of it's bullshit, but some of it isn't, right?

2:18:44.040 --> 2:18:46.840
 Some of it is learning to map sensory knowledge

2:18:46.840 --> 2:18:51.120
 into declarative and linguistic knowledge,

2:18:51.120 --> 2:18:56.040
 yet without necessarily making the sensory system itself

2:18:56.040 --> 2:19:00.640
 use a transparent and easily communicable representation.

2:19:00.640 --> 2:19:01.680
 Yeah, that's fascinating.

2:19:01.680 --> 2:19:05.440
 To think of neural networks as, like, dumb question

2:19:05.440 --> 2:19:10.920
 answers that you can just milk to build up a knowledge base.

2:19:10.920 --> 2:19:12.640
 And then it could be multiple networks, I suppose,

2:19:12.640 --> 2:19:13.600
 from different.

2:19:13.600 --> 2:19:14.120
 Yeah, yeah.

2:19:14.120 --> 2:19:18.320
 So I think if a group like DeepMind or OpenAI were

2:19:18.320 --> 2:19:22.320
 to build AGI, and I think DeepMind is like 1,000 times

2:19:22.320 --> 2:19:25.880
 more likely from where I could tell,

2:19:25.880 --> 2:19:30.040
 because they've hired a lot of people with broad minds

2:19:30.040 --> 2:19:34.400
 and many different approaches and angles on AGI.

2:19:34.400 --> 2:19:36.880
 Whereas OpenAI is also awesome, but I

2:19:36.880 --> 2:19:40.000
 see them as more of like a pure deep reinforcement

2:19:40.000 --> 2:19:40.920
 learning shock.

2:19:40.920 --> 2:19:41.560
 Yeah, this time.

2:19:41.560 --> 2:19:42.240
 I got you.

2:19:42.240 --> 2:19:42.760
 So far.

2:19:42.760 --> 2:19:44.440
 Yeah, there's a lot of, you're right.

2:19:44.440 --> 2:19:48.920
 There's, I mean, there's so much interdisciplinary work

2:19:48.920 --> 2:19:50.280
 at DeepMind, like Neuroscience.

2:19:50.280 --> 2:19:52.480
 And you put that together with Google Brain, which,

2:19:52.480 --> 2:19:54.800
 granted, they're not working that closely together now.

2:19:54.800 --> 2:19:57.200
 But my oldest son, Zarathustra, is

2:19:57.200 --> 2:20:00.160
 doing his PhD in machine learning applied

2:20:00.160 --> 2:20:03.880
 to automated theorem proving in Prague under Joseph Erbon.

2:20:03.880 --> 2:20:06.720
 So the first paper, DeepMath, which

2:20:06.720 --> 2:20:09.440
 applied deep neural nets to guide theorem proving

2:20:09.440 --> 2:20:10.760
 was out of Google Brain.

2:20:10.760 --> 2:20:14.960
 I mean, by now, the automated theorem proving community

2:20:14.960 --> 2:20:18.400
 is going way, way, way beyond anything Google was doing.

2:20:18.400 --> 2:20:20.360
 But still, yeah.

2:20:20.360 --> 2:20:23.760
 But anyway, if that community was going to make an AGI,

2:20:23.760 --> 2:20:29.280
 probably one way they would do it was take 25 different neural

2:20:29.280 --> 2:20:31.720
 modules, architected in different ways,

2:20:31.720 --> 2:20:33.800
 maybe resembling different parts of the brain,

2:20:33.800 --> 2:20:36.280
 like a basal ganglia model, cerebellum model,

2:20:36.280 --> 2:20:40.480
 a thalamus model, a few hippocampus models,

2:20:40.480 --> 2:20:43.000
 number of different models representing parts of the cortex,

2:20:43.000 --> 2:20:43.680
 right?

2:20:43.680 --> 2:20:49.960
 Take all of these and then wire them together to co train

2:20:49.960 --> 2:20:52.560
 and learn them together like that.

2:20:52.560 --> 2:20:57.280
 That would be an approach to creating an AGI.

2:20:57.280 --> 2:20:59.640
 One could implement something like that efficiently

2:20:59.640 --> 2:21:03.800
 on top of our true AGI, like OpenCog 2.0 system,

2:21:03.800 --> 2:21:05.480
 once it exists.

2:21:05.480 --> 2:21:08.800
 Although, obviously, Google has their own highly efficient

2:21:08.800 --> 2:21:10.240
 implementation architecture.

2:21:10.240 --> 2:21:13.240
 So I think that's a decent way to build AGI.

2:21:13.240 --> 2:21:15.640
 I was very interested in that in the mid 90s.

2:21:15.640 --> 2:21:19.440
 But I mean, the knowledge about how the brain works

2:21:19.440 --> 2:21:20.360
 sort of pissed me off.

2:21:20.360 --> 2:21:21.520
 Like, it wasn't there yet.

2:21:21.520 --> 2:21:23.080
 Like, you know, in the hippocampus,

2:21:23.080 --> 2:21:25.360
 you have these concept neurons, like the so called

2:21:25.360 --> 2:21:27.720
 grandmother neuron, which everyone laughed at.

2:21:27.720 --> 2:21:28.520
 It's actually there.

2:21:28.520 --> 2:21:31.040
 Like, I have some Lex Friedman neurons

2:21:31.040 --> 2:21:33.520
 that fire differentially when I see you and not

2:21:33.520 --> 2:21:34.960
 when I see any other person, right?

2:21:34.960 --> 2:21:35.360
 Yeah.

2:21:35.360 --> 2:21:38.880
 So how do these Lex Friedman neurons,

2:21:38.880 --> 2:21:41.400
 how do they coordinate with the distributed representation

2:21:41.400 --> 2:21:44.520
 of Lex Friedman I have in my cortex, right?

2:21:44.520 --> 2:21:47.680
 There's some back and forth in cortex and hippocampus

2:21:47.680 --> 2:21:50.080
 that lets these discrete symbolic representations

2:21:50.080 --> 2:21:53.200
 in hippocampus correlate and cooperate

2:21:53.200 --> 2:21:55.680
 with the distributed representations in cortex.

2:21:55.680 --> 2:21:58.080
 This probably has to do with how the brain does its version

2:21:58.080 --> 2:22:00.240
 of abstraction and quantifier logic, right?

2:22:00.240 --> 2:22:02.440
 Like, you can have a single neuron in the hippocampus

2:22:02.440 --> 2:22:05.840
 that activates the whole distributed activation pattern

2:22:05.840 --> 2:22:06.880
 in cortex.

2:22:06.880 --> 2:22:09.040
 Well, this may be how the brain does,

2:22:09.040 --> 2:22:11.080
 like, symbolization and abstraction

2:22:11.080 --> 2:22:14.240
 as in functional programming or something.

2:22:14.240 --> 2:22:15.320
 But we can't measure it.

2:22:15.320 --> 2:22:17.240
 Like, we don't have enough electrodes

2:22:17.240 --> 2:22:20.920
 stuck between the cortex and the hippocampus

2:22:20.920 --> 2:22:23.040
 in any known experiment to measure it.

2:22:23.040 --> 2:22:26.320
 So I got frustrated with that direction,

2:22:26.320 --> 2:22:27.560
 not because it's impossible.

2:22:27.560 --> 2:22:29.640
 Because we just don't understand enough yet.

2:22:29.640 --> 2:22:31.760
 Of course, it's a valid research direction.

2:22:31.760 --> 2:22:33.680
 You can try to understand more and more.

2:22:33.680 --> 2:22:35.800
 And we are measuring more and more about what

2:22:35.800 --> 2:22:38.120
 happens in the brain now than ever before.

2:22:38.120 --> 2:22:40.520
 So it's quite interesting.

2:22:40.520 --> 2:22:45.680
 On the other hand, I sort of got more of an engineering mindset

2:22:45.680 --> 2:22:46.440
 about AGI.

2:22:46.440 --> 2:22:50.120
 I'm like, well, OK, we don't know how the brain works that well.

2:22:50.120 --> 2:22:52.320
 We don't know how birds fly that well yet either.

2:22:52.320 --> 2:22:54.040
 We have no idea how a hummingbird flies,

2:22:54.040 --> 2:22:56.240
 in terms of the aerodynamics of it.

2:22:56.240 --> 2:23:00.080
 On the other hand, we know basic principles of like flapping

2:23:00.080 --> 2:23:01.720
 and pushing the air down.

2:23:01.720 --> 2:23:03.680
 And we know the basic principles of how

2:23:03.680 --> 2:23:05.680
 the different parts of the brain work.

2:23:05.680 --> 2:23:07.440
 So let's take those basic principles

2:23:07.440 --> 2:23:11.400
 and engineer something that embodies those basic principles,

2:23:11.400 --> 2:23:16.760
 but is well designed for the hardware that we have on hand

2:23:16.760 --> 2:23:18.040
 right now.

2:23:18.040 --> 2:23:20.720
 Yes, so do you think we can create AGI before we

2:23:20.720 --> 2:23:22.400
 understand how the brain works?

2:23:22.400 --> 2:23:25.040
 I think that's probably what will happen.

2:23:25.040 --> 2:23:28.600
 And maybe the AGI will help us do better brain imaging that

2:23:28.600 --> 2:23:31.040
 will then let us build artificial humans, which

2:23:31.040 --> 2:23:34.960
 is very, very interesting to us, because we are humans, right?

2:23:34.960 --> 2:23:38.800
 I mean, building artificial humans is super worthwhile.

2:23:38.800 --> 2:23:42.720
 I just think it's probably not the shortest path to AGI.

2:23:42.720 --> 2:23:45.640
 So it's a fascinating idea that we would build AGI

2:23:45.640 --> 2:23:47.040
 to help us understand ourselves.

2:23:50.040 --> 2:23:55.120
 A lot of people ask me if young people interested

2:23:55.120 --> 2:23:59.440
 in doing artificial intelligence, they look at sort of doing

2:23:59.440 --> 2:24:04.280
 graduate level, even undergrads, but graduate level research.

2:24:04.280 --> 2:24:06.720
 They see what the artificial intelligence community stands

2:24:06.720 --> 2:24:07.220
 now.

2:24:07.220 --> 2:24:10.120
 It's not really AGI type research for the most part.

2:24:10.120 --> 2:24:13.120
 So the natural question they ask is, what advice would you

2:24:13.120 --> 2:24:13.880
 give?

2:24:13.880 --> 2:24:17.000
 I mean, maybe I could ask, if people

2:24:17.000 --> 2:24:22.200
 were interested in working on OpenCog or in some kind of

2:24:22.200 --> 2:24:25.400
 direct or indirect connection to OpenCog or AGI research,

2:24:25.400 --> 2:24:28.280
 what would you recommend?

2:24:28.280 --> 2:24:31.160
 OpenCog, first of all, is open source project.

2:24:31.160 --> 2:24:35.520
 There's a Google Group discussion list.

2:24:35.520 --> 2:24:36.920
 There's a GitHub repository.

2:24:36.920 --> 2:24:40.960
 So if anyone's interested in lending a hand with that aspect

2:24:40.960 --> 2:24:46.160
 of AGI, introduce yourself on the OpenCog email list.

2:24:46.160 --> 2:24:48.080
 And there's a Slack as well.

2:24:48.080 --> 2:24:53.240
 I mean, we're certainly interested to have inputs

2:24:53.240 --> 2:24:57.640
 into our redesign process for a new version of OpenCog.

2:24:57.640 --> 2:25:01.120
 But also, we're doing a lot of very interesting research

2:25:01.120 --> 2:25:04.960
 I mean, we're working on data analysis for COVID clinical

2:25:04.960 --> 2:25:05.560
 trials.

2:25:05.560 --> 2:25:06.880
 We're working with Hanson Robotics.

2:25:06.880 --> 2:25:09.440
 We're doing a lot of cool things with the current version

2:25:09.440 --> 2:25:10.680
 of OpenCog now.

2:25:10.680 --> 2:25:14.720
 So there's certainly opportunity to jump into OpenCog

2:25:14.720 --> 2:25:18.680
 or various other open source AGI oriented projects.

2:25:18.680 --> 2:25:20.720
 So would you say there's like masters and PhD

2:25:20.720 --> 2:25:22.040
 thesis in there?

2:25:22.040 --> 2:25:23.920
 Plenty, yeah, plenty, of course.

2:25:23.920 --> 2:25:27.240
 I mean, the challenge is to find a supervisor who wants

2:25:27.240 --> 2:25:30.480
 to foster that sort of research, but it's way easier

2:25:30.480 --> 2:25:32.760
 than it was when I got my PhD.

2:25:32.760 --> 2:25:33.400
 So OK, great.

2:25:33.400 --> 2:25:36.320
 We talked about OpenCog, which is kind of one,

2:25:36.320 --> 2:25:41.360
 the software framework, but also the actual attempt

2:25:41.360 --> 2:25:44.160
 to build an AGI system.

2:25:44.160 --> 2:25:48.560
 And then there is this exciting idea of SingularityNet.

2:25:48.560 --> 2:25:53.120
 So maybe can you say first, what is SingularityNet?

2:25:53.120 --> 2:25:54.240
 Sure, sure.

2:25:54.240 --> 2:26:02.280
 SingularityNet is a platform for realizing

2:26:02.280 --> 2:26:08.280
 a decentralized network of artificial intelligences.

2:26:08.280 --> 2:26:14.440
 So Marvin Minsky, the AI pioneer who I knew a little bit,

2:26:14.440 --> 2:26:16.560
 he had the idea of a society of minds,

2:26:16.560 --> 2:26:19.560
 like you should achieve an AI not by writing one algorithm

2:26:19.560 --> 2:26:23.000
 or one program, but you should put a bunch of different AIs

2:26:23.000 --> 2:26:26.840
 out there, and the different AIs will

2:26:26.840 --> 2:26:29.440
 interact with each other, each playing their own role.

2:26:29.440 --> 2:26:32.520
 And then the totality of the society of AIs

2:26:32.520 --> 2:26:35.760
 would be the thing that displayed the human level

2:26:35.760 --> 2:26:36.520
 intelligence.

2:26:36.520 --> 2:26:40.800
 And when he was alive, I had many debates with Marvin

2:26:40.800 --> 2:26:42.960
 about this idea.

2:26:42.960 --> 2:26:49.360
 And I think he really thought the mind was more

2:26:49.360 --> 2:26:51.200
 like a society than I do.

2:26:51.200 --> 2:26:54.160
 Like, I think you could have a mind that

2:26:54.160 --> 2:26:56.680
 was as disorganized as a human society,

2:26:56.680 --> 2:26:58.480
 but I think a human like mind has a bit more

2:26:58.480 --> 2:27:00.360
 central control than that, actually.

2:27:00.360 --> 2:27:04.760
 I mean, we have this Thalamus and the Medulla and limbic system.

2:27:04.760 --> 2:27:07.960
 We have a sort of top down control system

2:27:07.960 --> 2:27:12.800
 that guides much of what we do, more so than a society does.

2:27:12.800 --> 2:27:16.840
 So I think he stretched that metaphor a little too far,

2:27:16.840 --> 2:27:20.760
 but I also think there's something interesting there.

2:27:20.760 --> 2:27:25.880
 And so in the 90s, when I started my first sort

2:27:25.880 --> 2:27:28.520
 of nonacademic AI project, WebMind,

2:27:28.520 --> 2:27:32.160
 which was an AI startup in New York in the Silicon Alley

2:27:32.160 --> 2:27:36.200
 area in the late 90s, what I was aiming to do there

2:27:36.200 --> 2:27:39.920
 was make a distributed society of AIs

2:27:39.920 --> 2:27:42.120
 the different parts of which would live on different computers

2:27:42.120 --> 2:27:43.560
 all around the world.

2:27:43.560 --> 2:27:45.760
 And each one would do its own thinking about the data

2:27:45.760 --> 2:27:46.800
 local to it.

2:27:46.800 --> 2:27:48.920
 But they would all share information with each other

2:27:48.920 --> 2:27:51.240
 and outsource work with each other and cooperate.

2:27:51.240 --> 2:27:54.000
 And the intelligence would be in the whole collective.

2:27:54.000 --> 2:27:56.600
 And I organized a conference together

2:27:56.600 --> 2:27:59.080
 with Francis Heiligen at Free University of Brussels

2:27:59.080 --> 2:28:02.880
 in 2001, which was the Global Brain 0 conference.

2:28:02.880 --> 2:28:05.600
 And we're planning the next version, the Global Brain 1

2:28:05.600 --> 2:28:08.360
 conference, at the Free University of Brussels

2:28:08.360 --> 2:28:11.960
 for next year, 2021, so 20 years after.

2:28:11.960 --> 2:28:14.520
 And then maybe we can have the next one 10 years after that,

2:28:14.520 --> 2:28:18.560
 like exponentially faster until the singularity comes, right?

2:28:18.560 --> 2:28:20.600
 The timing is right, yeah.

2:28:20.600 --> 2:28:22.120
 Yeah, yeah, exactly.

2:28:22.120 --> 2:28:24.920
 So the idea with the Global Brain

2:28:24.920 --> 2:28:28.040
 was maybe the AI won't just be in a program

2:28:28.040 --> 2:28:30.160
 on one guy's computer, but the AI

2:28:30.160 --> 2:28:32.880
 will be in the internet as a whole

2:28:32.880 --> 2:28:35.000
 with the cooperation of different AI modules

2:28:35.000 --> 2:28:37.040
 living in different places.

2:28:37.040 --> 2:28:39.360
 So one of the issues you face when

2:28:39.360 --> 2:28:45.040
 architecting a system like that is how is the whole thing

2:28:45.040 --> 2:28:45.600
 controlled?

2:28:45.600 --> 2:28:48.080
 Do you have a centralized control unit

2:28:48.080 --> 2:28:50.840
 that pulls the puppet strings of all the different modules

2:28:50.840 --> 2:28:51.600
 there?

2:28:51.600 --> 2:28:56.320
 Or do you have a fundamentally decentralized network

2:28:56.320 --> 2:29:00.160
 where the society of AIs is controlled

2:29:00.160 --> 2:29:02.000
 in some democratic and self organized way,

2:29:02.000 --> 2:29:05.680
 but all the AIs in that society, right?

2:29:05.680 --> 2:29:09.560
 And Francis and I had different view of many things,

2:29:09.560 --> 2:29:15.120
 but we both wanted to make like a global society of AI

2:29:15.120 --> 2:29:20.560
 of minds with a decentralized organizational mode.

2:29:20.560 --> 2:29:26.080
 Now, the main difference was he wanted the individual AIs

2:29:26.080 --> 2:29:29.280
 to be all incredibly simple and all the intelligence

2:29:29.280 --> 2:29:31.040
 to be on the collective level.

2:29:31.040 --> 2:29:33.880
 Whereas I thought that was cool, but I

2:29:33.880 --> 2:29:35.440
 thought a more practical way to do it

2:29:35.440 --> 2:29:39.520
 might be if some of the agents in the society of minds

2:29:39.520 --> 2:29:41.520
 were fairly generally intelligent on their own.

2:29:41.520 --> 2:29:44.520
 So like you could have a bunch of open cogs out there

2:29:44.520 --> 2:29:47.160
 and a bunch of simpler learning systems,

2:29:47.160 --> 2:29:49.080
 and then these are all cooperating,

2:29:49.080 --> 2:29:51.800
 coordinating together, sort of like in the brain.

2:29:51.800 --> 2:29:55.320
 OK, the brain as a whole is the general intelligence,

2:29:55.320 --> 2:29:57.000
 but some parts of the cortex, you could say,

2:29:57.000 --> 2:29:59.720
 have a fair bit of general intelligence on their own.

2:29:59.720 --> 2:30:02.120
 Whereas, say, parts of the cerebellum or limbic system

2:30:02.120 --> 2:30:04.560
 have very little general intelligence on their own,

2:30:04.560 --> 2:30:07.280
 and they're contributing to general intelligence

2:30:07.280 --> 2:30:10.880
 by way of their connectivity to other modules.

2:30:10.880 --> 2:30:14.000
 Do you see instantiations of the same kind of maybe

2:30:14.000 --> 2:30:16.120
 different versions of open cog, but also just

2:30:16.120 --> 2:30:18.400
 the same version of open cog, and maybe

2:30:18.400 --> 2:30:21.280
 many instantiations of it as part as being all parts of the brain?

2:30:21.280 --> 2:30:22.680
 That's what David and Hans and I

2:30:22.680 --> 2:30:25.360
 want to do with many Sophia and other robots.

2:30:25.360 --> 2:30:29.200
 Each one has its own individual mind living on a server,

2:30:29.200 --> 2:30:32.080
 but there's also a collective intelligence infusing them

2:30:32.080 --> 2:30:35.480
 and a part of the mind living on the edge in each robot.

2:30:35.480 --> 2:30:40.000
 So the thing is, at that time, as well as web mind being

2:30:40.000 --> 2:30:46.840
 implemented in Java 1.1 as a massive distributed system,

2:30:46.840 --> 2:30:48.120
 blockchain wasn't there yet.

2:30:48.120 --> 2:30:51.840
 So how to have them do this decentralized control?

2:30:51.840 --> 2:30:52.840
 We sort of knew it.

2:30:52.840 --> 2:30:54.320
 We knew about distributed systems.

2:30:54.320 --> 2:30:55.720
 We knew about encryption.

2:30:55.720 --> 2:30:58.040
 So I mean, we had the key principles

2:30:58.040 --> 2:31:00.000
 of what underlies blockchain now.

2:31:00.000 --> 2:31:01.720
 But I mean, we didn't put it together

2:31:01.720 --> 2:31:03.000
 in the way that's been done now.

2:31:03.000 --> 2:31:05.320
 So when Vitalik Buterin and colleagues

2:31:05.320 --> 2:31:09.080
 came out with Ethereum blockchain many, many years

2:31:09.080 --> 2:31:12.480
 later, like 2013 or something, then I was like,

2:31:12.480 --> 2:31:14.040
 well, this is interesting.

2:31:14.040 --> 2:31:16.960
 This is Solidity scripting language.

2:31:16.960 --> 2:31:18.520
 It's kind of dorky in a way.

2:31:18.520 --> 2:31:21.400
 And I don't see why you need a turn complete language

2:31:21.400 --> 2:31:22.440
 for this purpose.

2:31:22.440 --> 2:31:25.600
 But on the other hand, this is the first time

2:31:25.600 --> 2:31:29.880
 I could sit down and start to script infrastructure

2:31:29.880 --> 2:31:34.360
 for decentralized control of the AIs in the society of minds

2:31:34.360 --> 2:31:35.360
 in a tractable way.

2:31:35.360 --> 2:31:37.240
 You could hack the Bitcoin code base,

2:31:37.240 --> 2:31:39.840
 but it's really annoying, whereas Solidity

2:31:39.840 --> 2:31:41.680
 is Ethereum scripting language.

2:31:41.680 --> 2:31:44.400
 It is just nicer and easier to use.

2:31:44.400 --> 2:31:45.840
 I'm very annoyed with it by this point.

2:31:45.840 --> 2:31:48.320
 But like Java, I mean, these languages

2:31:48.320 --> 2:31:50.920
 are amazing when they first come out.

2:31:50.920 --> 2:31:52.800
 So then I came up with the idea that turned

2:31:52.800 --> 2:31:53.800
 into singularity net.

2:31:53.800 --> 2:31:58.160
 OK, let's make a decentralized agent system

2:31:58.160 --> 2:32:00.960
 where a bunch of different AIs wrapped up

2:32:00.960 --> 2:32:04.320
 in, say, different Docker containers or LXC containers.

2:32:04.320 --> 2:32:07.400
 Different AIs can each of them have their own identity

2:32:07.400 --> 2:32:08.720
 on the blockchain.

2:32:08.720 --> 2:32:11.760
 And the coordination of this community of AIs

2:32:11.760 --> 2:32:14.560
 has no central controller, no dictator.

2:32:14.560 --> 2:32:17.080
 And there's no central repository of information.

2:32:17.080 --> 2:32:19.360
 The coordination of the society of minds

2:32:19.360 --> 2:32:22.640
 is done entirely by the decentralized network

2:32:22.640 --> 2:32:25.800
 in a decentralized way by the algorithms.

2:32:25.800 --> 2:32:29.200
 Because the model of Bitcoin is in math we trust.

2:32:29.200 --> 2:32:30.800
 And so that's what you need.

2:32:30.800 --> 2:32:33.840
 You need the society of minds to trust only in math,

2:32:33.840 --> 2:32:37.680
 not trust only in one centralized server.

2:32:37.680 --> 2:32:40.600
 So the AI systems themselves are outside of the blockchain,

2:32:40.600 --> 2:32:42.040
 but then the communication between them.

2:32:42.040 --> 2:32:43.920
 At the moment, yeah, yeah.

2:32:43.920 --> 2:32:46.880
 I would have loved to put the AI's operations on chain

2:32:46.880 --> 2:32:48.680
 in some sense.

2:32:48.680 --> 2:32:50.480
 But in Ethereum, it's just too slow.

2:32:50.480 --> 2:32:52.680
 You can't still do it.

2:32:52.680 --> 2:32:55.080
 Somehow, it's the basic communication

2:32:55.080 --> 2:32:57.040
 between AI systems that's distributed.

2:32:57.040 --> 2:32:57.520
 Yeah, yeah.

2:32:57.520 --> 2:33:02.520
 So basically, an AI is just some software in singularity.

2:33:02.520 --> 2:33:05.920
 And AI is just some software process living in a container.

2:33:05.920 --> 2:33:09.280
 And there's a proxy that lives in that container along

2:33:09.280 --> 2:33:10.840
 with the AI that handles the interaction

2:33:10.840 --> 2:33:13.120
 with the rest of singularity net.

2:33:13.120 --> 2:33:16.400
 And then when one AI wants to contribute with another one

2:33:16.400 --> 2:33:18.640
 in the network, they set up a number of channels.

2:33:18.640 --> 2:33:22.560
 And the setup of those channels uses the Ethereum blockchain.

2:33:22.560 --> 2:33:24.480
 But once the channels are set up,

2:33:24.480 --> 2:33:26.120
 then data flows along those channels

2:33:26.120 --> 2:33:29.240
 without having to be on the blockchain.

2:33:29.240 --> 2:33:31.040
 All that goes on the blockchain is the fact

2:33:31.040 --> 2:33:33.160
 that some data went along that channel.

2:33:33.160 --> 2:33:34.240
 So you can do.

2:33:34.240 --> 2:33:38.720
 So there's not a shared knowledge.

2:33:38.720 --> 2:33:43.120
 Well, the identity of each agent is on the blockchain,

2:33:43.120 --> 2:33:44.800
 on the Ethereum blockchain.

2:33:44.800 --> 2:33:48.000
 If one agent rates the reputation of another agent,

2:33:48.000 --> 2:33:49.560
 that goes on the blockchain.

2:33:49.560 --> 2:33:52.040
 And agents can publish what APIs they

2:33:52.040 --> 2:33:54.480
 will fulfill on the blockchain.

2:33:54.480 --> 2:33:58.040
 But the actual data for AI and the results for AI

2:33:58.040 --> 2:33:58.840
 is not on the blockchain.

2:33:58.840 --> 2:33:59.680
 Do you think it could be?

2:33:59.680 --> 2:34:00.960
 Do you think it should be?

2:34:00.960 --> 2:34:04.080
 Um, in some cases, it should be.

2:34:04.080 --> 2:34:05.840
 In some cases, maybe it shouldn't be.

2:34:05.840 --> 2:34:09.240
 But I mean, I think that.

2:34:09.240 --> 2:34:10.080
 So I'll give you an example.

2:34:10.080 --> 2:34:11.600
 Using Ethereum, you can't do it.

2:34:11.600 --> 2:34:15.720
 Using now, there's more modern and faster

2:34:15.720 --> 2:34:21.880
 blockchains where you could start to do that in some cases.

2:34:21.880 --> 2:34:23.320
 Two years ago, that was less so.

2:34:23.320 --> 2:34:25.560
 It's a very rapidly evolving ecosystem.

2:34:25.560 --> 2:34:29.240
 So like one example, maybe you can comment on something

2:34:29.240 --> 2:34:31.800
 I worked a lot on is autonomous vehicles.

2:34:31.800 --> 2:34:35.680
 You can see each individual vehicle as an AI system.

2:34:35.680 --> 2:34:39.520
 And you can see vehicles from Tesla, for example,

2:34:39.520 --> 2:34:45.640
 and then Ford and GM and all these as also like larger.

2:34:45.640 --> 2:34:47.960
 I mean, they all are running the same kind of system

2:34:47.960 --> 2:34:50.200
 on each sets of vehicles.

2:34:50.200 --> 2:34:53.280
 So it's individual AI systems and individual vehicles,

2:34:53.280 --> 2:34:54.680
 but it's all different.

2:34:54.680 --> 2:34:58.440
 The satiation is the same AI system within the same company.

2:34:58.440 --> 2:35:03.240
 So you can envision a situation where all of those AI systems

2:35:03.240 --> 2:35:06.360
 are put on the singularity net, right?

2:35:06.360 --> 2:35:11.080
 And how do you see that happening

2:35:11.080 --> 2:35:12.520
 and what would be the benefit?

2:35:12.520 --> 2:35:14.160
 And could they share data?

2:35:14.160 --> 2:35:15.520
 I guess one of the biggest things

2:35:15.520 --> 2:35:18.760
 is that the power there is in the decentralized control.

2:35:18.760 --> 2:35:22.160
 But the benefit would have been, is really nice

2:35:22.160 --> 2:35:25.960
 if they can somehow share the knowledge in an open way

2:35:25.960 --> 2:35:27.040
 if they choose to.

2:35:27.040 --> 2:35:30.640
 Yeah, yeah, yeah, those are all quite good points.

2:35:30.640 --> 2:35:38.480
 So I think the benefit from being on the decentralized network

2:35:38.480 --> 2:35:42.040
 as we envision it is that we want the AIs and the network

2:35:42.040 --> 2:35:44.320
 to be outsourcing work to each other

2:35:44.320 --> 2:35:48.000
 and making API calls to each other frequently.

2:35:48.000 --> 2:35:51.360
 So the real benefit would be if that AI wanted

2:35:51.360 --> 2:35:54.800
 to outsource some cognitive processing or data

2:35:54.800 --> 2:35:57.200
 processing or data preprocessing whatever

2:35:57.200 --> 2:36:00.040
 to some other AIs in the network, which

2:36:00.040 --> 2:36:02.160
 specialize in something different.

2:36:02.160 --> 2:36:06.160
 And this really requires a different way

2:36:06.160 --> 2:36:08.520
 of thinking about AI software development, right?

2:36:08.520 --> 2:36:10.880
 So just like object oriented programming

2:36:10.880 --> 2:36:13.240
 was different than imperative programming.

2:36:13.240 --> 2:36:15.680
 And now object oriented programmers

2:36:15.680 --> 2:36:19.560
 all use these frameworks to do things rather than just

2:36:19.560 --> 2:36:21.480
 libraries even.

2:36:21.480 --> 2:36:23.600
 Shifting to agent based programming

2:36:23.600 --> 2:36:28.320
 where AI agent is asking other live real time evolving agents

2:36:28.320 --> 2:36:30.480
 for feedback in what they're doing,

2:36:30.480 --> 2:36:32.000
 that's a different way of thinking.

2:36:32.000 --> 2:36:33.480
 I mean, it's not a new one.

2:36:33.480 --> 2:36:35.800
 There was loads of papers on agent based programming

2:36:35.800 --> 2:36:37.600
 in the 80s and onward.

2:36:37.600 --> 2:36:41.480
 But if you're willing to shift to an agent based model

2:36:41.480 --> 2:36:45.880
 of development, then you can put less and less in your AI

2:36:45.880 --> 2:36:49.880
 and rely more and more on interactive calls to other AIs

2:36:49.880 --> 2:36:51.440
 running in the network.

2:36:51.440 --> 2:36:54.560
 And of course, that's not fully manifested yet

2:36:54.560 --> 2:36:57.320
 because although we've rolled out a nice working

2:36:57.320 --> 2:37:00.400
 version of SingularNet Platform, there's

2:37:00.400 --> 2:37:03.800
 only 5,200 AIs running in there now.

2:37:03.800 --> 2:37:05.880
 There's not tens of thousands of AIs.

2:37:05.880 --> 2:37:08.240
 So we don't have the critical mass

2:37:08.240 --> 2:37:11.840
 for the whole society of mind to be doing what we want.

2:37:11.840 --> 2:37:13.560
 Yeah, the magic really happens when

2:37:13.560 --> 2:37:15.280
 there's just a huge number of agents.

2:37:15.280 --> 2:37:16.080
 Yeah, yeah, yeah.

2:37:16.080 --> 2:37:16.680
 Exactly.

2:37:16.680 --> 2:37:19.600
 In terms of data, we're partnering closely

2:37:19.600 --> 2:37:23.560
 with another blockchain project called Ocean Protocol.

2:37:23.560 --> 2:37:27.240
 An Ocean Protocol, that's the project of Trent McConaughey,

2:37:27.240 --> 2:37:30.160
 who developed Big Chain DB, which is a blockchain based

2:37:30.160 --> 2:37:30.800
 database.

2:37:30.800 --> 2:37:35.400
 So Ocean Protocol is basically blockchain based big data.

2:37:35.400 --> 2:37:38.800
 It names it making it efficient for different AI

2:37:38.800 --> 2:37:40.800
 processes or statistical processes

2:37:40.800 --> 2:37:45.240
 or whatever to share large data sets or one process

2:37:45.240 --> 2:37:47.840
 can send a clone of itself to work on the other guy's data

2:37:47.840 --> 2:37:49.480
 set and send results back and so

2:37:49.480 --> 2:37:50.560
 forth.

2:37:50.560 --> 2:37:55.480
 So by getting Ocean and you have data lake,

2:37:55.480 --> 2:37:56.880
 so this is the data ocean, right?

2:37:56.880 --> 2:37:59.680
 So again, by getting Ocean and SingularNet

2:37:59.680 --> 2:38:03.040
 to interoperate, we're aiming to take

2:38:03.040 --> 2:38:05.920
 into account of the big data aspect also.

2:38:05.920 --> 2:38:09.000
 But it's quite challenging because to build

2:38:09.000 --> 2:38:10.920
 this whole decentralized blockchain based

2:38:10.920 --> 2:38:14.160
 infrastructure, your competitors are like Google,

2:38:14.160 --> 2:38:18.120
 Microsoft, Alibaba, and Amazon, which have so much money

2:38:18.120 --> 2:38:20.600
 behind their centralized infrastructures,

2:38:20.600 --> 2:38:23.360
 plus they're solving simpler algorithmic problems

2:38:23.360 --> 2:38:27.400
 because making it centralized in some ways is easier, right?

2:38:27.400 --> 2:38:32.680
 So they're very major computer science challenges.

2:38:32.680 --> 2:38:36.040
 And I think what you saw with the whole ICO boom

2:38:36.040 --> 2:38:38.240
 in the blockchain and cryptocurrency world

2:38:38.240 --> 2:38:43.560
 is a lot of young hackers who are hacking Bitcoin or Ethereum,

2:38:43.560 --> 2:38:45.760
 and they see, well, why don't we make this decentralized

2:38:45.760 --> 2:38:49.080
 on blockchain, then after they raise some money through an ICO,

2:38:49.080 --> 2:38:50.240
 they realize how hard it is.

2:38:50.240 --> 2:38:52.400
 It's like, actually, we're wrestling

2:38:52.400 --> 2:38:55.640
 with incredibly hard computer science and software

2:38:55.640 --> 2:39:00.040
 engineering and distributed systems problems which

2:39:00.040 --> 2:39:03.360
 can be solved, but they're just very difficult to solve.

2:39:03.360 --> 2:39:05.640
 And in some cases, the individuals

2:39:05.640 --> 2:39:09.040
 who started those projects were not well equipped

2:39:09.040 --> 2:39:12.560
 to actually solve the problems that they wanted to do.

2:39:12.560 --> 2:39:14.880
 So you think, would you say that's the main bottleneck?

2:39:14.880 --> 2:39:21.320
 If you look at the future of currency, the question is, well,

2:39:21.320 --> 2:39:24.160
 Currency, the main bottleneck is politics.

2:39:24.160 --> 2:39:26.720
 It's government, and the bands of armed thugs

2:39:26.720 --> 2:39:30.120
 that will shoot you if you bypass their currency restriction.

2:39:30.120 --> 2:39:30.600
 That's right.

2:39:30.600 --> 2:39:33.680
 So your sense is that versus the technical challenges,

2:39:33.680 --> 2:39:36.080
 because you just suggested the technical challenges are

2:39:36.080 --> 2:39:36.840
 quite high as well.

2:39:36.840 --> 2:39:39.280
 I mean, for making a distributed money,

2:39:39.280 --> 2:39:41.560
 you could do that on Algorand right now.

2:39:41.560 --> 2:39:45.960
 I mean, while Ethereum is too slow, there's Algorand,

2:39:45.960 --> 2:39:48.120
 and there's a few other more modern, more scalable

2:39:48.120 --> 2:39:52.920
 blockchains that would work fine for a decentralized global

2:39:52.920 --> 2:39:53.880
 currency.

2:39:53.880 --> 2:39:56.480
 So I think there were technical bottlenecks

2:39:56.480 --> 2:39:59.400
 to that two years ago, and maybe Ethereum 2.0

2:39:59.400 --> 2:40:00.800
 will be as fast as Algorand.

2:40:00.800 --> 2:40:01.960
 I don't know.

2:40:01.960 --> 2:40:04.120
 That's not fully written yet, right?

2:40:04.120 --> 2:40:08.840
 So I think the obstacle to currency being put on the blockchain

2:40:08.840 --> 2:40:11.520
 is that the currency will be on the blockchain.

2:40:11.520 --> 2:40:13.840
 It'll just be on the blockchain in a way

2:40:13.840 --> 2:40:17.320
 that enforces centralized control and government hedge

2:40:17.320 --> 2:40:18.920
 money rather than otherwise.

2:40:18.920 --> 2:40:22.160
 ER&P will probably be the first currency on the blockchain.

2:40:22.160 --> 2:40:23.320
 The Eruble maybe next.

2:40:23.320 --> 2:40:24.320
 Eruble?

2:40:24.320 --> 2:40:25.600
 Yeah, yeah, yeah.

2:40:25.600 --> 2:40:27.240
 I mean, that's hilarious.

2:40:27.240 --> 2:40:30.680
 Digital currency makes total sense,

2:40:30.680 --> 2:40:32.160
 but they would rather do it in the way

2:40:32.160 --> 2:40:34.680
 that Putin and Xi Jinping have access

2:40:34.680 --> 2:40:37.800
 to the global keys for everything, right?

2:40:37.800 --> 2:40:42.000
 So and then the analogy to that in terms of Singularity Net.

2:40:42.000 --> 2:40:43.600
 I mean, there's echoes.

2:40:43.600 --> 2:40:46.920
 I think you've mentioned before that Linux gives you hope.

2:40:46.920 --> 2:40:49.920
 AI is not as heavily regulated as money, right?

2:40:49.920 --> 2:40:50.960
 Not yet, right?

2:40:50.960 --> 2:40:51.920
 Not yet.

2:40:51.920 --> 2:40:54.200
 Oh, that's a lot slipperier than money, too, right?

2:40:54.200 --> 2:40:58.200
 I mean, money is easier to regulate

2:40:58.200 --> 2:41:00.720
 because it's kind of easier to define,

2:41:00.720 --> 2:41:04.000
 whereas AI is it's almost everywhere inside everything.

2:41:04.000 --> 2:41:06.400
 Where's the boundary between AI and software, right?

2:41:06.400 --> 2:41:09.160
 I mean, if you're going to regulate AI,

2:41:09.160 --> 2:41:11.680
 there's no IQ test for every hardware device

2:41:11.680 --> 2:41:12.800
 that has a learning algorithm.

2:41:12.800 --> 2:41:15.680
 You're going to be putting like head demonic regulation

2:41:15.680 --> 2:41:16.720
 on all software.

2:41:16.720 --> 2:41:19.040
 And I don't rule out that that could happen.

2:41:19.040 --> 2:41:21.040
 Any adaptive software.

2:41:21.040 --> 2:41:23.840
 Yeah, but how do you tell if software is adaptive?

2:41:23.840 --> 2:41:26.040
 Every software is going to be adaptive, I mean.

2:41:26.040 --> 2:41:31.080
 Well, maybe we're living in the golden age of open source

2:41:31.080 --> 2:41:33.320
 that will not always be open.

2:41:33.320 --> 2:41:36.160
 Maybe it'll become centralized control of software

2:41:36.160 --> 2:41:37.000
 by government.

2:41:37.000 --> 2:41:38.800
 It is entirely possible.

2:41:38.800 --> 2:41:42.560
 And part of what I think we're doing with things

2:41:42.560 --> 2:41:49.440
 like SingularityNet protocol is creating a toolset that

2:41:49.440 --> 2:41:52.760
 can be used to counteract that sort of thing.

2:41:52.760 --> 2:41:55.640
 Say a similar thing about mesh networking, right?

2:41:55.640 --> 2:41:57.960
 Plays a minor role now, the ability

2:41:57.960 --> 2:42:01.000
 to access internet directly phone to phone.

2:42:01.000 --> 2:42:03.480
 On the other hand, if your government starts

2:42:03.480 --> 2:42:06.080
 trying to control your use of the internet,

2:42:06.080 --> 2:42:09.240
 suddenly having mesh networking there

2:42:09.240 --> 2:42:10.800
 can be very convenient, right?

2:42:10.800 --> 2:42:15.960
 And so right now, something like a decentralized blockchain

2:42:15.960 --> 2:42:21.200
 based AGI framework or narrow AI framework, it's cool.

2:42:21.200 --> 2:42:22.680
 It's nice to have.

2:42:22.680 --> 2:42:23.960
 On the other hand, if government

2:42:23.960 --> 2:42:27.840
 start trying to tamp down on my AI,

2:42:27.840 --> 2:42:31.440
 interoperating with someone's AI in Russia or somewhere,

2:42:31.440 --> 2:42:35.520
 then suddenly having a decentralized protocol

2:42:35.520 --> 2:42:39.760
 that nobody owns or controls becomes an extremely valuable

2:42:39.760 --> 2:42:41.160
 part of the toolset.

2:42:41.160 --> 2:42:43.800
 And we've put that out there now.

2:42:43.800 --> 2:42:46.960
 It's not perfect, but it operates.

2:42:46.960 --> 2:42:51.120
 And it's pretty blockchain agnostic.

2:42:51.120 --> 2:42:54.640
 So we're talking to Algorand about making part of SingularityNet

2:42:54.640 --> 2:42:56.240
 run on Algorand.

2:42:56.240 --> 2:43:00.040
 My good friend Tufi Saliba has a cool blockchain project

2:43:00.040 --> 2:43:02.240
 called TOTA, which is a blockchain

2:43:02.240 --> 2:43:03.520
 without a distributed ledger.

2:43:03.520 --> 2:43:05.160
 It's like a whole other architecture.

2:43:05.160 --> 2:43:07.920
 So there's a lot of more advanced things

2:43:07.920 --> 2:43:09.840
 you can do in the blockchain world.

2:43:09.840 --> 2:43:14.680
 SingularityNet could be made multi chain,

2:43:14.680 --> 2:43:17.080
 important to a whole bunch of different blockchains.

2:43:17.080 --> 2:43:21.520
 And there's a lot of potential and a lot of importance

2:43:21.520 --> 2:43:23.600
 to putting this kind of toolset out there.

2:43:23.600 --> 2:43:27.960
 If you compare the OpenCog, what you could see is OpenCog

2:43:27.960 --> 2:43:32.400
 allows tight integration of a few AI algorithms that

2:43:32.400 --> 2:43:36.880
 share the same knowledge store in real time, in RAM.

2:43:36.880 --> 2:43:40.960
 SingularityNet allows loose integration

2:43:40.960 --> 2:43:42.440
 of multiple different AIs.

2:43:42.440 --> 2:43:45.240
 They can share knowledge, but they're mostly

2:43:45.240 --> 2:43:50.040
 not going to be sharing knowledge in RAM on the same machine.

2:43:50.040 --> 2:43:52.240
 And I think what we're going to have

2:43:52.240 --> 2:43:54.800
 is a network of networks.

2:43:54.800 --> 2:44:00.960
 I mean, you have the Knowledge Graph inside the OpenCog system.

2:44:00.960 --> 2:44:04.200
 And then you have a network of machines inside a distributed

2:44:04.200 --> 2:44:05.880
 OpenCog mind.

2:44:05.880 --> 2:44:10.240
 But then that OpenCog will interface with other AIs,

2:44:10.240 --> 2:44:14.400
 doing deep neural nets or custom biology data analysis

2:44:14.400 --> 2:44:17.760
 or whatever they're doing in SingularityNet, which

2:44:17.760 --> 2:44:21.400
 is a looser integration of different AIs, some of which

2:44:21.400 --> 2:44:24.040
 may be their own networks.

2:44:24.040 --> 2:44:27.880
 And I think at a very loose analogy,

2:44:27.880 --> 2:44:29.520
 you could see that in the human body.

2:44:29.520 --> 2:44:34.240
 The brain has regions like cortex or hippocampus, which

2:44:34.240 --> 2:44:36.800
 tightly interconnects like conical columns

2:44:36.800 --> 2:44:39.120
 within the cortex, for example.

2:44:39.120 --> 2:44:41.760
 Then there's looser connection within the different lobes

2:44:41.760 --> 2:44:42.640
 of the brain.

2:44:42.640 --> 2:44:45.000
 And then the brain interconnects with the endocrine system

2:44:45.000 --> 2:44:48.240
 and different parts of the body even more loosely.

2:44:48.240 --> 2:44:50.760
 Then your body interacts even more loosely

2:44:50.760 --> 2:44:53.280
 with the other people that you talk to.

2:44:53.280 --> 2:44:56.400
 So you often have networks within networks within networks

2:44:56.400 --> 2:45:01.320
 with progressively looser coupling as you get higher up

2:45:01.320 --> 2:45:02.720
 in that hierarchy.

2:45:02.720 --> 2:45:03.800
 I mean, you have that in biology.

2:45:03.800 --> 2:45:08.120
 You have that in the internet as just networking medium.

2:45:08.120 --> 2:45:09.840
 And I think that's what we're going

2:45:09.840 --> 2:45:15.920
 to have in the network of software processes leading to AGI.

2:45:15.920 --> 2:45:18.040
 That's a beautiful way to see the world.

2:45:18.040 --> 2:45:21.880
 Again, the same similar question as with OpenCog.

2:45:21.880 --> 2:45:24.600
 If somebody wanted to build an AI system

2:45:24.600 --> 2:45:27.000
 and plug into the singularity net,

2:45:27.000 --> 2:45:28.560
 what would you recommend?

2:45:28.560 --> 2:45:30.160
 Yeah, so that's much easier.

2:45:30.160 --> 2:45:33.840
 I mean, OpenCog is still a research system.

2:45:33.840 --> 2:45:36.640
 So it takes some expertise.

2:45:36.640 --> 2:45:40.560
 We have tutorials, but it's somewhat cognitively labor

2:45:40.560 --> 2:45:44.280
 intensive to get up to speed on OpenCog.

2:45:44.280 --> 2:45:45.680
 And I mean, what's one of the things

2:45:45.680 --> 2:45:49.880
 we hope to change with the true AGI OpenCog 2.0 version

2:45:49.880 --> 2:45:53.520
 is just make the learning curve more similar to TensorFlow

2:45:53.520 --> 2:45:54.360
 or Torch or something.

2:45:54.360 --> 2:45:57.560
 Because right now, OpenCog is amazingly powerful,

2:45:57.560 --> 2:46:00.640
 but not simple to do one.

2:46:00.640 --> 2:46:06.280
 On the other hand, singularity net, as an open platform,

2:46:06.280 --> 2:46:09.600
 was developed a little more with usability in mind.

2:46:09.600 --> 2:46:12.280
 Over the blockchain, it's still kind of a pain.

2:46:12.280 --> 2:46:14.920
 I mean, if you're a command line guy,

2:46:14.920 --> 2:46:16.320
 there's a command line interface.

2:46:16.320 --> 2:46:20.000
 It's quite easy to take any AI that has an API

2:46:20.000 --> 2:46:23.520
 and lives in a Docker container and put it online anywhere.

2:46:23.520 --> 2:46:25.680
 And then it joins the global singularity net.

2:46:25.680 --> 2:46:28.840
 And anyone who puts a request for services out

2:46:28.840 --> 2:46:31.280
 into the singularity net, the peer to peer discovery

2:46:31.280 --> 2:46:33.880
 mechanism will find your AI.

2:46:33.880 --> 2:46:36.560
 And if it does what was asked, it

2:46:36.560 --> 2:46:38.920
 can then start a conversation with your AI

2:46:38.920 --> 2:46:42.120
 about whether it wants to ask your AI to do something for it,

2:46:42.120 --> 2:46:43.480
 how much it would cost, and so on.

2:46:43.480 --> 2:46:46.840
 So that's fairly simple.

2:46:46.840 --> 2:46:51.840
 If you wrote an AI and want it listed on official singularity

2:46:51.840 --> 2:46:55.160
 net marketplace, which is on our website,

2:46:55.160 --> 2:46:57.800
 then we have a publisher portal.

2:46:57.800 --> 2:47:00.160
 And then there's a KYC process to go through,

2:47:00.160 --> 2:47:02.680
 because then we have some legal liability for what

2:47:02.680 --> 2:47:04.640
 goes on that website.

2:47:04.640 --> 2:47:07.280
 So in a way, that's been an education, too.

2:47:07.280 --> 2:47:08.360
 There's sort of two layers.

2:47:08.360 --> 2:47:11.640
 Like there's the open decentralized protocol.

2:47:11.640 --> 2:47:12.920
 And there's the market.

2:47:12.920 --> 2:47:15.480
 Yeah, anyone can use the open decentralized protocol.

2:47:15.480 --> 2:47:17.920
 So say some developers from Iran,

2:47:17.920 --> 2:47:20.640
 and there's brilliant AI guys in the University of Isfahan

2:47:20.640 --> 2:47:22.520
 in Tehran, they can put their stuff

2:47:22.520 --> 2:47:24.560
 on singularity net protocol.

2:47:24.560 --> 2:47:27.000
 And just like they can put something on the internet,

2:47:27.000 --> 2:47:28.280
 I don't control it.

2:47:28.280 --> 2:47:29.680
 But if we're going to list something

2:47:29.680 --> 2:47:31.960
 on the singularity net marketplace

2:47:31.960 --> 2:47:34.240
 and put a little picture and a link to it,

2:47:34.240 --> 2:47:38.800
 then if I put some Iranian AI genius's code on there,

2:47:38.800 --> 2:47:41.480
 then Donald Trump can send a bunch of jackbooted thugs

2:47:41.480 --> 2:47:45.920
 to my house to arrest me for doing business with Iran.

2:47:45.920 --> 2:47:48.960
 So I mean, we already see in some ways

2:47:48.960 --> 2:47:51.040
 the value of having a decentralized protocol.

2:47:51.040 --> 2:47:53.680
 Because what I hope is that someone in Iran

2:47:53.680 --> 2:47:57.040
 will put online an Iranian singularity net marketplace,

2:47:57.040 --> 2:47:59.800
 right, which you can pay in a cryptographic token, which

2:47:59.800 --> 2:48:01.520
 is not owned by any country.

2:48:01.520 --> 2:48:04.720
 And then if you're in Congo or somewhere that

2:48:04.720 --> 2:48:06.760
 doesn't have any problem with Iran,

2:48:06.760 --> 2:48:09.400
 you can subcontract AI services that you

2:48:09.400 --> 2:48:14.200
 find on that marketplace, right, even though US citizens

2:48:14.200 --> 2:48:16.000
 can't buy US law.

2:48:16.000 --> 2:48:20.120
 So right now, that's kind of a minor point.

2:48:20.120 --> 2:48:23.960
 As you alluded, if regulations go in the wrong direction,

2:48:23.960 --> 2:48:25.520
 it could become more of a major point.

2:48:25.520 --> 2:48:30.080
 But I think it also is the case that having these workarounds

2:48:30.080 --> 2:48:33.160
 to regulations in place is a defense mechanism

2:48:33.160 --> 2:48:36.640
 against those regulations being put into place.

2:48:36.640 --> 2:48:39.200
 And you can see that in the music industry, right?

2:48:39.200 --> 2:48:42.960
 I mean, Napster just happened and BitTorrent just happened.

2:48:42.960 --> 2:48:45.920
 And now most people in my kids generation

2:48:45.920 --> 2:48:50.000
 they're baffled by the idea of paying for music, right?

2:48:50.000 --> 2:48:52.960
 My dad pays for music.

2:48:52.960 --> 2:48:55.640
 But that because these decentralized mechanisms

2:48:55.640 --> 2:48:58.960
 happened, and then the regulations followed, right?

2:48:58.960 --> 2:49:01.200
 And the regulations would be very different

2:49:01.200 --> 2:49:04.320
 if they'd been put into place before there was Napster

2:49:04.320 --> 2:49:05.520
 and BitTorrent and so forth.

2:49:05.520 --> 2:49:08.600
 So in the same way, we got to put AI out there

2:49:08.600 --> 2:49:11.000
 in a decentralized vein and big data out there

2:49:11.000 --> 2:49:15.120
 in a decentralized vein now so that the most advanced AI

2:49:15.120 --> 2:49:18.280
 in the world is fundamentally decentralized.

2:49:18.280 --> 2:49:20.920
 And if that's the case, that's just the reality

2:49:20.920 --> 2:49:23.760
 the regulators have to deal with.

2:49:23.760 --> 2:49:25.560
 And then as in the music case, they're

2:49:25.560 --> 2:49:27.480
 going to come up with regulations

2:49:27.480 --> 2:49:32.920
 that sort of work with the decentralized reality.

2:49:32.920 --> 2:49:34.080
 Beautiful.

2:49:34.080 --> 2:49:38.040
 You were the chief scientist of Hansen Robotics.

2:49:38.040 --> 2:49:40.480
 You're still involved with Hansen Robotics,

2:49:40.480 --> 2:49:42.720
 doing a lot of really interesting stuff there.

2:49:42.720 --> 2:49:44.640
 This is for people who don't know the company that

2:49:44.640 --> 2:49:47.360
 created Sophia, the robot.

2:49:47.360 --> 2:49:51.440
 Can you tell me who Sophia is?

2:49:51.440 --> 2:49:55.200
 I'd rather start by telling you who David Hansen is.

2:49:55.200 --> 2:49:58.760
 David is the brilliant mind behind the Sophia robot.

2:49:58.760 --> 2:50:04.200
 And so far, he remains more interesting than his creation,

2:50:04.200 --> 2:50:07.440
 although she may be improving faster than he is, actually.

2:50:07.440 --> 2:50:11.280
 I mean, he's a good point.

2:50:11.280 --> 2:50:15.320
 I met David maybe 2007 or something

2:50:15.320 --> 2:50:18.360
 at some futurist conference we were both speaking at.

2:50:18.360 --> 2:50:22.840
 And I could see we had a great deal in common.

2:50:22.840 --> 2:50:25.000
 I mean, we were both kind of crazy,

2:50:25.000 --> 2:50:31.480
 but we also we both had a passion for AGI and the singularity.

2:50:31.480 --> 2:50:34.840
 And we were both huge fans of the work of Philip K. Dick,

2:50:34.840 --> 2:50:36.800
 the science fiction writer.

2:50:36.800 --> 2:50:43.480
 And I wanted to create benevolent AGI that would create

2:50:43.480 --> 2:50:47.520
 massively better life for all humans and all sentient beings,

2:50:47.520 --> 2:50:50.000
 including animals, plants, and superhuman beings.

2:50:50.000 --> 2:50:53.680
 And David, he wanted exactly the same thing,

2:50:53.680 --> 2:50:56.320
 but he had a different idea of how to do it.

2:50:56.320 --> 2:50:59.360
 He wanted to get computational compassion.

2:50:59.360 --> 2:51:03.880
 Like he wanted to get machines that would love people

2:51:03.880 --> 2:51:05.720
 and empathize with people.

2:51:05.720 --> 2:51:08.320
 And he thought the way to do that was to make a machine that

2:51:08.320 --> 2:51:13.520
 could look people eye to eye, face to face, look at people,

2:51:13.520 --> 2:51:15.640
 and make people love the machine.

2:51:15.640 --> 2:51:17.440
 And the machine loves the people back.

2:51:17.440 --> 2:51:21.440
 So I thought that was a very different way of looking at it,

2:51:21.440 --> 2:51:22.880
 because I'm very math oriented.

2:51:22.880 --> 2:51:27.320
 And I'm just thinking like, what is the abstract cognitive

2:51:27.320 --> 2:51:30.600
 algorithm that will let the system internalize

2:51:30.600 --> 2:51:33.160
 the complex patterns of human values, blah, blah, blah,

2:51:33.160 --> 2:51:35.920
 whereas he's like, look you in the face in the eye

2:51:35.920 --> 2:51:37.320
 and love you, right?

2:51:37.320 --> 2:51:41.320
 So we hit it off quite well.

2:51:41.320 --> 2:51:44.400
 And we talked to each other off and on.

2:51:44.400 --> 2:51:49.320
 Then I moved to Hong Kong in 2011.

2:51:49.320 --> 2:51:53.320
 So I've been living all over the place.

2:51:53.320 --> 2:51:54.920
 I've been in Australia and New Zealand

2:51:54.920 --> 2:51:59.320
 in my academic career then in Las Vegas for a while.

2:51:59.320 --> 2:52:03.600
 Was in New York in the late 90s, starting my entrepreneurial career

2:52:03.600 --> 2:52:06.360
 was in DC for nine years, doing a bunch of US government

2:52:06.360 --> 2:52:07.880
 consulting stuff.

2:52:07.880 --> 2:52:12.400
 Then moved to Hong Kong in 2011, mostly

2:52:12.400 --> 2:52:15.080
 because I met a Chinese girl who I fell in love with.

2:52:15.080 --> 2:52:16.040
 We got married.

2:52:16.040 --> 2:52:17.320
 She's actually not from Hong Kong.

2:52:17.320 --> 2:52:18.440
 She's from mainland China.

2:52:18.440 --> 2:52:21.280
 But we converged together in Hong Kong,

2:52:21.280 --> 2:52:24.120
 still married now, have a two year old baby.

2:52:24.120 --> 2:52:26.760
 So went to Hong Kong to see about a girl, I guess.

2:52:26.760 --> 2:52:29.000
 Yeah, pretty much, yeah.

2:52:29.000 --> 2:52:32.720
 And on the other hand, I started doing some cool research

2:52:32.720 --> 2:52:36.520
 there with Gino Yu at Hong Kong Polytechnic University.

2:52:36.520 --> 2:52:37.880
 I got involved with a project called

2:52:37.880 --> 2:52:40.480
 IDEA Using Machine Learning for Stock and Futures

2:52:40.480 --> 2:52:43.080
 Prediction, which was quite interesting.

2:52:43.080 --> 2:52:47.400
 And I also got to know something about the consumer electronics

2:52:47.400 --> 2:52:50.240
 and hardware manufacturer ecosystem in Shenzhen

2:52:50.240 --> 2:52:53.320
 across the border, which is the only place in the world that

2:52:53.320 --> 2:52:56.880
 makes sense to make complex consumer electronics at large

2:52:56.880 --> 2:52:57.840
 scale and low cost.

2:52:57.840 --> 2:53:00.920
 It's astounding, the hardware ecosystem

2:53:00.920 --> 2:53:03.800
 that you have in South China.

2:53:03.800 --> 2:53:07.240
 You ask, people here cannot imagine what it's like.

2:53:07.240 --> 2:53:12.080
 So David was starting to explore that also.

2:53:12.080 --> 2:53:15.720
 I invited him to Hong Kong to give a talk at Hong Kong PolyU.

2:53:15.720 --> 2:53:19.240
 And I introduced him in Hong Kong to some investors

2:53:19.240 --> 2:53:21.560
 who were interested in his robots.

2:53:21.560 --> 2:53:23.560
 And he didn't have Sophia then.

2:53:23.560 --> 2:53:25.960
 He had a robot of Philip K. Dick, our favorite science

2:53:25.960 --> 2:53:27.000
 fiction writer.

2:53:27.000 --> 2:53:28.160
 He had a robot Einstein.

2:53:28.160 --> 2:53:31.920
 He had some little toy robots that looked like his son, Zino.

2:53:31.920 --> 2:53:35.640
 So through the investors I connected him to,

2:53:35.640 --> 2:53:38.280
 he managed to get some funding to basically port

2:53:38.280 --> 2:53:40.640
 Hanson Robotics to Hong Kong.

2:53:40.640 --> 2:53:42.640
 And when he first moved to Hong Kong,

2:53:42.640 --> 2:53:47.240
 I was working on AGI research and also on this machine

2:53:47.240 --> 2:53:49.320
 learning trading project.

2:53:49.320 --> 2:53:52.920
 So I didn't get that tightly involved with Hanson Robotics.

2:53:52.920 --> 2:53:56.520
 But as I hung out with David more and more,

2:53:56.520 --> 2:54:01.200
 as we were both there in the same place,

2:54:01.200 --> 2:54:02.680
 I started to think about what you

2:54:02.680 --> 2:54:08.480
 could do to make his robots smarter than they were.

2:54:08.480 --> 2:54:10.280
 And so we started working together.

2:54:10.280 --> 2:54:13.000
 And for a few years, I was chief scientist and head

2:54:13.000 --> 2:54:15.720
 of software at Hanson Robotics.

2:54:15.720 --> 2:54:19.400
 Then when I got deeply into the blockchain side of things,

2:54:19.400 --> 2:54:24.320
 I stepped back from that and cofounded Singularity Net.

2:54:24.320 --> 2:54:27.480
 David Hanson was also one of the cofounders of Singularity

2:54:27.480 --> 2:54:27.980
 Net.

2:54:27.980 --> 2:54:30.040
 So part of our goal there had been

2:54:30.040 --> 2:54:33.920
 to make the blockchain based like cloud mind platform

2:54:33.920 --> 2:54:36.960
 for Sophia and the other other heads.

2:54:36.960 --> 2:54:41.760
 Sophia would be just one of the robots in this Singularity Net.

2:54:41.760 --> 2:54:43.280
 Yeah, yeah, yeah, exactly.

2:54:43.280 --> 2:54:47.360
 Sophia, many copies of the Sophia Robot

2:54:47.360 --> 2:54:51.440
 would be among the user interfaces

2:54:51.440 --> 2:54:54.400
 to the globally distributed Singularity Net cloud mind.

2:54:54.400 --> 2:54:58.640
 And I mean, David and I talked about that for quite a while

2:54:58.640 --> 2:55:01.480
 before cofounding Singularity Net.

2:55:01.480 --> 2:55:04.360
 By the way, in his vision and your vision,

2:55:04.360 --> 2:55:09.560
 was Sophia tightly coupled to a particular AI system?

2:55:09.560 --> 2:55:12.480
 Or was the idea that you could just

2:55:12.480 --> 2:55:14.440
 keep plugging in different AI systems within the head of it?

2:55:14.440 --> 2:55:22.000
 I think David's view was always that Sophia would

2:55:22.000 --> 2:55:25.160
 be a platform, much like, say, the Pepper Robot is

2:55:25.160 --> 2:55:26.840
 a platform from SoftBank.

2:55:26.840 --> 2:55:31.680
 Should be a platform with a set of nicely designed APIs

2:55:31.680 --> 2:55:34.840
 that anyone can use to experiment with their different AI

2:55:34.840 --> 2:55:38.600
 algorithms on that platform.

2:55:38.600 --> 2:55:41.520
 And Singularity Net, of course, fits right into that, right?

2:55:41.520 --> 2:55:44.040
 Because Singularity Net, it's an API marketplace.

2:55:44.040 --> 2:55:46.200
 So anyone can put their AI on there.

2:55:46.200 --> 2:55:49.040
 OpenCog is a little bit different.

2:55:49.040 --> 2:55:52.120
 I mean, David likes it, but I'd say it's my thing.

2:55:52.120 --> 2:55:53.040
 It's not his.

2:55:53.040 --> 2:55:57.240
 David has a little more passion for biologically based

2:55:57.240 --> 2:56:00.120
 approaches to AI than I do, which makes sense.

2:56:00.120 --> 2:56:02.760
 I mean, he's really into human physiology and biology.

2:56:02.760 --> 2:56:05.080
 He's a character sculptor, right?

2:56:05.080 --> 2:56:08.400
 So yeah, he's interested in, but he also

2:56:08.400 --> 2:56:11.360
 worked a lot with rule based and logic based AI systems, too.

2:56:11.360 --> 2:56:14.800
 So yeah, he's interested in not just Sophia,

2:56:14.800 --> 2:56:19.000
 but all the Hanson robots as a powerful social and emotional

2:56:19.000 --> 2:56:21.200
 robotics platform.

2:56:21.200 --> 2:56:27.320
 And what I saw in Sophia was a way

2:56:27.320 --> 2:56:33.840
 to get AI algorithms out there in front of a whole lot

2:56:33.840 --> 2:56:36.280
 of different people in an emotionally compelling way.

2:56:36.280 --> 2:56:39.800
 And part of my thought was really abstract,

2:56:39.800 --> 2:56:41.720
 connected to AGI ethics.

2:56:41.720 --> 2:56:45.480
 And many people are concerned AGI is

2:56:45.480 --> 2:56:47.720
 going to enslave everybody or turn everybody

2:56:47.720 --> 2:56:52.640
 into computronium to make extra hard drives

2:56:52.640 --> 2:56:55.520
 for their cognitive engine or whatever.

2:56:55.520 --> 2:57:01.640
 And emotionally, I'm not driven to that sort of paranoia.

2:57:01.640 --> 2:57:04.080
 I'm really just an optimist by nature.

2:57:04.080 --> 2:57:09.200
 But intellectually, I have to assign a nonzero probability

2:57:09.200 --> 2:57:12.120
 to those sorts of nasty outcomes.

2:57:12.120 --> 2:57:14.560
 Because if you're making something 10 times as smart

2:57:14.560 --> 2:57:16.440
 as you, how can you know what it's going to do?

2:57:16.440 --> 2:57:19.760
 There's an irreducible uncertainty there,

2:57:19.760 --> 2:57:22.800
 just as my dog can't predict what I'm going to do tomorrow.

2:57:22.800 --> 2:57:28.560
 So it seemed to me that based on our current state of knowledge,

2:57:28.560 --> 2:57:32.480
 the best way to bias the AGI's we create

2:57:32.480 --> 2:57:38.800
 toward benevolence would be to infuse them with love

2:57:38.800 --> 2:57:41.680
 and compassion the way that we do our own children.

2:57:41.680 --> 2:57:44.960
 So you want to interact with AGI's

2:57:44.960 --> 2:57:47.560
 in the context of doing compassionate, loving,

2:57:47.560 --> 2:57:49.920
 and beneficial things.

2:57:49.920 --> 2:57:51.720
 And in that way, as your children

2:57:51.720 --> 2:57:54.000
 will learn by doing compassionate, beneficial, loving

2:57:54.000 --> 2:57:56.520
 things alongside you, in that way,

2:57:56.520 --> 2:57:58.840
 the AI will learn in practice what

2:57:58.840 --> 2:58:02.360
 it means to be compassionate, beneficial, and loving.

2:58:02.360 --> 2:58:06.400
 It will get a sort of ingrained, intuitive sense of this,

2:58:06.400 --> 2:58:09.280
 which it can then abstract in its own way

2:58:09.280 --> 2:58:11.160
 as it gets more and more intelligent.

2:58:11.160 --> 2:58:12.760
 Now, David saw this the same way.

2:58:12.760 --> 2:58:14.840
 That's why he came up with the name

2:58:14.840 --> 2:58:18.160
 Sophia, which means wisdom.

2:58:18.160 --> 2:58:22.760
 So it seemed to me making these beautiful, loving robots

2:58:22.760 --> 2:58:26.080
 to be rolled out for beneficial applications

2:58:26.080 --> 2:58:31.200
 would be the perfect way to roll out early stage AGI systems

2:58:31.200 --> 2:58:34.400
 so they can learn from people and not just

2:58:34.400 --> 2:58:38.000
 learn factual knowledge, but learn human values and ethics

2:58:38.000 --> 2:58:41.480
 from people while being their home service robots,

2:58:41.480 --> 2:58:44.040
 their education assistants, their nursing robots.

2:58:44.040 --> 2:58:46.040
 So that was the grand vision.

2:58:46.040 --> 2:58:48.600
 Now, if you've ever worked with robots,

2:58:48.600 --> 2:58:50.440
 the reality is quite different, right?

2:58:50.440 --> 2:58:55.040
 Like the first principle is the robot is always broken.

2:58:55.040 --> 2:58:57.640
 I mean, I worked with robots in the 90s a bunch

2:58:57.640 --> 2:58:59.520
 when you had to solder them together yourself.

2:58:59.520 --> 2:59:02.600
 And I'd put neural nets during reinforcement learning

2:59:02.600 --> 2:59:07.560
 on like overturned solid bolt type robots in the 90s

2:59:07.560 --> 2:59:09.320
 when I was a professor.

2:59:09.320 --> 2:59:12.840
 Things, of course, advanced a lot, but the principle still

2:59:12.840 --> 2:59:13.340
 holds.

2:59:13.340 --> 2:59:16.520
 Yeah, the robot's always broken still holds.

2:59:16.520 --> 2:59:21.080
 Yeah, so faced with the reality of making Sophia do stuff,

2:59:21.080 --> 2:59:26.600
 many of my Robo AGI aspirations were temporarily cast aside.

2:59:26.600 --> 2:59:30.680
 And I mean, there's just a practical problem

2:59:30.680 --> 2:59:33.680
 of making this robot interact in a meaningful way.

2:59:33.680 --> 2:59:36.720
 Because you put nice computer vision on there,

2:59:36.720 --> 2:59:38.200
 but there's always glare.

2:59:38.200 --> 2:59:41.400
 And then you have a dialogue system.

2:59:41.400 --> 2:59:46.280
 But at the time I was there, no speech to text algorithm

2:59:46.280 --> 2:59:47.960
 could deal with Hong Kong, Hong Kong

2:59:47.960 --> 2:59:49.800
 East People's English accents.

2:59:49.800 --> 2:59:51.640
 So the speech to text was always bad.

2:59:51.640 --> 2:59:53.640
 So the robot always sounded stupid

2:59:53.640 --> 2:59:55.640
 because it wasn't getting the right text, right?

2:59:55.640 --> 3:00:00.880
 So I started to view that really as what in software

3:00:00.880 --> 3:00:03.040
 engineering you call a walking skeleton, which

3:00:03.040 --> 3:00:05.400
 is maybe the wrong metaphor to use for Sophia,

3:00:05.400 --> 3:00:07.000
 or maybe the right one.

3:00:07.000 --> 3:00:09.440
 I mean, what a walking skeleton is in software development

3:00:09.440 --> 3:00:14.000
 is if you're building a complex system, how do you get started?

3:00:14.000 --> 3:00:16.100
 Well, one way is to first build part one well,

3:00:16.100 --> 3:00:19.280
 then build part two well, then build part three well, and so on.

3:00:19.280 --> 3:00:22.040
 Another way is you make like a simple version

3:00:22.040 --> 3:00:24.240
 of the whole system and put something

3:00:24.240 --> 3:00:27.280
 in the place of every part the whole system will need

3:00:27.280 --> 3:00:29.680
 so that you have a whole system that does something.

3:00:29.680 --> 3:00:31.880
 And then you work on improving each part

3:00:31.880 --> 3:00:34.360
 in the context of that whole integrated system.

3:00:34.360 --> 3:00:38.120
 So that's what we did on a software level in Sophia.

3:00:38.120 --> 3:00:41.760
 We made like a walking skeleton software system where

3:00:41.760 --> 3:00:44.520
 so there's something that sees, there's something that hears,

3:00:44.520 --> 3:00:47.040
 there's something that moves, there's

3:00:47.040 --> 3:00:49.960
 something that remembers, there's something that learns.

3:00:49.960 --> 3:00:52.480
 You put a simple version of each thing in there

3:00:52.480 --> 3:00:54.400
 and you connect them all together

3:00:54.400 --> 3:00:56.640
 so that the system will do its thing.

3:00:56.640 --> 3:00:59.640
 So there's a lot of AI in there.

3:00:59.640 --> 3:01:01.360
 There's not any AGI in there.

3:01:01.360 --> 3:01:04.640
 I mean, there's computer vision to recognize people's faces,

3:01:04.640 --> 3:01:07.640
 recognize when someone comes in the room and leaves,

3:01:07.640 --> 3:01:11.640
 try to recognize whether two people are together or not.

3:01:11.640 --> 3:01:17.040
 The dialogue system, it's a mix of like hand coded rules

3:01:17.040 --> 3:01:21.560
 with deep neural nets that come up with their own responses.

3:01:21.560 --> 3:01:25.640
 And there's some attempt to have a narrative structure

3:01:25.640 --> 3:01:28.920
 and sort of try to pull the conversation into something

3:01:28.920 --> 3:01:30.760
 with the beginning, middle, and end

3:01:30.760 --> 3:01:32.160
 and the sort of story arc.

3:01:32.160 --> 3:01:36.440
 So it's, I mean, like if you look at the Lobner Prize

3:01:36.440 --> 3:01:39.040
 and the systems that beat the Turing test currently,

3:01:39.040 --> 3:01:42.360
 they're heavily rule based because like you had said,

3:01:42.360 --> 3:01:45.680
 narrative structure to create compelling conversations,

3:01:45.680 --> 3:01:48.400
 you currently, neural networks cannot do that well,

3:01:48.400 --> 3:01:50.640
 even with Google Mina.

3:01:50.640 --> 3:01:53.000
 When you actually look at full scale conversations,

3:01:53.000 --> 3:01:53.440
 it's just not.

3:01:53.440 --> 3:01:54.360
 Yeah, this is the thing.

3:01:54.360 --> 3:01:57.880
 So we've been, I've actually been running an experiment

3:01:57.880 --> 3:02:01.400
 the last couple of weeks taking Sophia's chatbot

3:02:01.400 --> 3:02:03.720
 and then Facebook's transformer chatbot,

3:02:03.720 --> 3:02:05.240
 which they opened the model.

3:02:05.240 --> 3:02:07.800
 We've had them chatting to each other for a number of weeks

3:02:07.800 --> 3:02:08.840
 on the server just.

3:02:08.840 --> 3:02:09.960
 That's funny.

3:02:09.960 --> 3:02:13.200
 We're generating training data of what Sophia says

3:02:13.200 --> 3:02:15.440
 and a wide variety of conversations.

3:02:15.440 --> 3:02:20.240
 But we can see, compared to Sophia's current chatbot,

3:02:20.240 --> 3:02:23.440
 the Facebook deep neural chatbot comes up

3:02:23.440 --> 3:02:27.280
 with a wider variety of fluent sounding sentences.

3:02:27.280 --> 3:02:30.080
 On the other hand, it rambles like mad.

3:02:30.080 --> 3:02:33.880
 The Sophia chatbot, it's a little more repetitive

3:02:33.880 --> 3:02:36.600
 in the sentence structures it uses.

3:02:36.600 --> 3:02:39.800
 On the other hand, it's able to keep like a conversation arc

3:02:39.800 --> 3:02:42.440
 over a much longer, longer period, right?

3:02:42.440 --> 3:02:46.600
 So there, now you can probably surmount that using Reformer

3:02:46.600 --> 3:02:51.160
 and like using various other deep neural architectures

3:02:51.160 --> 3:02:53.960
 to improve the way these transformer models are trained.

3:02:53.960 --> 3:02:58.280
 But in the end, neither one of them really understands

3:02:58.280 --> 3:02:59.120
 what's going on.

3:02:59.120 --> 3:03:02.640
 And I mean, that's the challenge I had with Sophia

3:03:02.640 --> 3:03:08.120
 is if I were doing a robotics project aimed at AGI,

3:03:08.120 --> 3:03:10.640
 I would want to make like a robot toddler that was just

3:03:10.640 --> 3:03:11.880
 learning about what it was seeing.

3:03:11.880 --> 3:03:13.160
 Because then the language is grounded

3:03:13.160 --> 3:03:14.880
 in the experience of the robot.

3:03:14.880 --> 3:03:17.680
 But what Sophia needs to do to be Sophia

3:03:17.680 --> 3:03:21.400
 is talk about sports or the weather or robotics

3:03:21.400 --> 3:03:24.040
 or the conference she's talking at.

3:03:24.040 --> 3:03:27.120
 She needs to be fluent talking about any damn thing

3:03:27.120 --> 3:03:28.320
 in the world.

3:03:28.320 --> 3:03:32.480
 And she doesn't have grounding for all those things.

3:03:32.480 --> 3:03:34.960
 So there's this just like, I mean,

3:03:34.960 --> 3:03:36.600
 Google Mina and Facebook's chatbot

3:03:36.600 --> 3:03:40.040
 don't have grounding for what they're talking about either.

3:03:40.040 --> 3:03:44.960
 So in a way, the need to speak fluently about things

3:03:44.960 --> 3:03:47.840
 where there's no non linguistic grounding

3:03:47.840 --> 3:03:53.840
 pushes what you can do for Sophia in the short term

3:03:53.840 --> 3:03:55.680
 a bit away from AGI.

3:03:55.680 --> 3:03:56.200
 Permission.

3:03:56.200 --> 3:04:00.920
 I mean, if it pushes you towards IBM Watson situation

3:04:00.920 --> 3:04:03.640
 where you basically have to heuristic and hardcore stuff

3:04:03.640 --> 3:04:07.120
 and rule based stuff, I have to ask you about this.

3:04:07.120 --> 3:04:18.880
 OK, so because in part Sophia is an art creation

3:04:18.880 --> 3:04:21.240
 because it's beautiful.

3:04:21.240 --> 3:04:24.760
 She's beautiful because she inspires

3:04:24.760 --> 3:04:29.560
 through our human nature of anthropomorphized things.

3:04:29.560 --> 3:04:32.600
 We immediately see an intelligent being there.

3:04:32.600 --> 3:04:34.080
 Because David is a great sculptor.

3:04:34.080 --> 3:04:35.480
 He is a great sculptor, that's right.

3:04:35.480 --> 3:04:40.880
 So in fact, if Sophia just had nothing inside her head,

3:04:40.880 --> 3:04:43.280
 said nothing, if she just sat there,

3:04:43.280 --> 3:04:45.960
 we already prescribed some intelligence to her.

3:04:45.960 --> 3:04:48.800
 There's a long selfie line in front of her after every talk.

3:04:48.800 --> 3:04:50.000
 That's right.

3:04:50.000 --> 3:04:53.840
 So it captivated the imagination of many people.

3:04:53.840 --> 3:04:56.960
 I was going to say the world, but yeah, I mean a lot of people.

3:04:56.960 --> 3:05:00.240
 And billions of people, which is amazing.

3:05:00.240 --> 3:05:02.040
 It's amazing, right?

3:05:02.040 --> 3:05:08.720
 Now, of course, many people have prescribed essentially

3:05:08.720 --> 3:05:12.480
 AGI type of capabilities to Sophia when they see her.

3:05:12.480 --> 3:05:19.880
 And of course, friendly French folk like Jan Lacune

3:05:19.880 --> 3:05:22.840
 immediately see that of the people from the AI community

3:05:22.840 --> 3:05:24.760
 and get really frustrated.

3:05:24.760 --> 3:05:27.080
 Because it's understandable.

3:05:27.080 --> 3:05:34.480
 So what, and then they criticize people like you who sit back

3:05:34.480 --> 3:05:37.720
 and don't say anything about, like basically allow

3:05:37.720 --> 3:05:40.640
 the imagination of the world, allow the world

3:05:40.640 --> 3:05:43.880
 to continue being captivated.

3:05:43.880 --> 3:05:49.120
 So what's your sense of that kind of annoyance

3:05:49.120 --> 3:05:51.160
 that the AI community has?

3:05:51.160 --> 3:05:55.400
 I think there's several parts to my reaction there.

3:05:55.400 --> 3:05:59.800
 First of all, if I weren't involved with Hanson Rebox

3:05:59.800 --> 3:06:03.400
 and didn't know David Hanson personally,

3:06:03.400 --> 3:06:06.440
 I probably would have been very annoyed initially

3:06:06.440 --> 3:06:08.000
 at Sophia as well.

3:06:08.000 --> 3:06:09.440
 I mean, I can understand the reaction.

3:06:09.440 --> 3:06:14.160
 I would have been like, wait, all these stupid people out

3:06:14.160 --> 3:06:16.240
 there think this is an AGI.

3:06:16.240 --> 3:06:18.000
 But it's not an AGI.

3:06:18.000 --> 3:06:23.040
 But they're tricking people that this very cool robot is an AGI.

3:06:23.040 --> 3:06:28.120
 And now those of us trying to raise funding to build AGI,

3:06:28.120 --> 3:06:31.160
 people will think it's already there and already works.

3:06:31.160 --> 3:06:36.960
 So on the other hand, I think even

3:06:36.960 --> 3:06:38.440
 if I weren't directly involved with it,

3:06:38.440 --> 3:06:41.600
 once I dug a little deeper into David and the robot

3:06:41.600 --> 3:06:44.680
 and the intentions behind it, I think

3:06:44.680 --> 3:06:47.000
 I would have stopped being pissed off.

3:06:47.000 --> 3:06:51.360
 Whereas folks like Jan Lacune have remained pissed off

3:06:51.360 --> 3:06:54.440
 after their initial reaction.

3:06:54.440 --> 3:06:55.160
 That's his thing.

3:06:55.160 --> 3:06:56.040
 That's his thing.

3:06:56.040 --> 3:07:01.920
 I think that in particular struck me as somewhat ironic

3:07:01.920 --> 3:07:05.840
 because Jan Lacune is working for Facebook, which

3:07:05.840 --> 3:07:08.440
 is using machine learning to program

3:07:08.440 --> 3:07:10.240
 the brains of the people in the world

3:07:10.240 --> 3:07:14.840
 toward vapid consumerism and political extremism.

3:07:14.840 --> 3:07:20.200
 So if your ethics allows you to use machine learning in such

3:07:20.200 --> 3:07:24.560
 a blatantly destructive way, why would your ethics not

3:07:24.560 --> 3:07:26.800
 allow you to use machine learning to make

3:07:26.800 --> 3:07:32.160
 a lovable theatrical robot that draws some foolish people

3:07:32.160 --> 3:07:34.440
 into its theatrical illusion?

3:07:34.440 --> 3:07:38.840
 Like if the pushback had come from Yoshua Benjiro,

3:07:38.840 --> 3:07:40.920
 I would have felt much more humbled by it.

3:07:40.920 --> 3:07:45.480
 Because he's not using AI for blatant evil, right?

3:07:45.480 --> 3:07:48.600
 On the other hand, he also is a super nice guy

3:07:48.600 --> 3:07:50.920
 and doesn't bother to go out there

3:07:50.920 --> 3:07:54.440
 trashing other people's work for no good reason, right?

3:07:54.440 --> 3:07:55.200
 Shots fired.

3:07:55.200 --> 3:07:56.000
 But I get you.

3:07:56.000 --> 3:07:58.080
 I mean, that's.

3:07:58.080 --> 3:08:01.200
 I mean, if you're going to ask, I'm going to answer.

3:08:01.200 --> 3:08:02.040
 No, for sure.

3:08:02.040 --> 3:08:03.360
 I think we'll go back and forth.

3:08:03.360 --> 3:08:04.560
 I'll talk to Jan again.

3:08:04.560 --> 3:08:06.120
 I would add on this, though.

3:08:06.120 --> 3:08:11.600
 I mean, David Hansen is an artist,

3:08:11.600 --> 3:08:14.240
 and he often speaks off the cuff.

3:08:14.240 --> 3:08:16.360
 And I have not agreed with everything

3:08:16.360 --> 3:08:19.360
 that David has said or done regarding Sophia.

3:08:19.360 --> 3:08:22.760
 And David also was not agreed with everything

3:08:22.760 --> 3:08:24.760
 David has said or done about Sophia.

3:08:24.760 --> 3:08:25.840
 That's an important point.

3:08:25.840 --> 3:08:30.160
 I mean, David is an artistic wild man,

3:08:30.160 --> 3:08:33.560
 and that's part of his charm.

3:08:33.560 --> 3:08:34.720
 That's part of his genius.

3:08:34.720 --> 3:08:39.400
 So certainly, there have been conversations

3:08:39.400 --> 3:08:42.280
 within Hansen Robotics in between me and David,

3:08:42.280 --> 3:08:46.880
 where I was like, let's be more open about how this thing is

3:08:46.880 --> 3:08:48.200
 working.

3:08:48.200 --> 3:08:52.040
 And I did have some influence in nudging Hansen Robotics

3:08:52.040 --> 3:08:57.480
 to be more open about how Sophia was working.

3:08:57.480 --> 3:09:00.480
 And David wasn't especially opposed to this.

3:09:00.480 --> 3:09:02.400
 And he was actually quite right about it.

3:09:02.400 --> 3:09:04.480
 What he said was, you can tell people

3:09:04.480 --> 3:09:08.000
 exactly how it's working, and they won't care.

3:09:08.000 --> 3:09:09.600
 They want to be drawn into the illusion.

3:09:09.600 --> 3:09:12.560
 And he was 100% correct.

3:09:12.560 --> 3:09:14.600
 I'll tell you what, this wasn't Sophia.

3:09:14.600 --> 3:09:15.720
 This was Philip K. Dick.

3:09:15.720 --> 3:09:19.240
 But we did some interactions between humans and Philip

3:09:19.240 --> 3:09:23.800
 K. Dick robot in Austin, Texas, a few years back.

3:09:23.800 --> 3:09:25.560
 And in this case, the Philip K. Dick

3:09:25.560 --> 3:09:28.600
 was just teleoperated by another human in the other room.

3:09:28.600 --> 3:09:31.280
 So during the conversations, we didn't tell people

3:09:31.280 --> 3:09:32.840
 the robot was teleoperated.

3:09:32.840 --> 3:09:35.360
 We just said, here, have a conversation with Phil Dick.

3:09:35.360 --> 3:09:37.120
 We're going to film you, right?

3:09:37.120 --> 3:09:39.720
 And they had a great conversation with Philip K. Dick,

3:09:39.720 --> 3:09:42.920
 teleoperated by my friend, Stefan Bugai.

3:09:42.920 --> 3:09:45.840
 After the conversation, we brought the people

3:09:45.840 --> 3:09:48.120
 in the back room to see Stefan, who

3:09:48.120 --> 3:09:53.480
 was controlling the Philip K. Dick robot.

3:09:53.480 --> 3:09:54.800
 But they didn't believe it.

3:09:54.800 --> 3:09:56.480
 These people were like, well, yeah,

3:09:56.480 --> 3:09:58.800
 but I know I was talking to Phil.

3:09:58.800 --> 3:10:02.240
 Maybe Stefan was typing, but the spirit of Phil

3:10:02.240 --> 3:10:05.120
 was animating his mind while he was typing.

3:10:05.120 --> 3:10:07.680
 So even though they knew it was a human in the loop,

3:10:07.680 --> 3:10:09.840
 even seeing the guy there, they still

3:10:09.840 --> 3:10:12.880
 believe that was Phil they were talking to.

3:10:12.880 --> 3:10:16.680
 A small part of me believes that they were right, actually.

3:10:16.680 --> 3:10:17.880
 Because our understanding.

3:10:17.880 --> 3:10:19.520
 Well, we don't understand the universe.

3:10:19.520 --> 3:10:20.160
 That's the thing.

3:10:20.160 --> 3:10:24.320
 There is a cosmic mind field that we're all embedded in

3:10:24.320 --> 3:10:28.240
 that yields many strange synchronicities in the world,

3:10:28.240 --> 3:10:31.600
 which is a topic we don't have time to go into too much here.

3:10:31.600 --> 3:10:37.280
 I mean, there's something to this where

3:10:37.280 --> 3:10:40.680
 our imagination about Sophia and people

3:10:40.680 --> 3:10:43.280
 like Jan Likun being frustrated about it

3:10:43.280 --> 3:10:45.840
 is all part of this beautiful dance

3:10:45.840 --> 3:10:48.920
 of creating artificial intelligence that's almost essential.

3:10:48.920 --> 3:10:53.600
 You see with Boston Dynamics, whom I'm a huge fan of as well,

3:10:53.600 --> 3:10:56.240
 the kind of, I mean, these robots are very

3:10:56.240 --> 3:10:59.640
 far from intelligent.

3:10:59.640 --> 3:11:01.920
 I played with their last one, actually.

3:11:01.920 --> 3:11:02.720
 With the spot menu.

3:11:02.720 --> 3:11:03.600
 Yeah, very cool.

3:11:03.600 --> 3:11:07.160
 I mean, it reacts quite in a fluid and flexible way.

3:11:07.160 --> 3:11:10.480
 But we immediately ascribe the kind of intelligence.

3:11:10.480 --> 3:11:12.480
 We immediately ascribe AGI to them.

3:11:12.480 --> 3:11:12.840
 Yeah, yeah.

3:11:12.840 --> 3:11:14.800
 If you kick it and it falls down and goes out,

3:11:14.800 --> 3:11:15.680
 you feel bad, right?

3:11:15.680 --> 3:11:17.280
 You can't help it.

3:11:17.280 --> 3:11:22.040
 And I mean, that's part of, that's

3:11:22.040 --> 3:11:23.600
 going to be part of our journey in creating

3:11:23.600 --> 3:11:25.640
 intelligent systems more and more and more and more.

3:11:25.640 --> 3:11:29.400
 Like, as Sophia starts out with a walking skeleton,

3:11:29.400 --> 3:11:31.920
 as you add more and more intelligence,

3:11:31.920 --> 3:11:34.280
 I mean, we're going to have to deal with this kind of idea.

3:11:34.280 --> 3:11:35.040
 Absolutely.

3:11:35.040 --> 3:11:38.400
 And about Sophia, I would say, I mean, first of all,

3:11:38.400 --> 3:11:39.840
 I have nothing against Jan Likun.

3:11:39.840 --> 3:11:40.800
 No, no, this is fine.

3:11:40.800 --> 3:11:41.440
 This is all for fun.

3:11:41.440 --> 3:11:42.200
 He's a nice guy.

3:11:42.200 --> 3:11:48.000
 If he wants to play the media banter game, I'm happy to play.

3:11:48.000 --> 3:11:51.440
 He's a good researcher and a good human being.

3:11:51.440 --> 3:11:53.600
 I'd happily work with the guy.

3:11:53.600 --> 3:11:54.960
 The other thing I was going to say

3:11:54.960 --> 3:12:00.320
 is I have been explicit about how Sophia works.

3:12:00.320 --> 3:12:04.560
 And I've posted online, and what, H Plus Magazine,

3:12:04.560 --> 3:12:06.440
 an online web scene.

3:12:06.440 --> 3:12:09.760
 I mean, I've posted a moderately detailed article

3:12:09.760 --> 3:12:12.840
 explaining, like, there are three software systems

3:12:12.840 --> 3:12:15.200
 we've used inside Sophia.

3:12:15.200 --> 3:12:17.840
 There's a timeline editor, which is like a rule based

3:12:17.840 --> 3:12:19.560
 authoring system, where she's really just

3:12:19.560 --> 3:12:22.640
 being an outlet for what a human scripted.

3:12:22.640 --> 3:12:24.720
 There's a chat bot, which has some rule based

3:12:24.720 --> 3:12:26.440
 on some neural aspects.

3:12:26.440 --> 3:12:29.400
 And then sometimes we've used OpenCog behind Sophia,

3:12:29.400 --> 3:12:31.920
 where there's more learning and reasoning.

3:12:31.920 --> 3:12:36.440
 And the funny thing is, I can't always tell which system

3:12:36.440 --> 3:12:37.680
 is operating here, right?

3:12:37.680 --> 3:12:41.720
 I mean, whether she's really learning or thinking

3:12:41.720 --> 3:12:44.600
 or just appears to be, over half hour, I could tell.

3:12:44.600 --> 3:12:48.440
 But over three or four minutes of interaction, I could tell.

3:12:48.440 --> 3:12:50.360
 Even having three systems that's already

3:12:50.360 --> 3:12:53.040
 sufficiently complex where you can't really tell right away.

3:12:53.040 --> 3:12:57.000
 Yeah, the thing is, even if you get up on stage

3:12:57.000 --> 3:12:59.560
 and tell people how Sophia is working,

3:12:59.560 --> 3:13:03.400
 and then they talk to her, they still attribute

3:13:03.400 --> 3:13:08.920
 more agency and consciousness to her than is really there.

3:13:08.920 --> 3:13:13.800
 So I think there's a couple levels of ethical issue there.

3:13:13.800 --> 3:13:19.240
 One issue is, should you be transparent about how

3:13:19.240 --> 3:13:21.560
 Sophia is working?

3:13:21.560 --> 3:13:22.960
 And I think you should.

3:13:22.960 --> 3:13:26.160
 And I think we have been.

3:13:26.160 --> 3:13:30.240
 I mean, there's articles online that there's

3:13:30.240 --> 3:13:33.440
 some TV special that goes through me explaining

3:13:33.440 --> 3:13:35.400
 the three subsystems behind Sophia.

3:13:35.400 --> 3:13:41.440
 So the way Sophia works is out there much more clearly

3:13:41.440 --> 3:13:43.400
 than how Facebook say I works or something, right?

3:13:43.400 --> 3:13:45.920
 I mean, we've been fairly explicit about it.

3:13:45.920 --> 3:13:48.960
 The other is, given that telling people

3:13:48.960 --> 3:13:52.400
 how it works doesn't cause them to not attribute

3:13:52.400 --> 3:13:54.600
 too much intelligence agency to it anyway,

3:13:54.600 --> 3:14:01.120
 then should you keep fooling them when they want to be fooled?

3:14:01.120 --> 3:14:03.640
 And I mean, the whole media industry

3:14:03.640 --> 3:14:06.720
 is based on fooling people the way they want to be fooled.

3:14:06.720 --> 3:14:13.560
 And we are fooling people 100% toward a good end.

3:14:13.560 --> 3:14:18.040
 I mean, we are playing on people's sense of empathy

3:14:18.040 --> 3:14:20.520
 and compassion so that we can give them

3:14:20.520 --> 3:14:23.640
 a good user experience with helpful robots

3:14:23.640 --> 3:14:29.440
 and so that we can fill the AI's mind with love and compassion.

3:14:29.440 --> 3:14:34.120
 So I've been talking a lot with Hansen Robotics lately

3:14:34.120 --> 3:14:37.560
 about collaborations in the area of medical robotics.

3:14:37.560 --> 3:14:41.480
 And we haven't quite pulled the trigger on a project

3:14:41.480 --> 3:14:44.680
 in that domain yet, but we may well do so quite soon.

3:14:44.680 --> 3:14:49.440
 So we've been talking a lot about robots can help with elder

3:14:49.440 --> 3:14:52.360
 care, robots can help with kids, David's and a lot of things

3:14:52.360 --> 3:14:56.520
 with autism therapy and robots before.

3:14:56.520 --> 3:14:58.720
 In the COVID era, having a robot that

3:14:58.720 --> 3:15:00.600
 can be a nursing assistant in various senses

3:15:00.600 --> 3:15:02.320
 can be quite valuable.

3:15:02.320 --> 3:15:04.200
 The robots don't spread infection,

3:15:04.200 --> 3:15:06.280
 and they can also deliver more attention

3:15:06.280 --> 3:15:07.880
 than human nurses can give, right?

3:15:07.880 --> 3:15:12.360
 So if you have a robot that's helping a patient with COVID,

3:15:12.360 --> 3:15:16.440
 if that patient attributes more understanding and compassion

3:15:16.440 --> 3:15:18.680
 and agency to that robot, then it really

3:15:18.680 --> 3:15:20.640
 has, because it looks like a human.

3:15:20.640 --> 3:15:22.920
 I mean, is that really bad?

3:15:22.920 --> 3:15:25.640
 I mean, we can tell them it doesn't fully understand you,

3:15:25.640 --> 3:15:27.160
 and they don't care, because they're

3:15:27.160 --> 3:15:29.240
 lying there with a fever and they're sick.

3:15:29.240 --> 3:15:31.000
 But they don't react better to that robot

3:15:31.000 --> 3:15:33.480
 with its loving, warm facial expression

3:15:33.480 --> 3:15:38.080
 than they would to a pepper robot or a metallic looking robot.

3:15:38.080 --> 3:15:41.360
 So it's really about how you use it, right?

3:15:41.360 --> 3:15:45.280
 If you made a human looking like door to door sales robot that

3:15:45.280 --> 3:15:48.840
 used its human looking appearance to scan people out

3:15:48.840 --> 3:15:52.600
 of their money, then you're using that connection

3:15:52.600 --> 3:15:57.040
 in a bad way, but you could also use it in a good way.

3:15:57.040 --> 3:16:01.760
 But then that's the same problem with every technology, right?

3:16:01.760 --> 3:16:03.000
 Beautifully put.

3:16:03.000 --> 3:16:08.840
 So like you said, we're living in the era of the COVID.

3:16:08.840 --> 3:16:14.720
 This is 2020, one of the craziest years in recent history.

3:16:14.720 --> 3:16:21.360
 So if we zoom out and look at this pandemic,

3:16:21.360 --> 3:16:24.800
 the coronavirus pandemic, maybe let

3:16:24.800 --> 3:16:29.800
 me ask you this kind of thing in viruses in general.

3:16:29.800 --> 3:16:33.320
 When you look at viruses, do you see them

3:16:33.320 --> 3:16:35.840
 as a kind of intelligence system?

3:16:35.840 --> 3:16:37.640
 I think the concept of intelligence

3:16:37.640 --> 3:16:40.600
 is not that natural of a concept in the end.

3:16:40.600 --> 3:16:43.800
 I think human minds and bodies are

3:16:43.800 --> 3:16:49.400
 a kind of complex, self organizing adaptive system.

3:16:49.400 --> 3:16:51.880
 And viruses certainly are that, right?

3:16:51.880 --> 3:16:55.000
 They're a very complex, self organizing adaptive system.

3:16:55.000 --> 3:16:57.680
 If you want to look at intelligence as Marcus Hoeter

3:16:57.680 --> 3:17:01.680
 defines it as sort of optimizing computable reward

3:17:01.680 --> 3:17:04.720
 functions over computable environments,

3:17:04.720 --> 3:17:07.560
 for sure viruses are doing that, right?

3:17:07.560 --> 3:17:13.800
 And I mean, in doing so, they're causing some harm to us.

3:17:13.800 --> 3:17:17.720
 So the human immune system is a very complex,

3:17:17.720 --> 3:17:19.480
 self organizing adaptive system, which

3:17:19.480 --> 3:17:21.080
 has a lot of intelligence to it.

3:17:21.080 --> 3:17:24.520
 And viruses are also adapting and dividing

3:17:24.520 --> 3:17:27.640
 into new mutant strains and so forth.

3:17:27.640 --> 3:17:29.520
 And ultimately, the solution is going

3:17:29.520 --> 3:17:31.960
 to be nanotechnology, right?

3:17:31.960 --> 3:17:35.960
 I mean, the solution is going to be making little nanobots that

3:17:35.960 --> 3:17:38.000
 fight the viruses, or?

3:17:38.000 --> 3:17:40.600
 Well, people will use them to make nastier viruses,

3:17:40.600 --> 3:17:44.320
 but hopefully we can also use them to just detect combat

3:17:44.320 --> 3:17:46.160
 and kill the viruses.

3:17:46.160 --> 3:17:52.600
 But I think now we're stuck with the biological mechanisms

3:17:52.600 --> 3:17:54.920
 to combat these viruses.

3:17:54.920 --> 3:17:59.440
 And yeah, we've been, AGI is not yet mature enough

3:17:59.440 --> 3:18:03.920
 to use against COVID, but we've been using machine learning

3:18:03.920 --> 3:18:07.000
 and also some machine reasoning in OPENCOG

3:18:07.000 --> 3:18:10.400
 to help some doctors to do personalized medicine

3:18:10.400 --> 3:18:11.040
 against COVID.

3:18:11.040 --> 3:18:14.080
 So the problem there is, given the person's genomics

3:18:14.080 --> 3:18:16.480
 and given their clinical medical indicators,

3:18:16.480 --> 3:18:20.240
 how do you figure out which combination of antivirals

3:18:20.240 --> 3:18:24.280
 is going to be most effective against COVID for that person?

3:18:24.280 --> 3:18:28.480
 And so that's something where machine learning is interesting,

3:18:28.480 --> 3:18:30.360
 but also we're finding the abstraction

3:18:30.360 --> 3:18:33.840
 we get in OPENCOG with machine reasoning is interesting,

3:18:33.840 --> 3:18:36.640
 because it can help with transfer learning

3:18:36.640 --> 3:18:40.360
 when you have not that many different cases to study

3:18:40.360 --> 3:18:43.840
 and qualitative differences between different strains

3:18:43.840 --> 3:18:47.120
 of a virus or people of different ages who may have COVID.

3:18:47.120 --> 3:18:50.640
 So there's a lot of different disparate data to work with

3:18:50.640 --> 3:18:53.600
 and small data sets and somehow integrating them.

3:18:53.600 --> 3:18:55.440
 This is one of the shameful things

3:18:55.440 --> 3:18:57.280
 that's very hard to get that data.

3:18:57.280 --> 3:19:00.320
 So I mean, we're working with a couple groups

3:19:00.320 --> 3:19:02.360
 doing clinical trials.

3:19:02.360 --> 3:19:06.840
 And they're sharing data with us under nondisclosure.

3:19:06.840 --> 3:19:11.880
 But what should be the case is every COVID clinical trial

3:19:11.880 --> 3:19:14.440
 should be putting data online somewhere,

3:19:14.440 --> 3:19:17.840
 like suitably encrypted to protect patient privacy,

3:19:17.840 --> 3:19:21.000
 so that anyone with the AI algorithms

3:19:21.000 --> 3:19:22.280
 should be able to help analyze it.

3:19:22.280 --> 3:19:24.520
 And any biologist should be able to analyze it by hand

3:19:24.520 --> 3:19:25.880
 to understand what they can.

3:19:25.880 --> 3:19:29.680
 Instead, that data is siloed inside whatever

3:19:29.680 --> 3:19:33.200
 hospital is running the clinical trial, which is completely

3:19:33.200 --> 3:19:36.400
 asinine and ridiculous.

3:19:36.400 --> 3:19:37.840
 Why the world works that way?

3:19:37.840 --> 3:19:39.160
 I mean, we could all analyze why,

3:19:39.160 --> 3:19:40.680
 but it's insane that it does.

3:19:40.680 --> 3:19:44.080
 You look at this hydrochloroquine, right?

3:19:44.080 --> 3:19:45.680
 All these clinical trials were done

3:19:45.680 --> 3:19:48.480
 were reported by Surgisphere, some little company

3:19:48.480 --> 3:19:50.200
 no one ever heard of.

3:19:50.200 --> 3:19:53.240
 And everyone paid attention to this.

3:19:53.240 --> 3:19:55.520
 So they were doing more clinical trials based on that.

3:19:55.520 --> 3:19:57.440
 Then they stopped doing clinical trials based on that.

3:19:57.440 --> 3:20:01.040
 Then they started again, and why isn't that data just

3:20:01.040 --> 3:20:02.520
 out there so everyone can analyze it

3:20:02.520 --> 3:20:05.040
 and see what's going on, right?

3:20:05.040 --> 3:20:10.560
 Do you have hope that data will be out there eventually

3:20:10.560 --> 3:20:11.840
 for future pandemics?

3:20:11.840 --> 3:20:13.920
 I mean, do you have hope that our society will move

3:20:13.920 --> 3:20:15.480
 in the direction of such?

3:20:15.480 --> 3:20:19.080
 Not in the immediate future, because the US and China

3:20:19.080 --> 3:20:21.560
 frictions are getting very high.

3:20:21.560 --> 3:20:25.520
 So it's hard to see US and China as moving in the direction

3:20:25.520 --> 3:20:27.640
 of openly sharing data with each other, right?

3:20:27.640 --> 3:20:28.960
 It's not.

3:20:28.960 --> 3:20:31.800
 There's some sharing of data, but different groups

3:20:31.800 --> 3:20:33.280
 are keeping their data private till they've

3:20:33.280 --> 3:20:36.240
 milked the best results from it, and then they share it, right?

3:20:36.240 --> 3:20:39.160
 So yeah, we're working with some data

3:20:39.160 --> 3:20:41.880
 that we've managed to get our hands on, something we're

3:20:41.880 --> 3:20:43.160
 doing to do good for the world.

3:20:43.160 --> 3:20:46.880
 And it's a very cool playground for putting deep neural nets

3:20:46.880 --> 3:20:47.880
 and open cog together.

3:20:47.880 --> 3:20:51.880
 So we have a bioadm space full of all sorts of knowledge

3:20:51.880 --> 3:20:53.640
 from many different biology experiments

3:20:53.640 --> 3:20:57.680
 about human longevity and from biology knowledge bases online.

3:20:57.680 --> 3:21:00.800
 And we can do graph to vector type embeddings,

3:21:00.800 --> 3:21:03.040
 where we take nodes from the hypergraph,

3:21:03.040 --> 3:21:05.120
 embed them into vectors, which can then

3:21:05.120 --> 3:21:07.920
 feed into neural nets for different types of analysis.

3:21:07.920 --> 3:21:12.440
 And we were doing this in the context of a project called

3:21:12.440 --> 3:21:15.520
 Rejuve that we spun off from SingularityNet

3:21:15.520 --> 3:21:18.600
 to do longevity analytics.

3:21:18.600 --> 3:21:20.920
 I can understand why people live to 105 years

3:21:20.920 --> 3:21:22.280
 over, and other people don't.

3:21:22.280 --> 3:21:25.680
 And then we had to spin off Singularity Studio,

3:21:25.680 --> 3:21:28.880
 where we're working with some health care companies

3:21:28.880 --> 3:21:31.760
 on data analytics.

3:21:31.760 --> 3:21:35.400
 So this bioadm space, we built for these more commercial

3:21:35.400 --> 3:21:38.120
 and longevity data analysis purposes.

3:21:38.120 --> 3:21:41.200
 We're repurposing and feeding COVID data

3:21:41.200 --> 3:21:45.880
 into the same bioadm space and playing around

3:21:45.880 --> 3:21:49.440
 with graph embeddings from that graph into neural nets

3:21:49.440 --> 3:21:51.160
 for bioinformatics.

3:21:51.160 --> 3:21:55.000
 So it's both being a cool testing ground for some

3:21:55.000 --> 3:21:57.200
 of our bio AI learning and reasoning.

3:21:57.200 --> 3:21:59.960
 And it seems we're able to discover things

3:21:59.960 --> 3:22:01.840
 that people weren't seeing otherwise.

3:22:01.840 --> 3:22:04.960
 Because the thing in this case is for each combination

3:22:04.960 --> 3:22:07.160
 of antivirals, you may have only a few patients who've

3:22:07.160 --> 3:22:08.840
 tried that combination.

3:22:08.840 --> 3:22:11.680
 And those few patients may have their particular characteristics,

3:22:11.680 --> 3:22:14.040
 like this combination of three was tried only

3:22:14.040 --> 3:22:16.200
 on people age 80 or over.

3:22:16.200 --> 3:22:18.280
 This other combination of three, which

3:22:18.280 --> 3:22:20.440
 has an overlap with the first combination,

3:22:20.440 --> 3:22:22.120
 was tried more on young people.

3:22:22.120 --> 3:22:25.560
 So how do you combine those different pieces of data?

3:22:25.560 --> 3:22:28.640
 It's a very dodgy transfer learning problem,

3:22:28.640 --> 3:22:31.040
 which is the kind of thing that the probabilistic reasoning

3:22:31.040 --> 3:22:34.800
 algorithms we have inside OpenCog are better at than deep neural

3:22:34.800 --> 3:22:35.300
 networks.

3:22:35.300 --> 3:22:38.280
 On the other hand, you have gene expression data,

3:22:38.280 --> 3:22:40.800
 where you have 25,000 genes and the expression level

3:22:40.800 --> 3:22:43.640
 of each gene in the peripheral blood of each person.

3:22:43.640 --> 3:22:46.600
 So that sort of data, either deep neural nets or tools

3:22:46.600 --> 3:22:50.160
 like XGBoost or CatBoost, these decision forest trees,

3:22:50.160 --> 3:22:52.080
 are better at dealing with it than OpenCog,

3:22:52.080 --> 3:22:55.320
 because it's just these huge, messy floating point

3:22:55.320 --> 3:22:59.200
 vectors that are annoying for a logic engine to deal with,

3:22:59.200 --> 3:23:02.560
 but are perfect for a decision forest or a neural net.

3:23:02.560 --> 3:23:07.840
 So it's a great playground for hybrid AI methodology,

3:23:07.840 --> 3:23:09.720
 and we can have a singularity net,

3:23:09.720 --> 3:23:12.760
 have OpenCog in one agent, and XGBoost in a different agent,

3:23:12.760 --> 3:23:14.520
 and they talk to each other.

3:23:14.520 --> 3:23:18.000
 But at the same time, it's highly practical,

3:23:18.000 --> 3:23:22.720
 because we're working with, for example, some physicians

3:23:22.720 --> 3:23:25.520
 on this project, and the group, physicians in the group

3:23:25.520 --> 3:23:30.160
 called Anthropinion, based out of Vancouver and Seattle,

3:23:30.160 --> 3:23:33.960
 who are, these guys are working every day in the hospital

3:23:33.960 --> 3:23:36.520
 with patients dying of COVID.

3:23:36.520 --> 3:23:41.520
 So it's quite cool to see neural symbolic AI where

3:23:41.520 --> 3:23:45.440
 the rubber hits the road, trying to save people's lives.

3:23:45.440 --> 3:23:48.520
 I've been doing bio AI since 2001,

3:23:48.520 --> 3:23:51.200
 but mostly human longevity research,

3:23:51.200 --> 3:23:53.760
 and fly longevity research, try to understand

3:23:53.760 --> 3:23:57.200
 why some organisms really live a long time.

3:23:57.200 --> 3:24:00.360
 This is the first time, like, race against the clock,

3:24:00.360 --> 3:24:04.680
 and try to use the AI to figure out stuff that,

3:24:04.680 --> 3:24:09.600
 like, if we take two months longer to solve the AI problem,

3:24:09.600 --> 3:24:11.120
 some more people will die, because we

3:24:11.120 --> 3:24:13.320
 don't know what combination of antivirals to give them.

3:24:13.320 --> 3:24:16.640
 Yeah. At the societal level, at the biological level,

3:24:16.640 --> 3:24:21.240
 at any level, are you hopeful about us

3:24:21.240 --> 3:24:24.960
 as a human species getting out of this pandemic?

3:24:24.960 --> 3:24:26.640
 What are your thoughts on it in general?

3:24:26.640 --> 3:24:28.960
 The pandemic will be gone in a year or two

3:24:28.960 --> 3:24:30.520
 once there's a vaccine for it.

3:24:30.520 --> 3:24:32.960
 So, I mean, that's.

3:24:32.960 --> 3:24:35.560
 A lot of pain and suffering can happen in that time.

3:24:35.560 --> 3:24:38.560
 So that could be reversible on me.

3:24:38.560 --> 3:24:43.160
 I think if you spend much time in sub Saharan Africa,

3:24:43.160 --> 3:24:46.720
 you can see there's a lot of pain and suffering happening

3:24:46.720 --> 3:24:49.640
 all the time, like, you walk through the streets

3:24:49.640 --> 3:24:53.280
 of any large city in sub Saharan Africa,

3:24:53.280 --> 3:24:56.800
 and there are loads, I mean, tens of thousands,

3:24:56.800 --> 3:24:59.240
 probably hundreds of thousands of people,

3:24:59.240 --> 3:25:01.480
 lying by the side of the road,

3:25:01.480 --> 3:25:06.000
 dying mainly of curable diseases without food or water,

3:25:06.000 --> 3:25:08.440
 and either ostracized by their families or they left

3:25:08.440 --> 3:25:09.640
 their family house because they didn't want

3:25:09.640 --> 3:25:11.160
 to infect their family, right?

3:25:11.160 --> 3:25:14.400
 I mean, there's tremendous human suffering

3:25:14.400 --> 3:25:17.200
 on the planet all the time,

3:25:17.200 --> 3:25:19.520
 which most folks in the developed world

3:25:20.520 --> 3:25:25.000
 pay no attention to, and COVID is not remotely the worst.

3:25:25.000 --> 3:25:27.880
 How many people are dying of malaria all the time?

3:25:27.880 --> 3:25:30.400
 I mean, so COVID is bad.

3:25:30.400 --> 3:25:33.160
 It is by no mean the worst thing happening.

3:25:33.160 --> 3:25:36.400
 And setting aside diseases, I mean,

3:25:36.400 --> 3:25:39.640
 there are many places in the world where you're at risk

3:25:39.640 --> 3:25:42.600
 of having like your teenage son kidnapped by armed militias

3:25:42.600 --> 3:25:45.280
 and forced to get killed in someone else's war,

3:25:45.280 --> 3:25:48.280
 fighting tribe against tribe, I mean, so.

3:25:48.280 --> 3:25:50.480
 Humanity has a lot of problems,

3:25:50.480 --> 3:25:52.040
 which we don't need to have,

3:25:52.040 --> 3:25:56.040
 given the state of advancement of our technology right now.

3:25:56.040 --> 3:25:59.840
 And I think COVID is one of the easier problems to solve

3:25:59.840 --> 3:26:02.320
 in the sense that there are many brilliant people

3:26:02.320 --> 3:26:03.560
 working on vaccines.

3:26:03.560 --> 3:26:06.000
 We have the technology to create vaccines,

3:26:06.000 --> 3:26:08.520
 and we're gonna create new vaccines.

3:26:08.520 --> 3:26:10.200
 We should be more worried that we haven't managed

3:26:10.200 --> 3:26:12.880
 to defeat malaria after so long

3:26:12.880 --> 3:26:14.640
 and after the Gates Foundation and others

3:26:14.640 --> 3:26:18.400
 putting so much money into it.

3:26:18.400 --> 3:26:23.160
 I mean, I think clearly the whole global medical system,

3:26:23.160 --> 3:26:24.960
 the global health system,

3:26:24.960 --> 3:26:28.200
 and the global political and socioeconomic system

3:26:28.200 --> 3:26:33.200
 are incredibly unethical and unequal and badly designed.

3:26:33.200 --> 3:26:38.200
 And I mean, I don't know how to solve that directly.

3:26:39.640 --> 3:26:42.480
 I think what we can do indirectly to solve it

3:26:42.480 --> 3:26:46.480
 is to make systems that operate in parallel

3:26:46.480 --> 3:26:49.760
 and off to the side of the governments

3:26:49.760 --> 3:26:52.640
 that are nominally controlling the world

3:26:52.640 --> 3:26:55.480
 with their armies and militias.

3:26:55.480 --> 3:26:59.160
 And to the extent that you can make compassionate,

3:26:59.160 --> 3:27:01.480
 peer to peer, decentralized,

3:27:01.480 --> 3:27:05.400
 decentralized frameworks for doing things,

3:27:05.400 --> 3:27:08.360
 these are things that can start out unregulated,

3:27:08.360 --> 3:27:09.600
 and then if they get traction

3:27:09.600 --> 3:27:11.680
 before the regulators come in,

3:27:11.680 --> 3:27:14.080
 then they've influenced the way the world works, right?

3:27:14.080 --> 3:27:18.760
 SingularityNet aims to do this with AI,

3:27:18.760 --> 3:27:22.160
 Rejuve, which is a spinoff from SingularityNet.

3:27:22.160 --> 3:27:24.120
 You can see it, Rejuve.io.

3:27:24.120 --> 3:27:25.080
 How do you spell that?

3:27:25.080 --> 3:27:28.560
 R E J U V E, Rejuve.io.

3:27:28.560 --> 3:27:30.440
 That aims to do the same thing for medicine.

3:27:30.440 --> 3:27:33.800
 So it's like peer to peer sharing of medical data.

3:27:33.800 --> 3:27:35.000
 So you can share medical data

3:27:35.000 --> 3:27:36.880
 into a secure data wallet.

3:27:36.880 --> 3:27:39.240
 You can get advice about your health and longevity

3:27:39.240 --> 3:27:43.280
 through apps that Rejuve will launch

3:27:43.280 --> 3:27:44.800
 within the next couple of months.

3:27:44.800 --> 3:27:48.160
 And then SingularityNet AI can analyze all this data,

3:27:48.160 --> 3:27:50.240
 but then the benefits from that analysis

3:27:50.240 --> 3:27:52.920
 are spread among all the members of the network.

3:27:52.920 --> 3:27:54.840
 But I mean, of course,

3:27:54.840 --> 3:27:56.720
 I'm gonna hawk my particular projects,

3:27:56.720 --> 3:27:59.040
 but I mean, whether or not SingularityNet

3:27:59.040 --> 3:28:01.920
 and Rejuve are the answer,

3:28:01.920 --> 3:28:06.920
 I think it's key to create decentralized mechanisms

3:28:07.960 --> 3:28:09.280
 for everything.

3:28:09.280 --> 3:28:13.440
 I mean, for AI, for human health, for politics,

3:28:13.440 --> 3:28:17.880
 for jobs and employment, for sharing social information,

3:28:17.880 --> 3:28:21.800
 and to the extent decentralized peer to peer methods

3:28:21.800 --> 3:28:25.640
 designed with universal compassion at the core

3:28:25.640 --> 3:28:27.080
 can gain traction,

3:28:27.080 --> 3:28:31.360
 then these will just decrease the role that government has.

3:28:31.360 --> 3:28:34.960
 And I think that's much more likely to do good

3:28:34.960 --> 3:28:39.280
 than trying to explicitly reform the global government system.

3:28:39.280 --> 3:28:40.840
 I mean, I'm happy other people

3:28:40.840 --> 3:28:42.280
 are trying to explicitly reform

3:28:42.280 --> 3:28:44.000
 the global government system.

3:28:44.000 --> 3:28:47.280
 On the other hand, you look at how much good the internet

3:28:47.280 --> 3:28:50.760
 or Google did or mobile phones did.

3:28:50.760 --> 3:28:54.160
 Even you're making something that's decentralized

3:28:54.160 --> 3:28:55.720
 and throwing it out everywhere,

3:28:55.720 --> 3:28:59.280
 and it takes hold, then government has to adapt.

3:28:59.280 --> 3:29:02.440
 And I mean, that's what we need to do with AI and with health.

3:29:02.440 --> 3:29:05.040
 And in that light, I mean,

3:29:05.040 --> 3:29:09.160
 the centralization of healthcare and of AI

3:29:09.160 --> 3:29:11.880
 is certainly not ideal, right?

3:29:11.880 --> 3:29:14.240
 Like most AI PhDs are being sucked in

3:29:14.240 --> 3:29:17.280
 by half dozen to a dozen big companies.

3:29:17.280 --> 3:29:20.880
 Most AI processing power is being bought

3:29:20.880 --> 3:29:23.720
 by a few big companies for their own proprietary good.

3:29:23.720 --> 3:29:28.560
 And most medical research is within a few pharmaceutical companies.

3:29:28.560 --> 3:29:31.000
 And clinical trials run by pharmaceutical companies

3:29:31.000 --> 3:29:34.120
 will stay solid within those pharmaceutical companies.

3:29:34.120 --> 3:29:37.240
 You know, these large centralized entities,

3:29:37.240 --> 3:29:40.520
 which are intelligences in themselves, these corporations,

3:29:40.520 --> 3:29:42.400
 but they're mostly malevolent,

3:29:42.400 --> 3:29:45.840
 psychopathic and sociopathic intelligences.

3:29:45.840 --> 3:29:47.640
 Not saying the people involved are out,

3:29:47.640 --> 3:29:51.520
 but the corporations as self organizing entities on their own,

3:29:51.520 --> 3:29:54.320
 which are concerned with maximizing shareholder value

3:29:54.320 --> 3:29:57.160
 as a sole objective function.

3:29:57.160 --> 3:29:59.880
 I mean, AI and medicine are being sucked

3:29:59.880 --> 3:30:04.120
 into these pathological corporate organizations

3:30:04.120 --> 3:30:06.440
 with government cooperation.

3:30:06.440 --> 3:30:09.840
 And Google cooperating with British and US government

3:30:09.840 --> 3:30:12.560
 on this as one among many, many different examples.

3:30:12.560 --> 3:30:17.520
 23andMe providing you the nice service of sequencing your genome

3:30:17.520 --> 3:30:20.080
 and then licensing the genome to GlaxoSmithKlyna

3:30:20.080 --> 3:30:21.880
 on an exclusive basis, right?

3:30:21.880 --> 3:30:24.880
 Now, you can take your own DNA and do whatever you want with it.

3:30:24.880 --> 3:30:28.160
 But the pooled collection of 23andMe sequence DNA

3:30:28.160 --> 3:30:30.880
 is just to GlaxoSmithKlyna.

3:30:30.880 --> 3:30:34.480
 Someone else could reach out to everyone who had worked

3:30:34.480 --> 3:30:37.120
 with 23andMe to sequence their DNA and say,

3:30:37.120 --> 3:30:40.920
 give us your DNA for our open and decentralized repository

3:30:40.920 --> 3:30:42.680
 that will make available to everyone.

3:30:42.680 --> 3:30:45.720
 But nobody's doing that because it's a pain to get organized.

3:30:45.720 --> 3:30:48.880
 And the customer list is proprietary to 23andMe, right?

3:30:48.880 --> 3:30:54.960
 So, yeah, I mean, this, I think, is a greater risk to humanity

3:30:54.960 --> 3:30:58.400
 from AI than rogue AGIs turning the universe

3:30:58.400 --> 3:31:01.080
 into paperclips or a computronium.

3:31:01.080 --> 3:31:05.920
 Because what you have here is mostly goodhearted and nice

3:31:05.920 --> 3:31:09.880
 people who are sucked into a mode of organization

3:31:09.880 --> 3:31:12.800
 of large corporations, which has evolved just

3:31:12.800 --> 3:31:14.760
 for no individual's fault, just because that's

3:31:14.760 --> 3:31:16.760
 the way society has evolved.

3:31:16.760 --> 3:31:18.600
 It's not altruistic as self interested

3:31:18.600 --> 3:31:20.480
 and become psychopathic, like you said.

3:31:20.480 --> 3:31:23.640
 The corporation is psychopathic, even if the people are not.

3:31:23.640 --> 3:31:26.600
 And that's really the disturbing thing about it,

3:31:26.600 --> 3:31:30.440
 because the corporations can do things

3:31:30.440 --> 3:31:33.480
 that are quite bad for society, even if nobody has,

3:31:33.480 --> 3:31:35.880
 nobody has a bad intention.

3:31:35.880 --> 3:31:37.960
 And then no individual member of that corporation

3:31:37.960 --> 3:31:38.640
 has a bad intention.

3:31:38.640 --> 3:31:40.960
 No, some probably do, but it's not

3:31:40.960 --> 3:31:43.120
 necessary that they do for the corporation.

3:31:43.120 --> 3:31:47.000
 Like, I mean, Google, I know a lot of people on Google,

3:31:47.000 --> 3:31:49.720
 and there are, with very few exceptions,

3:31:49.720 --> 3:31:51.960
 they're all very nice people who genuinely

3:31:51.960 --> 3:31:53.920
 want what's good for the world.

3:31:53.920 --> 3:31:56.880
 And Facebook, I know fewer people,

3:31:56.880 --> 3:31:59.000
 but it's probably mostly true.

3:31:59.000 --> 3:32:02.880
 It's probably like fine young geeks who want to build

3:32:02.880 --> 3:32:03.880
 cool technology.

3:32:03.880 --> 3:32:06.360
 I actually tend to believe that even the leaders, even Mark

3:32:06.360 --> 3:32:09.640
 Zuckerberg, one of the most disliked people in tech,

3:32:09.640 --> 3:32:11.840
 is also wants to do good for the world.

3:32:11.840 --> 3:32:13.840
 What do you think about Jamie Dimon?

3:32:13.840 --> 3:32:14.880
 Who's Jamie Dimon?

3:32:14.880 --> 3:32:16.440
 The heads of the great banks may have

3:32:16.440 --> 3:32:17.480
 a different psychology.

3:32:17.480 --> 3:32:17.980
 Oh, boy.

3:32:17.980 --> 3:32:18.480
 Yeah.

3:32:18.480 --> 3:32:22.760
 Well, I tend to be naive about these things

3:32:22.760 --> 3:32:26.080
 and see the best.

3:32:26.080 --> 3:32:27.760
 I tend to agree with you that I think

3:32:27.760 --> 3:32:30.480
 the individuals want to do good by the world.

3:32:30.480 --> 3:32:33.000
 But the mechanism of the company can sometimes

3:32:33.000 --> 3:32:34.760
 be its own intelligence system.

3:32:34.760 --> 3:32:38.480
 I mean, there's a, I, and one of my cousin, Mario Gertzels,

3:32:38.480 --> 3:32:41.680
 worked for Microsoft since 1985 or something.

3:32:41.680 --> 3:32:47.040
 And I can see for him, I mean, as well as just working

3:32:47.040 --> 3:32:50.560
 on cool projects, your coding stuff

3:32:50.560 --> 3:32:54.520
 that gets used by billions and billions of people.

3:32:54.520 --> 3:32:57.600
 And do you think if I improve this feature,

3:32:57.600 --> 3:33:00.200
 that's making billions of people's lives easier, right?

3:33:00.200 --> 3:33:03.040
 So of course, that's cool.

3:33:03.040 --> 3:33:06.360
 And the engineers are not in charge of running the company

3:33:06.360 --> 3:33:06.840
 anyway.

3:33:06.840 --> 3:33:10.080
 And of course, even if you're Mark Zuckerberg or Larry Page,

3:33:10.080 --> 3:33:13.520
 I mean, you still have a fiduciary responsibility.

3:33:13.520 --> 3:33:16.320
 And I mean, you're responsible to the shareholders,

3:33:16.320 --> 3:33:19.600
 your employees, who you want to keep paying them and so forth.

3:33:19.600 --> 3:33:22.880
 So yeah, you're invested in this system.

3:33:22.880 --> 3:33:27.000
 And when I worked in DC, I worked

3:33:27.000 --> 3:33:29.360
 a bunch with INSCOM, US Army Intelligence.

3:33:29.360 --> 3:33:31.840
 And I was heavily politically opposed

3:33:31.840 --> 3:33:34.680
 to what the US Army was doing in Iraq at that time,

3:33:34.680 --> 3:33:36.480
 like torturing people in Abu Ghraib.

3:33:36.480 --> 3:33:39.800
 But everyone I knew in US Army and INSCOM,

3:33:39.800 --> 3:33:42.560
 when I hung out with them, was a very nice person.

3:33:42.560 --> 3:33:43.480
 They were friendly to me.

3:33:43.480 --> 3:33:46.120
 They were nice to my kids and my dogs, right?

3:33:46.120 --> 3:33:48.480
 And they really believed that the US was

3:33:48.480 --> 3:33:49.600
 fighting the forces of evil.

3:33:49.600 --> 3:33:52.240
 And if you ask me about Abu Ghraib, they're like, well,

3:33:52.240 --> 3:33:54.400
 but these Arabs will chop us into pieces.

3:33:54.400 --> 3:33:58.320
 So how can you say we're wrong to waterboard them a bit, right?

3:33:58.320 --> 3:34:00.280
 Like that's much less than what they would do to us.

3:34:00.280 --> 3:34:04.040
 It's just in their worldview, what they were doing

3:34:04.040 --> 3:34:07.480
 was really genuinely for the good of humanity.

3:34:07.480 --> 3:34:08.960
 None of them woke up in the morning

3:34:08.960 --> 3:34:12.240
 and said like, I want to do harm to good people

3:34:12.240 --> 3:34:14.520
 because I'm just a nasty guy, right?

3:34:14.520 --> 3:34:18.200
 So, yeah, most people on the planet

3:34:18.200 --> 3:34:21.760
 setting aside a few genuine psychopaths and sociopaths,

3:34:21.760 --> 3:34:23.560
 I mean, most people on the planet

3:34:23.560 --> 3:34:27.560
 have a heavy dose of benevolence and wanting to do good.

3:34:27.560 --> 3:34:32.120
 And also a heavy capability to convince themselves,

3:34:32.120 --> 3:34:34.480
 whatever they feel like doing or whatever is best for them

3:34:34.480 --> 3:34:37.040
 is for the good of humankind, right?

3:34:37.040 --> 3:34:40.480
 So the more we can decentralize control of...

3:34:40.480 --> 3:34:44.960
 Decentralization, you know, the democracy is horrible,

3:34:44.960 --> 3:34:47.360
 but this is like Winston Churchill said,

3:34:47.360 --> 3:34:49.440
 you know, it's the worst possible system of government

3:34:49.440 --> 3:34:50.760
 except for all the others, right?

3:34:50.760 --> 3:34:53.960
 I mean, I think the whole mess of humanity

3:34:53.960 --> 3:34:56.960
 has many, many very bad aspects to it,

3:34:56.960 --> 3:35:00.360
 but so far the track record of elite groups

3:35:00.360 --> 3:35:02.560
 who know what's better for all of humanity

3:35:02.560 --> 3:35:04.600
 is much worse than the track record

3:35:04.600 --> 3:35:08.760
 of the whole teaming democratic participatory mess

3:35:08.760 --> 3:35:09.600
 of humanity, right?

3:35:09.600 --> 3:35:13.480
 I mean, none of them is perfect by any means.

3:35:13.480 --> 3:35:15.160
 The issue with a small elite group

3:35:15.160 --> 3:35:18.880
 that knows what's best is even if it starts out

3:35:18.880 --> 3:35:21.920
 as truly benevolent and doing good things

3:35:21.920 --> 3:35:25.000
 in accordance with its initial good intentions,

3:35:25.000 --> 3:35:26.640
 you find out you need more resources,

3:35:26.640 --> 3:35:28.080
 you need a bigger organization,

3:35:28.080 --> 3:35:31.320
 you pull in more people, internal politics arises,

3:35:31.320 --> 3:35:35.040
 the difference of opinions arise and bribery happens.

3:35:35.040 --> 3:35:39.520
 Like some opponent organization takes a second in command now

3:35:39.520 --> 3:35:40.960
 to make some, the first in command

3:35:40.960 --> 3:35:42.640
 of some other organization.

3:35:42.640 --> 3:35:45.600
 And I mean, that's, there's a lot of history

3:35:45.600 --> 3:35:47.360
 of what happens with elite groups

3:35:47.360 --> 3:35:50.080
 thinking they know what's best for the human race.

3:35:50.080 --> 3:35:53.080
 So yeah, if I have to choose,

3:35:53.080 --> 3:35:55.480
 I'm gonna reluctantly put my faith

3:35:55.480 --> 3:35:58.960
 in the vast democratic decentralized mass.

3:35:58.960 --> 3:36:02.920
 And I think corporations have a track record

3:36:02.920 --> 3:36:05.360
 of being ethically worse

3:36:05.360 --> 3:36:07.480
 than their constituent human parts.

3:36:07.480 --> 3:36:10.920
 And, you know, democratic governments

3:36:10.920 --> 3:36:14.720
 have a more mixed track record, but there are at least...

3:36:14.720 --> 3:36:15.880
 That's the best we got.

3:36:15.880 --> 3:36:18.520
 Yeah, I mean, you can, there's Iceland,

3:36:18.520 --> 3:36:19.680
 very nice country, right?

3:36:19.680 --> 3:36:23.360
 I mean, very democratic for 800 plus years,

3:36:23.360 --> 3:36:26.840
 very, very benevolent, beneficial government.

3:36:26.840 --> 3:36:28.840
 And I think, yeah, there are track records

3:36:28.840 --> 3:36:33.840
 of democratic modes of organization, Linux, for example.

3:36:34.240 --> 3:36:36.040
 Some of the people in charge of Linux

3:36:36.040 --> 3:36:38.600
 are overtly complete assholes, right?

3:36:38.600 --> 3:36:41.760
 And trying to reform themselves in many cases,

3:36:41.760 --> 3:36:46.000
 in other cases not, but the organization as a whole,

3:36:46.000 --> 3:36:49.720
 I think it's done a good job overall.

3:36:49.720 --> 3:36:53.960
 It's been very welcoming in the third world, for example.

3:36:53.960 --> 3:36:56.080
 And it's allowed advanced technology

3:36:56.080 --> 3:36:58.520
 to roll out on all sorts of different embedded devices

3:36:58.520 --> 3:37:01.240
 and platforms in places where people couldn't afford

3:37:01.240 --> 3:37:03.800
 to pay for proprietary software.

3:37:03.800 --> 3:37:08.800
 So I'd say the internet, Linux, and many democratic nations

3:37:09.160 --> 3:37:11.360
 are examples of how sort of an open,

3:37:11.360 --> 3:37:14.040
 decentralized democratic methodology

3:37:14.040 --> 3:37:16.600
 can be ethically better than the sum of the parts

3:37:16.600 --> 3:37:17.440
 rather than worse.

3:37:17.440 --> 3:37:21.440
 And corporations, that has happened only for a brief period

3:37:21.440 --> 3:37:24.600
 and then it goes sour, right?

3:37:24.600 --> 3:37:27.000
 I mean, I'd say a similar thing about universities.

3:37:27.000 --> 3:37:30.920
 Like university is a horrible way to organize research

3:37:30.920 --> 3:37:33.680
 and get things done, yet it's better than anything else

3:37:33.680 --> 3:37:34.840
 we've come up with, right?

3:37:34.840 --> 3:37:38.320
 Company can be much better, but for a brief period of time

3:37:38.320 --> 3:37:42.700
 and then it stops being so good, right?

3:37:42.700 --> 3:37:47.360
 So then I think if you believe that AGI

3:37:47.360 --> 3:37:50.720
 is gonna emerge sort of incrementally

3:37:50.720 --> 3:37:53.640
 out of AIs doing practical stuff in the world,

3:37:53.640 --> 3:37:57.080
 like controlling humanoid robots or driving cars

3:37:57.080 --> 3:38:01.240
 or diagnosing diseases or operating killer drones

3:38:01.240 --> 3:38:04.560
 or spying on people and reporting on to the government,

3:38:04.560 --> 3:38:09.560
 then what kind of organization creates more and more

3:38:09.640 --> 3:38:12.480
 advanced narrow AI verging toward AGI

3:38:12.480 --> 3:38:14.840
 may be quite important because it will guide like

3:38:14.840 --> 3:38:18.640
 what's in the mind of the early stage AGI

3:38:18.640 --> 3:38:21.780
 as it first gains the ability to rewrite its own code base

3:38:21.780 --> 3:38:24.720
 and project itself toward super intelligence.

3:38:24.720 --> 3:38:29.720
 And if you believe that AI may move toward AGI

3:38:31.160 --> 3:38:33.280
 out of this sort of synergetic activity

3:38:33.280 --> 3:38:35.760
 of many agents cooperating together,

3:38:35.760 --> 3:38:37.840
 rather than just to have one person's project,

3:38:37.840 --> 3:38:40.800
 then who owns and controls that platform

3:38:40.800 --> 3:38:45.800
 for AI cooperation becomes also very, very important, right?

3:38:47.280 --> 3:38:50.560
 And is that platform AWS, is it Google Cloud?

3:38:50.560 --> 3:38:51.800
 Is it Alibaba?

3:38:51.800 --> 3:38:53.400
 Or is it something more like the internet

3:38:53.400 --> 3:38:56.720
 or SingularityNet, which is open and decentralized?

3:38:56.720 --> 3:39:01.120
 So if all of my weird machinations come to pass, right?

3:39:01.120 --> 3:39:03.760
 I mean, we have the Hanson robots

3:39:03.760 --> 3:39:06.160
 being a beautiful user interface,

3:39:06.160 --> 3:39:09.080
 gathering information on human values

3:39:09.080 --> 3:39:11.440
 and being loving and compassionate to people

3:39:11.440 --> 3:39:14.600
 in medical home service, robot office applications.

3:39:14.600 --> 3:39:16.880
 You have SingularityNet in the backend

3:39:16.880 --> 3:39:19.440
 networking together many different AIs

3:39:19.440 --> 3:39:21.440
 toward cooperative intelligence,

3:39:21.440 --> 3:39:23.980
 fueling the robots among many other things.

3:39:23.980 --> 3:39:27.320
 You have OpenCog 2.0 and TrueAGI

3:39:27.320 --> 3:39:29.360
 as one of the sources of AI

3:39:29.360 --> 3:39:31.680
 inside this decentralized network,

3:39:31.680 --> 3:39:34.080
 powering the robot and medical AIs

3:39:34.080 --> 3:39:36.280
 helping us live a long time

3:39:36.280 --> 3:39:39.680
 and cure diseases among other things.

3:39:39.680 --> 3:39:43.720
 And this whole thing is operating in a democratic

3:39:43.720 --> 3:39:46.040
 and decentralized way, right?

3:39:46.040 --> 3:39:50.400
 I think if anyone can pull something like this off,

3:39:50.400 --> 3:39:53.200
 whether using the specific technologies

3:39:53.200 --> 3:39:55.480
 I've mentioned or something else,

3:39:55.480 --> 3:39:58.400
 I mean, then I think we have a higher odds

3:39:58.400 --> 3:40:02.740
 of moving toward a beneficial technological Singularity

3:40:02.740 --> 3:40:06.200
 rather than one in which the first super AGI

3:40:06.200 --> 3:40:08.760
 is indifferent to humans and just considers us

3:40:08.760 --> 3:40:11.120
 an inefficient use of molecules.

3:40:11.900 --> 3:40:15.540
 That was a beautifully articulated vision for the world.

3:40:15.540 --> 3:40:16.720
 So thank you for that.

3:40:16.720 --> 3:40:20.520
 Well, let's talk a little bit about life and death.

3:40:21.920 --> 3:40:23.880
 I'm pro life and anti death.

3:40:23.880 --> 3:40:28.080
 For most people, there's few exceptions

3:40:28.080 --> 3:40:29.680
 that I won't mention here.

3:40:29.680 --> 3:40:32.400
 I'm glad just like your dad,

3:40:32.400 --> 3:40:34.740
 you're taking a stand against death.

3:40:36.480 --> 3:40:40.000
 You have, by the way, you have a bunch of awesome music

3:40:40.000 --> 3:40:41.840
 where you play piano online.

3:40:41.840 --> 3:40:46.000
 One of the songs that I believe you've written,

3:40:46.000 --> 3:40:49.120
 the lyrics go, by the way, I like the way it sounds.

3:40:49.120 --> 3:40:51.440
 People should listen to it, it's awesome.

3:40:51.440 --> 3:40:54.960
 I considered, I probably will cover it, it's a good song.

3:40:54.960 --> 3:40:58.640
 Tell me why do you think it is a good thing

3:40:58.640 --> 3:41:00.600
 that we all get old and die?

3:41:00.600 --> 3:41:03.160
 One of the songs, I love the way it sounds.

3:41:03.160 --> 3:41:05.680
 But let me ask you about death first.

3:41:06.760 --> 3:41:08.320
 Do you think there's an element to death

3:41:08.320 --> 3:41:12.240
 that's essential to give our life meaning?

3:41:12.240 --> 3:41:14.600
 Like the fact that this thing ends?

3:41:14.600 --> 3:41:19.200
 Well, let me say, I'm pleased and a little embarrassed.

3:41:19.200 --> 3:41:21.480
 You've been listening to that music I put online.

3:41:21.480 --> 3:41:22.320
 That's awesome.

3:41:22.320 --> 3:41:24.960
 One of my regrets in life recently is

3:41:24.960 --> 3:41:28.440
 I would love to get time to really produce music well.

3:41:28.440 --> 3:41:32.040
 Like I haven't touched my sequencer software in like five years.

3:41:32.040 --> 3:41:37.040
 Like I would love to like rehearse and produce and edit

3:41:37.040 --> 3:41:39.680
 but with a two year old baby

3:41:39.680 --> 3:41:42.320
 and trying to create the singularity, there's no time.

3:41:42.320 --> 3:41:44.840
 So I just made the decision to,

3:41:45.760 --> 3:41:47.800
 when I'm playing random shit in an off moment.

3:41:47.800 --> 3:41:48.640
 Just record it.

3:41:48.640 --> 3:41:49.640
 Just record it.

3:41:49.640 --> 3:41:51.880
 Oh, you just put it out there like whatever.

3:41:51.880 --> 3:41:54.520
 Maybe if I'm unfortunate enough to die,

3:41:54.520 --> 3:41:56.320
 maybe that can be input to the AGI

3:41:56.320 --> 3:41:59.040
 when it tries to make an accurate mind upload of me, right?

3:41:59.040 --> 3:42:00.200
 Death is bad.

3:42:00.200 --> 3:42:02.760
 I mean, that's very simple.

3:42:02.760 --> 3:42:04.360
 It's battling, we should have to say that.

3:42:04.360 --> 3:42:08.760
 I mean, of course, people can make meaning out of death.

3:42:08.760 --> 3:42:10.960
 And if someone is tortured,

3:42:10.960 --> 3:42:13.240
 maybe they can make beautiful meaning out of that torture

3:42:13.240 --> 3:42:14.600
 and write a beautiful poem

3:42:14.600 --> 3:42:17.000
 about what it was like to be tortured, right?

3:42:17.000 --> 3:42:19.160
 I mean, we're very creative.

3:42:19.160 --> 3:42:22.440
 We can melt beauty and positivity

3:42:22.440 --> 3:42:25.320
 out of even the most horrible and shitty things.

3:42:25.320 --> 3:42:27.920
 But just because if I was tortured,

3:42:27.920 --> 3:42:29.720
 I could write a good song about what it was like

3:42:29.720 --> 3:42:32.000
 to be tortured doesn't make torture good.

3:42:32.000 --> 3:42:35.680
 And just because people are able to derive meaning

3:42:35.680 --> 3:42:37.520
 and value from death,

3:42:37.520 --> 3:42:39.680
 doesn't mean they wouldn't derive even better meaning

3:42:39.680 --> 3:42:42.600
 and value from ongoing life without death.

3:42:42.600 --> 3:42:43.440
 Which I very...

3:42:43.440 --> 3:42:44.280
 You're definite.

3:42:44.280 --> 3:42:45.120
 Yeah, yeah.

3:42:45.120 --> 3:42:47.800
 So if you could live forever, would you live forever?

3:42:47.800 --> 3:42:48.640
 Forever.

3:42:50.480 --> 3:42:52.880
 My goal with longevity research

3:42:52.880 --> 3:42:57.520
 is to abolish the plague of involuntary death.

3:42:57.520 --> 3:43:00.380
 I don't think people should die unless they choose to die.

3:43:00.380 --> 3:43:05.380
 If I had to choose forced immortality

3:43:05.780 --> 3:43:09.240
 versus dying, I would choose forced immortality.

3:43:09.240 --> 3:43:10.500
 On the other hand,

3:43:10.500 --> 3:43:13.540
 if I had the choice of immortality

3:43:13.540 --> 3:43:15.660
 with the choice of suicide whenever I felt like it,

3:43:15.660 --> 3:43:17.260
 of course I would take that instead.

3:43:17.260 --> 3:43:18.900
 And that's the more realistic choice.

3:43:18.900 --> 3:43:21.700
 I mean, there's no reason you should have forced immortality.

3:43:21.700 --> 3:43:23.380
 You should be able to live

3:43:23.380 --> 3:43:26.100
 until you get sick of living, right?

3:43:26.100 --> 3:43:29.820
 I mean, that will seem insanely obvious

3:43:29.820 --> 3:43:31.420
 to everyone 50 years from now.

3:43:31.420 --> 3:43:33.220
 And they will be so...

3:43:33.220 --> 3:43:36.020
 I mean, people who thought death gives meaning to life

3:43:36.020 --> 3:43:37.700
 so we should all die,

3:43:37.700 --> 3:43:39.420
 they will look at that 50 years from now.

3:43:39.420 --> 3:43:43.380
 The way we now look at the Anabaptists in the year 1000

3:43:43.380 --> 3:43:45.180
 who gave away all their positions,

3:43:45.180 --> 3:43:47.060
 went on top of the mountain for Jesus,

3:43:47.060 --> 3:43:50.260
 for Jesus to come and bring them to the ascension.

3:43:50.260 --> 3:43:55.260
 I mean, it's ridiculous that people think death is good

3:43:55.780 --> 3:43:59.300
 because you gain more wisdom

3:43:59.300 --> 3:44:00.140
 as you approach dying.

3:44:00.140 --> 3:44:01.980
 I mean, of course it's true.

3:44:01.980 --> 3:44:03.500
 I mean, I'm 53.

3:44:03.500 --> 3:44:08.260
 And the fact that I might have only a few more decades left,

3:44:08.260 --> 3:44:11.460
 it does make me reflect on things differently.

3:44:11.460 --> 3:44:15.740
 It does give me a deeper understanding of many things.

3:44:15.740 --> 3:44:18.100
 But I mean, so what?

3:44:18.100 --> 3:44:19.540
 You could get a deep understanding

3:44:19.540 --> 3:44:20.940
 in a lot of different ways.

3:44:20.940 --> 3:44:22.060
 Pain is the same way.

3:44:22.060 --> 3:44:24.300
 Like we're gonna abolish pain.

3:44:24.300 --> 3:44:27.460
 And that's even more amazing than abolishing death, right?

3:44:27.460 --> 3:44:30.420
 I mean, once we get a little better at neuroscience,

3:44:30.420 --> 3:44:32.660
 we'll be able to go in and adjust the brain

3:44:32.660 --> 3:44:34.780
 so that pain doesn't hurt anymore, right?

3:44:34.780 --> 3:44:37.140
 And that, you know, people will say that's bad

3:44:37.140 --> 3:44:39.460
 because there's so much beauty

3:44:39.460 --> 3:44:41.140
 and overcoming pain and suffering.

3:44:41.140 --> 3:44:42.380
 Oh, sure.

3:44:42.380 --> 3:44:44.740
 And there's beauty and overcoming torture too,

3:44:44.740 --> 3:44:46.900
 but, and some people like to cut themselves,

3:44:46.900 --> 3:44:48.100
 but not many, right?

3:44:48.100 --> 3:44:48.940
 I mean.

3:44:48.940 --> 3:44:50.580
 That's an interesting, so, but to push,

3:44:50.580 --> 3:44:52.300
 I mean, to push back again,

3:44:52.300 --> 3:44:53.340
 this is the Russian side of me,

3:44:53.340 --> 3:44:55.060
 I do romanticize suffering.

3:44:55.060 --> 3:44:56.420
 It's not obvious.

3:44:56.420 --> 3:44:59.500
 I mean, the way you put it, it seems very logical.

3:44:59.500 --> 3:45:03.940
 It's almost absurd to romanticize suffering or pain or death.

3:45:03.940 --> 3:45:07.780
 But to me, a world without suffering,

3:45:07.780 --> 3:45:09.580
 without pain, without death,

3:45:09.580 --> 3:45:10.900
 it's not obvious what that world looks like.

3:45:10.900 --> 3:45:13.540
 Well, then you can stay in the people's zoo

3:45:13.540 --> 3:45:15.500
 with the people torturing each other, right?

3:45:15.500 --> 3:45:18.180
 No, but what I'm saying is, I don't,

3:45:18.180 --> 3:45:20.260
 well, that's, I guess what I'm trying to say,

3:45:20.260 --> 3:45:22.860
 I don't know if I was presented with that choice,

3:45:22.860 --> 3:45:25.540
 what I would choose because it, to me.

3:45:25.540 --> 3:45:30.100
 This is a subtler, it's a subtler matter.

3:45:30.100 --> 3:45:33.980
 And I've posed it in this conversation

3:45:33.980 --> 3:45:37.100
 in an unnecessarily extreme way.

3:45:37.100 --> 3:45:41.060
 So I think, I think the way you should think about it

3:45:41.060 --> 3:45:44.700
 is what if there's a little dial on the side of your head

3:45:44.700 --> 3:45:48.180
 and you could turn to how much pain hurt,

3:45:48.180 --> 3:45:50.660
 turn it down to zero, turn up to 11,

3:45:50.660 --> 3:45:52.220
 like in spinal tap if it wants,

3:45:52.220 --> 3:45:53.980
 maybe through an actual spinal tap, right?

3:45:53.980 --> 3:45:58.580
 So I mean, would you opt to have that dial there or not?

3:45:58.580 --> 3:45:59.700
 That's the question.

3:45:59.700 --> 3:46:02.300
 The question isn't whether you would turn the pain down

3:46:02.300 --> 3:46:05.220
 to zero all the time.

3:46:05.220 --> 3:46:07.180
 Would you opt to have the dial or not?

3:46:07.180 --> 3:46:10.020
 My guess is that in some dark moment of your life,

3:46:10.020 --> 3:46:11.940
 you would choose to have the dial implanted

3:46:11.940 --> 3:46:13.340
 and then it would be there.

3:46:13.340 --> 3:46:17.180
 Just to confess a small thing, don't ask me why,

3:46:17.180 --> 3:46:20.740
 but I'm doing this physical challenge currently

3:46:20.740 --> 3:46:24.380
 where I'm doing 680 pushups and pullups a day.

3:46:25.820 --> 3:46:29.140
 And my shoulder is currently as we sit here

3:46:29.140 --> 3:46:30.660
 in a lot of pain.

3:46:30.660 --> 3:46:34.420
 And I don't know.

3:46:34.420 --> 3:46:36.820
 I would certainly right now, if you gave me a dial,

3:46:36.820 --> 3:46:40.460
 I would turn that sucker to zero as quickly as possible.

3:46:40.460 --> 3:46:45.460
 But I think the whole point of this journey is,

3:46:46.700 --> 3:46:47.540
 I don't know.

3:46:47.540 --> 3:46:49.500
 Well, because you're a twisted human being.

3:46:49.500 --> 3:46:50.660
 I'm a twisted.

3:46:50.660 --> 3:46:53.580
 So the question is, am I somehow twisted

3:46:53.580 --> 3:46:57.420
 because I created some kind of narrative for myself

3:46:57.420 --> 3:47:00.780
 so that I can deal with the injustice

3:47:00.780 --> 3:47:02.380
 and the suffering in the world?

3:47:03.660 --> 3:47:06.340
 Or is this actually going to be a source of happiness

3:47:06.340 --> 3:47:07.180
 from humans?

3:47:07.180 --> 3:47:10.780
 Well, to an extent is a research question

3:47:10.780 --> 3:47:12.260
 that humanity will undertake, right?

3:47:12.260 --> 3:47:19.740
 So I mean, human beings do have a particular biological makeup,

3:47:19.740 --> 3:47:23.740
 which sort of implies a certain probability distribution

3:47:23.740 --> 3:47:25.900
 over motivational systems, right?

3:47:25.900 --> 3:47:28.900
 So I mean, we, and that is there.

3:47:28.900 --> 3:47:29.740
 Well put.

3:47:29.740 --> 3:47:30.580
 That is there.

3:47:30.580 --> 3:47:35.580
 Now, the question is, how flexibly can that morph

3:47:36.580 --> 3:47:39.020
 as society and technology change, right?

3:47:39.020 --> 3:47:43.740
 So if we're given that dial and we're given a society

3:47:43.740 --> 3:47:47.540
 in which say we don't have to work for a living

3:47:47.540 --> 3:47:50.700
 and in which there's an ambient decentralized

3:47:50.700 --> 3:47:52.460
 benevolent AI network that will warn us

3:47:52.460 --> 3:47:54.420
 when we're about to hurt ourselves.

3:47:54.420 --> 3:47:57.060
 If we're in a different context,

3:47:57.060 --> 3:48:02.060
 can we consistently with being genuinely and fully human,

3:48:02.860 --> 3:48:05.900
 can we consistently get into a state of consciousness

3:48:05.900 --> 3:48:09.220
 where we just want to keep the pain dial turned

3:48:09.220 --> 3:48:12.420
 all the way down and yet we're leading very rewarding

3:48:12.420 --> 3:48:13.860
 and fulfilling lives, right?

3:48:13.860 --> 3:48:17.660
 Now, I suspect the answer is yes, we can do that,

3:48:17.660 --> 3:48:21.580
 but I don't know that for certain.

3:48:21.580 --> 3:48:25.980
 Yeah, now I'm more confident that we could create

3:48:25.980 --> 3:48:30.540
 a nonhuman AGI system which just didn't need

3:48:30.540 --> 3:48:33.100
 an analog of feeling pain.

3:48:33.100 --> 3:48:37.420
 And I think that AGI system will be fundamentally healthier

3:48:37.420 --> 3:48:39.780
 and more benevolent than human beings.

3:48:39.780 --> 3:48:42.380
 So I think it might or might not be true

3:48:42.380 --> 3:48:45.220
 that humans need a certain element of suffering

3:48:45.220 --> 3:48:47.420
 to be satisfied humans,

3:48:47.420 --> 3:48:49.460
 consistent with the human physiology.

3:48:49.460 --> 3:48:51.980
 If it is true, that's one of the things

3:48:51.980 --> 3:48:54.820
 that makes us fucked and disqualified

3:48:54.820 --> 3:48:58.420
 to be the super AGI, right?

3:48:58.420 --> 3:49:03.420
 I mean, the nature of the human motivational system

3:49:03.580 --> 3:49:08.580
 is that we seem to gravitate towards situations

3:49:08.580 --> 3:49:12.700
 where the best thing in the large scale

3:49:12.700 --> 3:49:15.820
 is not the best thing in the small scale,

3:49:15.820 --> 3:49:18.060
 according to our subjective value system.

3:49:18.060 --> 3:49:20.700
 So we gravitate towards subjective value judgments

3:49:20.700 --> 3:49:22.900
 where to gratify ourselves in the large,

3:49:22.900 --> 3:49:25.580
 we have to ungratify ourselves in the small.

3:49:25.580 --> 3:49:29.300
 And we do that in, you see that in music,

3:49:29.300 --> 3:49:31.700
 there's a theory of music which says

3:49:31.700 --> 3:49:33.740
 the key to musical aesthetics

3:49:33.740 --> 3:49:36.820
 is the surprising fulfillment of expectations.

3:49:36.820 --> 3:49:38.900
 Like you want something that will fulfill

3:49:38.900 --> 3:49:41.820
 the expectations elicited in the prior part of the music,

3:49:41.820 --> 3:49:44.780
 but in a way with a bit of a twist that surprises you.

3:49:44.780 --> 3:49:48.100
 And I mean, that's true not only in out there music

3:49:48.100 --> 3:49:51.980
 like my own or that of Zappa or Steve Vai

3:49:51.980 --> 3:49:55.460
 or Buckethead or Christoph Penderecki or something.

3:49:55.460 --> 3:49:57.980
 It's even there in Mozart or something.

3:49:57.980 --> 3:49:59.980
 It's not there in elevator music too much,

3:49:59.980 --> 3:50:02.940
 but that's why it's boring, right?

3:50:02.940 --> 3:50:05.380
 But wrapped up in there is,

3:50:05.380 --> 3:50:07.540
 we want to hurt a little bit

3:50:07.540 --> 3:50:11.300
 so that we can feel the pain go away.

3:50:11.300 --> 3:50:15.700
 Like we wanna be a little confused by what's coming next.

3:50:15.700 --> 3:50:17.500
 So then when the thing that comes next

3:50:17.500 --> 3:50:19.900
 actually makes sense, it's so satisfying, right?

3:50:19.900 --> 3:50:22.300
 It's the surprising fulfillment of expectations

3:50:22.300 --> 3:50:24.260
 that we said, so beautifully put.

3:50:24.260 --> 3:50:26.860
 Is there, we've been skirting around a little bit,

3:50:26.860 --> 3:50:29.420
 but if I were to ask you the most ridiculous big question

3:50:29.420 --> 3:50:34.420
 of what is the meaning of life, what would your answer be?

3:50:37.380 --> 3:50:40.100
 Three values, joy, growth and choice.

3:50:43.620 --> 3:50:46.460
 I think you need joy.

3:50:46.460 --> 3:50:48.100
 I mean, that's the basis of everything

3:50:48.100 --> 3:50:49.740
 if you want the number one value.

3:50:49.740 --> 3:50:53.340
 On the other hand, I'm unsatisfied

3:50:53.340 --> 3:50:56.260
 with a static joy that doesn't progress

3:50:56.260 --> 3:50:58.940
 perhaps because of some elemental element

3:50:58.940 --> 3:51:00.180
 of human perversity,

3:51:00.180 --> 3:51:02.260
 but the idea of something that grows

3:51:02.260 --> 3:51:04.900
 and becomes more and more and better and better

3:51:04.900 --> 3:51:06.820
 in some sense appeals to me.

3:51:06.820 --> 3:51:10.620
 But I also sort of like the idea of individuality

3:51:10.620 --> 3:51:14.580
 that as a distinct system, I have some agencies.

3:51:14.580 --> 3:51:18.860
 So there's some nexus of causality within this system

3:51:18.860 --> 3:51:22.460
 rather than the causality being wholly evenly distributed

3:51:22.460 --> 3:51:23.940
 over the joyous growing mass.

3:51:23.940 --> 3:51:27.100
 So you start with joy, growth and choice

3:51:27.100 --> 3:51:28.860
 as three basic values.

3:51:28.860 --> 3:51:31.980
 Those three things could continue indefinitely.

3:51:31.980 --> 3:51:35.180
 That's not, that's something that can last forever.

3:51:35.180 --> 3:51:38.780
 Is there some aspect of something you called

3:51:38.780 --> 3:51:43.780
 which I like super longevity that you find exciting,

3:51:44.220 --> 3:51:46.380
 that what, is there, research wise,

3:51:46.380 --> 3:51:48.380
 is there ideas in that space that?

3:51:48.380 --> 3:51:53.260
 I mean, I think, yeah, in terms of the meaning of life,

3:51:53.260 --> 3:51:58.060
 this really ties into that because for us as humans,

3:51:58.060 --> 3:52:02.300
 probably the way to get the most joy, growth and choice

3:52:02.300 --> 3:52:06.220
 is transhumanism and to go beyond the human form

3:52:06.220 --> 3:52:08.460
 that we have right now, right?

3:52:08.460 --> 3:52:11.020
 I mean, I think human body is great

3:52:11.020 --> 3:52:15.180
 and by no means to any of us maximize the potential

3:52:15.180 --> 3:52:18.580
 for joy, growth and choice imminent in our human bodies.

3:52:18.580 --> 3:52:21.820
 On the other hand, it's clear that other configurations

3:52:21.820 --> 3:52:25.300
 of matter could manifest even greater amounts

3:52:25.300 --> 3:52:29.660
 of joy, growth and choice than humans do,

3:52:29.660 --> 3:52:32.620
 maybe even finding ways to go beyond the realm of matter

3:52:32.620 --> 3:52:34.980
 that as we understand it right now.

3:52:34.980 --> 3:52:38.140
 So I think in a practical sense,

3:52:38.140 --> 3:52:40.780
 much of the meaning I see in human life

3:52:40.780 --> 3:52:42.940
 is to create something better than humans

3:52:42.940 --> 3:52:45.500
 and go beyond human life,

3:52:45.500 --> 3:52:48.020
 but certainly that's not all of it for me

3:52:48.020 --> 3:52:49.260
 in a practical sense, right?

3:52:49.260 --> 3:52:51.780
 Like I have four kids and a granddaughter

3:52:51.780 --> 3:52:55.100
 and many friends and parents and family

3:52:55.100 --> 3:52:59.780
 and just enjoying everyday human social existence.

3:52:59.780 --> 3:53:00.940
 But we can do even better.

3:53:00.940 --> 3:53:02.940
 Yeah, yeah, and I mean, I love,

3:53:02.940 --> 3:53:05.740
 I've always, when I could live near nature,

3:53:05.740 --> 3:53:08.780
 I spend a bunch of time out in nature in the forest

3:53:08.780 --> 3:53:10.980
 and on the water every day and so forth.

3:53:10.980 --> 3:53:15.060
 So I mean, enjoying the pleasant moment is part of it,

3:53:15.060 --> 3:53:19.100
 but the, you know, the growth and choice aspect

3:53:19.100 --> 3:53:22.460
 are severely limited by our human biology.

3:53:22.460 --> 3:53:26.020
 In particular, dying seems to inhibit your potential

3:53:26.020 --> 3:53:29.540
 for personal growth considerably as far as we know.

3:53:29.540 --> 3:53:33.020
 I mean, there's some element of life after death perhaps,

3:53:33.020 --> 3:53:35.020
 but even if there is,

3:53:35.020 --> 3:53:39.340
 why not also continue going in this biological realm, right?

3:53:39.340 --> 3:53:43.700
 In super longevity, I mean, you know,

3:53:43.700 --> 3:53:48.020
 we haven't yet cured aging, we haven't yet cured death.

3:53:48.020 --> 3:53:51.860
 Certainly there's very interesting progress all around.

3:53:51.860 --> 3:53:56.860
 I mean, CRISPR and gene editing can be an incredible tool.

3:53:57.220 --> 3:54:01.580
 And I mean, right now stem cells

3:54:01.580 --> 3:54:03.180
 could potentially prolong life a lot.

3:54:03.180 --> 3:54:05.980
 Like if you got stem cell injections

3:54:05.980 --> 3:54:08.660
 of just stem cells for every tissue

3:54:08.660 --> 3:54:11.380
 of your body injected into every tissue

3:54:11.380 --> 3:54:15.380
 and you can just have replacement of your old cells

3:54:15.380 --> 3:54:17.340
 with new cells produced by those stem cells.

3:54:17.340 --> 3:54:21.260
 I mean, that could be highly impactful at prolonging life.

3:54:21.260 --> 3:54:23.260
 Now we just need slightly better technology

3:54:23.260 --> 3:54:25.420
 for having them grow, right?

3:54:25.420 --> 3:54:28.860
 So using machine learning to guide procedures

3:54:28.860 --> 3:54:32.700
 for stem cell differentiation and trans differentiation,

3:54:32.700 --> 3:54:33.740
 it's kind of nitty gritty,

3:54:33.740 --> 3:54:36.700
 but I mean, that's quite interesting.

3:54:36.700 --> 3:54:41.100
 So I think there's a lot of different things being done

3:54:41.100 --> 3:54:44.780
 to help with prolongation of human life,

3:54:44.780 --> 3:54:47.580
 but we could do a lot better.

3:54:47.580 --> 3:54:51.500
 So for example, the extracellular matrix,

3:54:51.500 --> 3:54:52.660
 which is the bunch of proteins

3:54:52.660 --> 3:54:54.340
 in between the cells in your body,

3:54:54.340 --> 3:54:57.420
 they get stiffer and stiffer as you get older.

3:54:57.420 --> 3:55:01.340
 And the extracellular matrix transmits information

3:55:01.340 --> 3:55:03.580
 both electrically, mechanically,

3:55:03.580 --> 3:55:05.420
 and to some extent, biophotonically.

3:55:05.420 --> 3:55:07.300
 So there's all this transmission

3:55:07.300 --> 3:55:08.900
 through the parts of the body,

3:55:08.900 --> 3:55:11.900
 but the stiffer the extracellular matrix gets,

3:55:11.900 --> 3:55:13.540
 the less the transmission happens,

3:55:13.540 --> 3:55:15.660
 which makes your body get worse coordinated

3:55:15.660 --> 3:55:17.460
 between the different organs as you get older.

3:55:17.460 --> 3:55:19.420
 So my friend Christian Schaffmeister

3:55:19.420 --> 3:55:23.180
 at my alumnus organization, my alma mater,

3:55:23.180 --> 3:55:25.100
 the Great Temple University,

3:55:25.100 --> 3:55:28.620
 Christian Schaffmeister has a potential solution to this

3:55:28.620 --> 3:55:32.340
 where he has these novel molecules called spiral ligaments,

3:55:32.340 --> 3:55:34.420
 which are like polymers that are not organic.

3:55:34.420 --> 3:55:37.780
 They're specially designed polymers

3:55:37.780 --> 3:55:39.420
 so that you can algorithmically predict

3:55:39.420 --> 3:55:41.580
 exactly how they'll fold very simply.

3:55:41.580 --> 3:55:43.300
 So he designed the molecular scissors

3:55:43.300 --> 3:55:45.580
 that have spiral ligaments that you could eat

3:55:45.580 --> 3:55:49.220
 and would then cut through all the glucosapane

3:55:49.220 --> 3:55:50.620
 and other crosslink proteins

3:55:50.620 --> 3:55:52.780
 in your extracellular matrix, right?

3:55:52.780 --> 3:55:55.220
 But to make that technology really work

3:55:55.220 --> 3:55:56.860
 and be mature as several years of work,

3:55:56.860 --> 3:55:59.260
 as far as I know, no one's funding it at the moment.

3:55:59.260 --> 3:56:02.380
 But so there's so many different ways

3:56:02.380 --> 3:56:05.060
 that technology could be used to prolong longevity.

3:56:05.060 --> 3:56:08.060
 What we really need, we need an integrated database

3:56:08.060 --> 3:56:09.580
 of all biological knowledge

3:56:09.580 --> 3:56:11.980
 about human beings and model organisms,

3:56:11.980 --> 3:56:14.460
 like based, hopefully a massively distributed

3:56:14.460 --> 3:56:15.980
 open cog bioadm space,

3:56:15.980 --> 3:56:18.260
 but it can exist in other forms too.

3:56:18.260 --> 3:56:20.860
 We need that data to be opened up

3:56:20.860 --> 3:56:23.300
 in a suitably privacy protecting way.

3:56:23.300 --> 3:56:26.100
 We need massive funding into machine learning,

3:56:26.100 --> 3:56:29.260
 AGI, proto AGI statistical research,

3:56:29.260 --> 3:56:31.220
 aimed at solving biology,

3:56:31.220 --> 3:56:33.420
 both molecular biology and human biology,

3:56:33.420 --> 3:56:36.740
 based on this massive data set, right?

3:56:36.740 --> 3:56:39.620
 And then we need regulators

3:56:39.620 --> 3:56:42.660
 not to stop people from trying radical therapies

3:56:42.660 --> 3:56:46.740
 on themselves if they so wish to,

3:56:46.740 --> 3:56:49.420
 as well as better cloud based platforms

3:56:49.420 --> 3:56:51.220
 for like automated experimentation

3:56:51.220 --> 3:56:54.300
 on microorganisms, flies and mice and so forth.

3:56:54.300 --> 3:56:55.820
 And we could do all this.

3:56:55.820 --> 3:56:59.460
 You look after the last financial crisis, Obama,

3:56:59.460 --> 3:57:01.300
 who I generally like pretty well,

3:57:01.300 --> 3:57:03.740
 but he gave $4 trillion to large banks

3:57:03.740 --> 3:57:05.420
 and insurance companies.

3:57:05.420 --> 3:57:07.500
 You know, now in this COVID crisis,

3:57:08.420 --> 3:57:10.820
 trillions are being spent to help everyday people

3:57:10.820 --> 3:57:12.220
 and small businesses.

3:57:12.220 --> 3:57:14.580
 In the end, we probably will find many more trillions

3:57:14.580 --> 3:57:17.220
 are being given to large banks and insurance companies.

3:57:17.220 --> 3:57:21.020
 Anyway, like, could the world put $10 trillion

3:57:21.020 --> 3:57:24.220
 into making a massive holistic bio AI

3:57:24.220 --> 3:57:27.780
 and bio simulation and experimental biology infrastructure?

3:57:27.780 --> 3:57:30.580
 We could, we could put $10 trillion into that

3:57:30.580 --> 3:57:32.300
 without even screwing us up too badly,

3:57:32.300 --> 3:57:35.260
 just as in the end COVID and the last financial crisis

3:57:35.260 --> 3:57:37.900
 won't screw up the world economy so badly.

3:57:37.900 --> 3:57:39.900
 We're not putting $10 trillion into that.

3:57:39.900 --> 3:57:42.260
 Instead, all the speakers is siloed

3:57:42.260 --> 3:57:46.820
 inside a few big companies and government agencies.

3:57:46.820 --> 3:57:51.140
 And most of the data that comes from our individual bodies,

3:57:51.140 --> 3:57:55.140
 personally, that could feed this AI to solve aging and death,

3:57:55.140 --> 3:57:58.860
 most of that data is sitting in some hospitals database

3:57:58.860 --> 3:58:00.100
 doing nothing, right?

3:58:03.940 --> 3:58:07.140
 I got two more quick questions for you.

3:58:07.140 --> 3:58:09.780
 One, I know a lot of people are gonna ask me,

3:58:09.780 --> 3:58:11.700
 you are on the Joe Rogan podcast

3:58:11.700 --> 3:58:13.620
 wearing that same amazing hat.

3:58:14.860 --> 3:58:17.060
 Do you have a origin story for the hat?

3:58:17.060 --> 3:58:19.380
 Is there, does the hat have its own story

3:58:19.380 --> 3:58:21.340
 that you're able to share?

3:58:21.340 --> 3:58:23.140
 The hat story has not been told yet.

3:58:23.140 --> 3:58:24.180
 So we're gonna have to come back

3:58:24.180 --> 3:58:25.860
 and you can interview the hat.

3:58:27.860 --> 3:58:30.100
 We'll leave that for the hat's own interview.

3:58:30.100 --> 3:58:32.140
 It's too much to pack into a few seconds.

3:58:32.140 --> 3:58:32.980
 Is there a book?

3:58:32.980 --> 3:58:34.340
 Is the hat gonna write a book?

3:58:34.340 --> 3:58:35.500
 Okay, we'll.

3:58:36.940 --> 3:58:38.340
 It may transmit the information

3:58:38.340 --> 3:58:40.020
 through direct neural transmission.

3:58:40.020 --> 3:58:41.420
 Okay, so it actually,

3:58:41.420 --> 3:58:44.780
 there might be some neural link competition there.

3:58:44.780 --> 3:58:46.900
 Beautiful, we'll leave it as a mystery.

3:58:46.900 --> 3:58:49.060
 Maybe one last question.

3:58:49.060 --> 3:58:54.060
 If you build an AGI system, you're successful

3:58:56.420 --> 3:58:59.540
 at building the AGI system that could lead us

3:58:59.540 --> 3:59:02.500
 to the singularity and you get to talk to her

3:59:02.500 --> 3:59:05.780
 and ask her one question, what would that question be?

3:59:07.260 --> 3:59:08.180
 We're not allowed to ask,

3:59:08.180 --> 3:59:10.020
 what is the question I should be asking?

3:59:10.020 --> 3:59:12.260
 Yeah, that would be cheating,

3:59:12.260 --> 3:59:14.060
 but I guess that's a good question.

3:59:14.060 --> 3:59:18.620
 I'm thinking of a, I wrote a story with Stefan Bugawans

3:59:18.620 --> 3:59:23.420
 where these AI developers,

3:59:23.420 --> 3:59:25.940
 they created a super smart AI

3:59:25.940 --> 3:59:30.940
 aimed at answering all the philosophical questions

3:59:31.260 --> 3:59:32.100
 that have been worrying them,

3:59:32.100 --> 3:59:34.300
 like what is the meaning of life?

3:59:34.300 --> 3:59:35.700
 Is there free will?

3:59:35.700 --> 3:59:37.980
 What is consciousness and so forth?

3:59:37.980 --> 3:59:40.380
 So they got the super AGI built

3:59:40.380 --> 3:59:43.900
 and it turned a while, it said,

3:59:44.940 --> 3:59:46.580
 those are really stupid questions.

3:59:46.580 --> 3:59:48.300
 And then it puts off on a space shift

3:59:48.300 --> 3:59:51.420
 and left the earth, right?

3:59:51.420 --> 3:59:54.340
 So you'd be afraid of scaring it off?

3:59:54.340 --> 3:59:56.500
 That's it, yeah.

3:59:56.500 --> 4:00:01.500
 I mean, honestly, there is no one question

4:00:02.180 --> 4:00:07.180
 that rises among all the others really.

4:00:08.540 --> 4:00:10.060
 I mean, what interests me more

4:00:10.060 --> 4:00:13.540
 is upgrading my own intelligence

4:00:13.540 --> 4:00:17.420
 so that I can absorb the whole world view

4:00:17.420 --> 4:00:19.420
 of the super AGI.

4:00:19.420 --> 4:00:21.220
 But I mean, of course,

4:00:21.220 --> 4:00:23.140
 if the answer could be like,

4:00:23.140 --> 4:00:27.500
 what is the chemical formula for the immortality pill?

4:00:29.540 --> 4:00:33.380
 Then I would do that or emit a bit string

4:00:33.380 --> 4:00:38.380
 which will be the code for a super AGI

4:00:38.780 --> 4:00:41.260
 on the Intel i7 processor.

4:00:41.260 --> 4:00:42.900
 So those would be good questions.

4:00:42.900 --> 4:00:46.300
 So if your own mind was expanded

4:00:46.300 --> 4:00:49.380
 to become super intelligent like you're describing,

4:00:49.380 --> 4:00:53.540
 I mean, there's kind of a notion

4:00:53.540 --> 4:00:56.580
 that intelligence is a burden,

4:00:56.580 --> 4:01:00.060
 that it's possible that with greater and greater intelligence,

4:01:00.060 --> 4:01:03.060
 that other metric of joy that you mentioned

4:01:03.060 --> 4:01:04.820
 becomes more and more difficult.

4:01:04.820 --> 4:01:05.940
 What's your say?

4:01:05.940 --> 4:01:07.100
 Pretty stupid idea.

4:01:08.300 --> 4:01:09.900
 So you think if you're super intelligent,

4:01:09.900 --> 4:01:11.500
 you can also be super joyful?

4:01:11.500 --> 4:01:15.500
 I think getting root access to your own brain

4:01:15.500 --> 4:01:19.260
 will enable new forms of joy that we don't have now.

4:01:19.260 --> 4:01:22.780
 And I think as I've said before,

4:01:22.780 --> 4:01:27.780
 what I aim at is really make multiple versions of myself.

4:01:27.860 --> 4:01:30.220
 So I would like to keep one version

4:01:30.220 --> 4:01:33.620
 which is basically human like I am now,

4:01:33.620 --> 4:01:37.020
 but keep the dial to turn pain up and down

4:01:37.020 --> 4:01:42.020
 and get rid of death and make another version

4:01:42.020 --> 4:01:46.580
 which fuses its mind with super human AGI

4:01:46.580 --> 4:01:50.060
 and then will become massively transhuman.

4:01:50.060 --> 4:01:52.780
 And whether it will send some messages back

4:01:52.780 --> 4:01:55.580
 to the human me or not will be interesting to find out.

4:01:55.580 --> 4:01:58.500
 The thing is, once you're super AGI,

4:01:58.500 --> 4:02:01.540
 like one subjective second to a human

4:02:01.540 --> 4:02:03.620
 might be like a million subjective years

4:02:03.620 --> 4:02:04.980
 to that super AGI, right?

4:02:04.980 --> 4:02:06.980
 So it would be on a whole different basis.

4:02:06.980 --> 4:02:10.100
 And I mean, at very least those two copies

4:02:10.100 --> 4:02:13.220
 will be good to have, but it could be interesting

4:02:13.220 --> 4:02:16.820
 to put your mind into a dolphin or a space amoeba

4:02:16.820 --> 4:02:18.060
 or all sorts of other things.

4:02:18.060 --> 4:02:19.780
 Or you can imagine one version

4:02:19.780 --> 4:02:22.380
 that doubled its intelligence every year

4:02:22.380 --> 4:02:24.860
 and another version that just became a super AGI

4:02:24.860 --> 4:02:26.140
 as fast as possible, right?

4:02:26.140 --> 4:02:28.740
 So I mean, now we're sort of constrained

4:02:28.740 --> 4:02:32.980
 to think one mind, one self, one body, right?

4:02:32.980 --> 4:02:35.100
 But I think we actually,

4:02:35.100 --> 4:02:36.980
 we don't need to be that constrained

4:02:36.980 --> 4:02:40.820
 in thinking about future intelligence

4:02:40.820 --> 4:02:44.300
 after we've mastered AGI and nanotechnology

4:02:44.300 --> 4:02:47.820
 and longevity biology.

4:02:47.820 --> 4:02:49.540
 I mean, then each of our minds

4:02:49.540 --> 4:02:52.020
 is a certain pattern of organization, right?

4:02:52.020 --> 4:02:54.300
 And I know we haven't talked about consciousness,

4:02:54.300 --> 4:02:56.860
 but I sort of, I'm panpsychist.

4:02:56.860 --> 4:03:00.100
 I sort of view the universe as conscious.

4:03:00.100 --> 4:03:03.900
 And so, you know, a light bulb or a quark

4:03:03.900 --> 4:03:06.060
 or an ant or a worm or a monkey

4:03:06.060 --> 4:03:08.780
 have their own manifestations of consciousness

4:03:08.780 --> 4:03:11.940
 and the human manifestation of consciousness,

4:03:11.940 --> 4:03:15.300
 it's partly tied to the particular meat

4:03:15.300 --> 4:03:17.500
 that we're manifested by,

4:03:17.500 --> 4:03:19.980
 but it's largely tied to the pattern

4:03:19.980 --> 4:03:22.340
 of organization in the brain, right?

4:03:22.340 --> 4:03:25.020
 So if you upload yourself into a computer

4:03:25.020 --> 4:03:28.620
 or a robot or whatever else it is,

4:03:28.620 --> 4:03:31.740
 some element of your human consciousness may not be there

4:03:31.740 --> 4:03:34.220
 because it's just tied to the biological embodiment,

4:03:34.220 --> 4:03:36.260
 but I think most of it will be there

4:03:36.260 --> 4:03:39.980
 and these will be incarnations of your consciousness

4:03:39.980 --> 4:03:42.460
 in a slightly different flavor.

4:03:42.460 --> 4:03:45.580
 And, you know, creating these different versions

4:03:45.580 --> 4:03:46.540
 will be amazing.

4:03:46.540 --> 4:03:49.620
 And each of them will discover meanings of life

4:03:49.620 --> 4:03:51.980
 that have some overlap,

4:03:51.980 --> 4:03:54.260
 but probably not total overlap

4:03:54.260 --> 4:03:59.260
 with the human band's meaning of life.

4:03:59.260 --> 4:04:02.900
 The thing is to get to that future

4:04:02.900 --> 4:04:06.460
 where we can explore different varieties of joy,

4:04:06.460 --> 4:04:09.620
 different variations of human experience and values

4:04:09.620 --> 4:04:11.380
 and transhuman experiences and values.

4:04:11.380 --> 4:04:13.100
 To get to that future,

4:04:13.100 --> 4:04:16.740
 we need to navigate through a whole lot of human bullshit

4:04:16.740 --> 4:04:21.420
 of companies and governments and killer drones

4:04:21.420 --> 4:04:25.420
 and making and losing money and so forth, right?

4:04:25.420 --> 4:04:28.540
 And that's the challenge we're facing now

4:04:28.540 --> 4:04:30.700
 is if we do things right,

4:04:30.700 --> 4:04:33.540
 we can get to a benevolent singularity,

4:04:33.540 --> 4:04:36.300
 which is levels of joy, growth and choice

4:04:36.300 --> 4:04:39.900
 that are literally unimaginable to human beings.

4:04:39.900 --> 4:04:41.660
 If we do things wrong,

4:04:41.660 --> 4:04:44.100
 we could either annihilate all life on the planet

4:04:44.100 --> 4:04:47.020
 or we could lead to a scenario where, say,

4:04:47.020 --> 4:04:49.380
 all humans are annihilated

4:04:49.380 --> 4:04:52.700
 and there's some super AGI that goes on

4:04:52.700 --> 4:04:55.420
 and does its own thing unrelated to us

4:04:55.420 --> 4:04:58.340
 except via our role in originating it.

4:04:58.340 --> 4:05:02.100
 And we may well be at a bifurcation point now, right?

4:05:02.100 --> 4:05:05.780
 Where what we do now has significant causal impact

4:05:05.780 --> 4:05:06.700
 on what comes about.

4:05:06.700 --> 4:05:09.020
 And yet most people on the planet

4:05:09.020 --> 4:05:11.500
 aren't thinking that way whatsoever.

4:05:11.500 --> 4:05:16.220
 They're thinking only about their own narrow aims

4:05:16.220 --> 4:05:17.780
 and aims and goals, right?

4:05:17.780 --> 4:05:20.860
 No, of course, I'm thinking about my own narrow aims

4:05:20.860 --> 4:05:24.220
 and goals to some extent also,

4:05:24.220 --> 4:05:29.220
 but I'm trying to use as much of my energy and mind as I can

4:05:29.460 --> 4:05:33.180
 to push toward this more benevolent alternative,

4:05:33.180 --> 4:05:34.660
 which will be better for me,

4:05:34.660 --> 4:05:37.940
 but also for everybody else.

4:05:37.940 --> 4:05:39.340
 And that's it.

4:05:39.340 --> 4:05:43.220
 It's weird that so few people understand what's going on.

4:05:43.220 --> 4:05:44.780
 I know you interviewed Elon Musk

4:05:44.780 --> 4:05:47.340
 and he understands a lot of what's going on,

4:05:47.340 --> 4:05:49.100
 but he's much more paranoid than I am, right?

4:05:49.100 --> 4:05:52.420
 Because Elon gets that AGI is gonna be

4:05:52.420 --> 4:05:54.260
 way, way smarter than people.

4:05:54.260 --> 4:05:57.100
 And he gets that an AGI does not necessarily

4:05:57.100 --> 4:05:58.740
 have to give a shit about people

4:05:58.740 --> 4:06:01.660
 because we're a very elementary mode of organization

4:06:01.660 --> 4:06:04.700
 of matter compared to many AGI's.

4:06:04.700 --> 4:06:06.340
 But I don't think he has a clear vision

4:06:06.340 --> 4:06:11.340
 of how infusing early stage AGI's with compassion

4:06:12.100 --> 4:06:15.420
 and human warmth can lead to an AGI

4:06:15.420 --> 4:06:18.020
 that loves and helps people

4:06:18.020 --> 4:06:22.860
 rather than viewing us as a historical artifact

4:06:22.860 --> 4:06:26.820
 and a waste of mass energy.

4:06:26.820 --> 4:06:28.060
 But on the other hand,

4:06:28.060 --> 4:06:29.620
 while I have some disagreements with him,

4:06:29.620 --> 4:06:33.140
 like he understands way, way more of the story

4:06:33.140 --> 4:06:34.820
 than almost anyone else

4:06:34.820 --> 4:06:37.700
 in such a large scale corporate leadership position, right?

4:06:37.700 --> 4:06:40.740
 It's terrible how little understanding

4:06:40.740 --> 4:06:45.060
 of these fundamental issues exists out there now

4:06:45.060 --> 4:06:47.220
 that may be different five or 10 years from now though,

4:06:47.220 --> 4:06:51.180
 because I can see understanding of AGI and longevity

4:06:51.180 --> 4:06:54.660
 and other such issues is certainly much stronger

4:06:54.660 --> 4:06:57.620
 and more prevalent now than 10 or 15 years ago, right?

4:06:57.620 --> 4:07:00.580
 So I mean, humanity is, as a whole,

4:07:00.580 --> 4:07:05.460
 can be slow learners relative to what I would like,

4:07:05.460 --> 4:07:08.420
 but on a historical sense, on the other hand,

4:07:08.420 --> 4:07:11.260
 you could say the progress is astoundingly fast.

4:07:11.260 --> 4:07:15.660
 But Elon also said, I think on the Joe Rogan podcast

4:07:15.660 --> 4:07:17.380
 that love is the answer.

4:07:17.380 --> 4:07:21.860
 So maybe in that way, you and him are both on the same page

4:07:21.860 --> 4:07:24.420
 of how we should proceed with AGI.

4:07:24.420 --> 4:07:27.300
 I think there's no better place to end it.

4:07:27.300 --> 4:07:30.860
 I hope we get to talk again about the hat

4:07:30.860 --> 4:07:32.020
 and about consciousness

4:07:32.020 --> 4:07:34.460
 and about a million topics we didn't cover.

4:07:34.460 --> 4:07:36.300
 Ben, it's a huge honor to talk to you.

4:07:36.300 --> 4:07:37.540
 Thank you for making it out.

4:07:37.540 --> 4:07:38.580
 Thank you for talking today.

4:07:38.580 --> 4:07:39.580
 I really love it.

4:07:39.580 --> 4:07:40.420
 Thanks for having me.

4:07:40.420 --> 4:07:44.380
 This was really good fun

4:07:44.380 --> 4:07:47.420
 and we dug deep into some very important things.

4:07:47.420 --> 4:07:48.700
 So thanks for doing this.

4:07:48.700 --> 4:07:49.780
 Thanks very much.

4:07:49.780 --> 4:07:51.180
 Awesome.

4:07:51.180 --> 4:07:53.820
 Thanks for listening to this conversation with Ben Gertzel

4:07:53.820 --> 4:07:55.820
 and thank you to our sponsors,

4:07:55.820 --> 4:07:59.380
 The Jordan Harbinger Show and Masterclass.

4:07:59.380 --> 4:08:01.060
 Please consider supporting the podcast

4:08:01.060 --> 4:08:04.580
 by going to jordanharbinger.com slash lex

4:08:04.580 --> 4:08:09.580
 and signing up to masterclass and masterclass.com slash lex.

4:08:09.780 --> 4:08:12.260
 Click the links, buy the stuff.

4:08:12.260 --> 4:08:14.180
 It's the best way to support this podcast

4:08:14.180 --> 4:08:18.820
 and the journey I'm on in my research and startup.

4:08:18.820 --> 4:08:21.340
 If you enjoy this thing, subscribe on YouTube,

4:08:21.340 --> 4:08:23.700
 review it with five stars on a podcast,

4:08:23.700 --> 4:08:26.820
 support it on Patreon or connect with me on Twitter.

4:08:26.820 --> 4:08:31.820
 Alex Friedman spelled without the E, just F R I D M A N.

4:08:32.380 --> 4:08:35.260
 I'm sure eventually you will figure it out.

4:08:35.260 --> 4:08:38.180
 And now let me leave you with some words from Ben Gertzel.

4:08:39.100 --> 4:08:42.500
 Our language for describing emotions is very crude.

4:08:42.500 --> 4:08:45.080
 That's what music is for.

4:08:45.080 --> 4:09:13.700
 Thank you for listening and hope to see you next time.

