WEBVTT

00:00.000 --> 00:06.800
 The following is a conversation with Matt Potmanick, Director of Neuroscience Research at DeepMind.

00:06.800 --> 00:12.400
 He's a brilliant cross disciplinary mind navigating effortlessly between cognitive psychology,

00:12.400 --> 00:15.760
 computational neuroscience, and artificial intelligence.

00:16.640 --> 00:23.760
 Quick summary of the ads. Two sponsors. The Jordan Harbinger Show and Magic Spoon Serial.

00:23.760 --> 00:30.000
 Please consider supporting the podcast by going to jordanharbinger.com slash lex and

00:30.000 --> 00:37.680
 also going to magicspoon.com slash lex and using code lex at checkout after you buy all of their

00:37.680 --> 00:43.760
 serial. Click the links by the stuff. It's the best way to support this podcast and journey I'm on.

00:44.480 --> 00:49.760
 If you enjoy this podcast, subscribe on YouTube, review it with 5,000 apple podcast,

00:49.760 --> 00:56.000
 follow on Spotify, support on Patreon, or connect with me on Twitter at Lex Freedman, spelled

00:56.640 --> 01:03.840
 surprisingly without the E just F R I D M A N. As usual, I'll do a few minutes of ads now and

01:03.840 --> 01:09.120
 never any ads in the middle that can break the flow of the conversation. This episode is supported

01:09.120 --> 01:16.720
 by the Jordan Harbinger Show. Go to jordanharbinger.com slash lex. It's how he knows I sent you.

01:16.720 --> 01:23.280
 On that page, subscribe to his podcast, an apple podcast, Spotify, and you know where to look.

01:24.160 --> 01:30.080
 I've been binging on his podcast. Jordan is a great interviewer and even a better human being.

01:30.080 --> 01:35.120
 I recently listened to his conversation with Jack Barsky, former sleeper agent for the KGB in the

01:35.120 --> 01:41.040
 80s and author of deep undercover, which is a memoir that paints yet another interesting

01:41.040 --> 01:47.600
 perspective on the Cold War era. I've been reading a lot about the Stalin and then Gorbachev and

01:47.600 --> 01:52.320
 Putin eras of Russia, but this conversation made me realize that I need to do a deep dive into the

01:52.320 --> 01:59.840
 Cold War era to get a complete picture of Russia's recent history. Again, go to jordanharbinger.com

01:59.840 --> 02:05.680
 slash lex. Subscribe to his podcast. It's how he knows I sent you. It's awesome. You won't regret it.

02:05.680 --> 02:14.240
 This episode is also supported by Magic Spoon, low carb, keto friendly, super amazingly delicious

02:14.240 --> 02:20.000
 cereal. I've been on a keto or very low carb diet for a long time now. It helps with my

02:20.000 --> 02:25.200
 mental performance. It helps with my physical performance, even doing this crazy push up

02:25.200 --> 02:31.280
 pull up challenge I'm doing, including the running. It just feels great. I used to love cereal.

02:31.280 --> 02:36.640
 Obviously I can't have it now because most cereals have a crazy amount of sugar,

02:36.640 --> 02:44.720
 which is terrible for you. So I quit it years ago, but Magic Spoon amazingly somehow is a

02:44.720 --> 02:50.240
 totally different thing. Zero sugar, 11 grams of protein and only three net grams of carbs.

02:50.800 --> 02:56.880
 It tastes delicious. It has a lot of flavors, two new ones, including peanut butter. But if you

02:56.880 --> 03:03.040
 know what's good for you, you'll go with cocoa, my favorite flavor and the flavor of champions.

03:04.080 --> 03:09.760
 Click the magicspoon.com slash lex link in the description and use code lex at checkout for

03:09.760 --> 03:15.200
 free shipping and to let them know I sent you. They have agreed to sponsor this podcast for a long

03:15.200 --> 03:22.320
 time. They're an amazing sponsor and an even better cereal. I highly recommend it. It's delicious.

03:22.320 --> 03:27.520
 It's good for you. You won't regret it. And now here's my conversation with Matt

03:27.520 --> 03:31.920
 Botkinick. How much of the human brain do you think we understand?

03:33.280 --> 03:39.600
 I think we're at a weird moment in the history of neuroscience in the sense that

03:45.040 --> 03:48.160
 I feel like we understand a lot about the brain at a very high level,

03:48.160 --> 03:51.840
 but a very coarse level.

03:51.840 --> 03:55.920
 When you say high level, what are you thinking? Are you thinking functional? Are you thinking

03:55.920 --> 04:03.360
 structurally? In other words, what is the brain for? What kinds of computation does the brain do?

04:04.960 --> 04:13.680
 What kinds of behaviors would we have to explain if we were going to look down at the

04:13.680 --> 04:19.520
 mechanistic level? At that level, I feel like we understand much, much more about the brain

04:19.520 --> 04:25.280
 than we did when I was in high school. But it's almost like we're seeing it through a fog. It's

04:25.280 --> 04:30.080
 only at a very coarse level. We don't really understand what the neuronal mechanisms are

04:30.080 --> 04:35.680
 that underlie these computations. We've gotten better at saying, what are the functions that

04:35.680 --> 04:39.360
 the brain is computing that we would have to understand if we were going to get down to the

04:39.360 --> 04:49.120
 neuronal level? At the other end of the spectrum, in the last few years, incredible progress has been

04:49.120 --> 04:57.120
 made in terms of technologies that allow us to see, actually literally see in some cases,

04:57.120 --> 05:03.680
 what's going on at the single unit level, even the dendritic level, and then there's this

05:03.680 --> 05:08.240
 yawning gap in between. That's interesting. At the high level, so that's almost a cognitive

05:08.240 --> 05:15.120
 science level. And then at the neuronal level, that's neurobiology and neuroscience just studying

05:15.120 --> 05:21.360
 single neurons, the synaptic connections and all the dopamine, all the kind of neurotransmitters.

05:21.360 --> 05:29.200
 One blanket statement I should probably make is that as I've gotten older, I have become more and

05:29.200 --> 05:35.440
 more reluctant to make a distinction between psychology and neuroscience. To me, the point

05:35.440 --> 05:45.120
 of neuroscience is to study what the brain is for. If you're a nephrologist and you want to

05:45.120 --> 05:50.080
 learn about the kidney, you start by saying, what is this thing for? Well,

05:50.960 --> 05:59.760
 it seems to be for taking blood on one side that has metabolites in it that shouldn't be there,

05:59.760 --> 06:06.240
 sucking them out of the blood while leaving the good stuff behind, and then excreting that in

06:06.240 --> 06:11.280
 the form of urine. That's what the kidney is for. It's like obvious. So the rest of the work is

06:11.280 --> 06:16.960
 deciding how it does that. And this, it seems to me, is the right approach to take to the brain.

06:16.960 --> 06:22.640
 You say, well, what is the brain for? The brain, as far as I can tell, is for producing behavior.

06:22.640 --> 06:28.800
 It's for going from perceptual inputs to behavioral outputs. And the behavioral outputs

06:28.800 --> 06:34.720
 should be adaptive. So that's what psychology is about. It's about understanding the structure

06:34.720 --> 06:40.720
 of that function. And then the rest of neuroscience is about figuring out how those operations are

06:40.720 --> 06:47.920
 actually carried out at a mechanistic level. That's really interesting. But so unlike the kidney,

06:47.920 --> 06:57.520
 the brain, the gap between the electrical signal and behavior, you truly see neuroscience as the

06:57.520 --> 07:05.680
 science that touches behavior, how the brain generates behavior, or how the brain converts

07:05.680 --> 07:13.360
 raw visual information into understanding. You basically see cognitive science, psychology,

07:13.360 --> 07:21.680
 and neuroscience as all one science. It's a personal statement. Is that a hopeful or

07:21.680 --> 07:28.160
 realistic statement? So certainly you will be correct in your feeling in some number of years,

07:28.160 --> 07:32.480
 but that number of years could be 200, 300 years from now. Oh, well, well, there's a,

07:33.280 --> 07:38.320
 is that aspirational or is that pragmatic engineering feeling that you have?

07:38.320 --> 07:52.560
 It's both in the sense that this is what I hope and expect will bear fruit over the coming decades.

07:53.200 --> 08:01.440
 But it's also pragmatic in the sense that I'm not sure what we're doing in either psychology

08:01.440 --> 08:08.880
 or neuroscience, if that's not the framing. I don't know what it means to understand the brain

08:09.680 --> 08:19.920
 if part of the enterprise is not about understanding the behavior that's being produced.

08:19.920 --> 08:26.640
 I mean, yeah, but I would compare it to maybe astronomers looking at the movement of the

08:26.640 --> 08:33.680
 planets and the stars and without any interest of the underlying physics, right? And I would argue

08:33.680 --> 08:39.120
 that at least in the early days, there's some value to just tracing the movement of the planets

08:39.120 --> 08:44.320
 and the stars without thinking about the physics too much, because it's such a big leap to start

08:44.320 --> 08:49.360
 thinking about the physics before you even understand even the basic structural elements of...

08:49.360 --> 08:54.720
 Oh, I agree with that. I agree. But you're saying in the end, the goal should be to deeply understand.

08:54.720 --> 08:59.440
 Well, right. And I think... So I thought about this a lot when I was in grad school, because a lot

08:59.440 --> 09:06.880
 of what I studied in grad school was psychology. And I found myself a little bit confused about

09:06.880 --> 09:11.680
 what it meant to... It seems like what we were talking about a lot of the time were

09:12.480 --> 09:20.080
 virtual causal mechanisms. Like, oh, well, you know, attentional selection then selects some

09:20.080 --> 09:26.080
 object in the environment, and that is then passed on to the motor... Information about that is passed

09:26.080 --> 09:34.080
 on to the motor system. But these are virtual mechanisms. They're metaphors. There's no reduction

09:34.080 --> 09:39.360
 to... There's no reduction going on in that conversation to some physical mechanism that...

09:41.280 --> 09:47.040
 Which is really what it would take to fully understand how behavior is rising. But the

09:47.040 --> 09:51.360
 causal mechanisms are definitely neurons interacting. I'm willing to say that at this

09:51.360 --> 09:58.240
 point in history. So in psychology, at least for me personally, there was this strange insecurity

09:58.240 --> 10:04.800
 about trafficking in these metaphors, which we're supposed to explain the function of the mind.

10:06.640 --> 10:14.640
 If you can't ground them in physical mechanisms, then what is the explanatory validity of these

10:14.640 --> 10:25.760
 explanations? And I managed to soothe my own nerves by thinking about the history of

10:27.360 --> 10:34.560
 genetics research. So I'm very far from being an expert on the history of this field. But I know

10:34.560 --> 10:43.840
 enough to say that Mendelian genetics preceded Watson and Crick. And so there was a significant

10:43.840 --> 10:54.080
 period of time during which people were productively investigating the structure of inheritance

10:54.080 --> 11:00.000
 using what was essentially a metaphor, the notion of a gene. And genes do this and genes do that.

11:00.000 --> 11:07.040
 But we're the genes. They're sort of an explanatory thing that we made up. And we ascribed to them

11:07.040 --> 11:11.520
 these causal properties. So there's a dominant, there's a recessive, and then they recombine it.

11:11.520 --> 11:20.480
 And then later, there was a kind of blank there that was filled in with a physical mechanism.

11:21.440 --> 11:28.000
 That connection was made. But it was worth having that metaphor, because that gave us

11:28.640 --> 11:33.520
 a good sense of what kind of causal mechanism we were looking for.

11:33.520 --> 11:40.080
 Right. And the fundamental metaphor of cognition, you said, is the interaction of neurons.

11:40.080 --> 11:47.840
 Is that what is the metaphor? No, no, the metaphor, the metaphors we use in cognitive psychology are

11:50.240 --> 11:58.240
 things like attention, the way that memory works. I retrieve something from memory.

11:59.280 --> 12:08.000
 A memory retrieval occurs. What is that? That's not a physical mechanism that I can examine in

12:08.000 --> 12:13.680
 its own right. But it's still worth having that metaphorical level.

12:13.680 --> 12:18.800
 Yeah, I misunderstood, actually. So the higher level abstractions is the metaphor that's most

12:18.800 --> 12:31.760
 useful. But what about, how does that connect to the idea that that arises from interaction of

12:31.760 --> 12:39.040
 neurons? Is the interaction of neurons also not a metaphor to you? Or is it literally,

12:40.880 --> 12:46.800
 that's no longer a metaphor. That's already the lowest level of abstractions that could actually

12:46.800 --> 12:55.840
 be directly studied. Well, I'm hesitating because I think what I want to say could end up being

12:55.840 --> 13:02.240
 controversial. So what I want to say is, yes, the interactions of neurons, that's not

13:02.240 --> 13:08.320
 metaphorical. That's a physical fact. That's where the causal interactions actually occur.

13:08.320 --> 13:13.440
 Now, I suppose you couldn't say, well, even that is metaphorical relative to the quantum

13:13.440 --> 13:17.040
 events that underline. I don't want to go down that rabbit hole.

13:17.040 --> 13:20.080
 It's always turtles on top of turtles. It's all the way down.

13:20.080 --> 13:24.320
 There is a reduction that you can do. You can say these psychological phenomena

13:24.320 --> 13:29.600
 are can be explained through a very different kind of causal mechanism, which has to do with

13:29.600 --> 13:35.440
 neurotransmitter release. And so what we're really trying to do in neuroscience, large,

13:36.400 --> 13:43.520
 as I say, which for me includes psychology, is to take these psychological phenomena

13:43.520 --> 13:53.760
 and map them onto neural events. I think remaining forever at the level of

13:56.160 --> 14:02.160
 description that is natural for psychology, for me personally, would be disappointing.

14:02.160 --> 14:11.920
 I want to understand how mental activity arises from neural activity. But the converse is also

14:11.920 --> 14:18.000
 true. Studying neural activity without any sense of what you're trying to explain,

14:19.680 --> 14:26.320
 to me, feels like at best roping around at random.

14:27.200 --> 14:32.640
 Now, you've talked about this bridging of the gap between psychology and neuroscience,

14:32.640 --> 14:38.960
 but do you think it's possible? My love is, I fell in love with psychology and psychiatry in

14:38.960 --> 14:44.160
 general with Freud when I was really young and I hoped to understand the mind. And for me,

14:44.160 --> 14:49.600
 understanding the mind at least at a young age before I discovered AI and even neuroscience was

14:51.600 --> 14:56.800
 is psychology. And do you think it's possible to understand the mind without getting into all

14:56.800 --> 15:03.600
 the messy details of neuroscience? Like you kind of mentioned, to you, it's appealing to try to

15:03.600 --> 15:08.320
 understand the mechanisms at the lowest level. But do you think that's needed? That's required

15:08.320 --> 15:16.560
 to understand how the mind works? That's an important part of the whole picture. But I would

15:16.560 --> 15:25.920
 be the last person on earth to suggest that that reality renders psychology in its own right

15:28.480 --> 15:34.880
 unproductive. I trained as a psychologist. I am fond of saying that I have learned much more

15:34.880 --> 15:42.800
 from psychology than I have from neuroscience. To me, psychology is a hugely important discipline.

15:43.680 --> 15:47.040
 And one thing that warms in my heart is that

15:50.400 --> 15:58.400
 ways of investigating behavior that have been native to cognitive psychology since it's

15:58.400 --> 16:08.560
 dawn in the 60s are starting to become interesting to AI researchers for a variety of reasons.

16:09.280 --> 16:10.800
 And that's been exciting for me to see.

16:11.440 --> 16:19.120
 Can you maybe talk a little bit about what you see as beautiful aspects of psychology,

16:19.120 --> 16:24.960
 maybe limiting aspects of psychology? I mean, maybe just started off as a science, as a field.

16:24.960 --> 16:31.440
 To me, when I understood what psychology is, analytical psychology, like the way it's

16:31.440 --> 16:39.040
 actually carried out, it's really disappointing to see two aspects. One is how small the N is,

16:39.040 --> 16:45.200
 how small the number of subjects is in the studies. And two, it was disappointing to see

16:45.200 --> 16:52.480
 how controlled the entire, how much it was in the lab. It wasn't studying humans in the wild.

16:52.480 --> 16:56.320
 There was no mechanism for studying humans in the wild. So that's where I became a little bit

16:56.320 --> 17:02.880
 disillusioned to psychology. And then the modern world of the internet is so exciting to me,

17:02.880 --> 17:08.240
 the Twitter data or YouTube data, like data of human behavior on the internet becomes exciting

17:08.240 --> 17:14.320
 because the N grows and then in the wild grows. But that's just my narrow sense. Do you have

17:14.320 --> 17:19.600
 a optimistic or pessimistic cynical view of psychology? How do you see the field broadly?

17:19.600 --> 17:28.480
 When I was in graduate school, it was early enough that there was still a thrill in seeing

17:29.200 --> 17:39.040
 that there were ways of doing experimental science that provided insight to the structure of the

17:39.040 --> 17:45.040
 mind. One thing that impressed me most when I was at that stage in my education was

17:45.040 --> 17:54.560
 neuropsychology, analyzing the behavior of populations who had brain damage of different

17:54.560 --> 18:04.480
 kinds and trying to understand what the specific deficits were that arose from

18:04.480 --> 18:08.880
 a lesion in a particular part of the brain. And the kind of experimentation that was done and

18:08.880 --> 18:17.840
 that's still being done to get answers in that context was so creative and it was so deliberate.

18:19.280 --> 18:24.640
 It was good science. An experiment answered one question but raised another and somebody

18:24.640 --> 18:28.400
 would do an experiment that answered that question and you really felt like you were narrowing in on

18:29.360 --> 18:34.000
 some kind of approximate understanding of what this part of the brain was for.

18:34.000 --> 18:40.400
 Do you have an example from memory of what kind of aspects of the mind could be studied in this

18:40.400 --> 18:46.320
 kind of way? Oh, sure. I mean, the very detailed neuropsychological studies of language

18:48.480 --> 18:56.160
 function, looking at production and reception and the relationship between visual function,

18:56.160 --> 19:04.080
 you know, reading and auditory and semantic and still are these beautiful models that came

19:04.080 --> 19:08.560
 out of that kind of research that really made you feel like you understood something that you

19:08.560 --> 19:16.160
 hadn't understood before about how language processing is organized in the brain. But having

19:16.160 --> 19:27.520
 said all that, you know, I think you are, I mean, I agree with you that the cost of doing

19:28.560 --> 19:37.440
 highly controlled experiments is that you, by construction, miss out on the richness and complexity

19:37.440 --> 19:43.920
 of the real world. One thing that, so I was drawn into science by what in those days was

19:43.920 --> 19:49.680
 called connectionism, which is, of course, what we now call deep learning. And at that point in

19:49.680 --> 19:56.400
 history, neural networks were primarily being used in order to model human cognition. They

19:56.400 --> 20:01.280
 weren't yet really useful for industrial applications. So you always found neural

20:01.280 --> 20:07.680
 networks in biological form beautiful? Oh, neural networks were very concretely the thing that drew

20:07.680 --> 20:15.920
 me into science. I was handed, are you familiar with the PDP books from the 80s? I went to

20:15.920 --> 20:22.880
 medical school before I went into science. Really? Interesting. Wow. I also did a graduate

20:22.880 --> 20:28.960
 degree in art history, so I'm kind of exploring. Well, art history, I understand. That's just a

20:28.960 --> 20:34.720
 curious, creative mind. But medical school, with the dream of what, if we could take that

20:34.720 --> 20:41.680
 slight tangent, what did you want to be a surgeon? I actually was quite interested in surgery. I

20:41.680 --> 20:48.960
 was interested in surgery and psychiatry, and I thought that I must be the only person on the

20:48.960 --> 20:56.160
 planet who was torn between those two fields. And I said exactly that to my advisor in medical

20:56.160 --> 21:03.120
 school who turned out, I found out later to be a famous psychoanalyst. And he said to me,

21:03.120 --> 21:07.600
 no, no, it's actually not so uncommon to be interested in surgery and psychiatry.

21:07.600 --> 21:13.120
 And he conjectured that the reason that people develop these two interests is that

21:13.120 --> 21:18.000
 both fields are about going beneath the surface and kind of getting into the kind of secret.

21:18.960 --> 21:22.400
 I mean, maybe you understand this as someone who was interested in psychoanalysis.

21:24.960 --> 21:30.480
 There's a cliche phrase that people use now on like an NPR, the secret life of blankity blank.

21:30.480 --> 21:38.240
 Right? And that was part of the thrill of surgery was seeing the secret activity that's

21:38.240 --> 21:42.240
 inside everybody's abdomen and thorax. That's a very poetic way to connect it to

21:43.200 --> 21:46.400
 disciplines that are very practically speaking different from each other.

21:46.400 --> 21:52.240
 That's for sure. That's for sure. Yes. So how do we get on to medical school?

21:52.240 --> 21:58.720
 So I was in medical school and I was doing a psychiatry rotation and my kind of

21:58.720 --> 22:07.680
 advisor in that rotation asked me what I was interested in. And I said, well, maybe psychiatry.

22:07.680 --> 22:12.080
 He said, why? And I said, well, I've always been interested in how the brain works.

22:12.880 --> 22:19.440
 I'm pretty sure that nobody's doing scientific research that addresses my interests, which are,

22:20.400 --> 22:24.080
 I didn't have a word for it then, but I would have said cognition.

22:24.080 --> 22:29.440
 And he said, well, you know, I'm not sure that's true. You might be interested in these books.

22:29.440 --> 22:33.840
 And he pulled down the PDB books from his shelf and they were still shrink wrapped.

22:33.840 --> 22:38.720
 He hadn't read them, but he handed them to me. He said, you feel free to borrow these.

22:38.720 --> 22:43.280
 And that was, you know, I went back to my dorm room and I just, you know, read them cover to cover.

22:43.280 --> 22:49.520
 And what's PDP? Parallel distributed processing, which was one of the original names for deep

22:49.520 --> 22:57.520
 learning. And so I apologize for the romanticized question, but what, what idea in the space of

22:57.520 --> 23:02.640
 neural science in the space of the human brain is to you the most beautiful and mysterious,

23:02.640 --> 23:11.040
 surprising? What, what had always fascinated me, even when I was a pretty young kid, I think,

23:11.040 --> 23:27.840
 was the paradox that lies in the fact that the brain is so mysterious. And so it seems so distant.

23:30.640 --> 23:38.400
 But at the same time, it's responsible for the, the, the, the full transparency of everyday life.

23:38.400 --> 23:43.680
 It's, the brain is literally what makes everything obvious and familiar. And, and,

23:45.040 --> 23:50.400
 and, and there's always one in the room with you. I used to teach, when I taught at Princeton,

23:50.400 --> 23:55.520
 I used to teach a cognitive neuroscience course. And the very last thing I would say to the students

23:55.520 --> 24:03.440
 was, you know, people often, when people think of scientific inspiration,

24:03.440 --> 24:10.160
 the, the metaphor is often, well, look to the stars, you know, the stars will inspire you to

24:10.160 --> 24:15.840
 wonder at the universe and, and, you know, think about your place in it and how things work. And,

24:16.640 --> 24:22.320
 and I'm all for looking at the stars, but I've always been much more inspired and kind of my

24:22.320 --> 24:30.240
 sense of wonder comes from the, not from the distant mysterious stars, but from the extremely

24:30.240 --> 24:39.280
 intimately close brain. Yeah. There's something just endlessly fascinating to me about that.

24:39.920 --> 24:46.800
 The, like Jessica said, the, the, the one is close and yet distant in, in terms of our

24:46.800 --> 24:54.720
 understanding of it. Do you, are you also captivated by the, the fact that this very

24:54.720 --> 25:01.760
 conversation is happening because two brains are communicating? Yes. Exactly. The, I guess what I

25:01.760 --> 25:07.360
 mean is the subjective nature of the experience. If we can take a small tangent into the, the

25:07.360 --> 25:13.760
 mystical of it, the consciousness, or, or when you're saying you're captivated by the idea

25:13.760 --> 25:18.400
 of the brain, you're, are you talking about specifically the mechanism of cognition? Or,

25:18.400 --> 25:26.480
 are you also just like, at least for me, it's almost like paralyzing the beauty and the mystery

25:26.480 --> 25:31.600
 of the fact that it creates the entirety of the experience, not just the reasoning capability,

25:31.600 --> 25:41.840
 but the experience. Well, I definitely resonate with that, that latter thought. And I, I often find

25:41.840 --> 25:51.040
 discussions of artificial intelligence to be disappointingly narrow. You know, speaking

25:51.040 --> 25:57.200
 as someone who has always had an interest in, in, in art. Right. I was just going to go there

25:57.200 --> 26:02.160
 because it sounds like somebody who has an interest in art. Yeah. I mean, I, there, there,

26:02.160 --> 26:10.960
 there, there are many layers to, you know, to full bore human experience. And, and in some ways,

26:10.960 --> 26:14.800
 it's not enough to say, oh, well, don't worry, you know, we, we're talking about cognition,

26:14.800 --> 26:21.280
 but we'll add emotion, you know, there's, there's, there's an incredible scope to

26:22.480 --> 26:32.560
 what humans go through in, in every moment. And, and yes, so it's, that's part of what

26:32.560 --> 26:41.280
 fascinates me is that, is that our brains are producing that. But at the same time,

26:41.280 --> 26:49.680
 it's so mysterious to us how we literally, our brains are literally in our heads producing

26:49.680 --> 26:56.480
 this experience. And yet there, and yet there's, it's so mysterious to us. And so, and the scientific

26:56.480 --> 27:04.000
 challenge of getting at the, the actual explanation for that is so overwhelming. That's just, I don't

27:04.000 --> 27:10.080
 know that certain people have fixations on particular questions. And that's always, that's

27:10.080 --> 27:15.040
 just always been mine. Yeah, I would say the poetry that is fascinating. And I'm, I'm really

27:15.040 --> 27:19.360
 interested in natural language as well. And when you look at artificial intelligence community,

27:19.360 --> 27:27.200
 it always saddens me how much when you try to create a benchmark for the community together

27:27.200 --> 27:33.520
 around how much of the magic of language is lost when you create that benchmark, that there's

27:33.520 --> 27:39.360
 something we talk about experience, the, the music of the language, the wit, the something

27:39.360 --> 27:44.800
 that makes a rich experience, something that would be required to pass the spirit of the

27:44.800 --> 27:50.800
 touring test is lost in these benchmarks. And I wonder how to get it back in because it's very

27:50.800 --> 27:56.800
 difficult. The moment you try to do like real good rigorous science, you lose some of that magic.

27:56.800 --> 28:02.560
 When you try to study cognition in a rigorous scientific way, it feels like you're losing

28:02.560 --> 28:08.880
 some of the magic, the, the seeing cognition in a mechanistic way that AI folk at this stage in

28:08.880 --> 28:15.600
 our history. Okay, I agree with you. But at the same time, one, one thing that I found

28:17.040 --> 28:23.360
 really exciting about that first wave of deep learning models in cognition was

28:25.840 --> 28:31.600
 there was the fact that the people who were building these models were focused on the

28:31.600 --> 28:39.280
 richness and complexity of human cognition. So an early debate in cognitive science,

28:40.000 --> 28:44.160
 which I sort of witnessed as a grad student was about something that sounds very dry,

28:44.160 --> 28:50.640
 which is the formation of the past tense. But there were these two camps. One said, well,

28:50.640 --> 28:58.560
 the mind encodes certain rules. And it also has a list of exceptions. Because of course,

28:58.560 --> 29:02.720
 you know, the rule is add ED, but that's not always what you do. So you have to have a list of

29:02.720 --> 29:09.200
 exceptions. And, and then there were the connectionists who, you know, evolved into the deep learning

29:09.200 --> 29:14.880
 people who said, well, you know, if you look carefully at the data, if you look at actually

29:14.880 --> 29:21.680
 look at corpora, like language corpora, it's, it turns out to be very rich because, yes, there are,

29:21.680 --> 29:27.840
 there are, there's a, you know, the most verbs that, and, you know, you just tack on ED. And then

29:27.840 --> 29:32.800
 there are exceptions, but there are also, there's also, there are, there are rules that, you know,

29:32.800 --> 29:38.240
 there's the exceptions aren't just random. There are certain clues to which, which,

29:38.880 --> 29:44.240
 which verbs should be exceptional. And then there are exceptions to the exceptions. And

29:44.240 --> 29:51.040
 there was a word that was kind of deployed in order to capture this, which was quasi regular.

29:51.040 --> 29:56.720
 In other words, there are rules, but it's, it's messy. And there, there's, there's structure even

29:56.720 --> 30:01.920
 among the exceptions. And, and it would be, yeah, you could try to write down, you could try to write

30:01.920 --> 30:07.680
 down the structure in some sort of closed form, but really, the right way to understand how the

30:07.680 --> 30:13.360
 brain is handling all this, and by the way, producing all of this is to build a deep neural

30:13.360 --> 30:18.560
 network and train it on this data and see how it ends up representing all of this richness. So

30:18.560 --> 30:24.240
 the way that deep learning was deployed in cognitive psychology was, that was the spirit of it.

30:24.240 --> 30:30.720
 It was about that richness. And that's something that I always found very, very compelling, still

30:30.720 --> 30:36.480
 do. Is it, is there something especially interesting and profound to you in terms of our

30:36.480 --> 30:43.760
 current deep learning neural network, artificial neural network approaches, and the whatever

30:43.760 --> 30:49.120
 we do understand about the biological neural networks in our brain? Is there, there's some,

30:49.120 --> 30:55.120
 there's quite a few differences. Are some of them to you either interesting or perhaps

30:55.680 --> 31:04.320
 profound in terms of, in terms of the gap we might want to try to close in trying to create a human

31:04.320 --> 31:05.280
 level intelligence?

31:06.160 --> 31:10.240
 What I would say here is something that a lot of people are saying, which is that

31:10.240 --> 31:19.520
 one seeming limitation of the systems that we're building now is that they lack the kind of

31:19.520 --> 31:28.880
 flexibility, the readiness to sort of turn on a dime when the context calls for it. That is so

31:28.880 --> 31:35.120
 characteristic of human behavior. So is that connected to you to the, like which aspect of the

31:35.120 --> 31:41.520
 neural networks in our brain is that connected to? Is that closer to the cognitive science level of

31:44.080 --> 31:49.840
 now again, see like my natural inclination is to separate into three disciplines of

31:49.840 --> 31:55.600
 neuroscience, cognitive science and psychology. And you've already kind of shut that down by

31:55.600 --> 32:01.840
 saying you're kind of see them as separate. But just to look at those layers, I guess, where

32:01.840 --> 32:10.320
 is there something about the lowest layer of the way the neurons interact that is profound to you

32:10.320 --> 32:15.760
 in terms of this difference to the artificial neural networks? Or is all the key differences

32:15.760 --> 32:17.760
 at a higher level of abstraction?

32:19.200 --> 32:25.760
 One thing I often think about is that if you take an introductory computer science course

32:25.760 --> 32:35.200
 and they are introducing you to the notion of Turing machines, one way of articulating

32:35.760 --> 32:43.520
 what the significance of a Turing machine is, is that it's a machine emulator. It can emulate any

32:43.520 --> 32:56.880
 other machine. And that to me, that way of looking at a Turing machine really sticks with me. I

32:56.880 --> 33:05.920
 think of humans as maybe sharing in some of that character. We're capacity limited. We're not Turing

33:05.920 --> 33:14.080
 machines, obviously, but we have the ability to adapt behaviors that are very much unlike anything

33:14.080 --> 33:19.360
 we've done before. But there's some basic mechanism that's implemented in our brain that allows us to

33:19.360 --> 33:21.360
 run software.

33:21.360 --> 33:26.240
 But just on that point, you mentioned a Turing machine, but nevertheless, it's fundamentally

33:26.240 --> 33:30.000
 our brains are just computational devices in your view. Is that what you're getting at?

33:30.000 --> 33:37.600
 Like, it was a little bit unclear to this line you drew. Is there any magic in there,

33:37.600 --> 33:39.440
 or is it just basic computation?

33:40.480 --> 33:46.720
 I'm happy to think of it as just basic computation. But mind you, I won't be satisfied until somebody

33:46.720 --> 33:52.800
 explains to me what the basic computations are that are leading to the full richness of human

33:52.800 --> 33:59.120
 cognition. It's not going to be enough for me to understand what the computations are that allow

33:59.120 --> 34:06.320
 people to do arithmetic or play chess. I want the whole thing.

34:06.320 --> 34:11.440
 And a small tangent because you kind of mentioned coronavirus, there's group behavior.

34:13.360 --> 34:17.360
 Is there something interesting to your search of understanding the human mind

34:18.640 --> 34:24.800
 where behavior of large groups or just behavior of groups is interesting? Seeing that as a

34:24.800 --> 34:29.280
 collective mind, as a collective intelligence, perhaps seeing the groups of people as a single

34:29.840 --> 34:34.880
 intelligent organisms, especially looking at the reinforcement learning work you've done recently.

34:35.520 --> 34:43.600
 Well, yeah, I have the honor of working with a lot of incredibly smart people,

34:43.600 --> 34:48.880
 and I wouldn't want to take any credit for leading the way on the multiagent work that's

34:48.880 --> 34:56.320
 come out of my group or deep mind lately. But I do find it fascinating. And I think

34:59.680 --> 35:06.800
 it can't be debated. Human behavior arises within communities. That just seems to me

35:07.440 --> 35:14.800
 self evident. But to me, it is self evident, but that seems to be a profound aspects of

35:14.800 --> 35:20.080
 something that created. That was like, if you look at 2001 Space Odyssey, when the monkeys

35:20.080 --> 35:26.880
 touched the... Yeah. That's the magical moment, I think Yovar Harari argues that the ability of our

35:28.160 --> 35:32.240
 large numbers of humans to hold an idea, to converge towards idea together, like he said,

35:32.240 --> 35:40.800
 shaking hands versus bumping elbows, somehow converge without being in a room altogether,

35:40.800 --> 35:46.880
 just kind of this distributed convergence towards an idea over a particular period of time seems

35:46.880 --> 35:54.880
 to be fundamental to just every aspect of our cognition, of our intelligence. Because humans,

35:54.880 --> 36:00.160
 we'll talk about reward, but it seems like we don't really have a clear objective function under

36:00.160 --> 36:05.600
 which we operate, but we all kind of converge towards one somehow. And that, to me, has always

36:05.600 --> 36:13.520
 been a mystery that I think is somehow productive for also understanding AI systems.

36:13.520 --> 36:18.640
 But I guess that's the next step. The first step is try to understand the mind.

36:18.640 --> 36:22.640
 Well, I don't know. I mean, I think there's something to the argument that

36:24.240 --> 36:30.400
 that kind of bottom, like strictly bottom up approach is wrongheaded. In other words,

36:30.400 --> 36:36.960
 there are basic aspects of human intelligence that

36:38.240 --> 36:45.280
 can only be understood in the context of groups. I'm perfectly open to that. I've never been

36:45.280 --> 36:52.960
 particularly convinced by the notion that we should consider intelligence to adhere

36:53.920 --> 36:59.520
 at the level of communities. I don't know why. I'm sort of stuck on the notion that the basic

36:59.520 --> 37:05.840
 unit that we want to understand is individual humans. And if we have to understand that in

37:05.840 --> 37:13.520
 the context of other humans, fine. But for me, intelligence is just... I stubbornly define it

37:13.520 --> 37:19.440
 as something that is an aspect of an individual human. That's just my...

37:19.440 --> 37:24.640
 I'm with you, but that could be the reductionist dream of a scientist because you can understand

37:24.640 --> 37:31.600
 a single human. It also is very possible that intelligence can only arise when there's multiple

37:31.600 --> 37:38.960
 intelligences. When there's multiple... It's a sad thing if that's true because it's very difficult

37:38.960 --> 37:45.120
 to study. But if it's just one human, that one human would not be... Homo sapiens would not

37:45.120 --> 37:51.920
 become that intelligent. That's a possibility. I'm with you. One thing I will say along these lines

37:51.920 --> 38:02.960
 is that I think a serious effort to understand human intelligence,

38:05.440 --> 38:11.920
 and maybe to build a human like intelligence, needs to pay just as much attention to the

38:11.920 --> 38:20.720
 structure of the environment as to the structure of the cognizing system, whether it's a brain

38:20.720 --> 38:27.840
 or an AI system. That's one thing I took away actually from my early studies with the pioneers

38:27.840 --> 38:35.120
 of neural network research, people like Jay McClelland and John Cohen. The structure of

38:36.080 --> 38:44.400
 cognition is really... It's only partly a function of the architecture of the brain

38:44.400 --> 38:50.240
 and the learning algorithms that it implements. What really shapes it is the

38:50.240 --> 38:56.160
 interaction of those things with the structure of the world in which those things are embedded,

38:56.160 --> 39:00.800
 right? And that's especially important for... That's made most clear in reinforcement learning

39:00.800 --> 39:05.680
 where the simulated environment is... You can only learn as much as you can simulate,

39:05.680 --> 39:10.960
 and that's what made deep mind made very clear with the other aspect of the environment,

39:10.960 --> 39:17.280
 which is the self play mechanism of the other agent, of the competitive behavior, which

39:17.280 --> 39:22.480
 the other agent becomes the environment, essentially. And that's one of the most exciting

39:22.480 --> 39:28.480
 ideas in AI is the self play mechanism that's able to learn successfully. So there you go.

39:28.480 --> 39:34.880
 There's a thing where competition is essential for learning, at least in that context.

39:34.880 --> 39:41.040
 So if we can step back into another sort of beautiful world, which is the actual mechanics,

39:41.040 --> 39:48.000
 the dirty mess of it of the human brain, is there something for people who might not know?

39:49.360 --> 39:54.080
 Is there something you can comment on or describe the key parts of the brain that are

39:54.080 --> 39:58.640
 important for intelligence or just in general, what are the different parts of the brain that

39:58.640 --> 40:04.000
 you're curious about that you've studied and that are just good to know about when you're

40:04.000 --> 40:13.440
 thinking about cognition? Well, my area of expertise, if I have one, is prefrontal cortex.

40:14.080 --> 40:24.960
 So... What's that? Where do we... It depends on who you ask. The technical definition is anatomical.

40:25.680 --> 40:32.960
 There are parts of your brain that are responsible for motor behavior, and they're

40:32.960 --> 40:43.040
 very easy to identify. And the region of your cerebral cortex, the sort of outer crust of

40:43.040 --> 40:49.200
 your brain that lies in front of those is defined as the prefrontal cortex.

40:49.200 --> 40:56.080
 And when you say anatomical, sorry to interrupt. So that's referring to sort of the geographic

40:56.080 --> 41:00.080
 region as opposed to some kind of functional definition.

41:00.080 --> 41:05.920
 Exactly. So this is kind of the coward's way out. I'm telling you what the prefrontal cortex is

41:05.920 --> 41:09.520
 just in terms of what part of the real estate it occupies.

41:09.520 --> 41:10.800
 The thing in the front of the brain.

41:10.800 --> 41:18.800
 Yeah, exactly. And in fact, the early history of the neuroscientific

41:20.000 --> 41:26.080
 investigation of what this front part of the brain does is sort of funny to read because

41:26.080 --> 41:35.600
 it was really World War I that started people down this road of trying to figure out what

41:35.600 --> 41:41.520
 different parts of the brain the human brain do in the sense that there were a lot of people

41:41.520 --> 41:46.880
 with brain damage who came back from the war with brain damage. And that provided as tragic as that

41:46.880 --> 41:52.800
 was, it provided an opportunity for scientists to try to identify the functions of different brain

41:52.800 --> 41:58.160
 regions. And that was actually incredibly productive. But one of the frustrations that

41:58.160 --> 42:03.040
 neuropsychologists faced was they couldn't really identify exactly what the deficit was

42:03.760 --> 42:09.040
 that arose from damage to these most kind of frontal parts of the brain. It was just a very

42:09.040 --> 42:17.920
 difficult thing to pin down. There were a couple of neuropsychologists who identified

42:17.920 --> 42:23.520
 through a large amount of clinical experience and close observation, they started to

42:24.480 --> 42:27.760
 put their finger on a syndrome that was associated with frontal damage. Actually,

42:27.760 --> 42:33.600
 one of them was a Russian neuropsychologist named Luria, who students of cognitive

42:33.600 --> 42:42.400
 psychology still read. And what he started to figure out was that the frontal cortex was

42:42.400 --> 42:53.360
 somehow involved in flexibility, in guiding behaviors that required someone to override a

42:53.360 --> 43:00.960
 habit or to do something unusual or to change what they were doing in every flexible way from

43:00.960 --> 43:08.560
 one moment to another. So focused on like new experiences. And so the way your brain processes

43:08.560 --> 43:14.960
 and acts in new experiences. Yeah. What later helped bring this function into

43:15.760 --> 43:21.760
 better focus was a distinction between controlled and automatic behavior or in other

43:21.760 --> 43:27.440
 literatures this is referred to as habitual behavior versus goal directed behavior.

43:28.160 --> 43:36.400
 So it's very, very clear that the human brain has pathways that are dedicated to habits,

43:36.400 --> 43:43.280
 to things that you do all the time and they need to be automatized so that they don't

43:43.280 --> 43:48.400
 require you to concentrate too much. So that leaves your cognitive capacity free to do other

43:48.400 --> 43:56.240
 things. Just think about the difference between driving when you're learning to drive versus

43:56.240 --> 44:04.240
 driving after you're a fairly expert. There are brain pathways that slowly absorb those

44:04.240 --> 44:12.160
 frequently performed behaviors so that they can be habits, so that they can be automatic.

44:12.160 --> 44:16.480
 That's kind of like the purest form of learning. I guess it's happening there,

44:16.480 --> 44:21.680
 which is why, I mean, this is kind of jumping ahead, which is why that perhaps is the most

44:21.680 --> 44:26.400
 useful for us to focusing on and trying to see how artificial intelligence systems can learn.

44:27.120 --> 44:30.800
 Is that the way? It's interesting. I do think about this distinction between controlled and

44:30.800 --> 44:38.240
 automatic or goal directed and habitual behavior a lot in thinking about where we are in AI research.

44:42.800 --> 44:50.480
 But just to finish the kind of dissertation here, the role of the prefrontal cortex

44:51.280 --> 45:00.240
 is generally understood these days sort of in contradistinction to that habitual domain.

45:00.240 --> 45:04.720
 In other words, the prefrontal cortex is what helps you override those habits.

45:06.160 --> 45:10.640
 It's what allows you to say, well, what I usually do in this situation is X,

45:10.640 --> 45:16.640
 but given the context, I probably should do Y. I mean, the elbow bump is a great example.

45:18.000 --> 45:25.120
 Reaching out and shaking hands is probably a habitual behavior, and it's the prefrontal cortex

45:25.120 --> 45:30.560
 that allows us to bear in mind that there's something unusual going on right now. In this

45:30.560 --> 45:38.480
 situation, I need to not do the usual thing. The kind of behaviors that Luria reported,

45:38.480 --> 45:44.160
 and he built tests for detecting these kinds of things, were exactly like this. In other words,

45:44.880 --> 45:50.640
 when I stick out my hand, I want you instead to present your elbow. A patient with frontal

45:50.640 --> 45:55.680
 damage would have a great deal of trouble with that. Somebody proffering their hand would elicit

45:56.800 --> 46:03.760
 a handshake. The prefrontal cortex is what allows us to say, hold on. That's the usual thing,

46:03.760 --> 46:10.400
 but I have the ability to bear in mind even very unusual contexts and to reason about

46:10.400 --> 46:18.240
 what behavior is appropriate there. Just to get a sense, are us humans special in the presence of

46:18.240 --> 46:24.960
 the prefrontal cortex? Do mice have a prefrontal cortex? Do other mammals that we can study?

46:26.400 --> 46:29.520
 If no, then how do they integrate new experiences?

46:31.280 --> 46:38.160
 That's a really tricky question and a very timely question because we have

46:38.160 --> 46:50.880
 revolutionary new technologies for monitoring, measuring, and also causally influencing neural

46:50.880 --> 47:01.760
 behavior in mice and fruit flies. These techniques are not fully available even for studying

47:01.760 --> 47:15.680
 brain function in monkeys, let alone humans. It's a very urgent question whether the kinds

47:15.680 --> 47:20.400
 of things that we want to understand about human intelligence can be pursued in these

47:20.400 --> 47:31.520
 other organisms. To put it briefly, there's disagreement. People who study fruit flies

47:31.520 --> 47:36.720
 will often tell you, hey, fruit flies are smarter than you think. They'll point to experiments

47:36.720 --> 47:43.520
 where fruit flies were able to learn new behaviors, were able to generalize from one stimulus to

47:43.520 --> 47:48.880
 another in a way that suggests that they have abstractions that guide their generalization.

47:48.880 --> 47:59.120
 I've had many conversations in which I will start by recounting some

48:02.480 --> 48:09.120
 observation about mouse behavior, where it seemed like mice were taking an awfully long time to

48:09.120 --> 48:16.880
 learn a task that for a human would be profoundly trivial. I will conclude from that that mice

48:16.880 --> 48:21.040
 really don't have the cognitive flexibility that we want to explain, and that a mouse researcher

48:21.040 --> 48:28.480
 will say to me, well, hold on. That experiment may not have worked because you asked a mouse to

48:29.360 --> 48:35.680
 deal with stimuli and behaviors that were very unnatural for the mouse. If instead you

48:36.400 --> 48:44.320
 kept the logic of the experiment the same, but presented it the information in a way

48:44.320 --> 48:48.400
 that aligns with what mice are used to dealing with in their natural habitats,

48:48.400 --> 48:51.520
 you might find that a mouse actually has more intelligence than you think.

48:52.400 --> 48:57.280
 And then they'll go on to show you videos of mice doing things in their natural habitat,

48:57.280 --> 49:03.920
 which seem strikingly intelligent, dealing with physical problems. I have to drag this piece

49:03.920 --> 49:09.760
 of food back to my layer, but there's something in my way, and how do I get rid of that thing?

49:09.760 --> 49:14.880
 And so I think these are open questions to put it, to sum that up.

49:14.880 --> 49:20.320
 And then taking a small step back related to that, as you kind of mentioned, we're taking

49:20.320 --> 49:27.360
 that little shortcut by saying it's a geographic part of the prefrontal cortex is a region of

49:27.360 --> 49:34.800
 the brain. But what's your sense in a bigger philosophical view, prefrontal cortex and the

49:34.800 --> 49:40.400
 brain in general? Do you have a sense that it's a set of subsystems in the way we've kind of implied

49:41.760 --> 49:48.320
 that are pretty distinct? Or to what degree is it that? Or to what degree is it a giant

49:48.320 --> 49:53.680
 interconnected mess where everything kind of does everything and it's impossible to disentangle them?

49:54.800 --> 50:00.880
 I think there's overwhelming evidence that there's functional differentiation, that it's

50:00.880 --> 50:09.040
 clearly not the case that all parts of the brain are doing the same thing. This follows immediately

50:09.040 --> 50:17.360
 from the kinds of studies of brain damage that we were chatting about before. It's obvious from

50:17.360 --> 50:22.400
 what you see if you stick an electrode in the brain and measure what's going on at the level of

50:22.400 --> 50:32.640
 neural activity. Having said that, there are two other things to add which kind of, I don't know,

50:32.640 --> 50:41.120
 maybe tug in the other direction. One is that when you look carefully at functional differentiation

50:41.120 --> 50:48.080
 in the brain, what you usually end up concluding, at least this is my observation of the literature,

50:48.080 --> 50:54.400
 is that the differences between regions are graded rather than being discrete.

50:55.200 --> 51:05.360
 So, it doesn't seem like it's easy to divide the brain up into true modules that have clear

51:05.360 --> 51:15.360
 boundaries and that have clear channels of communication between them.

51:15.360 --> 51:20.240
 And this applies to the prefrontal cortex. Yeah, the prefrontal cortex is made up of a

51:20.240 --> 51:28.880
 bunch of different subregions, the functions of which are not clearly defined and the borders

51:28.880 --> 51:35.200
 of which seem to be quite vague. Then there's another thing that's popping up in very recent

51:35.200 --> 51:46.720
 research which involves application of these new techniques. There are a number of studies that

51:46.720 --> 51:54.960
 suggest that parts of the brain that we would have previously thought were quite focused

51:54.960 --> 52:01.200
 in their function are actually carrying signals that we wouldn't have thought would be there.

52:01.200 --> 52:06.240
 For example, looking in the primary visual cortex, which is classically thought of as

52:07.120 --> 52:11.360
 basically the first cortical way station for processing visual information, basically what

52:11.360 --> 52:18.000
 it should care about is where are the edges in this scene that I'm viewing? It turns out that

52:18.000 --> 52:22.720
 if you have enough data, you can recover information from primary visual cortex about all sorts of

52:22.720 --> 52:29.200
 things like what behavior the animal is engaged in right now and how much reward is on offer

52:29.200 --> 52:38.240
 in the task that it's pursuing. It's clear that even regions whose function is pretty well defined

52:38.800 --> 52:46.160
 at a core screen are nonetheless carrying some information about information from very different

52:46.160 --> 52:52.800
 domains. The history of neuroscience is this oscillation between the two views that you

52:52.800 --> 53:01.760
 articulated, the modular view and then the big mush view. I guess we're going to end up somewhere

53:01.760 --> 53:07.600
 in the middle, which is unfortunate for our understanding because there's something about

53:07.600 --> 53:13.200
 our conceptual system that finds it's easy to think about a modularized system and easy to

53:13.200 --> 53:18.880
 think about a completely undifferentiated system, but something that lies in between is confusing,

53:18.880 --> 53:23.680
 but we're going to have to get used to it, I think. Unless we can understand deeply the lower

53:23.680 --> 53:29.520
 level mechanism of neuronal communication and so on. On that topic, you mentioned information.

53:29.520 --> 53:34.560
 Just to get a sense, I imagine something that there's still mystery and disagreement on

53:34.560 --> 53:41.200
 is how does the brain carry information and signal? What in your sense is the basic

53:41.200 --> 53:49.440
 mechanism of communication in the brain? Well, I guess I'm old fashioned in that

53:50.000 --> 53:56.400
 I consider the networks that we use in deep learning research to be a reasonable approximation

53:56.960 --> 54:05.360
 to the mechanisms that carry information in the brain. The usual way of articulating that is to

54:05.360 --> 54:13.120
 say, what really matters is a rate code. What matters is how quickly is an individual neuron

54:13.120 --> 54:17.760
 spiking? What's the frequency at which it's spiking? Is it the timing of the spiking?

54:17.760 --> 54:23.920
 Yeah. Is it firing fast or slow? Let's put a number on that, and that number is enough to

54:23.920 --> 54:31.440
 capture what neurons are doing. There's still uncertainty about whether that's an

54:31.440 --> 54:42.080
 adequate description of how information is transmitted within the brain. There are studies

54:42.080 --> 54:49.600
 that suggest that the precise timing of spikes matters. There are studies that suggest that

54:49.600 --> 54:55.840
 there are computations that go on within the dendritic tree, within a neuron, that are quite

54:55.840 --> 55:00.480
 rich and structured and that really don't equate to anything that we're doing in our artificial

55:00.480 --> 55:10.080
 neural networks. Having said that, I feel like we're getting somewhere by sticking to this

55:10.080 --> 55:16.480
 high level of abstraction. By the way, we're talking about the electrical signal. I remember

55:16.480 --> 55:23.440
 reading some vague paper somewhere recently where the mechanical signal, like the vibrations or

55:23.440 --> 55:30.800
 something of the neurons also communicate information. I haven't seen that. There's

55:30.800 --> 55:37.280
 somebody was arguing that the electrical signal, this is in nature paper, something like that,

55:37.280 --> 55:44.240
 where the electrical signal is actually a side effect of the mechanical signal. I don't think

55:44.240 --> 55:50.160
 they changed the story, but it's almost an interesting idea that there could be a deeper.

55:50.160 --> 55:56.080
 It's always in physics with quantum mechanics, there's always a deeper story that could be

55:56.080 --> 56:01.040
 underlying the whole thing. You think it's basically the rate of spiking that gets us,

56:01.040 --> 56:03.840
 that's the lowest hanging fruit that can get us really far.

56:04.960 --> 56:11.760
 This is a classical view. The only way in which this stance would be controversial is

56:12.960 --> 56:17.760
 in the sense that there are members of the neuroscience community who are interested

56:17.760 --> 56:22.880
 in alternatives, but this is really a very mainstream view. The way that neurons communicate

56:22.880 --> 56:34.560
 is that neurotransmitters arrive, they wash up on a neuron. The neuron has receptors for

56:34.560 --> 56:41.440
 those transmitters. The meeting of the transmitter with these receptors changes the voltage of the

56:41.440 --> 56:48.160
 neuron. If enough voltage change occurs, then a spike occurs, one of these discrete events.

56:49.200 --> 56:54.400
 It's that spike that is conducted down the axon and leads to neurotransmitter release.

56:54.400 --> 56:58.800
 This is just like neuroscience 101. This is the way the brain is supposed to work.

57:00.560 --> 57:05.920
 What we do when we build artificial neural networks of the kind that are now popular in the AI

57:05.920 --> 57:13.120
 community is that we don't worry about those individual spikes, we just worry about the

57:13.120 --> 57:19.760
 frequency at which those spikes are being generated. People talk about that as the

57:19.760 --> 57:30.800
 activity of a neuron. The activity of units in a deep learning system is broadly analogous to

57:30.800 --> 57:37.920
 the spike rate of a neuron. There are people who believe that there are other forms of

57:37.920 --> 57:41.840
 communication in the brain. In fact, I've been involved in some research recently that suggests

57:41.840 --> 57:53.040
 that the voltage fluctuations that occur in populations of neurons that are below the

57:53.040 --> 58:00.080
 level of spike production may be important for communication, but I'm still pretty old school

58:00.080 --> 58:06.000
 in the sense that I think that the things that we're building in AI research constitute reasonable

58:06.880 --> 58:12.880
 models of how a brain would work. Let me ask just for fun a crazy question,

58:13.440 --> 58:19.440
 because I can. Do you think it's possible we're completely wrong about the way this basic

58:19.440 --> 58:24.960
 mechanism of neuronal communication, that the information is stored in some very different

58:24.960 --> 58:31.200
 kind of way in the brain? Heck yes. I wouldn't be a scientist if I didn't think there was any

58:31.200 --> 58:40.480
 chance we were wrong. If you look at the history of deep learning research as it's been applied to

58:40.480 --> 58:44.720
 neuroscience, of course, the vast majority of deep learning research these days isn't about

58:44.720 --> 58:53.520
 neuroscience, but if you go back to the 1980s, there's an unbroken chain of research in which

58:53.520 --> 59:00.720
 a particular strategy is taken, which is, hey, let's train a deep learning system. Let's train a

59:01.840 --> 59:11.040
 multi layer neural network on this task that we trained our rat on or our monkey on or this

59:11.040 --> 59:19.040
 human being on. Then let's look at what the units deep in the system are doing. Let's ask whether

59:19.040 --> 59:26.160
 they're doing resembles what we know about what neurons deep in the brain are doing. Over and

59:26.160 --> 59:33.440
 over and over and over, that strategy works in the sense that the learning algorithms that we

59:33.440 --> 59:42.080
 have access to, which typically center on back propagation, they give rise to patterns of activity,

59:42.080 --> 59:48.720
 patterns of response, patterns of neuronal behavior in these artificial models

59:48.720 --> 59:56.720
 that look hauntingly similar to what you see in the brain. Is that a coincidence?

59:57.360 --> 1:00:02.800
 At a certain point, it starts looking like such coincidence is unlikely to not be deeply meaningful.

1:00:04.720 --> 1:00:10.160
 The circumstantial evidence is overwhelmed. But you're always open to total flipping of the

1:00:10.160 --> 1:00:17.200
 table. Of course. You have coauthored several recent papers that weave beautifully between

1:00:17.200 --> 1:00:25.680
 the world of neuroscience and artificial intelligence. Maybe if we could just try to

1:00:25.680 --> 1:00:30.320
 dance around and talk about some of them, maybe try to pick out interesting ideas that jump to

1:00:30.320 --> 1:00:36.800
 your mind from memory. Maybe looking at, we're talking about the prefrontal cortex, the 2018,

1:00:36.800 --> 1:00:41.760
 I believe, paper called the prefrontal cortex is a matter of reinforcement learning system.

1:00:41.760 --> 1:00:46.400
 Yeah. Is there a key idea that you can speak to from that paper?

1:00:47.600 --> 1:00:54.800
 Yeah. The key idea is about meta learning. What is meta learning?

1:00:54.800 --> 1:01:04.640
 Meta learning is, by definition, a situation in which you have a learning algorithm,

1:01:04.640 --> 1:01:13.200
 and the learning algorithm operates in such a way that it gives rise to another learning algorithm.

1:01:14.000 --> 1:01:20.240
 In the earliest applications of this idea, you had one learning algorithm sort of adjusting

1:01:20.240 --> 1:01:25.680
 the parameters on another learning algorithm. But the case that we're interested in this paper is

1:01:25.680 --> 1:01:30.960
 one where you start with just one learning algorithm, and then another learning algorithm kind of

1:01:30.960 --> 1:01:37.360
 emerges out of thin air. I can say more about what I mean by that. I don't mean to be

1:01:38.800 --> 1:01:46.080
 scurrent. But that's the idea of meta learning. It relates to the old idea in psychology of

1:01:46.080 --> 1:01:56.640
 learning to learn. Situations where you have experiences that make you better at learning

1:01:56.640 --> 1:02:01.760
 something new. A familiar example would be learning a foreign language. The first time

1:02:01.760 --> 1:02:08.560
 you learn a foreign language, it may be quite laborious and disorienting and novel. But if

1:02:08.560 --> 1:02:14.240
 let's say you've learned two foreign languages, the third foreign language obviously is going

1:02:14.240 --> 1:02:20.080
 to be much easier to pick up. Why? Because you've learned how to learn. You know how this goes.

1:02:20.080 --> 1:02:22.480
 You know, okay, I'm going to have to learn how to conjugate. I'm going to have to...

1:02:22.480 --> 1:02:29.200
 But that's a simple form of meta learning, in the sense that there's some slow learning

1:02:29.200 --> 1:02:35.600
 mechanism that's helping you kind of update your fast learning mechanism. Does that bring

1:02:35.600 --> 1:02:41.600
 you into focus? From our understanding, from the psychology world, from the neuroscience,

1:02:42.240 --> 1:02:49.360
 our understanding how meta learning might work in the human brain, what lessons can we draw

1:02:49.360 --> 1:02:55.120
 from that that we can bring into the artificial intelligence world? Well, yeah. The origin of

1:02:55.120 --> 1:03:03.840
 that paper was in AI work that we were doing in my group. We were looking at what happens when you

1:03:03.840 --> 1:03:10.880
 train a recurrent neural network using standard reinforcement learning algorithms. But you train

1:03:10.880 --> 1:03:14.720
 that network not just in one task, but you train it in a bunch of interrelated tasks.

1:03:14.720 --> 1:03:21.600
 And then you ask what happens when you give it yet another task in that sort of line of

1:03:21.600 --> 1:03:31.280
 interrelated tasks. And what we started to realize is that a form of meta learning spontaneously

1:03:31.280 --> 1:03:37.360
 happens in recurrent neural networks. And the simplest way to explain it is to say

1:03:37.360 --> 1:03:46.480
 a recurrent neural network has a kind of memory in its activation patterns. It's recurrent by

1:03:46.480 --> 1:03:50.880
 definition in the sense that you have units that connect to other units that connect to other units.

1:03:50.880 --> 1:03:56.320
 So you have sort of loops of connectivity, which allows activity to stick around and be updated

1:03:56.320 --> 1:04:00.320
 over time. In psychology, in neuroscience, we call this working memory. It's like

1:04:00.320 --> 1:04:10.160
 actively holding something in mind. And so that memory gives the recurrent neural network

1:04:11.600 --> 1:04:19.840
 a dynamics. The way that the activity pattern evolves over time is inherent to the connectivity

1:04:19.840 --> 1:04:25.440
 of the recurrent neural network. So that's idea number one. Now, the dynamics of that network

1:04:25.440 --> 1:04:31.200
 are shaped by the connectivity, by the synaptic weights. And those synaptic weights are being

1:04:31.200 --> 1:04:35.680
 shaped by this reinforcement learning algorithm that you're training the network with.

1:04:37.600 --> 1:04:42.560
 So the punchline is, if you train a recurrent neural network with a reinforcement learning

1:04:42.560 --> 1:04:48.240
 algorithm that's adjusting its weights, and you do that for long enough, the activation dynamics

1:04:48.240 --> 1:04:55.360
 will become very interesting. So imagine I give you a task where you have to press one button

1:04:55.360 --> 1:05:01.360
 or another, left button or right button. And there's some probability that I'm going to give you

1:05:01.360 --> 1:05:06.240
 an M&M if you press the left button. And there's some probability I'll give you an M&M if you

1:05:06.240 --> 1:05:10.000
 press the other button. And you have to figure out what those probabilities are just by trying

1:05:10.000 --> 1:05:16.240
 things out. But as I said before, instead of just giving you one of these tasks, I give you a whole

1:05:16.240 --> 1:05:20.080
 sequence. You know, I give you two buttons, and you figure out which one's best. And I go,

1:05:20.080 --> 1:05:23.920
 good job. Here's a new box. Two new buttons. You have to figure out which one's best.

1:05:23.920 --> 1:05:28.160
 Good job. Here's a new box. And every box has its own probabilities, and you have to figure.

1:05:28.160 --> 1:05:32.240
 So if you train a recurrent neural network on that kind of sequence of tasks,

1:05:33.600 --> 1:05:38.240
 what happens, it seemed almost magical to us when we first started kind of

1:05:39.280 --> 1:05:45.120
 realizing what was going on. The slow learning algorithm that's adjusting the synaptic weights,

1:05:46.800 --> 1:05:50.160
 those slow synaptic changes give rise to a network dynamics

1:05:50.160 --> 1:05:57.280
 that the dynamics themselves turn into a learning algorithm. So in other words,

1:05:57.280 --> 1:06:00.960
 you can tell this is happening by just freezing the synaptic weights, saying,

1:06:00.960 --> 1:06:06.880
 okay, no more learning. You're done. Here's a new box. Figure out which button is best.

1:06:07.440 --> 1:06:12.960
 And the recurrent neural network will do this just fine. It figures out which button is best.

1:06:12.960 --> 1:06:18.000
 It kind of transitions from exploring the two buttons to just pressing the one that it likes

1:06:18.000 --> 1:06:24.640
 best in a very rational way. How is that happening? It's happening because the activity dynamics of

1:06:24.640 --> 1:06:29.200
 the network have been shaped by this slow learning process that's occurred over many,

1:06:29.200 --> 1:06:36.080
 many boxes. And so what's happened is that this slow learning algorithm that's slowly adjusting

1:06:36.080 --> 1:06:41.920
 the weights is changing the dynamics of the network, the activity dynamics, into its own

1:06:41.920 --> 1:06:52.480
 learning algorithm. And as we were realizing that this is a thing, it just so happened that the

1:06:52.480 --> 1:06:58.240
 group that was working on this included a bunch of neuroscientists. And it started kind of ringing

1:06:58.240 --> 1:07:04.000
 a bell for us, which is to say that we thought, this sounds a lot like the distinction between

1:07:04.560 --> 1:07:10.000
 synaptic learning and activity, synaptic memory and activity based memory in the brain.

1:07:10.000 --> 1:07:16.960
 And it also reminded us of recurrent connectivity that's very characteristic of

1:07:16.960 --> 1:07:23.200
 prefrontal function. So this is kind of why it's good to have people working on AI

1:07:24.080 --> 1:07:29.440
 that know a little bit about neuroscience and vice versa, because we started thinking about

1:07:29.440 --> 1:07:33.520
 whether we could apply this principle to neuroscience. And that's where the paper came from.

1:07:33.520 --> 1:07:39.440
 So the kind of principle of the recurrence they can see in the prefrontal cortex,

1:07:39.440 --> 1:07:46.800
 then you start to realize that it's possible for something like an idea of a learning to learn

1:07:48.400 --> 1:07:51.600
 emerging from this learning process, as long as you keep

1:07:52.640 --> 1:07:59.440
 varying the environment sufficiently. Exactly. So the kind of metaphorical transition we made

1:07:59.440 --> 1:08:04.880
 to neuroscience was to think, okay, well, we know that the prefrontal cortex is highly recurrent.

1:08:04.880 --> 1:08:10.320
 We know that it's an important locus for working memory for activation based memory.

1:08:11.280 --> 1:08:16.160
 So maybe the prefrontal cortex supports reinforcement learning. In other words,

1:08:18.160 --> 1:08:21.520
 what is reinforcement learning? You take an action, you see how much reward you got,

1:08:21.520 --> 1:08:26.800
 you update your policy of behavior. Maybe the prefrontal cortex is doing that sort of thing

1:08:26.800 --> 1:08:32.720
 strictly in its activation patterns. It's keeping around a memory in its activity patterns of what

1:08:32.720 --> 1:08:40.080
 you did, how much reward you got. And it's using that activity based memory as a basis for updating

1:08:40.080 --> 1:08:44.720
 behavior. But then the question is, well, how did the prefrontal cortex get so smart? In other

1:08:44.720 --> 1:08:50.720
 words, where did these activity dynamics come from? How did that program that's implemented in

1:08:50.720 --> 1:08:56.880
 the recurrent dynamics of the prefrontal cortex arise? And one answer that became evident in this

1:08:56.880 --> 1:09:04.880
 work was, well, maybe the mechanisms that operate on the synaptic level, which we believe are mediated

1:09:04.880 --> 1:09:12.800
 by dopamine, are responsible for shaping those dynamics. So this may be a silly question, but

1:09:12.800 --> 1:09:22.080
 because this kind of several temporal classes of learning are happening and the learning to

1:09:22.080 --> 1:09:30.480
 learn emerges, can you keep building stacks of learning to learn to learn, learning to learn

1:09:30.480 --> 1:09:35.680
 to learn to learn to learn? Because it keeps, I mean, basically abstractions of more powerful

1:09:36.240 --> 1:09:43.440
 abilities to generalize of learning complex rules. Or is this that's overstretching

1:09:44.960 --> 1:09:49.520
 this kind of mechanism? Well, one of the people in AI who

1:09:49.520 --> 1:09:55.520
 who started thinking about meta learning from very early on, Juergen and Schmitthuber,

1:09:57.040 --> 1:10:05.360
 sort of cheekily suggested, I think it may have been in his PhD thesis, that we should think

1:10:05.360 --> 1:10:11.760
 about meta, meta, meta, meta, meta, meta learning. That's really what's going to get us to true

1:10:11.760 --> 1:10:19.440
 intelligence. Certainly, there's a poetic aspect to it. And it seems interesting and correct that

1:10:19.440 --> 1:10:22.880
 that kind of levels of abstraction would be powerful. But is that something you see in the

1:10:22.880 --> 1:10:30.800
 brain? Is it useful to think of learning in these meta, meta, meta way, or is it just meta

1:10:30.800 --> 1:10:38.640
 learning? Well, one thing that really fascinated me about this mechanism that we were starting to

1:10:38.640 --> 1:10:45.440
 look at, and other groups started talking about very similar things at the same time. And then

1:10:45.440 --> 1:10:49.760
 a kind of explosion of interest in meta learning happened in the AI community shortly after that.

1:10:50.400 --> 1:10:55.520
 I don't know if we had anything to do with that. But I was gratified to see that a lot of people

1:10:55.520 --> 1:11:01.280
 started talking about meta learning. One of the things that I liked about the kind of flavor

1:11:01.280 --> 1:11:06.640
 of meta learning that we were studying was that it didn't require anything special. It was just

1:11:06.640 --> 1:11:13.440
 if you took a system that had some form of memory, that the function of which could be shaped by

1:11:13.440 --> 1:11:20.800
 pick your RL algorithm, then this would just happen. I mean, there are a lot of forms of,

1:11:20.800 --> 1:11:25.520
 there are a lot of meta learning algorithms that have been proposed since then that are fascinating

1:11:25.520 --> 1:11:32.160
 and effective in their domains of application. But they're engineered. There are things that

1:11:32.160 --> 1:11:35.600
 somebody had to say, well, gee, if we wanted meta learning to happen, how would we do that?

1:11:35.600 --> 1:11:39.440
 Here's an algorithm that would, but there's something about the kind of meta learning

1:11:39.440 --> 1:11:45.280
 that we were studying that seemed to me special in the sense that it wasn't an algorithm. It was

1:11:45.280 --> 1:11:51.360
 just something that automatically happened if you had a system that had memory and it was

1:11:51.360 --> 1:11:58.480
 trained with a reinforcement learning algorithm. And in that sense, it can be as meta as it wants

1:11:58.480 --> 1:12:06.480
 to be. There's no limit on how abstract the meta learning can get because it's not reliant on

1:12:06.480 --> 1:12:13.680
 a human engineering a particular meta learning algorithm to get there. And that's, I also,

1:12:14.640 --> 1:12:19.120
 I don't know, I guess I hope that that's relevant in the brain. I think there's a kind of beauty

1:12:19.120 --> 1:12:25.520
 in the ability of this emergent. The emergent aspect of it. Yeah, it's something that's

1:12:25.520 --> 1:12:32.560
 engineered. Exactly. It's something that just happens in a sense. In a sense, you can't avoid

1:12:32.560 --> 1:12:37.760
 this happening. If you have a system that has memory, and the function of that memory is

1:12:39.200 --> 1:12:44.640
 shaped by reinforcement learning, and this system is trained in a series of interrelated tasks,

1:12:45.920 --> 1:12:50.080
 this is going to happen. You can't stop it. As long as you have certain properties,

1:12:50.080 --> 1:12:54.240
 maybe like a recurrent structure to. You have to have memory. It actually doesn't have to be

1:12:54.240 --> 1:12:59.680
 a recurrent neural network. A paper that I was honored to be involved with even earlier

1:12:59.680 --> 1:13:07.600
 used a kind of slot based memory. Do you remember the title? It was memory augmented neural

1:13:07.600 --> 1:13:12.560
 networks. I think the title was meta learning in memory augmented neural networks.

1:13:14.560 --> 1:13:21.680
 And it was the same exact story. If you have a system with memory, here it was a different

1:13:21.680 --> 1:13:30.320
 kind of memory. But the function of that memory is shaped by reinforcement learning. Here it was

1:13:31.200 --> 1:13:37.760
 the reads and writes that occurred on this slot based memory. This will just happen.

1:13:39.920 --> 1:13:44.240
 This brings us back to something I was saying earlier about the importance of the environment.

1:13:44.240 --> 1:13:52.960
 This will happen if the system is being trained in a setting where there's a sequence of tasks

1:13:52.960 --> 1:13:58.320
 that all share some abstract structure. Sometimes we talk about task distributions.

1:14:00.000 --> 1:14:06.000
 That's something that's very obviously true of the world that humans inhabit.

1:14:06.000 --> 1:14:16.240
 But if you just think about what you do every day, you never do exactly the same thing that

1:14:16.240 --> 1:14:21.360
 you did the day before. But everything that you do has a family resemblance. It shares

1:14:21.360 --> 1:14:30.240
 structure with something that you did before. And so the real world is saturated with this

1:14:30.240 --> 1:14:38.400
 kind of this property. It's endless variety with endless redundancy. And that's the setting in

1:14:38.400 --> 1:14:44.000
 which this kind of meta learning happens. And it does seem like we're just so good at finding,

1:14:44.880 --> 1:14:49.200
 just like in this emergent phenomenon you described, we're really good at finding that

1:14:49.200 --> 1:14:54.640
 redundancy, finding those similarities, the family resemblance. Some people call it sort of,

1:14:54.640 --> 1:15:00.880
 what is it? Melanie Mitchell was talking about analogies. So we're able to connect concepts

1:15:00.880 --> 1:15:07.760
 together in this kind of way, in this same kind of automated emergent way, which there's so many

1:15:07.760 --> 1:15:15.760
 echoes here of psychology and neuroscience and obviously now with reinforcement learning with

1:15:15.760 --> 1:15:20.720
 recurrent neural networks at the core. If we could talk a little bit about dopamine, you have really,

1:15:20.720 --> 1:15:27.280
 you're a part of coauthoring really exciting recent paper, very recent in terms of release

1:15:27.840 --> 1:15:34.240
 on dopamine and temporal difference learning. Can you describe the key ideas of that paper?

1:15:34.800 --> 1:15:40.240
 Sure. Yeah. I mean, one thing I want to pause to do is acknowledge my coauthors on actually

1:15:40.240 --> 1:15:42.640
 both of the papers we're talking about. So this dopamine paper.

1:15:42.640 --> 1:15:45.520
 I'll just, I'll certainly post all their names.

1:15:45.520 --> 1:15:50.320
 Okay, wonderful. Yeah. Because I'm sort of a bashed to be the spokesperson for

1:15:50.320 --> 1:15:56.960
 these papers when I had such amazing collaborators on both. So it's a comfort to me to know that

1:15:56.960 --> 1:16:00.080
 you'll acknowledge them. Yeah, this is an incredible team there. But yeah.

1:16:00.080 --> 1:16:05.600
 Oh, yeah. It's such a, it's so much fun. And in the case of the dopamine paper,

1:16:06.240 --> 1:16:11.600
 we also collaborate with Naouchita at Harvard, who obviously a paper simply wouldn't have happened

1:16:11.600 --> 1:16:17.440
 without him. But so you were asking for like a thumbnail sketch of?

1:16:17.440 --> 1:16:22.400
 Yeah, thumbnail sketch or key ideas or, you know, things, the insights that, you know,

1:16:22.400 --> 1:16:26.800
 continue on our kind of discussion here between neuroscience and AI.

1:16:26.800 --> 1:16:30.640
 Yeah. I mean, this was another, a lot of the work that we've done so far is

1:16:32.080 --> 1:16:39.200
 taking ideas that have bubbled up in AI and, you know, asking the question of whether the

1:16:39.200 --> 1:16:45.840
 brain might be doing something related, which I think on the surface sounds like something that's

1:16:45.840 --> 1:16:54.240
 really mainly of use to neuroscience. We see it also as a way of validating what we're doing

1:16:54.240 --> 1:17:00.400
 on the AI side. If we can gain some evidence that the brain is using some technique that

1:17:00.400 --> 1:17:07.600
 we've been trying out in our AI work, that gives us confidence that, you know, it may be a good idea

1:17:07.600 --> 1:17:14.000
 that it'll, you know, scale to rich complex tasks that it'll interface well with other

1:17:14.000 --> 1:17:18.800
 mechanisms. So you see it as a two way road, just because a particular paper is a little

1:17:18.800 --> 1:17:26.160
 bit focused on from one to the, from AI, from neural networks to neuroscience. Ultimately,

1:17:26.160 --> 1:17:32.080
 the discussion, the thinking, the productive long term aspect of it is the two way road

1:17:32.080 --> 1:17:38.080
 nature of the whole. Yeah. I mean, we've talked about the notion of a virtuous circle between

1:17:38.080 --> 1:17:46.240
 AI and neuroscience. And, you know, the way I see it, that's always been there since the two fields,

1:17:47.360 --> 1:17:53.360
 you know, jointly existed. There have been some phases in that history when AI was sort of ahead.

1:17:53.360 --> 1:17:58.480
 There are some phases when neuroscience was sort of ahead. I feel like, given the burst of

1:17:59.920 --> 1:18:06.160
 innovation that's happened recently on the AI side, AI is kind of ahead in the sense that

1:18:06.160 --> 1:18:12.800
 there are all of these ideas that we, you know, for which it's exciting to consider that there

1:18:12.800 --> 1:18:22.240
 might be neural analogs. And neuroscience, you know, in a sense has been focusing on approaches

1:18:22.240 --> 1:18:27.360
 to studying behavior that come from, you know, that are kind of derived from this earlier era

1:18:27.360 --> 1:18:34.240
 of cognitive psychology. And, you know, so in some ways, fail to connect with some of the issues

1:18:34.240 --> 1:18:39.200
 that we're grappling with in AI, like how do we deal with, you know, large, you know, complex

1:18:39.200 --> 1:18:48.000
 environments. But, you know, I think it's inevitable that this circle will keep turning and there

1:18:48.000 --> 1:18:54.480
 will be a moment in the not too different distant future when neuroscience is pelting AI researchers

1:18:54.480 --> 1:19:00.080
 with insights that may change the direction of our work. Just a quick human question.

1:19:00.080 --> 1:19:08.320
 Is it you have parts of your brain? This is very meta, but they're able to both think about

1:19:08.320 --> 1:19:17.680
 neuroscience and AI. You know, I don't often meet people like that. So do you think, let me ask a

1:19:17.680 --> 1:19:23.360
 meta plasticity question. Do you think a human being can be both good at AI and neuroscience?

1:19:23.360 --> 1:19:30.160
 They're like, what on the team at DeepMind, what kind of human can occupy these two realms? And

1:19:30.160 --> 1:19:36.240
 is that something you see everybody should be doing, can be doing, or is that a very special

1:19:36.240 --> 1:19:40.880
 few can kind of jump? Just like we talked about art history, I would think it's a special person

1:19:40.880 --> 1:19:48.080
 that can major in art history and also consider being a surgeon. Otherwise known as a dilettante?

1:19:48.080 --> 1:19:58.880
 A dilettante, yeah. Easily distracted. No. I think it does take a special kind of person to be

1:19:59.920 --> 1:20:07.920
 truly world class at both AI and neuroscience and I am not on that list. I happen to be someone

1:20:08.880 --> 1:20:17.520
 who's interested in neuroscience and psychology involved using the kinds of modeling techniques

1:20:17.520 --> 1:20:25.440
 that are now very central in AI. And that sort of, I guess, bought me a ticket to be involved in

1:20:25.440 --> 1:20:31.360
 all of the amazing things that are going on in AI research right now. I do know a few people who I

1:20:31.360 --> 1:20:37.040
 would consider pretty expert on both fronts, and I won't embarrass them by naming them. But

1:20:37.040 --> 1:20:43.040
 there are exceptional people out there who are like this. The one thing that I find

1:20:43.040 --> 1:20:54.320
 is a barrier to being truly world class on both fronts is the complexity of the technology

1:20:55.040 --> 1:21:04.080
 that's involved in both disciplines now. So the engineering expertise that it takes to do

1:21:05.040 --> 1:21:10.640
 truly front line hands on AI research is really, really considerable.

1:21:10.640 --> 1:21:15.280
 The learning curve of the tools, just like the specifics of just whether it's programming

1:21:15.280 --> 1:21:19.360
 or the kind of tools necessary to collect the data, to manage the data, to distribute,

1:21:19.360 --> 1:21:22.400
 to compute all that kind of stuff. And on the neuroscience, I guess, side,

1:21:22.400 --> 1:21:26.240
 there'll be all different sets of tools. Exactly, especially with the recent

1:21:26.240 --> 1:21:37.600
 explosion in neuroscience methods. So having said all that, I think the best scenario

1:21:37.600 --> 1:21:48.320
 for both neuroscience and AI is to have people interacting who live at every point on this

1:21:48.320 --> 1:21:55.520
 spectrum from exclusively focused on neuroscience to exclusively focused on the engineering side

1:21:55.520 --> 1:22:03.680
 of AI. But to have those people inhabiting a community where they're talking to people who

1:22:03.680 --> 1:22:09.920
 live elsewhere on the spectrum. And I may be someone who's very close to the center in the

1:22:09.920 --> 1:22:15.600
 sense that I have one foot in the neuroscience world and one foot in the AI world. And that

1:22:15.600 --> 1:22:22.160
 central position I will admit prevents me, at least someone with my limited cognitive capacity,

1:22:22.160 --> 1:22:28.720
 from having true technical expertise in either domain. But at the same time,

1:22:28.720 --> 1:22:35.520
 I at least hope that it's worthwhile having people around who can kind of see the connections.

1:22:35.520 --> 1:22:42.640
 Yeah, the community, the emergent intelligence of the community when it's nicely distributed

1:22:43.360 --> 1:22:47.040
 is useful. Okay, so. Exactly, yeah. So hopefully that, I mean, I've seen that work,

1:22:47.040 --> 1:22:53.600
 I've seen that work out well at DeepMind. There are people who, I mean, even if you just focus on

1:22:53.600 --> 1:22:59.680
 the AI work that happens at DeepMind, it's been a good thing to have some people around doing

1:22:59.680 --> 1:23:07.280
 that kind of work whose PhDs are in neuroscience or psychology. Every academic discipline has its

1:23:08.480 --> 1:23:16.880
 kind of blind spots and kind of unfortunate obsessions and its metaphors and its reference

1:23:16.880 --> 1:23:26.480
 points. And having some intellectual diversity is really healthy. People get each other unstuck,

1:23:27.040 --> 1:23:33.440
 I think. I see it all the time at DeepMind. And I like to think that the people who bring

1:23:33.440 --> 1:23:37.280
 some neuroscience background to the table are helping with that.

1:23:37.280 --> 1:23:42.240
 So one of the, one of my, like, probably the deepest passion for me, what I would say,

1:23:42.240 --> 1:23:49.520
 maybe we kind of spoke off, Mike, a little bit about it, but that I think is a blind spot for

1:23:49.520 --> 1:23:55.760
 at least robotics and AI folks is human robot interaction, human agent interaction. Maybe

1:23:56.960 --> 1:24:03.760
 do you have thoughts about how we reduce the size of that blind spot? Do you also share

1:24:04.960 --> 1:24:10.160
 the feeling that not enough folks are studying this aspect of interaction?

1:24:10.160 --> 1:24:16.720
 Well, I'm actually pretty intensively interested in this issue now. And there are people in my

1:24:16.720 --> 1:24:23.200
 group who've actually pivoted pretty hard over the last few years from doing more traditional

1:24:23.200 --> 1:24:29.040
 cognitive psychology and cognitive neuroscience to doing experimental work on human agent

1:24:29.040 --> 1:24:35.520
 interaction. And there are a couple of reasons that I'm pretty passionately interested in this.

1:24:35.520 --> 1:24:46.640
 One is it's kind of the outcome of having thought for a few years now about what we're up to.

1:24:48.000 --> 1:24:55.360
 What are we doing? What is this what is this aid AI research for? So what does it mean to

1:24:55.360 --> 1:25:00.160
 make the world a better place? I think I'm pretty sure that means making life better for humans.

1:25:00.160 --> 1:25:09.200
 Yeah. And so how do you make life better for humans? That's a proposition that when you look at it

1:25:09.920 --> 1:25:18.880
 carefully and honestly is rather horrendously complicated, especially when the AI systems that

1:25:21.200 --> 1:25:30.080
 you're building are learning systems. You're not programming something that you then introduce

1:25:30.080 --> 1:25:37.440
 to the world and it just works as programmed like Google Maps or something. We're building systems

1:25:37.440 --> 1:25:43.760
 that learn from experience. So that typically leads to AI safety questions. How do we keep

1:25:43.760 --> 1:25:48.880
 these things from getting out of control? How do we keep them from doing things that harm humans?

1:25:48.880 --> 1:25:56.320
 And I mean, I hasten to say, I consider those hugely important issues. And there are large

1:25:56.320 --> 1:26:01.600
 sectors of the research community at DeepMind and, of course, elsewhere who are dedicated to

1:26:01.600 --> 1:26:09.440
 thinking hard all day every day about that. But I guess I would say a positive side to this too,

1:26:09.440 --> 1:26:17.440
 which is to say, well, what would it mean to make human life better? And how can we imagine

1:26:17.440 --> 1:26:24.640
 learning systems doing that? And in talking to my colleagues about that, we reached the

1:26:24.640 --> 1:26:30.880
 initial conclusion that it's not sufficient to philosophize about that. You actually have to

1:26:30.880 --> 1:26:38.880
 take into account how humans actually work and what humans want and the difficulties of knowing

1:26:39.440 --> 1:26:45.760
 what humans want. And the difficulties that arise when humans want different things.

1:26:45.760 --> 1:26:53.520
 And so human agent interaction has become a quite intensive focus of my group lately.

1:26:54.880 --> 1:27:02.880
 If for no other reason that, in order to really address that issue in an adequate way,

1:27:02.880 --> 1:27:05.840
 you have to, I mean, psychology becomes part of the picture.

1:27:05.840 --> 1:27:12.480
 Yeah. And so there's a few elements there. So if you focus on solving, if you focus on the

1:27:12.480 --> 1:27:19.520
 if you focus on the robotics problem, let's say AGI without humans in the picture is you're missing

1:27:20.080 --> 1:27:25.120
 fundamentally the final step. When you do want to help human civilization, you eventually have

1:27:25.120 --> 1:27:31.920
 to interact with humans. And when you create a learning system, just as you said, that will

1:27:31.920 --> 1:27:39.440
 eventually have to interact with humans, the interaction itself has to become part of the

1:27:39.440 --> 1:27:45.200
 learning process. So you can't just watch, well, my sense is, it sounds like your sense is you

1:27:45.200 --> 1:27:50.080
 can't just watch humans to learn about humans. You have to also be part of the human world.

1:27:50.080 --> 1:27:56.080
 You have to interact with humans. Yeah, exactly. And I mean, then questions arise that start

1:27:56.080 --> 1:28:03.520
 imperceptibly, but inevitably to slip beyond the realm of engineering. So questions like,

1:28:03.520 --> 1:28:08.400
 if you have an agent that can do something that you can't do,

1:28:10.720 --> 1:28:19.200
 under what conditions do you want that agent to do it? So if I have a robot that can play

1:28:22.480 --> 1:28:30.000
 Beethoven sonatas better than any human in the sense that the sensitivity,

1:28:30.000 --> 1:28:36.800
 the expression is just beyond what any human, do I want to listen to that? Do I want to go to

1:28:36.800 --> 1:28:43.040
 a concert and hear a robot play? These aren't engineering questions. These are questions

1:28:43.040 --> 1:28:47.760
 about human preference and human culture. Psychology, bordering on philosophy.

1:28:48.880 --> 1:28:56.400
 And then you start asking, well, even if we knew the answer to that, is it our place as AI

1:28:56.400 --> 1:29:01.680
 engineers to build that into these agents? Probably the agents should interact with humans

1:29:03.440 --> 1:29:07.360
 beyond the population of AI engineers and figure out what those humans want.

1:29:08.640 --> 1:29:11.840
 And then when you start, I referred this the moment ago, but

1:29:12.720 --> 1:29:18.320
 even that becomes complicated, be quote, what if two humans want different things?

1:29:19.200 --> 1:29:23.680
 And you have only one agent that's able to interact with them and try to satisfy their

1:29:23.680 --> 1:29:32.480
 preferences, then you're into the realm of economics and social choice theory and even

1:29:32.480 --> 1:29:39.200
 politics. So there's a sense in which if you follow what we're doing to its logical conclusion,

1:29:39.920 --> 1:29:47.680
 then it goes beyond questions of engineering and technology and starts to shade in perceptibly

1:29:47.680 --> 1:29:55.680
 into questions about what kind of society do you want? And actually, once that dawned on me,

1:29:55.680 --> 1:30:02.160
 I actually felt, I don't know what the right word is, quite refreshed in my involvement

1:30:02.160 --> 1:30:08.320
 in AI research. It's almost like building this kind of stuff is going to lead us back to asking

1:30:08.320 --> 1:30:16.560
 really fundamental questions about what's the good life and who gets to decide.

1:30:16.560 --> 1:30:25.840
 And bringing in viewpoints from multiple sub communities to help us shape the way that we

1:30:25.840 --> 1:30:34.720
 live. It started making me feel like doing AI research in a fully responsible way

1:30:34.720 --> 1:30:47.600
 could potentially lead to a kind of cultural renewal. It's the way to understand human

1:30:47.600 --> 1:30:53.440
 beings at the individual, the societal level, and maybe come a way to answer all the human

1:30:53.440 --> 1:30:57.840
 questions of the meaning of life and all those kinds of things. Even if it doesn't give us a

1:30:57.840 --> 1:31:05.760
 way of answering those questions, it may force us back to thinking about them. And it might

1:31:05.760 --> 1:31:13.520
 restore a certain, I don't know, a certain depth to, or even, dare I say, spirituality to

1:31:14.640 --> 1:31:19.280
 the way that, to the world. Maybe that's too grandiose.

1:31:19.280 --> 1:31:28.160
 Well, I'm with you. I think AI will be the philosophy of the 21st century, the way which

1:31:28.160 --> 1:31:31.760
 will open the door. I think a lot of AI researchers are afraid to open that door

1:31:32.320 --> 1:31:39.360
 of exploring the beautiful richness of the human agent interaction, human AI interaction.

1:31:39.360 --> 1:31:43.520
 I'm really happy that somebody like you have opened that door.

1:31:43.520 --> 1:31:50.160
 And one thing I often think about is the usual schema for thinking about

1:31:52.720 --> 1:31:59.600
 human agent interaction is this kind of dystopian, oh, our robot overlords.

1:32:00.400 --> 1:32:05.760
 And again, I hasten to say AI safety is hugely important. And I'm not saying we

1:32:05.760 --> 1:32:10.000
 shouldn't be thinking about those risks. Totally on board for that. But there's it.

1:32:10.000 --> 1:32:21.680
 Having said that, what often follows for me is the thought that there's another

1:32:21.680 --> 1:32:29.680
 kind of narrative that might be relevant, which is when we think of humans gaining more and more

1:32:30.320 --> 1:32:37.440
 information about human life, the narrative there is usually that they gain more and more

1:32:37.440 --> 1:32:44.720
 wisdom and they get closer to enlightenment and they become more benevolent. The Buddha is

1:32:44.720 --> 1:32:51.280
 like that's a totally different narrative. And why isn't it the case that we imagine that the AI

1:32:51.280 --> 1:32:55.200
 systems that we're creating, they're going to figure out more and more about the way the world

1:32:55.200 --> 1:32:59.840
 works and the way that humans interact and they'll become beneficent. I'm not saying that will

1:32:59.840 --> 1:33:07.920
 happen. I don't honestly expect that to happen without some careful setting things up very

1:33:07.920 --> 1:33:14.240
 carefully. But it's another way things could go, right? And I would even push back on that. I

1:33:14.240 --> 1:33:24.080
 personally believe that the most trajectories, natural human trajectories will lead us towards

1:33:24.080 --> 1:33:30.800
 progress. So for me, there is a kind of sense that most trajectories in AI development will

1:33:30.800 --> 1:33:38.320
 lead us into trouble to me. And we over focus on the worst case. It's like in computer science,

1:33:38.320 --> 1:33:42.640
 theoretical computer science has been this focus on worst case analysis. There's something

1:33:42.640 --> 1:33:49.280
 appealing to our human mind at some lowest level to be good. We don't want to be eaten by the tiger,

1:33:49.280 --> 1:33:55.760
 I guess. So we want to do the worst case analysis. But the reality is that shouldn't stop us from

1:33:55.760 --> 1:34:01.360
 actually building out all the other trajectories, which are potentially leading to all the positive

1:34:01.360 --> 1:34:06.800
 worlds, all the, all the enlightenment, this book enlightenment now was even Panker and so on.

1:34:06.800 --> 1:34:12.160
 This is looking generally at human progress. And there's so many ways that human progress

1:34:12.160 --> 1:34:17.360
 can happen with AI. And I think you have to do that research. You have to do that work. You

1:34:17.360 --> 1:34:23.360
 have to do the, not just the AI safety work of the one worst case analysis, how do we prevent that,

1:34:23.360 --> 1:34:31.520
 but the actual tools and the glue and the mechanisms of human AI interaction that would

1:34:31.520 --> 1:34:36.320
 lead to all the positive actions that can go. It's a super exciting area, right?

1:34:36.320 --> 1:34:41.600
 Yeah. We should be spending, we should be spending a lot of our time saying what can go wrong.

1:34:41.600 --> 1:34:49.920
 I think it's harder to see that there's work to be done to bring into focus the question of what

1:34:50.640 --> 1:34:58.880
 it would look like for things to go right. That's not obvious. And we wouldn't be doing this if we

1:34:58.880 --> 1:35:06.160
 didn't have the sense there was huge potential. We're not doing this for no reason. We have a

1:35:06.160 --> 1:35:14.240
 sense that AGI would be a major boom to humanity. But I think it's worth starting now, even when

1:35:14.240 --> 1:35:20.320
 our technology is quite primitive, asking, well, exactly what would that mean? We can start now

1:35:20.320 --> 1:35:24.560
 with applications that are already going to make the world a better place, like solving protein

1:35:24.560 --> 1:35:30.560
 folding. I think this deep mind has gotten heavy into science applications lately, which I think

1:35:30.560 --> 1:35:37.760
 is a wonderful, wonderful move for us to be making. But when we think about AGI, when we think

1:35:37.760 --> 1:35:43.440
 about building fully intelligent agents that are going to be able to, in a sense, do whatever they

1:35:43.440 --> 1:35:50.880
 want, we should start thinking about what do we want them to want? What kind of world do we want

1:35:50.880 --> 1:35:56.880
 to live in? That's not an easy question. And I think we just need to start working on it.

1:35:56.880 --> 1:36:01.600
 And even on the path to sort of AGI, it doesn't have to be AGI, but just intelligent agents that

1:36:01.600 --> 1:36:07.120
 interact with us and help us enrich our own existence on social networks, for example, and

1:36:07.120 --> 1:36:10.640
 recommend our systems of various intelligence. There's so much interesting interaction that's

1:36:10.640 --> 1:36:18.880
 yet to be understood and studied. And how do you create, I mean, Twitter is struggling with this

1:36:18.880 --> 1:36:24.320
 very idea, how do you create AI systems that increase the quality and the health of a conversation?

1:36:24.320 --> 1:36:28.400
 For sure. That's a beautiful, beautiful human psychology question.

1:36:28.400 --> 1:36:37.520
 And how do you do that without deception being involved, without manipulation being involved,

1:36:39.200 --> 1:36:47.040
 maximizing human autonomy? And how do you make these choices in a democratic way? How do you,

1:36:47.040 --> 1:36:57.920
 how do we, again, I'm speaking for myself here, how do we face the fact that it's a small group

1:36:57.920 --> 1:37:06.640
 of people who have the skill set to build these kinds of systems. But what it means to make the

1:37:06.640 --> 1:37:14.880
 world a better place is something that we all have to be talking about. The world that we're

1:37:14.880 --> 1:37:20.160
 trying to make a better place includes a huge variety of different kinds of people.

1:37:20.160 --> 1:37:26.560
 Yeah. How do we cope with that? This is a problem that has been discussed in gory,

1:37:26.560 --> 1:37:33.840
 extensive detail in social choice theory. One thing I'm really enjoying about the recent

1:37:33.840 --> 1:37:38.480
 direction work has taken in some parts of my team is that, yeah, we're reading the AI literature,

1:37:38.480 --> 1:37:43.200
 we're reading the neuroscience literature, but we've also started reading economics and,

1:37:43.200 --> 1:37:46.800
 as I mentioned, social choice theory, even some political theory, because it turns out that

1:37:48.800 --> 1:37:57.120
 it all becomes relevant. It all becomes relevant. But at the same time, we've been trying not to

1:37:57.120 --> 1:38:02.320
 write philosophy papers, right? We've been trying not to write position papers. We're trying to

1:38:02.320 --> 1:38:07.840
 figure out ways of doing actual empirical research that kind of take the first small steps to

1:38:07.840 --> 1:38:14.720
 thinking about what it really means for humans with all of their complexity and contradiction and

1:38:16.000 --> 1:38:22.800
 paradox to be brought into contact with these AI systems in a way that

1:38:24.240 --> 1:38:27.680
 really makes the world a better place. And often reinforcement learning frameworks actually

1:38:27.680 --> 1:38:33.920
 kind of allow you to do that machine learning. That's the exciting thing about AI is it allows

1:38:33.920 --> 1:38:38.880
 you to reduce the unsolvable problem, philosophical problem into something more

1:38:39.680 --> 1:38:44.400
 concrete that you can get a hold of. Yeah, and it allows you to kind of define the problem in some

1:38:44.400 --> 1:38:52.560
 way that allows for growth in the system that's sort of, you know, you're not responsible for the

1:38:52.560 --> 1:38:57.920
 details, right? You say, this is generally what I want you to do, and then learning takes care of

1:38:57.920 --> 1:39:05.120
 the rest. Of course, the safety issues arise in that context. But I think also some of these

1:39:05.120 --> 1:39:10.880
 positive issues arise in that context. What would it mean for an AI system to really come to understand

1:39:10.880 --> 1:39:24.000
 what humans want? And with all of the subtleties of that, humans want help with certain things,

1:39:24.000 --> 1:39:29.680
 but they don't want everything done for them, right? There is part of the satisfaction that

1:39:29.680 --> 1:39:34.400
 humans get from life is in accomplishing things. So if there were devices around that did everything

1:39:34.400 --> 1:39:39.120
 for, you know, I often think of the movie Wally, right? That's like dystopian in a totally different

1:39:39.120 --> 1:39:44.000
 way. It's like, the machines are doing everything for us. That's not what we want it. You know,

1:39:44.000 --> 1:39:49.600
 anyway, I just, I find this, you know, this kind of opens up a whole landscape of research

1:39:49.600 --> 1:39:54.800
 that feels affirmative and exciting. Yeah. To me, it's one of the most exciting and it's

1:39:54.800 --> 1:39:59.280
 wide open. Yeah. We have to, because it's a cool paper, talk about dopamine.

1:39:59.280 --> 1:40:03.840
 Oh, yeah. Okay. So I can, we were going to, we were going to, I was going to give you a quick

1:40:04.480 --> 1:40:09.200
 summary. Yeah. It's a quick summary of what's the title of the paper?

1:40:10.560 --> 1:40:16.560
 I think we called it a distributional, a distributional code for value in dopamine

1:40:16.560 --> 1:40:23.040
 based reinforcement learning. Yes. So that's another project that grew out of

1:40:24.000 --> 1:40:31.120
 pure AI research. A number of people that DeepMind and a few other places had started working

1:40:32.160 --> 1:40:40.160
 on a new version of reinforcement learning, which was defined by taking something in traditional

1:40:40.160 --> 1:40:44.240
 reinforcement learning and just tweaking it. So the thing that they took from traditional

1:40:44.240 --> 1:40:50.080
 reinforcement learning was a value signal. So at the center of reinforcement learning,

1:40:50.080 --> 1:40:55.360
 at least most algorithms, is some representation of how well things are going. You're expected

1:40:56.320 --> 1:41:03.040
 cumulative future reward. And that's usually represented as a single number. So if you imagine

1:41:03.040 --> 1:41:09.200
 a gambler in a casino and the gambler's thinking, well, I have this probability of winning such

1:41:09.200 --> 1:41:12.080
 and such an amount of money and I have this probability of losing such and such an amount

1:41:12.080 --> 1:41:17.680
 of money, that situation would be represented as a single number, which is like the expected,

1:41:17.680 --> 1:41:24.000
 the weighted average of all those outcomes. And this new form of reinforcement learning

1:41:24.000 --> 1:41:29.120
 said, well, what if we generalize that to a distributional representation? So now we think

1:41:29.120 --> 1:41:33.440
 of the gambler as literally thinking, well, there's this probability that I'll win this

1:41:33.440 --> 1:41:36.400
 amount of money and there's this probability that I'll lose that amount of money. And we

1:41:36.400 --> 1:41:43.040
 don't reduce that to a single number. And it had been observed through experiments, through just

1:41:43.040 --> 1:41:50.960
 trying this out, that that kind of distributional representation really accelerated reinforcement

1:41:50.960 --> 1:41:56.560
 learning and led to better policies. What's your intuition about, so we're talking about rewards.

1:41:57.120 --> 1:42:01.040
 So what's your intuition? Why that is? Why does it depend? Well, it's kind of a

1:42:01.040 --> 1:42:06.240
 a surprising historical note, at least surprised me when I learned it, that

1:42:07.200 --> 1:42:11.520
 this had been tried out in a kind of heuristic way. People thought, well, gee, what would happen

1:42:11.520 --> 1:42:17.920
 if we tried and then it had this empirically, it had this striking effect. And it was only then

1:42:17.920 --> 1:42:24.480
 that people started thinking, well, gee, why? Why? Why? Why is this working? And that's led to a

1:42:24.480 --> 1:42:30.240
 series of studies just trying to figure out why it works, which is ongoing. But one thing that's

1:42:30.240 --> 1:42:35.120
 already clear from that research is that one reason that it helps is that it drives

1:42:36.480 --> 1:42:43.600
 richer representation learning. So if you imagine two situations that have the same

1:42:44.160 --> 1:42:50.640
 expected value, that the same kind of weighted average value, standard deep reinforcement learning

1:42:50.640 --> 1:42:55.520
 algorithms are going to take those two situations and kind of in terms of the way they're represented

1:42:55.520 --> 1:43:02.400
 internally, they're going to squeeze them together. Because the thing that you're trying to represent,

1:43:02.400 --> 1:43:06.080
 which is their expected value, is the same. So all the way through the system,

1:43:06.080 --> 1:43:11.440
 things are going to be mushed together. But what if those two situations actually have

1:43:11.440 --> 1:43:17.600
 different value distributions? They have the same average value, but they have different

1:43:17.600 --> 1:43:23.600
 distributions of value. In that situation, distributional learning will maintain the

1:43:23.600 --> 1:43:27.920
 distinction between these two things. So to make a long story short, distributional learning

1:43:27.920 --> 1:43:34.160
 can keep things separate in the internal representation that might otherwise be conflated

1:43:34.160 --> 1:43:39.600
 or squished together. And maintaining those distinctions can be useful in when the system is

1:43:39.600 --> 1:43:42.160
 now faced with some other task where the distinction is important.

1:43:43.120 --> 1:43:47.040
 If we look at optimistic and pessimistic dopamine neurons, so first of all,

1:43:47.040 --> 1:44:00.640
 what is dopamine? Why is this at all useful to think about in the artificial intelligence sense?

1:44:00.640 --> 1:44:06.320
 But what do we know about dopamine in the human brain? What is it? Why is it useful?

1:44:06.320 --> 1:44:10.160
 Why is it interesting? What does it have to do with the prefrontal cortex and learning in general?

1:44:10.160 --> 1:44:19.520
 Yeah. So, well, this is also a case where there's a huge amount of detail and debate.

1:44:19.520 --> 1:44:29.040
 But one currently prevailing idea is that the function of this neurotransmitter dopamine

1:44:29.040 --> 1:44:37.600
 resembles a particular component of standard reinforcement learning algorithms, which is

1:44:37.600 --> 1:44:44.080
 called the reward prediction error. So I was talking a moment ago about these value representations.

1:44:44.080 --> 1:44:49.600
 How do you learn them? How do you update them based on experience? Well, if you made some

1:44:49.600 --> 1:44:54.320
 prediction about a future reward, and then you get more reward than you were expecting,

1:44:54.320 --> 1:45:00.560
 then probably retrospectively, you want to go back and increase the value representation

1:45:00.560 --> 1:45:06.080
 that you attached to that earlier situation. If you got less reward than you were expecting,

1:45:06.080 --> 1:45:10.240
 you should probably decrement that estimate. And that's the process of temporal difference.

1:45:10.240 --> 1:45:14.160
 Exactly. This is the central mechanism of temporal difference learning, which is one of

1:45:14.160 --> 1:45:24.240
 several sort of the backbone of our armamentarium in RL. And this connection between the reward

1:45:24.240 --> 1:45:33.360
 prediction error and dopamine was made in the 1990s. And there's been a huge amount of research

1:45:33.360 --> 1:45:38.080
 that seems to back it up. Dopamine may be doing other things, but this is clearly,

1:45:38.880 --> 1:45:45.440
 at least roughly, one of the things that it's doing. But the usual idea was that dopamine was

1:45:45.440 --> 1:45:53.840
 representing these reward prediction errors, again, in this single number way, representing your

1:45:53.840 --> 1:46:00.160
 surprise with a single number. And in distributional reinforcement learning, this kind of new

1:46:00.160 --> 1:46:07.120
 elaboration of the standard approach, it's not only the value function that's represented as a

1:46:07.120 --> 1:46:16.000
 single number, it's also the reward prediction error. And so what happened was that Will Dabney,

1:46:16.000 --> 1:46:20.320
 one of my collaborators, who was one of the first people to work on distributional

1:46:20.320 --> 1:46:25.600
 temporal difference learning, talked to a guy in my group, Zeb Kurt Nelson,

1:46:25.600 --> 1:46:31.280
 who's a computational neuroscientist, and said, gee, is it possible that dopamine might be doing

1:46:31.280 --> 1:46:35.200
 something like this distributional coding thing? And they started looking at what was in the

1:46:35.200 --> 1:46:40.080
 literature, and then they brought me in, and we started talking to Naochida. And we came up with

1:46:40.080 --> 1:46:45.120
 some specific predictions about if the brain is using this kind of distributional coding,

1:46:45.120 --> 1:46:49.840
 then in the tasks that now has studied, you should see this, this, this, and this. And that's where

1:46:49.840 --> 1:46:55.120
 the paper came from. We enumerated a set of predictions, all of which ended up being fairly

1:46:55.120 --> 1:47:01.120
 clearly confirmed, and all of which leads to at least some initial indication that the brain

1:47:01.120 --> 1:47:05.840
 might be doing something like this distributional coding, that dopamine might be representing

1:47:05.840 --> 1:47:10.560
 surprise signals in a way that is not just collapsing everything to a single number,

1:47:10.560 --> 1:47:16.480
 but instead is kind of respecting the variety of future outcomes, if that makes sense.

1:47:16.480 --> 1:47:21.200
 So yeah, so that's showing, suggesting possibly that dopamine has a really interesting

1:47:21.200 --> 1:47:28.720
 representation scheme in the human brain for its reward signal. Exactly. That's fascinating.

1:47:29.520 --> 1:47:33.840
 That's another beautiful example of AI revealing something nice about neuroscience,

1:47:34.400 --> 1:47:38.960
 potentially suggesting possibilities. Well, you never know. So the minute you publish paper like

1:47:38.960 --> 1:47:44.160
 that, the next thing you think is, I hope that replicates. I hope we see that same thing in

1:47:44.160 --> 1:47:49.280
 other data sets. But of course, several labs now are doing the follow up experiments. So we'll

1:47:49.280 --> 1:47:55.920
 know soon. But it has been a lot of fun for us to take these ideas from AI and bring them into

1:47:55.920 --> 1:48:02.080
 neuroscience and see how far we can get. So we talked about it a little bit, but where do you see

1:48:02.080 --> 1:48:08.480
 the field of neuroscience and artificial intelligence heading broadly? What are the

1:48:09.280 --> 1:48:16.240
 possible exciting areas that you can see breakthroughs in the next, let's get crazy,

1:48:16.240 --> 1:48:26.800
 not just three or five years, but next 10, 20, 30 years that would make you excited and perhaps

1:48:26.800 --> 1:48:35.040
 you'd be part of. On the neuroscience side, there's a great deal of interest now in what's

1:48:35.040 --> 1:48:50.000
 going on in AI. At the same time, I feel like neuroscience, especially the part of neuroscience

1:48:50.000 --> 1:48:59.360
 that's focused on circuits and systems, really mechanism focused, there's been this explosion

1:48:59.360 --> 1:49:08.400
 in new technology. Up until recently, the experiments that have exploited this technology

1:49:10.720 --> 1:49:15.120
 have not involved a lot of interesting behavior. And this is for a variety of reasons,

1:49:16.320 --> 1:49:22.160
 one of which is in order to employ some of these technologies, if you're studying a mouse,

1:49:22.160 --> 1:49:25.920
 you have to head fix the mouse. In other words, you have to immobilize the mouse.

1:49:25.920 --> 1:49:31.360
 And so it's been tricky to come up with ways of eliciting interesting behavior from a mouse

1:49:31.360 --> 1:49:39.200
 that's restrained in this way, but people have begun to create very interesting solutions to

1:49:39.200 --> 1:49:46.080
 this, like virtual reality environments where the animal can move a trackball. And as people have

1:49:47.840 --> 1:49:51.280
 begun to explore what you can do with these technologies, I feel like more and more people

1:49:51.280 --> 1:49:58.160
 are asking, well, let's try to bring behavior into the picture. Let's try to reintroduce behavior,

1:49:58.160 --> 1:50:05.920
 which was supposed to be what this whole thing was about. And I'm hoping that those two trends,

1:50:06.640 --> 1:50:14.000
 the growing interest in behavior and the widespread interest in what's going on in AI,

1:50:14.000 --> 1:50:22.560
 will come together to kind of open a new chapter in neuroscience research, where there's a kind of

1:50:22.560 --> 1:50:29.600
 a rebirth of interest in the structure of behavior and its underlying substrates. But that research

1:50:29.600 --> 1:50:37.920
 is being informed by computational mechanisms that we're coming to understand in AI. If we

1:50:37.920 --> 1:50:43.520
 can do that, then we might be taking a step closer to this utopian future that we were talking about

1:50:43.520 --> 1:50:48.640
 earlier, where there's really no distinction between psychology and neuroscience. Neuroscience

1:50:48.640 --> 1:50:56.400
 is about studying the mechanisms that underlie whatever it is the brain is for, and what is

1:50:56.400 --> 1:51:02.960
 the brain for? It's for behavior. I feel like we could maybe take a step toward that now,

1:51:02.960 --> 1:51:09.920
 if people are motivated in the right way. You also asked about AI. So that was the neuroscience

1:51:09.920 --> 1:51:14.160
 question. You said neuroscience. That's right. And especially places like DeepMind are interested

1:51:14.160 --> 1:51:18.320
 in both branches. What about the engineering of intelligence systems?

1:51:20.640 --> 1:51:30.640
 I think one of the key challenges that a lot of people are seeing now in AI is to build systems

1:51:30.640 --> 1:51:39.840
 that have the kind of flexibility that humans have in two senses. One is that humans

1:51:39.840 --> 1:51:45.040
 can be good at many things. They're not just expert at one thing. And they're also flexible

1:51:45.040 --> 1:51:51.440
 in the sense that they can switch between things very easily, and they can pick up new things

1:51:51.440 --> 1:51:58.240
 very quickly, because they very ably see what a new task has in common with other things that

1:51:58.240 --> 1:52:10.560
 they've done. And that's something that our AI systems do not have. There are some people who

1:52:10.560 --> 1:52:16.960
 like to argue that deep learning and deep RL are simply wrong for getting that kind of flexibility.

1:52:16.960 --> 1:52:23.600
 I don't share that belief, but the simpler fact of the matter is we're not building things yet

1:52:23.600 --> 1:52:29.040
 that do have that kind of flexibility. And I think the attention of a large part of the AI

1:52:29.040 --> 1:52:34.480
 community is starting to pivot to that question. How do we get that? That's going to lead to

1:52:35.440 --> 1:52:42.640
 a focus on abstraction. It's going to lead to a focus on what in psychology we call cognitive

1:52:42.640 --> 1:52:48.000
 control, which is the ability to switch between tasks, the ability to quickly put together a

1:52:48.000 --> 1:52:53.200
 program of behavior that you've never executed before, but you know makes sense for a particular

1:52:53.200 --> 1:52:59.760
 set of demands. It's very closely related to what the prefrontal cortex does on the neuroscience

1:52:59.760 --> 1:53:06.400
 side. So I think it's going to be an interesting new chapter. So that's the reasoning side and

1:53:06.400 --> 1:53:12.320
 cognition side, but let me ask the over romanticized question. Do you think we'll ever engineer an

1:53:12.320 --> 1:53:20.880
 AGI system that we humans would be able to love and that would love us back? So have that level

1:53:20.880 --> 1:53:32.320
 and depth of connection? I love that question. And it relates closely to things that I've been

1:53:32.320 --> 1:53:39.040
 thinking about a lot lately in the context of this human AI research. There's social psychology

1:53:39.040 --> 1:53:46.880
 research in particular by Susan Fisk at Princeton in the department where I used to work,

1:53:46.880 --> 1:53:59.280
 where she dissects human attitudes toward other humans into a two dimensional scheme.

1:54:01.520 --> 1:54:07.920
 One dimension is about ability. How able, how capable is this other person?

1:54:10.000 --> 1:54:16.480
 But the other dimension is warmth. So you can imagine another person who's very skilled and

1:54:16.480 --> 1:54:24.720
 capable, but it's very cold. And you wouldn't really like highly, you might have some reservations

1:54:24.720 --> 1:54:31.280
 about that other person. But there's also a kind of reservation that we might have about

1:54:31.280 --> 1:54:38.160
 another person who elicits in us or displays a lot of human warmth, but is not good at getting

1:54:38.160 --> 1:54:45.680
 things done. The greatest esteem that we, we reserve our greatest esteem really for people who

1:54:45.680 --> 1:54:56.160
 are both highly capable and also quite warm. That's like the best of the best. This isn't a

1:54:56.160 --> 1:55:02.240
 normative statement I'm making. This is just an empirical statement. These are the two dimensions

1:55:02.240 --> 1:55:07.920
 that people seem to kind of like, along which people size other people up. And in AI research,

1:55:07.920 --> 1:55:12.960
 we really focus on this capability thing. We want our agents to be able to do stuff. This thing

1:55:12.960 --> 1:55:18.320
 can play go at a super human level. That's awesome. But that's only one dimension. What's the,

1:55:18.320 --> 1:55:25.440
 what about the other dimension? What would it mean for an AI system to be warm? And I don't know,

1:55:25.440 --> 1:55:30.720
 maybe there are easy solutions here like we can put a face on our AI systems. It's cute. It has big

1:55:30.720 --> 1:55:35.520
 ears. I mean, that's probably part of it. But I think it also has to do with a pattern of behavior,

1:55:35.520 --> 1:55:42.240
 a pattern of, you know, what would it mean for an AI system to display caring, compassionate

1:55:42.240 --> 1:55:48.720
 behavior in a way that actually made us feel like it was for real, that we didn't feel like it was

1:55:48.720 --> 1:55:54.960
 simulated. We didn't feel like we were being duped. To me, that, you know, people talk about the

1:55:54.960 --> 1:56:00.560
 Turing test or some, some descendant of it. I feel like that's the ultimate Turing test.

1:56:00.560 --> 1:56:06.400
 You know, is there, is there an AI system that can not only convince us that it knows how to

1:56:06.400 --> 1:56:13.360
 reason and it knows how to interpret language, but that we're comfortable saying, yeah, that AI

1:56:13.360 --> 1:56:18.560
 system is a good guy. You know, like, I mean, that on the warmth scale, whatever warmth is,

1:56:18.560 --> 1:56:26.800
 we kind of intuitively understand it. But we also want to be able to, yeah, we don't understand it

1:56:26.800 --> 1:56:33.200
 explicitly enough yet to be able to engineer it. Exactly. And that's, and that's an open scientific

1:56:33.200 --> 1:56:37.840
 question. You kind of alluded to it several times in the human AI interaction. That's a question

1:56:37.840 --> 1:56:43.680
 that should be studied. And probably one of the most important questions as we move to AI.

1:56:43.680 --> 1:56:49.280
 Humans are so good at it. Yeah. You know, it's not just weird. It's not just that we're born warm,

1:56:49.280 --> 1:56:54.080
 you know, like, I suppose some people are, are warmer than others, given, you know, whatever

1:56:54.080 --> 1:57:00.160
 genes they manage to inherit. But there's also, there's also, there are also learned skills

1:57:00.160 --> 1:57:05.280
 involved, right? I mean, there are ways of communicating to other people that you care,

1:57:06.320 --> 1:57:12.320
 that they matter to you, that you're enjoying interacting with them, right? And we learn these

1:57:12.320 --> 1:57:19.200
 skills from one another. And it's not out of the question that we could build engineered systems.

1:57:19.200 --> 1:57:24.240
 I think it's hopeless, as you say, that we could somehow hand design these sorts of, these sorts

1:57:24.240 --> 1:57:29.360
 of behaviors. But it's not out of the question that we could build systems that kind of we,

1:57:29.360 --> 1:57:35.920
 we, we instill in them something that sets them out in the right direction. So that they,

1:57:36.480 --> 1:57:42.720
 they end up learning what it is to interact with humans in a way that's gratifying to humans.

1:57:42.720 --> 1:57:49.920
 I mean, honestly, if that's not where we're headed, I want out.

1:57:52.080 --> 1:57:58.320
 I think it's exciting as a scientific problem, just as you described. I honestly don't see a

1:57:58.320 --> 1:58:04.800
 better way to end it than talking about warmth and love. And Matt, I don't think I've ever had such a

1:58:04.800 --> 1:58:09.200
 wonderful conversation where my questions were so bad, and your answers were so beautiful.

1:58:09.200 --> 1:58:14.000
 So I deeply appreciate it. I really enjoyed it. It's been very fun. As you know, as you can probably

1:58:14.000 --> 1:58:20.000
 tell, I, I really, you know, I, there's something I like about kind of thinking outside the box and

1:58:20.000 --> 1:58:24.400
 like, um, so it's good having fun with that. Awesome. Thanks so much for doing it.

1:58:25.440 --> 1:58:30.320
 Thanks for listening to this conversation with Matt Bopinik. And thank you to our sponsors,

1:58:30.320 --> 1:58:37.120
 The Jordan Harbinger Show and Magic Spoon Low Carb Keto Cereal. Please consider supporting

1:58:37.120 --> 1:58:43.600
 this podcast by going to Jordan Harbinger dot com slash Lex and also going to magic spoon dot com

1:58:43.600 --> 1:58:51.360
 slash Lex and using code Lex at checkout. Click the links, buy all the stuff. It's the best way

1:58:51.360 --> 1:58:57.680
 to support this podcast and the journey I'm on in my research and the startup. If you enjoy this

1:58:57.680 --> 1:59:03.680
 thing, subscribe on YouTube, review it with the five stars in a podcast, support on Patreon,

1:59:03.680 --> 1:59:10.960
 follow on Spotify, or connect with me on Twitter at Lex Friedman. Again, spelled miraculously

1:59:10.960 --> 1:59:18.160
 without the E just F R I D M A N. And now let me leave you with some words from neurologist

1:59:18.160 --> 1:59:24.320
 V S Sama Chandran. How can a three pound mass of jelly that you can hold in your palm,

1:59:24.960 --> 1:59:30.960
 imagine angels, contemplate the meaning of an infinity, even question its own place in cosmos,

1:59:30.960 --> 1:59:38.320
 especially awe inspiring is the fact that any single brain, including yours, is made up of atoms

1:59:38.320 --> 1:59:46.080
 that were forged in the hearts of countless far flung stars billions of years ago. These particles

1:59:46.080 --> 1:59:53.280
 drifted for eons and light years until gravity and change brought them together here now. These

1:59:53.280 --> 1:59:59.840
 atoms now form a conglomerate, your brain, they cannot only ponder the very stars they gave

1:59:59.840 --> 2:00:06.720
 at birth, but can also think about its own ability to think and wonder about its own ability to wonder.

2:00:07.600 --> 2:00:13.680
 With the arrival of humans, it has been said the universe has suddenly become conscious of itself.

2:00:13.680 --> 2:00:31.680
 This truly is the greatest mystery of all. Thank you for listening and hope to see you next time.

