WEBVTT

00:00.000 --> 00:05.060
 As part of MIT course 6S 099 Artificial General Intelligence, I've gotten the chance to sit

00:05.060 --> 00:06.740
 down with Max Tagmark.

00:06.740 --> 00:13.780
 He is a professor here at MIT, he's a physicist, spent a large part of his career studying the

00:13.780 --> 00:20.660
 mysteries of our cosmological universe, but he's also studied and delved into the beneficial

00:20.660 --> 00:25.860
 possibilities and the existential risks of artificial intelligence.

00:25.860 --> 00:32.220
 Amongst many other things, he's the cofounder of the Future of Life Institute, author of

00:32.220 --> 00:35.140
 two books, both of which I highly recommend.

00:35.140 --> 00:40.220
 First, our mathematical universe, second is Life 3.0.

00:40.220 --> 00:45.060
 He's truly an out of the box thinker and a fun personality, so I really enjoy talking

00:45.060 --> 00:46.060
 to him.

00:46.060 --> 00:49.500
 If you'd like to see more of these videos in the future, please subscribe and also click

00:49.500 --> 00:52.980
 the little bell icon to make sure you don't miss any videos.

00:52.980 --> 01:00.260
 Also, Twitter, LinkedIn, AGI.MIT.IDU, if you want to watch other lectures or conversations

01:00.260 --> 01:01.260
 like this one.

01:01.260 --> 01:07.980
 Better yet, go read Max's book, Life 3.0, chapter 7 on goals is my favorite.

01:07.980 --> 01:12.300
 It's really where philosophy and engineering come together and it opens with a quote by

01:12.300 --> 01:18.460
 Dostoevsky, the mystery of human existence lies not in just staying alive, but in finding

01:18.460 --> 01:20.300
 something to live for.

01:20.300 --> 01:27.100
 Lastly, I believe that every failure rewards us with an opportunity to learn, in that sense

01:27.100 --> 01:33.060
 I've been very fortunate to fail in so many new and exciting ways and this conversation

01:33.060 --> 01:34.060
 was no different.

01:34.060 --> 01:41.260
 I've learned about something called Radio Frequency Interference, RFI, look it up.

01:41.260 --> 01:45.500
 Apparently music and conversations from local radio stations can bleed into the audio that

01:45.500 --> 01:49.380
 you're recording in such a way that almost completely ruins that audio.

01:49.380 --> 01:52.460
 It's an exceptionally difficult sound source to remove.

01:52.460 --> 01:59.620
 So, I've gotten the opportunity to learn how to avoid RFI in the future during recording

01:59.620 --> 02:00.620
 sessions.

02:00.620 --> 02:06.260
 I've also gotten the opportunity to learn how to use Adobe Audition and iZotope RX6

02:06.260 --> 02:11.740
 to do some audio repair.

02:11.740 --> 02:14.940
 Of course, this is an exceptionally difficult noise to remove.

02:14.940 --> 02:20.380
 I am an engineer, I'm not an audio engineer, neither is anybody else in our group, but

02:20.380 --> 02:21.780
 we did our best.

02:21.780 --> 02:26.780
 Nevertheless, I thank you for your patience and I hope you're still able to enjoy this

02:26.780 --> 02:27.780
 conversation.

02:27.780 --> 02:31.460
 Do you think there's intelligent life out there in the universe?

02:31.460 --> 02:33.420
 Let's open up with an easy question.

02:33.420 --> 02:36.260
 I have a minority view here actually.

02:36.260 --> 02:41.180
 When I give public lectures, I often ask for show of hands who thinks there's intelligent

02:41.180 --> 02:47.060
 life out there somewhere else and almost everyone puts their hands up and when I ask why, they'll

02:47.060 --> 02:52.060
 be like, oh, there's so many galaxies out there, there's got to be.

02:52.060 --> 02:54.660
 But I'm a number nerd, right?

02:54.660 --> 02:59.180
 So when you look more carefully at it, it's not so clear at all.

02:59.180 --> 03:03.140
 When we talk about our universe, first of all, we don't mean all of space.

03:03.140 --> 03:05.900
 We actually mean, I don't know, you can throw me the universe if you want, it's behind you

03:05.900 --> 03:06.900
 there.

03:06.900 --> 03:14.540
 We simply mean the spherical region of space from which light has had time to reach us

03:14.540 --> 03:19.460
 so far during the 13.8 billion years since our big bang.

03:19.460 --> 03:23.020
 There's more space here, but this is what we call a universe because that's all we have

03:23.020 --> 03:24.140
 access to.

03:24.140 --> 03:31.220
 So is there intelligent life here that's gotten to the point of building telescopes and computers?

03:31.220 --> 03:39.500
 My guess is no, actually, the probability of it happening on any given planet is some

03:39.500 --> 03:42.860
 number we don't know what it is.

03:42.860 --> 03:49.340
 And what we do know is that the number can't be super high because there's over a billion

03:49.340 --> 03:54.780
 Earth like planets in the Milky Way galaxy alone, many of which are billions of years

03:54.780 --> 04:01.740
 older than Earth, and aside from some UFO believers, you know, there isn't much evidence

04:01.740 --> 04:05.740
 that any super advanced civilization has come here at all.

04:05.740 --> 04:08.700
 And so that's the famous Fermi paradox, right?

04:08.700 --> 04:13.620
 And then if you work the numbers, what you find is that if you have no clue what the

04:13.620 --> 04:18.500
 probability is of getting life on a given planet, so it could be 10 to the minus 10,

04:18.500 --> 04:23.620
 10 to the minus 20, or 10 to the minus two, or any power of 10 is sort of equally likely

04:23.620 --> 04:27.700
 if you want to be really open minded, that translates into it being equally likely that

04:27.700 --> 04:34.700
 our nearest neighbor is 10 to the 16 meters away, 10 to the 17 meters away, 10 to the

04:34.700 --> 04:35.700
 18.

04:35.700 --> 04:42.860
 Now, by the time you get much less than 10 to the 16 already, we pretty much know there

04:42.860 --> 04:46.220
 is nothing else that's close.

04:46.220 --> 04:49.740
 And when you get because it would have discovered us, they, yeah, they would have discovered

04:49.740 --> 04:53.540
 us longer or if they're really close, we would have probably noted some engineering projects

04:53.540 --> 04:54.540
 that they're doing.

04:54.540 --> 05:00.140
 And if it's beyond 10 to the 26 meters, that's already outside of here.

05:00.140 --> 05:06.340
 So my guess is actually that there are, we are the only life in here that's gotten the

05:06.340 --> 05:14.020
 point of building advanced tech, which I think is very, puts a lot of responsibility on our

05:14.020 --> 05:18.140
 shoulders, not screw up, you know, I think people who take for granted that it's okay

05:18.140 --> 05:23.300
 for us to screw up, have an accidental nuclear war or go extinct somehow because there's

05:23.300 --> 05:27.460
 a sort of Star Trek like situation out there where some other life forms are going to come

05:27.460 --> 05:30.380
 and bail us out and it doesn't matter so much.

05:30.380 --> 05:33.380
 I think they're leveling us into a false sense of security.

05:33.380 --> 05:37.540
 I think it's much more prudent to say, let's be really grateful for this amazing opportunity

05:37.540 --> 05:44.180
 we've had and make the best of it just in case it is down to us.

05:44.180 --> 05:50.220
 So from a physics perspective, do you think intelligent life, so it's unique from a sort

05:50.220 --> 05:55.860
 of statistical view of the size of the universe, but from the basic matter of the universe,

05:55.860 --> 06:00.100
 how difficult is it for intelligent life to come about with the kind of advanced tech

06:00.100 --> 06:06.300
 building life is implied in your statement that it's really difficult to create something

06:06.300 --> 06:07.620
 like a human species?

06:07.620 --> 06:14.740
 Well, I think what we know is that going from no life to having life that can do our level

06:14.740 --> 06:21.140
 of tech, there's some sort of to going beyond that than actually settling our whole universe

06:21.140 --> 06:22.300
 with life.

06:22.300 --> 06:30.700
 There's some road major roadblock there, which is some great filter as I just sometimes called

06:30.700 --> 06:37.180
 which, which tough to get through, it's either that that roadblock is either behind us or

06:37.180 --> 06:38.620
 in front of us.

06:38.620 --> 06:40.980
 I'm hoping very much that it's behind us.

06:40.980 --> 06:46.900
 I'm super excited every time we get a new report from NASA saying they failed to find

06:46.900 --> 06:53.260
 any life on Mars, because that suggests that the hard part, maybe it was getting the first

06:53.260 --> 06:59.540
 ribosome or some some very low level kind of stepping stone.

06:59.540 --> 07:03.620
 So they were home free because if that's true, then the future is really only limited by

07:03.620 --> 07:04.620
 our own imagination.

07:04.620 --> 07:11.460
 It would be much suckier if it turns out that this level of life is kind of a diamond dozen,

07:11.460 --> 07:12.780
 but maybe there's some other problem.

07:12.780 --> 07:17.220
 Like as soon as a civilization gets advanced technology within 100 years, they get into

07:17.220 --> 07:21.740
 some stupid fight with themselves and poof, you know, that would be a bummer.

07:21.740 --> 07:22.740
 Yeah.

07:22.740 --> 07:28.980
 So you've explored the mysteries of the universe, the cosmological universe, the one that's

07:28.980 --> 07:36.340
 between us today, I think you've also begun to explore the other universe, which is sort

07:36.340 --> 07:42.860
 of the mystery, the mysterious universe of the mind of intelligence, of intelligent life.

07:42.860 --> 07:48.260
 So is there a common thread between your interests or the way you think about space and intelligence?

07:48.260 --> 07:49.260
 Oh, yeah.

07:49.260 --> 07:57.700
 When I was a teenager, I was already very fascinated by the biggest questions and I felt that the

07:57.700 --> 08:03.660
 two biggest mysteries of all in science were our universe out there and our universe in

08:03.660 --> 08:04.660
 here.

08:04.660 --> 08:05.660
 Yeah.

08:05.660 --> 08:11.260
 So it's quite natural after having spent a quarter of a century on my career thinking

08:11.260 --> 08:12.260
 a lot about this one.

08:12.260 --> 08:15.980
 And now I'm indulging in the luxury of doing research on this one.

08:15.980 --> 08:17.660
 It's just so cool.

08:17.660 --> 08:25.260
 I feel the time is ripe now for you transparently deepening our understanding of this.

08:25.260 --> 08:26.420
 Just start exploring this one.

08:26.420 --> 08:32.500
 Yeah, because I think a lot of people view intelligence as something mysterious that

08:32.500 --> 08:38.340
 can only exist in biological organisms like us and therefore dismiss all talk about artificial

08:38.340 --> 08:41.260
 general intelligence is science fiction.

08:41.260 --> 08:47.260
 But from my perspective as a physicist, I am a blob of quirks and electrons moving around

08:47.260 --> 08:50.180
 in a certain pattern and processing information in certain ways.

08:50.180 --> 08:53.580
 And this is also a blob of quirks and electrons.

08:53.580 --> 08:57.860
 I'm not smarter than the water bottle because I'm made of different kind of quirks.

08:57.860 --> 09:02.220
 I'm made of up quirks and down quirks exact same kind as this.

09:02.220 --> 09:07.020
 It's a there's no secret sauce, I think in me, it's it's all about the pattern of the

09:07.020 --> 09:08.820
 information processing.

09:08.820 --> 09:16.020
 And this means that there's no law of physics saying that we can't create technology, which

09:16.020 --> 09:21.740
 can help us by being incredibly intelligent and help us crack mysteries that we couldn't.

09:21.740 --> 09:25.580
 In other words, I think we've really only seen the tip of the intelligence iceberg so

09:25.580 --> 09:26.580
 far.

09:26.580 --> 09:27.580
 Yeah.

09:27.580 --> 09:34.380
 So the perceptronium, yeah, so you coined this amazing term, it's a hypothetical state

09:34.380 --> 09:39.420
 of matter, sort of thinking from a physics perspective, what is the kind of matter that

09:39.420 --> 09:44.500
 can help as you're saying, subjective experience emerge, consciousness emerge.

09:44.500 --> 09:50.140
 So how do you think about consciousness from this physics perspective?

09:50.140 --> 09:51.980
 Very good question.

09:51.980 --> 10:03.060
 So, again, I think many people have underestimated our ability to make progress on this by convincing

10:03.060 --> 10:08.500
 themselves it's hopeless because somehow we're missing some ingredient that we need.

10:08.500 --> 10:13.020
 There's some new consciousness particle or whatever.

10:13.020 --> 10:19.660
 I happen to think that we're not missing anything and that it's not the interesting thing about

10:19.660 --> 10:25.900
 consciousness that gives us this amazing subjective experience of colors and sounds and emotions

10:25.900 --> 10:32.300
 and so on is rather something at the higher level about the patterns of information processing.

10:32.300 --> 10:38.300
 And that's why I like to think about this idea of perceptronium.

10:38.300 --> 10:44.220
 What does it mean for an arbitrary physical system to be conscious in terms of what its

10:44.220 --> 10:47.100
 particles are doing or its information is doing?

10:47.100 --> 10:52.300
 I hate carbon chauvinism, this attitude, you have to be made of carbon atoms to be smart

10:52.300 --> 10:53.300
 or conscious.

10:53.300 --> 10:58.180
 So something about the information processing that this kind of matter performs.

10:58.180 --> 11:02.700
 Yeah, and you can see I have my favorite equations here describing various fundamental

11:02.700 --> 11:04.660
 aspects of the world.

11:04.660 --> 11:09.620
 I think one day, maybe someone who's watching this will come up with the equations that

11:09.620 --> 11:12.140
 information processing has to satisfy to be conscious.

11:12.140 --> 11:19.580
 And I'm quite convinced there is big discovery to be made there because let's face it, we

11:19.580 --> 11:25.900
 know that some information processing is conscious because we are conscious.

11:25.900 --> 11:28.980
 But we also know that a lot of information processing is not conscious.

11:28.980 --> 11:32.980
 Most of the information processing happening in your brain right now is not conscious.

11:32.980 --> 11:38.380
 There are like 10 megabytes per second coming in even just through your visual system.

11:38.380 --> 11:42.940
 You're not conscious about your heartbeat regulation or most things.

11:42.940 --> 11:47.300
 Even if I just ask you to read what it says here, you look at it and then, oh, now you

11:47.300 --> 11:48.300
 know what it said.

11:48.300 --> 11:51.820
 But you're not aware of how the computation actually happened.

11:51.820 --> 11:57.020
 Your consciousness is like the CEO that got an email at the end with the final answer.

11:57.020 --> 12:01.140
 So what is it that makes a difference?

12:01.140 --> 12:06.620
 I think that's both a great science mystery, we're actually studying it a little bit in

12:06.620 --> 12:12.260
 my lab here at MIT, but I also think it's a really urgent question to answer.

12:12.260 --> 12:16.460
 For starters, I mean, if you're an emergency room doctor and you have an unresponsive patient

12:16.460 --> 12:24.180
 coming in, wouldn't it be great if in addition to having a CT scanner, you had a conscious

12:24.180 --> 12:30.780
 scanner that could figure out whether this person is actually having locked in syndrome

12:30.780 --> 12:33.580
 or is actually comatose.

12:33.580 --> 12:40.740
 And in the future, imagine if we build robots or the machine that we can have really good

12:40.740 --> 12:45.100
 conversations with, I think it's very likely to happen, right?

12:45.100 --> 12:50.020
 Wouldn't you want to know if your home helper robot is actually experiencing anything or

12:50.020 --> 12:52.980
 just like a zombie?

12:52.980 --> 12:53.980
 Would you prefer it?

12:53.980 --> 12:54.980
 What would you prefer?

12:54.980 --> 12:57.820
 Would you prefer that it's actually unconscious so that you don't have to feel guilty about

12:57.820 --> 12:59.980
 switching it off or giving boring chores?

12:59.980 --> 13:02.380
 What would you prefer?

13:02.380 --> 13:09.780
 Well, certainly we would prefer, I would prefer the appearance of consciousness, but the question

13:09.780 --> 13:15.300
 is whether the appearance of consciousness is different than consciousness itself.

13:15.300 --> 13:21.420
 And sort of ask that as a question, do you think we need to understand what consciousness

13:21.420 --> 13:28.420
 is, solve the hard problem of consciousness in order to build something like an AGI system?

13:28.420 --> 13:29.420
 No.

13:29.420 --> 13:31.140
 I don't think that.

13:31.140 --> 13:36.220
 I think we will probably be able to build things even if we don't answer that question.

13:36.220 --> 13:41.100
 But if we want to make sure that what happens is a good thing, we better solve it first.

13:41.100 --> 13:47.220
 So it's a wonderful controversy you're raising there, where you have basically three points

13:47.220 --> 13:50.220
 of view about the hard problem.

13:50.220 --> 13:55.060
 There are two different points of view that both conclude that the hard problem of consciousness

13:55.060 --> 13:56.060
 is BS.

13:56.060 --> 14:01.100
 On one hand, you have some people like Daniel Dennett who say that consciousness is just

14:01.100 --> 14:05.140
 BS because consciousness is the same thing as intelligence.

14:05.140 --> 14:06.580
 There's no difference.

14:06.580 --> 14:13.620
 So anything which acts conscious is conscious, just like we are.

14:13.620 --> 14:18.820
 And then there are also a lot of people, including many top AI researchers I know, who say, oh,

14:18.820 --> 14:22.820
 consciousness is just bullshit because of course machines can never be conscious.

14:22.820 --> 14:28.020
 They're always going to skiddy zombies, never have to feel guilty about how you treat them.

14:28.020 --> 14:35.380
 And then there's a third group of people, including Giulio Tononi, for example, and another, and

14:35.380 --> 14:40.020
 Gustav Koch and a number of others, I would put myself on this middle camp who say that

14:40.020 --> 14:44.260
 actually some information processing is conscious and some is not.

14:44.260 --> 14:49.380
 So let's find the equation which can be used to determine which it is.

14:49.380 --> 14:53.980
 And I think we've just been a little bit lazy kind of running away from this problem for

14:53.980 --> 14:55.100
 a long time.

14:55.100 --> 15:01.940
 It's been almost taboo to even mention the C word in a lot of circles because, but we

15:01.940 --> 15:03.700
 should stop making excuses.

15:03.700 --> 15:10.940
 This is a science question and there are ways we can even test any theory that makes predictions

15:10.940 --> 15:12.140
 for this.

15:12.140 --> 15:16.060
 And coming back to this helper robot, I mean, so you said you would want your helper robot

15:16.060 --> 15:21.340
 to certainly act conscious and treat you, like have conversations with you and stuff.

15:21.340 --> 15:24.860
 But wouldn't you, would you feel a little bit creeped out if you realized that it was

15:24.860 --> 15:31.700
 just a glossed up tape recorder, you know, that was just zombie and it's a faking emotion?

15:31.700 --> 15:37.220
 Would you prefer that it actually had an experience or would you prefer that it's actually not

15:37.220 --> 15:42.300
 experiencing anything so you feel, you don't have to feel guilty about what you do to it?

15:42.300 --> 15:46.580
 It's such a difficult question because, you know, it's like when you're in a relationship

15:46.580 --> 15:49.860
 and you say, well, I love you and the other person said I love you back.

15:49.860 --> 15:53.860
 It's like asking, well, do they really love you back or are they just saying they love

15:53.860 --> 15:54.860
 you back?

15:54.860 --> 15:59.620
 Don't you really want them to actually love you?

15:59.620 --> 16:08.100
 It's hard to, it's hard to really know the difference between everything seeming like

16:08.100 --> 16:14.820
 there's consciousness present, there's intelligence present, there's affection, passion, love,

16:14.820 --> 16:16.180
 and it actually being there.

16:16.180 --> 16:17.180
 I'm not sure.

16:17.180 --> 16:18.180
 Do you have...

16:18.180 --> 16:19.180
 Can I ask you a question about this?

16:19.180 --> 16:20.180
 Yes.

16:20.180 --> 16:21.180
 To make it a bit more pointed.

16:21.180 --> 16:23.140
 So Mass General Hospital is right across the river, right?

16:23.140 --> 16:29.180
 Suppose you're going in for a medical procedure and they're like, you know, for anesthesia

16:29.180 --> 16:32.180
 what we're going to do is we're going to give you muscle relaxance so you won't be able

16:32.180 --> 16:36.140
 to move and you're going to feel excruciating pain during the whole surgery but you won't

16:36.140 --> 16:37.660
 be able to do anything about it.

16:37.660 --> 16:42.020
 But then we're going to give you this drug that erases your memory of it.

16:42.020 --> 16:45.420
 Would you be cool about that?

16:45.420 --> 16:51.100
 What's the difference that you're conscious about it or not if there's no behavioral change,

16:51.100 --> 16:52.100
 right?

16:52.100 --> 16:53.100
 Right.

16:53.100 --> 16:55.220
 And that's a really clear way to put it.

16:55.220 --> 17:01.100
 Yeah, it feels like in that sense, experiencing it is a valuable quality.

17:01.100 --> 17:09.140
 So actually being able to have subjective experiences, at least in that case, is valuable.

17:09.140 --> 17:14.060
 And I think we humans have a little bit of a bad track record also of making these self

17:14.060 --> 17:17.940
 serving arguments that other entities aren't conscious.

17:17.940 --> 17:20.700
 You know, people often say, oh, these animals can't feel pain.

17:20.700 --> 17:21.700
 Right.

17:21.700 --> 17:25.580
 It's okay to boil lobsters because we asked them if it hurt and they didn't say anything.

17:25.580 --> 17:29.180
 And now there was just a paper out saying lobsters did do feel pain when you boil them

17:29.180 --> 17:31.180
 and they're banning it in Switzerland.

17:31.180 --> 17:36.300
 And we did this with slaves too often and said, oh, they don't mind.

17:36.300 --> 17:41.180
 They don't maybe aren't conscious or women don't have souls or whatever.

17:41.180 --> 17:46.540
 So I'm a little bit nervous when I hear people just take as an axiom that machines can't

17:46.540 --> 17:48.900
 have experience ever.

17:48.900 --> 17:52.500
 I think this is just a really fascinating science question is what it is.

17:52.500 --> 17:57.420
 Let's research it and try to figure out what it is that makes the difference between unconscious

17:57.420 --> 18:01.220
 intelligent behavior and conscious intelligent behavior.

18:01.220 --> 18:07.140
 So in terms of, so if you think of a Boston Dynamics human or robot being sort of with

18:07.140 --> 18:13.420
 a broom being pushed around, it starts pushing on a consciousness question.

18:13.420 --> 18:20.060
 So let me ask, do you think an AGI system, like a few neuroscientists believe needs to

18:20.060 --> 18:25.860
 have a physical embodiment, needs to have a body or something like a body?

18:25.860 --> 18:28.340
 No, I don't think so.

18:28.340 --> 18:30.620
 You mean to have a conscious experience?

18:30.620 --> 18:33.140
 To have consciousness.

18:33.140 --> 18:37.860
 I do think it helps a lot to have a physical embodiment to learn the kind of things about

18:37.860 --> 18:42.820
 the world that are important to us humans for sure.

18:42.820 --> 18:47.460
 But I don't think the physical embodiment is necessary after you've learned it.

18:47.460 --> 18:48.860
 Just have the experience.

18:48.860 --> 18:51.500
 Think about it when you're dreaming, right?

18:51.500 --> 18:55.500
 Your eyes are closed, you're not getting any sensory input, you're not behaving or moving

18:55.500 --> 18:59.780
 in any way, but there's still an experience there, right?

18:59.780 --> 19:03.220
 And so clearly the experience that you have when you see something cool in your dreams

19:03.220 --> 19:08.660
 isn't coming from your eyes, it's just the information processing itself in your brain,

19:08.660 --> 19:11.100
 which is that experience, right?

19:11.100 --> 19:16.660
 But if I put it another way, I'll say because it comes from neuroscience is the reason you

19:16.660 --> 19:24.620
 want to have a body and a physical, something like a physical system is because you want

19:24.620 --> 19:27.100
 to be able to preserve something.

19:27.100 --> 19:35.740
 In order to have a self, you could argue, you'd need to have some kind of embodiment

19:35.740 --> 19:38.180
 of self to want to preserve.

19:38.180 --> 19:45.940
 Well, now we're getting a little bit anthropomorphic, anthropomorphizing things, maybe talking about

19:45.940 --> 19:47.820
 self preservation instincts.

19:47.820 --> 19:50.700
 We are evolved organisms, right?

19:50.700 --> 19:57.020
 So Darwinian evolution endowed us and other evolved organisms with self preservation instinct

19:57.020 --> 20:03.100
 because those that didn't have those self preservation genes got cleaned out of the gene pool.

20:03.100 --> 20:09.180
 But if you build an artificial general intelligence, the mind space that you can design is much,

20:09.180 --> 20:14.500
 much larger than just a specific subset of minds that can evolve that have.

20:14.500 --> 20:19.260
 So an AGI mind doesn't necessarily have to have any self preservation instinct.

20:19.260 --> 20:24.100
 It also doesn't necessarily have to be so individualistic as us.

20:24.100 --> 20:28.140
 Like imagine if you could just, first of all, we're also very afraid of death, you know,

20:28.140 --> 20:32.180
 as opposed to you could back yourself up every five minutes and then your airplane is about

20:32.180 --> 20:33.180
 to crash.

20:33.180 --> 20:37.340
 You're like, shucks, I'm just, I'm going to lose the last five minutes of experiences

20:37.340 --> 20:41.580
 since my last cloud backup, dang, you know, it's not as big a deal.

20:41.580 --> 20:47.380
 Or if we could just copy experiences between our minds easily, like which we could easily

20:47.380 --> 20:50.620
 do if we were silicon based, right?

20:50.620 --> 20:55.860
 Then maybe we would feel a little bit more like a hive mind, actually, that maybe it's

20:55.860 --> 21:01.220
 the, so, so there's, so I don't think we should take for granted at all that AGI will have

21:01.220 --> 21:06.820
 to have any of those sort of competitive as alpha male instincts.

21:06.820 --> 21:07.820
 Right.

21:07.820 --> 21:12.820
 On the other hand, you know, this is really interesting because I think some people go

21:12.820 --> 21:17.900
 too far and say, of course, we don't have to have any concerns either that advanced

21:17.900 --> 21:22.700
 AI will have those instincts because we can build anything we want.

21:22.700 --> 21:27.420
 That there's, there's a very nice set of arguments going back to Steve Omohandro and

21:27.420 --> 21:32.900
 Nick Bostrom and others just pointing out that when we build machines, we normally build

21:32.900 --> 21:37.700
 them with some kind of goal, you know, win this chess game, drive this car safely or

21:37.700 --> 21:38.700
 whatever.

21:38.700 --> 21:42.540
 And as soon as you put in a goal into machine, especially if it's kind of open ended goal

21:42.540 --> 21:48.460
 and the machine is very intelligent, it'll break that down into a bunch of sub goals.

21:48.460 --> 21:53.500
 And one of those goals will almost always be self preservation because if it breaks

21:53.500 --> 21:56.140
 or dies in the process, it's not going to accomplish the goal, right?

21:56.140 --> 21:59.540
 Like, suppose you just build a little, you have a little robot and you tell it to go

21:59.540 --> 22:05.460
 down the store market here and, and get you some food, make you cook your Italian dinner,

22:05.460 --> 22:09.540
 you know, and then someone mugs it and tries to break it on the way.

22:09.540 --> 22:15.380
 That robot has an incentive to not get destroyed and defend itself for a runaway because otherwise

22:15.380 --> 22:17.780
 it's going to fail and cooking your dinner.

22:17.780 --> 22:22.940
 It's not afraid of death, but it really wants to complete the dinner cooking goal.

22:22.940 --> 22:24.780
 So it will have a self preservation instinct.

22:24.780 --> 22:26.820
 It will continue being a functional agent.

22:26.820 --> 22:27.820
 Yeah.

22:27.820 --> 22:35.860
 And, and, and similarly, if you give any kind of more ambitious goal to an AGI, it's very

22:35.860 --> 22:39.940
 likely they want to acquire more resources so it can do that better.

22:39.940 --> 22:44.500
 And it's exactly from those sort of sub goals that we might not have intended that some

22:44.500 --> 22:50.740
 of the concerns about AGI safety come, you give it some goal that seems completely harmless.

22:50.740 --> 22:55.540
 And then before you realize it, it's also trying to do these other things which you

22:55.540 --> 22:59.220
 didn't want it to do and it's maybe smarter than us.

22:59.220 --> 23:08.220
 So, so, and let me pause just because I am in a very kind of human centric way, see fear

23:08.220 --> 23:11.900
 of death as a valuable motivator.

23:11.900 --> 23:17.220
 So you don't think you think that's an artifact of evolution.

23:17.220 --> 23:21.980
 So that's the kind of mind space evolution created that we're sort of almost obsessed

23:21.980 --> 23:22.980
 about self preservation.

23:22.980 --> 23:23.980
 Yeah.

23:23.980 --> 23:29.500
 Some kind of genetic well, you don't think that's necessary to be afraid of death.

23:29.500 --> 23:34.980
 So not just a kind of sub goal of self preservation just so you can keep doing the thing, but

23:34.980 --> 23:42.980
 more fundamentally sort of have the finite thing like this ends for you at some point.

23:42.980 --> 23:43.980
 Interesting.

23:43.980 --> 23:47.500
 Do I think it's necessary for what precisely?

23:47.500 --> 23:51.020
 For intelligence, but also for consciousness.

23:51.020 --> 23:58.220
 So for those for both, do you think really like a finite death and the fear of it is

23:58.220 --> 24:01.020
 important?

24:01.020 --> 24:06.980
 So before I can answer, before we can agree on whether it's necessary for intelligence

24:06.980 --> 24:10.660
 or for consciousness, we should be clear on how we define those two words because a lot

24:10.660 --> 24:13.340
 are really smart people define them in very different ways.

24:13.340 --> 24:18.500
 I was in this on this panel with AI experts and they couldn't, they couldn't agree on

24:18.500 --> 24:20.180
 how to define intelligence even.

24:20.180 --> 24:24.860
 So I define intelligence simply as the ability to accomplish complex goals.

24:24.860 --> 24:30.740
 I like your broad definition because again, I don't want to be a carbon chauvinist.

24:30.740 --> 24:36.580
 And in that case, no, certainly it doesn't require fear of death.

24:36.580 --> 24:40.100
 I would say AlphaGo AlphaZero is quite intelligent.

24:40.100 --> 24:44.260
 I don't think AlphaZero has any fear of being turned off because it doesn't understand the

24:44.260 --> 24:52.180
 concept of even and similarly consciousness, I mean, you can certainly imagine a very simple

24:52.180 --> 24:57.660
 kind of experience if certain plants have any kind of experience, I don't think they're

24:57.660 --> 25:00.940
 very afraid of dying or there's nothing they can do about it anyway much.

25:00.940 --> 25:08.420
 So there wasn't that much value and but more seriously, I think if you ask not just about

25:08.420 --> 25:15.460
 being conscious, but maybe having what you would, we might call an exciting life for

25:15.460 --> 25:23.300
 you for your passion and really appreciate the things, maybe there, somehow, maybe there

25:23.300 --> 25:29.180
 perhaps it does help having a backdrop that, hey, it's finite, you know, let's make the

25:29.180 --> 25:31.380
 most of this, let's live to the fullest.

25:31.380 --> 25:36.220
 So if you knew you were going to just live forever, do you think you would change your

25:36.220 --> 25:40.500
 career? Yeah, I mean, in some perspective, it would

25:40.500 --> 25:44.020
 be an incredibly boring life living forever.

25:44.020 --> 25:49.740
 So in the sort of loose, subjective terms that you said of something exciting and something

25:49.740 --> 25:55.180
 in this that other humans would understand, I think, is yeah, it seems that the finiteness

25:55.180 --> 25:56.660
 of it is important.

25:56.660 --> 26:02.420
 Well, the good news I have for you then is based on what we understand about cosmology,

26:02.420 --> 26:10.460
 things in our universe is probably finite, although big crunch or big or big, what's

26:10.460 --> 26:11.460
 the extent of the infinite?

26:11.460 --> 26:16.820
 Yeah, we could have a big chill or a big crunch or a big rip or death, the big snap or death

26:16.820 --> 26:17.820
 bubbles.

26:17.820 --> 26:20.140
 All of them are more than a billion years away.

26:20.140 --> 26:29.500
 So we should we certainly have vastly more time than our ancestors thought, but still

26:29.500 --> 26:35.580
 pretty hard to squeeze in an infinite number of compute cycles, even though there are some

26:35.580 --> 26:37.820
 loopholes that just might be possible.

26:37.820 --> 26:44.620
 But I think, you know, some people like to say that you should live as if you're about

26:44.620 --> 26:48.100
 to you're going to die in five years or so, and that's sort of optimal.

26:48.100 --> 26:54.740
 Maybe it's a good as some we should build our civilization as if it's all finite to

26:54.740 --> 26:55.740
 be on the safe side.

26:55.740 --> 27:02.020
 Right, exactly. So you mentioned in defining intelligence as the ability to solve complex

27:02.020 --> 27:03.020
 goals.

27:03.020 --> 27:04.940
 So where would you draw a line?

27:04.940 --> 27:10.940
 How would you try to define human level intelligence and super human level intelligence?

27:10.940 --> 27:13.380
 Where is consciousness part of that definition?

27:13.380 --> 27:16.860
 No, consciousness does not come into this definition.

27:16.860 --> 27:21.580
 So so I think of intelligence as it's a spectrum, but there are very many different kinds of

27:21.580 --> 27:22.580
 goals you can have.

27:22.580 --> 27:27.140
 You have a goal to be a good chess player, a good goal player, a good car driver, a good

27:27.140 --> 27:31.260
 investor, good poet, etc.

27:31.260 --> 27:35.740
 So intelligence that bind by its very nature, isn't something you can measure, but it's

27:35.740 --> 27:39.900
 one number, some overall goodness, no, no, there are some people who are more better

27:39.900 --> 27:42.540
 at this, some people are better at that.

27:42.540 --> 27:48.380
 Right now we have machines that are much better than us at some very narrow tasks like multiplying

27:48.380 --> 27:57.620
 large numbers fast, memorizing large databases, playing chess, playing go, soon driving cars.

27:57.620 --> 28:03.340
 But there's still no machine that can match a human child in general intelligence.

28:03.340 --> 28:08.420
 But artificial general intelligence, AGI, the name of your course, of course, that

28:08.420 --> 28:16.460
 is by its very definition, the quest to build a machine that can do everything as well as

28:16.460 --> 28:17.460
 we can.

28:17.460 --> 28:24.060
 Up to the old Holy Grail of AI from back to its inception in the 60s.

28:24.060 --> 28:27.500
 If that ever happens, of course, I think it's going to be the biggest transition in the

28:27.500 --> 28:33.860
 history of life on Earth, but it doesn't necessarily have to wait the big impact until machines

28:33.860 --> 28:35.780
 are better than us at knitting.

28:35.780 --> 28:41.940
 The really big change doesn't come exactly at the moment they're better than us at everything.

28:41.940 --> 28:45.820
 The really big change comes, first, their big change is when they start becoming better

28:45.820 --> 28:51.140
 at us at doing most of the jobs that we do, because that takes away much of the demand

28:51.140 --> 28:53.380
 for human labor.

28:53.380 --> 29:01.300
 And then the really warping change comes when they become better than us at AI research.

29:01.300 --> 29:07.900
 Because right now, the time scale of AI research is limited by the human research and development

29:07.900 --> 29:14.100
 cycle of years, typically, along the take from one release of some software or iPhone

29:14.100 --> 29:16.300
 or whatever to the next.

29:16.300 --> 29:25.820
 But once Google can replace 40,000 engineers by 40,000 equivalent pieces of software or

29:25.820 --> 29:29.660
 whatever, then there's no reason that has to be years.

29:29.660 --> 29:32.020
 It can be, in principle, much faster.

29:32.020 --> 29:38.900
 And the time scale of future progress in AI and all of science and technology will be

29:38.900 --> 29:40.980
 driven by machines, not humans.

29:40.980 --> 29:49.660
 So it's this simple point, which gives right this incredibly fun controversy about whether

29:49.660 --> 29:54.540
 there can be intelligence explosion, so called singularity, as Werner Winge called it.

29:54.540 --> 30:00.060
 The idea, as articulated by I.J. Good, is obviously way back fifties, but you can see

30:00.060 --> 30:07.220
 Alan Turing and others thought about it even earlier.

30:07.220 --> 30:12.980
 You asked me what exactly what I define human level intelligence.

30:12.980 --> 30:18.540
 So the glib answer is just to say something which is better than us at all cognitive tasks

30:18.540 --> 30:21.980
 or better than any human at all cognitive tasks.

30:21.980 --> 30:25.900
 But the really interesting bar, I think, goes a little bit lower than that, actually.

30:25.900 --> 30:33.260
 It's when they're better than us at AI programming and general learning so that they can, if

30:33.260 --> 30:37.340
 they want to, get better than us at anything by just starting out.

30:37.340 --> 30:43.100
 So there better is a key word and better is towards this kind of spectrum of the complexity

30:43.100 --> 30:45.740
 of goals it's able to accomplish.

30:45.740 --> 30:53.060
 So another way to, and that's certainly a very clear definition of human love.

30:53.060 --> 30:56.300
 So there's, it's almost like a sea that's rising, you can do more and more and more

30:56.300 --> 30:57.300
 things.

30:57.300 --> 30:59.900
 It's actually a graphic that you show, it's really nice way to put it.

30:59.900 --> 31:04.340
 So there's some peaks and there's an ocean level elevating and you solve more and more

31:04.340 --> 31:05.340
 problems.

31:05.340 --> 31:09.220
 But, you know, just kind of to take a pause and we took a bunch of questions and a lot

31:09.220 --> 31:14.380
 of social networks and a bunch of people asked a sort of a slightly different direction

31:14.380 --> 31:22.260
 on creativity and on things that perhaps aren't a peak.

31:22.260 --> 31:28.620
 It's, you know, human beings are flawed and perhaps better means having being having contradiction

31:28.620 --> 31:30.260
 being flawed in some way.

31:30.260 --> 31:34.980
 So let me sort of, yeah, start and start easy, first of all.

31:34.980 --> 31:36.620
 So you have a lot of cool equations.

31:36.620 --> 31:39.660
 Let me ask, what's your favorite equation, first of all?

31:39.660 --> 31:43.580
 I know they're all like your children, but which one is that?

31:43.580 --> 31:49.060
 This is the Shreddinger equation, it's the master key of quantum mechanics of the micro

31:49.060 --> 31:50.060
 world.

31:50.060 --> 31:55.340
 So this equation can take everything to do with atoms and all the fuels and all the

31:55.340 --> 32:04.020
 way up toâ€¦ Yeah, so, okay, so quantum mechanics is certainly a beautiful mysterious formulation

32:04.020 --> 32:05.020
 of our world.

32:05.020 --> 32:10.740
 So I'd like to sort of ask you, just as an example, it perhaps doesn't have the same

32:10.740 --> 32:17.420
 beauty as physics does, but in mathematics abstract, the Andrew Wiles who proved the

32:17.420 --> 32:19.460
 Fermat's last theory.

32:19.460 --> 32:24.180
 So he just saw this recently and it kind of caught my eye a little bit.

32:24.180 --> 32:27.980
 This is 358 years after it was conjectured.

32:27.980 --> 32:32.940
 So this very simple formulation, everybody tried to prove it, everybody failed.

32:32.940 --> 32:38.820
 And so here's this guy comes along and eventually proves it and then fails to prove it and then

32:38.820 --> 32:41.340
 proves it again in 94.

32:41.340 --> 32:45.940
 And he said like the moment when everything connected into place, in an interview he said

32:45.940 --> 32:47.980
 it was so indescribably beautiful.

32:47.980 --> 32:53.580
 That moment when you finally realize the connecting piece of two conjectures, he said it was so

32:53.580 --> 32:56.940
 indescribably beautiful, it was so simple and so elegant.

32:56.940 --> 33:01.540
 I couldn't understand how I'd missed it and I just stared at it in disbelief for 20

33:01.540 --> 33:02.540
 minutes.

33:02.540 --> 33:08.100
 Then during the day I walked around the department and I keep coming back to my desk looking

33:08.100 --> 33:09.820
 to see if it was still there.

33:09.820 --> 33:10.820
 It was still there.

33:10.820 --> 33:11.820
 I couldn't contain myself.

33:11.820 --> 33:12.820
 I was so excited.

33:12.820 --> 33:16.180
 It was the most important moment of my working life.

33:16.180 --> 33:18.940
 Nothing I ever do again will mean as much.

33:18.940 --> 33:24.860
 So that particular moment and it kind of made me think of what would it take?

33:24.860 --> 33:28.380
 And I think we have all been there at small levels.

33:28.380 --> 33:34.820
 Maybe let me ask, have you had a moment like that in your life where you just had an idea

33:34.820 --> 33:40.060
 it's like, wow, yes.

33:40.060 --> 33:44.700
 I wouldn't mention myself in the same breath as Andrew Wiles, but I certainly had a number

33:44.700 --> 33:54.820
 of aha moments when I realized something very cool about physics just completely made

33:54.820 --> 33:55.820
 my head explode.

33:55.820 --> 33:59.580
 In fact, some of my favorite discoveries I made later, I later realized that they had

33:59.580 --> 34:03.340
 been discovered earlier by someone who's sometimes got quite famous for it.

34:03.340 --> 34:07.460
 So there's too late for me to even publish it, but that doesn't diminish in any way.

34:07.460 --> 34:12.340
 The emotional experience you have when you realize it like, wow.

34:12.340 --> 34:17.460
 So what would it take in that moment, that wow, that was yours in that moment?

34:17.460 --> 34:23.420
 So what do you think it takes for an intelligent system, an AGI system, an AI system to have

34:23.420 --> 34:24.980
 a moment like that?

34:24.980 --> 34:29.420
 It's a tricky question because there are actually two parts to it, right?

34:29.420 --> 34:37.260
 One of them is, can it accomplish that proof, can it prove that you can never write A to

34:37.260 --> 34:46.420
 the N plus B to the N equals 3 to the N for all integers, etc., etc., when N is bigger

34:46.420 --> 34:49.420
 than 2.

34:49.420 --> 34:51.580
 That's simply the question about intelligence.

34:51.580 --> 34:54.420
 Can you build machines that are that intelligent?

34:54.420 --> 34:59.860
 And I think by the time we get a machine that can independently come up with that level

34:59.860 --> 35:03.460
 of proofs, probably quite close to AGI.

35:03.460 --> 35:07.860
 But the second question is a question about consciousness.

35:07.860 --> 35:13.060
 When will we, how likely is it that such a machine would actually have any experience

35:13.060 --> 35:16.500
 at all as opposed to just being like a zombie?

35:16.500 --> 35:22.940
 And would we expect it to have some sort of emotional response to this or anything at

35:22.940 --> 35:31.140
 all akin to human emotion where when it accomplishes its machine goal, it views it as something

35:31.140 --> 35:39.260
 very positive and sublime and deeply meaningful.

35:39.260 --> 35:45.260
 I would certainly hope that if in the future we do create machines that are our peers or

35:45.260 --> 35:53.700
 even our descendants, I would certainly hope that they do have this sort of sublime appreciation

35:53.700 --> 36:06.020
 of life in a way, my absolutely worst nightmare would be that at some point in the future,

36:06.020 --> 36:10.620
 the distant future, maybe our cosmos is teeming with all this post biological life, doing

36:10.620 --> 36:13.180
 all the seemingly cool stuff.

36:13.180 --> 36:20.660
 And maybe the last humans by the time our species eventually fizzles out will be like,

36:20.660 --> 36:26.140
 well, that's okay, because we're so proud of our descendants here and look, my worst

36:26.140 --> 36:30.580
 nightmare is that we haven't solved the consciousness problem.

36:30.580 --> 36:34.100
 And we haven't realized that these are all the zombies, they're not aware of anything

36:34.100 --> 36:37.900
 anymore than a tape recorder, as in any kind of experience.

36:37.900 --> 36:41.660
 So the whole thing has just become a play for empty benches.

36:41.660 --> 36:44.700
 That would be like the ultimate zombie apocalypse to me.

36:44.700 --> 36:52.900
 So I would much rather, in that case, that we have these beings which can really appreciate

36:52.900 --> 36:57.060
 how amazing it is.

36:57.060 --> 37:02.260
 And in that picture, what would be the role of creativity, what a few people ask about

37:02.260 --> 37:03.260
 creativity?

37:03.260 --> 37:04.260
 Yeah.

37:04.260 --> 37:08.700
 And do you think, when you think about intelligence, I mean, certainly the story you told at the

37:08.700 --> 37:14.100
 beginning of your book involved, you know, creating movies and so on, sort of making

37:14.100 --> 37:18.580
 money, you know, you can make a lot of money in our modern world with music and movies.

37:18.580 --> 37:23.100
 So if you are an intelligent system, you may want to get good at that.

37:23.100 --> 37:26.300
 But that's not necessarily what I mean by creativity.

37:26.300 --> 37:32.620
 Is it important on that complex goals where the sea is rising for there to be something

37:32.620 --> 37:39.940
 creative, or am I being very human centric and thinking creativity somehow special relative

37:39.940 --> 37:41.940
 to intelligence?

37:41.940 --> 37:50.940
 My hunch is that we should think of creativity simply as an aspect of intelligence.

37:50.940 --> 37:57.820
 And we have to be very careful with human vanity.

37:57.820 --> 38:01.540
 We have this tendency to very often want to say, as soon as machines can do something,

38:01.540 --> 38:05.980
 we try to diminish it and say, oh, but that's not like real intelligence, you know, is

38:05.980 --> 38:12.620
 it not creative or this or that, the other thing, if we ask ourselves to write down a

38:12.620 --> 38:18.500
 definition of what we actually mean by being creative, what we mean by Andrew Wiles, what

38:18.500 --> 38:23.660
 he did there, for example, don't we often mean that someone takes a very unexpected

38:23.660 --> 38:26.060
 leap?

38:26.060 --> 38:33.740
 It's not like taking 573 and multiplying by 224 by just a step of straightforward cookbook

38:33.740 --> 38:36.500
 like rules, right?

38:36.500 --> 38:40.660
 You can maybe make a connection between two things that people have never thought was

38:40.660 --> 38:41.660
 connected.

38:41.660 --> 38:42.660
 It's very surprising.

38:42.660 --> 38:44.300
 Something like that.

38:44.300 --> 38:50.660
 I think this is an aspect of intelligence, and this is actually one of the most important

38:50.660 --> 38:53.260
 aspects of it.

38:53.260 --> 38:57.940
 Maybe the reason we humans tend to be better at it than traditional computers is because

38:57.940 --> 39:02.020
 it's something that comes more naturally if you're a neural network than if you're a

39:02.020 --> 39:05.820
 traditional logic gates based computer machine.

39:05.820 --> 39:11.900
 We physically have all these connections, and if you activate here, activate here, activate

39:11.900 --> 39:20.980
 here, it ping, you know, my hunch is that if we ever build a machine where you could

39:20.980 --> 39:31.020
 just give it the task, hey, hey, you say, hey, you know, I just realized I want to travel

39:31.020 --> 39:32.380
 around the world instead this month.

39:32.380 --> 39:34.700
 Can you teach my AGI course for me?

39:34.700 --> 39:36.100
 And it's like, okay, I'll do it.

39:36.100 --> 39:39.860
 And it does everything that you would have done and it improvises and stuff.

39:39.860 --> 39:42.860
 That would in my mind involve a lot of creativity.

39:42.860 --> 39:45.660
 Yeah, so it's actually a beautiful way to put it.

39:45.660 --> 39:54.540
 I think we do try to grasp at the definition of intelligence as everything we don't understand

39:54.540 --> 39:57.580
 how to build.

39:57.580 --> 40:02.180
 So we as humans try to find things that we have and machines don't have, and maybe creativity

40:02.180 --> 40:05.940
 is just one of the things, one of the words we used to describe that.

40:05.940 --> 40:06.940
 That's a really interesting way to put it.

40:06.940 --> 40:09.820
 I don't think we need to be that defensive.

40:09.820 --> 40:14.700
 I don't think anything good comes out of saying, we're somehow special, you know, it's

40:14.700 --> 40:27.540
 very wise, there are many examples in history of where trying to pretend they were somehow

40:27.540 --> 40:36.220
 superior to all other intelligent beings has led to pretty bad results, right?

40:36.220 --> 40:39.700
 Nazi Germany, they said that they were somehow superior to other people.

40:39.700 --> 40:44.580
 Today, we still do a lot of cruelty to animals by saying they were so superior somehow on

40:44.580 --> 40:50.500
 the other, they can't feel pain, slavery was justified by the same kind of really weak

40:50.500 --> 40:52.420
 arguments.

40:52.420 --> 40:58.700
 And I don't think if we actually go ahead and build artificial general intelligence,

40:58.700 --> 41:01.100
 it can do things better than us.

41:01.100 --> 41:08.980
 I don't think we should try to found our self worth on some sort of bogus claims of superiority

41:08.980 --> 41:11.940
 in terms of our intelligence.

41:11.940 --> 41:21.780
 I think we should instead find our calling and the meaning of life from the experiences

41:21.780 --> 41:22.780
 that we have.

41:22.780 --> 41:23.780
 Right.

41:23.780 --> 41:30.260
 You know, I can have very meaningful experiences even if there are other people who are smarter

41:30.260 --> 41:35.860
 than me, you know, when I go to faculty meeting here and I was talking about something and

41:35.860 --> 41:39.420
 then I certainly realized, oh, he has an old prize, he has an old prize, he has an old

41:39.420 --> 41:40.420
 prize.

41:40.420 --> 41:41.420
 Yeah.

41:41.420 --> 41:47.660
 You know, it doesn't make me enjoy life any less or enjoy talking to those people less.

41:47.660 --> 41:49.780
 Of course not.

41:49.780 --> 41:57.420
 And contrary to that, I feel very honored and privileged to get to interact with other

41:57.420 --> 42:00.820
 very intelligent beings that are better than me and a lot of stuff.

42:00.820 --> 42:05.420
 So I don't think there's any reason why we can't have the same approach with intelligent

42:05.420 --> 42:06.420
 machines.

42:06.420 --> 42:08.900
 That's a really interesting, so people don't often think about that.

42:08.900 --> 42:14.380
 They think about if there's machines that are more intelligent, you naturally think

42:14.380 --> 42:19.100
 that that's not going to be a beneficial type of intelligence.

42:19.100 --> 42:24.060
 You don't realize it could be, you know, like peers with no ball prizes that would be just

42:24.060 --> 42:25.060
 fun to talk with.

42:25.060 --> 42:30.580
 And they might be clever about certain topics and you can have fun having a few drinks with

42:30.580 --> 42:31.580
 them.

42:31.580 --> 42:38.620
 Well, also, you know, another example we can all relate to why it doesn't have to be a

42:38.620 --> 42:42.580
 terrible thing to be impressed, the presence of people who are even smarter than us all

42:42.580 --> 42:47.980
 around is when you and I were both two years old, I mean, our parents were much more intelligent

42:47.980 --> 42:48.980
 than us.

42:48.980 --> 42:49.980
 Right.

42:49.980 --> 42:50.980
 Worked out okay.

42:50.980 --> 42:54.140
 Because their goals were aligned with our goals.

42:54.140 --> 43:01.380
 And that I think is really the number one key issue we have to solve if we value align

43:01.380 --> 43:07.380
 the value alignment problem exactly because people who see too many Hollywood movies with

43:07.380 --> 43:12.260
 lousy science fiction plot lines, they worry about the wrong thing, right?

43:12.260 --> 43:16.500
 They worry about some machine suddenly turning evil.

43:16.500 --> 43:21.500
 It's not malice that we should that is the concern.

43:21.500 --> 43:23.000
 It's competence.

43:23.000 --> 43:29.580
 By definition, intelligence makes you makes you very competent if you have a more intelligent

43:29.580 --> 43:35.300
 goal playing machine computer playing as a less intelligent one and when we define intelligence

43:35.300 --> 43:37.740
 as the ability to accomplish go winning, right?

43:37.740 --> 43:40.780
 It's going to be the more intelligent one that wins.

43:40.780 --> 43:47.860
 And if you have a human and then you have an AGI that's more intelligent in all ways

43:47.860 --> 43:50.500
 and they have different goals, guess who's going to get their way, right?

43:50.500 --> 43:58.060
 So I was just reading about this particular rhinoceros species that was driven extinct

43:58.060 --> 43:59.060
 just a few years ago.

43:59.060 --> 44:05.740
 Alan Bummer is looking at this cute picture of a mommy rhinoceros with its child, you

44:05.740 --> 44:09.140
 know, and why did we humans drive it to extinction?

44:09.140 --> 44:12.860
 It wasn't because we were evil rhino haters as a whole.

44:12.860 --> 44:16.380
 It was just because we our goals weren't aligned with those of the rhinoceros and it didn't

44:16.380 --> 44:19.660
 work out so well for the rhinoceros because we were more intelligent, right?

44:19.660 --> 44:27.220
 So I think it's just so important that if we ever do build AGI before we unleash anything,

44:27.220 --> 44:37.380
 we have to make sure that it learns to understand our goals, that it adopts our goals and retains

44:37.380 --> 44:38.380
 those goals.

44:38.380 --> 44:45.740
 So the cool interesting problem there is being able, us as human beings, trying to formulate

44:45.740 --> 44:47.240
 our values.

44:47.240 --> 44:52.540
 So you know, you could think of the United States Constitution as a way that people sat

44:52.540 --> 44:59.780
 down at the time a bunch of white men, which is a good example, I should say.

44:59.780 --> 45:03.460
 They formulated the goals for this country and a lot of people agree that those goals

45:03.460 --> 45:05.540
 actually held up pretty well.

45:05.540 --> 45:09.600
 It's an interesting formulation of values and failed miserably in other ways.

45:09.600 --> 45:15.500
 So for the value alignment problem and the solution to it, we have to be able to put

45:15.500 --> 45:23.420
 on paper or in a program, human values, how difficult do you think that is?

45:23.420 --> 45:24.420
 Very.

45:24.420 --> 45:25.980
 But it's so important.

45:25.980 --> 45:30.340
 We really have to give it our best and it's difficult for two separate reasons.

45:30.340 --> 45:37.660
 There's the technical value alignment problem of figuring out just how to make machines

45:37.660 --> 45:40.660
 understand our goals, adopt them and retain them.

45:40.660 --> 45:46.140
 And then there's the separate part of it, the philosophical part, whose values anyway.

45:46.140 --> 45:51.700
 And since we, it's not like we have any great consensus on this planet on values, what mechanism

45:51.700 --> 45:56.780
 should we create then to aggregate and decide, okay, what's a good compromise?

45:56.780 --> 46:01.260
 That second discussion can't just be left the tech nerds like myself, right?

46:01.260 --> 46:02.260
 That's right.

46:02.260 --> 46:06.820
 And if we refuse to talk about it and then AGI gets built, who's going to be actually

46:06.820 --> 46:10.660
 making the decision about whose values, it's going to be a bunch of dudes in some tech

46:10.660 --> 46:12.380
 company, right?

46:12.380 --> 46:18.420
 And are they necessarily so representative of all of humankind that we want to just

46:18.420 --> 46:19.580
 endorse it to them?

46:19.580 --> 46:25.220
 Are they even uniquely qualified to speak to future human happiness just because they're

46:25.220 --> 46:26.460
 good at programming AI?

46:26.460 --> 46:30.380
 I'd much rather have this be a really inclusive conversation.

46:30.380 --> 46:32.700
 But do you think it's possible?

46:32.700 --> 46:38.820
 You create a beautiful vision that includes sort of the diversity, cultural diversity

46:38.820 --> 46:43.900
 and various perspectives on discussing rights, freedoms, human dignity.

46:43.900 --> 46:46.620
 But how hard is it to come to that consensus?

46:46.620 --> 46:52.140
 Do you think it's certainly a really important thing that we should all try to do, but do

46:52.140 --> 46:54.460
 you think it's feasible?

46:54.460 --> 47:01.660
 I think there's no better way to guarantee failure than to refuse to talk about it or

47:01.660 --> 47:02.980
 refuse to try.

47:02.980 --> 47:08.060
 And I also think it's a really bad strategy to say, okay, let's first have a discussion

47:08.060 --> 47:09.060
 for a long time.

47:09.060 --> 47:13.540
 And then once we reach complete consensus, then we'll try to load it into some machine.

47:13.540 --> 47:16.980
 No, we shouldn't let perfect be the enemy of good.

47:16.980 --> 47:22.140
 Instead, we should start with the kindergarten ethics that pretty much everybody agrees on

47:22.140 --> 47:24.580
 and put that into our machines now.

47:24.580 --> 47:26.100
 We're not doing that even.

47:26.100 --> 47:32.980
 Look at anyone who builds a passenger aircraft wants it to never under any circumstances

47:32.980 --> 47:35.900
 fly into a building or mountain, right?

47:35.900 --> 47:38.860
 Yet the September 11 hijackers were able to do that.

47:38.860 --> 47:44.220
 And even more embarrassingly, Andreas Lubitz, this depressed German wings pilot, when he

47:44.220 --> 47:50.220
 flew his passenger jet into the Alps, killing over 100 people, he just told the autopilot

47:50.220 --> 47:51.220
 to do it.

47:51.220 --> 47:55.140
 He told the freaking computer to change the altitude to 100 meters.

47:55.140 --> 48:01.820
 And even though it had the GPS maps, everything, the computer was like, okay, no, so we should

48:01.820 --> 48:07.300
 take those very basic values, though, where the problem is not that we don't agree.

48:07.300 --> 48:12.460
 The problem is just we've been too lazy to try to put it into our machines and make sure

48:12.460 --> 48:17.460
 that from now on, airplanes will just, which all have computers in them, but we'll just

48:17.460 --> 48:19.820
 never just refuse to do something like that.

48:19.820 --> 48:25.580
 We go into safe mode, maybe lock the cockpit door, go to the nearest airport, and there's

48:25.580 --> 48:31.340
 so much other technology in our world as well now where it's really coming quite timely

48:31.340 --> 48:34.300
 to put in some sort of very basic values like this.

48:34.300 --> 48:41.460
 Even in cars, we've had enough vehicle terrorism attacks by now where people have driven trucks

48:41.460 --> 48:47.300
 and vans into pedestrians that it's not at all a crazy idea to just have that hardwired

48:47.300 --> 48:51.420
 into the car, because yeah, there are a lot of, there's always going to be people who

48:51.420 --> 48:55.620
 for some reason want to harm others, but most of those people don't have the technical

48:55.620 --> 48:58.620
 expertise to figure out how to work around something like that.

48:58.620 --> 49:01.780
 So if the car just won't do it, it helps.

49:01.780 --> 49:02.940
 So let's start there.

49:02.940 --> 49:05.020
 So there's a lot of, that's a great point.

49:05.020 --> 49:06.900
 So not chasing perfect.

49:06.900 --> 49:10.780
 There's a lot of things that most of the world agrees on.

49:10.780 --> 49:11.940
 Yeah, let's start there.

49:11.940 --> 49:12.940
 Let's start there.

49:12.940 --> 49:18.140
 And then once we start there, we'll also get into the habit of having these kind of conversations

49:18.140 --> 49:21.940
 about, okay, what else should we put in here and have these discussions?

49:21.940 --> 49:24.100
 This should be a gradual process then.

49:24.100 --> 49:25.100
 Great.

49:25.100 --> 49:31.380
 So, but that also means describing these things and describing it to a machine.

49:31.380 --> 49:35.620
 So one thing, we had a few conversations with Steven Wolfram.

49:35.620 --> 49:37.140
 I'm not sure if you're familiar with Steven Wolfram.

49:37.140 --> 49:38.500
 Oh yeah, I know him quite well.

49:38.500 --> 49:43.380
 So he has, you know, he works with a bunch of things, but you know, cellular automata,

49:43.380 --> 49:47.660
 these simple computable things, these computation systems.

49:47.660 --> 49:52.380
 And he kind of mentioned that, you know, we probably have already within these systems

49:52.380 --> 49:59.580
 already something that's AGI, meaning like we just don't know it because we can't talk

49:59.580 --> 50:00.580
 to it.

50:00.580 --> 50:06.380
 So if you give me this chance to try it, to try to at least form a question out of this,

50:06.380 --> 50:12.780
 because I think it's an interesting idea to think that we can have intelligent systems,

50:12.780 --> 50:17.260
 but we don't know how to describe something to them and they can't communicate with us.

50:17.260 --> 50:21.220
 I know you're doing a little bit of work in explainable AI, trying to get AI to explain

50:21.220 --> 50:22.220
 itself.

50:22.220 --> 50:28.340
 So what are your thoughts of natural language processing or some kind of other communication?

50:28.340 --> 50:30.220
 How does the AI explain something to us?

50:30.220 --> 50:33.740
 How do we explain something to it, to machines?

50:33.740 --> 50:35.420
 Or you think of it differently?

50:35.420 --> 50:40.100
 So there are two separate parts to your question there.

50:40.100 --> 50:43.900
 One of them has to do with communication, which is super interesting and I'll get to

50:43.900 --> 50:44.900
 that in a sec.

50:44.900 --> 50:50.100
 The other is whether we already have AGI, we just haven't noticed it.

50:50.100 --> 50:54.340
 There, I beg to differ.

50:54.340 --> 50:58.420
 And don't think there's anything in any cellular automaton or anything or the internet itself

50:58.420 --> 51:05.400
 or whatever that has artificial general intelligence in that it didn't really do exactly everything

51:05.400 --> 51:06.980
 we humans can do better.

51:06.980 --> 51:14.100
 I think the day that happens, when that happens, we will very soon notice and we'll probably

51:14.100 --> 51:17.980
 notice even before because in a very, very big way.

51:17.980 --> 51:18.980
 For the second part though.

51:18.980 --> 51:20.700
 Can I just, sorry.

51:20.700 --> 51:30.260
 Because you have this beautiful way to formulate in consciousness as information processing

51:30.260 --> 51:33.740
 and you can think of intelligence and information processing and you can think of the entire

51:33.740 --> 51:34.740
 universe.

51:34.740 --> 51:40.220
 These particles and these systems roaming around that have this information processing

51:40.220 --> 51:47.500
 power, you don't think there is something with the power to process information in the

51:47.500 --> 51:55.460
 way that we human beings do that's out there that needs to be sort of connected to.

51:55.460 --> 51:59.980
 It seems a little bit philosophical perhaps, but there's something compelling to the idea

51:59.980 --> 52:06.100
 that the power is already there, the focus should be more on being able to communicate

52:06.100 --> 52:07.100
 with it.

52:07.100 --> 52:15.340
 Well, I agree that in a certain sense, the hardware processing power is already out there

52:15.340 --> 52:21.180
 because our universe itself can think of it as being a computer already.

52:21.180 --> 52:25.540
 It's constantly computing what water waves, how it devolved the water waves and the river

52:25.540 --> 52:29.860
 Charles and how to move the air molecules around that Seth Lloyd has pointed out.

52:29.860 --> 52:33.940
 My colleague here that you can even in a very rigorous way think of our entire universe

52:33.940 --> 52:35.660
 is just being a quantum computer.

52:35.660 --> 52:40.900
 It's pretty clear that our universe supports this amazing processing power because you

52:40.900 --> 52:46.580
 can even within this physics computer that we live in, we can even build actual laptops

52:46.580 --> 52:47.580
 and stuff.

52:47.580 --> 52:49.140
 So clearly the power is there.

52:49.140 --> 52:53.420
 It's just that most of the compute power that nature has, it's in my opinion kind of wasting

52:53.420 --> 52:57.140
 on boring stuff like simulating yet another ocean wave somewhere where no one is even

52:57.140 --> 52:58.140
 looking.

52:58.140 --> 53:03.820
 So in a sense, what life does, what we are doing when we build computers is we're rechanneling

53:03.820 --> 53:09.380
 all this compute that nature is doing anyway into doing things that are more interesting

53:09.380 --> 53:14.220
 than just yet another ocean wave and do something cool here.

53:14.220 --> 53:21.100
 So the raw hardware power is there for sure, and even just computing what's going to happen

53:21.100 --> 53:25.540
 for the next five seconds in this water ball, you know, it takes a ridiculous amount of

53:25.540 --> 53:28.060
 compute if you do it on a human computer.

53:28.060 --> 53:30.040
 This water ball just did it.

53:30.040 --> 53:36.020
 But that does not mean that this water ball has AGI and this because AGI means it should

53:36.020 --> 53:40.300
 also be able to like I've written my book done this interview.

53:40.300 --> 53:42.100
 And I don't think it's just communication problems.

53:42.100 --> 53:47.020
 I don't think it can do it.

53:47.020 --> 53:51.780
 So Buddhists say when they watch the water and that there is some beauty, that there's

53:51.780 --> 53:55.380
 some depth and beauty in nature that they can communicate with.

53:55.380 --> 54:01.180
 Communication is also very important because I mean, look, part of my job is being a teacher

54:01.180 --> 54:09.940
 and I know some very intelligent professors even who just have a better hard time communicating.

54:09.940 --> 54:14.620
 They come up with all these brilliant ideas, but to communicate with somebody else, you

54:14.620 --> 54:17.140
 have to also be able to simulate their own mind.

54:17.140 --> 54:18.140
 Yes.

54:18.140 --> 54:22.020
 And build well enough and understand that model of their mind that you can say things

54:22.020 --> 54:24.500
 that they will understand.

54:24.500 --> 54:26.700
 And that's quite difficult.

54:26.700 --> 54:31.620
 And that's why today it's so frustrating if you have a computer that makes some cancer

54:31.620 --> 54:36.260
 diagnosis and you ask it, well, why are you saying I should have a surgery?

54:36.260 --> 54:43.620
 And if you don't want to reply, I was trained on five terabytes of data and this is my diagnosis

54:43.620 --> 54:49.220
 boop, boop, beep, beep, doesn't really instill a lot of confidence, right?

54:49.220 --> 54:54.420
 So I think we have a lot of work to do on communication there.

54:54.420 --> 54:59.380
 So what kind of, I think you're doing a little bit of work in explainable AI.

54:59.380 --> 55:01.340
 What do you think are the most promising avenues?

55:01.340 --> 55:07.100
 Is it mostly about sort of the Alexa problem of natural language processing of being able

55:07.100 --> 55:13.220
 to actually use human interpretable methods of communication?

55:13.220 --> 55:17.500
 So being able to talk to a system and talk back to you, or is there some more fundamental

55:17.500 --> 55:18.500
 problems to be solved?

55:18.500 --> 55:21.180
 I think it's all of the above.

55:21.180 --> 55:27.180
 The natural language processing is obviously important, but there are also more nerdy fundamental

55:27.180 --> 55:28.180
 problems.

55:28.180 --> 55:39.180
 Like if you take, you play chess, Russian, I have to, when did you learn Russian?

55:39.180 --> 55:45.700
 I speak Russian very poorly, but I bought a book, teach yourself Russian, I read a lot,

55:45.700 --> 55:47.700
 but it was very difficult.

55:47.700 --> 55:48.700
 Wow.

55:48.700 --> 55:49.700
 That's why I speak so poorly.

55:49.700 --> 55:51.700
 How many languages do you know?

55:51.700 --> 55:52.700
 Wow.

55:52.700 --> 55:53.700
 That's really impressive.

55:53.700 --> 55:54.700
 I don't know.

55:54.700 --> 55:58.740
 My wife has some calculations, but my point was, if you played chess, have you looked

55:58.740 --> 56:00.260
 at the AlphaZero games?

56:00.260 --> 56:01.260
 Yeah.

56:01.260 --> 56:02.260
 Oh, the actual games now.

56:02.260 --> 56:03.260
 Check it out.

56:03.260 --> 56:09.900
 Some of them are just mind blowing, really beautiful.

56:09.900 --> 56:12.460
 If you ask, how did it do that?

56:12.460 --> 56:14.500
 You got that.

56:14.500 --> 56:20.540
 Talk to Demis Osabis, others from DeepMind, all they'll ultimately be able to give you

56:20.540 --> 56:26.940
 is big tables of numbers, matrices that define the neural network, and you can stare at these

56:26.940 --> 56:32.980
 tables numbers till your face turned blue, and you're not going to understand much about

56:32.980 --> 56:35.860
 why it made that move.

56:35.860 --> 56:40.540
 Even if you have a natural language processing that can tell you in human language about,

56:40.540 --> 56:44.180
 oh, five, seven, point two, eight, still not going to really help.

56:44.180 --> 56:50.660
 I think there's a whole spectrum of fun challenges there involved in taking computation that

56:50.660 --> 56:59.940
 does intelligent things and transforming it into something equally good, equally intelligent,

56:59.940 --> 57:02.060
 but that's more understandable.

57:02.060 --> 57:08.180
 I think that's really valuable because I think as we put machines in charge of ever more

57:08.180 --> 57:13.540
 infrastructure in our world, the power grid, the trading on the stock market, weapon systems,

57:13.540 --> 57:19.620
 and so on, it's absolutely crucial that we can trust these AIs that do all we want and

57:19.620 --> 57:25.860
 trust really comes from understanding in a very fundamental way.

57:25.860 --> 57:29.940
 That's why I'm working on this, because I think the more if we're going to have some

57:29.940 --> 57:34.700
 hope of ensuring that machines have adopted our goals and that they're going to retain

57:34.700 --> 57:41.260
 them, that kind of trust, I think, needs to be based on things you can actually understand,

57:41.260 --> 57:47.140
 preferably even improve theorems on, even with a self driving car, right?

57:47.140 --> 57:51.020
 If someone just tells you it's been trained on tons of data and never crashed, it's less

57:51.020 --> 57:54.460
 reassuring than if someone actually has a proof.

57:54.460 --> 57:58.820
 Maybe it's a computer verified proof, but still it says that under no circumstances

57:58.820 --> 58:02.420
 is this car just going to swerve into oncoming traffic.

58:02.420 --> 58:09.460
 And that kind of information helps build trust and helps build the alignment of goals, at

58:09.460 --> 58:12.300
 least awareness that your goals, your values are aligned.

58:12.300 --> 58:17.620
 And I think even in the very short term, if you look at how today, this absolutely pathetic

58:17.620 --> 58:25.980
 state of cybersecurity that we have, where is it, 3 billion Yahoo accounts are packed

58:25.980 --> 58:34.300
 and almost every American's credit card and so on, you know, why is this happening?

58:34.300 --> 58:39.940
 It's ultimately happening because we have software that nobody fully understood how

58:39.940 --> 58:41.460
 it worked.

58:41.460 --> 58:45.100
 That's why the bugs hadn't been found, right?

58:45.100 --> 58:50.340
 And I think AI can be used very effectively for offense for hacking, but it can also be

58:50.340 --> 59:00.580
 used for defense, hopefully, automating verifiability and creating systems that are built in different

59:00.580 --> 59:03.140
 ways so you can actually prove things about them.

59:03.140 --> 59:05.460
 And it's important.

59:05.460 --> 59:09.740
 So speaking of software that nobody understands how it works, of course, a bunch of people

59:09.740 --> 59:14.820
 ask about your paper about your thoughts of why does deep and cheap learning work so well?

59:14.820 --> 59:19.280
 That's the paper, but what are your thoughts on deep learning, these kind of simplified

59:19.280 --> 59:26.620
 models of our own brains that have been able to do some successful perception work, pattern

59:26.620 --> 59:30.940
 recognition work, and now with AlphaZero and so on, do some clever things?

59:30.940 --> 59:35.740
 What are your thoughts about the promise limitations of this piece?

59:35.740 --> 59:37.140
 Great.

59:37.140 --> 59:44.300
 I think there are a number of very important insights, very important lessons we can always

59:44.300 --> 59:47.340
 draw from these kind of successes.

59:47.340 --> 59:50.460
 One of them is when you look at the human brain, you see it's very complicated, a tenth

59:50.460 --> 59:54.140
 of 11 neurons, and there are all these different kinds of neurons, and yada yada, and there's

59:54.140 --> 59:57.980
 been this long debate about whether the fact that we have dozens of different kinds is

59:57.980 --> 1:00:01.580
 actually necessary for intelligence.

1:00:01.580 --> 1:00:06.500
 We can now, I think, quite convincingly answer that question of no, it's enough to have just

1:00:06.500 --> 1:00:07.500
 one kind.

1:00:07.500 --> 1:00:11.780
 If you look under the hood of AlphaZero, there's only one kind of neuron, and it's ridiculously

1:00:11.780 --> 1:00:15.060
 simple, a simple mathematical thing.

1:00:15.060 --> 1:00:21.380
 So it's just like in physics, if you have a gas with waves in it, it's not the detailed

1:00:21.380 --> 1:00:24.380
 nature of the molecules that matter.

1:00:24.380 --> 1:00:27.060
 It's the collective behavior, somehow.

1:00:27.060 --> 1:00:33.060
 Similarly, it's this higher level structure of the network that matters, not that you

1:00:33.060 --> 1:00:34.060
 have 20 kinds of neurons.

1:00:34.060 --> 1:00:41.740
 I think our brain is such a complicated mess because it wasn't evolved just to be intelligent,

1:00:41.740 --> 1:00:51.740
 it was evolved to also be self assembling, and self repairing, and evolutionarily attainable.

1:00:51.740 --> 1:00:53.660
 And patches and so on.

1:00:53.660 --> 1:00:58.700
 So I think it's pretty, my hunch is that we're going to understand how to build AGI before

1:00:58.700 --> 1:01:01.060
 we fully understand how our brains work.

1:01:01.060 --> 1:01:06.260
 Just like we understood how to build flying machines long before we were able to build

1:01:06.260 --> 1:01:07.260
 a mechanical bird.

1:01:07.260 --> 1:01:08.260
 Yeah, that's right.

1:01:08.260 --> 1:01:15.300
 You've given the example of mechanical birds and airplanes, and airplanes do a pretty good

1:01:15.300 --> 1:01:18.620
 job of flying without really mimicking bird flight.

1:01:18.620 --> 1:01:23.180
 And even now, after 100 years later, did you see the TED talk with this German group of

1:01:23.180 --> 1:01:24.180
 mechanical birds?

1:01:24.180 --> 1:01:25.180
 I did not.

1:01:25.180 --> 1:01:26.180
 I've heard you mention it.

1:01:26.180 --> 1:01:27.180
 Check it out.

1:01:27.180 --> 1:01:28.180
 It's amazing.

1:01:28.180 --> 1:01:30.180
 But even after that, we still don't fly in mechanical birds because it turned out the

1:01:30.180 --> 1:01:34.580
 way we came up with simpler, and it's better for our purposes, and I think it might be the

1:01:34.580 --> 1:01:35.580
 same there.

1:01:35.580 --> 1:01:38.140
 So that's one lesson.

1:01:38.140 --> 1:01:42.020
 Another lesson is one of what our paper was about.

1:01:42.020 --> 1:01:47.420
 Well, first, as a physicist thought, it was fascinating how there's a very close mathematical

1:01:47.420 --> 1:01:50.900
 relationship, actually, between our artificial neural networks.

1:01:50.900 --> 1:01:56.580
 And a lot of things that we've studied for in physics go by nerdy names like the renormalization

1:01:56.580 --> 1:02:01.100
 group equation and Hamiltonians and yada, yada, yada.

1:02:01.100 --> 1:02:11.380
 And when you look a little more closely at this, you have, at first, I was like, well,

1:02:11.380 --> 1:02:18.700
 there's something crazy here that doesn't make sense because we know that if you even

1:02:18.700 --> 1:02:23.380
 want to build a super simple neural network to tell apart cat pictures and dog pictures,

1:02:23.380 --> 1:02:27.260
 right, that you can do that very, very well now.

1:02:27.260 --> 1:02:31.540
 But if you think about it a little bit, you convince yourself it must be impossible because

1:02:31.540 --> 1:02:36.420
 if I have one megapixel, even if each pixel is just black or white, there's two to the

1:02:36.420 --> 1:02:40.900
 power of one million possible images, which is way more than there are atoms in our universe.

1:02:40.900 --> 1:02:47.220
 So in order to, and then for each one of those, I have to assign a number, which is the probability

1:02:47.220 --> 1:02:49.100
 that it's a dog.

1:02:49.100 --> 1:02:55.900
 So an arbitrary function of images is a list of more numbers than there are atoms in our

1:02:55.900 --> 1:02:56.900
 universe.

1:02:56.900 --> 1:03:02.220
 So clearly, I can't store that under the hood of my, my GPU or my, my computer yet somehow

1:03:02.220 --> 1:03:03.220
 works.

1:03:03.220 --> 1:03:04.220
 So what does that mean?

1:03:04.220 --> 1:03:12.940
 Well, it means that out of all of the problems that you could try to solve with a neural network,

1:03:12.940 --> 1:03:17.940
 almost all of them are impossible to solve with a reasonably sized one.

1:03:17.940 --> 1:03:24.820
 But then what we showed in our paper was, was that the, the fraction, the kind of problems,

1:03:24.820 --> 1:03:29.740
 the fraction of all the problems that you could possibly pose that the, that we actually

1:03:29.740 --> 1:03:34.980
 care about given the laws of physics is also an infinitesimally tiny little part.

1:03:34.980 --> 1:03:37.180
 And amazingly, they're basically the same part.

1:03:37.180 --> 1:03:38.180
 Yeah.

1:03:38.180 --> 1:03:41.180
 It's almost like our world was created for, I mean, they kind of come together.

1:03:41.180 --> 1:03:42.180
 Yeah.

1:03:42.180 --> 1:03:44.940
 You, but you could say maybe where the world created the world that the world was created

1:03:44.940 --> 1:03:50.300
 for us, but I have a more modest interpretation, which is that instead evolution endowments

1:03:50.300 --> 1:03:54.700
 with neural networks, precisely for that reason, because this particular architecture has

1:03:54.700 --> 1:04:02.380
 opposed to the one in your laptop is very, very well adapted to solving the kind of problems

1:04:02.380 --> 1:04:05.540
 that nature kept presenting our ancestors with, right?

1:04:05.540 --> 1:04:09.380
 So it makes sense that why do we have a brain in the first place?

1:04:09.380 --> 1:04:12.940
 It's to be able to make predictions about the future and so on.

1:04:12.940 --> 1:04:17.580
 So if we had a sucky system, which could never solve it, it wouldn't have a lot.

1:04:17.580 --> 1:04:23.420
 So, but it's, so this is, this is a, I think a very beautiful fact.

1:04:23.420 --> 1:04:24.420
 Yeah.

1:04:24.420 --> 1:04:28.780
 And you also realize that there's, there, that we, there've been, it's been earlier

1:04:28.780 --> 1:04:34.140
 work on, on why deeper networks are good, but we were able to show an additional cool

1:04:34.140 --> 1:04:40.260
 fact there, which is that even incredibly simple problems, like suppose I give you a

1:04:40.260 --> 1:04:45.020
 thousand numbers and ask you to multiply them together and you can write a few lines of

1:04:45.020 --> 1:04:46.820
 code, boom, done, trivial.

1:04:46.820 --> 1:04:52.580
 If you just try to do that with a neural network that has only one single hidden layer in it,

1:04:52.580 --> 1:04:59.940
 you can do it, but you're going to need two to the power of thousand neurons to multiply

1:04:59.940 --> 1:05:03.260
 a thousand numbers, which is again, more neurons than their atoms in our universe.

1:05:03.260 --> 1:05:05.740
 So that's fascinating.

1:05:05.740 --> 1:05:11.580
 But if you allow, if you allow yourself, make it a deep network of many layers, you only

1:05:11.580 --> 1:05:15.340
 need four thousand neurons, it's perfectly feasible.

1:05:15.340 --> 1:05:17.500
 So that's really interesting.

1:05:17.500 --> 1:05:18.500
 Yeah.

1:05:18.500 --> 1:05:19.500
 Yeah.

1:05:19.500 --> 1:05:24.460
 So architecture type, I mean, you mentioned Schrodinger's equation and what are your thoughts

1:05:24.460 --> 1:05:32.860
 about quantum computing and the role of this kind of computational unit in creating an

1:05:32.860 --> 1:05:34.900
 intelligent system?

1:05:34.900 --> 1:05:41.100
 In some Hollywood movies that I don't mention my name because I don't want to spoil them.

1:05:41.100 --> 1:05:46.820
 The way they get AGI is building a quantum computer because the word quantum sounds

1:05:46.820 --> 1:05:47.820
 cool and so on.

1:05:47.820 --> 1:05:48.820
 That's right.

1:05:48.820 --> 1:05:54.940
 But first of all, I think we don't need quantum computers to build AGI.

1:05:54.940 --> 1:06:01.740
 I suspect your brain is not quantum computer in any found sense.

1:06:01.740 --> 1:06:03.460
 So you don't even wrote a paper about that.

1:06:03.460 --> 1:06:09.060
 Many years ago, I calculated the so called decoherence time that how long it takes until

1:06:09.060 --> 1:06:16.900
 the quantum computerness of what your neurons are doing gets erased by just random noise

1:06:16.900 --> 1:06:21.420
 from the environment and it's about 10 to the minus 21 seconds.

1:06:21.420 --> 1:06:27.420
 So as cool as it would be to have a quantum computer in my head, I don't think that fast.

1:06:27.420 --> 1:06:35.820
 On the other hand, there are very cool things you could do with quantum computers or I think

1:06:35.820 --> 1:06:40.780
 we'll be able to do soon when we get bigger ones that might actually help machine learning

1:06:40.780 --> 1:06:43.180
 do even better than the brain.

1:06:43.180 --> 1:06:58.620
 So for example, one, this is just a moonshot, but hey, learning is very much same thing

1:06:58.620 --> 1:07:00.860
 as search.

1:07:00.860 --> 1:07:05.460
 If you're trying to train a neural network to get really learned to do something really

1:07:05.460 --> 1:07:10.820
 well, you have some loss function, you have a bunch of knobs you can turn represented

1:07:10.820 --> 1:07:14.420
 by a bunch of numbers and you're trying to tweak them so that it becomes as good as possible

1:07:14.420 --> 1:07:15.420
 at this thing.

1:07:15.420 --> 1:07:22.580
 So if you think of a landscape with some valley, where each dimension of the landscape corresponds

1:07:22.580 --> 1:07:25.780
 to some number you can change, you're trying to find the minimum.

1:07:25.780 --> 1:07:29.980
 And it's well known that if you have a very high dimensional landscape, complicated things,

1:07:29.980 --> 1:07:34.140
 it's super hard to find the minimum.

1:07:34.140 --> 1:07:37.500
 Quantum mechanics is amazingly good at this.

1:07:37.500 --> 1:07:42.980
 If I want to know what's the lowest energy state this water can possibly have incredibly

1:07:42.980 --> 1:07:47.860
 hard to compute, but nature will happily figure this out for you if you just cool it down,

1:07:47.860 --> 1:07:50.860
 make it very, very cold.

1:07:50.860 --> 1:07:55.260
 If you put a ball somewhere, it'll roll down to its minimum and this happens metaphorically

1:07:55.260 --> 1:07:57.620
 at the energy landscape too.

1:07:57.620 --> 1:08:02.940
 And quantum mechanics even uses some clever tricks which today's machine learning systems

1:08:02.940 --> 1:08:03.940
 don't.

1:08:03.940 --> 1:08:07.940
 If you're trying to find the minimum and you get stuck in the little local minimum here

1:08:07.940 --> 1:08:14.180
 in quantum mechanics, you can actually tunnel through the barrier and get unstuck again.

1:08:14.180 --> 1:08:15.420
 And that's really interesting.

1:08:15.420 --> 1:08:16.420
 Yeah.

1:08:16.420 --> 1:08:22.940
 So maybe for example, we'll one day use quantum computers that help train neural networks

1:08:22.940 --> 1:08:23.940
 better.

1:08:23.940 --> 1:08:24.940
 That's really interesting.

1:08:24.940 --> 1:08:25.940
 Okay.

1:08:25.940 --> 1:08:32.020
 So as a component of kind of the learning process, for example, let me ask sort of wrapping

1:08:32.020 --> 1:08:34.060
 up here a little bit.

1:08:34.060 --> 1:08:40.540
 Let me return to the questions of our human nature and love, as I mentioned.

1:08:40.540 --> 1:08:48.020
 So do you think you mentioned sort of a helper robot that you could think of also personal

1:08:48.020 --> 1:08:49.020
 robots.

1:08:49.020 --> 1:08:55.300
 Do you think the way we human beings fall in love and get connected to each other is

1:08:55.300 --> 1:09:00.420
 possible to achieve in an AI system and human level AI intelligence system.

1:09:00.420 --> 1:09:06.100
 Do you think we would ever see that kind of connection or, you know, in all this discussion

1:09:06.100 --> 1:09:11.460
 about solving complex goals, as this kind of human social connection, do you think that's

1:09:11.460 --> 1:09:16.460
 one of the goals on the peaks and valleys that were the raising sea levels that we'd

1:09:16.460 --> 1:09:17.460
 be able to achieve?

1:09:17.460 --> 1:09:22.180
 Or do you think that's something that's ultimately, or at least in the short term, relative to

1:09:22.180 --> 1:09:23.620
 the other goals is not achievable?

1:09:23.620 --> 1:09:25.220
 I think it's all possible.

1:09:25.220 --> 1:09:31.780
 And I mean, in recent, there's a very wide range of guesses, as you know, among AI researchers

1:09:31.780 --> 1:09:35.300
 when we're going to get AGI.

1:09:35.300 --> 1:09:39.620
 Some people, you know, like our friend Rodney Brooks said, it's going to be hundreds of

1:09:39.620 --> 1:09:41.140
 years at least.

1:09:41.140 --> 1:09:44.780
 And then there are many others that think it's going to happen relatively much sooner.

1:09:44.780 --> 1:09:52.140
 Recent polls, maybe half or so, AI researchers think we're going to get AGI within decades.

1:09:52.140 --> 1:09:56.260
 So if that happens, of course, then I think these things are all possible.

1:09:56.260 --> 1:10:01.860
 But in terms of whether it will happen, I think we shouldn't spend so much time asking,

1:10:01.860 --> 1:10:04.260
 what do we think will happen in the future?

1:10:04.260 --> 1:10:08.980
 As if we are just some sort of pathetic, passive bystanders, you know, waiting for the future

1:10:08.980 --> 1:10:12.740
 to happen to us, hey, we're the ones creating this future, right?

1:10:12.740 --> 1:10:18.340
 So we should be proactive about it and ask ourselves what sort of future we would like

1:10:18.340 --> 1:10:19.340
 to have happen.

1:10:19.340 --> 1:10:20.340
 That's right.

1:10:20.340 --> 1:10:21.340
 Trying to make it like that.

1:10:21.340 --> 1:10:25.660
 Well, what I prefer is some sort of incredibly boring zombie like future where there's all

1:10:25.660 --> 1:10:30.220
 these mechanical things happening and there's no passion, no emotion, no experience, maybe

1:10:30.220 --> 1:10:31.220
 even.

1:10:31.220 --> 1:10:35.740
 No, I would, of course, much rather prefer it if all the things that we find that we

1:10:35.740 --> 1:10:44.180
 value the most about humanity are a subjective experience, passion, inspiration, love, you

1:10:44.180 --> 1:10:50.780
 know, if we can create a future where those things do exist.

1:10:50.780 --> 1:10:56.500
 You know, I think ultimately it's not our universe giving meaning to us, it's us giving

1:10:56.500 --> 1:10:58.500
 meaning to our universe.

1:10:58.500 --> 1:11:03.620
 And if we build more advanced intelligence, let's make sure we build it in such a way

1:11:03.620 --> 1:11:09.100
 that meaning is part of it.

1:11:09.100 --> 1:11:13.900
 A lot of people that seriously study this problem and think of it from different angles have

1:11:13.900 --> 1:11:20.140
 trouble in the majority of cases, if they think through that happen, are the ones that

1:11:20.140 --> 1:11:22.620
 are not beneficial to humanity.

1:11:22.620 --> 1:11:27.260
 And so, yeah, so what are your thoughts?

1:11:27.260 --> 1:11:33.820
 What should people, you know, I really don't like people to be terrified, what's the way

1:11:33.820 --> 1:11:38.660
 for people to think about it in a way that, in a way we can solve it and we can make it

1:11:38.660 --> 1:11:39.660
 better.

1:11:39.660 --> 1:11:40.660
 Yeah.

1:11:40.660 --> 1:11:44.780
 No, I don't think panicking is going to help in any way, it's not going to increase chances

1:11:44.780 --> 1:11:46.060
 of things going well either.

1:11:46.060 --> 1:11:49.340
 Even if you are in a situation where there is a real threat, does it help if everybody

1:11:49.340 --> 1:11:50.620
 just freaks out?

1:11:50.620 --> 1:11:51.620
 Right.

1:11:51.620 --> 1:11:53.620
 No, of course not.

1:11:53.620 --> 1:11:59.740
 I think, yeah, there are, of course, ways in which things can go horribly wrong.

1:11:59.740 --> 1:12:04.460
 First of all, it's important when we think about this thing, this, about the problems

1:12:04.460 --> 1:12:08.780
 and risks, to also remember how huge the upsides can be if we get it right.

1:12:08.780 --> 1:12:13.420
 Everything we love about society and civilization is a product of intelligence.

1:12:13.420 --> 1:12:17.980
 So if we can amplify our intelligence with machine intelligence and not anymore lose

1:12:17.980 --> 1:12:23.380
 our loved ones, what we're told is an uncurable disease and things like this, of course, we

1:12:23.380 --> 1:12:24.940
 should aspire to that.

1:12:24.940 --> 1:12:28.700
 So that can be a motivator, I think, reminding yourselves that the reason we try to solve

1:12:28.700 --> 1:12:34.140
 problems is not just because we're trying to avoid gloom, but because we're trying to

1:12:34.140 --> 1:12:35.900
 do something great.

1:12:35.900 --> 1:12:43.340
 But then in terms of the risks, I think the really important question is to ask, what

1:12:43.340 --> 1:12:47.740
 can we do today that will actually help make the outcome good, right?

1:12:47.740 --> 1:12:52.700
 And dismissing the risk is not one of them, you know, I find it quite funny often when

1:12:52.700 --> 1:13:01.540
 I'm in discussion panels about these things, how the people who work for companies will

1:13:01.540 --> 1:13:05.100
 always be like, oh, nothing to worry about, nothing to worry about, nothing to worry about.

1:13:05.100 --> 1:13:09.980
 And it's always, it's only academics sometimes express concerns.

1:13:09.980 --> 1:13:10.980
 That's not surprising at all.

1:13:10.980 --> 1:13:17.500
 If you think about it, often Sinclair quipped, right, that it's hard to make a man believe

1:13:17.500 --> 1:13:20.620
 in something when his income depends on not believing in it.

1:13:20.620 --> 1:13:25.580
 And frankly, we know a lot of these people in companies that they're just as concerned

1:13:25.580 --> 1:13:26.580
 as anyone else.

1:13:26.580 --> 1:13:30.300
 But if you're the CEO of a company, that's not something you want to go on record saying

1:13:30.300 --> 1:13:34.980
 when you have silly journalists who are going to put a picture of a Terminator robot when

1:13:34.980 --> 1:13:35.980
 they quote you.

1:13:35.980 --> 1:13:39.380
 So, so the issues are real.

1:13:39.380 --> 1:13:45.660
 And the way I think about what the issue is, is basically, you know, the real choice we

1:13:45.660 --> 1:13:51.980
 have is, first of all, are we going to dismiss this, the risks and say, well, you know, let's

1:13:51.980 --> 1:13:57.140
 just go ahead and build machines that can do everything we can do better and cheaper,

1:13:57.140 --> 1:14:00.940
 you know, let's just make ourselves obsolete as fast as possible or what could possibly

1:14:00.940 --> 1:14:01.940
 go wrong.

1:14:01.940 --> 1:14:02.940
 Right.

1:14:02.940 --> 1:14:03.940
 That's one attitude.

1:14:03.940 --> 1:14:09.380
 The opposite attitude that I think is to say, it's incredible potential, you know, let's

1:14:09.380 --> 1:14:14.900
 think about what kind of future we're really, really excited about.

1:14:14.900 --> 1:14:18.700
 What are the shared goals that we can really aspire towards?

1:14:18.700 --> 1:14:22.100
 And then let's think really hard about how we can actually get there.

1:14:22.100 --> 1:14:23.100
 So start with it.

1:14:23.100 --> 1:14:24.460
 Don't start thinking about the risks.

1:14:24.460 --> 1:14:26.940
 Start thinking about the goals.

1:14:26.940 --> 1:14:30.540
 And then when you do that, then you can think about the obstacles you want to avoid, right?

1:14:30.540 --> 1:14:34.420
 I often get students coming in right here into my office for career advice.

1:14:34.420 --> 1:14:38.060
 Always ask them this very question, where do you want to be in the future?

1:14:38.060 --> 1:14:42.580
 If all she can say is, oh, maybe I'll have cancer, maybe I'll run over by a truck.

1:14:42.580 --> 1:14:44.420
 Focus on the obstacles instead of the goal.

1:14:44.420 --> 1:14:49.340
 She's just going to end up a hypochondriac paranoid, whereas if she comes in and fire

1:14:49.340 --> 1:14:54.060
 in her eyes and is like, I want to be there, and then we can talk about the obstacles and

1:14:54.060 --> 1:14:56.100
 see how we can circumvent them.

1:14:56.100 --> 1:14:59.100
 That's I think a much, much healthier attitude.

1:14:59.100 --> 1:15:01.540
 And that's really what we're in.

1:15:01.540 --> 1:15:09.420
 And I feel it's very challenging to come up with a vision for the future, which we're

1:15:09.420 --> 1:15:10.660
 unequivocally excited about.

1:15:10.660 --> 1:15:14.300
 I'm not just talking now in the vague terms like, yeah, let's cure cancer.

1:15:14.300 --> 1:15:18.500
 I'm talking about what kind of society do we want to create?

1:15:18.500 --> 1:15:25.380
 What do we want it to mean to be human in the age of AI, in the age of AGI?

1:15:25.380 --> 1:15:31.460
 So if we can have this conversation, broad, inclusive conversation, and gradually start

1:15:31.460 --> 1:15:38.100
 converging towards some future with some direction at least that we want to steer towards, right?

1:15:38.100 --> 1:15:42.340
 Then we'll be much more motivated to constructively take on the obstacles.

1:15:42.340 --> 1:15:54.260
 And I think if I wrap this up in a more succinct way, I think we can all agree already now that

1:15:54.260 --> 1:16:05.540
 we should aspire to build AGI that doesn't overpower us, but that empowers us.

1:16:05.540 --> 1:16:10.820
 And think of the many various ways that can do that, whether that's from my side of the

1:16:10.820 --> 1:16:12.860
 world of autonomous vehicles.

1:16:12.860 --> 1:16:17.020
 I'm personally actually from the camp that believes this human level intelligence is

1:16:17.020 --> 1:16:22.780
 required to achieve something like vehicles that would actually be something we would

1:16:22.780 --> 1:16:25.380
 enjoy using and being part of.

1:16:25.380 --> 1:16:26.380
 So that's the one example.

1:16:26.380 --> 1:16:31.140
 And certainly there's a lot of other types of robots and medicine and so on.

1:16:31.140 --> 1:16:35.300
 So focusing on those and then coming up with the obstacles, coming up with the ways that

1:16:35.300 --> 1:16:38.420
 that can go wrong and solving those one at a time.

1:16:38.420 --> 1:16:42.980
 And just because you can build an autonomous vehicle, even if you could build one that

1:16:42.980 --> 1:16:47.500
 would drive this final AGI, maybe there are some things in life that we would actually

1:16:47.500 --> 1:16:48.500
 want to do ourselves.

1:16:48.500 --> 1:16:49.500
 That's right.

1:16:49.500 --> 1:16:50.500
 Right?

1:16:50.500 --> 1:16:54.660
 Like, for example, if you think of our society as a whole, there are some things that we

1:16:54.660 --> 1:16:57.540
 find very meaningful to do.

1:16:57.540 --> 1:17:02.100
 And that doesn't mean we have to stop doing them just because machines can do them better.

1:17:02.100 --> 1:17:06.660
 I'm not going to stop playing tennis just the day someone builds a tennis robot and

1:17:06.660 --> 1:17:07.660
 beat me.

1:17:07.660 --> 1:17:09.900
 People are still playing chess and even go.

1:17:09.900 --> 1:17:10.900
 Yeah.

1:17:10.900 --> 1:17:19.100
 And in this very near term, even some people are advocating basic income, replace jobs.

1:17:19.100 --> 1:17:22.780
 But if the government is going to be willing to just hand out cash to people for doing

1:17:22.780 --> 1:17:27.660
 nothing, then one should also seriously consider whether the government should also just hire

1:17:27.660 --> 1:17:33.380
 a lot more teachers and nurses and the kind of jobs which people often find great fulfillment

1:17:33.380 --> 1:17:34.380
 in doing, right?

1:17:34.380 --> 1:17:38.900
 We get very tired of hearing politicians saying, oh, we can't afford hiring more teachers,

1:17:38.900 --> 1:17:41.700
 but we're going to maybe have basic income.

1:17:41.700 --> 1:17:46.340
 If we can have more serious research and thought into what gives meaning to our lives, the

1:17:46.340 --> 1:17:50.700
 jobs give so much more than income, right?

1:17:50.700 --> 1:18:00.020
 And then think about, in the future, what are the roles that we want to have people

1:18:00.020 --> 1:18:03.180
 continually feeling empowered by machines?

1:18:03.180 --> 1:18:08.900
 And I think sort of, I come from the Russia, from the Soviet Union, and I think for a lot

1:18:08.900 --> 1:18:14.100
 of people in the 20th century, going to the moon, going to space was an inspiring thing.

1:18:14.100 --> 1:18:21.300
 I feel like the universe of the mind, so AI, understanding, creating intelligence is that

1:18:21.300 --> 1:18:23.380
 for the 21st century.

1:18:23.380 --> 1:18:26.740
 So it's really surprising, and I've heard you mention this, it's really surprising to

1:18:26.740 --> 1:18:31.940
 me both on the research funding side that it's not funded as greatly as it could be.

1:18:31.940 --> 1:18:36.500
 But most importantly, on the politician side, that it's not part of the public discourse

1:18:36.500 --> 1:18:44.300
 except in killer bots, terminator kind of view, that people are not yet, I think, perhaps

1:18:44.300 --> 1:18:48.260
 excited by the possible positive future that we can build together.

1:18:48.260 --> 1:18:54.660
 So we should be, because politicians usually just focus on the next election cycle, right?

1:18:54.660 --> 1:18:59.340
 The single most important thing I feel we humans have learned in the entire history of science

1:18:59.340 --> 1:19:07.460
 is they were the masters of underestimation, underestimated the size of our cosmos, again

1:19:07.460 --> 1:19:11.380
 and again, realizing that everything we thought existed was just a small part of something

1:19:11.380 --> 1:19:12.380
 grander, right?

1:19:12.380 --> 1:19:18.580
 Planet, solar system, the galaxy, clusters of galaxies, the universe.

1:19:18.580 --> 1:19:25.700
 And we now know that we have the future has just so much more potential than our ancestors

1:19:25.700 --> 1:19:27.820
 could ever have dreamt of.

1:19:27.820 --> 1:19:39.820
 This cosmos, imagine if all of Earth was completely devoid of life except for Cambridge, Massachusetts.

1:19:39.820 --> 1:19:44.220
 Wouldn't it be kind of lame if all we ever aspired to was to stay in Cambridge, Massachusetts

1:19:44.220 --> 1:19:49.660
 forever and then go extinct in one week, even though Earth was going to continue on for

1:19:49.660 --> 1:19:50.660
 longer?

1:19:50.660 --> 1:19:57.300
 That sort of attitude I think we have now on the cosmic scale, we can, life can flourish

1:19:57.300 --> 1:20:00.820
 on Earth, not for four years, but for billions of years.

1:20:00.820 --> 1:20:06.340
 I can even tell you about how to move it out of harm's way when the sun gets too hot.

1:20:06.340 --> 1:20:11.900
 And then we have so much more resources out here, which today, maybe there are a lot of

1:20:11.900 --> 1:20:19.380
 other planets with bacteria or cow like life on them, but most of this, all this opportunity

1:20:19.380 --> 1:20:25.380
 seems as far as we can tell to be largely dead, like the Sahara Desert, and yet we have the

1:20:25.380 --> 1:20:30.380
 opportunity to help life flourish around this for billions of years.

1:20:30.380 --> 1:20:37.420
 So like, let's quit squabbling about whether some little border should be drawn one mile

1:20:37.420 --> 1:20:43.380
 to the left or right and look up into the skies and realize, hey, we can do such incredible

1:20:43.380 --> 1:20:44.380
 things.

1:20:44.380 --> 1:20:45.380
 Yeah.

1:20:45.380 --> 1:20:49.980
 And that's I think why it's really exciting that you and others are connected with some

1:20:49.980 --> 1:20:54.740
 of the work Elon Musk is doing because he's literally going out into that space, really

1:20:54.740 --> 1:20:56.260
 exploring our universe.

1:20:56.260 --> 1:20:57.260
 And it's wonderful.

1:20:57.260 --> 1:21:02.340
 That is exactly why Elon Musk is so misunderstood, right?

1:21:02.340 --> 1:21:05.300
 Misconstrued with some kind of pessimistic doomsayer.

1:21:05.300 --> 1:21:10.860
 The reason he cares so much about AI safety is because he more than almost anyone else

1:21:10.860 --> 1:21:13.340
 appreciates these amazing opportunities.

1:21:13.340 --> 1:21:16.340
 It will squander if we wipe out here on Earth.

1:21:16.340 --> 1:21:22.740
 We're not just going to wipe out the next generation, but all generations and this incredible

1:21:22.740 --> 1:21:25.580
 opportunity that's out there and that would be really be a waste.

1:21:25.580 --> 1:21:32.740
 And AI, for people who think that there would be better to do without technology, let me

1:21:32.740 --> 1:21:37.740
 just mention that if we don't improve our technology, the question isn't whether humanity

1:21:37.740 --> 1:21:38.740
 is going to go extinct.

1:21:38.740 --> 1:21:43.620
 The question is just whether we're going to get taken out by the next big asteroid or

1:21:43.620 --> 1:21:49.540
 the next super volcano or something else dumb that we could easily prevent with more tech,

1:21:49.540 --> 1:21:50.540
 right?

1:21:50.540 --> 1:21:56.220
 If we want life to flourish throughout the cosmos, AI is the key to it.

1:21:56.220 --> 1:22:04.780
 As I mentioned in a lot of detail in my book, even many of the most inspired sci fi writers

1:22:04.780 --> 1:22:11.580
 I feel have totally underestimated the opportunities for space travel, especially to other galaxies,

1:22:11.580 --> 1:22:17.100
 because they weren't thinking about the possibility of AGI, which just makes it so much easier.

1:22:17.100 --> 1:22:18.100
 Right.

1:22:18.100 --> 1:22:25.900
 Yeah, so that goes to a view of AGI that enables our progress, that enables a better life.

1:22:25.900 --> 1:22:30.060
 So that's a beautiful way to put it and something to strive for.

1:22:30.060 --> 1:22:31.580
 So Max, thank you so much.

1:22:31.580 --> 1:22:32.580
 Thank you for your time today.

1:22:32.580 --> 1:22:33.580
 It's been awesome.

1:22:33.580 --> 1:22:34.580
 Thank you so much.

1:22:34.580 --> 1:22:35.580
 Thanks.

1:22:35.580 --> 1:22:36.580
 Merci beaucoup.

1:22:36.580 --> 1:22:49.100
 Thank you so much for your time today and thank you so much for your time and for your

1:22:49.100 --> 1:22:50.100
 time.

1:22:50.100 --> 1:22:51.100
 Thank you.

1:22:51.100 --> 1:22:52.100
 Thank you.

1:22:52.100 --> 1:22:53.100
 Bye.

1:22:53.100 --> 1:22:54.100
 Bye.

1:22:54.100 --> 1:22:55.100
 Bye.

1:22:55.100 --> 1:22:56.100
 Bye.

1:22:56.100 --> 1:22:57.100
 Bye.

1:22:57.100 --> 1:22:58.100
 Bye.

1:22:58.100 --> 1:22:59.100
 Bye.

1:22:59.100 --> 1:23:00.100
 Bye.

