WEBVTT

00:00.000 --> 00:04.600
 The following is a conversation with David Eagleman, a neuroscientist and one of the

00:04.600 --> 00:10.920
 great science communicators of our time, exploring the beauty and mystery of the human brain.

00:10.920 --> 00:15.920
 He is an author of a lot of amazing books about the human mind and his new one called

00:15.920 --> 00:18.040
 LiveWired.

00:18.040 --> 00:24.600
 LiveWired is a work of ten years on a topic that is fascinating to me, which is neuroplasticity

00:24.600 --> 00:27.840
 or the malleability of the human brain.

00:27.840 --> 00:32.880
 Quick summary of the sponsors, Athletic Greens, BetterHelp, and Cash App.

00:32.880 --> 00:37.880
 Click the sponsor links in the description to get a discount and to support this podcast.

00:37.880 --> 00:44.440
 As a side note, let me say that the adaptability of the human mind at the biological, chemical,

00:44.440 --> 00:50.800
 cognitive, psychological, and even sociological levels is the very thing that captivated me

00:50.800 --> 00:56.280
 many years ago when I first began to wonder how would my engineer something like it in

00:56.280 --> 00:57.280
 the machine.

00:57.280 --> 01:03.800
 The open question today in the 21st century is what are the limits of this adaptability?

01:03.800 --> 01:09.560
 As new, smarter and smarter devices and AI systems come to life, or as better and better

01:09.560 --> 01:14.120
 brain computer interfaces are engineered, will our brain be able to adapt, to catch

01:14.120 --> 01:16.520
 up, to excel?

01:16.520 --> 01:21.480
 I personally believe yes, that we are far from reaching the limitation of the human mind

01:21.480 --> 01:27.000
 and the human brain, just as we are far from reaching the limitations of our computational

01:27.000 --> 01:28.000
 systems.

01:28.000 --> 01:33.520
 If you enjoy this thing, subscribe on YouTube, or review it with 5 stars on our podcast,

01:33.520 --> 01:40.160
 follow us on Spotify, support on Patreon, or connect with me on Twitter, at Lex Freedman.

01:40.160 --> 01:43.360
 As usual, I'll do a few minutes of ads now and no ads in the middle.

01:43.360 --> 01:48.000
 I try to make these interesting, but I give you timestamps so you can skip.

01:48.000 --> 01:51.800
 But please do check out the sponsors by clicking the links in the description.

01:51.800 --> 01:55.160
 It's the best way to support this podcast.

01:55.160 --> 02:00.480
 This show is brought to you by Athletic Greens, the all in one daily drink to support better

02:00.480 --> 02:02.880
 health and peak performance.

02:02.880 --> 02:07.200
 Even with a balanced diet, it's difficult to cover all your nutritional bases.

02:07.200 --> 02:09.240
 That's where Athletic Greens will help.

02:09.240 --> 02:13.960
 Their daily drink is like nutritional insurance for your body as delivered straight to your

02:13.960 --> 02:15.460
 door.

02:15.460 --> 02:20.800
 As you may know, I fast often, sometimes intermittent fasting for 16 hours, sometimes

02:20.800 --> 02:24.480
 24 hours, dinner to dinner, sometimes more.

02:24.480 --> 02:26.640
 I break the fast with Athletic Greens.

02:26.640 --> 02:30.960
 It's delicious, refreshing, just makes me feel good.

02:30.960 --> 02:35.440
 I think it's like 50 calories, less than a gram of sugar, but has a ton of nutrients

02:35.440 --> 02:40.280
 to make sure my body has what it needs, despite what I'm eating.

02:40.280 --> 02:49.240
 Go to athleticgreens.com.lex to claim a special offer of a free vitamin D3K2 for a year.

02:49.240 --> 02:53.280
 If you listen to the Joe Rogan experience, you might have listened to him rant about

02:53.280 --> 02:56.240
 how awesome vitamin D is for your immune system.

02:56.240 --> 02:57.680
 So there you have it.

02:57.680 --> 03:03.960
 So click the athleticgreens.com.lex in the description to get the free stuff and to

03:03.960 --> 03:06.520
 support this podcast.

03:06.520 --> 03:11.480
 This show is sponsored by BetterHelp, spelled H.E.L.P. Help.

03:11.480 --> 03:14.280
 Check it out at betterhelp.com.lex.

03:14.280 --> 03:17.680
 They figure out what you need and match you with a licensed professional therapist in

03:17.680 --> 03:18.680
 under 48 hours.

03:18.680 --> 03:24.080
 It's not a crisis line, it's not self help, it's professional counseling done securely

03:24.080 --> 03:25.080
 online.

03:25.080 --> 03:31.040
 I'm a bit from the David Goggins line of creatures and so have some demons to contend with, usually

03:31.040 --> 03:34.920
 on long runs or all nights full of self doubt.

03:34.920 --> 03:40.000
 I think suffering is essential for creation, but you can suffer beautifully in a way that

03:40.000 --> 03:41.720
 doesn't destroy you.

03:41.720 --> 03:45.400
 For most people, I think a good therapist can help on this.

03:45.400 --> 03:47.280
 So it's at least worth a try.

03:47.280 --> 03:52.720
 Check out their reviews, they're good, it's easy, private, affordable, available worldwide.

03:52.720 --> 03:58.440
 You can communicate by text and your time and schedule a weekly audio and video session.

03:58.440 --> 04:02.520
 Check it out at betterhelp.com.lex.

04:02.520 --> 04:06.440
 This show is presented by Cash App, the number one finance app in the App Store.

04:06.440 --> 04:09.480
 When you get it, use code LEX Podcast.

04:09.480 --> 04:12.960
 Cash App lets you send money to friends by Bitcoin and invest in the stock market with

04:12.960 --> 04:14.480
 as little as $1.

04:14.480 --> 04:19.120
 Since Cash App allows you to buy Bitcoin, let me mention that cryptocurrency in the context

04:19.120 --> 04:21.800
 of the history of money is fascinating.

04:21.800 --> 04:25.480
 I recommend Ascent of Money as a great book on this history.

04:25.480 --> 04:31.080
 Davidson credits on ledgers started around 30,000 years ago and the first decentralized

04:31.080 --> 04:34.280
 cryptocurrency released just over 10 years ago.

04:34.280 --> 04:39.400
 So given that history, cryptocurrency is still very much in its early days of development,

04:39.400 --> 04:44.960
 but it's still aiming to and just might redefine the nature of money.

04:44.960 --> 04:49.880
 So again, if you get Cash App from the App Store, Google Play and use code LEX Podcast,

04:49.880 --> 04:55.360
 you get $10 and Cash App will also donate $10 to FIRST, an organization that is helping

04:55.360 --> 05:00.520
 to advance robotics and STEM education for young people around the world.

05:00.520 --> 05:05.900
 And now, here's my conversation with David Eagleman.

05:05.900 --> 05:10.440
 You have a new book coming out on the changing brain.

05:10.440 --> 05:13.280
 Can you give a high level overview of the book?

05:13.280 --> 05:14.800
 It's called LiveWired, by the way.

05:14.800 --> 05:15.800
 Yeah.

05:15.800 --> 05:19.080
 The thing is, we typically think about the brain in terms of the metaphors we already

05:19.080 --> 05:24.000
 have like hardware and software, that's how we build all our stuff, but what's happening

05:24.000 --> 05:26.240
 in the brain is fundamentally so different.

05:26.240 --> 05:32.000
 So I coined this new term LiveWire, which is a system that's constantly reconfiguring

05:32.000 --> 05:37.160
 itself physically as it learns and adapts to the world around it.

05:37.160 --> 05:38.720
 It's physically changing.

05:38.720 --> 05:43.280
 So it's LiveWire meaning like hardware, but changing.

05:43.280 --> 05:44.680
 Yeah, exactly.

05:44.680 --> 05:47.600
 Well, the hardware and the software layers are blended.

05:47.600 --> 05:54.400
 And so typically, engineers are praised for their efficiency and making something really

05:54.400 --> 05:55.400
 clean and clear.

05:55.400 --> 05:57.800
 Okay, here's the hardware layer, then I'm going to run software on top of it, and there's

05:57.800 --> 06:02.880
 all sorts of universality that you get out of a piece of hardware like that that's useful.

06:02.880 --> 06:07.560
 But what the brain is doing is completely different, and I am so excited about where

06:07.560 --> 06:14.440
 this is all going because I feel like this is where our engineering will go.

06:14.440 --> 06:20.400
 So currently, we build all our devices a particular way, but I can't tear half the circuitry out

06:20.400 --> 06:23.960
 of your cell phone and expect it to still function.

06:23.960 --> 06:27.560
 But you can do that with the brain.

06:27.560 --> 06:32.680
 So just as an example, kids who are under about seven years old can get one half of their

06:32.680 --> 06:33.680
 brain removed.

06:33.680 --> 06:36.040
 It's called a Hemisphere Actomy.

06:36.040 --> 06:37.040
 And they're fine.

06:37.040 --> 06:40.880
 They have a slight limp on the other side of their body, but they can function just fine

06:40.880 --> 06:41.880
 that way.

06:41.880 --> 06:46.440
 And this is generally true, sometimes children are born without a hemisphere.

06:46.440 --> 06:53.240
 And their visual system rewires so that everything is on the single remaining hemisphere.

06:53.240 --> 06:59.720
 But thousands of cases like this teach us is that it's a very malleable system that

06:59.720 --> 07:05.520
 is simply trying to accomplish the tasks in front of it by rewiring itself with the available

07:05.520 --> 07:06.520
 real estate.

07:06.520 --> 07:11.320
 How much of that is a quirk or a feature of evolution?

07:11.320 --> 07:12.520
 How hard is it to engineer?

07:12.520 --> 07:15.720
 Because evolution took a lot of work.

07:15.720 --> 07:22.240
 Trillions of organisms had to die to create this thing we have in our skull.

07:22.240 --> 07:27.440
 As you said, you kind of look forward to the idea that we might be engineering our systems

07:27.440 --> 07:30.840
 like this in the future, like creating wide war systems.

07:30.840 --> 07:33.120
 How hard do you think is it to create systems like that?

07:33.120 --> 07:34.120
 Great question.

07:34.120 --> 07:37.200
 It has proven itself to be a difficult challenge.

07:37.200 --> 07:42.120
 What I mean by that is even though it's taken evolution a really long time to get where

07:42.120 --> 07:47.760
 it is now, all we have to do now is peek at the blueprints.

07:47.760 --> 07:50.920
 It's just three pounds, this organ, and we just figure out how to do it.

07:50.920 --> 07:56.240
 That's the part that I mean is a difficult challenge because there are tens of thousands

07:56.240 --> 07:57.240
 of neuroscientists.

07:57.240 --> 08:00.280
 We're all poking and prodding and trying to figure this out, but it's an extremely complicated

08:00.280 --> 08:01.280
 system.

08:01.280 --> 08:05.920
 But it's only going to be complicated until we figure out the general principles.

08:05.920 --> 08:10.840
 Exactly like if you had a magic camera and you could look inside the nucleus of a cell

08:10.840 --> 08:15.040
 and you'd see hundreds of thousands of things moving around or whatever, and then it takes

08:15.040 --> 08:18.680
 a quick and watch and say, oh, you're just trying to maintain the order of the base pairs

08:18.680 --> 08:20.600
 and all the rest is details.

08:20.600 --> 08:24.000
 Then it simplifies it and we come to understand something.

08:24.000 --> 08:27.880
 That was my goal in Livewire, which I've written over 10 years, by the way, is to try to distill

08:27.880 --> 08:34.480
 things down to the principles of what plastic systems are trying to accomplish.

08:34.480 --> 08:39.040
 But to even just linger, you said it's possible to be born with just one hemisphere and you

08:39.040 --> 08:42.120
 still are able to function.

08:42.120 --> 08:44.320
 First of all, just a pause on that.

08:44.320 --> 08:47.920
 That's amazing.

08:47.920 --> 08:51.840
 I don't know if people quite... I mean, you hear things here and there.

08:51.840 --> 08:57.800
 This is why I'm really excited about your book, because I don't know if there's definitive

08:57.800 --> 09:01.880
 popular sources to think about this stuff.

09:01.880 --> 09:07.320
 There's a lot of... I think from my perspective, what I heard is there's been debates over

09:07.320 --> 09:13.520
 decades about how much neuroplasticity there is in the brain and so on.

09:13.520 --> 09:17.560
 People have learned a lot of things and now it's converging towards people that are understanding

09:17.560 --> 09:21.680
 this much more plastic than people realize.

09:21.680 --> 09:28.600
 Just linger on that topic, how malleable is the hardware of the human brain?

09:28.600 --> 09:32.960
 Maybe you said children at each stage of life.

09:32.960 --> 09:33.960
 Here's the whole thing.

09:33.960 --> 09:38.760
 I think part of the confusion about plasticity has been that there are studies at all sorts

09:38.760 --> 09:44.160
 of different ages and then people might read that from a distance and they think, oh, well,

09:44.160 --> 09:49.040
 Fred didn't recover when half his brain was taken out and so clearly you're not plastic,

09:49.040 --> 09:54.720
 but then you do it with a child and they are plastic.

09:54.720 --> 09:59.200
 Part of my goal here was to pull together the tens of thousands of papers on this, both

09:59.200 --> 10:04.360
 from clinical work and from all the way down to the molecular and understand what are the

10:04.360 --> 10:05.360
 principles here.

10:05.360 --> 10:08.400
 The principles are that plasticity diminishes.

10:08.400 --> 10:09.400
 That's no surprise.

10:09.400 --> 10:11.640
 By the way, maybe I should just define plasticity.

10:11.640 --> 10:16.480
 It's the ability of a system to mold into a new shape and then hold that shape.

10:16.480 --> 10:22.840
 That's why we make things that we call plastic because they are moldable and they can hold

10:22.840 --> 10:26.000
 that new shape like a plastic toy or something.

10:26.000 --> 10:31.080
 Maybe we'll use a lot of terms that are synonymous.

10:31.080 --> 10:37.840
 Something is plastic, something is malleable, changing, live wire, the name of the book is

10:37.840 --> 10:38.840
 like...

10:38.840 --> 10:43.040
 I'll tell you exactly right, but I'll tell you why I chose live wire instead of plasticity.

10:43.040 --> 10:50.040
 I used the term plasticity in the book, but sparingly because that was a term coined by

10:50.040 --> 10:55.720
 William James over 100 years ago and he was, of course, very impressed with plastic manufacturing

10:55.720 --> 10:59.560
 that you could mold something in shape and then it holds that, but that's not what's

10:59.560 --> 11:01.560
 actually happening in the brain.

11:01.560 --> 11:03.720
 It's constantly rewiring your entire life.

11:03.720 --> 11:06.480
 You never hit an endpoint.

11:06.480 --> 11:08.840
 The whole point is for it to keep changing.

11:08.840 --> 11:12.600
 Even in the few minutes of conversation that we've been having, your brain is changing,

11:12.600 --> 11:15.680
 my brain is changing.

11:15.680 --> 11:19.800
 Next time I see your face, I will remember, oh yeah, that time Lex and I sat together

11:19.800 --> 11:21.400
 and we did these things.

11:21.400 --> 11:25.480
 I wonder if your brain will have a Lex thing going on for the next few months.

11:25.480 --> 11:29.440
 You'll stay there until you get rid of it because it was useful for now.

11:29.440 --> 11:30.920
 No, I'll probably never get rid of it.

11:30.920 --> 11:34.400
 Let's say for some circumstance, you and I don't see each other for the next 35 years.

11:34.400 --> 11:36.480
 When I run into you, I'll be like, oh yeah.

11:36.480 --> 11:37.480
 That looks familiar.

11:37.480 --> 11:43.160
 Yeah, we sat down for a podcast back when there were podcasts.

11:43.160 --> 11:46.600
 Back when we lived outside virtual reality.

11:46.600 --> 11:47.600
 Exactly.

11:47.600 --> 11:50.320
 So you chose LiveWire to mold your plastic.

11:50.320 --> 11:54.440
 Exactly, because plastic implies, I mean, it's the term that's used in the field and

11:54.440 --> 11:59.040
 so that's why we need to use it still for a while, but yeah, it implies something gets

11:59.040 --> 12:02.400
 mold into shape and then holds that shape forever, but in fact, the whole system is

12:02.400 --> 12:03.400
 completely changing.

12:03.400 --> 12:08.920
 And then back to how malleable is the human brain at each stage of life.

12:08.920 --> 12:14.000
 So what, just at a high level, is it malleable?

12:14.000 --> 12:20.280
 So yes, and plasticity diminishes, but one of the things that I felt like I was able

12:20.280 --> 12:25.560
 to put together for myself after reading thousands of papers on this issue is that different

12:25.560 --> 12:30.920
 parts of the brain have different plasticity windows.

12:30.920 --> 12:36.040
 So for example, with the visual cortex, that cements itself into place pretty quickly over

12:36.040 --> 12:37.960
 the course of a few years.

12:37.960 --> 12:41.120
 And I argue that's because of the stability of the data.

12:41.120 --> 12:44.800
 In other words, what you're getting in from the world, you've got a certain number of

12:44.800 --> 12:50.060
 angles, colors, shapes, you know, it's essentially the world is visually stable.

12:50.060 --> 12:52.560
 So that hardens around that data.

12:52.560 --> 12:56.480
 As opposed to let's say the somatosensory cortex, which is the part that's taking information

12:56.480 --> 13:00.400
 from your body or the motor cortex right next to it, which is what drives your body.

13:00.400 --> 13:01.920
 The fact is bodies are always changing.

13:01.920 --> 13:07.000
 You get taller over time, you get fatter, thinner over time, you might break a leg and

13:07.000 --> 13:08.680
 have to limp for a while, stuff like that.

13:08.680 --> 13:12.920
 So because the data there is always changing, by the way, you might get on a bicycle, you

13:12.920 --> 13:15.880
 might get a surfboard, things like that.

13:15.880 --> 13:19.400
 Because that data is always changing, that stays more malleable.

13:19.400 --> 13:25.000
 And when you look through the brain, you find that it appears to be the, you know, how stable

13:25.000 --> 13:28.120
 the data is determines how fast something hardens into place.

13:28.120 --> 13:31.680
 But the point is, different parts of the brain harden into place at different times.

13:31.680 --> 13:38.960
 Do you think it's possible that depending on how much data you get on different sensors,

13:38.960 --> 13:41.560
 that it stays more malleable longer?

13:41.560 --> 13:46.600
 So like, you know, if you look at different cultures, it's experienced like, if you keep

13:46.600 --> 13:50.260
 your eyes closed, or maybe you're blind, I don't know, but let's say you keep your eyes

13:50.260 --> 13:57.720
 closed for your entire life, then the visual cortex might be much less malleable.

13:57.720 --> 14:02.760
 So the reason I bring that up is like, you know, maybe we'll talk about brain, computer

14:02.760 --> 14:09.800
 interfaces a little bit down the line, but, you know, like, is this, is the malleability

14:09.800 --> 14:14.800
 a genetic thing, or is it more about the data, like you said, that comes in?

14:14.800 --> 14:18.040
 So the malleability itself is a genetic thing.

14:18.040 --> 14:22.600
 The big trick that Mother Nature discovered with humans is make a system that's really

14:22.600 --> 14:28.120
 flexible, as opposed to most other creatures to different degrees.

14:28.120 --> 14:34.240
 So if you take an alligator, it's born, its brain does the same thing every generation.

14:34.240 --> 14:37.400
 If you compare an alligator 100,000 years ago to an alligator now, they're essentially

14:37.400 --> 14:39.920
 the same.

14:39.920 --> 14:44.560
 We on the other hand, as humans drop into a world with a half baked brain, and what

14:44.560 --> 14:49.760
 we require is to absorb the culture around us, and the language and the beliefs and the

14:49.760 --> 14:56.520
 customs and so on, that's what Mother Nature has done with us, and it's been a tremendously

14:56.520 --> 15:00.200
 successful trick we've taken over the whole planet as a result of this.

15:00.200 --> 15:01.200
 So that's an interesting point.

15:01.200 --> 15:06.400
 I mean, just to link on it that, I mean, this is a nice feature, like if you were to design

15:06.400 --> 15:13.480
 a thing to survive in this world, do you put it at age zero already equipped to deal with

15:13.480 --> 15:18.960
 the world in a like hard coded way, or do you put it, do you make it malleable and just

15:18.960 --> 15:24.120
 throw it in, take the risk that you're maybe going to die, but you're going to learn a

15:24.120 --> 15:25.120
 lot in the process.

15:25.120 --> 15:29.360
 And if you don't die, you'll learn a hell of a lot to be able to survive in the environment.

15:29.360 --> 15:34.400
 So this is the experiment that Mother Nature ran, and it turns out that for better or worse,

15:34.400 --> 15:35.400
 we've won.

15:35.400 --> 15:38.560
 I mean, yeah, we put other animals into zoos, and we, yeah, that's right.

15:38.560 --> 15:39.560
 AI might do better.

15:39.560 --> 15:40.720
 Okay, fair enough.

15:40.720 --> 15:41.800
 That's true.

15:41.800 --> 15:46.760
 And maybe what the trick Mother Nature did is just the stepping stone to AI.

15:46.760 --> 15:51.200
 So that's a beautiful feature of the human brain that is malleable.

15:51.200 --> 15:56.520
 But let's, on the topic of Mother Nature, what do we start with?

15:56.520 --> 15:58.920
 Like how blank is the slate?

15:58.920 --> 16:01.160
 So it's not actually a blank slate.

16:01.160 --> 16:06.960
 What it's, it's terrific engineering that's set up in there, but much of that engineering

16:06.960 --> 16:10.240
 has to do with, okay, just make sure that things get to the right place.

16:10.240 --> 16:14.240
 For example, like the fibers from the eye is getting to the visual cortex or all this

16:14.240 --> 16:17.720
 very complicated machinery in the ear getting to the auditory cortex and so on.

16:17.720 --> 16:19.960
 So things, first of all, there's that.

16:19.960 --> 16:25.000
 And then what we also come equipped with is the ability to absorb language and culture

16:25.000 --> 16:27.120
 and beliefs and so on.

16:27.120 --> 16:28.640
 So you're already set up for that.

16:28.640 --> 16:33.160
 So no matter what you're exposed to, you will, you will absorb some sort of language.

16:33.160 --> 16:37.440
 That's the trick is how do you engineer something just enough that it's then a sponge that's

16:37.440 --> 16:40.160
 ready to take in and fill in the blanks?

16:40.160 --> 16:42.480
 How much of the malleability is hardware?

16:42.480 --> 16:43.480
 How much of software?

16:43.480 --> 16:45.160
 How much of that useful at all in the brain?

16:45.160 --> 16:46.960
 So like, what are we talking about?

16:46.960 --> 16:53.880
 So there's like, there's neurons, there's synapses and all kinds of different synapses

16:53.880 --> 16:58.920
 and there's chemical communication, like electrical signals and there's chemical communication

16:58.920 --> 17:03.520
 from those in the synapses.

17:03.520 --> 17:10.920
 What I would say the software would be the timing and the nature of the electrical signals,

17:10.920 --> 17:14.240
 I guess, and the hardware would be the actual synapses.

17:14.240 --> 17:15.240
 So here's the thing.

17:15.240 --> 17:19.440
 This is why really, if we can, I want to get away from the hardware and software metaphor

17:19.440 --> 17:25.160
 because what happens is as activity passes through the system, it changes things.

17:25.160 --> 17:30.360
 Now the thing that computer engineers are really used to thinking about is synapses where

17:30.360 --> 17:31.360
 two neurons connect.

17:31.360 --> 17:33.760
 Of course, each neuron connects with 10,000 of its neighbors.

17:33.760 --> 17:38.280
 But at a point where they connect, what we're all used to thinking about is the changing

17:38.280 --> 17:43.520
 of the strength of that connection, the synaptic weight.

17:43.520 --> 17:45.640
 But in fact, everything is changing.

17:45.640 --> 17:50.800
 The receptor distribution inside that neuron so that you're more or less sensitive to the

17:50.800 --> 17:55.460
 neurotransmitter, then the structure of the neuron itself and what's happening there

17:55.460 --> 18:00.480
 all the way down to biochemical cascades inside the cell, all the way down to the nucleus.

18:00.480 --> 18:05.200
 And for example, the epigenome, which is the, you know, these little proteins that are

18:05.200 --> 18:10.880
 attached to the DNA that cause conformational changes, that cause more genes to be expressed

18:10.880 --> 18:15.080
 or repressed, all of these things are plastic.

18:15.080 --> 18:20.360
 The reason that most people only talk about the synaptic weights is because that's really

18:20.360 --> 18:22.160
 all we can measure well.

18:22.160 --> 18:25.000
 And all this other stuff is really, really hard to see with our current technology.

18:25.000 --> 18:27.440
 So essentially that just gets ignored.

18:27.440 --> 18:34.160
 But in fact, the system is plastic at all these different levels and my way of thinking

18:34.160 --> 18:39.000
 about this is an analogy to paste layers.

18:39.000 --> 18:44.160
 So paste layers is a concept that Stewart Brand suggested about how to think about cities.

18:44.160 --> 18:47.560
 So you have fashion, which changes rapidly in cities.

18:47.560 --> 18:52.960
 You have governance, which changes more slowly.

18:52.960 --> 18:56.580
 You have the structure, the buildings of a city, which changes more slowly all the way

18:56.580 --> 18:58.800
 down to nature.

18:58.800 --> 19:02.320
 You've got all these different layers of things that are changing at different paces at different

19:02.320 --> 19:03.320
 speeds.

19:03.320 --> 19:08.880
 I've taken that idea and mapped it onto the brain, which is to say you have some biochemical

19:08.880 --> 19:12.040
 class gays that are just changing really rapidly when something happens all the way down to

19:12.040 --> 19:15.240
 things that are more and more cemented in there.

19:15.240 --> 19:20.880
 And this is actually, this actually allows us to understand a lot about particular kinds

19:20.880 --> 19:21.880
 of things that happen.

19:21.880 --> 19:26.040
 For example, one of the oldest, probably the oldest rule in neurology is called Rybo's

19:26.040 --> 19:30.200
 law, which is that older memories are more stable than newer memories.

19:30.200 --> 19:36.800
 So when you get old and demented, you'll be able to remember things from your young life.

19:36.800 --> 19:40.320
 Maybe you'll remember this podcast, but you won't remember what you did a month ago or

19:40.320 --> 19:42.040
 a year ago.

19:42.040 --> 19:43.960
 And this is a very weird structure, right?

19:43.960 --> 19:49.040
 No other system works this way where older memories are more stable than newer memories.

19:49.040 --> 19:55.080
 But it's because through time, things get more and more cemented into deeper layers

19:55.080 --> 19:58.320
 of the system.

19:58.320 --> 20:02.560
 And so this is, I think, the way we have to think about the brain, not as, okay, you've

20:02.560 --> 20:05.360
 got neurons, you've got synaptic weights, and that's it.

20:05.360 --> 20:06.360
 So yeah.

20:06.360 --> 20:17.400
 So the idea of live wear and live wired is that it's a gradual spectrum between software

20:17.400 --> 20:18.400
 and hardware.

20:18.400 --> 20:23.480
 And so the metaphors completely doesn't make sense because when you talk about software

20:23.480 --> 20:26.400
 and hardware, it's really hard lines.

20:26.400 --> 20:34.320
 I mean, of course, software is unlike hardware, but even hardware.

20:34.320 --> 20:38.800
 But like, so there's two groups, but in the software world, there's levels of abstractions,

20:38.800 --> 20:39.800
 right?

20:39.800 --> 20:44.600
 There's the operating system, there's machine code, and then it gets higher and higher levels.

20:44.600 --> 20:48.800
 But somehow that's actually fundamentally different than the layers of abstractions

20:48.800 --> 20:49.800
 in the hardware.

20:49.800 --> 20:55.360
 But in the brain, it's all like the same and all of the city, the city metaphor.

20:55.360 --> 21:01.520
 I mean, yeah, it's kind of mind blowing because it's hard to know what to think about that.

21:01.520 --> 21:07.600
 Like if I were to ask the question, this is important question for machine learning is,

21:07.600 --> 21:09.880
 how does the brain learn?

21:09.880 --> 21:17.040
 So essentially, you're saying that, I mean, it just learns on all of these different levels

21:17.040 --> 21:19.480
 at all different paces.

21:19.480 --> 21:20.480
 Exactly right.

21:20.480 --> 21:25.320
 And as a result, what happens is as you practice something, get good at something, you're physically

21:25.320 --> 21:31.160
 changing the circuitry, you're adapting your brain around the thing that is relevant to

21:31.160 --> 21:32.160
 you.

21:32.160 --> 21:34.600
 So let's say you take up, do you know how to surf?

21:34.600 --> 21:35.600
 Nope.

21:35.600 --> 21:36.600
 Okay, great.

21:36.600 --> 21:37.600
 So let's say you take up surfing.

21:37.600 --> 21:38.600
 Yeah.

21:38.600 --> 21:41.480
 Now, at this age, what happens is, you'll be terrible at first, you don't know how to

21:41.480 --> 21:44.080
 operate your body, you don't know how to read the waves, things like that.

21:44.080 --> 21:45.720
 And through time, you get better and better.

21:45.720 --> 21:48.600
 What you're doing is you're burning that into the actual circuitry of your brain.

21:48.600 --> 21:51.640
 You're, of course, conscious when you're first doing it, you're thinking about, okay,

21:51.640 --> 21:52.640
 where am I doing?

21:52.640 --> 21:53.760
 What's my body weight?

21:53.760 --> 21:57.240
 But eventually, when you become a pro at it, you are not conscious of it at all.

21:57.240 --> 22:00.320
 In fact, you can't even unpack what it is that you did.

22:00.320 --> 22:02.040
 Think about riding a bicycle.

22:02.040 --> 22:04.360
 You can't describe how you're doing it.

22:04.360 --> 22:08.200
 You're just doing it or changing your balance when you do this to go to a stop and so on.

22:08.200 --> 22:14.560
 So this is what we're constantly doing, is actually shaping our own circuitry based

22:14.560 --> 22:16.000
 on what is relevant for us.

22:16.000 --> 22:18.880
 Survival, of course, being the top thing that's relevant.

22:18.880 --> 22:23.720
 But interestingly, especially with humans, we have these particular goals in our lives,

22:23.720 --> 22:25.720
 under science, neuroscience, whatever.

22:25.720 --> 22:28.320
 And so we actually shape our circuitry around that.

22:28.320 --> 22:34.240
 I mean, you mentioned this, get slower and slower with age, but is there, I think I've

22:34.240 --> 22:41.480
 read and spoken offline, even on this podcast, with a developmental neurobiologist, I guess

22:41.480 --> 22:47.160
 would be the right terminology, is like looking at the very early, like from embryonic stem

22:47.160 --> 22:50.560
 cells to creation of the brain.

22:50.560 --> 22:55.160
 And that's mind blowing, how much stuff happens there.

22:55.160 --> 23:00.520
 So it's very malleable at that stage.

23:00.520 --> 23:04.720
 But after that, at which point does it stop being malleable?

23:04.720 --> 23:08.560
 So that's the interesting thing, is that it remains malleable your whole life.

23:08.560 --> 23:13.200
 So even when you're an old person, you'll be able to remember new faces and names.

23:13.200 --> 23:16.040
 You'll be able to learn new sorts of tasks.

23:16.040 --> 23:19.480
 And thank goodness, because the world is changing rapidly in terms of technology and so on.

23:19.480 --> 23:23.800
 I just sent my mother and Alexa and she figured out how to go on the settings and do the thing

23:23.800 --> 23:27.160
 and I was really impressed by it, that she was able to do it.

23:27.160 --> 23:30.640
 So there are parts of the brain that remain malleable their whole life.

23:30.640 --> 23:36.200
 The interesting part is that really your goal is to make an internal model of the world.

23:36.200 --> 23:42.800
 Your goal is to say, okay, the brain is trapped in silence and darkness and it's trying to

23:42.800 --> 23:45.720
 understand how the world works out there, right?

23:45.720 --> 23:47.200
 I love that image.

23:47.200 --> 23:48.720
 Yeah, I guess it is.

23:48.720 --> 23:49.720
 You forget.

23:49.720 --> 23:50.720
 You forget.

23:50.720 --> 23:56.400
 It's like this lonely thing is sitting in its own container and trying to actually throw

23:56.400 --> 23:58.920
 a few sensors, figure out what the hell's going on.

23:58.920 --> 24:06.520
 You know what I sometimes think about is that movie, The Martian with Matt Damon, the movie

24:06.520 --> 24:11.400
 poster shows Matt Damon all alone on the red planet and I think, God, that's actually what

24:11.400 --> 24:18.560
 it's like to be inside your head and my head and anybody's head is that you're essentially

24:18.560 --> 24:22.380
 on your own planet in there and I'm essentially on my own planet and everyone's got their

24:22.380 --> 24:28.000
 own world where you've absorbed all of your experiences up to this moment in your life

24:28.000 --> 24:33.560
 that made you exactly who you are and same for me and everyone.

24:33.560 --> 24:38.840
 And we've got this very thin bandwidth of communication and I'll say something like,

24:38.840 --> 24:43.160
 oh yeah, that tastes just like peaches and you'll say, oh, I know what you mean.

24:43.160 --> 24:47.920
 But the experience, of course, might be vastly different for us.

24:47.920 --> 24:48.920
 But anyway, yes.

24:48.920 --> 24:52.040
 So the brain is trapped in silence and darkness, each one of us.

24:52.040 --> 24:55.720
 And what it's trying to do, this is the important part, is trying to make an internal model

24:55.720 --> 24:59.000
 of what's going on out there as in how do I function in the world?

24:59.000 --> 25:00.960
 How do I interact with other people?

25:00.960 --> 25:04.160
 Do I say something nice or polite or do I say something aggressive and mean?

25:04.160 --> 25:08.880
 Do I, you know, all these things that it's putting together about the world.

25:08.880 --> 25:14.880
 And I think what happens when people get older and older, it may not be that plasticity is

25:14.880 --> 25:15.880
 diminishing.

25:15.880 --> 25:20.320
 It may be that their internal model essentially has set itself up in a way where it says,

25:20.320 --> 25:23.840
 OK, I've pretty much got a really good understanding of the world now and I don't really need to

25:23.840 --> 25:25.560
 change, right?

25:25.560 --> 25:30.920
 So when old, when much older people find themselves in a situation where they need to change,

25:30.920 --> 25:33.480
 they can actually are able to do it.

25:33.480 --> 25:37.240
 It's just that I think this notion that we all have that plasticity diminishes as we

25:37.240 --> 25:42.160
 grow older is in part because the motivation isn't there.

25:42.160 --> 25:45.920
 But if you were 80 and you get fired from your job and suddenly had to figure out how

25:45.920 --> 25:49.200
 to program a WordPress site or something, you'd figure it out.

25:49.200 --> 25:50.200
 Got it.

25:50.200 --> 25:53.400
 So the the capability, the possibility of change is there.

25:53.400 --> 26:01.080
 But let me ask the highest challenge, the interesting challenge to this plasticity,

26:01.080 --> 26:03.600
 to this liveware system.

26:03.600 --> 26:09.520
 If we could talk about brain computer interfaces and Neuralink, what are your thoughts about

26:09.520 --> 26:16.360
 the efforts of Elon Musk, Neuralink, PCI in general, in this regard, which is adding

26:16.360 --> 26:22.920
 a machine, a computer, the capability of a computer to communicate with the brain and

26:22.920 --> 26:27.360
 the brain to communicate with the computer at the very basic applications and then like

26:27.360 --> 26:29.080
 the futuristic kind of thoughts.

26:29.080 --> 26:30.080
 Yeah.

26:30.080 --> 26:32.960
 First of all, it's terrific that people are jumping into doing that because it's clearly

26:32.960 --> 26:34.520
 the the future.

26:34.520 --> 26:39.000
 The interesting part is our brains have pretty good methods of interacting with technology.

26:39.000 --> 26:43.680
 So maybe it's your fat thumbs on a cell phone or something, but or maybe it's watching

26:43.680 --> 26:46.000
 a YouTube video and getting into your eye that way.

26:46.000 --> 26:50.160
 But we have pretty rapid ways of communicating with technology and getting data.

26:50.160 --> 26:56.760
 So if you actually crack open the skull and go into the inner sanctum of the brain, you

26:56.760 --> 26:58.360
 might be able to get a little bit faster.

26:58.360 --> 27:06.960
 But I'll tell you, I'm not so sanguine on the future of that as a business and I'll tell

27:06.960 --> 27:12.920
 you why it's because there are various ways of getting data in and out and an open head

27:12.920 --> 27:14.680
 surgery is a big deal.

27:14.680 --> 27:18.440
 Neurosurgeons don't want to do it because there's always risk of death and infection

27:18.440 --> 27:19.960
 on the table.

27:19.960 --> 27:24.480
 And also it's not clear how many people would say I'm going to volunteer to get something

27:24.480 --> 27:29.880
 in my head so that I can text faster, you know, 20% faster.

27:29.880 --> 27:36.480
 So I think it's, you know, mother nature surrounds the brain with this armored, you know, bunker

27:36.480 --> 27:39.640
 of the skull because it's a very delicate material.

27:39.640 --> 27:46.880
 And there's an expression in neurosurgery about the brain is, you know, the person is

27:46.880 --> 27:49.000
 never the same after you open up their skull.

27:49.000 --> 27:53.480
 Now, whether or not that's true or whatever, who cares, but it's a big deal to do an open

27:53.480 --> 27:54.480
 head surgery.

27:54.480 --> 27:58.520
 So what I'm interested in is how can we get information in and out of the brain without

27:58.520 --> 28:00.760
 having to crack the skull open?

28:00.760 --> 28:01.760
 Got it.

28:01.760 --> 28:08.120
 We're messing with the biological part, like directly connecting or messing with the intricate

28:08.120 --> 28:11.640
 biological thing that we got going on, it seems to be working.

28:11.640 --> 28:12.640
 Yeah, exactly.

28:12.640 --> 28:17.680
 And by the way, where NeuroLink is going, which is wonderful is going to be in patient

28:17.680 --> 28:22.000
 cases, it really matters for all kinds of surgeries that a person needs, whether for

28:22.000 --> 28:24.080
 Parkinson's or epilepsy or whatever.

28:24.080 --> 28:28.600
 It's a terrific new technology for essentially sewing electrodes in there and getting more

28:28.600 --> 28:29.960
 higher density of electrodes.

28:29.960 --> 28:30.960
 So that's great.

28:30.960 --> 28:37.600
 I just don't think as far as the future of BCI goes, I don't suspect that people will

28:37.600 --> 28:40.600
 go in and say, yeah, drill a hole in my head and do that.

28:40.600 --> 28:45.160
 Well, it's interesting because I think there's a similar intuition, but I say in the world

28:45.160 --> 28:51.560
 of autonomous vehicles, that folks know how hard it is and it seems damn impossible.

28:51.560 --> 28:55.960
 The similar intuition about I'm sticking on the Elon Musk thing is just a good, easy

28:55.960 --> 28:56.960
 example.

28:56.960 --> 29:01.960
 There's a similar intuition about colonizing Mars, if you really think about it, it seems

29:01.960 --> 29:10.760
 extremely difficult and almost, I mean, just technically difficult to a degree where you

29:10.760 --> 29:14.960
 want to ask, is it really worth doing, worth trying?

29:14.960 --> 29:22.560
 And then the same is applied with BCI, but the thing about the future is it's hard to

29:22.560 --> 29:23.560
 predict.

29:23.560 --> 29:29.960
 So the exciting thing to me with, so once it does, once if successful, it's able to

29:29.960 --> 29:37.680
 help patients, it may be able to discover something very surprising about our ability

29:37.680 --> 29:39.680
 to directly communicate with the brain.

29:39.680 --> 29:46.800
 So exactly what you're interested in is figuring out how to play with this malleable brain,

29:46.800 --> 29:49.400
 but like help assist it somehow.

29:49.400 --> 29:54.320
 I mean, it's such a compelling notion to me that we're now working on all these exciting

29:54.320 --> 30:00.120
 machine learning systems that are able to learn from data.

30:00.120 --> 30:08.800
 And then if we can have this other brain that's a learning system that's live wired on the

30:08.800 --> 30:14.560
 human side and them to be able to communicate, it's like a self play mechanism was able to

30:14.560 --> 30:21.560
 beat the world champion ago so they can play with each other, the computer and the brain,

30:21.560 --> 30:22.560
 like when you sleep.

30:22.560 --> 30:27.720
 I mean, there's a lot of futuristic kind of things that it's just exciting possibilities,

30:27.720 --> 30:34.000
 but I hear you, we understand so little about the actual intricacies of the communication

30:34.000 --> 30:38.680
 of the brain that it's hard to find the common language.

30:38.680 --> 30:47.080
 Well, interestingly, the technologies that have been built don't actually require the

30:47.080 --> 30:48.400
 perfect common language.

30:48.400 --> 30:52.440
 So for example, hundreds of thousands of people are walking around with artificial ears and

30:52.440 --> 30:57.040
 artificial eyes, meaning cochlear implants or retinal implants.

30:57.040 --> 31:01.200
 So this is, you know, you take essentially digital microphone, you slip an electrode

31:01.200 --> 31:05.640
 stripped into the inner ear, and people can learn how to hear that way, or you take an

31:05.640 --> 31:10.040
 electrode grid and you plug it into the retina at the back of the eye, and people can learn

31:10.040 --> 31:11.200
 how to see that way.

31:11.200 --> 31:17.080
 The interesting part is those devices don't speak exactly the natural biological language,

31:17.080 --> 31:20.440
 they speak the dialect of Silicon Valley.

31:20.440 --> 31:25.160
 And it turns out that as recently as about 25 years ago, a lot of people thought this

31:25.160 --> 31:26.560
 was never going to work.

31:26.560 --> 31:30.400
 They thought it wasn't going to work for that reason, but the brain figures it out.

31:30.400 --> 31:34.080
 It's really good at saying, okay, look, there's some correlation between what I can touch

31:34.080 --> 31:38.700
 and feel and hear and so on, and the data that's coming in or between, you know, I

31:38.700 --> 31:43.000
 clap my hands and I have signals coming in there, and it figures out how to speak any

31:43.000 --> 31:44.000
 language.

31:44.000 --> 31:45.080
 Oh, that's fascinating.

31:45.080 --> 31:52.880
 So like no matter if it's Neuralink, so directly communicating with the brain or it's a smartphone

31:52.880 --> 31:58.760
 or Google Glass, or the brain figures out the efficient way of communication.

31:58.760 --> 32:00.520
 Well, exactly, exactly.

32:00.520 --> 32:07.120
 And what I propose is the potato head theory of evolution, which is that our eyes and nose

32:07.120 --> 32:10.960
 and mouth and ears and fingertips, all this stuff is just plug and play.

32:10.960 --> 32:14.360
 And the brain can figure out what to do with the data that comes in.

32:14.360 --> 32:19.120
 And part of the reason that I think this is right, and I care so deeply about this, is

32:19.120 --> 32:22.100
 when you look across the animal kingdom, you find all kinds of weird peripheral devices

32:22.100 --> 32:25.800
 plugged in, and the brain figures out what to do with the data.

32:25.800 --> 32:31.480
 And I don't believe that Mother Nature has to reinvent the principles of brain operation

32:31.480 --> 32:34.720
 each time, to say, oh, now I'm going to have heat pits to detect infrared, now I'm going

32:34.720 --> 32:39.640
 to have something to detect, you know, electro receptors on the body, now I'm going to detect

32:39.640 --> 32:44.080
 something to pick up the magnetic field of the earth with cryptochromes in the eye.

32:44.080 --> 32:47.280
 And so instead, the brain says, oh, I got it, there's data coming in.

32:47.280 --> 32:48.280
 Is that useful?

32:48.280 --> 32:49.280
 Can I use something with it?

32:49.280 --> 32:50.280
 Oh, great.

32:50.280 --> 32:52.280
 I'm going to mold myself around the data that's coming in.

32:52.280 --> 32:57.640
 It's kind of fascinating to think that we think of smartphones and all this new technology

32:57.640 --> 33:04.000
 as novel, as totally novel, as outside of what evolution ever intended, or like what

33:04.000 --> 33:08.960
 nature ever intended, it's fascinating to think that like, the entirety of the process

33:08.960 --> 33:14.840
 of evolution is perfectly fine and ready for the smartphone, and the internet, like it's

33:14.840 --> 33:20.520
 ready, it's ready to be valuable to that, and whatever comes to cyborgs, to virtual

33:20.520 --> 33:24.720
 reality, we kind of think like, this is, you know, there's all these like books written

33:24.720 --> 33:31.200
 about natural, what's natural, and we're like destroying our natural cells by like embracing

33:31.200 --> 33:35.880
 all this technology, it's kind of, you know, we're not probably not giving the brain enough

33:35.880 --> 33:40.200
 credit, like, this thing is just fine with new tech.

33:40.200 --> 33:41.200
 Oh, exactly.

33:41.200 --> 33:44.480
 It wraps yourself around, and by the way, wait till you have kids, you'll see the ease

33:44.480 --> 33:46.440
 with which they pick up on stuff.

33:46.440 --> 33:47.440
 Yeah.

33:47.440 --> 33:54.560
 Yeah, as Kevin Kelly said, technology is what gets invented after you're born.

33:54.560 --> 33:57.680
 But the stuff that already exists when you're born, that's not even tech, that's just background

33:57.680 --> 34:01.400
 furniture, like the fact that the iPad exists for my son and daughter, like that's just

34:01.400 --> 34:02.400
 background furniture.

34:02.400 --> 34:09.560
 So, yeah, it's, because we have this incredibly malleable system, it just absorbs whatever

34:09.560 --> 34:11.800
 is going on in the world and learns what to do with it.

34:11.800 --> 34:17.280
 So do you think, just to link up for a little bit more, do you think it's perfect?

34:17.280 --> 34:27.280
 It's possible to coadjust, like, we're kind of, you know, for the machine to adjust to

34:27.280 --> 34:31.040
 the brain, for the brain to adjust to the machine, I guess that's what's already happening.

34:31.040 --> 34:32.040
 Sure.

34:32.040 --> 34:33.040
 That is what's happening.

34:33.040 --> 34:37.080
 So, for example, when you put electrodes in the motor cortex to control a robotic arm

34:37.080 --> 34:41.400
 for somebody who's paralyzed, the engineers do a lot of work to figure out, okay, what

34:41.400 --> 34:45.600
 can we do with the algorithm here so that we can detect what's going on from these cells

34:45.600 --> 34:50.960
 and figure out how to best program the robotic arm to move, given the data that we're measuring

34:50.960 --> 34:52.120
 from these cells.

34:52.120 --> 34:54.640
 But also, the brain is learning too.

34:54.640 --> 34:59.000
 So, you know, the paralyzed woman says, wait, I'm trying to grab this thing.

34:59.000 --> 35:01.000
 And by the way, it's all about relevance.

35:01.000 --> 35:07.400
 So if there's a piece of food there and she's hungry, she'll figure out how to get this food

35:07.400 --> 35:11.840
 into her mouth with the robotic arm, because that is what matters.

35:11.840 --> 35:17.680
 Well, that's, okay, first of all, that pain is really promising and beautiful.

35:17.680 --> 35:24.160
 For some reason, a really optimistic picture that, you know, our brain is able to adjust

35:24.160 --> 35:30.720
 to so much that, you know, so many things happen this year, 2020, that you think, like,

35:30.720 --> 35:33.480
 how we're ever going to deal with it.

35:33.480 --> 35:40.720
 It's somehow encouraging and inspiring that, like, we're going to be okay.

35:40.720 --> 35:41.720
 Well, that's right.

35:41.720 --> 35:47.240
 I actually think so 2020 has been an awful year for almost everybody in many ways.

35:47.240 --> 35:52.520
 But the one silver lining has to do with brain plasticity, which is to say, we've all been

35:52.520 --> 35:58.040
 on our, you know, on our gerbil wheels, we've all been in our routines and, and, you know,

35:58.040 --> 36:02.360
 as I mentioned, our internal models are all about how do you maximally succeed?

36:02.360 --> 36:07.440
 How do you optimize your operation in this circumstance where you are, right?

36:07.440 --> 36:12.160
 And then all of a sudden, bang, 2020 comes, we're completely off our wheels, we're having

36:12.160 --> 36:16.240
 to create new things all the time and figure out how to do it.

36:16.240 --> 36:21.840
 And that is terrific for brain plasticity because, and we know this because there are

36:21.840 --> 36:28.200
 very large studies on older people who stay cognitively active their whole lives.

36:28.200 --> 36:33.360
 Some, some fraction of them have Alzheimer's disease physically, but nobody knows that

36:33.360 --> 36:37.240
 when they're alive, even though their brain is getting chewed up with the ravages of

36:37.240 --> 36:40.120
 Alzheimer's, cognitively they're doing just fine.

36:40.120 --> 36:41.120
 Why?

36:41.120 --> 36:44.200
 It's because they're, they're, they're challenged all the time.

36:44.200 --> 36:47.400
 They've got all these new things going on, all this novelty, all these responsibilities,

36:47.400 --> 36:49.800
 chores, social life, all these things happening.

36:49.800 --> 36:54.880
 And as a result, they're constantly building new roadways, even as parts degrade.

36:54.880 --> 36:59.360
 And, and, and that's the only good news is that we are in a situation where suddenly

36:59.360 --> 37:04.440
 we can't just operate like automata anymore, we have to think of completely new ways to

37:04.440 --> 37:05.440
 do things.

37:05.440 --> 37:07.840
 And that's wonderful.

37:07.840 --> 37:11.240
 I don't know why this question popped into my head.

37:11.240 --> 37:17.480
 It's quite absurd, but are we going to be okay?

37:17.480 --> 37:20.760
 You said this, it's a promising civil lining just from your own, because you've written

37:20.760 --> 37:25.400
 about this and thought about this outside of maybe even the plasticity of the brain,

37:25.400 --> 37:33.400
 but just this whole pandemic kind of changed the way it knocked us out of this hamster

37:33.400 --> 37:35.560
 wheel like that of habit.

37:35.560 --> 37:42.160
 A lot of people had to reinvent themselves, unfortunately, and I have a lot of friends

37:42.160 --> 37:49.800
 who either already or, or are going to lose their business, you know, is basically it's

37:49.800 --> 37:56.680
 taking the dreams that people have had and said, like, said this, this dream, this particular

37:56.680 --> 37:59.440
 dream you've had will no longer be possible.

37:59.440 --> 38:02.440
 So you have to find something new.

38:02.440 --> 38:06.000
 What are your, are we going to be okay?

38:06.000 --> 38:11.040
 Yeah, we'll be okay in the sense that, I mean, it's going to be a rough time for many or

38:11.040 --> 38:19.760
 most people, but in the sense that it is sometimes useful to find that what you thought was your

38:19.760 --> 38:24.760
 dream was not the thing that you're going to do.

38:24.760 --> 38:27.920
 This is obviously the plot in lots of Hollywood movies that someone says, I'm going to do

38:27.920 --> 38:31.200
 this, and then that gets foiled and they end up doing something better.

38:31.200 --> 38:39.640
 But this is true in life, I mean, in general, even though we plan our lives as best we can,

38:39.640 --> 38:44.160
 it's predicated on our notion of, okay, given everything that's around me, this is what's

38:44.160 --> 38:47.200
 possible for me next.

38:47.200 --> 38:51.040
 But it takes 20, 20 to knock you off that where you think, oh, well, actually, maybe

38:51.040 --> 38:53.960
 there's something I can be doing that's bigger, that's better.

38:53.960 --> 38:59.760
 Yeah, you know, for me, one exciting thing, and I just talked to Grant Sanderson, I don't

38:59.760 --> 39:03.880
 know if you know who he is, he's a three blue one brown, it's a YouTube channel.

39:03.880 --> 39:08.360
 He does, he's a, if you see it, you would recognize it.

39:08.360 --> 39:13.520
 He's like a really famous math guy, and he's a math educator, and he does these incredible

39:13.520 --> 39:15.160
 beautiful videos.

39:15.160 --> 39:20.800
 And now I see sort of at MIT, folks are struggling to try to figure out, you know, if we do teach

39:20.800 --> 39:23.680
 remotely, how do we do it effectively?

39:23.680 --> 39:31.160
 You have these world class researchers and professors trying to figure out how to put

39:31.160 --> 39:34.000
 content online that teaches people.

39:34.000 --> 39:42.440
 And to me, a possible future of that is, you know, Nobel Prize winning faculty become

39:42.440 --> 39:43.440
 YouTubers.

39:43.440 --> 39:50.800
 Like, like that, that to me is so exciting, like what Grant said, which is like the possibility

39:50.800 --> 39:55.360
 of creating canonical videos on the thing you're a world expert in, you know, there's

39:55.360 --> 40:02.200
 so many topics, it just, the world doesn't, you know, there's faculty, I mentioned Tedric,

40:02.200 --> 40:07.960
 there's all these people in robotics that are experts in a particular beautiful field,

40:07.960 --> 40:11.080
 on which there's only just papers.

40:11.080 --> 40:17.080
 There's no popular book, there's no, there's no clean canonical video showing the beauty

40:17.080 --> 40:18.360
 of a subject.

40:18.360 --> 40:25.560
 And one possibility is they try to create that and share it with the world.

40:25.560 --> 40:26.560
 This is the beautiful thing.

40:26.560 --> 40:28.760
 This, of course, has been happening for a while already.

40:28.760 --> 40:33.000
 I mean, for example, when I go and I give book talks, often what will happen is some

40:33.000 --> 40:36.640
 13 year old will come up to me afterwards and say something, and I'll say, my God, that

40:36.640 --> 40:37.640
 was so small.

40:37.640 --> 40:38.640
 Like, how did you know that?

40:38.640 --> 40:40.680
 And they'll say, oh, I saw it on a TED talk.

40:40.680 --> 40:43.000
 Well, what an amazing opportunity.

40:43.000 --> 40:50.560
 We got the best person in the world on subject X giving a 15 minute talk as beautifully as

40:50.560 --> 40:52.080
 he or she can.

40:52.080 --> 40:53.560
 And the 13 year old just grows up with that.

40:53.560 --> 40:55.280
 That's just the mother's milk, right?

40:55.280 --> 41:01.680
 As opposed to when we grew up, you know, I had whatever homeroom teacher I had and, you

41:01.680 --> 41:06.320
 know, whatever classmates I had and hopefully that person knew what he or she was teaching

41:06.320 --> 41:08.880
 and often didn't and, you know, just made things up.

41:08.880 --> 41:14.480
 So the opportunity now has become extraordinary to get the best of the world.

41:14.480 --> 41:18.880
 And the reason this matters, of course, is because obviously, back to plasticity, the

41:18.880 --> 41:24.480
 way that we, the way our brain gets molded is by absorbing everything from the world,

41:24.480 --> 41:28.680
 all of the knowledge and the data and so on that it can get.

41:28.680 --> 41:33.160
 And then springboarding off of that.

41:33.160 --> 41:40.440
 And we're in a very lucky time now because we grew up with a lot of just in case learning.

41:40.440 --> 41:43.760
 So you know, just in case you ever need to know these dates in Mongolian history, here

41:43.760 --> 41:44.760
 there.

41:44.760 --> 41:48.840
 But what kids are grown up with now, like my kids, is tons of just in time learning.

41:48.840 --> 41:51.880
 So as soon as they're curious about something, they ask Alexa, they ask Google Home, they

41:51.880 --> 41:54.640
 get the answer right there in the context of the curiosity.

41:54.640 --> 42:00.680
 The reason this matters is because for plasticity to happen, you need to carry, you need to

42:00.680 --> 42:02.760
 be curious about something.

42:02.760 --> 42:06.400
 And this is something, by the way, that the ancient Romans had noted.

42:06.400 --> 42:09.280
 They had outlined seven different levels of learning and the highest level is when you're

42:09.280 --> 42:10.720
 curious about a topic.

42:10.720 --> 42:15.080
 But anyway, so kids now are getting tons of just in time learning.

42:15.080 --> 42:18.080
 And as a result, they're going to be so much smarter than we are.

42:18.080 --> 42:19.800
 They're just, and we can already see that.

42:19.800 --> 42:22.360
 I mean, my boy is eight years old, my girl is five.

42:22.360 --> 42:27.840
 But I mean, the things that he knows are amazing because it's not just him having to do the

42:27.840 --> 42:29.960
 rote numberization stuff that we did.

42:29.960 --> 42:33.840
 Yeah, that's just fascinating with the brain, what young brains look like now, because of

42:33.840 --> 42:37.080
 all those TED Talks, just to just load it in there.

42:37.080 --> 42:42.560
 And there's also, I mean, a lot of people, right, kind of, there's a sense that our attention

42:42.560 --> 42:45.000
 span is growing shorter.

42:45.000 --> 42:51.200
 But it's complicated because, for example, most people, majority of people, it's the

42:51.200 --> 42:57.080
 80 plus percent of people listen to the entirety of these things, two, three hours for podcasts,

42:57.080 --> 43:01.040
 long form podcasts, or are becoming more and more popular.

43:01.040 --> 43:04.280
 So like that's, it's all really giant, complicated mess.

43:04.280 --> 43:10.680
 And the point is that the brain is able to adjust to it and somehow like form a world

43:10.680 --> 43:19.720
 view within this new medium of like information that we have, you have like these short tweets

43:19.720 --> 43:24.920
 and you have these three, four hour podcasts and you have Netflix movie.

43:24.920 --> 43:30.040
 I mean, it's just adjusting to the entirety and just absorbing it and taking it all in

43:30.040 --> 43:38.360
 and then pops up COVID that forces us all to be home and it all just adjusts and figures

43:38.360 --> 43:39.360
 it out.

43:39.360 --> 43:40.360
 Yeah, yeah, exactly.

43:40.360 --> 43:41.360
 It's fascinating.

43:41.360 --> 43:48.360
 You know, we've been talking about the brain as if it's something separate from the human

43:48.360 --> 43:53.280
 that carries it a little bit, like whenever you talk about the brain, it's easy to forget

43:53.280 --> 43:57.200
 that that's us.

43:57.200 --> 44:05.320
 How much is the whole thing predetermined?

44:05.320 --> 44:20.320
 How much is it already encoded in there and how much is it, the actions, the decisions,

44:20.320 --> 44:21.320
 the judgments.

44:21.320 --> 44:23.360
 You mean like who you are?

44:23.360 --> 44:24.360
 Who you are.

44:24.360 --> 44:25.360
 Oh, yeah, yeah.

44:25.360 --> 44:26.360
 Okay, great question.

44:26.360 --> 44:27.360
 Right.

44:27.360 --> 44:30.920
 So there used to be a big debate about nature versus nurture and we now know that it's always

44:30.920 --> 44:31.920
 both.

44:31.920 --> 44:35.520
 You can't even separate them because you come to the table with a certain amount of nature,

44:35.520 --> 44:37.760
 for example, your whole genome and so on.

44:37.760 --> 44:41.920
 The experiences you have in the womb, like whether your mother is smoking or drinking,

44:41.920 --> 44:45.520
 things like that, whether she's stressed, so on, those all influence how you're going

44:45.520 --> 44:47.400
 to pop out of the womb.

44:47.400 --> 44:55.600
 From there, everything is an interaction between all of your experiences and the nature.

44:55.600 --> 45:01.560
 What I mean is, I think of it like a space time cone where you have, you dropped in the

45:01.560 --> 45:04.240
 world and depending on the experience that you have, you might go off in this direction,

45:04.240 --> 45:08.720
 that direction, that direction because there's interaction all the way.

45:08.720 --> 45:12.400
 Your experiences determine what happens with the expression of your genes.

45:12.400 --> 45:17.280
 So some genes get repressed, some get expressed and so on and you actually become a different

45:17.280 --> 45:19.000
 person based on your experiences.

45:19.000 --> 45:24.520
 There's a whole field called epigenomics, or epigenetics, I should say, which is about

45:24.520 --> 45:31.520
 the epigenome and that is the layer that sits on top of the DNA and causes the genes to

45:31.520 --> 45:32.520
 express differently.

45:32.520 --> 45:35.200
 That is directly related to the experiences that you have.

45:35.200 --> 45:41.440
 So just as an example, they take rat pups and one group is placed away from their parents

45:41.440 --> 45:44.760
 and the other group is groomed and licked and taken good care of.

45:44.760 --> 45:46.920
 That changes their gene expression for the rest of their life.

45:46.920 --> 45:51.120
 They go off in different directions in this space time cone.

45:51.120 --> 45:58.240
 So yeah, this is of course why it matters that we take care of children and pour money

45:58.240 --> 46:05.000
 into things like education and good childcare and so on for children broadly because these

46:05.000 --> 46:08.440
 formed of years matter so much.

46:08.440 --> 46:11.720
 So is there a free will?

46:11.720 --> 46:17.200
 This is a great question for the absurd high level philosophical question.

46:17.200 --> 46:19.200
 No, no, these are my favorite kind of questions.

46:19.200 --> 46:20.200
 Here's the thing.

46:20.200 --> 46:21.200
 Here's the thing.

46:21.200 --> 46:22.200
 We don't know.

46:22.200 --> 46:27.720
 If you ask most neuroscientists, they'll say that we can't really think of how you would

46:27.720 --> 46:30.320
 get free will in there because as far as we can tell, it's a machine.

46:30.320 --> 46:36.280
 It's a very complicated machine, enormously sophisticated, 86 billion neurons about the

46:36.280 --> 46:38.320
 same number of glial cells.

46:38.320 --> 46:41.360
 Each of these things is as complicated as the city of San Francisco.

46:41.360 --> 46:43.600
 Each neuron in your head has the entire human genome in it.

46:43.600 --> 46:47.720
 It's expressing millions of gene products.

46:47.720 --> 46:50.080
 These are incredibly complicated biochemical cascades.

46:50.080 --> 46:54.880
 Each one is connected to 10,000 of its neighbors, which means you have half a quadrillion connections

46:54.880 --> 46:55.880
 in the brain.

46:55.880 --> 47:02.680
 So it's incredibly complicated, but it is fundamentally appears to just be a machine.

47:02.680 --> 47:07.960
 And therefore, if there's nothing in it that's not being driven by something else, then it

47:07.960 --> 47:12.680
 seems it's hard to understand where free will would come from.

47:12.680 --> 47:16.760
 So that's the camp that pretty much all of us fall into, but I will say our science is

47:16.760 --> 47:18.360
 still quite young.

47:18.360 --> 47:22.680
 And I'm a fan of the history of science, and the thing that always strikes me as interesting

47:22.680 --> 47:28.800
 is when you look back at any moment in science, everybody believes something is true, and

47:28.800 --> 47:33.280
 they simply didn't know about what Einstein revealed or whatever.

47:33.280 --> 47:35.000
 And so who knows?

47:35.000 --> 47:39.920
 And they all feel like at any moment in history, they all feel like we've converged to the

47:39.920 --> 47:40.920
 final answer.

47:40.920 --> 47:41.920
 Exactly.

47:41.920 --> 47:42.920
 Exactly.

47:42.920 --> 47:44.000
 Like all the pieces of the puzzle are there.

47:44.000 --> 47:48.280
 And I think that's a funny illusion that's worth getting rid of, and in fact, this is

47:48.280 --> 47:52.760
 what drives good science is recognizing that we don't have most of the puzzle pieces.

47:52.760 --> 47:55.680
 So as far as the free will question goes, I don't know.

47:55.680 --> 47:59.040
 At the moment, it seems, wow, it would be really impossible to figure out how something

47:59.040 --> 48:04.600
 else could fit in there, but 100 years from now, our textbooks might be very different

48:04.600 --> 48:05.600
 than they are now.

48:05.600 --> 48:10.720
 I mean, could I ask you to speculate, where do you think free will could be squeezed into

48:10.720 --> 48:11.720
 there?

48:11.720 --> 48:17.600
 Like, what's that even, is it possible that our brain just creates kinds of illusions

48:17.600 --> 48:20.040
 that are useful for us?

48:20.040 --> 48:24.320
 Or like what, where could it possibly be squeezed in?

48:24.320 --> 48:31.080
 Well, let me give a speculation answer to your very nice question, but you know, and

48:31.080 --> 48:33.160
 the listeners of this podcast, don't quote me on this.

48:33.160 --> 48:36.160
 Yeah, exactly, I'm not saying this is what I believe to be true, but let me just give

48:36.160 --> 48:37.160
 an example.

48:37.160 --> 48:39.040
 I give this at the end of my book Incognito.

48:39.040 --> 48:42.480
 So the whole book of Incognito is about, you know, all the what's happening in the

48:42.480 --> 48:46.120
 brain and essentially I'm saying, look, here's all the reasons to think that free will probably

48:46.120 --> 48:47.120
 does not exist.

48:47.120 --> 48:54.640
 But at the very end, I say, look, imagine that you are, you know, imagine that you're

48:54.640 --> 48:59.960
 a Kalahari Bushman, and you find a radio in the sand, and you've never seen anything

48:59.960 --> 49:05.920
 like this, and you, you look at this radio and you realize that when you turn this knob,

49:05.920 --> 49:08.840
 you hear voices coming from their voices coming from it.

49:08.840 --> 49:13.560
 So being a, you know, a radio materialist, you try to figure out like, how does this

49:13.560 --> 49:14.560
 thing operate?

49:14.560 --> 49:17.040
 So you take off the back cover, and you realize there's all these wires.

49:17.040 --> 49:22.440
 And when you take out some wires, the voices get garbled or stop or whatever.

49:22.440 --> 49:26.040
 And so what you end up developing is a whole theory about how this connection is patterned

49:26.040 --> 49:31.920
 of wires gives rise to voices, but it would never strike you that in distant cities, there's

49:31.920 --> 49:36.320
 a radio tower and there's invisible stuff beaming, and that's actually the origin of

49:36.320 --> 49:37.320
 the voices.

49:37.320 --> 49:38.860
 And this is just necessary for it.

49:38.860 --> 49:44.520
 So I mentioned this just as a speculation, say, look, how would we know what we know

49:44.520 --> 49:47.920
 about the brain for absolutely certain is that if when you damage pieces and parts of

49:47.920 --> 49:50.680
 it, things get jumbled up.

49:50.680 --> 49:54.320
 But how would you know if there's something else going on that we can't see like electromagnetic

49:54.320 --> 49:58.280
 radiation, that is what's actually generating this?

49:58.280 --> 49:59.280
 Yeah.

49:59.280 --> 50:09.400
 You paint a beautiful example of how totally, because we don't know most of how our universe

50:09.400 --> 50:17.800
 works, how totally off base we might be with our science until, I mean, yeah, I mean, that's

50:17.800 --> 50:18.800
 inspiring.

50:18.800 --> 50:19.800
 That's beautiful.

50:19.800 --> 50:20.800
 It's kind of terrifying.

50:20.800 --> 50:21.800
 It's humbling.

50:21.800 --> 50:23.800
 It's all of the above.

50:23.800 --> 50:28.720
 And the important part just to recognize is that, of course, we're in the position of

50:28.720 --> 50:32.000
 having massive unknowns.

50:32.000 --> 50:37.720
 And we have, of course, the known unknowns, and that's all the things we're pursuing in

50:37.720 --> 50:41.360
 our labs and trying to figure out that, but there's this whole space of unknown unknowns

50:41.360 --> 50:43.760
 that we haven't even realized we haven't asked yet.

50:43.760 --> 50:51.560
 Let me kind of ask a weird, maybe a difficult question part that has to do with, I've been

50:51.560 --> 50:54.280
 recently reading a lot about World War II.

50:54.280 --> 51:00.400
 I'm currently reading a book I recommend for people, which is, as a Jew, it's been difficult

51:00.400 --> 51:04.960
 to read, but the rise and fall of the Third Reich.

51:04.960 --> 51:10.040
 So let me just ask about like the nature of genius, the nature of evil.

51:10.040 --> 51:19.680
 If we look at somebody like Einstein, we look at Hitler, Stalin, modern day, Jeffrey Epstein,

51:19.680 --> 51:26.240
 those folks who through their life have done with Einstein, done works of genius, and with

51:26.240 --> 51:30.880
 the others I mentioned have done evil on this world.

51:30.880 --> 51:34.600
 What do we think about that in a live wired brain?

51:34.600 --> 51:39.440
 Like, how do we think about these extreme people?

51:39.440 --> 51:41.840
 Here's what I'd say.

51:41.840 --> 51:47.880
 This is a very big and difficult question, but what I would say briefly on it is, first

51:47.880 --> 51:54.560
 of all, I saw a cover of Time Magazine some years ago, and it was a big, you know, sagittal

51:54.560 --> 51:59.040
 slice of the brain, and it said something like, what makes us good and evil?

51:59.040 --> 52:01.400
 And there was a little spot pointing to it, and there was a picture of Gandhi, and there

52:01.400 --> 52:03.600
 was a little spot that was pointing to Hitler.

52:03.600 --> 52:08.080
 And these Time Magazine covers always make me mad because it's so goofy to think that

52:08.080 --> 52:10.760
 we're going to find some spot in the brain or something.

52:10.760 --> 52:18.600
 Instead, the interesting part is, because we're live wired, we are all about the world

52:18.600 --> 52:20.760
 and the culture around us.

52:20.760 --> 52:27.720
 So somebody like Adolf Hitler got all this positive feedback about what was going on,

52:27.720 --> 52:32.720
 and the crazier and crazier the ideas he had, he's like, let's set up death camps and murder

52:32.720 --> 52:34.440
 a bunch of people and so on.

52:34.440 --> 52:38.720
 Somehow he was getting positive feedback from that, and all these other people, they're

52:38.720 --> 52:43.000
 all, you know, spun each other up, and you look at anything like, I mean, look at the,

52:43.000 --> 52:51.200
 you know, the cultural revolution in China or the, you know, the Russian revolution or

52:51.200 --> 52:54.360
 things like this, where you look at these, you think, my God, how do people all behave

52:54.360 --> 52:55.360
 like this?

52:55.360 --> 52:59.880
 But it's easy to see groups of people spinning themselves up in particular ways where they

52:59.880 --> 53:05.000
 all say, well, what I have thought this was right in a different circumstance, I don't

53:05.000 --> 53:07.440
 know, but Fred thinks it's right, and Steve thinks it's right, everyone around me thinks

53:07.440 --> 53:08.440
 it's right.

53:08.440 --> 53:14.320
 So part of the maybe downside of having a live wired brain is that you can get crowds

53:14.320 --> 53:17.560
 of people doing things as a group.

53:17.560 --> 53:21.240
 So it's interesting to, you know, we would pinpoint Hitler as saying that's the evil

53:21.240 --> 53:30.880
 guy, but in a sense, I think it was Tolstoy who said the king becomes slave to the people.

53:30.880 --> 53:35.800
 In other words, you know, Hitler was just a representation of whatever was going on

53:35.800 --> 53:39.520
 with that huge crowd that he was surrounded with.

53:39.520 --> 53:47.000
 So I only bring that up to say that it's, you know, it's very difficult to say what

53:47.000 --> 53:49.120
 it is about this person's brain and that person's brain.

53:49.120 --> 53:51.680
 He obviously got feedback for what he was doing.

53:51.680 --> 53:59.320
 The other thing, by the way, about what we often think of as being evil in society is

53:59.320 --> 54:07.160
 my lab recently published some work on in groups and out groups, which is a very important

54:07.160 --> 54:08.360
 part of this puzzle.

54:08.360 --> 54:14.960
 So it turns out that we are very, we are very, you know, engineered to care about in groups

54:14.960 --> 54:18.780
 versus out groups, and this seems to be like a really fundamental thing.

54:18.780 --> 54:23.120
 So we did this experiment in my lab where we brought people in, we stick them in the

54:23.120 --> 54:27.000
 scanner, and we, I don't know, and it's not me if you know this, but we show them on the

54:27.000 --> 54:33.840
 screen six hands, and the computer goes around randomly picks a hand, and then you see that

54:33.840 --> 54:35.800
 hand gets stabbed with a syringe needle.

54:35.800 --> 54:40.320
 So you actually see a syringe needle enter the hand and come out, and it's really what

54:40.320 --> 54:46.080
 that does is that triggers parts of the pain matrix, this area is in your brain that involved

54:46.080 --> 54:47.320
 in feeling physical pain.

54:47.320 --> 54:50.040
 Now the interesting thing is it's not your hand that was stabbed.

54:50.040 --> 54:51.720
 So what you're seeing is empathy.

54:51.720 --> 54:56.120
 This is you seeing someone else's hand get stabbed, you feel like, oh God, this awful,

54:56.120 --> 54:57.560
 okay, okay.

54:57.560 --> 55:01.320
 We contrast that by the way with somebody's hand getting poked as a Q tip, which is, you

55:01.320 --> 55:06.320
 know, looks visually the same, but it's, you don't have that same level of response.

55:06.320 --> 55:12.240
 Now what we do is we label each hand with a, with a one word label, Christian, Jewish,

55:12.240 --> 55:14.480
 Muslim, atheist, Scientologist, Hindu.

55:14.480 --> 55:19.360
 And now the computer goes around picks a hand stabs the hand and the question is, how

55:19.360 --> 55:24.240
 much does your brain care about all the people in your out group versus the one label that

55:24.240 --> 55:26.800
 happens to match you?

55:26.800 --> 55:30.680
 And it turns out for everybody across all religions, they care much more about their

55:30.680 --> 55:31.680
 in group than their out group.

55:31.680 --> 55:35.920
 And when I say they care, what I mean is you get a bigger response from their brain.

55:35.920 --> 55:36.920
 Everything's the same.

55:36.920 --> 55:38.920
 It's the same hands.

55:38.920 --> 55:40.680
 It's just a one word label.

55:40.680 --> 55:43.120
 You care much more about your in group than your out group.

55:43.120 --> 55:45.680
 And I wish this weren't true, but this is how humans are.

55:45.680 --> 55:53.360
 I wonder how fundamental that is, or if it's a, it's the emergent thing about culture.

55:53.360 --> 55:58.680
 Like if we lived alone with, like if it's genetically built into the brain, like this,

55:58.680 --> 56:00.480
 this longing for tribe.

56:00.480 --> 56:02.360
 So I'll tell you, we addressed that.

56:02.360 --> 56:03.360
 So here's what we did.

56:03.360 --> 56:08.000
 There are two, actually there are two other things we did as part of this study that I

56:08.000 --> 56:09.840
 think matter for this point.

56:09.840 --> 56:12.840
 One is, so, okay, so we show that you have a much bigger response.

56:12.840 --> 56:14.440
 And by the way, this is, this is not a cognitive thing.

56:14.440 --> 56:18.720
 It's a very low level basic response to seeing pain in somebody.

56:18.720 --> 56:19.720
 Okay.

56:19.720 --> 56:20.720
 Great study, by the way.

56:20.720 --> 56:21.720
 Thanks.

56:21.720 --> 56:22.720
 Thanks.

56:22.720 --> 56:27.200
 So one of the things is we, we next have it where we say, okay, the year is 2025.

56:27.200 --> 56:30.840
 And these three religions are now in a war against these three religions.

56:30.840 --> 56:31.840
 And it's all randomized, right?

56:31.840 --> 56:36.240
 But what you see is your thing, and you have two allies now against these others.

56:36.240 --> 56:40.160
 And now it happens over the course of many trials, you see everybody gets stabbed at

56:40.160 --> 56:41.160
 different times.

56:41.160 --> 56:43.280
 And the question is, do you care more about your allies?

56:43.280 --> 56:44.280
 And the answer is yes.

56:44.280 --> 56:47.360
 Suddenly, people who a moment ago, you didn't really care when they got stabbed.

56:47.360 --> 56:51.400
 Now, simply with this one word thing that you're there now, your allies, you care more

56:51.400 --> 56:52.760
 about them.

56:52.760 --> 56:57.720
 But then what I wanted to do was look at how ingrained is this or how arbitrary is it?

56:57.720 --> 57:02.720
 So we brought new participants in and we said, here's a coin, toss the coin.

57:02.720 --> 57:04.240
 If it's heads, you're an Augustinian.

57:04.240 --> 57:07.320
 If it's a tails, you're a Justinian, totally made up.

57:07.320 --> 57:08.320
 Okay.

57:08.320 --> 57:10.160
 So they toss it, they get whatever.

57:10.160 --> 57:15.560
 We give them a band that says, you know, Augustinian on it, whatever tribe they're in now.

57:15.560 --> 57:19.200
 And they get in the scanner and they see a thing on the screen says the Augustinians

57:19.200 --> 57:21.160
 and Justinians are two warring tribes.

57:21.160 --> 57:24.720
 You see a bunch of hands, some are labeled Augustinians, some are Justinian.

57:24.720 --> 57:28.960
 And now you care more about whichever team you're on than the other team, even though

57:28.960 --> 57:33.040
 it's totally arbitrary and you know it's arbitrary because you're the one who tossed the coin.

57:33.040 --> 57:37.080
 So it's a state that's very easy to find ourselves in.

57:37.080 --> 57:41.000
 In other words, just before walking in the door, they'd never even heard of Augustinian

57:41.000 --> 57:42.000
 versus Justinian.

57:42.000 --> 57:46.440
 And now their brain is representing it simply because they're told they're on this team.

57:46.440 --> 57:51.120
 You know, now I did my own personal study of this.

57:51.120 --> 57:56.720
 It's a, so once you're an Augustinian, that tends to be sticky because I've been a Packers

57:56.720 --> 57:59.200
 fan, a good bad Packers fan of my whole life.

57:59.200 --> 58:05.360
 Now when I'm in Boston with like the Patriots, it's been tough going from my live wire brain

58:05.360 --> 58:11.040
 to switch to the Patriots to be, so once you become, it's, it's interesting once the tribe

58:11.040 --> 58:12.040
 is sticky.

58:12.040 --> 58:13.040
 Yeah.

58:13.040 --> 58:14.040
 But that's true.

58:14.040 --> 58:15.040
 That's, that's it.

58:15.040 --> 58:18.440
 You know, we never tried that about saying, okay, now you're a Justinian and you were

58:18.440 --> 58:19.440
 an Augustinian.

58:19.440 --> 58:30.520
 So how, how sticky it is, but there are studies of this, of monkey troops on some island.

58:30.520 --> 58:34.480
 And what happens is they look at the way monkeys behave when they're part of this tribe and

58:34.480 --> 58:37.960
 how they treat members of the other tribe of monkeys.

58:37.960 --> 58:41.520
 And then what they do, I've forgotten how they do that exactly, but they end up switching

58:41.520 --> 58:43.200
 a monkey so he ends up in the other troop.

58:43.200 --> 58:47.800
 And very quickly they end up becoming a part of the other troop and, and hating and behaving

58:47.800 --> 58:50.480
 badly towards the original troops.

58:50.480 --> 58:55.320
 These are fascinating studies, by the way, this is, this is beautiful.

58:55.320 --> 59:01.320
 In your, in your book, you have a, you have a good light bulb joke.

59:01.320 --> 59:04.720
 How many psychiatrists does it take to change a light bulb?

59:04.720 --> 59:11.040
 Only one, but the light bulb has to want to change, I'm sorry, I'm a sucker for a good

59:11.040 --> 59:12.040
 light bulb joke.

59:12.040 --> 59:13.040
 Okay.

59:13.040 --> 59:19.160
 So, you know, I've been interested in psychiatry my whole life, just maybe tangentially.

59:19.160 --> 59:25.960
 I've kind of early on dreamed to be a psychiatrist until I understood what it entails.

59:25.960 --> 59:33.720
 But you know, what, you know, is there hope for psychiatry, for somebody else to help

59:33.720 --> 59:36.800
 this live wired brain to adjust?

59:36.800 --> 59:37.800
 Oh yeah.

59:37.800 --> 59:42.120
 I mean, in the sense that, and this has to do with this issue about us being trapped

59:42.120 --> 59:46.960
 on our own planet, forget psychiatrists, just think of like when you're talking with a friend

59:46.960 --> 59:49.160
 and you say, oh, I'm so upset about this.

59:49.160 --> 59:54.640
 And your friend says, hey, just look at it this way, you know, all we have access to

59:54.640 --> 59:57.520
 under normal circumstances is just the way we're seeing something.

59:57.520 --> 1:00:03.400
 And so it's super helpful to have friends and communities and psychiatrists and so on

1:00:03.400 --> 1:00:05.840
 to help things change that way.

1:00:05.840 --> 1:00:07.320
 So that's how psychiatrists sort of helped us.

1:00:07.320 --> 1:00:13.400
 But more importantly, the role that psychiatrists have played is that there's this sort of naive

1:00:13.400 --> 1:00:16.680
 assumption that we all come to the table with, which is that everyone is fundamentally just

1:00:16.680 --> 1:00:18.680
 like us.

1:00:18.680 --> 1:00:21.240
 And when you're a kid, you believe this entirely.

1:00:21.240 --> 1:00:25.120
 But as you get older and you start realizing, okay, there's something called schizophrenia

1:00:25.120 --> 1:00:26.560
 and that's a real thing.

1:00:26.560 --> 1:00:30.400
 And to be inside that person's head is totally different than what it is to be inside my head

1:00:30.400 --> 1:00:32.520
 or their psychopathy.

1:00:32.520 --> 1:00:36.280
 And to be inside this psychopath's head, he doesn't care about other people.

1:00:36.280 --> 1:00:37.520
 He doesn't care about hurting other people.

1:00:37.520 --> 1:00:41.120
 He's just doing what he needs to do to get what he needs.

1:00:41.120 --> 1:00:42.400
 That's a different head.

1:00:42.400 --> 1:00:45.600
 There's a million different things going on.

1:00:45.600 --> 1:00:50.280
 And it is different to be inside those heads that this is where the field of psychiatry

1:00:50.280 --> 1:00:51.280
 comes in.

1:00:51.280 --> 1:00:56.080
 Now, I think it's an interesting question about the degree to which neuroscience is

1:00:56.080 --> 1:01:00.480
 leaking into and taking over psychiatry and what the landscape will look like 50 years

1:01:00.480 --> 1:01:01.480
 from now.

1:01:01.480 --> 1:01:08.480
 But psychiatry as a profession changes a lot or maybe goes away entirely and neuroscience

1:01:08.480 --> 1:01:12.320
 will essentially be able to take over some of these functions, but it has been extremely

1:01:12.320 --> 1:01:19.160
 useful to understand the differences between how people behave and why and what you can

1:01:19.160 --> 1:01:25.880
 tell about what's going on inside their brain just based on observation of their behavior.

1:01:25.880 --> 1:01:29.000
 This might be years ago, but I'm not sure.

1:01:29.000 --> 1:01:35.320
 There's an Atlantic article you've written about moving away from a distinction between

1:01:35.320 --> 1:01:42.440
 neurological disorders, quote unquote, brain problems and psychiatric disorders or quote

1:01:42.440 --> 1:01:45.600
 unquote, mind problems.

1:01:45.600 --> 1:01:47.720
 So on that topic, how do you think about this gray area?

1:01:47.720 --> 1:01:48.720
 Yeah.

1:01:48.720 --> 1:01:52.680
 This is exactly the evolution that things are going is there was psychiatry and then there

1:01:52.680 --> 1:01:57.640
 were guys and gals in labs poking cells and so on, those were the neuroscientists.

1:01:57.640 --> 1:02:01.280
 But yeah, I think these are moving together for exactly the reason you just cited.

1:02:01.280 --> 1:02:05.760
 And where this matters a lot, the Atlantic article that I wrote was called the brain

1:02:05.760 --> 1:02:12.360
 on trial where this matters a lot is it's the legal system because the way we run our

1:02:12.360 --> 1:02:16.280
 legal system now, and this is true everywhere in the world is, you know, someone shows up

1:02:16.280 --> 1:02:19.600
 in front of the judge's bench or let's say there's five people in front of the judge's

1:02:19.600 --> 1:02:21.920
 bench and they've all committed the same crime.

1:02:21.920 --> 1:02:25.240
 What we do because we feel like, hey, this is fair is because you're going to get the

1:02:25.240 --> 1:02:26.240
 same sense.

1:02:26.240 --> 1:02:28.320
 You're going to get three years in prison or whatever it is.

1:02:28.320 --> 1:02:30.320
 But in fact, brains can be so different.

1:02:30.320 --> 1:02:31.320
 This guy's got schizophrenia.

1:02:31.320 --> 1:02:32.320
 This guy's psychopath.

1:02:32.320 --> 1:02:37.160
 This guy's tweaked down on drugs and so on and so on that it actually doesn't make sense

1:02:37.160 --> 1:02:38.160
 to keep doing that.

1:02:38.160 --> 1:02:44.480
 And what we do in this country more than anywhere in the world is we imagine that incarceration

1:02:44.480 --> 1:02:49.320
 is a one size fits all solution and you may know we have the America has the highest incarceration

1:02:49.320 --> 1:02:53.440
 rate in the whole world in terms of the percentage of our population we put behind bars.

1:02:53.440 --> 1:02:59.960
 So there's a much more refined thing we can do as neuroscience comes in and changes and

1:02:59.960 --> 1:03:03.920
 has the opportunity to change the legal system, which is to say, this doesn't let anybody

1:03:03.920 --> 1:03:04.920
 off the hook.

1:03:04.920 --> 1:03:06.680
 It doesn't say, oh, it's not your fault and so on.

1:03:06.680 --> 1:03:09.920
 But what it does is it changes the equation.

1:03:09.920 --> 1:03:15.160
 So it's not about, hey, how blame worthy are you, but instead is about, hey, what do we

1:03:15.160 --> 1:03:16.160
 do from here?

1:03:16.160 --> 1:03:17.160
 What's the best thing to do from here?

1:03:17.160 --> 1:03:20.620
 So if you take somebody with schizophrenia and you have them break rocks in the hot summer

1:03:20.620 --> 1:03:27.640
 sun in a chain gang, that doesn't help the schizophrenia, that doesn't fix the problem.

1:03:27.640 --> 1:03:32.200
 If you take somebody with a drug addiction who's in jail for being caught with two ounces

1:03:32.200 --> 1:03:37.560
 of some illegal substance and you put them in prison, it doesn't actually fix the addiction,

1:03:37.560 --> 1:03:40.720
 it doesn't help anything.

1:03:40.720 --> 1:03:44.680
 Happily what neuroscience and psychiatry bring to the table is lots of really useful

1:03:44.680 --> 1:03:48.880
 things you can do with schizophrenia, with drug addiction, things like this.

1:03:48.880 --> 1:03:52.080
 And that's why, so I don't know if you know it's better to run a national law and profit

1:03:52.080 --> 1:03:54.000
 called the Center for Science and Law.

1:03:54.000 --> 1:03:57.480
 And it's all about this intersection of neuroscience and legal system.

1:03:57.480 --> 1:04:02.600
 And we're trying to implement changes in every county and every state.

1:04:02.600 --> 1:04:06.640
 I'll just, without going down that rabbit hole, I'll just say one of the very simplest

1:04:06.640 --> 1:04:12.640
 things to do is to set up specialized court systems where you have a mental health court

1:04:12.640 --> 1:04:15.840
 that has judges and juries with expertise in mental illness.

1:04:15.840 --> 1:04:20.800
 Because if you go, by the way, to a regular court and the person says, or the defense

1:04:20.800 --> 1:04:25.440
 lawyer says, this person has schizophrenia, most of the jury will say, man, I call bullshit

1:04:25.440 --> 1:04:26.440
 on that.

1:04:26.440 --> 1:04:27.440
 Why?

1:04:27.440 --> 1:04:28.440
 Because they don't know about schizophrenia.

1:04:28.440 --> 1:04:30.440
 They don't know what it's about.

1:04:30.440 --> 1:04:36.240
 And it turns out people who know about schizophrenia feel very differently as a juror than someone

1:04:36.240 --> 1:04:39.640
 who happens not to know any about schizophrenia, they think it's an excuse.

1:04:39.640 --> 1:04:44.120
 So you have judges and jurors with expertise in mental illness and they know the rehabilitative

1:04:44.120 --> 1:04:45.920
 strategies that are available.

1:04:45.920 --> 1:04:46.920
 That's one thing.

1:04:46.920 --> 1:04:49.880
 Having a drug court where you have judges and jurors with expertise in rehabilitative

1:04:49.880 --> 1:04:54.240
 strategies and what can be done and so on, a specialized prostitution court and so on.

1:04:54.240 --> 1:04:55.800
 All these different things.

1:04:55.800 --> 1:04:59.400
 By the way, this is very easy for counties to implement this sort of thing.

1:04:59.400 --> 1:05:05.160
 And this is, I think, where this matters to get neuroscience into public policy.

1:05:05.160 --> 1:05:08.400
 What's the process of injecting expertise into this?

1:05:08.400 --> 1:05:10.560
 Yeah, I'll tell you exactly what it is.

1:05:10.560 --> 1:05:12.440
 A county needs to run out of money first.

1:05:12.440 --> 1:05:14.560
 I've seen this happen over and over.

1:05:14.560 --> 1:05:18.680
 So what happens is a county has a completely full jail and they say, you know what?

1:05:18.680 --> 1:05:19.880
 We need to build another jail.

1:05:19.880 --> 1:05:21.280
 And then they realize, God, we don't have any money.

1:05:21.280 --> 1:05:22.360
 We can't afford this.

1:05:22.360 --> 1:05:23.600
 We've got too many people in jail.

1:05:23.600 --> 1:05:26.800
 And that's when they turn to, God, we need something smarter.

1:05:26.800 --> 1:05:31.040
 And that's when they set up specialized court systems.

1:05:31.040 --> 1:05:34.360
 We're all function best when our back is against the wall.

1:05:34.360 --> 1:05:36.280
 And that's what COVID is good for.

1:05:36.280 --> 1:05:40.880
 It's because we've all had our routines and we are optimized for the things we do and

1:05:40.880 --> 1:05:43.000
 suddenly our backs are against the wall, all of us.

1:05:43.000 --> 1:05:49.080
 Yeah, it's really, I mean, one of the exciting things about COVID, I mean, I'm a big believer

1:05:49.080 --> 1:05:56.600
 in the possibility of what government can do for the people.

1:05:56.600 --> 1:06:01.640
 And when it becomes too big of a bureaucracy, it starts functioning poorly, it starts wasting

1:06:01.640 --> 1:06:02.720
 money.

1:06:02.720 --> 1:06:07.360
 It's nice to, I mean, COVID is, and reveals that nicely.

1:06:07.360 --> 1:06:14.520
 And it lessons to be learned about who gets elected and who goes into government.

1:06:14.520 --> 1:06:20.800
 Hopefully this, hopefully this inspires talented and young people to go into government to

1:06:20.800 --> 1:06:23.080
 revolutionize different aspects of it.

1:06:23.080 --> 1:06:24.080
 Yeah.

1:06:24.080 --> 1:06:27.680
 So that's, that's the positive silver lining of COVID.

1:06:27.680 --> 1:06:32.240
 I mean, I thought it'd be fun to ask you, I don't know if you're paying attention to

1:06:32.240 --> 1:06:39.560
 machine learning world and GPT three, so the GPT three is this language model is neural

1:06:39.560 --> 1:06:45.040
 network that's able to, it has 175 billion parameters.

1:06:45.040 --> 1:06:54.280
 So it's very large and it's trained and unsupervised way on the internet is just reads a lot of

1:06:54.280 --> 1:06:59.480
 unstructured texts and it's able to generate some pretty impressive things.

1:06:59.480 --> 1:07:06.200
 The human brain compared to that has about a thousand times more synapses.

1:07:06.200 --> 1:07:13.800
 People get so upset when machine learning people compare the brain and we know synapses

1:07:13.800 --> 1:07:14.800
 are different.

1:07:14.800 --> 1:07:20.720
 It was very different, very different, but like, do you, what do you think about GPT three?

1:07:20.720 --> 1:07:21.720
 Here's what I think.

1:07:21.720 --> 1:07:22.720
 Here's what I think a few things.

1:07:22.720 --> 1:07:27.360
 What GPT three is doing is extremely impressive, but it's very different from what the brain

1:07:27.360 --> 1:07:28.360
 does.

1:07:28.360 --> 1:07:36.880
 It's a good impersonator, but just as one example, everybody takes a passage that GPT three has

1:07:36.880 --> 1:07:40.480
 written and they say, wow, look at this and it's pretty good, right?

1:07:40.480 --> 1:07:43.760
 But it's already gone through a filtering process of humans looking at it and saying,

1:07:43.760 --> 1:07:44.760
 okay, well, that's crap.

1:07:44.760 --> 1:07:45.760
 That's crap.

1:07:45.760 --> 1:07:46.760
 Okay.

1:07:46.760 --> 1:07:47.760
 Oh, here's what, here's a sentence that's pretty cool.

1:07:47.760 --> 1:07:49.160
 Now here's the thing.

1:07:49.160 --> 1:07:53.040
 Human creativity is about absorbing everything around it and remixing that and coming up

1:07:53.040 --> 1:07:54.040
 with stuff.

1:07:54.040 --> 1:07:59.880
 We're sort of like GPT three, you know, we're remixing what we've gotten in before, but

1:07:59.880 --> 1:08:05.080
 we also know, we also have very good models of what it is to be another human.

1:08:05.080 --> 1:08:08.880
 And so, you know, I don't know if you speak French or something, but I'm not going to

1:08:08.880 --> 1:08:11.560
 start speaking in French because then you'll say, wait, what are you doing?

1:08:11.560 --> 1:08:12.560
 I don't understand you.

1:08:12.560 --> 1:08:16.240
 Instead, everything coming out of my mouth is meant for your ears.

1:08:16.240 --> 1:08:18.120
 I know what you'll understand.

1:08:18.120 --> 1:08:20.120
 I know the vocabulary that you know and don't know.

1:08:20.120 --> 1:08:24.000
 I know what parts you care about.

1:08:24.000 --> 1:08:25.240
 That's a huge part of it.

1:08:25.240 --> 1:08:31.800
 And so, of all the possible sentences I could say, I'm navigating this thin bandwidth so

1:08:31.800 --> 1:08:34.280
 that it's something useful for our conversation.

1:08:34.280 --> 1:08:39.800
 Yeah, in real time, but also throughout your life, I mean, we're covolving together.

1:08:39.800 --> 1:08:42.920
 We're learning how to communicate together.

1:08:42.920 --> 1:08:43.920
 Exactly.

1:08:43.920 --> 1:08:46.320
 But this is what GPT three does not do.

1:08:46.320 --> 1:08:49.560
 All it's doing is saying, okay, I'm going to take all these senses and remix stuff and

1:08:49.560 --> 1:08:51.240
 pop some stuff out.

1:08:51.240 --> 1:08:54.720
 But it doesn't know how to make it so that you, Lex, will feel like, oh, yeah, that's

1:08:54.720 --> 1:08:57.080
 exactly what I needed to hear.

1:08:57.080 --> 1:09:00.280
 That's the next sentence that I needed to know about for something.

1:09:00.280 --> 1:09:04.280
 Well, of course, it could be all the impressive results we see.

1:09:04.280 --> 1:09:09.520
 The question is, if you raise the number of parameters, whether it's going to be after

1:09:09.520 --> 1:09:10.520
 some...

1:09:10.520 --> 1:09:11.520
 It will not be.

1:09:11.520 --> 1:09:14.320
 No, raising more parameters won't...

1:09:14.320 --> 1:09:15.320
 Here's the thing.

1:09:15.320 --> 1:09:18.640
 It's not that I don't think neural networks can't be like the human brain because I suspect

1:09:18.640 --> 1:09:20.480
 they will be at some point, 50 years.

1:09:20.480 --> 1:09:21.480
 We know.

1:09:21.480 --> 1:09:28.400
 But what we are missing in artificial neural networks is we've got this basic structure.

1:09:28.400 --> 1:09:32.720
 We've got units and you've got synapses that are connected.

1:09:32.720 --> 1:09:33.720
 And that's great.

1:09:33.720 --> 1:09:38.880
 And it's done incredibly mind blowing impressive things, but it's not doing the same algorithms

1:09:38.880 --> 1:09:40.380
 as the human brain.

1:09:40.380 --> 1:09:45.520
 So when I look at my children as little kids, you know, as infants, they can do things that

1:09:45.520 --> 1:09:47.760
 no GPT three can do.

1:09:47.760 --> 1:09:50.640
 They can navigate a complex room.

1:09:50.640 --> 1:09:55.000
 They can navigate social conversation with an adult.

1:09:55.000 --> 1:09:56.000
 They can lie.

1:09:56.000 --> 1:09:58.320
 They can do a million things.

1:09:58.320 --> 1:10:03.320
 They are active thinkers in our world and doing things.

1:10:03.320 --> 1:10:08.160
 And this, of course, I mean, look, we totally agree on how incredibly awesome artificial

1:10:08.160 --> 1:10:09.160
 neural networks are right now.

1:10:09.160 --> 1:10:14.960
 But we also know the things that they can't do well, like be generally intelligent, do

1:10:14.960 --> 1:10:15.960
 all these different things.

1:10:15.960 --> 1:10:20.760
 And that's the reason about the world efficiently, learn efficiently, adapt efficiently.

1:10:20.760 --> 1:10:24.720
 But it's still the rate of improvement.

1:10:24.720 --> 1:10:28.160
 To me, it's possible that it will be surprised.

1:10:28.160 --> 1:10:29.160
 I agree.

1:10:29.160 --> 1:10:30.160
 Awesome.

1:10:30.160 --> 1:10:31.160
 It will be surprised.

1:10:31.160 --> 1:10:36.280
 But what I would assert, and I'm glad I'm getting to say this on your podcast so we

1:10:36.280 --> 1:10:40.480
 can look back at this in two years and 10 years and so on, is that we've got to be much

1:10:40.480 --> 1:10:44.200
 more sophisticated than units and synapses between them.

1:10:44.200 --> 1:10:48.240
 Let me give you an example, and this is something I talk about in LiveWired, is despite the

1:10:48.240 --> 1:10:55.200
 amazing impressiveness, mind blowing impressiveness, computers don't have some basic things, artificial

1:10:55.200 --> 1:10:59.560
 neural networks don't have some basic things that we like caring about relevance, for example.

1:10:59.560 --> 1:11:05.080
 So as humans, we are confronted with tons of data all the time, and we only encode particular

1:11:05.080 --> 1:11:07.920
 things that are relevant to us.

1:11:07.920 --> 1:11:11.840
 We have this very deep sense of relevance that I mentioned earlier is based on survival

1:11:11.840 --> 1:11:16.600
 at the most basic level, but then all the things about my life and your life, what's

1:11:16.600 --> 1:11:19.800
 relevant to you, that we encode.

1:11:19.800 --> 1:11:20.800
 This is very useful.

1:11:20.800 --> 1:11:24.320
 Computers at the moment don't have that, they don't have a yen to survive and things

1:11:24.320 --> 1:11:25.320
 like that.

1:11:25.320 --> 1:11:30.280
 So we filter out a bunch of the junk we don't need, or really good at efficiently zooming

1:11:30.280 --> 1:11:32.120
 in on things we need.

1:11:32.120 --> 1:11:38.840
 Again, could be argued, let me put on my Freud hat, maybe it's, I mean, that's our conscious

1:11:38.840 --> 1:11:44.520
 mind, you know, we're not, you know, there's no reason that neural networks aren't doing

1:11:44.520 --> 1:11:45.520
 the same kind of filtration.

1:11:45.520 --> 1:11:52.280
 I mean, in the sense with GPT3 is doing, so there's a priming step, it's doing an essential

1:11:52.280 --> 1:11:58.960
 kind of filtration when you ask it to generate tweets from, from, I don't know, from an

1:11:58.960 --> 1:12:04.360
 Elon Musk or something like that, it's doing a filtration of it's throwing away all the

1:12:04.360 --> 1:12:09.880
 parameters it doesn't need for this task, and it's figuring out how to do that successfully.

1:12:09.880 --> 1:12:14.000
 And then ultimately it's not doing a very good job right now, but it's doing a lot better

1:12:14.000 --> 1:12:15.000
 job than we expected.

1:12:15.000 --> 1:12:17.560
 But it won't ever do a really good job.

1:12:17.560 --> 1:12:18.560
 And I'll tell you why.

1:12:18.560 --> 1:12:23.200
 I mean, so, so let's say we say, hey, produce an Elon Musk tweet, and we see like, oh, wow,

1:12:23.200 --> 1:12:24.200
 it produced these three.

1:12:24.200 --> 1:12:25.200
 That's great.

1:12:25.200 --> 1:12:29.120
 But again, it's not, we're not seeing the 3000 produced that didn't really make any sense.

1:12:29.120 --> 1:12:33.080
 It's because it has no idea what it is like to be a human.

1:12:33.080 --> 1:12:35.680
 And all the things that you might want to say, and all the reasons you wouldn't, like

1:12:35.680 --> 1:12:39.040
 when you go to write a tweet, you might write something, you think, yeah, it's not going

1:12:39.040 --> 1:12:43.480
 to come off quite right in this modern political climate or whatever, you know, you change things.

1:12:43.480 --> 1:12:49.280
 So and it somehow boils down to fear of mortality and all of these human things at the end of

1:12:49.280 --> 1:12:52.960
 the day, all contained with that tweeting experience.

1:12:52.960 --> 1:12:57.560
 Well, interestingly, the fear of mortality is at the bottom of this, but you've got all

1:12:57.560 --> 1:13:02.800
 these more things like, you know, oh, I want to just in case the chairman of my department

1:13:02.800 --> 1:13:03.800
 reads this.

1:13:03.800 --> 1:13:04.800
 I wanted to come off with that.

1:13:04.800 --> 1:13:08.360
 Just in case my mom looks at this tweet, I want to make sure she, you know, and so on.

1:13:08.360 --> 1:13:13.280
 So that those are all the things that humans are able to sort of throw into the calculation.

1:13:13.280 --> 1:13:18.920
 I mean, what it required, what it requires, though, is having a model of your chairman,

1:13:18.920 --> 1:13:23.920
 having a model of your mother, having a model of, you know, the person you want to go on

1:13:23.920 --> 1:13:26.280
 a date with who might look at your tweet and so on.

1:13:26.280 --> 1:13:31.160
 All these things are, you're running, what it is like to be them.

1:13:31.160 --> 1:13:36.720
 So in terms of the structure of the brain, again, this may be going into speculation

1:13:36.720 --> 1:13:37.720
 land.

1:13:37.720 --> 1:13:46.400
 I hope you go along with me is, okay, so the brain seems to be intelligent and our systems

1:13:46.400 --> 1:13:48.520
 aren't very currently.

1:13:48.520 --> 1:13:53.000
 So where do you think intelligence arises in the brain?

1:13:53.000 --> 1:13:55.840
 Like what, what is it about the brain?

1:13:55.840 --> 1:13:59.960
 So if you mean where location wise, it's no single spot.

1:13:59.960 --> 1:14:06.560
 It would be equivalent to asking, I'm looking at New York City, where is the economy?

1:14:06.560 --> 1:14:08.200
 The answer is you can't point to anywhere.

1:14:08.200 --> 1:14:12.320
 The economy is all about the interaction of all of the pieces and parts of the city.

1:14:12.320 --> 1:14:16.040
 And that's what, you know, intelligence, whatever we mean by that in the brain is interacting

1:14:16.040 --> 1:14:18.280
 from everything going on in the ones.

1:14:18.280 --> 1:14:24.960
 In terms of a structure, so we look humans are much smarter than fish, maybe not dolphins,

1:14:24.960 --> 1:14:26.680
 but dolphins are mammals, right?

1:14:26.680 --> 1:14:30.720
 I assert that what we mean by smarter has to do with live wiring.

1:14:30.720 --> 1:14:33.720
 So what we mean when we say, oh, we're smarter is, oh, we can figure out a new thing and

1:14:33.720 --> 1:14:37.080
 figure out a new pathway to get where we need to go.

1:14:37.080 --> 1:14:40.520
 And that's because fish are essentially coming to the table with, you know, okay, here's

1:14:40.520 --> 1:14:43.920
 the hardware, go, swim, mate, eat.

1:14:43.920 --> 1:14:47.840
 But we have the capacity to say, okay, look, I'm going to absorb, oh, oh, but, you know,

1:14:47.840 --> 1:14:51.880
 I saw someone else do this thing and, and I read once that you could do this other thing

1:14:51.880 --> 1:14:52.880
 and so on.

1:14:52.880 --> 1:14:59.080
 And do you think there's, is there something, I know these are mysteries, but like architecturally

1:14:59.080 --> 1:15:06.800
 speaking, what's feature of the brain of, of the live wire aspect of it that is really

1:15:06.800 --> 1:15:08.200
 useful for intelligence?

1:15:08.200 --> 1:15:14.360
 So like, is it the ability of neurons to reconnect?

1:15:14.360 --> 1:15:20.080
 Like, is there something, is there any lessons about the human brain you think might be inspiring

1:15:20.080 --> 1:15:26.560
 for us in, to take into the artificial, into the machine learning world?

1:15:26.560 --> 1:15:30.400
 Yeah, I'm actually just trying to write some up on this now called, you know, if you want

1:15:30.400 --> 1:15:32.640
 to build a robot, start with the stomach.

1:15:32.640 --> 1:15:37.080
 And what I mean by that, what I mean by that is a robot has to care, it has to have hunger,

1:15:37.080 --> 1:15:40.640
 it has to care about surviving, that kind of thing.

1:15:40.640 --> 1:15:41.640
 Here's an example.

1:15:41.640 --> 1:15:46.760
 So the penultimate chapter of my book, I titled the wolf in the Mars rover.

1:15:46.760 --> 1:15:52.120
 And I just look at this simple comparison of, you look at a wolf, it gets its leg caught

1:15:52.120 --> 1:15:53.120
 in a trap.

1:15:53.120 --> 1:15:54.120
 What does it do?

1:15:54.120 --> 1:15:58.360
 It gnaws its leg off, and then it figures out how to walk on three legs.

1:15:58.360 --> 1:15:59.360
 No problem.

1:15:59.360 --> 1:16:05.400
 Now the Mars rover, Curiosity, got its front wheel stuck in some Martian soil, and it died.

1:16:05.400 --> 1:16:10.720
 This project cut that cost billions of dollars died because the guy's wheels wouldn't be

1:16:10.720 --> 1:16:15.440
 terrific if we could build a robot that chewed off its front wheel and figured out how to

1:16:15.440 --> 1:16:18.040
 operate with a slightly different body plan.

1:16:18.040 --> 1:16:22.640
 That's the kind of thing that we want to be able to build and to get there.

1:16:22.640 --> 1:16:27.240
 What we need, the whole reason the wolf is able to do that is because its motor and somatic

1:16:27.240 --> 1:16:28.720
 sensory systems are live wired.

1:16:28.720 --> 1:16:31.840
 So it says, oh, you know what, turns out we got a body plan that's different than what

1:16:31.840 --> 1:16:38.920
 I thought a few minutes ago, but I have a yen to survive and I care about relevance,

1:16:38.920 --> 1:16:42.680
 which in this case is getting to food, getting back to my pack and so on.

1:16:42.680 --> 1:16:46.560
 So I'm just going to figure out how to operate with this, oh, that didn't work, oh, okay,

1:16:46.560 --> 1:16:48.640
 I'm kind of getting it to work.

1:16:48.640 --> 1:16:49.960
 But the Mars rover doesn't do that.

1:16:49.960 --> 1:16:53.480
 It just says, oh, geez, I was pre programmed to have four wheels, now I have three, I'm

1:16:53.480 --> 1:16:54.480
 screwed.

1:16:54.480 --> 1:16:58.280
 Yeah, you know, I don't know if you're familiar with a philosopher named Ernest Becker.

1:16:58.280 --> 1:17:03.480
 He wrote a book called The Now of Death, and there's a few psychologists, Sheldon Salmon,

1:17:03.480 --> 1:17:10.080
 I think I just spoke with him in his podcast, who developed terror management theory, which

1:17:10.080 --> 1:17:17.360
 is, like Ernest Becker is a philosopher that basically said that fear of mortality is at

1:17:17.360 --> 1:17:19.120
 the core of it.

1:17:19.120 --> 1:17:25.840
 And so I don't know, it sounds compelling as an idea that all of the civilization we've

1:17:25.840 --> 1:17:28.920
 constructed is based on this, but it's...

1:17:28.920 --> 1:17:30.640
 I'm familiar with his work.

1:17:30.640 --> 1:17:31.640
 Here's what I think.

1:17:31.640 --> 1:17:36.760
 I think that, yes, fundamentally this desire to survive is at the core of it, I would agree

1:17:36.760 --> 1:17:38.240
 with that.

1:17:38.240 --> 1:17:41.720
 But how that expresses itself in your life ends up being very different.

1:17:41.720 --> 1:17:47.760
 The reason you do what you do is, I mean, you could list the 100 reasons why you chose

1:17:47.760 --> 1:17:50.960
 to write your tweet this way and that way, and it really has nothing to do with the survival

1:17:50.960 --> 1:17:51.960
 part.

1:17:51.960 --> 1:17:54.960
 It has to do with trying to impress fellow humans and surprise them and say something.

1:17:54.960 --> 1:17:59.480
 Yeah, so many things built on top of each other, but it's fascinating to think that in

1:17:59.480 --> 1:18:05.480
 artificial intelligence systems, we want to be able to somehow engineer this drive for

1:18:05.480 --> 1:18:11.600
 survival for immortality, I mean, because as humans, we're not just about survival,

1:18:11.600 --> 1:18:16.600
 we're aware of the fact that we're going to die, which is a very kind of, we're aware

1:18:16.600 --> 1:18:17.600
 like space time.

1:18:17.600 --> 1:18:18.600
 Most people aren't, by the way.

1:18:18.600 --> 1:18:19.600
 Aren't?

1:18:19.600 --> 1:18:20.600
 Aren't.

1:18:20.600 --> 1:18:25.960
 Confucius said, he said, each person has two lives.

1:18:25.960 --> 1:18:31.200
 The second one begins when you realize that you have just one, but it takes a long time

1:18:31.200 --> 1:18:32.360
 for most people to get there.

1:18:32.360 --> 1:18:40.080
 I mean, you could argue this kind of Freudian thing, which Urs Becker argues is they actually

1:18:40.080 --> 1:18:47.320
 figured it out early on, and the terror they felt was like the reason it's been suppressed

1:18:47.320 --> 1:18:50.760
 and the reason most people, when I ask them about whether they're afraid of death, they

1:18:50.760 --> 1:18:53.120
 basically say, no.

1:18:53.120 --> 1:19:01.120
 They basically say, I'm afraid I won't submit the paper before I die.

1:19:01.120 --> 1:19:06.880
 They see death as a kind of inconvenient deadline for a particular set of, like a book you're

1:19:06.880 --> 1:19:10.720
 writing, as opposed to like, what the hell?

1:19:10.720 --> 1:19:15.760
 This thing ends at any moment.

1:19:15.760 --> 1:19:22.240
 Most people, as I've encountered, do not meditate on the idea that right now you could die.

1:19:22.240 --> 1:19:30.800
 Right now, in the next five minutes, it could be all over and meditate on that idea.

1:19:30.800 --> 1:19:37.480
 I think that somehow brings you closer to the core of the motivations and the core of

1:19:37.480 --> 1:19:39.480
 the human cognition condition.

1:19:39.480 --> 1:19:43.560
 I think it might be the core, but like I said, it is not what's right now.

1:19:43.560 --> 1:19:47.240
 There's so many things on top of it, but it is interesting.

1:19:47.240 --> 1:19:55.040
 As the ancient poet said, death whispers at my ear live for I come.

1:19:55.040 --> 1:19:59.400
 It is certainly motivating when we think about that, okay, I've got some deadline, I don't

1:19:59.400 --> 1:20:02.280
 know exactly what it is, but I better make stuff happen.

1:20:02.280 --> 1:20:08.040
 It is motivating, but I know for just speaking for me personally, that's not what motivates

1:20:08.040 --> 1:20:09.040
 me day to day.

1:20:09.040 --> 1:20:15.680
 It's instead, oh, I want to get this program up and running before this, or I want to make

1:20:15.680 --> 1:20:18.520
 sure my coauthor isn't mad at me because I haven't gotten this in, or I don't want to

1:20:18.520 --> 1:20:22.080
 miss this grant deadline, or whatever the thing is.

1:20:22.080 --> 1:20:24.080
 It's too distant in a sense.

1:20:24.080 --> 1:20:31.440
 Nevertheless, it is good to reconnect, but for the AI systems, none of that is there.

1:20:31.440 --> 1:20:37.840
 A neural network does not fear its mortality, and that seems to be somehow fundamentally

1:20:37.840 --> 1:20:39.640
 missing the point.

1:20:39.640 --> 1:20:42.720
 I think that's missing the point, but I wonder, it's an interesting speculation about whether

1:20:42.720 --> 1:20:47.280
 you can build an AI system that is much closer to being a human without the mortality and

1:20:47.280 --> 1:20:51.640
 survival piece, but just the thing of relevance.

1:20:51.640 --> 1:20:52.920
 I care about this versus that.

1:20:52.920 --> 1:20:56.040
 Right now, if you have a robot roll into the room, it's going to be frozen because it doesn't

1:20:56.040 --> 1:20:57.720
 have any reason to go there versus there.

1:20:57.720 --> 1:21:05.400
 It doesn't have any particular set of things about, this is how I should navigate my next

1:21:05.400 --> 1:21:07.400
 move because I want something.

1:21:07.400 --> 1:21:08.400
 Yeah.

1:21:08.400 --> 1:21:13.840
 The thing about humans is they seem to generate goals.

1:21:13.840 --> 1:21:19.920
 They're like, you said live wired, I mean, it's very flexible in terms of the goals

1:21:19.920 --> 1:21:23.800
 and creative in terms of the goals we generate when we enter a room.

1:21:23.800 --> 1:21:28.280
 You show up to a party without a goal usually, and then you figure it out along the way.

1:21:28.280 --> 1:21:33.480
 Yes, but this goes back to the question about free will, which is when I walk into the party,

1:21:33.480 --> 1:21:38.400
 if you rewound it 10,000 times, would I go and talk to that couple over there versus

1:21:38.400 --> 1:21:39.400
 that person?

1:21:39.400 --> 1:21:43.920
 Like, I might do this exact same thing every time because I've got some goal stack and

1:21:43.920 --> 1:21:48.000
 I think, okay, well, at this party, I really want to meet these kind of people or I feel

1:21:48.000 --> 1:21:50.880
 awkward or whatever my goals are.

1:21:50.880 --> 1:21:55.560
 By the way, there was something that I meant to mention earlier, if you don't mind going

1:21:55.560 --> 1:22:00.400
 back, which is this, when we were talking about BCI, so I don't know if you know this,

1:22:00.400 --> 1:22:03.480
 but what I'm spending 90% of my time doing now is running a company.

1:22:03.480 --> 1:22:04.480
 Do you know about this?

1:22:04.480 --> 1:22:05.480
 Yes.

1:22:05.480 --> 1:22:08.000
 I wasn't sure what the company is involved in.

1:22:08.000 --> 1:22:09.000
 Right.

1:22:09.000 --> 1:22:10.000
 Being talk about it?

1:22:10.000 --> 1:22:11.000
 Yeah.

1:22:11.000 --> 1:22:12.000
 Yeah.

1:22:12.000 --> 1:22:20.720
 If you're a BCI, you can put stuff into the brain invasively, but my interest has been

1:22:20.720 --> 1:22:24.240
 how you can get data streams into the brain noninvasively.

1:22:24.240 --> 1:22:29.720
 So I run a company called Neosensory and what we build is this little wristband.

1:22:29.720 --> 1:22:31.400
 We've built this in many different factors.

1:22:31.400 --> 1:22:32.400
 Oh, that's it?

1:22:32.400 --> 1:22:33.400
 Yeah, this is it.

1:22:33.400 --> 1:22:35.560
 And it's got these vibratory motors in it.

1:22:35.560 --> 1:22:40.960
 So these things, as I'm speaking, for example, it's capturing my voice and running algorithms

1:22:40.960 --> 1:22:44.600
 and then turning that into patterns of vibration here.

1:22:44.600 --> 1:22:50.920
 So people who are deaf, for example, learn to hear through their skin.

1:22:50.920 --> 1:22:55.920
 So the information is getting up to their brain this way and they learn how to hear.

1:22:55.920 --> 1:22:59.640
 So it turns out on day one, people are pretty good, like better than you'd expect at being

1:22:59.640 --> 1:23:01.520
 able to say, oh, that's weird.

1:23:01.520 --> 1:23:02.520
 Was that a dog barking?

1:23:02.520 --> 1:23:03.520
 Was that a baby crying?

1:23:03.520 --> 1:23:04.520
 Was that a door knock?

1:23:04.520 --> 1:23:05.520
 A doorbell?

1:23:05.520 --> 1:23:09.560
 Like people are pretty good at it, but with time they get better and better and what

1:23:09.560 --> 1:23:12.440
 it becomes is a new qualia.

1:23:12.440 --> 1:23:15.520
 In other words, a new subjective internal experience.

1:23:15.520 --> 1:23:18.840
 So on day one, they say, whoa, what was it?

1:23:18.840 --> 1:23:20.800
 Oh, that was the dog barking.

1:23:20.800 --> 1:23:24.600
 But by three months later, they say, oh, there's a dog barking somewhere.

1:23:24.600 --> 1:23:25.600
 Oh, there's the dog.

1:23:25.600 --> 1:23:26.600
 That's fascinating.

1:23:26.600 --> 1:23:29.480
 And by the way, that's exactly how you learn how to use your ears.

1:23:29.480 --> 1:23:33.160
 So of course, you don't remember this, but when you were an infant, all you have are

1:23:33.160 --> 1:23:38.960
 your eardrum vibrating causes spikes to go down, your auditory nerves and impinging your

1:23:38.960 --> 1:23:39.960
 auditory cortex.

1:23:39.960 --> 1:23:44.800
 Your brain doesn't know what those mean automatically, but what happens is you learn how to hear

1:23:44.800 --> 1:23:49.160
 by looking for correlations, you know, you clap your hands as a baby, you know, you look

1:23:49.160 --> 1:23:53.480
 at your mother's mouth moving and that correlates with what's going on there.

1:23:53.480 --> 1:23:57.280
 And eventually your brain says, all right, I'm just going to summarize this as an internal

1:23:57.280 --> 1:23:59.600
 experience, as a conscious experience.

1:23:59.600 --> 1:24:01.360
 And that's exactly what happens here.

1:24:01.360 --> 1:24:05.520
 The weird part is that you can feed data into the brain, not through the ears, but through

1:24:05.520 --> 1:24:09.240
 any channel that gets there, as long as the information gets there, your brain figures

1:24:09.240 --> 1:24:10.240
 out what to do with it.

1:24:10.240 --> 1:24:11.240
 That's fascinating.

1:24:11.240 --> 1:24:20.440
 And like expanding the set of sensors, it could be arbitrarily, could expand arbitrarily,

1:24:20.440 --> 1:24:21.440
 which is fascinating.

1:24:21.440 --> 1:24:22.440
 Well, exactly.

1:24:22.440 --> 1:24:25.920
 And by the way, the reason I use this skin, you know, there's all kinds of cool stuff

1:24:25.920 --> 1:24:28.320
 going on in the AR world with glasses.

1:24:28.320 --> 1:24:31.400
 But the fact is your eyes are overtaxed and your ears are overtaxed and you need to be

1:24:31.400 --> 1:24:36.240
 able to see and hear other stuff, but you're covered with the skin, which is this incredible

1:24:36.240 --> 1:24:40.920
 computational material with which you can feed information and we don't use our skin

1:24:40.920 --> 1:24:43.600
 for much of anything nowadays.

1:24:43.600 --> 1:24:46.680
 My joke in the lab is that I say we don't call this the waste for nothing because originally

1:24:46.680 --> 1:24:51.760
 we built this the vest and you're passing in all this information that way.

1:24:51.760 --> 1:24:59.520
 And what I'm doing here with the deaf community is what's called sensory substitution, where

1:24:59.520 --> 1:25:04.040
 I'm capturing sound and sent, you know, I'm just replacing the ears with the skin and

1:25:04.040 --> 1:25:05.040
 that works.

1:25:05.040 --> 1:25:10.000
 One of the things I talk about live wire is sensory expansion.

1:25:10.000 --> 1:25:13.760
 So what if you took something like your visual system, which picks up on a very thin slice

1:25:13.760 --> 1:25:18.880
 of the electromagnetic spectrum and you could see infrared or ultraviolet.

1:25:18.880 --> 1:25:22.760
 So we've hooked that up infrared and ultraviolet detectors and you know, I can feel what's

1:25:22.760 --> 1:25:23.760
 going on.

1:25:23.760 --> 1:25:26.480
 So just as an example, the first night I built the infrared, one of my engineers built

1:25:26.480 --> 1:25:30.080
 that the infrared detector, I was walking in the dark between two houses and suddenly

1:25:30.080 --> 1:25:32.040
 I felt all this infrared radiation.

1:25:32.040 --> 1:25:33.040
 I was like, where does that come from?

1:25:33.040 --> 1:25:37.520
 And I just followed my wrist and I found an infrared camera, a night vision camera that

1:25:37.520 --> 1:25:40.720
 was, but like, you know, I immediately, oh, there's that thing there.

1:25:40.720 --> 1:25:45.880
 Of course, I would have never seen it, but now it's just part of my reality.

1:25:45.880 --> 1:25:46.880
 That's fascinating.

1:25:46.880 --> 1:25:47.880
 Yeah.

1:25:47.880 --> 1:25:50.400
 And then of course, what I'm really interested in is sensory addition.

1:25:50.400 --> 1:25:55.280
 What if you could pick up on stuff that isn't even part of what we normally pick up

1:25:55.280 --> 1:26:00.120
 on like, you know, like the magnetic field of the earth or Twitter or stock market or

1:26:00.120 --> 1:26:01.120
 things like that.

1:26:01.120 --> 1:26:04.480
 Or the, I don't know, some weird stuff, like the moons of other people or something like

1:26:04.480 --> 1:26:05.480
 that.

1:26:05.480 --> 1:26:06.480
 Sure.

1:26:06.480 --> 1:26:07.480
 Now what you need is a way to measure that.

1:26:07.480 --> 1:26:09.760
 So as long as there's a machine that can measure it, it's easy, it's trivial to feed

1:26:09.760 --> 1:26:14.840
 this in here and you come to be, it comes to be part of your reality.

1:26:14.840 --> 1:26:16.760
 It's like you have another sensor.

1:26:16.760 --> 1:26:21.800
 And that kind of thing is without doing like, if you look in your link without, I forgot

1:26:21.800 --> 1:26:26.480
 how you put it, but it was eloquent, you know, without getting, cutting into the brain basically.

1:26:26.480 --> 1:26:27.480
 Yeah, exactly.

1:26:27.480 --> 1:26:28.480
 Exactly.

1:26:28.480 --> 1:26:30.480
 So this, this costs at the moment, $399.

1:26:30.480 --> 1:26:32.600
 That's not going to kill you.

1:26:32.600 --> 1:26:33.880
 It's not going to kill you.

1:26:33.880 --> 1:26:36.520
 You just put it on and when you're done, you take it off.

1:26:36.520 --> 1:26:37.520
 Yeah.

1:26:37.520 --> 1:26:41.880
 And so, and the name of the company, by the way, is Neo Sensory for new senses because

1:26:41.880 --> 1:26:44.040
 the whole idea is beautiful.

1:26:44.040 --> 1:26:47.720
 You can, as I said, you know, you come to the table with certain plug and play devices

1:26:47.720 --> 1:26:48.720
 and then that's it.

1:26:48.720 --> 1:26:51.520
 Like I can pick up on this little bit of the electromagnetic radiation and pick up on,

1:26:51.520 --> 1:26:56.720
 on this little frequency band for hearing and so on, but, but, but I'm stuck there and

1:26:56.720 --> 1:26:58.240
 there's no reason we have to be stuck there.

1:26:58.240 --> 1:27:01.800
 We can expand our umvelte by adding new senses.

1:27:01.800 --> 1:27:02.800
 Yeah.

1:27:02.800 --> 1:27:03.800
 What's umvelte?

1:27:03.800 --> 1:27:04.800
 Oh, I'm sorry.

1:27:04.800 --> 1:27:07.000
 The umvelte is the slice of reality that you pick up on.

1:27:07.000 --> 1:27:08.520
 So each animal has its own.

1:27:08.520 --> 1:27:09.520
 Hell of a word.

1:27:09.520 --> 1:27:10.520
 Umvelte.

1:27:10.520 --> 1:27:11.520
 Yeah, exactly.

1:27:11.520 --> 1:27:12.520
 Nice.

1:27:12.520 --> 1:27:13.520
 I'm sorry, I forgot to define it before.

1:27:13.520 --> 1:27:19.080
 It's, it's, it's such an important concept, which is to say, for example, if you are a

1:27:19.080 --> 1:27:24.040
 tick, you pick up on butyric acid, you pick up on odor and you pick up on temperature.

1:27:24.040 --> 1:27:25.040
 That's it.

1:27:25.040 --> 1:27:27.280
 That's how you construct your realities with those two sensors.

1:27:27.280 --> 1:27:31.560
 If you are a blind echolocating bat, you're picking up on air compression waves coming

1:27:31.560 --> 1:27:33.000
 back, you know, echolocation.

1:27:33.000 --> 1:27:38.280
 If you are the black ghost knife fish, you're picking up on changes in the electrical field

1:27:38.280 --> 1:27:40.840
 around you with electroreception.

1:27:40.840 --> 1:27:43.680
 That's how they swim around and tell there's a rock there and so on.

1:27:43.680 --> 1:27:45.560
 But that's, that's all they pick up on.

1:27:45.560 --> 1:27:47.040
 That's their umvelte.

1:27:47.040 --> 1:27:51.600
 It's, that's their, the signals they get from the world from which to construct their reality

1:27:51.600 --> 1:27:54.480
 and they can be totally different umvelts.

1:27:54.480 --> 1:27:59.920
 And so our human umvelte is, you know, we've got little bits that we can pick up on.

1:27:59.920 --> 1:28:04.920
 One of the things I like to do with my students is talk about, um, imagine that you are a

1:28:04.920 --> 1:28:06.200
 bloodhound dog, right?

1:28:06.200 --> 1:28:10.120
 You are a bloodhound dog with a huge snout with 200 million cent receptors in it and

1:28:10.120 --> 1:28:11.720
 your whole world is about smelling.

1:28:11.720 --> 1:28:15.760
 You know, you've got slits in your nostril, take big nosefuls of air and so on.

1:28:15.760 --> 1:28:16.760
 Do you have a dog?

1:28:16.760 --> 1:28:17.760
 Nope, used to.

1:28:17.760 --> 1:28:18.760
 Used to.

1:28:18.760 --> 1:28:19.760
 Okay, right.

1:28:19.760 --> 1:28:21.440
 So, you know, you walk your dog around and your dog is smelling everything.

1:28:21.440 --> 1:28:25.080
 The whole world is full of signals that you do not pick up on and so imagine if you were

1:28:25.080 --> 1:28:28.400
 that dog and you looked at your human master and thought, my God, what is it like to have

1:28:28.400 --> 1:28:30.880
 the pitiful little nose of a human?

1:28:30.880 --> 1:28:33.840
 How could you not know that there's a cat 100 yards away or that your friend was here

1:28:33.840 --> 1:28:35.000
 six hours ago?

1:28:35.000 --> 1:28:39.160
 And so the idea is because we're stuck in our umvelte, because we have this little pitiful

1:28:39.160 --> 1:28:43.560
 nose is we think, okay, well, yeah, we're seeing reality, but, but you can have very

1:28:43.560 --> 1:28:47.360
 different sorts of realities depending on the peripheral plug and play devices you're

1:28:47.360 --> 1:28:48.360
 equipped with.

1:28:48.360 --> 1:28:54.160
 It's fascinating to think that, like, if we're being honest, probably our umvelte is, you

1:28:54.160 --> 1:29:03.600
 know, some infinitely tiny percent of the possibilities of how you can sense quote, unquote, reality,

1:29:03.600 --> 1:29:04.600
 even if you could.

1:29:04.600 --> 1:29:12.440
 I mean, there's a guy named Donald Hoffman, yeah, who basically says we're really far

1:29:12.440 --> 1:29:16.000
 away from reality in terms of our ability to sense anything.

1:29:16.000 --> 1:29:21.000
 Like we're very, we're almost like we're floating out there.

1:29:21.000 --> 1:29:24.280
 That's almost like completely to attach to the actual physical reality.

1:29:24.280 --> 1:29:29.200
 It's fascinating that we can have extra senses that could help us get a little bit a little

1:29:29.200 --> 1:29:30.200
 bit closer.

1:29:30.200 --> 1:29:31.200
 Exactly.

1:29:31.200 --> 1:29:35.880
 And by the way, this has been the, the fruits of science is realized, like, yeah, for example,

1:29:35.880 --> 1:29:38.320
 you know, you open your eyes and there's the world around you, right?

1:29:38.320 --> 1:29:42.360
 But of course, depending on how you calculate it, it's less than a 10 trillion of the

1:29:42.360 --> 1:29:44.960
 electromagnetic spectrum that we call visible light.

1:29:44.960 --> 1:29:48.680
 The reason I say it depends, because, you know, it's actually infinite in all directions,

1:29:48.680 --> 1:29:49.680
 presumably.

1:29:49.680 --> 1:29:50.680
 Yeah.

1:29:50.680 --> 1:29:51.680
 And so that's exactly that.

1:29:51.680 --> 1:29:55.240
 And then science allows you to actually look into the rest of it.

1:29:55.240 --> 1:29:56.240
 Exactly.

1:29:56.240 --> 1:29:57.240
 Sort of understanding how big the world is out there.

1:29:57.240 --> 1:30:00.240
 And the same with the world of really small and the world of really large.

1:30:00.240 --> 1:30:01.240
 Exactly.

1:30:01.240 --> 1:30:03.000
 That does beyond our ability to sense.

1:30:03.000 --> 1:30:04.000
 Exactly.

1:30:04.000 --> 1:30:07.760
 And so the reason I think this kind of thing matters is because we now have an opportunity

1:30:07.760 --> 1:30:12.680
 for that first time in human history to say, okay, well, I'm just going to include other

1:30:12.680 --> 1:30:13.680
 things in my umbel.

1:30:13.680 --> 1:30:18.040
 So I'm going to include infrared radiation and, and have a direct perceptual experience

1:30:18.040 --> 1:30:19.200
 of that.

1:30:19.200 --> 1:30:23.400
 And so I'm very, you know, I mean, so, you know, I've given up my lab and I run this

1:30:23.400 --> 1:30:25.600
 company 90% of my time now.

1:30:25.600 --> 1:30:26.600
 That's what I'm doing.

1:30:26.600 --> 1:30:29.880
 I still teach at Stanford and I'm, you know, teaching courses and stuff like that.

1:30:29.880 --> 1:30:33.080
 But this is like a, this is your, your passion.

1:30:33.080 --> 1:30:34.960
 The fire is, is, is on this.

1:30:34.960 --> 1:30:35.960
 Yeah.

1:30:35.960 --> 1:30:37.600
 I feel like this is the most important thing that's

1:30:37.600 --> 1:30:38.880
 happening right now.

1:30:38.880 --> 1:30:42.480
 I mean, obviously I think that because that's what I'm devoting my time in my life to, but

1:30:42.480 --> 1:30:43.480
 um.

1:30:43.480 --> 1:30:45.280
 I mean, it's a brilliant set of ideas.

1:30:45.280 --> 1:30:53.040
 It certainly is like it, um, it's a step in, uh, in a very vibrant future, I would say.

1:30:53.040 --> 1:30:56.120
 Like the possibilities there are, are endless.

1:30:56.120 --> 1:30:57.120
 Exactly.

1:30:57.120 --> 1:31:01.160
 So if you ask what I think about Neural Link, I think it's amazing what those guys are doing

1:31:01.160 --> 1:31:04.920
 and working on, but I think it's not practical for almost everybody.

1:31:04.920 --> 1:31:09.200
 For example, for people who are deaf, they buy this and, you know, every day we're getting

1:31:09.200 --> 1:31:12.840
 tons of emails and tweets or whatever from people saying, wow, I picked up on this and

1:31:12.840 --> 1:31:16.920
 then I had no idea that was a, I didn't even know that was happening out there and they're

1:31:16.920 --> 1:31:21.320
 coming to hear, by the way, this is, you know, less than a tenth of the price of a hearing

1:31:21.320 --> 1:31:25.320
 aid and like 250 times less than a cochlear implant.

1:31:25.320 --> 1:31:27.360
 That's amazing.

1:31:27.360 --> 1:31:33.240
 People love hearing about, uh, what, you know, brilliant folks like yourself, uh, could

1:31:33.240 --> 1:31:37.080
 recommend in terms of books, of course you're an author of many books.

1:31:37.080 --> 1:31:40.480
 So I'll, in the introduction, mentioned all the books you've written.

1:31:40.480 --> 1:31:42.480
 People should definitely read LiveWired.

1:31:42.480 --> 1:31:44.960
 I've gotten a chance to read some of it and it's amazing.

1:31:44.960 --> 1:31:52.200
 But is there three books, technical, fiction, philosophical, that had an impact on you when

1:31:52.200 --> 1:31:58.880
 you were younger or today and, uh, books, perhaps some of which you would, uh, want to

1:31:58.880 --> 1:32:03.200
 recommend that others read, uh, you know, as an undergraduate, I majored in British

1:32:03.200 --> 1:32:04.200
 and American literature.

1:32:04.200 --> 1:32:06.760
 That was my major because I love literature.

1:32:06.760 --> 1:32:08.680
 I grew up with, um, literature.

1:32:08.680 --> 1:32:13.680
 My father had these extensive bookshelves and so I grew up in the mountains in New Mexico

1:32:13.680 --> 1:32:16.240
 and so that was mostly why I spent my time was reading books.

1:32:16.240 --> 1:32:23.640
 But, um, you know, I love, uh, you know, Faulkner, Hemingway.

1:32:23.640 --> 1:32:28.200
 I love many South American authors, Gabriel Garcia Marquez and Italo Calvino.

1:32:28.200 --> 1:32:29.840
 I would actually recommend Invisible Cities.

1:32:29.840 --> 1:32:31.560
 I just, I loved that book.

1:32:31.560 --> 1:32:33.760
 Uh, Italo Calvino, sorry.

1:32:33.760 --> 1:32:40.040
 It's a book of fiction, um, uh, Anthony Doar wrote a book called All the Light We Cannot

1:32:40.040 --> 1:32:45.880
 See, which actually, uh, was inspired by incognito by exactly what we were talking about earlier

1:32:45.880 --> 1:32:50.760
 about how you can only see a little bit of the, what we call visible light in the electromagnetic

1:32:50.760 --> 1:32:51.760
 radiation.

1:32:51.760 --> 1:32:54.800
 I wrote about this in incognito and then he reviewed incognito for the Washington Post.

1:32:54.800 --> 1:32:55.800
 Oh no, that's awesome.

1:32:55.800 --> 1:32:59.120
 And then he wrote this book called, the book has nothing to do with that, but that's where

1:32:59.120 --> 1:33:00.120
 the title comes from.

1:33:00.120 --> 1:33:01.120
 Yeah.

1:33:01.120 --> 1:33:06.080
 The only thing that we cannot see is about the rest of the spectrum, but, um, the, that's

1:33:06.080 --> 1:33:07.760
 a absolutely gorgeous book.

1:33:07.760 --> 1:33:09.280
 That's a book of fiction.

1:33:09.280 --> 1:33:10.280
 Yeah, it's a book of fiction.

1:33:10.280 --> 1:33:12.280
 What's, what's it about?

1:33:12.280 --> 1:33:16.600
 It takes place during World War II, uh, about these two young people, one of whom is blind.

1:33:16.600 --> 1:33:17.600
 Got it.

1:33:17.600 --> 1:33:18.600
 And, um, yeah.

1:33:18.600 --> 1:33:19.600
 Anything else?

1:33:19.600 --> 1:33:21.880
 So, what any, so you mentioned Hemingway.

1:33:21.880 --> 1:33:27.720
 I mean, all men, uh, all men on the sea, uh, what, uh, what's your favorite, uh, um,

1:33:27.720 --> 1:33:32.480
 Snow's a Kilimanjaro, uh, collection of short stories I love, um, as far as not, as far

1:33:32.480 --> 1:33:37.800
 as nonfiction goes, I grew up, uh, with Cosmos, both watching the PBS series that read the

1:33:37.800 --> 1:33:41.040
 book and that influenced me a huge amount in terms of what I do.

1:33:41.040 --> 1:33:45.560
 I, as from the time I was a kid, I felt like I want to be Carl Sagan, like I just, that's

1:33:45.560 --> 1:33:46.560
 what I left.

1:33:46.560 --> 1:33:51.040
 And in the end, I just, you know, I studied space physics for a while as an undergrad,

1:33:51.040 --> 1:33:56.000
 but then I, in my last semester, discovered neuroscience last semester, and I just thought

1:33:56.000 --> 1:33:57.640
 wow, I'm hooked on that.

1:33:57.640 --> 1:34:03.200
 So the Carl Sagan of the brain is the aspiration.

1:34:03.200 --> 1:34:04.200
 Yeah.

1:34:04.200 --> 1:34:08.000
 I mean, uh, you're doing, you're doing, um, an incredible job of it.

1:34:08.000 --> 1:34:13.840
 So you opened the book live wide with a quote by Heidegger, every man is born as many men

1:34:13.840 --> 1:34:16.080
 and dies as a single one.

1:34:16.080 --> 1:34:19.000
 Uh, well, what do you mean?

1:34:19.000 --> 1:34:20.000
 Or what?

1:34:20.000 --> 1:34:21.440
 Well, I'll tell you what I meant by it.

1:34:21.440 --> 1:34:22.440
 I'll tell you.

1:34:22.440 --> 1:34:25.760
 So he, he had his own reason why he was writing that, but I meant this in terms of brain plasticity

1:34:25.760 --> 1:34:29.560
 in terms of library, which is this issue that I mentioned before about this, you know, this

1:34:29.560 --> 1:34:35.960
 cone, the space time cone that we are in, which is that when you dropped into the world,

1:34:35.960 --> 1:34:38.040
 you Lex had all this different potential.

1:34:38.040 --> 1:34:44.160
 You could have been a great surfer or a great chess player or a, you could have been thousands

1:34:44.160 --> 1:34:46.360
 of different men when you grew up.

1:34:46.360 --> 1:34:50.760
 But what you did is things that were not your choice and your choice along the way,

1:34:50.760 --> 1:34:54.240
 you know, you ended up navigating a particular path and now you're exactly who you are.

1:34:54.240 --> 1:34:59.200
 You used to have lots of potential, but the day you die, you will be exactly Lex.

1:34:59.200 --> 1:35:01.120
 You will be that one person.

1:35:01.120 --> 1:35:02.120
 Yeah.

1:35:02.120 --> 1:35:07.360
 So on that, in that context, I mean, first of all, it's just a beautiful, it's a humbling

1:35:07.360 --> 1:35:12.800
 picture, but it's a beautiful one because it's, uh, all the possible trajectories and

1:35:12.800 --> 1:35:16.440
 you pick one, you walk down that road and it's the Robert Frost poem.

1:35:16.440 --> 1:35:21.520
 But on that topic, let me ask the, the biggest and the most ridiculous question.

1:35:21.520 --> 1:35:25.400
 So in this live wide brain, when we choose all these different trajectories and end up

1:35:25.400 --> 1:35:28.160
 with one, what's the meaning of it all?

1:35:28.160 --> 1:35:32.320
 What's, uh, is there, is there a why here?

1:35:32.320 --> 1:35:34.200
 What's the meaning of life?

1:35:34.200 --> 1:35:35.200
 Yeah.

1:35:35.200 --> 1:35:36.200
 David Engelman.

1:35:36.200 --> 1:35:37.200
 That's it.

1:35:37.200 --> 1:35:45.040
 I mean, this is the question that everyone has attacked from their own live art point

1:35:45.040 --> 1:35:49.680
 of view, by which I mean culturally, if you grow up in a religious society, you have one

1:35:49.680 --> 1:35:53.120
 way of attacking that question, so if you grow up in a secular or scientific society,

1:35:53.120 --> 1:35:55.320
 you have a different way of attacking that question.

1:35:55.320 --> 1:35:57.360
 Obviously, I don't know.

1:35:57.360 --> 1:35:59.680
 I abstain on that question.

1:35:59.680 --> 1:36:00.680
 Yeah.

1:36:00.680 --> 1:36:05.480
 Uh, I mean, I think one of the fundamental things, I guess, in that, in all those possible

1:36:05.480 --> 1:36:12.000
 trajectories is, uh, you're always asking, I mean, that's the act of asking what the

1:36:12.000 --> 1:36:19.440
 heck is this thing for is equivalent to, or at least runs in parallel to all the choices

1:36:19.440 --> 1:36:23.320
 that you're making because it's kind of, that's the underlying question.

1:36:23.320 --> 1:36:24.320
 Well, that's right.

1:36:24.320 --> 1:36:27.960
 And by the way, you know, this is the interesting thing about human psychology, you know, we've

1:36:27.960 --> 1:36:30.920
 got all these layers of things at which we can ask questions.

1:36:30.920 --> 1:36:35.840
 And so if you keep asking yourself the question about what is the optimal way for me to be

1:36:35.840 --> 1:36:36.840
 spending my time?

1:36:36.840 --> 1:36:37.840
 What should I be doing?

1:36:37.840 --> 1:36:38.840
 What charity should I get involved with?

1:36:38.840 --> 1:36:44.720
 And so if you're asking those big questions that, that steers you appropriately.

1:36:44.720 --> 1:36:47.520
 If you're the type of person who never asks, Hey, is there something better I could be

1:36:47.520 --> 1:36:52.720
 doing with my time, then presumably you won't optimize whatever it is that is important

1:36:52.720 --> 1:36:53.720
 to you.

1:36:53.720 --> 1:37:01.000
 So you've, uh, I think just in your eyes, in your work, there's a passion, uh, that

1:37:01.000 --> 1:37:05.200
 just is obvious and it's inspiring, it's contagious.

1:37:05.200 --> 1:37:12.640
 What, um, if you were to give advice to us, a young person today in the crazy chaos that

1:37:12.640 --> 1:37:21.680
 we live today, uh, about life, about how to, how to, uh, how to discover their passion.

1:37:21.680 --> 1:37:25.360
 Is there some words that you could give?

1:37:25.360 --> 1:37:30.420
 First of all, I would say the main thing for a young person is stay adaptable.

1:37:30.420 --> 1:37:34.400
 And this is back to this issue of why COVID is useful for us because it forces us off

1:37:34.400 --> 1:37:36.160
 our tracks.

1:37:36.160 --> 1:37:40.520
 The fact is the jobs that will exist 20 years from now, we don't even have names for it.

1:37:40.520 --> 1:37:43.120
 We can't even imagine the jobs that are going to exist.

1:37:43.120 --> 1:37:46.640
 And so when young people that I know go into college and they say, Hey, what should I major

1:37:46.640 --> 1:37:48.320
 in and so on?

1:37:48.320 --> 1:37:52.560
 College is and should be less and less vocational as in, Oh, I'm going to learn how to do this

1:37:52.560 --> 1:37:54.600
 and then I'm going to do that the rest of my career.

1:37:54.600 --> 1:37:58.400
 The world just isn't that way anymore with the, the exponential speed of things.

1:37:58.400 --> 1:38:04.240
 So the important thing is learning how to learn, learning how to be live wired and adaptable.

1:38:04.240 --> 1:38:05.240
 That's really key.

1:38:05.240 --> 1:38:10.360
 And what I tell, what I advise young people when I talk to them is, you know, what you

1:38:10.360 --> 1:38:16.320
 digest that, that's what gives you the raw storehouse of things that you can remix and

1:38:16.320 --> 1:38:17.920
 be creative with.

1:38:17.920 --> 1:38:21.960
 And so eat broadly and widely.

1:38:21.960 --> 1:38:25.040
 And obviously this is the wonderful thing about the internet world we live in now is

1:38:25.040 --> 1:38:26.040
 you kind of can't help it.

1:38:26.040 --> 1:38:29.320
 You're constantly, whoa, I didn't, you know, you go down some molehole of Wikipedia and

1:38:29.320 --> 1:38:31.160
 you think, Oh, I didn't even realize that was a thing.

1:38:31.160 --> 1:38:32.760
 I didn't know that existed.

1:38:32.760 --> 1:38:35.480
 And so embrace that, embrace that.

1:38:35.480 --> 1:38:36.480
 Yeah, exactly.

1:38:36.480 --> 1:38:41.320
 And what I tell people is just always do a gut check about, okay, I'm reading this paper

1:38:41.320 --> 1:38:47.320
 and yeah, I think that, but this paper, wow, that really, I really cared about that.

1:38:47.320 --> 1:38:50.600
 I tell them just to keep a real sniff out for that.

1:38:50.600 --> 1:38:53.360
 And when you find those things, keep going down those paths.

1:38:53.360 --> 1:38:55.040
 Yeah, don't be afraid.

1:38:55.040 --> 1:38:59.360
 I mean, that's one of the, the challenges and the downsides of having so many beautiful

1:38:59.360 --> 1:39:04.920
 options is that sometimes people are a little bit afraid to really commit, but that that's

1:39:04.920 --> 1:39:05.920
 very true.

1:39:05.920 --> 1:39:10.720
 I mean, if there's something that just sparks your interest and passion, just run with it.

1:39:10.720 --> 1:39:16.560
 I mean, that's, it goes back to the Hydra quote, I mean, we only get this one life and

1:39:16.560 --> 1:39:20.320
 that trajectory, it doesn't last forever.

1:39:20.320 --> 1:39:25.160
 So just if something sparks your imagination, your passion is run with it.

1:39:25.160 --> 1:39:26.160
 Yeah, exactly.

1:39:26.160 --> 1:39:29.880
 I don't think there's a more beautiful way to end it.

1:39:29.880 --> 1:39:32.720
 David, it's a huge honor to finally meet you.

1:39:32.720 --> 1:39:36.280
 Your work is inspiring so many people, I've talked to so many people who are passionate

1:39:36.280 --> 1:39:40.960
 about neuroscience, about the brain, even outside that read your book.

1:39:40.960 --> 1:39:43.720
 So I hope, I hope you keep doing so.

1:39:43.720 --> 1:39:46.080
 I, I think you're already there with Carl Sagan.

1:39:46.080 --> 1:39:47.600
 I hope you continue growing.

1:39:47.600 --> 1:39:50.200
 Um, yeah, it was a honor talking with you today.

1:39:50.200 --> 1:39:51.200
 Thanks so much.

1:39:51.200 --> 1:39:52.200
 Great, you too, Lex.

1:39:52.200 --> 1:39:53.200
 Wonderful.

1:39:53.200 --> 1:39:58.080
 Thanks for listening to this conversation with David Eagleman and thank you to our sponsors,

1:39:58.080 --> 1:40:01.680
 Athletic Greens, BetterHelp and Cash App.

1:40:01.680 --> 1:40:06.720
 Click the sponsor links in the description to get a discount and to support this podcast.

1:40:06.720 --> 1:40:11.960
 If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple podcast,

1:40:11.960 --> 1:40:18.520
 follow on Spotify, support on Patreon or connect with me on Twitter at Lex Freedman.

1:40:18.520 --> 1:40:23.400
 And now let me leave you with some words from David Eagleman in his book, Some, Four Details

1:40:23.400 --> 1:40:25.560
 from the Afterlives.

1:40:25.560 --> 1:40:30.400
 Imagine for a moment, there were nothing but the product of billions of years of molecules

1:40:30.400 --> 1:40:35.240
 coming together and ratcheting up through natural selection.

1:40:35.240 --> 1:40:40.240
 There were composed only of highways of fluids and chemicals sliding along roadways within

1:40:40.240 --> 1:40:46.400
 billions of dancing cells, the trillions of synaptic connections hum in parallel that

1:40:46.400 --> 1:40:53.640
 this vast egg like fabric of micro thin circuitry runs algorithms undreamt of in modern science

1:40:53.640 --> 1:41:01.200
 and that these neural programs give rise to our decision making, loves, desires, fears

1:41:01.200 --> 1:41:03.080
 and aspirations.

1:41:03.080 --> 1:41:09.640
 To me, understanding this would be a numinous experience, better than anything ever proposed

1:41:09.640 --> 1:41:12.240
 in any holy text.

1:41:12.240 --> 1:41:25.760
 Thank you for listening and hope to see you next time.

