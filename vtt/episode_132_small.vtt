WEBVTT

00:00.000 --> 00:05.440
 The following is a conversation with George Hots, a.k.a. Geohot, his second time in the

00:05.440 --> 00:12.800
 podcast. He's the founder of Kama AI, an autonomous and semi autonomous vehicle technology company

00:12.800 --> 00:20.400
 that seeks to be, to Tesla autopilot, what Android is, to the iOS. They sell the Kama

00:20.400 --> 00:26.480
 2 device for $1,000 that when installed in many of their supported cars can keep the vehicle

00:26.480 --> 00:33.120
 centered in the lane even when there are no lane markings. It includes driver sensing that ensures

00:33.120 --> 00:38.640
 that the driver's eyes are on the road. As you may know, I'm a big fan of driver sensing. I do

00:38.640 --> 00:44.640
 believe Tesla autopilot and others should definitely include it in their sensor suite. Also, I'm a fan

00:44.640 --> 00:50.640
 of Android and a big fan of George, for many reasons, including his nonlinear out of the box

00:50.640 --> 00:56.800
 brilliance and the fact that he's a superstar programmer of a very different style than myself.

00:57.360 --> 01:03.360
 Styles make fights and styles make conversations. So I really enjoyed this chat. I'm sure we'll

01:03.360 --> 01:08.560
 talk many more times on this podcast. Quick mention of a sponsor followed by some thoughts

01:08.560 --> 01:15.920
 related to the episode. First is ForSigmatic, the maker of delicious mushroom coffee. Second

01:15.920 --> 01:21.120
 is the coding digital, a podcast on tech and entrepreneurship that I listen to and enjoy.

01:22.000 --> 01:28.480
 And finally, ExpressVPN, the VPN I've used for many years to protect my privacy on the internet.

01:29.200 --> 01:34.640
 Please check out the sponsors in the description to get a discount and to support this podcast.

01:34.640 --> 01:40.400
 As a side note, let me say that my work at MIT on autonomous and semi autonomous vehicles

01:40.400 --> 01:46.480
 led me to study the human side of autonomy enough to understand that it's a beautifully complicated

01:46.480 --> 01:52.320
 and interesting problem space much richer than what can be studied in the lab. In that sense,

01:52.320 --> 01:58.400
 the data that comma AI, Tesla autopilot and perhaps others like Cadillac supercruiser collecting

01:58.400 --> 02:05.360
 gives us a chance to understand how we can design safe semi autonomous vehicles for real human beings

02:05.360 --> 02:11.600
 in real world conditions. I think this requires bold innovation and a serious exploration of the

02:11.600 --> 02:17.760
 first principles of the driving task itself. If you enjoyed this thing, subscribe on YouTube,

02:17.760 --> 02:23.360
 review it with five stars and up a podcast, follow on Spotify, support on Patreon or connect with me

02:23.360 --> 02:31.920
 on Twitter at Lex Friedman. And now here's my conversation with George Hottz. So last time

02:31.920 --> 02:36.640
 we started talking about the simulation, this time let me ask you, do you think there's intelligent

02:36.640 --> 02:42.080
 life out there in the universe? I always maintained my answer to the Fermi paradox. I think there

02:42.720 --> 02:47.760
 has been intelligent life elsewhere in the universe. So intelligent civilizations existed,

02:47.760 --> 02:51.360
 but they've blown themselves up. So your general intuition is that intelligent

02:52.080 --> 02:58.560
 civilizations quickly, like there's that parameter in the Drake equation, your sense is they don't

02:58.560 --> 03:04.160
 last very long. How are we doing on that? Have we lasted pretty good? How do we do?

03:05.360 --> 03:13.360
 Yeah. I mean, not quite yet. I tell you, as Yuckowski, IQ required to destroy the world

03:13.360 --> 03:20.160
 falls by one point every year. Okay. So technology democratizes the destruction of the world.

03:20.880 --> 03:22.240
 When can a meme destroy the world?

03:22.240 --> 03:27.120
 It kind of is already, right?

03:27.120 --> 03:32.960
 Somewhat. I don't think we've seen anywhere near the worst of it yet. World's going to get weird.

03:33.760 --> 03:39.360
 Well, maybe a meme can save the world. You thought about that, the meme Lord Elon Musk fighting on

03:39.360 --> 03:46.800
 the side of good versus the meme Lord of the darkness, which is not saying anything bad about

03:46.800 --> 03:52.960
 Donald Trump, but he is the Lord of the meme on the dark side. He's a Darth Vader of memes.

03:53.600 --> 03:59.840
 I think in every fairy tale, they always end it with, and they lived happily ever after. And

03:59.840 --> 04:04.560
 I'm like, please tell me more about this happily ever after. I've heard 50% of marriages end in

04:04.560 --> 04:09.360
 divorce. Why doesn't your marriage end up there? You can't just say happily ever after. So

04:10.880 --> 04:16.320
 the thing about destruction is it's over after the destruction. We have to do everything right

04:16.320 --> 04:21.920
 in order to avoid it. And one thing wrong. Actually, this is what I really like about

04:21.920 --> 04:25.600
 cryptography. Cryptography, it seems like we live in a world where the defense wins

04:27.680 --> 04:33.200
 versus nuclear weapons. The opposite is true. It is much easier to build a warhead that splits

04:33.200 --> 04:37.920
 into 100 little warheads than to build something that can take out 100 little warheads.

04:38.720 --> 04:43.040
 The offense has the advantage there. So maybe our future is in crypto.

04:43.040 --> 04:51.440
 So cryptography, right. The Goliath is the defense. And then all the different hackers

04:51.440 --> 04:58.640
 are the Davids. And that equation is flipped for nuclear war. Because there's so many,

04:58.640 --> 05:01.920
 like one nuclear weapon destroys everything, essentially.

05:01.920 --> 05:07.680
 Yeah. And it is much easier to attack with a nuclear weapon than it is to like,

05:07.680 --> 05:12.080
 the technology required to intercept and destroy a rocket is much more complicated than the

05:12.080 --> 05:16.160
 technology required to just orbital trajectory, send a rocket to somebody.

05:17.280 --> 05:22.160
 So okay, your intuition that there were intelligent civilizations out there,

05:22.800 --> 05:27.520
 but it's very possible that they're no longer there. It's kind of a sad picture.

05:27.520 --> 05:31.280
 They enter some steady state. They all wirehead themselves.

05:31.280 --> 05:31.920
 What's wirehead?

05:33.840 --> 05:38.880
 Stimulate their pleasure centers and just live forever in this kind of stasis.

05:38.880 --> 05:43.600
 Oh. Well, I mean, I think the reason I believe

05:43.600 --> 05:49.600
 this is because where are they? If there's some reason they stopped expanding,

05:50.400 --> 05:53.200
 because otherwise they would have taken over the universe. The universe isn't that big.

05:53.200 --> 05:55.840
 Or at least, you know, let's just talk about the galaxy, right?

05:55.840 --> 06:00.080
 About 70,000 light years across. I took that number from Star Trek Voyager. I don't know how

06:00.080 --> 06:06.320
 true it is. But yeah, that's not big, right? 70,000 light years is nothing.

06:06.320 --> 06:11.440
 For some possible technology that you can imagine that can leverage like wormholes or

06:11.440 --> 06:14.880
 something like that. You don't even need wormholes. Just a von Neumann probe is enough.

06:14.880 --> 06:18.240
 A von Neumann probe and a million years of sub light travel,

06:18.240 --> 06:23.760
 and you'd have taken over the whole universe. That clearly didn't happen. So something stopped it.

06:23.760 --> 06:29.120
 So you mean for like a few million years, if you sent out probes that travel close,

06:29.680 --> 06:32.080
 what's sub light? You mean close to the speed of light?

06:32.080 --> 06:33.520
 Let's say 0.1c.

06:33.520 --> 06:36.880
 And it just spreads. Interesting. Actually, that's an interesting calculation.

06:38.640 --> 06:42.000
 So what makes you think that we'd be able to communicate with them?

06:45.280 --> 06:48.880
 Why do you think we would be able to comprehend

06:48.880 --> 06:54.720
 intelligent lies that are out there? Even if they were among this kind of thing,

06:55.440 --> 06:56.880
 or even just flying around?

06:56.880 --> 07:03.680
 Well, I mean, that's possible. It's possible that there is some sort of prime directive.

07:04.480 --> 07:08.480
 That'd be a really cool universe to live in. And there's some reason they're not making

07:08.480 --> 07:14.000
 themselves visible to us. But it makes sense that they would use the same,

07:15.040 --> 07:16.240
 well, at least the same entropy.

07:16.880 --> 07:20.560
 Well, you're implying the same laws of physics. I don't know what you mean by entropy in this case.

07:20.560 --> 07:23.760
 Oh, yeah. I mean, if entropy is the scarce resource in the universe.

07:23.760 --> 07:28.160
 So what do you think about like Steven Wolfram and everything is a computation?

07:28.720 --> 07:32.560
 And then what if they are traveling through this world of computation?

07:32.560 --> 07:35.840
 So if you think of the universe as just information processing,

07:36.480 --> 07:43.440
 then what you're referring to with entropy, and then these pockets of interesting complex

07:43.440 --> 07:46.720
 computation swimming around, how do we know they're not already here?

07:46.720 --> 07:54.240
 How do we know that all the different amazing things that are full of mystery

07:54.240 --> 08:00.560
 on Earth are just like little footprints of intelligence from light years away?

08:01.520 --> 08:06.880
 Maybe. I mean, I tend to think that as civilizations expand, they use more and more energy.

08:07.680 --> 08:11.680
 And you can never overcome the problem of waste heat. So where is their waste heat?

08:11.680 --> 08:18.000
 So we'd be able to, with our crude methods, be able to see like there's a whole lot of energy here.

08:18.800 --> 08:23.280
 But it could be something we're not, I mean, we don't understand dark energy, right? Dark matter.

08:23.280 --> 08:25.360
 It could be just stuff we don't understand at all.

08:25.920 --> 08:31.600
 Or they could have a fundamentally different physics, you know, like that we just don't even

08:31.600 --> 08:34.960
 comprehend. Well, I think, okay, I mean, it depends how far out you want to go.

08:34.960 --> 08:38.000
 I don't think physics is very different on the other side of the galaxy.

08:38.000 --> 08:44.720
 I would suspect that they have, I mean, if they're in our universe, they have the same physics.

08:45.520 --> 08:50.160
 Well, yeah, that's the assumption we have. But there could be like super trippy things like

08:51.760 --> 08:59.280
 like our cognition only gets to a slice and all the possible instruments that we can design

08:59.280 --> 09:03.920
 only get to a particular slice of the universe. And there's something much like weirder.

09:03.920 --> 09:10.960
 Maybe we can try a thought experiment. Would people from the past be able to detect

09:11.760 --> 09:17.200
 their remnants of our, be able to detect our modern civilization? I think the answer is

09:17.200 --> 09:20.480
 obviously yes. You mean past from a hundred years ago?

09:20.480 --> 09:23.200
 Well, let's even go back further. Let's go to a million years ago.

09:24.320 --> 09:27.520
 The humans who were lying around in the desert probably didn't even have,

09:27.520 --> 09:33.840
 maybe they just barely had fire. They would understand if a 747 flew overhead.

09:35.040 --> 09:44.880
 Oh, in this vicinity, but not if a 747 flew on Mars. Because they wouldn't be able to see far,

09:44.880 --> 09:48.720
 because we're not actually communicating that well with the rest of the universe.

09:48.720 --> 09:53.120
 We're doing okay, just sending out random like 50s tracks of music.

09:53.120 --> 09:59.520
 True. And yeah, I mean, they'd have to do, you know, the, we've only been broadcasting radio waves

09:59.520 --> 10:08.080
 for 150 years and well, there's your light cone. So yeah, okay. What do you make about all the,

10:08.800 --> 10:15.680
 I recently came across this, having talked to David Fravor. I don't know if you caught

10:15.680 --> 10:23.840
 what the videos that Pentagon released and the New York Times reporting of the UFO sightings. So I

10:23.840 --> 10:32.160
 kind of looked into it quote unquote. And there's actually been like hundreds of thousands of UFO

10:32.160 --> 10:38.000
 sightings, right? And a lot of it you can explain away in different kinds of ways. So one is it

10:38.000 --> 10:43.520
 could be interesting physical phenomena. Two, it could be people wanting to believe.

10:43.520 --> 10:46.880
 And therefore they conjure up a lot of different things that just, you know, when you see different

10:46.880 --> 10:53.200
 kinds of lights, some basic physics phenomena, and then you just conjure up ideas of possible

10:53.200 --> 11:00.880
 out there mysterious worlds. But, you know, it's also possible like you have a case of David Fravor,

11:00.880 --> 11:08.400
 who is a Navy pilot, who's, you know, as legit as it gets in terms of humans who are able to

11:08.400 --> 11:14.800
 perceive things in the environment and make conclusions, whether those things are a threat

11:14.800 --> 11:21.440
 or not. And he and several other pilots saw a thing, I don't know if you followed this,

11:21.440 --> 11:26.320
 but they saw a thing that they've since then called TikTok that moved in all kinds of weird

11:26.320 --> 11:34.720
 ways. They don't know what it is. It could be technology developed by the United States,

11:34.720 --> 11:40.800
 and they're just not aware of it and the surface level from the Navy, right? It could be different

11:40.800 --> 11:45.600
 kind of lighting technology or drone technology, all that kind of stuff. It could be the Russians

11:45.600 --> 11:52.960
 and the Chinese, all that kind of stuff. And of course their mind, our mind can also venture

11:52.960 --> 11:58.320
 into the possibility that it's from another world. Have you looked into this at all? What do you

11:58.320 --> 12:05.600
 think about it? I think all the news is a scythe. I think that the most plausible... Nothing is real.

12:06.320 --> 12:14.240
 Yeah, I listened to the, I think it was Bob Lazar on Joe Rogan. And like, I believe everything

12:14.240 --> 12:18.880
 this guy is saying. And then I think that it's probably just some like MK Ultra kind of thing,

12:18.880 --> 12:26.000
 you know? What do you mean? Like, they made some weird thing and they called it an alien spaceship.

12:26.000 --> 12:30.400
 You know, maybe it was just to like stimulate young physicists minds and tell them it's alien

12:30.400 --> 12:35.200
 technology and we'll see what they come up with, right? Do you find any conspiracy theories

12:35.200 --> 12:42.240
 compelling? Like, have you pulled at the string of the rich complex world of conspiracy theories

12:42.240 --> 12:47.280
 that's out there? I think that I've heard a conspiracy theory that conspiracy theories

12:47.280 --> 12:55.520
 were invented by the CIA in the 60s to discredit true things. Yeah. So, you know, you can go to

12:55.520 --> 13:04.480
 ridiculous conspiracy theories like Flat Earth and Pizza Gate and, you know, these things are

13:04.480 --> 13:09.360
 almost to hide like conspiracy theories that like, you know, remember when the Chinese like locked

13:09.360 --> 13:12.960
 up the doctors who discovered coronavirus? Like, I tell people this and I'm like, no, no, no,

13:12.960 --> 13:16.880
 that's not a conspiracy theory. That actually happened. Do you remember the time that the

13:16.880 --> 13:21.840
 money used to be backed by gold and now it's backed by nothing? This is not a conspiracy theory. This

13:21.840 --> 13:28.080
 actually happened. Well, that's one of my worries today with the idea of fake news is that

13:29.440 --> 13:38.560
 when nothing is real, then like, you dilute the possibility of anything being true by conjuring

13:38.560 --> 13:43.440
 up all kinds of conspiracy theories. And then you don't know what to believe. And then like,

13:43.440 --> 13:48.960
 the idea of truth of objectivity is lost completely. Everybody has their own truth.

13:48.960 --> 13:55.360
 So, you used to control information by censoring it. Then the internet happened and governments

13:55.360 --> 14:01.200
 were like, oh, shit, we can't censor things anymore. I know what we'll do. You know, it's

14:01.200 --> 14:06.880
 the old story of the story of like tying a flag with a leprechaun tells you as gold is buried

14:06.880 --> 14:10.400
 and you tie one flag and you make the leprechaun swear to not remove the flag and you come back

14:10.400 --> 14:15.920
 to the field later with a shovel and this flag is everywhere. That's one way to maintain privacy,

14:15.920 --> 14:22.880
 right? In order to protect the contents of this conversation, for example, we could just generate

14:23.680 --> 14:28.960
 like millions of deep fake conversations where you and I talk and say random things.

14:28.960 --> 14:33.760
 So, this is just one of them and nobody knows which one is the real one. This could be fake

14:33.760 --> 14:39.840
 right now. Classic steganography technique. Okay, another absurd question about intelligent life

14:39.840 --> 14:45.840
 because you're an incredible programmer outside of everything else we'll talk about just as a

14:45.840 --> 14:54.400
 programmer. Do you think intelligent beings out there, the civilizations that were out there

14:54.400 --> 15:02.320
 had computers and programming? Did they naturally have to develop something where we engineer

15:02.320 --> 15:10.800
 machines and are able to encode both knowledge into those machines and instructions that process

15:10.800 --> 15:16.560
 that knowledge, process that information to make decisions and actions and so on? And would those

15:16.560 --> 15:22.400
 programming languages, if you think they exist, be at all similar to anything we've developed?

15:24.080 --> 15:29.440
 So, I don't see that much of a difference between quote unquote natural languages and

15:29.440 --> 15:38.880
 programming languages. I think there are so many similarities. So, when asked the question

15:38.880 --> 15:46.320
 what do alien languages look like, I imagine they're not all that dissimilar from ours,

15:46.320 --> 15:52.720
 and I think translating in and out of them wouldn't be that crazy.

15:52.720 --> 16:00.480
 Well, it's difficult to compile DNA to Python and then to C. There is a little bit of a gap

16:00.480 --> 16:08.720
 in the kind of languages we use for touring machines and the kind of languages nature seems

16:08.720 --> 16:14.560
 to use a little bit. Maybe that's just we just haven't understood the kind of language that

16:14.560 --> 16:21.840
 nature uses as well yet. DNA is a CAD model. It's not quite a programming language. It has no sort

16:21.840 --> 16:31.280
 of serial execution. It's not quite a CAD model. So, I think in that sense we actually

16:31.280 --> 16:38.080
 completely understand it. The problem is simulating on these CAD models. I played with it a bit this

16:38.080 --> 16:42.800
 year is super computationally intensive. If you want to go down to like the molecular level

16:43.520 --> 16:49.360
 where you need to go to see a lot of these phenomenon like protein folding. So, yeah,

16:49.360 --> 16:54.240
 it's not that we don't understand it. It just requires a whole lot of compute to kind of compile

16:54.240 --> 17:00.560
 it. For human minds it's inefficient both for the data representation and for the programming.

17:00.560 --> 17:04.880
 It runs well on raw nature. It runs well on raw nature and when we try to build

17:04.880 --> 17:09.280
 emulators or simulators for that, well the mad sloth kind of tried it.

17:10.480 --> 17:18.160
 It runs in that, yeah, you've commented elsewhere. I don't remember where that one of the problems

17:18.160 --> 17:26.400
 is simulating nature is tough. And if you want to sort of deploy a prototype, I forgot how you

17:26.400 --> 17:32.400
 put it but it made me laugh, but animals or humans would need to be involved in order to

17:32.400 --> 17:40.240
 you know, to try to run some prototype code on like if we're talking about COVID and viruses and so

17:40.240 --> 17:45.680
 on. Yeah. If you were to try to engineer some kind of defense mechanisms like a vaccine

17:47.280 --> 17:52.480
 against COVID or all that kind of stuff that doing any kind of experimentation like you can

17:52.480 --> 17:58.640
 with like autonomous vehicles would be very technically cost, technically and ethically

17:58.640 --> 18:05.440
 costly. I'm not sure about that. I think you can do tons of crazy biology in test tubes. I think

18:05.440 --> 18:13.360
 my bigger complaint is more all the tools are so bad. Like literally you mean like like

18:13.360 --> 18:19.920
 libraries and I don't know. I'm not pipetting shit. Like your hand in me, I gotta, no, no, no,

18:19.920 --> 18:28.960
 there has to be some like automating stuff and like the human biology is messy. Like it seems like

18:29.680 --> 18:33.680
 look at those Taranos videos. They were joke. It's like a little gantry. It's like a little

18:33.680 --> 18:39.040
 xy gantry high school science project with the pipet. I'm like, really? Gotta be something better.

18:39.040 --> 18:44.480
 You can't build like nice microfluidics and I can program the, you know, computation to

18:44.480 --> 18:50.880
 bio interface. I mean, this is going to happen. But like right now, if you are asking me to pipet

18:50.880 --> 18:59.120
 50 milliliters of solution, I'm out. This is so crude. Yeah. Okay, let's get all the crazy out of

18:59.120 --> 19:04.000
 the way. So a bunch of people asked me, since we talked about the simulation last time,

19:04.880 --> 19:10.640
 we talked about hacking the simulation. Do you have any updates, any insights about how we might

19:10.640 --> 19:15.920
 be able to go about hacking simulation if we indeed do live in a simulation?

19:17.040 --> 19:22.800
 I think a lot of people misinterpreted the point of that South by talk. The point of the

19:22.800 --> 19:27.360
 South by talk was not literally to hack the simulation. I think that this

19:31.600 --> 19:35.440
 this is an idea is literally just I think theoretical physics. I think that's the whole,

19:35.440 --> 19:42.160
 you know, the whole goal, right? You want your grand unified theory, but then, okay,

19:42.160 --> 19:46.800
 build a grand unified theory search for exploits, right? I think we're nowhere near actually there

19:46.800 --> 19:53.120
 yet. My hope with that was just more to like, like, are you people kidding me with the things

19:53.120 --> 19:58.960
 you spend time thinking about? Do you understand like kind of how small you are? You are, you are

19:58.960 --> 20:05.520
 bytes and God's computer. Really? And the things that people get worked up about and, you know.

20:06.560 --> 20:13.520
 So basically, it was more a message of we should humble ourselves that we get to

20:15.040 --> 20:23.120
 like, what are we humans in this byte code? Yeah. And not just humble ourselves, but like,

20:23.120 --> 20:26.320
 I'm not trying to like make people guilty or anything like that. I'm trying to say like,

20:26.320 --> 20:31.200
 literally, look at what you are spending time on, right? What are you referring to? You're

20:31.200 --> 20:35.920
 referring to the Kardashians? What are we talking about? I'm referring to no, the Kardashians.

20:35.920 --> 20:43.760
 Everyone knows that's kind of fun. I'm referring more to like, economy, you know, this idea that

20:46.640 --> 20:54.720
 we got up our stock price. Or what is the goal function of humanity?

20:54.720 --> 20:59.360
 You don't like the game of capitalism? Like, you don't like the games we've constructed for

20:59.360 --> 21:04.640
 ourselves as humans? I'm a big fan of capitalism. I don't think that's really the game we're playing

21:04.640 --> 21:11.360
 right now. I think we're playing a different game where the rules are rigged. Okay, which games

21:11.360 --> 21:16.640
 are interesting to you that we humans have constructed and which aren't? Which are productive

21:16.640 --> 21:22.080
 and which are not? Actually, maybe that's the real point of the talk. It's like,

21:22.080 --> 21:27.920
 stop playing these fake human games. There's a real game here. We can play the real game.

21:28.480 --> 21:33.520
 The real game is, you know, nature wrote the rules. This is a real game. There still is

21:33.520 --> 21:37.520
 a game to play. But if you look at, sorry to interrupt, I don't know if you've seen the

21:37.520 --> 21:44.640
 Instagram account, nature is metal. The game that nature seems to be playing is a lot more cruel

21:45.280 --> 21:51.120
 than we humans want to put up with. Or at least we see it as cruel. It's like, the bigger thing

21:51.120 --> 21:59.600
 eats the smaller thing and does it to impress another big thing so it can mate with that thing.

22:00.320 --> 22:06.560
 And that's it. That seems to be the entirety of it. Well, there's no art. There's no music.

22:07.120 --> 22:12.160
 There's no comma AI. There's no comma one, no comma two, no George

22:13.280 --> 22:18.320
 Hott's with his brilliant talks at South by Southwest. See, I disagree though. I disagree

22:18.320 --> 22:26.720
 that this is what nature is. I think nature just provided basically an open world MMORPG.

22:26.720 --> 22:31.600
 And you know, here it's open world. I mean, if that's the game you want to play, you can play

22:31.600 --> 22:37.200
 that game. Isn't that beautiful? I know if you play Diablo, they used to have, I think, cow level

22:38.000 --> 22:46.160
 where it's, so everybody will go just, they figured out this, like the best way to gain

22:46.160 --> 22:54.480
 experience points. This is just slaughter cows over and over and over. And so they figured out this

22:54.480 --> 22:59.760
 little sub game within the bigger game that this is the most efficient way to get experience points.

22:59.760 --> 23:04.720
 And everybody somehow agreed they're getting experience points in RPG context where you always

23:04.720 --> 23:10.320
 want to be getting more stuff, more skills, more levels, keep advancing. That seems to be good.

23:10.320 --> 23:17.520
 So might as well spend sacrifice, actual enjoyment of playing a game, exploring a world,

23:17.520 --> 23:24.400
 and spending like hundreds of hours a year time in cow level. I mean, the number of hours I spent

23:24.400 --> 23:29.760
 in cow level, I'm not like the most impressive person because people have probably thousands

23:29.760 --> 23:35.360
 of hours there, but it's ridiculous. So that's a little absurd game that brought me joints on

23:35.360 --> 23:42.160
 weird dopamine drug kind of way. So you don't like those games. You don't think that's us humans

23:43.280 --> 23:51.280
 feeling the nature. And that was the point of the talk. So how do we hack it then?

23:51.280 --> 23:57.760
 Well, I want to live forever. And this is the goal. Well, that's a game against nature.

23:59.120 --> 24:02.320
 Yeah. Immortality is the good objective function to you.

24:02.320 --> 24:06.000
 I mean, start there and then you can do whatever else you want because you've got a long time.

24:07.200 --> 24:12.960
 What if immortality makes the game just totally not fun? I mean, why do you assume immortality

24:13.760 --> 24:20.960
 is somehow a good objective function? It's not immortality that I want. A true immortality

24:20.960 --> 24:26.160
 where I could not die, I would prefer what we have right now. But I want to choose my own death,

24:26.160 --> 24:32.720
 of course. I don't want nature to decide when I die, I'm going to win. I'm going to be you.

24:32.720 --> 24:40.080
 And then at some point, if you choose commit suicide, how long do you think you'd live?

24:41.440 --> 24:42.240
 Until I get bored.

24:42.960 --> 24:49.120
 See, I don't think people, like brilliant people like you that really ponder

24:49.120 --> 24:57.680
 they're living a long time, are really considering how meaningless life becomes.

24:58.400 --> 25:00.320
 Well, I want to know everything and then I'm ready to die.

25:03.360 --> 25:03.920
 As long as there's...

25:03.920 --> 25:08.800
 But why do you want, isn't it possible that you want to know everything because it's finite?

25:09.600 --> 25:14.320
 Like the reason you want to know quote unquote, everything is because you don't have enough time

25:14.320 --> 25:21.280
 to know everything. And once you have unlimited time, then you realize like, why do anything?

25:22.080 --> 25:23.680
 Like why learn anything?

25:24.800 --> 25:26.880
 I want to know everything and then I'm ready to die.

25:26.880 --> 25:27.760
 So you have... Yeah.

25:27.760 --> 25:33.680
 Well, it's not a... It's a terminal value. It's not in service of anything else.

25:34.560 --> 25:40.400
 I'm conscious of the possibility, this is not a certainty, but the possibility of that

25:40.400 --> 25:49.680
 engine of curiosity that you're speaking to is actually a symptom of the finiteness of life.

25:49.680 --> 25:56.800
 Like without that finiteness, your curiosity would vanish like a morning fog.

25:56.800 --> 25:57.520
 All right, cool.

25:57.520 --> 25:59.200
 Bukowski talked about love like that.

25:59.200 --> 26:03.600
 Then let me solve immortality. Let me change the thing in my brain that reminds me of the

26:03.600 --> 26:07.120
 fact that I'm immortal tells me that life is finite shit. Maybe I'll have it tell me that

26:07.120 --> 26:13.360
 life ends next week. All right? I'm okay with some self manipulation like that. I'm okay with

26:13.360 --> 26:17.040
 deceiving myself. Oh, oh, Rika. Changing the code.

26:17.040 --> 26:21.680
 If that's the problem, right? If the problem is that I will no longer have that curiosity,

26:21.680 --> 26:25.520
 I'd like to have backup copies of myself. Revert, yeah.

26:25.520 --> 26:29.120
 Well, which I check in with occasionally to make sure they're okay with the trajectory

26:29.120 --> 26:33.040
 and they can kind of override it. Maybe a nice like, I think of like those wave nets,

26:33.040 --> 26:35.120
 those like logarithmic go back to the copies.

26:35.120 --> 26:39.040
 But sometimes it's not reversible. Like I've done this with video games.

26:39.840 --> 26:43.840
 Once you figure out the cheat code or like you look up how to cheat old school,

26:43.840 --> 26:46.800
 like single player, it ruins the game for you.

26:46.800 --> 26:51.760
 Absolutely. I know that feeling. But again, that just means our brain manipulation

26:51.760 --> 26:54.560
 technology is not good enough yet. Remove that cheat code from your brain.

26:55.680 --> 27:02.720
 So it's also possible that if we figure out immortality that all of us will kill ourselves

27:02.720 --> 27:08.080
 before we advance far enough to be able to revert the change.

27:08.080 --> 27:10.560
 I'm not killing myself till I know everything. So...

27:11.680 --> 27:14.720
 That's what you say now because your life is finite.

27:15.760 --> 27:20.880
 You know, I think self modifying systems comes up with all these hairy complexities.

27:20.880 --> 27:23.120
 And can I promise that I'll do it perfectly? No.

27:23.120 --> 27:25.440
 But I think I can put good safety structures in place.

27:25.440 --> 27:36.720
 So that talk and your thinking here is not literally referring to a simulation in that our

27:38.000 --> 27:41.360
 universe is a kind of computer program running on a computer.

27:42.080 --> 27:43.920
 That's more of a thought experiment.

27:45.760 --> 27:48.960
 Do you also think of the potential of the sort of

27:48.960 --> 27:59.520
 Bostrom, Elon Musk, and others that talk about an actual program that simulates our universe?

27:59.520 --> 28:05.120
 Oh, I don't doubt that we're in a simulation. I just think that it's not quite that important.

28:05.120 --> 28:08.880
 I mean, I'm interested only in simulation theory as far as like it gives me power over nature.

28:09.600 --> 28:12.960
 If it's totally unfalsifiable, then who cares?

28:12.960 --> 28:15.120
 I mean, what do you think that experiment would look like?

28:15.120 --> 28:20.640
 Like somebody on Twitter asked George what signs we would look for

28:20.640 --> 28:24.720
 to know whether or not we're in the simulation, which is exactly what you're asking is like

28:26.080 --> 28:31.520
 the step that precedes the step of knowing how to get more power from this knowledge

28:32.080 --> 28:35.120
 is to get an indication that there's some power to be gained.

28:35.120 --> 28:42.000
 So get an indication that you can discover and exploit cracks in the simulation

28:42.000 --> 28:45.280
 or it doesn't have to be in the physics of the universe.

28:45.280 --> 28:50.320
 Yeah. Show me. I mean, like a memory leak would be cool.

28:51.600 --> 28:53.920
 Some scrying technology, you know?

28:53.920 --> 28:55.280
 What kind of technology?

28:55.280 --> 28:56.080
 Scrying.

28:56.080 --> 28:56.560
 What's that?

28:56.560 --> 29:04.320
 Oh, that's a weird. Scrying is the paranormal ability to like remote viewing,

29:04.320 --> 29:06.720
 like being able to see somewhere where you're not.

29:06.720 --> 29:11.120
 So, you know, I don't think you can do it by chanting in a room,

29:11.120 --> 29:14.400
 but if we could find, it's a memory leak, basically.

29:16.080 --> 29:17.360
 It's a memory leak. Yeah.

29:17.360 --> 29:19.840
 You're able to access parts you're not supposed to.

29:19.840 --> 29:20.480
 Yeah, yeah, yeah.

29:20.480 --> 29:22.000
 And thereby discover shortcut.

29:22.000 --> 29:24.560
 Yeah. Maybe a memory leak means the other thing as well,

29:24.560 --> 29:26.960
 but I mean like, yeah, like an ability to read arbitrary memory.

29:26.960 --> 29:27.440
 Yeah.

29:27.440 --> 29:29.520
 Right. And that one's not that horrifying.

29:29.520 --> 29:31.280
 All right. The right ones start to be horrifying.

29:31.280 --> 29:34.720
 Read it. Right. So the reading is not the problem.

29:34.720 --> 29:37.360
 Yeah. It's like Heartbleed for the universe.

29:37.360 --> 29:39.600
 Oh, boy, the writing is a big, big problem.

29:40.560 --> 29:41.680
 It's a big problem.

29:42.880 --> 29:44.480
 It's the moment you can write anything,

29:44.480 --> 29:46.000
 even if it's just random noise.

29:47.520 --> 29:48.480
 That's terrifying.

29:49.040 --> 29:51.360
 I mean, even without, even without that,

29:51.360 --> 29:54.320
 like even some of the, you know, the nanotech stuff that's coming, I think.

29:56.880 --> 29:59.120
 I don't know if you're paying attention, but actually,

29:59.120 --> 30:02.000
 Eric Weisstein came out with the theory of everything.

30:02.000 --> 30:03.360
 I mean, that came out.

30:03.360 --> 30:06.240
 He's been working on a theory of everything in the physics world

30:06.240 --> 30:07.920
 called geometric unity.

30:07.920 --> 30:10.400
 And then for me, from computer science person,

30:10.400 --> 30:14.000
 like you, Steven Wolfram's theory of everything,

30:14.000 --> 30:16.880
 of like, hypergraphs is super interesting and beautiful.

30:17.440 --> 30:19.200
 But not from a physics perspective,

30:19.200 --> 30:20.720
 but from a computational perspective.

30:20.720 --> 30:23.040
 I don't know. Have you paid attention to any of that?

30:23.040 --> 30:26.240
 So again, like what would make me pay attention

30:26.240 --> 30:28.560
 and like why like a hate string theory is,

30:29.360 --> 30:31.120
 okay, make a testable prediction.

30:31.120 --> 30:33.520
 Right. I'm only interested in,

30:33.520 --> 30:35.840
 I'm not interested in theories for their intrinsic beauty.

30:35.840 --> 30:38.160
 I'm interested in theories that give me power over the universe.

30:39.760 --> 30:42.080
 So if these theories do, I'm very interested.

30:42.960 --> 30:44.960
 Can I just say how beautiful that is?

30:44.960 --> 30:46.960
 Because a lot of physicists say,

30:46.960 --> 30:49.840
 I'm interested in experimental validation,

30:49.840 --> 30:52.720
 and they skip out the part where they say,

30:52.720 --> 30:55.360
 to give me more power in the universe.

30:55.360 --> 30:56.320
 I just love the,

30:57.280 --> 30:59.600
 yo, I want, I want, I want the clarity of that.

30:59.600 --> 31:01.840
 I want a hundred gigahertz processors.

31:01.840 --> 31:04.000
 I want transistors that are smaller than atoms.

31:04.000 --> 31:05.200
 I want like power.

31:07.920 --> 31:10.400
 That's, that's true.

31:10.400 --> 31:13.360
 And that's where people from aliens to this kind of technology

31:13.360 --> 31:16.720
 where people are worried that governments,

31:16.720 --> 31:18.160
 like who owns that power?

31:19.200 --> 31:20.640
 Is it George Haas?

31:20.640 --> 31:24.880
 Is it thousands of distributed hackers across the world?

31:24.880 --> 31:25.760
 Is it governments?

31:26.560 --> 31:28.560
 You know, is it Mark Zuckerberg?

31:28.560 --> 31:31.280
 There's a lot of people that,

31:32.320 --> 31:35.440
 I don't know if anyone trusts anyone individual with power,

31:35.440 --> 31:36.480
 so they're always worried.

31:37.120 --> 31:38.400
 It's the beauty of blockchains.

31:39.200 --> 31:42.320
 That's the beauty of blockchains, which we'll talk about.

31:43.040 --> 31:45.280
 On Twitter, somebody pointed me to a story,

31:46.080 --> 31:49.200
 a bunch of people pointed me to a story a few months ago

31:49.200 --> 31:51.440
 where you went into a restaurant in New York,

31:51.440 --> 31:53.040
 and you can correct me if any of this is wrong,

31:53.040 --> 31:57.840
 and ran into a bunch of folks from a company, a crypto company,

31:58.640 --> 32:00.240
 who are trying to scale up Ethereum,

32:01.680 --> 32:03.200
 and they had a technical deadline

32:03.200 --> 32:06.480
 related to a solidity to OVM compiler.

32:07.200 --> 32:09.680
 So these are all Ethereum technologies.

32:09.680 --> 32:13.120
 So you stepped in, they recognized you,

32:14.560 --> 32:16.080
 pulled you aside, explained their problem,

32:16.080 --> 32:18.320
 and you stepped in and helped them solve the problem,

32:18.320 --> 32:22.640
 thereby creating legend status story.

32:24.640 --> 32:28.800
 Can you tell me the story a little more detail?

32:28.800 --> 32:30.320
 It seems kind of incredible.

32:31.360 --> 32:32.160
 Did this happen?

32:32.160 --> 32:33.200
 Yeah, yeah, it's a true story.

32:33.200 --> 32:33.920
 It's a true story.

32:33.920 --> 32:36.320
 I mean, they wrote a very flattering account of it.

32:39.280 --> 32:43.600
 So optimism is the spin, the company's called Optimism.

32:43.600 --> 32:45.120
 It's a spin off of Plasma.

32:45.120 --> 32:47.680
 They're trying to build L2 solutions on Ethereum.

32:47.680 --> 32:52.480
 So right now, every Ethereum node

32:52.480 --> 32:55.600
 has to run every transaction on the Ethereum network.

32:56.560 --> 32:58.400
 And this kind of doesn't scale, right?

32:58.400 --> 32:59.920
 Because if you have n computers,

32:59.920 --> 33:02.160
 well, if that becomes two n computers,

33:02.160 --> 33:04.080
 you actually still get the same amount of compute.

33:05.200 --> 33:10.000
 This is like O of 1 scaling, because they all have to run it.

33:10.000 --> 33:12.720
 Okay, fine, you get more blockchain security,

33:12.720 --> 33:15.120
 but the blockchain's already so secure.

33:15.120 --> 33:17.840
 Can we trade some of that off for speed?

33:17.840 --> 33:19.280
 That's kind of what these L2 solutions are.

33:20.400 --> 33:26.160
 They built this thing, which kind of sandbox for Ethereum contracts,

33:26.160 --> 33:28.080
 so they can run it in this L2 world,

33:28.080 --> 33:30.800
 and it can't do certain things in L1.

33:30.800 --> 33:32.240
 Can I ask you for some definitions?

33:32.240 --> 33:33.200
 What's L2?

33:33.200 --> 33:34.720
 Oh, L2 is Layer 2.

33:34.720 --> 33:37.040
 So L1 is like the base Ethereum chain,

33:37.040 --> 33:44.240
 and then Layer 2 is like a computational layer that runs elsewhere,

33:44.240 --> 33:46.800
 but still is kind of secured by Layer 1.

33:47.600 --> 33:49.600
 And I'm sure a lot of people know,

33:49.600 --> 33:51.840
 but Ethereum is a cryptocurrency,

33:51.840 --> 33:54.560
 probably one of the most popular cryptocurrency, second to Bitcoin.

33:55.200 --> 33:58.640
 And a lot of interesting technological innovations there.

33:58.640 --> 34:03.120
 Maybe you could also slip in whenever you talk about this,

34:03.120 --> 34:06.240
 any things that are exciting to you in the Ethereum space.

34:06.240 --> 34:07.680
 And why Ethereum?

34:07.680 --> 34:10.800
 Well, I mean, Bitcoin is not turned complete.

34:10.800 --> 34:15.040
 Ethereum is not technically turned complete with a gas limit, but close enough.

34:15.840 --> 34:16.720
 With a gas limit?

34:16.720 --> 34:17.840
 What's the gas limit?

34:17.840 --> 34:18.960
 Resources?

34:18.960 --> 34:20.800
 Yeah, I mean, no computer is actually turned complete.

34:22.960 --> 34:23.840
 They're just fine at RAM.

34:23.840 --> 34:25.680
 You know, guys, I can actually solve the whole problem.

34:25.680 --> 34:26.640
 What's the word gas limit?

34:26.640 --> 34:28.480
 You just have so many brilliant words.

34:28.480 --> 34:29.200
 I'm not even going to ask.

34:29.200 --> 34:30.800
 Well, that's not my word.

34:30.800 --> 34:31.680
 That's Ethereum's word.

34:31.680 --> 34:32.480
 Gas limit.

34:32.480 --> 34:35.120
 Ethereum, you have to spend gas per instruction.

34:35.120 --> 34:37.680
 So like different op codes, you use different amounts of gas,

34:37.680 --> 34:41.920
 and you buy gas with Ether to prevent people from basically DDoSing the network.

34:42.640 --> 34:45.360
 So Bitcoin is proof of work.

34:45.360 --> 34:46.960
 And then what's Ethereum?

34:46.960 --> 34:48.160
 It's also proof of work.

34:48.160 --> 34:50.880
 They're working on some proof of stake Ethereum 2.0 stuff.

34:50.880 --> 34:52.560
 But right now it's proof of work.

34:52.560 --> 34:54.560
 It uses a different hash function from Bitcoin.

34:54.560 --> 34:56.560
 That's more ASIC resistance because you need RAM.

34:57.200 --> 34:59.200
 So we're all talking about Ethereum 1.0.

34:59.840 --> 35:03.760
 So what were they trying to do to scale this whole process?

35:03.760 --> 35:06.880
 So they were like, well, if we could run contracts elsewhere,

35:06.880 --> 35:10.240
 and then only save the results of that computation,

35:12.960 --> 35:14.560
 well, we don't actually have to do the compute on the chain.

35:14.560 --> 35:17.280
 We can do the compute off chain and just post what the results are.

35:17.280 --> 35:20.560
 Now, the problem with that is, well, somebody could lie about what the results are.

35:21.120 --> 35:23.200
 So you need a resolution mechanism.

35:23.200 --> 35:28.880
 And the resolution mechanism can be really expensive because you just have to make sure

35:28.880 --> 35:33.680
 that the person who is saying, look, I swear that this is the real computation.

35:33.680 --> 35:36.000
 I'm staking $10,000 on that fact.

35:36.000 --> 35:39.120
 And if you prove it wrong, yeah,

35:39.120 --> 35:42.640
 it might cost you $3,000 in gas fees to prove wrong,

35:42.640 --> 35:44.560
 but you'll get the $10,000 bounty.

35:44.560 --> 35:46.880
 So you can secure using those kind of systems.

35:48.720 --> 35:51.840
 So it's effectively a sandbox which runs contracts.

35:52.800 --> 35:55.440
 And like just like any kind of normal sandbox,

35:55.440 --> 36:00.320
 you have to replace syscalls with calls into the hypervisor.

36:00.320 --> 36:05.920
 Uh, sandbox, syscalls, hypervisor, what do these things mean?

36:07.520 --> 36:09.040
 As long as it's interesting to talk about.

36:09.040 --> 36:12.640
 Yeah, I mean, you can take like the Chrome sandbox is maybe the one to think about, right?

36:12.640 --> 36:14.960
 So the Chrome process is doing a rendering.

36:15.920 --> 36:18.160
 Can't, for example, read a file from the file system.

36:18.160 --> 36:18.640
 Yeah.

36:18.640 --> 36:21.600
 It has, if it tries to make an open syscall in Linux,

36:21.600 --> 36:23.600
 the open syscall, you can't make an open syscall.

36:23.600 --> 36:24.640
 No, no, no.

36:24.640 --> 36:29.040
 You have to request from the kind of a hypervisor process

36:29.040 --> 36:31.600
 or like, I don't know what it's called in Chrome.

36:31.600 --> 36:36.240
 But the, hey, could you open this file for me?

36:36.240 --> 36:38.320
 And then it does all these checks and then it passes the file,

36:38.320 --> 36:39.920
 handle back in if it's approved.

36:39.920 --> 36:40.400
 Got it.

36:41.040 --> 36:41.600
 So that's, yeah.

36:42.400 --> 36:44.640
 So what's the, in the context of Ethereum,

36:45.200 --> 36:48.320
 what are the boundaries of the sandbox that we're talking about?

36:48.320 --> 36:53.840
 Well, like one of the calls that you actually reading and writing any state

36:53.840 --> 36:56.720
 to the Ethereum contract, to the Ethereum blockchain.

36:56.720 --> 37:03.200
 Writing state is one of those calls that you're going to have to sandbox

37:03.200 --> 37:09.120
 in layer two, because if you let layer two just arbitrarily write to the Ethereum blockchain.

37:10.320 --> 37:15.040
 So layer two is really sitting on top of layer one.

37:15.040 --> 37:18.160
 So you're going to have a lot of different kinds of ideas that you can play with.

37:18.160 --> 37:18.560
 Yeah.

37:18.560 --> 37:24.480
 And they're all, they're not fundamentally changing the source code level of Ethereum.

37:24.480 --> 37:30.240
 Well, you have to replace a bunch of calls with calls into the hypervisor.

37:30.240 --> 37:35.680
 So instead of doing the syscall directly, you, you replace it with a call to the hypervisor.

37:36.640 --> 37:42.800
 So originally they were doing this by first running the, so solidity is the language

37:42.800 --> 37:46.080
 that most Ethereum contracts are written in, it compiles to a bytecode.

37:46.880 --> 37:49.360
 And then they wrote this thing they called the transpiler.

37:49.360 --> 37:53.600
 And the transpiler took the bytecode and it transpiled it into

37:53.600 --> 37:58.560
 OVM safe bytecode, basically bytecode that didn't make any of those restricted syscalls

37:58.560 --> 38:00.160
 and added the calls to the hypervisor.

38:01.520 --> 38:04.160
 This transpiler was a 3000 line mess.

38:05.440 --> 38:06.880
 And it's hard to do.

38:06.880 --> 38:10.480
 It's hard to do if you're trying to do it like that, because you have to kind of like deconstruct

38:10.480 --> 38:14.160
 the bytecode, change things about it, and then reconstruct it.

38:15.200 --> 38:18.880
 And I mean, as soon as I hear this, I'm like, well, why don't you just change the compiler?

38:19.760 --> 38:20.240
 All right.

38:20.240 --> 38:24.480
 Why not the first place you build the bytecode, just do it in the compiler?

38:25.440 --> 38:27.760
 So, yeah, you know, I asked them how much they wanted it.

38:28.960 --> 38:31.840
 Of course, measured in dollars and I'm like, well, okay.

38:32.960 --> 38:34.480
 And yeah.

38:34.480 --> 38:35.840
 You wrote the compiler.

38:35.840 --> 38:36.800
 Yeah, I modified.

38:36.800 --> 38:39.040
 I wrote a 300 line diff to the compiler.

38:39.040 --> 38:39.600
 It's on source.

38:39.600 --> 38:40.800
 You can look at it.

38:40.800 --> 38:42.880
 Yeah, I looked at the code last night.

38:42.880 --> 38:45.440
 Yeah, it's cute.

38:45.440 --> 38:46.000
 Yeah, exactly.

38:46.000 --> 38:46.560
 Oh, too massive.

38:46.560 --> 38:48.400
 Cute is a good word for it.

38:48.400 --> 38:51.680
 And it's C++.

38:51.680 --> 38:52.640
 C++, yeah.

38:54.240 --> 39:01.520
 So, when asked how you were able to do it, you said you just got to think and then do it right.

39:02.960 --> 39:04.560
 So, can you break that apart a little bit?

39:04.560 --> 39:08.800
 What's your process of one, thinking, and two, doing it right?

39:09.760 --> 39:13.360
 You know, the people who I was working for were amused that I said that.

39:13.360 --> 39:14.720
 It doesn't really mean anything.

39:14.720 --> 39:14.960
 Okay.

39:14.960 --> 39:23.440
 Yeah, I mean, is there some deep profound insights to draw from like how you problem solve from that?

39:23.440 --> 39:24.480
 This is always what I say.

39:24.480 --> 39:25.920
 I'm like, do you want to be a good programmer?

39:25.920 --> 39:26.880
 Do it for 20 years?

39:27.760 --> 39:29.520
 Yeah, there's no shortcuts.

39:29.520 --> 39:31.520
 Yeah.

39:31.520 --> 39:33.360
 What are your thoughts on crypto in general?

39:33.360 --> 39:39.840
 So, what parts technically or philosophically define especially beautiful maybe?

39:39.840 --> 39:42.720
 Oh, I'm extremely bullish on crypto long term.

39:42.720 --> 39:49.280
 Not any specific crypto project, but this idea of, well, two ideas.

39:50.320 --> 39:58.480
 One, the Nakamoto consensus algorithm is I think one of the greatest innovations of the 21st century.

39:58.480 --> 40:01.120
 This idea that people can reach consensus.

40:01.120 --> 40:07.440
 You can reach a group consensus using a relatively straightforward algorithm is wild.

40:07.440 --> 40:15.440
 And like Satoshi Nakamoto, people always ask me who I look up to.

40:15.440 --> 40:17.040
 It's like whoever that is.

40:17.600 --> 40:18.320
 Who do you think it is?

40:19.280 --> 40:19.840
 Elon Musk?

40:20.880 --> 40:21.280
 Is it you?

40:22.160 --> 40:25.280
 It is definitely not me and I do not think it's Elon Musk.

40:26.080 --> 40:34.240
 But yeah, this idea of groups reaching consensus in a decentralized yet formulaic way

40:34.240 --> 40:37.440
 is one extremely powerful idea from crypto.

40:40.400 --> 40:44.800
 Maybe the second idea is this idea of smart contracts.

40:45.680 --> 40:49.600
 When you write a contract between two parties, any contract,

40:50.800 --> 40:55.040
 this contract, if there are disputes, it's interpreted by lawyers.

40:56.080 --> 40:59.840
 Lawyers are just really shitty overpaid interpreters.

40:59.840 --> 41:04.080
 Imagine you had, let's talk about them in terms of like, let's compare a lawyer to Python.

41:04.080 --> 41:04.400
 Right?

41:06.240 --> 41:06.880
 Well, okay.

41:06.880 --> 41:07.920
 That's brilliant.

41:07.920 --> 41:09.840
 Oh, I never thought of it that way.

41:09.840 --> 41:10.800
 It's hilarious.

41:11.520 --> 41:15.760
 So Python, I'm paying even 10 cents an hour.

41:15.760 --> 41:17.040
 I'll use the nice Azure machine.

41:17.040 --> 41:18.640
 I can run Python for 10 cents an hour.

41:19.520 --> 41:21.280
 Lawyers cost $1,000 an hour.

41:21.280 --> 41:25.520
 So Python is 10,000x better on that axis.

41:26.720 --> 41:29.200
 Lawyers don't always return the same answer.

41:29.200 --> 41:32.640
 Python almost always does.

41:36.560 --> 41:36.960
 Cost.

41:37.680 --> 41:37.920
 Yeah.

41:37.920 --> 41:42.960
 I mean, just cost, reliability, everything about Python is so much better than lawyers.

41:43.680 --> 41:48.640
 So if you can make smart contracts, this whole concept of code is law.

41:50.240 --> 41:54.640
 I love and I would love to live in a world where everybody accepted that fact.

41:54.640 --> 42:00.160
 So maybe you can talk about what smart contracts are.

42:01.120 --> 42:10.800
 So let's say we have even something as simple as a safety deposit box.

42:11.520 --> 42:14.560
 A safety deposit box that holds a million dollars.

42:14.560 --> 42:18.560
 I have a contract with the bank that says two out of these three parties

42:18.560 --> 42:24.400
 must be present to open the safety deposit box and get the money out.

42:25.040 --> 42:28.160
 So that's a contract with the bank and it's only as good as the bank and the lawyers.

42:28.880 --> 42:34.880
 Let's say somebody dies and now we're going to go through a big legal dispute about whether,

42:34.880 --> 42:35.920
 oh, was it in the will?

42:35.920 --> 42:36.800
 Was it not in the will?

42:38.080 --> 42:43.520
 It's just so messy and the cost to determine truth is so expensive.

42:44.400 --> 42:48.080
 Versus a smart contract, which just uses cryptography to check if two out of three

42:48.080 --> 42:48.880
 keys are present.

42:49.920 --> 42:55.680
 Well, I can look at that and I can have certainty in the answer that it's going to return.

42:55.680 --> 42:58.000
 And that's what all businesses want is certainty.

42:58.000 --> 43:00.640
 You know, they say businesses don't care via YouTube.

43:00.640 --> 43:04.000
 YouTube's like, look, we don't care which way this lawsuit goes.

43:04.000 --> 43:06.400
 Just please tell us so we can have certainty.

43:06.960 --> 43:11.680
 And I wonder how many agreements in this, because we're talking about financial transactions

43:11.680 --> 43:13.680
 only in this case, correct?

43:13.680 --> 43:15.440
 The smart contracts.

43:15.440 --> 43:17.120
 Oh, you can go to anything.

43:17.120 --> 43:19.280
 You can put a prenup in the theorem blockchain.

43:21.760 --> 43:22.880
 Married smart contract.

43:22.880 --> 43:24.080
 Sorry, divorce lawyer.

43:24.080 --> 43:26.560
 Sorry, you're going to be replaced by Python.

43:29.440 --> 43:34.800
 Okay, so that's another beautiful idea.

43:34.800 --> 43:40.160
 Do you think there's something that's appealing to you about any one specific implementation?

43:40.160 --> 43:47.360
 So if you look 10, 20, 50 years down the line, do you see any like Bitcoin, Ethereum,

43:48.000 --> 43:50.960
 any of the other hundreds of cryptocurrencies winning out?

43:50.960 --> 43:53.120
 Is there like, what's your intuition about the space?

43:53.120 --> 43:57.200
 Are you just sitting back and watching the chaos and look who cares what emerges?

43:57.200 --> 43:58.000
 Oh, I don't.

43:58.000 --> 43:58.800
 I don't speculate.

43:58.800 --> 43:59.760
 I don't really care.

43:59.760 --> 44:02.000
 I don't really care which one of these projects wins.

44:02.560 --> 44:04.720
 I'm kind of in the Bitcoin as a meme coin camp.

44:05.280 --> 44:06.880
 I mean, why does Bitcoin have value?

44:06.880 --> 44:14.160
 It's technically kind of not great, like the block size debate.

44:14.160 --> 44:17.760
 When I found out what the block size debate was, I'm like, are you guys kidding?

44:17.760 --> 44:18.960
 What's the block size debate?

44:21.280 --> 44:21.760
 You know what?

44:21.760 --> 44:23.120
 It's really, it's too stupid to even talk.

44:24.800 --> 44:26.560
 People can look it up, but I'm like, wow.

44:27.360 --> 44:30.160
 Ethereum seems, the governance of Ethereum seems much better.

44:31.120 --> 44:33.840
 I've come around a bit on proof of stake ideas.

44:33.840 --> 44:37.680
 Because very smart people thinking about some things.

44:37.680 --> 44:40.320
 Yeah, governance is interesting.

44:40.320 --> 44:46.000
 It does feel like Vitalik, it does feel like it opens,

44:46.000 --> 44:49.760
 even in these distributed systems, leaders are helpful

44:51.120 --> 44:55.120
 because they kind of help you drive the mission and the vision

44:55.680 --> 44:57.440
 and they put a face to a project.

44:58.240 --> 45:00.000
 It's a weird thing about us humans.

45:00.000 --> 45:02.000
 Geniuses are helpful, like Vitalik.

45:02.000 --> 45:02.720
 Right.

45:02.720 --> 45:04.720
 Yeah, brilliant.

45:06.480 --> 45:07.680
 Leaders, I mean.

45:07.680 --> 45:09.120
 Are not necessarily, yeah.

45:10.160 --> 45:16.400
 So you think the reason he's the face of Ethereum is because he's a genius.

45:16.960 --> 45:17.760
 That's interesting.

45:21.360 --> 45:24.560
 It's interesting to think about that we need to create systems

45:24.560 --> 45:31.760
 in which the quote unquote leaders that emerge are the geniuses in the system.

45:33.040 --> 45:36.880
 I mean, that's arguably why the current state of democracy is broken

45:36.880 --> 45:40.880
 is the people who are emerging as the leaders are not the most competent,

45:40.880 --> 45:43.200
 are not the superstars of the system.

45:43.200 --> 45:46.000
 And it seems like at least for now in the crypto world,

45:46.000 --> 45:48.560
 oftentimes the leaders are the superstars.

45:49.280 --> 45:52.800
 Imagine at the debate, they asked, what's the Sixth Amendment?

45:52.800 --> 45:55.440
 What are the four fundamental forces in the universe?

45:55.440 --> 45:56.080
 Right.

45:56.080 --> 45:57.600
 What's the integral of two to the X?

45:58.720 --> 46:01.520
 Yeah, I'd love to see those questions asked.

46:01.520 --> 46:02.720
 And that's what I want as our leader.

46:03.760 --> 46:04.800
 What's Bayes rule?

46:07.040 --> 46:10.560
 Yeah, I mean, even, oh, wow, you're hurting my brain.

46:13.360 --> 46:16.400
 My standard was even lower, but I would have loved to see

46:17.520 --> 46:20.400
 just this basic brilliance.

46:20.400 --> 46:22.000
 Like I've talked to historians.

46:22.000 --> 46:25.200
 There's just these, they're not even like, they don't have a PhD

46:25.200 --> 46:26.560
 or even education history.

46:26.560 --> 46:32.640
 They just like a Dan Carlin type character who just like, holy shit,

46:32.640 --> 46:35.200
 how did all this information get into your head?

46:35.200 --> 46:41.120
 They're able to just connect Genghis Khan to the entirety of the history of the 20th century.

46:41.760 --> 46:46.000
 They know everything about every single battle that happened.

46:46.000 --> 46:54.960
 And they know the game of thrones of the different power plays and all that happened there.

46:54.960 --> 46:58.400
 And they know the individuals and all the documents involved.

46:58.400 --> 47:01.920
 And they integrate that into their regular life.

47:01.920 --> 47:03.840
 It's not like they're ultra history nerds.

47:03.840 --> 47:06.240
 They're just, they know this information.

47:06.240 --> 47:07.840
 That's what competence looks like.

47:07.840 --> 47:08.160
 Yeah.

47:08.880 --> 47:10.480
 Because I've seen that with programmers too, right?

47:10.480 --> 47:11.920
 That's what great programmers do.

47:11.920 --> 47:16.560
 But yeah, it would be, it's really unfortunate that those kinds of people

47:16.560 --> 47:18.640
 aren't emerging as our leaders.

47:19.200 --> 47:23.120
 But for now, at least in the crypto world, that seems to be the case.

47:23.120 --> 47:28.640
 I don't know if that always, you could imagine that in 100 years, it's not the case, right?

47:28.640 --> 47:31.760
 Crypto world has one very powerful idea going for it.

47:31.760 --> 47:34.160
 And that's the idea of forks, right?

47:34.160 --> 47:46.480
 I mean, imagine, we'll use a less controversial example, this was actually in my joke app in

47:46.480 --> 47:50.880
 2012, I was like, Barack Obama, Mitt Romney, let's let them both be president, right?

47:50.880 --> 47:54.320
 Like imagine we could fork America and just let them both be president.

47:54.320 --> 47:58.000
 And then the Americas could compete and people could invest in one,

47:58.000 --> 48:00.320
 pull their liquidity out of one, put it in the other.

48:00.320 --> 48:02.240
 You have this in the crypto world.

48:02.240 --> 48:04.560
 Ethereum forks into Ethereum and Ethereum Classic.

48:05.360 --> 48:08.160
 And you can pull your liquidity out of one and put it in another.

48:08.800 --> 48:16.240
 And people vote with their dollars, which forks, companies should be able to fork.

48:16.240 --> 48:17.440
 I'd love to fork Nvidia.

48:20.320 --> 48:25.440
 Yeah, like different business strategies and then try them out and see what works.

48:25.440 --> 48:36.480
 Like even take, yeah, take common AI that closes its source and then take one that's open source

48:36.480 --> 48:43.440
 and see what works, take one that's purchased by GM and one that remains Android Renegade

48:43.440 --> 48:45.200
 in all these different versions and see.

48:45.200 --> 48:47.840
 The beauty of common AI is someone can actually do that.

48:47.840 --> 48:49.360
 Please take common AI and fork it.

48:50.480 --> 48:51.040
 That's right.

48:51.040 --> 48:52.160
 That's the beauty of open source.

48:52.160 --> 48:59.520
 So you're, I mean, we'll talk about autonomous vehicle space, but it does seem that you're

49:01.120 --> 49:03.840
 really knowledgeable about a lot of different topics.

49:03.840 --> 49:06.400
 So the natural question a bunch of people ask this, which is,

49:07.680 --> 49:09.280
 how do you keep learning new things?

49:09.840 --> 49:11.600
 Do you have like practical advice?

49:12.320 --> 49:16.800
 If you were to introspect, like taking notes, allocate time,

49:17.600 --> 49:21.200
 or do you just mess around and just allow your curiosity to drive?

49:21.200 --> 49:24.720
 I'll write these people a self help book and I'll charge $67 for it.

49:26.240 --> 49:30.240
 And I will write on the cover of the self help book.

49:30.240 --> 49:32.480
 All of this advice is completely meaningless.

49:32.480 --> 49:34.640
 You're going to be a sucker and buy this book anyway.

49:34.640 --> 49:37.840
 And the one lesson that I hope they take away from the book

49:38.720 --> 49:41.520
 is that I can't give you a meaningful answer to that.

49:42.400 --> 49:43.200
 That's interesting.

49:43.200 --> 49:52.560
 And let me translate that is you haven't really thought about what it is you do systematically

49:52.560 --> 49:53.760
 because you could reduce it.

49:53.760 --> 49:58.480
 And there's some people, I mean, I've met brilliant people that this is really

49:58.480 --> 49:59.520
 clear with athletes.

50:00.080 --> 50:04.080
 Some are just, you know, the best in the world at something.

50:04.960 --> 50:08.800
 And they, they have zero interest in writing a, like a self help book,

50:08.800 --> 50:11.440
 but or how to master this game.

50:11.440 --> 50:15.440
 And then there's some athletes who become great coaches

50:15.440 --> 50:18.640
 and they love the analysis, perhaps the overall analysis.

50:18.640 --> 50:21.600
 And you right now, at least at your age, which is an interesting,

50:21.600 --> 50:23.040
 you're in the middle of the battle.

50:23.040 --> 50:26.240
 You're like the warriors that have zero interest in writing books.

50:27.600 --> 50:28.960
 So you're in the middle of the battle.

50:28.960 --> 50:30.400
 So you have, yeah.

50:30.400 --> 50:31.600
 This is, this is a fair point.

50:31.600 --> 50:39.520
 I do think I have a certain aversion to this kind of deliberate, intentional way of living life.

50:39.520 --> 50:45.040
 You eventually, the hilarity of this, especially since this is recorded,

50:47.040 --> 50:50.560
 it will reveal beautifully the absurdity when you finally do publish this book.

50:51.120 --> 50:52.480
 That guarantee you will.

50:53.440 --> 50:58.640
 The story of comma AI, maybe it'll be a biography written about you.

50:58.640 --> 51:00.240
 That'll be, that'll be better, I guess.

51:00.240 --> 51:03.360
 And you might be able to learn some cute lessons if you're starting a company

51:03.360 --> 51:05.200
 like comma AI from that book.

51:05.200 --> 51:10.000
 But if you're asking generic questions like, how do I be good at things?

51:10.000 --> 51:10.800
 Dude, I don't know.

51:11.600 --> 51:13.040
 Well, learn, I mean, the interesting,

51:13.040 --> 51:14.080
 Do them a lot.

51:14.080 --> 51:14.720
 Do them a lot.

51:14.720 --> 51:21.680
 But the interesting thing here is learning things outside of your current trajectory,

51:21.680 --> 51:23.840
 which is what it feels like from an outsider's perspective.

51:27.840 --> 51:33.040
 I don't know if there's a device on that, but it is an interesting curiosity.

51:33.040 --> 51:35.760
 When you become really busy, you're running a company.

51:37.680 --> 51:38.240
 Hard time.

51:40.160 --> 51:40.480
 Yeah.

51:41.360 --> 51:48.240
 But like, there's a natural inclination and trend, like just the momentum of life

51:48.240 --> 51:50.880
 carries you into a particular direction of wanting to focus.

51:50.880 --> 51:57.920
 And this kind of dispersion that curiosity can lead to gets harder and harder with time.

51:57.920 --> 52:03.600
 Because you get really good at certain things and it sucks trying things that you're not good at,

52:03.600 --> 52:05.040
 like trying to figure them out.

52:05.040 --> 52:09.840
 When you do this with your live streams, you're on the fly figuring stuff out.

52:09.840 --> 52:11.280
 You don't mind looking dumb.

52:13.840 --> 52:15.760
 You just figured out, figured out pretty quickly.

52:16.480 --> 52:18.320
 Sometimes I try things and I don't figure them out.

52:18.320 --> 52:18.960
 You fail.

52:18.960 --> 52:23.120
 My chest rating is like a 1400 despite putting like a couple of hundred hours in.

52:23.120 --> 52:24.080
 It's pathetic.

52:24.080 --> 52:26.640
 I mean, to be fair, I know that I could do it better.

52:26.640 --> 52:29.680
 If I did it better, like, don't play, you know, don't play five minute games,

52:29.680 --> 52:31.200
 play 15 minute games at least.

52:31.200 --> 52:33.520
 Like, I know these things, but it just doesn't.

52:34.080 --> 52:36.000
 It doesn't stick nicely in my knowledge stream.

52:37.040 --> 52:37.360
 All right.

52:37.360 --> 52:38.560
 Let's talk about Kama AI.

52:39.200 --> 52:41.120
 What's the mission of the company?

52:41.920 --> 52:43.680
 Let's like look at the biggest picture.

52:44.560 --> 52:45.920
 Oh, I have an exact statement.

52:46.800 --> 52:50.640
 Solve self driving cars while delivering shipable intermediaries.

52:50.640 --> 52:57.120
 So long term vision is have fully autonomous vehicles and make sure you're making money along the way.

52:58.160 --> 52:59.920
 I think it doesn't really speak to money, but I can talk.

52:59.920 --> 53:02.320
 I can talk about what solve self driving cars means.

53:02.320 --> 53:07.120
 Solve self driving cars, of course, means you're not building a new car.

53:07.120 --> 53:18.640
 You're building a person replacement that person can sit in the driver's seat and drive you anywhere a person can drive with a human or better level of safety, speed, quality, comfort.

53:18.640 --> 53:20.240
 And what's the second part of that?

53:20.240 --> 53:24.560
 Deliver and shipable intermediaries is, well, it's a way to fund the company.

53:24.560 --> 53:25.040
 That's true.

53:25.040 --> 53:27.040
 But it's also a way to keep us honest.

53:29.040 --> 53:36.880
 If you don't have that, it is very easy with this technology to think you are making progress when you're not.

53:36.880 --> 53:46.880
 I've heard it best described on Hacker News as you can set any arbitrary milestone, meet that milestone, and still be able to do that.

53:46.880 --> 53:50.480
 Meet that milestone and still be infinitely far away from solving self driving cars.

53:51.600 --> 54:00.480
 So it's hard to have like real deadlines when you're like cruise or Waymo when you don't have revenue.

54:02.000 --> 54:07.360
 Is that, I mean, is revenue essentially the thing we're talking about here?

54:07.840 --> 54:10.800
 Revenue is, capitalism is based around consent.

54:11.360 --> 54:14.240
 Capitalism, the way that you get revenue is a kind of real capitalism.

54:14.240 --> 54:16.480
 Common is in the real capitalism camp.

54:16.480 --> 54:19.520
 There's definitely scams out there, but real capitalism is based around consent.

54:19.520 --> 54:24.640
 It's based around this idea that like, if we're getting revenue, it's because we're providing at least that much value to another person.

54:24.640 --> 54:29.920
 When someone buys $1,000 comma two from us, we're providing them at least $1,000 of value, but they wouldn't buy it.

54:29.920 --> 54:30.640
 Brilliant.

54:30.640 --> 54:37.920
 So can you give a world wind overview of the products that Kamiai provides throughout its history and today?

54:37.920 --> 54:40.800
 I mean, yeah, the past ones aren't really that interesting.

54:40.800 --> 54:44.320
 It's kind of just been refinement of the same idea.

54:45.120 --> 54:48.160
 The real only product we sell today is the comma two.

54:48.160 --> 54:50.080
 Which is a piece of hardware with cameras.

54:52.320 --> 54:55.360
 So the comma two, I mean, you can think about it kind of like a person.

54:56.560 --> 54:59.360
 You know, in future hardware will probably be even more and more person like.

55:00.160 --> 55:08.320
 So it has, you know, eyes, ears, a mouth, a brain, and a way to interface with the car.

55:08.320 --> 55:09.920
 Does it have consciousness?

55:09.920 --> 55:10.720
 Just kidding.

55:10.720 --> 55:11.920
 That was a trick question.

55:11.920 --> 55:13.920
 I don't have consciousness either.

55:13.920 --> 55:15.520
 Me and the comma two are the same.

55:15.520 --> 55:16.240
 You're the same?

55:16.240 --> 55:17.600
 I have a little more compute than it.

55:17.600 --> 55:19.200
 It only has like the same computer.

55:19.200 --> 55:19.760
 How interesting.

55:19.760 --> 55:21.920
 B, you know.

55:21.920 --> 55:25.520
 You're more efficient energy wise for the compute you're doing.

55:25.520 --> 55:26.960
 Far more efficient energy wise.

55:27.680 --> 55:29.600
 20 pay to flops, 20 wants, crazy.

55:29.600 --> 55:30.960
 Do you lack consciousness?

55:30.960 --> 55:31.600
 Sure.

55:31.600 --> 55:32.560
 Do you fear death?

55:32.560 --> 55:33.360
 You do.

55:33.360 --> 55:34.400
 You want immortality?

55:34.400 --> 55:35.200
 Of course I fear death.

55:35.200 --> 55:36.480
 Does Kamiai fear death?

55:36.480 --> 55:37.760
 I don't think so.

55:37.760 --> 55:38.560
 I don't think so.

55:39.360 --> 55:40.320
 Of course it does.

55:40.320 --> 55:42.720
 It very much fears, well, fears negative loss.

55:42.720 --> 55:43.200
 Oh, yeah.

55:45.600 --> 55:46.080
 Okay.

55:46.080 --> 55:49.120
 So comma two, when did that come out?

55:49.120 --> 55:50.240
 That was a year ago?

55:50.240 --> 55:50.800
 No, two.

55:51.920 --> 55:52.560
 Early this year.

55:53.440 --> 55:53.920
 Wow.

55:53.920 --> 55:55.680
 Time, it feels, yeah.

55:56.240 --> 56:00.560
 To 2020 feels like it's taken 10 years to get to the end.

56:00.560 --> 56:01.680
 It's a long year.

56:01.680 --> 56:03.040
 It's a long year.

56:03.040 --> 56:09.040
 So what's the sexiest thing about comma two, feature wise?

56:09.840 --> 56:14.160
 So, I mean, maybe you can also link on like, what is it?

56:14.160 --> 56:15.600
 Like, what's its purpose?

56:15.600 --> 56:17.760
 Because there's a hardware, there's a software component.

56:18.400 --> 56:20.960
 You've mentioned the sensors, but also like,

56:20.960 --> 56:22.960
 what is it, its features and capabilities?

56:22.960 --> 56:24.640
 I think our slogan summarizes it well.

56:25.200 --> 56:26.880
 Comma slogan is make driving chill.

56:28.720 --> 56:29.200
 I love it.

56:29.200 --> 56:34.240
 Yeah, I mean, it is, you know, if you like cruise control,

56:34.240 --> 56:36.320
 imagine cruise control, but much, much more.

56:37.840 --> 56:40.880
 So it can do adaptive cruise control things,

56:40.880 --> 56:42.880
 which is like slow down for cars in front of it,

56:42.880 --> 56:44.080
 maintain a certain speed.

56:44.080 --> 56:47.840
 And it can also do lane keeping, so stay in the lane

56:47.840 --> 56:49.840
 and do it better and better and better over time.

56:49.840 --> 56:51.680
 That's very much machine learning based.

56:51.680 --> 56:58.400
 So there's cameras, there's a driver facing camera too.

56:58.400 --> 57:01.840
 What else is there?

57:01.840 --> 57:02.640
 What am I thinking?

57:02.640 --> 57:04.320
 So the hardware versus software.

57:04.320 --> 57:08.240
 So open pilot versus the actual hardware, the device.

57:08.880 --> 57:10.400
 What's, can you draw that distinction?

57:10.400 --> 57:11.440
 What's one, what's the other?

57:11.440 --> 57:13.680
 I mean, the hardware is pretty much a cell phone

57:13.680 --> 57:16.240
 with a few additions, a cell phone with a cooling system

57:16.880 --> 57:20.080
 and with a car interface connected to it.

57:20.080 --> 57:24.240
 And by cell phone, you mean like Qualcomm Snapdragon?

57:24.240 --> 57:27.360
 Yeah, the current hardware is a Snapdragon 821.

57:29.120 --> 57:31.680
 It has a Wi Fi radio, it has an LTE radio, it has a screen.

57:33.760 --> 57:35.120
 We use every part of the cell phone.

57:35.840 --> 57:38.320
 And then the interface of the car is specific to the car,

57:38.320 --> 57:40.080
 so you keep supporting more and more cars.

57:41.280 --> 57:42.720
 Yeah, through the interface of the car, I mean,

57:42.720 --> 57:45.120
 the device itself just has four can buses,

57:45.120 --> 57:46.640
 has four can interfaces on it,

57:46.640 --> 57:48.320
 they're connected through the USB port to the phone.

57:48.320 --> 57:52.400
 And then, yeah, on those four can buses,

57:53.200 --> 57:54.320
 you connect it to the car.

57:54.320 --> 57:56.240
 And there's a little harness to do this.

57:56.240 --> 57:57.680
 Cars are actually surprisingly similar.

57:58.320 --> 58:01.440
 So can is the protocol by which cars communicate.

58:01.440 --> 58:04.160
 And then you're able to read stuff and write stuff

58:04.160 --> 58:06.800
 to be able to control the car, depending on the car.

58:06.800 --> 58:08.160
 So what's the software side?

58:08.160 --> 58:09.040
 What's open pilot?

58:10.240 --> 58:12.720
 So I mean, open pilot is, the hardware is pretty simple

58:12.720 --> 58:13.680
 compared to open pilot.

58:13.680 --> 58:20.880
 Open pilot is, well, so you have a machine learning model,

58:20.880 --> 58:25.440
 which it's an open pilot, it's a blob, it's just a blob of weights.

58:25.440 --> 58:27.280
 It's not like people are like, oh, it's closed source.

58:27.280 --> 58:28.720
 I'm like, what's a blob of weights?

58:28.720 --> 58:29.440
 What do you expect?

58:31.200 --> 58:32.800
 It's primarily neural network based.

58:33.600 --> 58:36.480
 Well, open pilot is all the software kind of around

58:36.480 --> 58:38.480
 that neural network, that if you have a neural network

58:38.480 --> 58:40.640
 that says here's where you want to send the car,

58:40.640 --> 58:44.560
 open pilot actually goes and executes all of that.

58:44.560 --> 58:46.720
 It cleans up the input to the neural network,

58:46.720 --> 58:48.880
 it cleans up the output and executes on it.

58:48.880 --> 58:51.680
 So connects, it's the glue that connects everything together.

58:51.680 --> 58:54.240
 Runs the sensors, does a bunch of calibration

58:54.240 --> 58:57.840
 for the neural network, does, deals with like,

58:57.840 --> 59:01.280
 if the car is on a banked road, you have to counter steer

59:01.280 --> 59:01.840
 against that.

59:01.840 --> 59:03.600
 And the neural network can't necessarily know that

59:03.600 --> 59:04.560
 by looking at the picture.

59:06.160 --> 59:08.400
 So you can do that with other sensors, infusion,

59:08.400 --> 59:12.960
 and localizer, open pilot also is responsible for sending

59:12.960 --> 59:15.760
 the data up to our servers, so we can learn from it,

59:16.560 --> 59:18.960
 logging it, recording it, running the cameras,

59:18.960 --> 59:22.480
 thermally managing the device, managing the disk space

59:22.480 --> 59:24.560
 on the device, managing all the resources on the device.

59:24.560 --> 59:27.440
 So what, since we last spoke, I don't remember when,

59:27.440 --> 59:29.200
 maybe a year ago, maybe a little bit longer,

59:30.000 --> 59:32.880
 how has open pilot improved?

59:32.880 --> 59:34.640
 We did exactly what I promised you.

59:34.640 --> 59:36.560
 I promised you that by the end of the year,

59:36.560 --> 59:38.480
 we would be able to remove the lanes.

59:40.320 --> 59:45.840
 The lateral policy is now almost completely end to end.

59:45.840 --> 59:47.520
 You can turn the lanes off and it will drive,

59:48.320 --> 59:50.880
 drive slightly worse on the highway if you turn the lanes off,

59:50.880 --> 59:53.440
 but you can turn the lanes off and it will drive well

59:54.160 --> 59:56.640
 trained completely end to end on user data.

59:57.280 --> 59:58.720
 And this year, we hope to do the same for the

59:58.720 --> 59:59.920
 longitudinal policy.

59:59.920 --> 1:00:02.160
 So that's the interesting thing is you're not doing,

1:00:03.440 --> 1:00:05.200
 you don't appear to be, you can correct me,

1:00:05.200 --> 1:00:08.640
 you don't appear to be doing lane detection

1:00:08.640 --> 1:00:12.320
 or lane marking detection or kind of the segmentation task

1:00:12.320 --> 1:00:15.040
 or any kind of object detection task.

1:00:15.040 --> 1:00:17.440
 You're doing what's traditionally more called

1:00:17.440 --> 1:00:18.640
 like end to end learning.

1:00:19.360 --> 1:00:24.080
 So entrained on actual behavior of drivers

1:00:24.080 --> 1:00:25.840
 when they're driving the car manually.

1:00:27.600 --> 1:00:29.280
 And this is hard to do.

1:00:29.280 --> 1:00:31.120
 You know, it's not supervised learning.

1:00:32.080 --> 1:00:34.640
 Yeah, but so the nice thing is there's a lot of data,

1:00:34.640 --> 1:00:36.320
 so it's hard and easy, right?

1:00:37.760 --> 1:00:39.840
 We have a lot of high quality data, right?

1:00:39.840 --> 1:00:41.600
 Like more than you need in the center.

1:00:41.600 --> 1:00:43.280
 Well, we've way more than we do.

1:00:43.280 --> 1:00:44.800
 We've way more data than we need.

1:00:44.800 --> 1:00:46.960
 I mean, it's an interesting question, actually,

1:00:46.960 --> 1:00:50.080
 because in terms of amount, you have more than you need.

1:00:51.440 --> 1:00:54.160
 But the, you know, driving is full of edge cases.

1:00:54.160 --> 1:00:56.880
 So how do you select the data you train on?

1:00:58.160 --> 1:01:00.480
 I think this is an interesting open question.

1:01:00.480 --> 1:01:04.080
 Like, what's the cleverest way to select data?

1:01:04.080 --> 1:01:06.160
 That's the question Tesla is probably working on.

1:01:07.520 --> 1:01:09.760
 That's, I mean, the entirety of machine learning can be,

1:01:09.760 --> 1:01:10.880
 they don't seem to really care.

1:01:10.880 --> 1:01:12.160
 They just kind of select data.

1:01:12.160 --> 1:01:14.720
 But I feel like that if you want to solve,

1:01:14.720 --> 1:01:16.080
 if you want to create intelligence systems,

1:01:16.080 --> 1:01:18.720
 you have to pick data well, right?

1:01:18.720 --> 1:01:22.800
 And so would you have any hints, ideas of how to do it well?

1:01:22.800 --> 1:01:26.160
 So in some ways, that is the definition I like

1:01:26.160 --> 1:01:28.320
 of reinforcement learning versus supervised learning.

1:01:28.320 --> 1:01:34.160
 In supervised learning, the weights depend on the data, right?

1:01:34.160 --> 1:01:38.160
 And this is obviously true, but in reinforcement learning,

1:01:38.160 --> 1:01:39.280
 the data depends on the weights.

1:01:40.320 --> 1:01:40.800
 Yeah.

1:01:40.800 --> 1:01:41.280
 Right?

1:01:41.280 --> 1:01:42.800
 And actually, both ways.

1:01:42.800 --> 1:01:43.920
 That's poetry.

1:01:43.920 --> 1:01:46.160
 So how does it know what data to train on?

1:01:46.160 --> 1:01:47.360
 Well, let it pick.

1:01:47.360 --> 1:01:49.360
 We're not there yet, but that's the eventual.

1:01:49.360 --> 1:01:52.160
 So you're thinking this almost like a reinforcement learning

1:01:52.160 --> 1:01:53.120
 framework.

1:01:53.120 --> 1:01:54.320
 We're going to do RL on the world.

1:01:55.280 --> 1:01:58.000
 Every time a car makes a mistake, user disengages,

1:01:58.000 --> 1:02:00.000
 we train on that and do RL on the world.

1:02:00.000 --> 1:02:00.800
 Ship out a new model.

1:02:00.800 --> 1:02:01.760
 That's an epoch, right?

1:02:03.200 --> 1:02:08.320
 And for now, you're not doing the Elon style promising

1:02:08.320 --> 1:02:09.600
 that it's going to be fully autonomous.

1:02:09.600 --> 1:02:11.520
 You really are sticking to level two.

1:02:12.320 --> 1:02:14.400
 And it's supposed to be supervised.

1:02:15.360 --> 1:02:16.720
 It is definitely supposed to be supervised.

1:02:16.720 --> 1:02:18.160
 And we enforce the fact that it's supervised.

1:02:19.680 --> 1:02:22.880
 We look at our rate of improvement in disengagement.

1:02:23.440 --> 1:02:25.520
 OpenPilot now has an unplanned disengagement

1:02:25.520 --> 1:02:26.560
 about every 100 miles.

1:02:26.560 --> 1:02:36.080
 This is up from 10 miles, like maybe a year ago.

1:02:36.080 --> 1:02:36.560
 Yeah.

1:02:36.560 --> 1:02:38.320
 So maybe we've seen 10x improvement in a year,

1:02:38.320 --> 1:02:42.560
 but 100 miles is still a far cry from the 100,000 you're

1:02:42.560 --> 1:02:43.120
 going to need.

1:02:43.680 --> 1:02:48.240
 So you're going to somehow need to get three more 10xs in there.

1:02:49.680 --> 1:02:51.200
 And what's your intuition?

1:02:52.160 --> 1:02:54.320
 You're basically hoping that there's exponential

1:02:54.320 --> 1:02:56.240
 improvement built into the baked into the cake

1:02:56.240 --> 1:02:56.720
 somewhere.

1:02:56.720 --> 1:02:58.240
 Well, that's even, I mean, 10x improvement,

1:02:58.240 --> 1:03:00.320
 that's already assuming exponential, right?

1:03:00.320 --> 1:03:02.400
 There's definitely exponential improvement.

1:03:02.400 --> 1:03:04.160
 And I think when Elon talks about exponential,

1:03:04.160 --> 1:03:06.880
 like these things, these systems are going to exponentially

1:03:06.880 --> 1:03:07.760
 improve.

1:03:07.760 --> 1:03:10.880
 Just exponential doesn't mean you're getting 100 gigahertz

1:03:10.880 --> 1:03:13.120
 processors tomorrow, right?

1:03:13.120 --> 1:03:16.000
 Like it's going to still take a while because the gap

1:03:16.000 --> 1:03:19.440
 between even our best system and humans is still large.

1:03:20.080 --> 1:03:22.160
 So that's an interesting distinction to draw.

1:03:22.160 --> 1:03:24.720
 So if you look at the way Tesla is approaching the problem,

1:03:24.720 --> 1:03:29.040
 and the way you're approaching the problem, which is very

1:03:29.040 --> 1:03:32.560
 different than the rest of the self driving car world.

1:03:32.560 --> 1:03:35.760
 So let's put them aside is you're treating most the driving

1:03:35.760 --> 1:03:37.440
 tasks as a machine learning problem.

1:03:37.440 --> 1:03:39.520
 And the way Tesla is approaching it is with the

1:03:39.520 --> 1:03:43.120
 multitask learning, where you break the task of driving

1:03:43.120 --> 1:03:45.280
 into hundreds of different tasks.

1:03:45.280 --> 1:03:48.880
 And you have this multi headed neural network that's very

1:03:48.880 --> 1:03:51.520
 good at performing each task.

1:03:51.520 --> 1:03:55.120
 And there's presumably something on top that's stitching

1:03:55.120 --> 1:04:00.080
 stuff together in order to make control decisions, policy

1:04:00.080 --> 1:04:02.080
 decisions about how you move the car.

1:04:02.080 --> 1:04:04.320
 But what that allows you, there's a brilliance to this

1:04:04.320 --> 1:04:09.040
 because it allows you to master each task, like lane

1:04:09.040 --> 1:04:13.920
 detection, stop sign detection, the traffic light

1:04:13.920 --> 1:04:17.120
 detection, drivable area segmentation,

1:04:17.120 --> 1:04:21.280
 you know, vehicle, bicycle pedestrian detection.

1:04:21.280 --> 1:04:24.800
 There's some localization tasks in there.

1:04:24.800 --> 1:04:32.240
 Also predicting, like, yeah, predicting how the entities

1:04:32.240 --> 1:04:33.840
 in the scene are going to move.

1:04:33.840 --> 1:04:36.240
 Like everything is basically a machine learning task

1:04:36.240 --> 1:04:39.520
 where there's a classification, segmentation, prediction.

1:04:39.520 --> 1:04:44.240
 And it's nice because you can have this entire engine,

1:04:44.240 --> 1:04:48.480
 data engine that's mining for edge cases for each one

1:04:48.480 --> 1:04:49.280
 of these tasks.

1:04:49.280 --> 1:04:51.600
 And you can have people, like engineers, that are

1:04:51.600 --> 1:04:53.600
 basically masters of that task.

1:04:53.600 --> 1:04:56.880
 They become the best person in the world at, as you talk

1:04:56.880 --> 1:04:59.520
 about the cone guy for Waymo.

1:04:59.520 --> 1:05:00.960
 Yeah, they're a good old cone guy.

1:05:00.960 --> 1:05:05.440
 They become the best person in the world at cone

1:05:05.440 --> 1:05:05.920
 detection.

1:05:07.120 --> 1:05:09.680
 So that's a compelling notion from a supervised

1:05:09.680 --> 1:05:14.720
 learning perspective, automating much of the process of

1:05:14.720 --> 1:05:17.200
 edge case discovery and retraining neural network

1:05:17.200 --> 1:05:19.760
 for each of the individual perception tasks.

1:05:19.760 --> 1:05:22.080
 And then you're looking at the machine learning in a

1:05:22.080 --> 1:05:26.240
 more holistic way, basically doing end to end

1:05:26.240 --> 1:05:30.080
 learning on the driving tasks, supervised, trained

1:05:30.080 --> 1:05:34.480
 on the data of the actual driving of people they

1:05:34.480 --> 1:05:37.920
 use comma AI, like actual human drivers do manual

1:05:37.920 --> 1:05:43.920
 control, plus the moments of disengagement that maybe

1:05:43.920 --> 1:05:46.640
 with some labeling could indicate the failure of

1:05:46.640 --> 1:05:47.200
 the system.

1:05:47.200 --> 1:05:51.680
 So you have a huge amount of data for positive

1:05:51.680 --> 1:05:54.000
 control of the vehicle, like successful control of

1:05:54.000 --> 1:05:58.800
 the vehicle, both maintaining the lane as I think

1:05:58.800 --> 1:06:00.960
 you're also working on longitudinal control of

1:06:00.960 --> 1:06:04.000
 the vehicle, and then failure cases where the

1:06:04.000 --> 1:06:08.240
 vehicle does something wrong that needs disengagement.

1:06:08.240 --> 1:06:11.440
 So like what, why do you think you're right and

1:06:11.440 --> 1:06:13.440
 Tesla is wrong on this?

1:06:14.240 --> 1:06:17.440
 And do you think you'll come around the Tesla way?

1:06:17.440 --> 1:06:19.840
 Do you think Tesla will come around to your way?

1:06:21.280 --> 1:06:23.920
 If you were to start a chess engine company, would

1:06:23.920 --> 1:06:25.120
 you hire a bishop guy?

1:06:26.000 --> 1:06:29.760
 See, we have, this is Monday morning quarterbacking

1:06:29.760 --> 1:06:33.360
 is, yes, probably.

1:06:35.840 --> 1:06:37.280
 Oh, our Rook guy.

1:06:37.280 --> 1:06:39.280
 Oh, we stole the Rook guy from that company.

1:06:39.280 --> 1:06:40.800
 Oh, we're going to have real good Rooks.

1:06:40.800 --> 1:06:42.640
 Well, there's not many pieces, right?

1:06:43.840 --> 1:06:48.640
 You can, there's not many guys and gals to hire.

1:06:48.640 --> 1:06:51.200
 You just have a few that work in the bishop, a few

1:06:51.200 --> 1:06:52.480
 that work in the Rook.

1:06:52.480 --> 1:06:55.280
 But is that not ludicrous today to think about in

1:06:55.280 --> 1:06:56.560
 the world of AlphaZero?

1:06:56.560 --> 1:07:00.160
 But AlphaZero is a chess game, so the fundamental

1:07:00.160 --> 1:07:03.600
 question is how hard is driving compared to chess?

1:07:04.240 --> 1:07:10.480
 Because so long term, end to end will be the right solution.

1:07:10.480 --> 1:07:12.640
 The question is how many years away is that?

1:07:13.280 --> 1:07:15.680
 End to end is going to be the only solution for level five.

1:07:15.680 --> 1:07:17.040
 For the only way we get there.

1:07:17.040 --> 1:07:18.720
 Of course, and of course Tesla is going to come

1:07:18.720 --> 1:07:19.520
 around to my way.

1:07:19.520 --> 1:07:21.520
 And if you're a Rook guy out there, I'm sorry.

1:07:22.800 --> 1:07:23.600
 The Kone guy.

1:07:24.800 --> 1:07:25.280
 I don't know.

1:07:25.280 --> 1:07:26.720
 We're going to specialize each task.

1:07:26.720 --> 1:07:29.040
 We're going to really understand Rook placement.

1:07:29.040 --> 1:07:31.920
 Yeah, I understand the intuition you have.

1:07:31.920 --> 1:07:36.880
 I mean, that is a very compelling notion that we

1:07:36.880 --> 1:07:38.960
 can learn the task end to end.

1:07:38.960 --> 1:07:40.800
 Like the same compelling notion you might have for

1:07:40.800 --> 1:07:42.400
 natural language conversation.

1:07:42.400 --> 1:07:48.800
 But I'm not sure because one thing you sneaked in there

1:07:48.800 --> 1:07:52.400
 is the assertion that it's impossible to get to level

1:07:52.400 --> 1:07:55.760
 five without this kind of approach. I don't know if

1:07:55.760 --> 1:07:56.960
 that's obvious.

1:07:56.960 --> 1:07:58.160
 I don't know if that's obvious either.

1:07:58.160 --> 1:08:01.200
 I don't actually mean that.

1:08:01.200 --> 1:08:04.240
 I think that it is much easier to get to level five

1:08:04.240 --> 1:08:05.520
 with an end to end approach.

1:08:05.520 --> 1:08:08.240
 I think that the other approach is doable.

1:08:08.800 --> 1:08:11.280
 But the magnitude of the engineering challenge

1:08:11.280 --> 1:08:12.800
 may exceed what humanity is capable of.

1:08:13.600 --> 1:08:17.760
 So, but what do you think of the Tesla data engine

1:08:17.760 --> 1:08:20.960
 approach, which to me is an active learning task.

1:08:20.960 --> 1:08:22.320
 It's kind of fascinating.

1:08:22.320 --> 1:08:25.520
 It's breaking it down into these multiple tasks

1:08:25.520 --> 1:08:29.360
 and mining their data constantly for like edge cases

1:08:29.360 --> 1:08:30.240
 for these different tasks.

1:08:30.240 --> 1:08:32.240
 Yeah, but the tasks themselves are not being learned.

1:08:32.240 --> 1:08:33.280
 This is feature engineering.

1:08:35.760 --> 1:08:41.040
 I mean, it's a higher abstraction level of feature

1:08:41.040 --> 1:08:42.560
 engineering for the different tasks.

1:08:43.120 --> 1:08:44.720
 It's task engineering in a sense.

1:08:44.720 --> 1:08:46.720
 It's slightly better feature engineering,

1:08:46.720 --> 1:08:49.200
 but it's still fundamentally is feature engineering.

1:08:49.200 --> 1:08:51.760
 And if anything about the history of AI has taught us

1:08:51.760 --> 1:08:54.560
 anything, it's that feature engineering approaches

1:08:54.560 --> 1:08:57.600
 will always be replaced and lose to end to end.

1:08:57.600 --> 1:09:02.000
 Now, to be fair, I cannot really make promises on timelines,

1:09:02.000 --> 1:09:05.680
 but I can say that when you look at the code for stock fish

1:09:05.680 --> 1:09:06.960
 and the code for alpha zero,

1:09:06.960 --> 1:09:08.320
 one is a lot shorter than the other.

1:09:09.040 --> 1:09:10.640
 A lot more elegant and required a lot less

1:09:10.640 --> 1:09:11.680
 programmer hours to write.

1:09:11.680 --> 1:09:20.960
 Yeah, but there was a lot more murder of bad agents

1:09:21.680 --> 1:09:24.000
 on the alpha zero side.

1:09:24.800 --> 1:09:29.280
 By murder, I mean agents that played a game

1:09:29.280 --> 1:09:30.480
 and failed miserably.

1:09:30.480 --> 1:09:30.720
 Yeah.

1:09:31.520 --> 1:09:32.240
 Oh, oh.

1:09:32.240 --> 1:09:34.800
 In simulation, that failure is less costly.

1:09:34.800 --> 1:09:35.520
 Yeah.

1:09:35.520 --> 1:09:37.280
 In real world, it's...

1:09:37.280 --> 1:09:38.240
 Do you mean in practice,

1:09:38.240 --> 1:09:40.160
 like alpha zero has lost games miserably?

1:09:40.160 --> 1:09:41.200
 No.

1:09:41.200 --> 1:09:43.200
 Oh, I haven't seen that.

1:09:43.200 --> 1:09:46.800
 No, but I know, but the requirement for alpha zero

1:09:46.800 --> 1:09:51.360
 is to be able to like evolution, human evolution,

1:09:51.360 --> 1:09:52.160
 not human evolution,

1:09:52.160 --> 1:09:55.440
 biological evolution of life on earth from the origin of life

1:09:56.160 --> 1:10:00.640
 has murdered trillions upon trillions of organisms

1:10:00.640 --> 1:10:02.000
 on the path to us humans.

1:10:02.960 --> 1:10:06.880
 So the question is, can we stitch together a human like object

1:10:06.880 --> 1:10:09.760
 without having to go through the entirety process of evolution?

1:10:09.760 --> 1:10:11.840
 Well, no, but do the evolution in simulation?

1:10:11.840 --> 1:10:12.720
 Yeah, that's the question.

1:10:12.720 --> 1:10:13.360
 Can we simulate?

1:10:13.360 --> 1:10:16.080
 So do you ever sense that it's possible to simulate some aspect of it?

1:10:16.080 --> 1:10:18.080
 Mu zero is exactly this.

1:10:18.080 --> 1:10:21.200
 Mu zero is the solution to this.

1:10:21.200 --> 1:10:23.840
 Mu zero, I think, is going to be looked back

1:10:23.840 --> 1:10:25.040
 as the canonical paper.

1:10:25.040 --> 1:10:26.720
 And I don't think deep learning is everything.

1:10:26.720 --> 1:10:29.280
 I think that there's still a bunch of things missing to get there.

1:10:29.280 --> 1:10:31.360
 But Mu zero, I think, is going to be looked back

1:10:31.360 --> 1:10:36.960
 as the kind of cornerstone paper of this whole deep learning era.

1:10:36.960 --> 1:10:39.440
 And Mu zero is the solution to self driving cars.

1:10:39.440 --> 1:10:41.040
 You have to make a few tweaks to it.

1:10:41.040 --> 1:10:42.640
 But Mu zero does effectively that.

1:10:42.640 --> 1:10:47.440
 It does those rollouts and those murdering in a learned simulator

1:10:47.440 --> 1:10:48.480
 in a learned dynamics model.

1:10:50.000 --> 1:10:50.560
 It's interesting.

1:10:50.560 --> 1:10:51.440
 It doesn't get enough love.

1:10:51.440 --> 1:10:54.080
 I was blown away when I was blown away when I read that paper.

1:10:54.080 --> 1:10:56.880
 I'm like, you know, OK, I've always had a comma.

1:10:56.880 --> 1:10:58.320
 I'm going to sit and I'm going to wait for the solution

1:10:58.320 --> 1:10:59.600
 to self driving cars to come along.

1:11:00.160 --> 1:11:00.880
 This year I saw it.

1:11:00.880 --> 1:11:01.600
 It's Mu zero.

1:11:01.600 --> 1:11:01.840
 No.

1:11:04.960 --> 1:11:08.080
 So sit back and let the winning roll in.

1:11:08.080 --> 1:11:12.560
 So your sense, just to elaborate a little bit the link on the topic,

1:11:12.560 --> 1:11:15.280
 your sense is neural networks will solve driving.

1:11:15.280 --> 1:11:15.760
 Yes.

1:11:15.760 --> 1:11:17.360
 Like we don't need anything else.

1:11:17.360 --> 1:11:21.040
 I think the same way chess was maybe the chess and maybe Google

1:11:21.040 --> 1:11:24.000
 are the pinnacle of like search algorithms

1:11:24.000 --> 1:11:25.760
 and things that look kind of like A star.

1:11:27.600 --> 1:11:31.760
 The pinnacle of this era is going to be self driving cars.

1:11:33.680 --> 1:11:37.120
 But on the path that you have to deliver products,

1:11:37.120 --> 1:11:42.800
 and it's possible that the path to full self driving cars will take decades.

1:11:43.360 --> 1:11:44.000
 I doubt it.

1:11:44.000 --> 1:11:45.920
 So how long would you put on it?

1:11:46.800 --> 1:11:47.440
 Like what?

1:11:47.440 --> 1:11:48.000
 What are we?

1:11:48.480 --> 1:11:49.680
 You're chasing it.

1:11:49.680 --> 1:11:51.200
 Tesla is chasing it.

1:11:52.400 --> 1:11:53.360
 What are we talking about?

1:11:53.360 --> 1:11:54.800
 Five years, 10 years, 50 years?

1:11:54.800 --> 1:11:56.160
 Let's say in the 2020s.

1:11:56.960 --> 1:11:58.080
 In the 2020s.

1:11:58.080 --> 1:12:00.080
 The later part of the 2020s.

1:12:02.480 --> 1:12:03.920
 With the neural network.

1:12:04.480 --> 1:12:05.600
 That would be nice to see.

1:12:05.600 --> 1:12:08.400
 And on the path to that, you're delivering your products,

1:12:08.400 --> 1:12:09.840
 which is a nice L2 system.

1:12:09.840 --> 1:12:12.160
 That's what Tesla is doing, a nice L2 system.

1:12:12.160 --> 1:12:13.360
 It just gets better every time.

1:12:13.360 --> 1:12:15.920
 The only difference between L2 and the other levels

1:12:15.920 --> 1:12:16.880
 is who takes liability.

1:12:16.880 --> 1:12:18.160
 And I'm not a liability guy.

1:12:18.160 --> 1:12:19.200
 I don't want to take liability.

1:12:19.200 --> 1:12:20.320
 I'm going to level two forever.

1:12:21.920 --> 1:12:25.360
 Now, on that little transition, I mean,

1:12:26.320 --> 1:12:28.240
 how do you make the transition work?

1:12:28.240 --> 1:12:31.440
 Is this where driver sensing comes in?

1:12:31.440 --> 1:12:34.960
 Like how do you make the, because you said 100 miles,

1:12:34.960 --> 1:12:40.400
 like, is there some sort of human factor psychology thing

1:12:40.400 --> 1:12:42.320
 where people start to over trust the system?

1:12:42.320 --> 1:12:43.920
 All those kinds of effects.

1:12:43.920 --> 1:12:45.920
 Once it gets better and better and better and better,

1:12:45.920 --> 1:12:47.680
 they get lazier and lazier and lazier.

1:12:48.560 --> 1:12:50.960
 Is that, like, how do you get that transition right?

1:12:51.600 --> 1:12:53.520
 First off, our monitoring is already adaptive.

1:12:53.520 --> 1:12:55.280
 Our monitoring is already seen adaptive.

1:12:55.760 --> 1:12:56.960
 Driver monitoring.

1:12:56.960 --> 1:12:59.040
 Is this the camera that's looking at the driver?

1:12:59.040 --> 1:13:01.200
 You have an infrared camera in the...

1:13:01.200 --> 1:13:04.800
 Our policy for how we enforce the driver monitoring

1:13:04.800 --> 1:13:05.920
 is seen adaptive.

1:13:05.920 --> 1:13:06.800
 What's that mean?

1:13:06.800 --> 1:13:09.760
 Well, for example, in one of the extreme cases,

1:13:11.120 --> 1:13:13.120
 if the car is not moving,

1:13:13.120 --> 1:13:16.080
 we do not actively enforce driver monitoring.

1:13:16.080 --> 1:13:22.160
 If you are going through like a 45 mile an hour road

1:13:22.160 --> 1:13:26.320
 with lights and stop signs and potentially pedestrians,

1:13:26.320 --> 1:13:28.960
 we enforce a very tight driver monitoring policy.

1:13:28.960 --> 1:13:31.520
 If you are alone on a perfectly straight highway,

1:13:32.320 --> 1:13:34.960
 and it's all machine learning, none of that is hand coded.

1:13:34.960 --> 1:13:36.960
 Actually, the stop is hand coded, but...

1:13:36.960 --> 1:13:40.400
 So there's some kind of machine learning estimation of risk?

1:13:40.400 --> 1:13:40.960
 Yes.

1:13:40.960 --> 1:13:43.920
 Yeah, I mean, I've always been a huge fan of that.

1:13:43.920 --> 1:13:51.360
 That's difficult to do every step into that direction

1:13:51.360 --> 1:13:52.960
 is a worthwhile stop to take.

1:13:52.960 --> 1:13:54.560
 It might be difficult to do really well.

1:13:54.560 --> 1:13:57.360
 Like, us humans are able to estimate risk pretty damn well.

1:13:57.360 --> 1:13:58.960
 Whatever the hell that is,

1:13:58.960 --> 1:14:02.800
 that feels like one of the nice features of us humans.

1:14:02.800 --> 1:14:06.160
 Because we humans are really good drivers

1:14:06.160 --> 1:14:08.320
 when we're really tuned in.

1:14:08.320 --> 1:14:10.160
 And we're good at estimating risk,

1:14:10.160 --> 1:14:12.160
 like when are we supposed to be tuned in?

1:14:12.160 --> 1:14:13.200
 Yeah.

1:14:13.200 --> 1:14:14.800
 And people are like,

1:14:14.800 --> 1:14:17.600
 oh, well, why would you ever make the driver monitoring policy

1:14:17.600 --> 1:14:18.400
 less aggressive?

1:14:18.400 --> 1:14:21.200
 Why would you always not keep it at its most aggressive?

1:14:21.200 --> 1:14:23.200
 Because then people are just going to get fatigued from it.

1:14:23.200 --> 1:14:24.800
 Yeah, well, they get annoyed.

1:14:24.800 --> 1:14:26.000
 You know, you know,

1:14:26.000 --> 1:14:30.800
 when they get annoyed, you want the experience to be pleasant.

1:14:30.800 --> 1:14:32.400
 Obviously, I want the experience to be pleasant,

1:14:32.400 --> 1:14:35.200
 but even just from a straight up safety perspective,

1:14:35.920 --> 1:14:38.640
 if you alert people when they look around

1:14:38.640 --> 1:14:40.880
 and they're like, why is this thing alerting me?

1:14:40.880 --> 1:14:42.880
 There's nothing I could possibly hit right now.

1:14:42.880 --> 1:14:44.240
 People will just learn to tune it out.

1:14:45.040 --> 1:14:46.720
 People will just learn to tune it out,

1:14:46.720 --> 1:14:47.920
 to put weights on the steering wheel,

1:14:47.920 --> 1:14:49.760
 to do whatever, to overcome it.

1:14:49.760 --> 1:14:53.440
 And remember that you're always part of this adaptive system.

1:14:53.440 --> 1:14:56.800
 So all I can really say about how this scale is going forward

1:14:56.800 --> 1:14:58.240
 is, yeah, something we have to monitor for.

1:14:59.200 --> 1:15:00.000
 Ooh, we don't know.

1:15:00.000 --> 1:15:01.920
 This is a great psychology experiment at scale.

1:15:01.920 --> 1:15:03.040
 Like, we'll see.

1:15:03.040 --> 1:15:03.840
 Yeah, it's fascinating.

1:15:03.840 --> 1:15:04.480
 Track it.

1:15:04.480 --> 1:15:08.160
 And making sure you have a good understanding of attention

1:15:08.960 --> 1:15:11.280
 is a very key part of that psychology problem.

1:15:11.280 --> 1:15:14.000
 Yeah, I think you and I probably have a different,

1:15:14.000 --> 1:15:15.040
 come to it differently.

1:15:15.040 --> 1:15:19.520
 But to me, it's a fascinating psychology problem

1:15:19.520 --> 1:15:21.920
 to explore something much deeper than just driving.

1:15:21.920 --> 1:15:25.600
 It's such a nice way to explore human attention

1:15:26.480 --> 1:15:30.080
 and human behavior, which is why, again,

1:15:30.080 --> 1:15:33.680
 we've probably both criticized Mr. Elon Musk

1:15:33.680 --> 1:15:37.280
 on this one topic from different avenues.

1:15:38.000 --> 1:15:41.120
 So both offline and online, I had little chats with Elon.

1:15:44.000 --> 1:15:45.840
 Like, I love human beings.

1:15:45.840 --> 1:15:49.360
 As a computer vision problem, as an AI problem,

1:15:49.360 --> 1:15:50.160
 it's fascinating.

1:15:50.160 --> 1:15:52.960
 He wasn't so much interested in that problem.

1:15:54.400 --> 1:15:57.360
 It's like, in order to solve driving, the whole point

1:15:57.360 --> 1:15:59.360
 is you want to remove the human from the picture.

1:16:01.120 --> 1:16:04.000
 And it seems like you can't do that quite yet.

1:16:04.000 --> 1:16:05.040
 Eventually, yes.

1:16:05.040 --> 1:16:06.880
 But you can't quite do that yet.

1:16:07.840 --> 1:16:12.160
 So this is the moment where, and you can't yet say,

1:16:12.160 --> 1:16:17.600
 I told you so, to Tesla, but it's getting there

1:16:17.600 --> 1:16:19.120
 because I don't know if you've seen this.

1:16:19.120 --> 1:16:21.040
 There's some reporting that they're, in fact,

1:16:21.040 --> 1:16:23.040
 starting to do driver monitoring.

1:16:23.040 --> 1:16:24.320
 Yeah, they ship the model in shadow mode.

1:16:26.160 --> 1:16:29.040
 With, though, I believe only a visible light camera.

1:16:29.040 --> 1:16:30.640
 It might even be fisheye.

1:16:31.680 --> 1:16:33.120
 It's like a low resolution.

1:16:33.120 --> 1:16:34.640
 Low resolution visible light.

1:16:34.640 --> 1:16:37.040
 I mean, to be fair, that's what we have in the Eon as well.

1:16:37.040 --> 1:16:38.720
 Our last generation product.

1:16:38.720 --> 1:16:42.000
 This is the one area where I can say our hardware's ahead of Tesla.

1:16:42.000 --> 1:16:43.680
 The rest of our hardware way, way behind,

1:16:43.680 --> 1:16:45.120
 but our driver monitoring camera.

1:16:45.120 --> 1:16:50.080
 So you think, I think on the third row, Tesla podcast,

1:16:50.080 --> 1:16:53.760
 or somewhere else, I've heard you say that obviously,

1:16:53.760 --> 1:16:55.600
 eventually, they're going to have driver monitoring.

1:16:56.320 --> 1:16:58.880
 I think what I've said is Elon will definitely ship

1:16:58.880 --> 1:17:00.880
 driver monitoring before he ships level five.

1:17:00.880 --> 1:17:01.840
 Level three, four, level five.

1:17:01.840 --> 1:17:03.280
 And I'm willing to bet 10 grand on that.

1:17:03.280 --> 1:17:05.280
 And you bet 10 grand on that.

1:17:06.160 --> 1:17:07.440
 I mean, now I know what to take the bet,

1:17:07.440 --> 1:17:08.800
 but before, maybe someone would have thought,

1:17:08.800 --> 1:17:09.840
 I should have got my money in.

1:17:09.840 --> 1:17:10.640
 Yeah.

1:17:10.640 --> 1:17:12.080
 It's an interesting bet.

1:17:12.080 --> 1:17:15.520
 I think you're right.

1:17:15.520 --> 1:17:22.480
 I'm actually on a human level, because he's made the decision.

1:17:23.520 --> 1:17:26.320
 Like he said that driver monitoring is the wrong way to go.

1:17:26.800 --> 1:17:30.240
 But you have to think of as a human, as a CEO,

1:17:30.240 --> 1:17:33.840
 I think that's the right thing to say when...

1:17:37.040 --> 1:17:39.280
 Sometimes you have to say things publicly

1:17:39.280 --> 1:17:40.960
 that are different than what you actually believe,

1:17:40.960 --> 1:17:44.080
 because when you're producing a large number of vehicles,

1:17:44.080 --> 1:17:46.960
 and the decision was made not to include the camera,

1:17:46.960 --> 1:17:48.240
 like what are you supposed to say?

1:17:48.240 --> 1:17:48.960
 Yeah.

1:17:48.960 --> 1:17:52.160
 Like, our cars don't have the thing that I think is right to have.

1:17:53.040 --> 1:17:56.640
 It's an interesting thing, but like on the other side,

1:17:56.640 --> 1:17:59.040
 as a CEO, I mean, something you could probably speak to

1:17:59.040 --> 1:18:02.800
 as a leader, I think about me as a human

1:18:04.080 --> 1:18:06.160
 to publicly change your mind on something.

1:18:06.160 --> 1:18:07.120
 How hard is that?

1:18:07.120 --> 1:18:09.840
 Well, especially when assholes like George Haas say,

1:18:09.840 --> 1:18:11.360
 I told you so.

1:18:12.160 --> 1:18:14.480
 All I will say is I am not a leader,

1:18:14.480 --> 1:18:16.080
 and I am happy to change my mind.

1:18:17.040 --> 1:18:18.160
 You think Elon will?

1:18:20.480 --> 1:18:21.040
 Yeah, I do.

1:18:22.320 --> 1:18:24.160
 I think he'll come up with a good way

1:18:24.160 --> 1:18:27.200
 to make it psychologically okay for him.

1:18:27.200 --> 1:18:29.520
 Well, it's such an important thing, man,

1:18:29.520 --> 1:18:31.280
 especially for a first principles thinker,

1:18:31.280 --> 1:18:34.640
 because he made a decision that driver monitoring

1:18:34.640 --> 1:18:35.600
 is not the right way to go.

1:18:35.600 --> 1:18:36.880
 And I could see that decision,

1:18:37.440 --> 1:18:39.040
 and I could even make that decision.

1:18:39.040 --> 1:18:41.440
 Like, I was on the fence too.

1:18:43.600 --> 1:18:46.480
 Driver monitoring is such an obvious,

1:18:47.200 --> 1:18:49.040
 simple solution to the problem of attention.

1:18:49.840 --> 1:18:52.720
 It's not obvious to me that just by putting a camera there,

1:18:52.720 --> 1:18:54.080
 you solve things.

1:18:54.080 --> 1:18:58.880
 You have to create an incredible, compelling experience,

1:18:58.880 --> 1:19:00.880
 just like you're talking about.

1:19:00.880 --> 1:19:03.120
 I don't know if it's easy to do that.

1:19:03.120 --> 1:19:05.280
 It's not at all easy to do that, in fact, I think.

1:19:05.280 --> 1:19:10.960
 So, as a creator of a car that's trying to create a product

1:19:10.960 --> 1:19:13.280
 that people love, which is what Tesla tries to do.

1:19:13.280 --> 1:19:13.600
 Right?

1:19:14.240 --> 1:19:18.320
 It's not obvious to me that as a design decision,

1:19:18.320 --> 1:19:20.960
 whether adding a camera is a good idea.

1:19:20.960 --> 1:19:22.480
 From a safety perspective either,

1:19:23.200 --> 1:19:25.200
 in the human factors community,

1:19:25.200 --> 1:19:27.920
 everybody says that you should obviously have

1:19:27.920 --> 1:19:29.920
 driver sensing, driver monitoring.

1:19:29.920 --> 1:19:36.960
 But that's like saying it's obvious as parents

1:19:36.960 --> 1:19:38.880
 you shouldn't let your kids go out at night.

1:19:39.920 --> 1:19:40.720
 But okay.

1:19:42.000 --> 1:19:45.440
 But they're still going to find ways to do drugs.

1:19:47.600 --> 1:19:48.320
 Yeah.

1:19:48.320 --> 1:19:49.920
 You have to also be good parents.

1:19:49.920 --> 1:19:53.120
 So, it's much more complicated than just you need

1:19:53.120 --> 1:19:54.320
 to have driver monitoring.

1:19:54.320 --> 1:19:58.720
 I totally disagree on, okay, if you have a camera there,

1:19:58.720 --> 1:20:00.320
 and the camera is watching the person,

1:20:00.320 --> 1:20:02.880
 but never throws an alert, they'll never think about it.

1:20:03.600 --> 1:20:03.840
 Right?

1:20:04.480 --> 1:20:08.480
 The driver monitoring policy that you choose to,

1:20:08.480 --> 1:20:10.320
 how you choose to communicate with the user,

1:20:10.320 --> 1:20:14.320
 is entirely separate from the data collection perspective.

1:20:14.320 --> 1:20:15.120
 Right.

1:20:15.120 --> 1:20:15.360
 Right.

1:20:15.920 --> 1:20:21.280
 So, there's one thing to say,

1:20:22.480 --> 1:20:24.480
 tell your teenager they can't do something.

1:20:24.480 --> 1:20:27.040
 There's another thing to gather the data.

1:20:27.040 --> 1:20:29.280
 So, you can make informed decisions, that's really interesting.

1:20:29.280 --> 1:20:32.000
 But you have to make that, that's the interesting thing

1:20:32.000 --> 1:20:32.720
 about cars.

1:20:33.520 --> 1:20:37.600
 But even true with ComAI, you don't have to manufacture

1:20:37.600 --> 1:20:40.080
 the thing into the car, is you have to make a decision

1:20:40.080 --> 1:20:44.160
 that anticipates the right strategy long term.

1:20:44.160 --> 1:20:45.920
 So, you have to start collecting the data

1:20:46.640 --> 1:20:47.760
 and start making decisions.

1:20:47.760 --> 1:20:50.000
 It started at three years ago.

1:20:50.000 --> 1:20:52.400
 I believe that we have the best driver monitoring

1:20:52.400 --> 1:20:53.200
 solution in the world.

1:20:53.200 --> 1:20:56.080
 I think that when you compare it to Supercruise,

1:20:56.080 --> 1:20:57.840
 it's the only other one that I really know that shipped,

1:20:57.840 --> 1:20:59.360
 and ours is better.

1:20:59.360 --> 1:21:04.160
 What do you like and not like about Supercruise?

1:21:04.160 --> 1:21:06.080
 I mean, I had a few.

1:21:06.080 --> 1:21:10.240
 Supercruise, the sun would be shining through the window,

1:21:10.240 --> 1:21:12.560
 would blind the camera, and it would say I wasn't paying

1:21:12.560 --> 1:21:14.560
 attention when I was looking completely straight.

1:21:14.560 --> 1:21:17.120
 I couldn't reset the attention with a steering wheel,

1:21:17.120 --> 1:21:19.120
 touch, and Supercruise would disengage.

1:21:19.120 --> 1:21:21.120
 Like, I was communicating to the car, I'm like,

1:21:21.120 --> 1:21:23.280
 look, I'm here, I'm paying attention.

1:21:23.280 --> 1:21:25.360
 Why are you really going to force me to disengage?

1:21:25.360 --> 1:21:26.240
 And it did.

1:21:27.520 --> 1:21:30.640
 So, it's a constant conversation with the user,

1:21:30.640 --> 1:21:32.800
 and yeah, there's no way to ship a system like this

1:21:32.800 --> 1:21:33.600
 if you can't OTA.

1:21:34.400 --> 1:21:35.840
 We're shipping a new one every month.

1:21:35.840 --> 1:21:38.800
 Sometimes we balance it with our users on Discord.

1:21:38.800 --> 1:21:41.040
 Sometimes we make the driver monitoring a little more

1:21:41.040 --> 1:21:42.400
 aggressive and people complain.

1:21:42.400 --> 1:21:43.600
 Sometimes they don't.

1:21:43.600 --> 1:21:45.760
 We want it to be as aggressive as possible

1:21:45.760 --> 1:21:47.840
 where people don't complain and it doesn't feel intrusive.

1:21:47.840 --> 1:21:49.920
 So, being able to update the system over the air

1:21:49.920 --> 1:21:51.520
 is an essential component.

1:21:51.520 --> 1:21:54.320
 I mean, that's probably, to me, you mentioned,

1:21:55.600 --> 1:21:59.840
 I mean, to me, that is the biggest innovation of Tesla,

1:21:59.840 --> 1:22:03.840
 that it made it, people realize that over the air updates

1:22:03.840 --> 1:22:04.800
 is essential.

1:22:04.800 --> 1:22:05.640
 Yeah.

1:22:06.640 --> 1:22:09.040
 I mean, was that not obvious from the iPhone?

1:22:09.040 --> 1:22:11.760
 The iPhone was the first real product that OTA'd, I think.

1:22:11.760 --> 1:22:14.000
 Was it actually, that's brilliant, you're right.

1:22:14.000 --> 1:22:15.840
 I mean, the game consoles used to not, right?

1:22:15.840 --> 1:22:17.840
 The game consoles were maybe the second thing that did.

1:22:17.840 --> 1:22:20.080
 Well, I didn't really think about it.

1:22:20.080 --> 1:22:23.200
 One of the amazing features of a smartphone

1:22:23.200 --> 1:22:26.880
 isn't just, like, the touchscreen isn't the thing.

1:22:26.880 --> 1:22:29.840
 It's the ability to constantly update.

1:22:29.840 --> 1:22:31.120
 Yeah, it gets better.

1:22:31.120 --> 1:22:32.320
 It gets better.

1:22:34.320 --> 1:22:35.600
 I love my iOS 14.

1:22:35.600 --> 1:22:37.440
 Yeah.

1:22:37.440 --> 1:22:40.800
 Well, one thing that I probably disagree with you

1:22:40.800 --> 1:22:44.720
 on, on driver monitoring is you said that it's easy.

1:22:44.720 --> 1:22:48.640
 I mean, you tend to say stuff is easy.

1:22:48.640 --> 1:22:52.640
 I'm sure, I guess you said it's easy

1:22:52.640 --> 1:22:55.520
 relative to the external perception problem there.

1:22:58.160 --> 1:23:00.880
 Can you elaborate why you think it's easy?

1:23:00.880 --> 1:23:03.440
 Feature engineering works for driver monitoring.

1:23:03.440 --> 1:23:05.840
 Feature engineering does not work for the external.

1:23:05.840 --> 1:23:09.760
 So human faces are not, human faces

1:23:09.760 --> 1:23:13.520
 and the movement of human faces and head and body

1:23:13.520 --> 1:23:16.080
 is not as variable as the external environment

1:23:16.080 --> 1:23:17.200
 is your intuition.

1:23:17.200 --> 1:23:20.080
 Yes, and there's another big difference as well.

1:23:20.080 --> 1:23:22.400
 Your reliability of a driver monitoring system

1:23:22.400 --> 1:23:24.240
 doesn't actually need to be that hot.

1:23:24.240 --> 1:23:27.120
 The uncertainty, if you have something that's detecting

1:23:27.120 --> 1:23:29.680
 whether the human's paying attention and it only works 92%

1:23:29.680 --> 1:23:32.640
 of the time, you're still getting almost all the benefit of that

1:23:32.640 --> 1:23:35.360
 because the human, like, you're training the human, right?

1:23:35.360 --> 1:23:38.960
 You're dealing with a system that's really helping you out.

1:23:38.960 --> 1:23:40.160
 It's a conversation.

1:23:40.160 --> 1:23:43.360
 It's not like the external thing where, guess what?

1:23:43.360 --> 1:23:46.000
 If you swerve into a tree, you swerve into a tree, right?

1:23:46.000 --> 1:23:48.160
 Like, you get no margin for error.

1:23:48.160 --> 1:23:49.520
 Yeah, I think that's really well put.

1:23:49.520 --> 1:23:57.200
 I think that's the right, exactly the place where we're comparing

1:23:57.200 --> 1:23:59.360
 to the external perception and the control problem.

1:24:00.160 --> 1:24:02.080
 Driver monitoring is easier because you don't,

1:24:02.080 --> 1:24:04.160
 the bar for success is much lower.

1:24:05.200 --> 1:24:07.840
 Yeah, but I still think like the human face

1:24:09.040 --> 1:24:12.000
 is more complicated actually than the external environment.

1:24:12.000 --> 1:24:13.520
 But for driving, you don't give a damn.

1:24:14.160 --> 1:24:20.080
 I don't need something that complicated to have to communicate

1:24:20.080 --> 1:24:23.120
 the idea to the human that I want to communicate,

1:24:23.120 --> 1:24:25.600
 which is, yo, system might mess up here.

1:24:25.600 --> 1:24:26.480
 You got to pay attention.

1:24:27.680 --> 1:24:32.480
 Yeah. See, that's my love and fascination is the human face.

1:24:32.480 --> 1:24:38.240
 And it feels like this is a nice place to create products

1:24:38.240 --> 1:24:39.920
 that create an experience in the car.

1:24:39.920 --> 1:24:45.280
 So like, it feels like there should be more richer experiences

1:24:45.280 --> 1:24:45.760
 in the car.

1:24:47.040 --> 1:24:50.560
 You know, like that's an opportunity for like something

1:24:50.560 --> 1:24:53.680
 like Kama AI or just any kind of system like a Tesla

1:24:53.680 --> 1:24:55.360
 or any of the autonomous vehicle companies

1:24:56.080 --> 1:24:59.120
 is because software is, there's much more sensors

1:24:59.120 --> 1:25:00.800
 and so much is running on software and you're

1:25:00.800 --> 1:25:01.920
 doing machine learning anyway.

1:25:02.720 --> 1:25:05.360
 There's an opportunity to create totally new experiences

1:25:06.080 --> 1:25:08.080
 that we're not even anticipating.

1:25:08.080 --> 1:25:10.480
 You don't think so? No.

1:25:10.480 --> 1:25:12.720
 You think it's a box that gets you from A to B

1:25:12.720 --> 1:25:14.000
 and you want to do it chill?

1:25:14.960 --> 1:25:17.360
 Yeah. I mean, I think as soon as we get to level three on highways,

1:25:17.360 --> 1:25:19.120
 okay, enjoy your Candy Crush.

1:25:19.120 --> 1:25:20.160
 Enjoy your Hulu.

1:25:20.160 --> 1:25:23.520
 Enjoy your, you know, whatever, whatever.

1:25:23.520 --> 1:25:24.160
 Sure, you get this.

1:25:24.160 --> 1:25:26.800
 You can look at screens basically versus right now,

1:25:26.800 --> 1:25:27.520
 what do you have?

1:25:27.520 --> 1:25:28.480
 Music and audiobooks.

1:25:28.480 --> 1:25:30.560
 So level three is where you can kind of disengage

1:25:30.560 --> 1:25:31.920
 in stretches of time.

1:25:34.800 --> 1:25:37.200
 Well, you think level three is possible.

1:25:37.200 --> 1:25:38.960
 Like on the highway going for 100 miles

1:25:38.960 --> 1:25:40.320
 and you can just go to sleep?

1:25:40.320 --> 1:25:42.640
 Oh yeah, sleep.

1:25:43.440 --> 1:25:47.120
 So again, I think it's really all on a spectrum.

1:25:47.120 --> 1:25:49.920
 I think that being able to use your phone

1:25:49.920 --> 1:25:53.280
 while you're on the highway and like this all being okay

1:25:53.280 --> 1:25:55.200
 and being aware that the car might alert you

1:25:55.200 --> 1:25:56.960
 when you have five seconds to basically.

1:25:56.960 --> 1:25:58.800
 So the five second thing that you think is possible?

1:25:58.800 --> 1:25:59.520
 Yeah, I think it is.

1:25:59.520 --> 1:26:01.280
 Oh yeah, not in all scenarios.

1:26:01.920 --> 1:26:03.200
 Some scenarios it's not.

1:26:03.200 --> 1:26:06.080
 It's the whole risk thing that you mentioned is nice

1:26:06.080 --> 1:26:10.480
 is to be able to estimate like how risk is this situation.

1:26:10.480 --> 1:26:12.000
 That's really important to understand.

1:26:12.800 --> 1:26:17.280
 One other thing you mentioned comparing comma and autopilot

1:26:17.280 --> 1:26:22.320
 is that something about the haptic feel

1:26:22.320 --> 1:26:25.760
 of the way comma controls the car when things are uncertain.

1:26:25.760 --> 1:26:27.600
 Like it behaves a little bit more uncertain

1:26:27.600 --> 1:26:28.560
 when things are uncertain.

1:26:29.440 --> 1:26:30.960
 That's kind of an interesting point

1:26:30.960 --> 1:26:33.920
 and then autopilot is much more confident always

1:26:33.920 --> 1:26:35.440
 even when it's uncertain

1:26:35.440 --> 1:26:37.120
 until it runs into trouble.

1:26:39.120 --> 1:26:40.800
 That's a funny thing.

1:26:40.800 --> 1:26:42.560
 I actually mentioned that to Elon I think

1:26:42.560 --> 1:26:46.160
 and then the first time we talked he was inviting

1:26:46.160 --> 1:26:47.920
 is like communicating uncertainty.

1:26:48.720 --> 1:26:52.480
 I guess comma doesn't really communicate uncertainty explicitly.

1:26:52.480 --> 1:26:54.240
 It communicates it through haptic feel.

1:26:54.960 --> 1:26:58.000
 Like what's the role of communicating uncertainty do you think?

1:26:58.000 --> 1:26:59.680
 We do some stuff explicitly.

1:26:59.680 --> 1:27:01.680
 Like we do detect the lanes when you're on the highway

1:27:01.680 --> 1:27:04.240
 and we'll show you how many lanes we're using to drive with.

1:27:04.240 --> 1:27:06.080
 You can look at where things the lanes are.

1:27:06.080 --> 1:27:08.320
 You can look at the path.

1:27:08.320 --> 1:27:10.240
 And we want to be better about this

1:27:10.240 --> 1:27:11.360
 when we're actually hiring.

1:27:11.360 --> 1:27:12.800
 We want to hire some new UI people.

1:27:12.800 --> 1:27:14.160
 UI people, you mentioned this.

1:27:14.160 --> 1:27:16.560
 Because it's a UI problem too, right?

1:27:16.560 --> 1:27:19.200
 We have a great designer now

1:27:19.200 --> 1:27:21.040
 but we need people who are just going to build this

1:27:21.040 --> 1:27:22.240
 and debug these UIs.

1:27:22.240 --> 1:27:24.480
 QT people and QT.

1:27:24.480 --> 1:27:26.320
 Is that what the UI has done with this QT?

1:27:26.320 --> 1:27:27.760
 Moving the new UIs and QT.

1:27:29.120 --> 1:27:30.160
 C++ QT.

1:27:31.840 --> 1:27:32.560
 Tesla uses it too.

1:27:33.200 --> 1:27:33.440
 Yeah.

1:27:33.440 --> 1:27:34.000
 Yeah.

1:27:34.000 --> 1:27:35.680
 We had some React stuff in there.

1:27:37.760 --> 1:27:39.280
 React.js or just React.

1:27:39.280 --> 1:27:40.960
 React has its own language, right?

1:27:40.960 --> 1:27:41.760
 React Native.

1:27:41.760 --> 1:27:42.560
 React Native.

1:27:42.560 --> 1:27:44.000
 React is a JavaScript framework.

1:27:45.040 --> 1:27:46.480
 It's all based on JavaScript.

1:27:48.720 --> 1:27:49.680
 I like C++.

1:27:51.280 --> 1:27:54.960
 What do you think about Dojo with Tesla

1:27:54.960 --> 1:27:58.160
 and their foray into what appears to be

1:27:58.160 --> 1:28:02.880
 specialized hardware for training on that?

1:28:04.960 --> 1:28:07.200
 I guess it's something maybe you can correct me

1:28:07.200 --> 1:28:09.760
 for my shallow looking at it.

1:28:09.760 --> 1:28:11.920
 It seems like something that Google did with TPUs

1:28:11.920 --> 1:28:15.440
 but specialized for driving data.

1:28:15.440 --> 1:28:17.040
 I don't think it's specialized for driving data.

1:28:18.240 --> 1:28:19.920
 It's just legit, just TPU.

1:28:19.920 --> 1:28:22.000
 They want to go the Apple way.

1:28:22.000 --> 1:28:25.520
 Basically everything required in the chain is done in house.

1:28:25.520 --> 1:28:27.680
 Well, so you have a problem right now

1:28:27.680 --> 1:28:31.600
 and this is one of my concerns.

1:28:31.600 --> 1:28:33.680
 I really would like to see somebody deal with this

1:28:33.680 --> 1:28:35.120
 if anyone out there is doing it.

1:28:35.120 --> 1:28:36.800
 I'd like to help them if I can.

1:28:37.840 --> 1:28:40.320
 You basically have two options right now to train.

1:28:41.680 --> 1:28:43.520
 Your options are Nvidia or Google.

1:28:45.760 --> 1:28:48.320
 So Google is not even an option.

1:28:50.000 --> 1:28:52.240
 Their TPUs are only available in Google Cloud.

1:28:52.240 --> 1:28:56.960
 Google has absolutely onerous terms of service restrictions.

1:28:56.960 --> 1:29:00.400
 They may have changed it but back in Google's terms of service

1:29:00.400 --> 1:29:03.600
 it said explicitly you are not allowed to use Google Cloud ML

1:29:03.600 --> 1:29:05.120
 for training autonomous vehicles

1:29:05.120 --> 1:29:07.120
 or for doing anything that competes with Google

1:29:07.120 --> 1:29:08.720
 without Google's prior written permission.

1:29:10.080 --> 1:29:12.080
 I mean Google is not a platform company.

1:29:13.920 --> 1:29:16.560
 I wouldn't touch TPUs with a 10 foot pole.

1:29:16.560 --> 1:29:18.320
 So that leaves you with the monopoly.

1:29:19.040 --> 1:29:19.600
 In video.

1:29:19.600 --> 1:29:20.160
 In video.

1:29:20.160 --> 1:29:21.920
 Yeah. So I mean.

1:29:21.920 --> 1:29:23.200
 That you're not a fan of.

1:29:23.840 --> 1:29:28.400
 Well look I was a huge fan of in 2016 Nvidia.

1:29:28.400 --> 1:29:29.680
 Jensen came sat in the car.

1:29:31.760 --> 1:29:34.240
 Cool guy when the stock was $30 a share.

1:29:35.280 --> 1:29:37.040
 Nvidia stock has skyrocketed.

1:29:38.080 --> 1:29:40.960
 I witnessed a real change and who was in management

1:29:40.960 --> 1:29:42.560
 over there in like 2018.

1:29:43.440 --> 1:29:46.560
 And now they are let's exploit.

1:29:46.560 --> 1:29:49.440
 Let's take every dollar we possibly can out of this ecosystem.

1:29:49.440 --> 1:29:51.600
 Let's charge $10,000 for A100s

1:29:51.600 --> 1:29:53.120
 because we know we got the best shit in the game.

1:29:54.080 --> 1:29:57.760
 And let's charge $10,000 for an A100

1:29:57.760 --> 1:30:00.000
 when it's really not that different from a 3080

1:30:00.000 --> 1:30:01.200
 which is $699.

1:30:03.360 --> 1:30:06.080
 The margins that they are making off of those high end chips

1:30:06.080 --> 1:30:10.080
 are so high that I mean I think they're shooting themselves

1:30:10.080 --> 1:30:12.000
 in the foot just from a business perspective

1:30:12.000 --> 1:30:14.080
 because there's a lot of people talking like me now

1:30:14.800 --> 1:30:16.800
 who are like somebody's got to take Nvidia down.

1:30:16.800 --> 1:30:20.880
 Yeah where they could dominate.

1:30:20.880 --> 1:30:22.320
 Nvidia could be the new Intel.

1:30:22.320 --> 1:30:29.040
 Yeah to be inside everything essentially and yet the winners

1:30:29.040 --> 1:30:32.560
 in certain spaces like in autonomous driving the winners

1:30:33.600 --> 1:30:36.400
 only the people who are like desperately falling back

1:30:36.400 --> 1:30:38.400
 and trying to catch up and have a ton of money

1:30:38.400 --> 1:30:40.960
 like the big automakers are the ones interested

1:30:40.960 --> 1:30:42.160
 in partnering with Nvidia.

1:30:42.960 --> 1:30:45.760
 Oh and I think a lot of those things are going to fall through.

1:30:45.760 --> 1:30:48.160
 If I were Nvidia sell chips.

1:30:49.280 --> 1:30:51.280
 Sell chips at a reasonable markup.

1:30:52.080 --> 1:30:52.800
 To everybody.

1:30:52.800 --> 1:30:53.440
 To everybody.

1:30:53.440 --> 1:30:54.720
 Without any restrictions.

1:30:54.720 --> 1:30:56.000
 Without any restrictions.

1:30:56.000 --> 1:30:57.120
 Intel did this.

1:30:57.120 --> 1:30:58.080
 Look at Intel.

1:30:58.080 --> 1:30:59.760
 They had a great long run.

1:30:59.760 --> 1:31:02.240
 Nvidia is trying to turn their they're like trying to

1:31:02.240 --> 1:31:05.440
 productize their chips way too much.

1:31:05.440 --> 1:31:07.680
 They're trying to extract way more value

1:31:07.680 --> 1:31:09.280
 than they can sustainably.

1:31:09.280 --> 1:31:10.560
 Sure you can do it tomorrow.

1:31:10.560 --> 1:31:12.080
 Is it going to up your share price?

1:31:12.080 --> 1:31:14.400
 Sure if you're one of those CEOs is like how much can I strip

1:31:14.400 --> 1:31:17.760
 mine this company and you know and that's what's weird about it too.

1:31:17.760 --> 1:31:19.200
 Like the CEO is the founder.

1:31:19.200 --> 1:31:20.000
 It's the same guy.

1:31:20.000 --> 1:31:20.320
 Yeah.

1:31:20.320 --> 1:31:22.240
 I mean I still think Jensen's a great guy.

1:31:22.240 --> 1:31:23.280
 He is great.

1:31:23.280 --> 1:31:24.560
 Why do this?

1:31:25.120 --> 1:31:26.080
 You have a choice.

1:31:26.800 --> 1:31:27.840
 You have a choice right now.

1:31:27.840 --> 1:31:28.720
 Are you trying to cash out?

1:31:28.720 --> 1:31:29.600
 Are you trying to buy a yacht?

1:31:30.560 --> 1:31:31.440
 If you are fine.

1:31:32.000 --> 1:31:36.080
 But if you're trying to be the next huge semiconductor company

1:31:36.080 --> 1:31:37.120
 sell chips.

1:31:37.120 --> 1:31:39.120
 Well the interesting thing about Jensen

1:31:40.080 --> 1:31:42.000
 is he is a big vision guy.

1:31:42.000 --> 1:31:47.600
 So he has a plan like for 50 years down the road.

1:31:48.640 --> 1:31:50.400
 So it makes me wonder like.

1:31:50.400 --> 1:31:51.760
 How does price gouging fit into it?

1:31:51.760 --> 1:31:56.240
 Yeah how does that like it's it doesn't seem to make sense as a plan.

1:31:56.960 --> 1:31:59.200
 I worry that he's listening to the wrong people.

1:31:59.200 --> 1:32:02.880
 Yeah that that's the sense I have too sometimes because I

1:32:04.000 --> 1:32:08.880
 despite everything I think NVIDIA is an incredible company.

1:32:08.880 --> 1:32:14.160
 Well one I'm deeply grateful to NVIDIA for the products they've created in the past.

1:32:14.160 --> 1:32:14.160
 Me too.

1:32:14.160 --> 1:32:14.720
 Right.

1:32:14.720 --> 1:32:15.120
 And so.

1:32:16.000 --> 1:32:17.840
 The 1080Ti was a great GPU.

1:32:17.840 --> 1:32:18.800
 Still have a lot of them.

1:32:18.800 --> 1:32:19.840
 Still is yeah.

1:32:21.760 --> 1:32:24.000
 But at the same time it just feels like.

1:32:26.720 --> 1:32:29.360
 Feels like you don't want to put all your stock in NVIDIA.

1:32:29.360 --> 1:32:34.080
 And so the Elon is doing what Tesla is doing with autopilot and Dojo

1:32:34.880 --> 1:32:38.720
 is the Apple way is because they're not going to share Dojo.

1:32:38.720 --> 1:32:40.000
 With George Hott's.

1:32:41.600 --> 1:32:43.600
 I know they should sell that chip.

1:32:43.600 --> 1:32:46.160
 Oh they should sell that even their their accelerator.

1:32:46.160 --> 1:32:48.320
 The accelerator that's in all the cars the 30 watt one.

1:32:49.040 --> 1:32:50.240
 Sell it why not.

1:32:51.440 --> 1:32:52.560
 So open it up.

1:32:52.560 --> 1:32:55.120
 Make me why does this has to be a car company.

1:32:55.680 --> 1:32:57.920
 Well if you sell the chip here's what you get.

1:32:57.920 --> 1:32:58.160
 Yeah.

1:32:58.800 --> 1:33:00.000
 Makes the money all the chips.

1:33:00.000 --> 1:33:01.840
 It doesn't take away from your chip.

1:33:01.840 --> 1:33:03.760
 You're going to make some money free money.

1:33:03.760 --> 1:33:08.880
 And also the world is going to build an ecosystem of tooling for you.

1:33:08.880 --> 1:33:09.120
 Right.

1:33:09.120 --> 1:33:12.640
 You're not going to have to fix the bug in your 10H layer.

1:33:12.640 --> 1:33:13.600
 Someone else already did.

1:33:14.960 --> 1:33:16.560
 Well the question that's an interesting question.

1:33:16.560 --> 1:33:18.560
 I mean that's the question Steve Jobs asked.

1:33:18.560 --> 1:33:23.200
 That's the question Elon Musk is perhaps asking is

1:33:24.960 --> 1:33:27.360
 do you want Tesla stuff inside other vehicles

1:33:27.920 --> 1:33:32.480
 in inside potentially inside like iRobot Vacuum Cleaner.

1:33:32.480 --> 1:33:33.840
 Yeah.

1:33:34.640 --> 1:33:37.120
 I think you should decide where your advantages are.

1:33:37.120 --> 1:33:40.240
 I'm not saying Tesla should start selling battery packs to to automakers

1:33:40.240 --> 1:33:43.520
 because battery packs to automakers they're straight up in competition with you.

1:33:43.520 --> 1:33:45.920
 If I were Tesla I'd keep the battery technology totally.

1:33:45.920 --> 1:33:46.160
 Yeah.

1:33:46.160 --> 1:33:47.840
 As far as we make batteries.

1:33:47.840 --> 1:33:53.040
 But the thing about the Tesla TPU is anybody can build that.

1:33:53.040 --> 1:33:57.280
 It's just a question of you know are you willing to spend the you know the money.

1:33:57.280 --> 1:33:59.440
 It could be a huge source of revenue potentially.

1:34:00.000 --> 1:34:01.280
 Are you willing to spend 100 million dollars.

1:34:01.280 --> 1:34:02.240
 Right.

1:34:02.240 --> 1:34:03.040
 Anyone can build it.

1:34:03.600 --> 1:34:04.560
 And someone will.

1:34:04.560 --> 1:34:08.000
 And a bunch of companies now are starting trying to build AI accelerators.

1:34:08.000 --> 1:34:09.360
 Somebody's going to get the idea right.

1:34:10.080 --> 1:34:11.520
 And yeah.

1:34:11.520 --> 1:34:15.680
 Hopefully they don't get greedy because they'll just lose to the next guy who finally

1:34:15.680 --> 1:34:18.320
 and then eventually the Chinese are going to make knockoff and video chips and that's.

1:34:19.440 --> 1:34:23.360
 From your perspective I don't know if you're also paying attention to Stan Tesla for a moment.

1:34:24.080 --> 1:34:27.600
 Dave Elon Musk has talked about a complete rewrite

1:34:27.600 --> 1:34:33.920
 of the neural net that they're using that seems to again I'm half paying attention.

1:34:34.640 --> 1:34:40.320
 But it seems to involve basically a kind of integration of all the sensors to where

1:34:42.000 --> 1:34:47.440
 it's a four dimensional view you know you have a 3D model of the world over time.

1:34:47.440 --> 1:34:53.680
 And then you can I think it's done both for the for the actually you know so the neural

1:34:53.680 --> 1:34:59.120
 network is able to in a more holistic way deal with the world and make predictions and so on.

1:34:59.120 --> 1:35:06.560
 But also to make the annotation task more you know easier like you can annotate the world

1:35:07.120 --> 1:35:11.680
 in one place and they kind of distribute itself across the sensors and across a different

1:35:12.800 --> 1:35:16.400
 like the hundreds of tasks that are involved in the hydranet.

1:35:16.400 --> 1:35:22.240
 What are your thoughts about this rewrite is it just like some details that are kind of obvious

1:35:22.240 --> 1:35:27.120
 that are steps that should be taken or is there something fundamental that could challenge your

1:35:27.120 --> 1:35:32.880
 idea that end to end is the right solution. We're in the middle of a big rewrite now as well

1:35:32.880 --> 1:35:37.440
 we haven't shipped a new model in a bit. Of what kind? We're going from 2D to 3D.

1:35:38.160 --> 1:35:42.160
 Right now all our stuff like for example when the car pitches back the lane lines also pitch back

1:35:42.880 --> 1:35:48.800
 because we're assuming the flat world hypothesis the new models do not do this the new models

1:35:48.800 --> 1:35:55.600
 output everything in 3D. But there's still no annotation so the 3D is it's more about the

1:35:55.600 --> 1:36:06.480
 output. We have Z's in everything. We've had a disease. We unified a lot of stuff as well.

1:36:06.480 --> 1:36:13.840
 We switched from TensorFlow to PyTorch. My understanding of what Tesla's thing is is

1:36:13.840 --> 1:36:22.400
 that their annotator now annotates across the time dimension. I mean cute. Why are you building an

1:36:22.400 --> 1:36:31.600
 annotator? I find their entire pipeline. I find your vision I mean the vision of end to end

1:36:31.600 --> 1:36:36.560
 very compelling but I also like the engineering of the data engine that they've created.

1:36:36.560 --> 1:36:45.120
 In terms of supervised learning pipelines that thing is damn impressive. You're basically the

1:36:45.120 --> 1:36:51.040
 idea is that you have hundreds of thousands of people that are doing data collection for you

1:36:51.040 --> 1:36:57.120
 by doing their experience so that's kind of similar to the Kama AI model and you're able to

1:36:58.240 --> 1:37:05.840
 mine that data based on the kind of education you need. I think it's harder to do in the end

1:37:05.840 --> 1:37:11.840
 to end learning. The mining of the right edge case. That's what feature engineering is actually

1:37:12.400 --> 1:37:18.640
 really powerful because us humans are able to do this kind of mining a little better.

1:37:19.760 --> 1:37:24.480
 But yeah there's obvious constraints and limitations to that idea.

1:37:25.680 --> 1:37:31.680
 Carpathia just tweeted. He's like you get really interesting insights if you sort your validation

1:37:31.680 --> 1:37:40.640
 set by loss and look at the highest loss examples. Yeah. So yeah I mean you can do we have we have

1:37:40.640 --> 1:37:44.960
 a little data engine like thing we're training a segnat anyway it's not fancy it's just like

1:37:45.520 --> 1:37:50.800
 okay train the new segnat run it on a hundred thousand images and now take the thousand with

1:37:50.800 --> 1:37:56.880
 highest loss select a hundred of those by human put those get those ones labeled retrain do it

1:37:56.880 --> 1:38:03.200
 again. So it's a much less well written data engine and yeah you can take these things really

1:38:03.200 --> 1:38:09.760
 far and it is impressive engineering and if you truly need supervised data for a problem

1:38:09.760 --> 1:38:15.840
 yeah things like data engine are the high end of what is attention is a human paying attention.

1:38:15.840 --> 1:38:19.440
 I mean we're going to probably build something that looks like data engine to push our driver

1:38:19.440 --> 1:38:25.120
 monitoring further but for driving itself you have it all annotated beautifully by what the human

1:38:25.120 --> 1:38:30.320
 does so. Yeah that's interesting I mean that applies to driver attention as well. Do you want to

1:38:30.320 --> 1:38:35.040
 detect the eyes do you want to detect blinking and pupil movement do you want to detect all the

1:38:35.040 --> 1:38:41.120
 like a face alignment so landmark detection and so on and then doing kind of reasoning based on

1:38:41.120 --> 1:38:45.680
 that or do you want to take the entirety of the face over time and do end to end. I mean it's

1:38:45.680 --> 1:38:50.320
 obvious that over eventually you have to do end to end with some calibration with some fixes and

1:38:50.320 --> 1:38:58.080
 so on but it's like I don't know when that's the right move. Even if it's end to end there actually

1:38:58.080 --> 1:39:04.960
 is there is no kind of um you have to supervise that with humans. Whether a human is paying attention

1:39:04.960 --> 1:39:10.880
 or not is a completely subjective judgment um like you can try to like automatically do it

1:39:10.880 --> 1:39:17.040
 with some stuff but you don't have if I record a video of a human I don't have true annotations

1:39:17.040 --> 1:39:22.720
 anywhere in that video. The only way to get them is with you know other humans labeling it really.

1:39:22.720 --> 1:39:30.000
 Well I don't know you so if you think deeply about it you could you might be able to just

1:39:30.000 --> 1:39:35.600
 depending on the task you may be a discover self annotating things like you know you can look at

1:39:35.600 --> 1:39:39.760
 like steering wheel reverse or something like that. You can discover little moments of lapse of

1:39:39.760 --> 1:39:45.600
 attention. Yeah I mean that's that's where psychology comes in is there indicate because you

1:39:45.600 --> 1:39:51.280
 have so so much data to look at so you might be able to find moments when there's like just

1:39:52.320 --> 1:39:57.440
 inattention that even with smartphone if you want to text smartphone use yeah you can start to zoom

1:39:57.440 --> 1:40:03.200
 in I mean that's the goldmine a sort of the comma AI I mean Tesla's doing this too right is there

1:40:03.200 --> 1:40:11.440
 they're doing annotation based on it's like uh self supervised learning too it's just a small part

1:40:11.440 --> 1:40:18.560
 of the entire picture it's that's kind of the challenge of solving a problem in machine learning

1:40:18.560 --> 1:40:26.080
 if you can discover self annotating parts of the problem right. Our driver monitoring team is

1:40:26.080 --> 1:40:31.120
 half a person right now half a person you know once we have skill to a full once we have two people

1:40:31.120 --> 1:40:35.120
 once we have two three people on that team I definitely want to look at self annotating stuff

1:40:35.120 --> 1:40:44.640
 for attention. Let's go back for a sec to to a comma and what you know for people who are curious

1:40:44.640 --> 1:40:52.480
 to try it out how do you install a comma in say a 2022 or a Corolla or like what are the cars that

1:40:52.480 --> 1:40:58.720
 are supported what are the cars that you recommend and what does it take you have a few videos out

1:40:58.720 --> 1:41:02.720
 but maybe through words can you explain now what's it take to actually install a thing.

1:41:02.720 --> 1:41:09.040
 So we support I think it's 91 cars 91 makes models you get to 100 this year.

1:41:10.000 --> 1:41:21.040
 Nice. The yeah the 2020 Corolla great choice the 2020 Sonata it's using the stock longitudinal

1:41:21.040 --> 1:41:26.480
 it's using just our lateral control but it's a very refined car their longitudinal control is not

1:41:26.480 --> 1:41:34.160
 bad at all. So yeah Corolla Sonata or if you're willing to get your hands a little dirty and

1:41:34.160 --> 1:41:38.880
 look in the right places on the internet the Honda Civic is great but you're going to have to install

1:41:38.880 --> 1:41:43.200
 a modified EPS firmware in order to get a little bit more torque and I can't help you with that

1:41:43.200 --> 1:41:48.560
 comma does not efficiently endorse that but we have been doing it we didn't ever release it

1:41:49.520 --> 1:41:54.320
 we waited for someone else to discover it and then you know. And you have a discord server

1:41:54.320 --> 1:42:02.240
 where people there's a very active developer community I suppose so depending on the level

1:42:02.240 --> 1:42:09.200
 of experimentation you're willing to do that's a community. If you if you just want to buy it and

1:42:09.200 --> 1:42:16.160
 you have a supported car yeah it's 10 minutes to install there's YouTube videos it's Ikea

1:42:16.160 --> 1:42:20.640
 furniture level if you can set up a table from Ikea you can install a comma two in your supported

1:42:20.640 --> 1:42:25.680
 car and it will just work now you're like oh but I want this high end feature or I want to fix this

1:42:25.680 --> 1:42:31.760
 bug okay well welcome to the developer community. So what if I wanted to this is something I asked

1:42:31.760 --> 1:42:43.280
 you I'll find like a few months ago if I wanted to run my own code to so use comma as a platform

1:42:43.280 --> 1:42:49.520
 and try to run something like open pilot what does it take to do that? So there's a toggle in the

1:42:49.520 --> 1:42:55.200
 settings called enable ssh and if you toggle that you can ssh into your device you can modify the

1:42:55.200 --> 1:43:00.960
 code you can upload whatever code you want to it. There's a whole lot of people so about 60% of people

1:43:00.960 --> 1:43:07.360
 are running stock comma about 40% of people are running forks and there's a community of there's

1:43:07.360 --> 1:43:13.440
 a bunch of people who maintain these forks and these forks support different cars or they have

1:43:14.080 --> 1:43:17.920
 you know different toggles we try to keep away from the toggles that are like disabled

1:43:17.920 --> 1:43:22.160
 driver monitoring but you know there's some people might want that kind of thing and like you know

1:43:22.160 --> 1:43:30.880
 yeah you can it's your car it's your I'm not here to tell you you know we have some you know we ban

1:43:30.880 --> 1:43:34.240
 if you're trying to subvert safety features you're banned from our discord I don't want anything to

1:43:34.240 --> 1:43:42.720
 do with you but there's some forks doing that. Got it. So you encourage responsible forking.

1:43:42.720 --> 1:43:48.000
 Yeah yeah we encourage some people you know yeah some people like like there's forks that will do

1:43:48.000 --> 1:43:53.360
 some people just like having a lot of readouts on the UI like a lot of like flashing numbers so

1:43:53.360 --> 1:43:57.440
 there's forks that do that. Some people don't like the fact that it disengages when you press

1:43:57.440 --> 1:44:03.920
 the gas pedal there's forks that disable that. Got it. Now the the stock experience is is what

1:44:03.920 --> 1:44:09.760
 like so it does both lane keeping and longitudinal control all together so it's not separate like

1:44:09.760 --> 1:44:15.280
 it is an autopilot. No so okay some cars we use the stock longitudinal control we don't do the

1:44:15.280 --> 1:44:19.680
 longitudinal control in all the cars. Some cars the ACC's are pretty good in the cars it's the

1:44:19.680 --> 1:44:24.800
 lane keep that's atrocious in anything except for autopilot and supercruise. But you know you just

1:44:24.800 --> 1:44:31.600
 turn it on and it works what does this engagement look like? Yeah so we have I mean I'm very concerned

1:44:31.600 --> 1:44:38.320
 about mode confusion I've experienced it on supercruise and autopilot where like autopilot like

1:44:38.320 --> 1:44:44.560
 autopilot disengages I don't realize that the ACC is still on the lead car moves slightly over and

1:44:44.560 --> 1:44:49.520
 then the Tesla accelerates to like whatever my set speed is super fast and like what's going on here.

1:44:51.200 --> 1:44:56.640
 We have engaged and disengaged and this is similar to my understanding I'm not a pilot but my

1:44:56.640 --> 1:45:03.280
 understanding is either the pilot is in control or the copilot is in control and we have the same

1:45:03.280 --> 1:45:09.040
 kind of transition system either open pilot is engaged or open pilot is disengaged engage with

1:45:09.040 --> 1:45:15.920
 cruise control disengage with either gas break or cancel. Let's talk about money what's the business

1:45:15.920 --> 1:45:25.040
 strategy for comma profitable well it's your you did it congratulations what so it's basically

1:45:25.040 --> 1:45:30.800
 selling we should say comma cost a thousand bucks comma two two hundred for the interface

1:45:30.800 --> 1:45:37.280
 to the car as well it's 1200 I'll send that nobody's usually up front like this you gotta add the

1:45:37.280 --> 1:45:43.360
 tack on right yeah I love it this I'm not gonna lie to you trust me it will add 1200 a value to

1:45:43.360 --> 1:45:48.240
 your life yes it's still super cheap 30 days no questions asked money back guarantee and

1:45:48.240 --> 1:45:53.200
 prices are only going up you know if there ever is future hardware it costs a lot more than 1200

1:45:53.200 --> 1:45:59.600
 dollars so comma three is in the works so it could be all I all I will say is future hardware is

1:45:59.600 --> 1:46:05.680
 going to cost a lot more than the current hardware yeah like the people that use the people have

1:46:05.680 --> 1:46:12.640
 spoken with that use comma they use open pilot they first of all they use it a lot so people that

1:46:12.640 --> 1:46:18.240
 use it they they fall in love with oh our retention rate is insane there's a good sign yeah it's a

1:46:18.240 --> 1:46:24.960
 really good sign um 70 percent of comma two buyers are daily active users yeah it's amazing

1:46:24.960 --> 1:46:32.800
 um oh also we don't plan on stopping selling the comma two like like it's you know so whatever

1:46:32.800 --> 1:46:40.640
 you create that's beyond comma two it would be uh it would be potentially a phase shift

1:46:40.640 --> 1:46:45.520
 like it's it's so much better that like you could use comma two and you can use comma whatever

1:46:45.520 --> 1:46:51.840
 depends what you want it's kind of 41 42 yeah you know autopilot hardware one versus hardware two

1:46:51.840 --> 1:46:55.520
 the comma two is kind of like hardware one got it got it you can still use both got it got it

1:46:56.160 --> 1:47:00.640
 I think I heard you talk about retention rate with uh BR headsets that the average is just

1:47:00.640 --> 1:47:05.040
 once yeah just fast I mean it's such a fascinating way to think about technology

1:47:05.680 --> 1:47:09.120
 and this is a really really good sign and the other thing that people say about comma is like

1:47:09.120 --> 1:47:14.560
 they can't believe they're getting this four thousand bucks right it's it seems it seems like

1:47:14.560 --> 1:47:21.680
 some kind of steal so but in terms of like long term business strategies that basically to put so

1:47:21.680 --> 1:47:33.680
 it's currently in like a thousand plus cars uh 1200 more uh so yeah dailies is about uh

1:47:34.640 --> 1:47:40.320
 dailies is about 2000 weekly is about 2500 monthly is over 3000 wow we've grown a lot since

1:47:40.880 --> 1:47:46.880
 we last talked is the goal that can we talk crazy for a second I mean what's the the goal to overtake

1:47:46.880 --> 1:47:52.640
 tesla let's talk okay so I mean android did overtake ira that's exactly it right so

1:47:54.560 --> 1:48:00.560
 they did it I actually don't know the timeline of that one they but let let let's talk uh because

1:48:00.560 --> 1:48:05.360
 everything is in alpha now the autopilot you could argue is in alpha in terms of towards the big

1:48:05.360 --> 1:48:12.000
 mission of autonomous driving right and so what yeah it's your goal to overtake to get millions of

1:48:12.000 --> 1:48:18.560
 cars essentially of course where would it stop like it's open source software it might not be

1:48:18.560 --> 1:48:24.400
 millions of cars with a piece of comma hardware but yeah I think open pilot at some point will

1:48:24.400 --> 1:48:30.320
 cross over autopilot in in in users just like android crossed over ios how does google make money

1:48:30.320 --> 1:48:39.440
 from android uh it's it's complicated their own devices make money google google makes money by

1:48:39.440 --> 1:48:45.760
 just kind of having you on the internet uh yes google search is built in gmail is built in android

1:48:45.760 --> 1:48:51.120
 is just a shield for the rest of google's ecosystem kind yeah but the problem is android is not is a

1:48:51.120 --> 1:48:58.960
 brilliant thing i mean android arguably changed the world so there you go that's you can you can

1:48:58.960 --> 1:49:05.840
 feel good ethically speaking but as a business strategy it's questionable oh so hardware so

1:49:05.840 --> 1:49:08.960
 hardware i mean it took google a long time to come around to it but they are now making money on the

1:49:08.960 --> 1:49:16.320
 pixel you're not about money you're more about winning yeah but if only if only 10 percent of

1:49:16.320 --> 1:49:21.920
 open pilot devices come from comma ai we still make a lot that is still yes that is a ton of

1:49:21.920 --> 1:49:27.680
 money for our company but can't somebody create a better comma using open pilot or you're basically

1:49:27.680 --> 1:49:31.280
 saying we'll outcompete them well i'll compete you is can you create a better android phone than

1:49:31.280 --> 1:49:37.280
 the google pixel right i mean you can but like i love that so you're confident like you know what

1:49:37.280 --> 1:49:44.320
 the hell you're doing yeah it's it's uh uh competence and merit i mean our money our money

1:49:44.320 --> 1:49:48.880
 comes from we're consumer electronics company yeah and put it this way so we sold we sold like

1:49:48.880 --> 1:50:00.560
 3 000 comma twos um i mean 2500 right now uh and like okay we're probably gonna sell 10 000 units

1:50:00.560 --> 1:50:05.120
 next year right 10 000 units and even just a thousand dollars a unit okay we're at 10 million

1:50:05.760 --> 1:50:12.080
 in uh in in revenue um get that up to a hundred thousand maybe double the price of the unit now

1:50:12.080 --> 1:50:18.080
 we're talking like 200 million revenue yeah actually making money one of the rare semi autonomous

1:50:18.080 --> 1:50:24.320
 autonomous vehicle companies that are actually making money yeah yeah you know if you have if you

1:50:24.320 --> 1:50:27.680
 look at a model when we were just talking about this yesterday if you look at a model and like

1:50:27.680 --> 1:50:32.240
 you're testing like your ab testing your model and if your your your one branch of the ab test

1:50:32.240 --> 1:50:37.440
 the losses go down very fast in the first five epochs yeah that model is probably going to converge

1:50:37.440 --> 1:50:41.840
 to something considerably better than the one with the losses going down slower why do people

1:50:41.840 --> 1:50:45.680
 think this is going to stop why do people think one day there's going to be a great like well

1:50:45.680 --> 1:50:53.680
 waymo's eventually going to surpass you guys oh they're not do you see like a world where like

1:50:53.680 --> 1:51:00.080
 a tesla or a car like a tesla would be able to basically press a button and you like switch

1:51:00.080 --> 1:51:06.320
 to open pilot you know you you know they've load in i don't know so i think so first off

1:51:06.320 --> 1:51:12.160
 i think that we may surpass tesla in terms of users i do not think we're going to surpass tesla

1:51:12.160 --> 1:51:17.920
 ever in terms of revenue i think tesla can capture a lot more revenue per user than we can but this

1:51:17.920 --> 1:51:22.560
 mimics the android ios model exactly there may be more android devices but you know there's a

1:51:22.560 --> 1:51:26.720
 lot more iphones than google pixels so i think there'll be a lot more tesla cars sold than pieces

1:51:26.720 --> 1:51:36.800
 of comma hardware um and then as far as a tesla owner being able to switch to open pilot uh does

1:51:36.800 --> 1:51:44.080
 ios does iphones run android no but you can if you really want to do it but it doesn't really make

1:51:44.080 --> 1:51:49.280
 sense like it's not it doesn't make sense who cares what about if uh a large company like

1:51:49.280 --> 1:51:57.040
 automakers for GM Toyota came to george hots or on the tech space amazon facebook google

1:51:57.920 --> 1:52:05.120
 came with a large pile of cash uh would would you consider being um purchased

1:52:07.360 --> 1:52:15.760
 what did you see that as a one possible not seriously no um i would probably uh see how much

1:52:15.760 --> 1:52:21.840
 uh shit they'll entertain for me um and if they're willing to like jump through a bunch of my hoops

1:52:21.840 --> 1:52:27.040
 then maybe but like no not the way that mna works today i mean we've been approached and i laugh in

1:52:27.040 --> 1:52:32.880
 these people's faces i'm like are you kidding yeah you know because it's so it's so it's so

1:52:32.880 --> 1:52:39.680
 demeaning the mna people are so demeaning to companies they treat the startup world as their

1:52:39.680 --> 1:52:43.920
 innovation ecosystem and they think that i'm cool with going along with that so i can have some of

1:52:43.920 --> 1:52:49.360
 their scam fake fed dollars you know fed coin i'm what am i gonna do with more fed coin you know

1:52:49.360 --> 1:52:55.280
 head coin fed coin man i love that so that's the cool thing about podcasting actually is uh

1:52:55.280 --> 1:53:00.720
 people criticize i don't know if you're familiar with uh less spotify uh giving joe rogan a hundred

1:53:00.720 --> 1:53:08.480
 million i'd talk about that and you know they respect despite all the shit that people are

1:53:08.480 --> 1:53:16.000
 talking about spotify people understand that podcasters like joe rogan know what the hell they're

1:53:16.000 --> 1:53:24.320
 doing yeah so they give them money and say just do what you do and like the equivalent for you

1:53:24.320 --> 1:53:29.360
 would be like george do what the hell you do because you're good at it try not to murder

1:53:29.360 --> 1:53:34.960
 too many people like try like there's some kind of common sense things like just don't go on a weird

1:53:34.960 --> 1:53:44.000
 rampage of yeah it comes down to what companies i could respect right um you know could i respect

1:53:44.000 --> 1:53:52.960
 gm never um no i couldn't i mean could i respect like a hundai more some right that's that's a

1:53:52.960 --> 1:54:00.160
 lot closer to yoda what's your nah no it's like the korean is the way i think i think that you

1:54:00.160 --> 1:54:04.560
 know the japanese the germans the u.s they're all too they're all too you know they all think

1:54:04.560 --> 1:54:10.480
 they're too great to be about the tech companies apple apple is of the tech companies that i could

1:54:10.480 --> 1:54:16.400
 respect apples the closest yeah i mean i could never should be ironic would be ironic oh if

1:54:16.400 --> 1:54:21.760
 if kama ai is uh is acquired by apple i mean facebook look i quit facebook 10 years ago

1:54:21.760 --> 1:54:27.680
 because i didn't respect the business model um google has declined so fast in the last five years

1:54:27.680 --> 1:54:34.880
 what are your thoughts about wemo and its present and its future is let me let me say let me start

1:54:34.880 --> 1:54:43.600
 by saying something uh nice which is uh i've visited them a few times and i've uh have written

1:54:43.600 --> 1:54:51.600
 in their cars and the engineering that they're doing both the research and the actual development

1:54:51.600 --> 1:54:56.240
 and the engineering they're doing and the scale they're actually achieving by doing it all themselves

1:54:56.240 --> 1:55:03.120
 is really impressive and the the balance of safety and innovation and like the cars work

1:55:04.480 --> 1:55:10.800
 really well for the routes they drive like they drive fast which was very surprising to me like

1:55:10.800 --> 1:55:17.600
 it drives like the speed limit or faster the speed limit is it goes and it works really damn well

1:55:17.600 --> 1:55:21.840
 and the interface is nice and channel or zone yeah yeah and channel there's a very specific

1:55:21.840 --> 1:55:28.960
 environment so it i you know it gives me enough material in my mind to push back against the

1:55:28.960 --> 1:55:37.040
 madman of the world like george hotz to be like because you kind of imply there's zero probability

1:55:37.040 --> 1:55:44.560
 they're going to win yeah and after i've used after i've written in it to me it's not zero oh

1:55:44.560 --> 1:55:50.320
 it's not for technology reasons bureaucracy no it's worse than that it's actually for product

1:55:50.320 --> 1:55:56.000
 reasons i think oh you think they're just not capable of creating an amazing product uh no i

1:55:56.000 --> 1:56:03.440
 think that the product that they're building doesn't make sense um so a few things uh you

1:56:03.440 --> 1:56:10.160
 say the waymos are fast um benchmark a waymo against a competent uber driver right the uber

1:56:10.160 --> 1:56:14.560
 driver is faster it's not even about speed it's the thing you said it's about the experience of

1:56:14.560 --> 1:56:20.880
 being stuck at a stop sign because pedestrians are crossing nonstop that i like when my uber

1:56:20.880 --> 1:56:28.320
 driver doesn't come to a full stop at the stop sign yeah you know and so let's say the waymos

1:56:28.320 --> 1:56:34.240
 are 20 slower than than an uber right um you can argue that they're going to be cheaper

1:56:34.880 --> 1:56:40.160
 and i argue that users already have the choice to trade off money for speed it's called uber pool

1:56:40.160 --> 1:56:47.920
 um i think it's like 15 percent of rides at uber pools right users are not willing to trade off

1:56:47.920 --> 1:56:53.840
 money for speed so the whole product that they're building is not going to be competitive

1:56:54.640 --> 1:57:04.240
 with traditional ride sharing networks right um like and also whether there's profit to be made

1:57:04.240 --> 1:57:11.120
 depends entirely on one company having a monopoly i think that the level for autonomous ride sharing

1:57:11.120 --> 1:57:16.400
 vehicles market is going to look a lot like the scooter market if even the technology does come

1:57:16.400 --> 1:57:22.160
 to exist which i question who's doing well in that market yeah it's a race to the bottom you know

1:57:22.160 --> 1:57:27.840
 well they could be it could be closer like an uber in a lift where it's just a one or two players

1:57:27.840 --> 1:57:34.720
 well the scooter people have given up trying to market scooters as a practical means of

1:57:34.720 --> 1:57:39.040
 transportation and they're just like they're super fun to ride look at wheels i love those things and

1:57:39.040 --> 1:57:44.480
 they're great on that front yeah but from an actual transportation product perspective

1:57:44.480 --> 1:57:48.240
 i do not think scooters are viable and i do not think level four autonomous cars are viable

1:57:49.040 --> 1:57:56.800
 if you uh let's play a fun experiment if you ran let's do uh tesla and let's do waymo

1:57:56.800 --> 1:58:03.760
 if uh ilan musk took a vacation for a year he just said screw it i'm gonna go live in an island

1:58:03.760 --> 1:58:08.320
 no electronics and the board decides that we need to find somebody to run the company

1:58:09.040 --> 1:58:13.840
 and they they decide that you should run the company for a year how do you run tesla differently

1:58:14.720 --> 1:58:19.520
 i wouldn't change much do you think they're on the right track i wouldn't change i mean i'd have

1:58:19.520 --> 1:58:26.720
 some minor changes but even even my debate with tesla about you know end to end versus segnets

1:58:27.440 --> 1:58:33.120
 like that's just software who cares right like it's not gonna it's not like you're doing something

1:58:33.120 --> 1:58:36.640
 terrible with segnets you're probably building something that's at least going to help you debug

1:58:36.640 --> 1:58:43.280
 the end to end system a lot right it's very easy to transition from what they have to like an end

1:58:43.280 --> 1:58:51.200
 to end kind of thing right uh and then i presume you would uh in the model y or maybe in the model

1:58:51.200 --> 1:58:56.000
 three start adding driver sensing with infrared yes i would add i would add i would add infrared

1:58:56.000 --> 1:59:04.160
 camera infrared lights right away to those cars um and start collecting that data and do all that

1:59:04.160 --> 1:59:08.880
 kind of stuff yeah very much i think they're already kind of doing it it's it's an incredibly minor

1:59:08.880 --> 1:59:12.560
 change if i actually were to have tesla first off i'd be horrified that i wouldn't be able to do a

1:59:12.560 --> 1:59:17.600
 better job as elon and then i would try to you know understand the way he's done things before

1:59:17.600 --> 1:59:23.520
 you would also have to take over his twitter so i don't tweet yeah what's your twitter situation

1:59:23.520 --> 1:59:29.520
 why why why are you so quiet on twitter i mean du comma is like what what's your social network

1:59:29.520 --> 1:59:36.160
 presence like because you on instagram you're you uh you do live streams you're you're you're um

1:59:36.720 --> 1:59:42.000
 you understand the music of the internet but you don't always fully engage into it you're

1:59:42.000 --> 1:59:47.520
 part time why you still have a twitter yeah i mean it's the instagram is a pretty place

1:59:47.520 --> 1:59:52.640
 instagram is a beautiful place it glorifies beauty i like i like instagram's values as a network

1:59:52.640 --> 1:59:58.560
 got um twitter glorifies conflict quarter glorifies you know like like like like like

1:59:58.560 --> 2:00:02.000
 you know just shots taking shots of people and it's like you know

2:00:03.520 --> 2:00:10.400
 you know twitter and donald trump are perfectly perfect for each other so teslas on uh teslas

2:00:10.400 --> 2:00:16.800
 on the right track in your view yeah okay so let's try let's like really try this experiment if you

2:00:16.800 --> 2:00:22.640
 ran waymo let's say they're i don't know if you agree but they seem to be at the head of the pack of

2:00:22.640 --> 2:00:29.040
 the kind of uh what would you call that approach like it's not necessarily lighter based because

2:00:29.040 --> 2:00:34.880
 it's not about lighter level four robotaxi level four robotaxi all in before any before making

2:00:34.880 --> 2:00:42.640
 any revenue uh so they're probably at the head of the pack if you were said uh hey george can

2:00:42.640 --> 2:00:48.320
 you please run this company for a year how would you change it uh i would go i would get anthony

2:00:48.320 --> 2:00:57.520
 levandowski out of jail and i would put him in charge of the company um let's try to break

2:00:57.520 --> 2:01:02.320
 that apart one do you want to make you want to destroy the company by doing that or do you mean

2:01:02.320 --> 2:01:11.280
 or do you mean uh you like renegade style thinking that pushes that that like throws away bureaucracy

2:01:11.280 --> 2:01:15.120
 and goes to first principle thinking what what do you mean by that um i think anthony levandowski

2:01:15.120 --> 2:01:20.800
 is a genius and i think he would come up with a much better idea of what to do with waymo than me

2:01:22.240 --> 2:01:27.920
 so you mean that unironically he is a genius oh yes oh absolutely without a doubt i mean

2:01:27.920 --> 2:01:35.920
 i'm not saying there's no shortcomings but in the interactions i've had with him yeah what um he's

2:01:35.920 --> 2:01:40.560
 also willing to take like who knows what he would do with waymo i mean he's also out there like

2:01:40.560 --> 2:01:45.840
 far more out there than i am yeah his big risks yeah what do you make of him i was i was going to

2:01:45.840 --> 2:01:51.840
 talk to him in his pockets and i was going back and forth i'm such a gullible naive human like

2:01:51.840 --> 2:01:57.840
 i see the best in people and i slowly started to realize that there might be some people out

2:01:57.840 --> 2:02:08.400
 there that like have multiple faces to the world they're like deceiving and dishonest i still

2:02:09.520 --> 2:02:14.880
 refuse to like i i just i trust people and i don't care if i get hurt by it but like

2:02:15.600 --> 2:02:19.520
 you know sometimes you have to be a little bit careful especially platform wise and

2:02:19.520 --> 2:02:25.040
 podcast wise what are you what am i supposed to think so you think you think he's a good person

2:02:25.040 --> 2:02:31.920
 oh i don't know i don't really make moral judgments it's difficult to all i mean this

2:02:31.920 --> 2:02:36.080
 about the waymo i actually i mean that whole idea very non ironically about what i would do

2:02:36.080 --> 2:02:40.400
 the problem with putting me in charge of waymo is waymo is already 10 billion dollars in the

2:02:40.400 --> 2:02:46.560
 hall right whatever idea waymo does look comma's profitable comma's raised 8.1 million dollars

2:02:46.560 --> 2:02:50.720
 that's small you know that's small money like i can build a reasonable consumer electronics company

2:02:50.720 --> 2:02:57.040
 and succeed wildly at that and still never be able to pay back waymo's 10 billion so i i think the

2:02:57.040 --> 2:03:02.560
 basic idea with waymo will forget the 10 billion because they have some backing but your basic

2:03:02.560 --> 2:03:07.840
 thing is like what can we do to start making some money well no i mean my bigger idea is like

2:03:07.840 --> 2:03:12.560
 whatever the idea is that's going to save waymo i don't have it it's going to have to be a big

2:03:12.560 --> 2:03:18.320
 risk idea and i cannot think of a better person than anthony levendowski to do it so that is

2:03:18.320 --> 2:03:22.480
 completely what i would do ceo of waymo i would call myself a transitionary ceo

2:03:22.480 --> 2:03:29.760
 do everything i can to fix that situation up yeah uh yeah because i can't i can't do it right like

2:03:29.760 --> 2:03:35.520
 i can't i can't i mean i can talk about how what i really want to do is just apologize for all those

2:03:35.520 --> 2:03:40.880
 corny uh you know ad campaigns and be like here's the real state of the technology yeah that's

2:03:40.880 --> 2:03:46.000
 like i have several criticism i'm a little bit more bullish on waymo than than you seem to be

2:03:46.000 --> 2:03:52.560
 but one criticism i have is it went into corny mode too early like it's still a startup it hasn't

2:03:52.560 --> 2:03:58.800
 delivered on anything so it should be like more renegade and show off the engineering that they're

2:03:58.800 --> 2:04:04.320
 doing which just can be impressive as opposed to doing these weird commercials of like your friendly

2:04:05.360 --> 2:04:09.840
 your friendly car company i mean that's my biggest my biggest snipe at waymo was always

2:04:09.840 --> 2:04:14.480
 that guy's a paid actor that guy's not a waymo user he's a paid actor look here i found his call

2:04:14.480 --> 2:04:20.800
 sheet do kind of like what spacex is doing with the rocket launch is just get put the nerds up front

2:04:20.800 --> 2:04:27.760
 put the engineers up front and just like show failures too just i love i love spacex is yeah

2:04:27.760 --> 2:04:33.040
 yeah the thing they're doing it is right and it just feels like the right but we're all so excited

2:04:33.040 --> 2:04:39.200
 to see them succeed yeah i can't wait to see waymo fail you know like you lie to me i want you to fail

2:04:39.200 --> 2:04:42.160
 you tell me the truth you be honest with me i want you to succeed yeah

2:04:42.160 --> 2:04:51.760
 yeah uh yeah and that requires the uh the renegade ceo right i'm with you i'm with you i still have

2:04:51.760 --> 2:04:58.880
 a little bit of faith in waymo to for for the renegade ceo to step forward but it's not it's not

2:04:58.880 --> 2:05:06.560
 john kraftzak yeah it's uh you can't it's not Chris homestead and i'm those people may be very good

2:05:06.560 --> 2:05:11.920
 at certain things yeah but they're not renegades yeah because these companies are fundamentally

2:05:11.920 --> 2:05:17.520
 even though we're talking about billion dollars all these crazy numbers they're still like early

2:05:17.520 --> 2:05:23.280
 stage startups i mean and i i just i if you are pre revenue and you've raised 10 billion dollars i

2:05:23.280 --> 2:05:28.000
 have no idea like like this just doesn't work you know it's against everything silicon valley where's

2:05:28.000 --> 2:05:34.000
 your minimum viable product you know where's your users was your growth numbers this is

2:05:34.000 --> 2:05:39.360
 traditional silicon valley why do you not apply it to what you think you're too big to fail already

2:05:39.360 --> 2:05:49.040
 like how do you think autonomous driving will change society so the mission is for comma to

2:05:49.040 --> 2:05:55.440
 solve self driving do you have like a vision of the world of how it'll be different

2:05:57.760 --> 2:06:02.080
 is it as simple as a to b transportation or is there like because these are robots

2:06:02.080 --> 2:06:09.520
 it's not about autonomous driving in and of itself it's what the technology enables

2:06:09.520 --> 2:06:15.440
 it's i think it's the coolest applied ai problem i like it because it has a clear path to monetary

2:06:15.440 --> 2:06:24.320
 value um but as far as that being the thing that changes the world i mean no like like there's

2:06:24.320 --> 2:06:27.040
 cute things we're doing in common like who'd have thought you could stick a phone on the

2:06:27.040 --> 2:06:32.640
 windshield and it'll drive um but like really the product that you're building is not something that

2:06:32.640 --> 2:06:37.040
 people were not capable of imagining 50 years ago so no it doesn't change the world on that front

2:06:37.600 --> 2:06:42.320
 could people have imagined the internet 50 years ago only true junior genius visionaries yeah

2:06:42.320 --> 2:06:46.880
 everyone could have imagined autonomous cars 50 years ago it's like a car but i don't drive it

2:06:46.880 --> 2:06:55.040
 see i i have the sense and i told you like i'm my long term dream is robots with which you have

2:06:55.040 --> 2:07:02.240
 deep with whom you have deep connections right and uh there's different trajectories towards that

2:07:03.440 --> 2:07:10.320
 and i've been thinking so been thinking of launching a startup i see autonomous vehicles as a potential

2:07:10.320 --> 2:07:17.520
 trajectory to that that that i'm that's not where the direction i would like to go but i also see

2:07:17.520 --> 2:07:25.840
 tesla or even kama ai like pivoting into into robotics broadly defined that's at some stage

2:07:25.840 --> 2:07:31.520
 in the way like you're mentioning the internet didn't expect let's solve you know what i say

2:07:31.520 --> 2:07:36.000
 a comma about this we could talk about this but let's solve self driving guys first gotta stay

2:07:36.000 --> 2:07:40.160
 focused on the mission don't don't don't you're not too big to fail for however much i think

2:07:40.160 --> 2:07:44.960
 calm is winning like no no no no you're winning when you solve level five self driving cars and

2:07:44.960 --> 2:07:49.200
 until then you haven't won and won and you know again you want to be arrogant in the face of

2:07:49.200 --> 2:07:53.920
 other people great you want to be arrogant in the face of nature you're an idiot right stay

2:07:53.920 --> 2:07:59.440
 mission focused brilliantly put uh like i mentioned thinking of launching a startup i've been considering

2:07:59.440 --> 2:08:06.800
 actually before covid i've been thinking of moving to san francisco oh i wouldn't go there so why is uh

2:08:07.520 --> 2:08:14.800
 well and now i'm thinking about potentially austin and we're in san diego now san diego come here

2:08:14.800 --> 2:08:21.840
 so why what um i mean you're you're such an interesting human you've launched so many successful

2:08:21.840 --> 2:08:31.120
 things what uh why san diego what do you recommend why not san francisco have you thought well so

2:08:31.120 --> 2:08:36.000
 in your case san diego with qualcomm is now dragon i mean that's an amazing combination

2:08:36.880 --> 2:08:41.520
 but that wasn't really why that wasn't the why no i mean qualcomm was an afterthought qualcomm

2:08:41.520 --> 2:08:45.600
 was it was a nice thing to think about it's like you can have a tech company here and a good one

2:08:45.600 --> 2:08:50.640
 i mean you know i like qualcomm but no um well so why san diego better than san francisco why does

2:08:50.640 --> 2:08:55.120
 san francisco suck well so okay so first off we all kind of said like we want to stay in california

2:08:55.120 --> 2:09:01.760
 people like the ocean you know california for for its flaws it's like a lot of the flaws of

2:09:01.760 --> 2:09:05.600
 california are not necessarily california as a whole and they're much more san francisco specific

2:09:05.600 --> 2:09:13.680
 yeah um san francisco so i think first year cities in general have stopped wanting growth uh well you

2:09:13.680 --> 2:09:18.800
 have like in san francisco you know the voting class always votes to not build more houses because

2:09:18.800 --> 2:09:23.360
 they own all the houses and they're like well you know once people have figured out how to vote

2:09:23.360 --> 2:09:29.120
 themselves more money they're going to do it it is so insanely corrupt um it is not balanced at all

2:09:29.120 --> 2:09:35.520
 like political party wise you know it's it's it's a one party city and for all the discussion of

2:09:35.520 --> 2:09:44.560
 diversity yeah it's has it stops lacking real diversity of thought of background of uh approaches

2:09:44.560 --> 2:09:52.960
 the strategies of yeah ideas it's it's kind of a strange place that it's the loudest people about

2:09:52.960 --> 2:09:58.720
 diversity and the biggest lack of diversity well i mean that's that's what they say right it's the

2:09:58.720 --> 2:10:05.120
 projection projection yeah yeah it's interesting and even people in silicon valley tell me that's uh

2:10:06.000 --> 2:10:10.800
 like high up people that everybody is like this is a terrible place it doesn't make i mean and

2:10:10.800 --> 2:10:17.360
 coronavirus is really what killed it yeah uh san francisco was the number one uh exodus during

2:10:17.360 --> 2:10:24.720
 coronavirus we still think san diego's uh is a good place to be yeah yeah i mean we'll see we'll

2:10:24.720 --> 2:10:31.520
 see what happens with california a bit longer term yeah like austin's and austin's an interesting

2:10:31.520 --> 2:10:36.160
 choice i wouldn't i wouldn't i don't really anything bad to say about austin either except for the

2:10:36.160 --> 2:10:40.640
 extreme heat in the summer um which you know but that's like very on the surface right i think as

2:10:40.640 --> 2:10:46.880
 far as like an ecosystem goes it's it's cool i personally love colorado colorado is great uh

2:10:46.880 --> 2:10:53.600
 yeah i mean you have these states that are you know like just way better run um california is you

2:10:53.600 --> 2:11:01.440
 know it's especially san francisco it's on its high horse and like yeah can i ask you for advice to

2:11:01.440 --> 2:11:08.080
 me and to others about what's the take to build a successful startup oh i don't know i haven't

2:11:08.080 --> 2:11:16.320
 done that talk to someone who did that well you've you know uh this is like another book of years

2:11:16.320 --> 2:11:24.000
 i'll buy for sixty seven dollars i suppose uh so there's um one of these days i'll sell out

2:11:24.000 --> 2:11:29.360
 yeah that's right jail breaks are going to be a dollar and books are going to be 67 how i uh how

2:11:29.360 --> 2:11:35.200
 i jail broke the iphone by george hotz that's right how i jail broke the iphone and you can

2:11:35.200 --> 2:11:43.200
 do you can't do 67 in in 21 days that's right that's right oh god okay i can't wait but quite

2:11:43.200 --> 2:11:51.840
 so you haven't introspected you have built a very unique company i mean not not you but you and others

2:11:53.040 --> 2:11:59.520
 but i don't know um there's no there's nothing you haven't introspect you haven't really sat down

2:11:59.520 --> 2:12:05.120
 and thought about like well like if you and i were having a bunch of we're having some beers

2:12:06.080 --> 2:12:11.520
 and you're seeing that i'm depressed and whatever i'm struggling there's no advice you can give

2:12:11.520 --> 2:12:14.880
 oh i mean more beer more beer

2:12:18.400 --> 2:12:25.040
 yeah i think it's all very like situation dependent um here's okay if i can give a generic

2:12:25.040 --> 2:12:32.560
 piece of advice it's the technology always wins the better technology always wins and lying always

2:12:32.560 --> 2:12:41.280
 loses build technology and don't lie i'm with you i agree very much the long run long run

2:12:41.280 --> 2:12:45.440
 sure it's the long run you know what the market can remain irrational longer than you can remain

2:12:45.440 --> 2:12:51.680
 solvent true fact well this is this is an interesting point because i ethically and just as a human

2:12:51.680 --> 2:13:01.280
 believe that um like like hype and smoke and mirrors is not at any stage of the company is a good

2:13:01.280 --> 2:13:08.080
 strategy i mean there's some like you know pr magic kind of like you know you want a new product

2:13:08.080 --> 2:13:12.960
 yeah if there's a call to action if there's like a call to action like buy my new gpu look at it

2:13:12.960 --> 2:13:17.920
 it takes up three slots and it's this big it's huge buy my gpu yeah that's great if you look at

2:13:17.920 --> 2:13:23.840
 you know especially in that in ai space broadly but autonomous vehicles like you can raise a huge

2:13:23.840 --> 2:13:31.200
 amount of money on nothing and the question to me is like i'm against that i'll never be part of

2:13:31.200 --> 2:13:40.560
 that i don't think i hope not willingly not but like is there something to be said to uh

2:13:42.000 --> 2:13:48.480
 essentially lying to raise money like fake it till you make a kind of thing i mean this is

2:13:48.480 --> 2:13:54.000
 billy mcfardle in the fire festival like we all we all experienced uh you know what happens with

2:13:54.000 --> 2:14:00.560
 that no no don't fake it till you make it be honest and hope you make it the whole way the

2:14:00.560 --> 2:14:06.800
 technology wins right the technology wins and like there is i'm not i use like the anti hype you

2:14:06.800 --> 2:14:14.640
 know that's that's a slava kpss reference but um hype isn't necessarily bad i loved camping out for

2:14:14.640 --> 2:14:21.520
 the iphones um you know and as long as the hype is backed by like substance as long as it's backed

2:14:21.520 --> 2:14:27.760
 by something i can actually buy and like it's real then hype is great and it's a great feeling

2:14:27.760 --> 2:14:33.040
 it's when the hype is backed by lies that it's a bad feeling i mean a lot of people call you on

2:14:33.040 --> 2:14:38.080
 musk a fraud how could it be a fraud i've noticed this this kind of interesting effect which is

2:14:38.880 --> 2:14:46.560
 he does tend to over promise and deliver what's what's the better way to phrase it promise a

2:14:46.560 --> 2:14:52.800
 timeline that he doesn't deliver on he delivers much later on what do you think about that because

2:14:52.800 --> 2:14:58.480
 i i do that i think that's a programmer thing yeah i do that as well you think that's a really

2:14:58.480 --> 2:15:05.200
 bad thing to do is is that okay i think that's again as long as like you're working toward it

2:15:05.200 --> 2:15:12.960
 and you're gonna deliver on and it's not too far off right yeah right like like you know the whole

2:15:12.960 --> 2:15:19.440
 the whole autonomous vehicle thing it's like i mean i still think tassel is on track to to beat

2:15:19.440 --> 2:15:24.800
 us i still think even with their even with their missteps they have advantages we don't have um

2:15:25.520 --> 2:15:33.600
 you know elon is is better than me at at at like marshalling massive amounts of resources so you

2:15:33.600 --> 2:15:39.280
 know i still think given the fact they're maybe making some wrong decisions they'll end up winning

2:15:39.280 --> 2:15:46.080
 and like it's fine to hype it if you're actually gonna win right like if elon says look we're

2:15:46.080 --> 2:15:52.160
 gonna be landing rockets back on earth in a year and it takes four like you know he landed a rocket

2:15:52.160 --> 2:15:56.960
 back on earth and he was working toward it the whole time i think there's some amount of like

2:15:56.960 --> 2:16:01.520
 i think what it becomes wrong is if you know you're not gonna meet that deadline if you're lying

2:16:01.520 --> 2:16:08.320
 yeah that's brilliantly put like this is what people don't understand i think like elon believes

2:16:08.320 --> 2:16:15.360
 everything he says he does as far as i can tell he does and i i detected that in myself too like if i

2:16:16.320 --> 2:16:23.760
 it's only bullshit if you if you're like conscious of yourself lying yeah i think so yeah no you can't

2:16:23.760 --> 2:16:28.160
 take that to such an extreme right like in a way i think maybe billy mcfarland believed everything

2:16:28.160 --> 2:16:34.880
 he said too right that's how you start a cult and and everybody uh kills themselves yeah yeah like

2:16:34.880 --> 2:16:40.640
 it's you need you need if there's like some factor on it it's fine and you need some people to like

2:16:41.280 --> 2:16:47.360
 you know keep you in check but like if you deliver on most of the things you say and just the timelines

2:16:47.360 --> 2:16:54.720
 are off man it does piss people off though i wonder but who cares in in a long arc of history the

2:16:54.720 --> 2:16:59.280
 people everybody gets pissed off at the people who succeed which is one of the things that

2:16:59.280 --> 2:17:07.760
 frustrates me about this world is um they don't celebrate the success of others like

2:17:09.040 --> 2:17:16.880
 there's so many people that want elon to fail it's so fascinating to me like what is wrong with you

2:17:18.000 --> 2:17:23.360
 like so elon most talks about like people it's short like they talk about financial

2:17:23.360 --> 2:17:27.120
 but i think it's much bigger than the financials i've seen like the human factors community

2:17:27.120 --> 2:17:35.520
 be they want they want other people to fail why why why like even people the harshest thing is like

2:17:36.560 --> 2:17:41.200
 you know even people that like seem to really hate donald trump they want him to fail

2:17:41.840 --> 2:17:46.080
 or like the other president or they want rock obama to fail it's like

2:17:49.600 --> 2:17:54.400
 it's weird but i i want that i would love to inspire that part of the world to change because

2:17:54.400 --> 2:18:01.040
 well dammit if the human species is going to survive we can celebrate success like it seems

2:18:01.040 --> 2:18:06.640
 like the efficient thing to do in this objective function that like we're all striving for is to

2:18:06.640 --> 2:18:11.840
 celebrate the ones that like figure out how to like do better at that objective function as

2:18:11.840 --> 2:18:18.560
 opposed to like dragging them down back into them into the mud i think there is this is the speech

2:18:18.560 --> 2:18:23.520
 i always give about the commenters on hacker news um so first off something to remember about the

2:18:23.520 --> 2:18:29.520
 internet in general is commenters are not representative of the population yeah i don't

2:18:29.520 --> 2:18:35.200
 comment on anything i don't you know commenters are are are representative of a a certain sliver

2:18:35.200 --> 2:18:40.960
 of the population and on hacker news a common thing i'll see is when you'll see something that's like

2:18:41.920 --> 2:18:49.120
 you know promises to be wild out there and innovative there is some amount of you know

2:18:49.120 --> 2:18:53.440
 know checking them back to earth but there's also some amount of if this thing succeeds

2:18:55.200 --> 2:18:59.920
 well i'm 36 and i've worked at large tech companies my whole life

2:19:02.160 --> 2:19:06.800
 they can't succeed because if they succeed that would mean that i could have done something

2:19:06.800 --> 2:19:10.720
 different with my life but we know that i couldn't have we know that i couldn't have and and that's

2:19:10.720 --> 2:19:15.040
 why they're gonna fail and they have to root for them to fail to kind of maintain their world image

2:19:15.040 --> 2:19:22.560
 so tune it out and they comment well it's hard i so one of the things one of the things i'm

2:19:22.560 --> 2:19:31.200
 considering startup wise is to change that because i think the i think it's also a technology

2:19:31.200 --> 2:19:37.040
 problem it's a platform problem i agree it's like because the thing you said most people don't comment

2:19:37.040 --> 2:19:45.280
 it i think most people want to comment they just don't because it's all the assholes for

2:19:45.280 --> 2:19:49.280
 commenting i don't want to be grouped in with them i'm not you don't want to be at a party

2:19:49.280 --> 2:19:54.880
 where everyone is an asshole and so they but that's a platform's problem that's i can't believe

2:19:54.880 --> 2:20:02.080
 what reddits become i can't believe the group thinking reddit comments there's a reddit is

2:20:02.080 --> 2:20:08.400
 interesting one because they're subreddits and so you can still see especially small subreddits

2:20:09.120 --> 2:20:16.960
 that like that are little like havens of like joy and positivity and like deep even disagreement

2:20:16.960 --> 2:20:23.280
 but like nuanced discussion but it's only like small little pockets but that's that's emergent

2:20:23.280 --> 2:20:30.160
 the platform is not helping that or hurting that so i guess naturally something about the internet

2:20:30.160 --> 2:20:36.560
 that if you don't put in a lot of effort to encourage nuance and positive good vibes

2:20:37.120 --> 2:20:42.800
 it's naturally going to decline into chaos i would love to see someone do this well

2:20:42.800 --> 2:20:51.280
 yeah i think it's yeah very doable this is i think actually so i feel like twitter could be overthrown

2:20:51.280 --> 2:21:00.400
 yeah ashwabak talked about how like uh if you have like and retweet like that's only positive

2:21:00.400 --> 2:21:09.200
 wiring right the only way to do anything like negative there is um with a comment and that's

2:21:09.200 --> 2:21:16.320
 like that asymmetry is what gives you know twitter its particular toxicness whereas i find youtube

2:21:16.320 --> 2:21:21.760
 comments to be much better because youtube comments have a have a have an up and a down and they don't

2:21:21.760 --> 2:21:28.560
 show the downloads without getting into depth of this particular discussion the point is to explore

2:21:28.560 --> 2:21:33.920
 possibilities and get a lot of data on it because uh i mean i could disagree with what you just said

2:21:33.920 --> 2:21:41.040
 it's the point is it's unclear it's a it hasn't been explored in a really rich way uh like these

2:21:41.040 --> 2:21:48.720
 questions of how to create platforms that encourage positivity yeah i think it's a it's a technology

2:21:48.720 --> 2:21:53.600
 problem and i think we'll look back at twitter as it is now maybe it'll happen within twitter

2:21:53.600 --> 2:22:00.560
 but most likely somebody overthrows them is we'll look back at twitter and and say we can't

2:22:00.560 --> 2:22:05.840
 believe we put up with this level of toxicity you need a different business model too any any social

2:22:05.840 --> 2:22:10.320
 network that fundamentally has advertising as a business model this was in the social dilemma

2:22:10.320 --> 2:22:13.200
 which i didn't watch but i liked it it's like you know there's always the you know you're the

2:22:13.200 --> 2:22:19.200
 product you're not the uh but they had a nuance take on it that i really liked and it said the

2:22:19.200 --> 2:22:27.520
 product being sold is influence over you the product being sold is literally your you know

2:22:27.520 --> 2:22:35.760
 influence on you like that can't be if that's your idea okay well you know guess what it can't

2:22:35.760 --> 2:22:42.000
 not be toxic yeah maybe there's ways to spin it like with with giving a lot more control to the

2:22:42.000 --> 2:22:47.120
 user and transparency to see what is happening to them as opposed to in the shadows as possible

2:22:47.120 --> 2:22:52.560
 but that can't be the primary source but the users aren't no one's going to use that it depends it

2:22:52.560 --> 2:22:58.400
 depends it depends i think i think that the you're you're not going to you can't depend on

2:22:58.400 --> 2:23:03.520
 self awareness of the users it's a it's another it's a longer discussion because uh you can't

2:23:03.520 --> 2:23:11.840
 depend on it but you can reward self awareness like if for the ones who are willing to put in

2:23:11.840 --> 2:23:17.680
 the work of self awareness you can reward them and incentivize and perhaps be pleasantly surprised

2:23:17.680 --> 2:23:24.800
 how many people are willing to be self aware on the internet like we are in real life like i'm

2:23:24.800 --> 2:23:29.200
 putting in a lot of effort with you right now being self aware about if i say something stupid

2:23:29.200 --> 2:23:34.320
 or mean i'll like look at your like body language like i'm putting in that effort it's costly for

2:23:34.320 --> 2:23:42.800
 an intro very costly but on the internet fuck it like most people are like i don't care if this hurts

2:23:42.800 --> 2:23:49.040
 somebody i don't care if this uh is not interesting or if this is yeah the mean or whatever i think

2:23:49.040 --> 2:23:53.760
 so much of the engagement today on the internet is so disingenuine too yeah you're not doing this

2:23:53.760 --> 2:23:57.520
 out of a genuine this is what you think you're doing this just straight up to manipulate others

2:23:57.520 --> 2:24:03.600
 whether you're in you just became an ad okay let's talk about a fun topic which is programming

2:24:04.240 --> 2:24:10.000
 here's another book idea for you let me pitch uh what's your uh perfect programming setup so like

2:24:11.280 --> 2:24:19.280
 this by george hotz so uh like what listen your give me give me a mac book air sit me in a corner

2:24:19.280 --> 2:24:24.000
 of a hotel room and you know i'll still ask so you really don't care you don't fetishize like

2:24:24.000 --> 2:24:30.480
 multiple monitors keyboard uh those things are nice and i'm not going to say no to them but

2:24:30.480 --> 2:24:35.280
 do they automatically unlock tons of productivity no not at all i have definitely been more productive

2:24:35.280 --> 2:24:43.920
 on a mac book air in a corner of a hotel room what about um ide so uh which operating system

2:24:44.480 --> 2:24:51.200
 do you love what uh text editor do you use ide what um is there is there something that

2:24:51.200 --> 2:24:57.600
 that is like the perfect if you could just say the perfect productivity setup for george hotz

2:24:57.600 --> 2:25:01.840
 doesn't matter doesn't it doesn't matter literally doesn't matter you know i i guess i code most of

2:25:01.840 --> 2:25:05.840
 the time in vim like literally i'm using an editor from the 70s you know you didn't

2:25:05.840 --> 2:25:09.760
 didn't make anything better okay vs code is nice for reading code there's a few things that are nice

2:25:09.760 --> 2:25:15.440
 about it uh i think that there you can build much better tools how like idas x refs work way better

2:25:15.440 --> 2:25:23.520
 than vx vs codes why yeah actually that's a good question like why i i still use sorry emacs for

2:25:23.520 --> 2:25:31.120
 most uh i've actually never i have to confess something dark so i've never used them okay

2:25:31.920 --> 2:25:42.400
 this i think maybe i'm just afraid that my life has been a like a waste i'm so i'm not i'm not even

2:25:42.400 --> 2:25:48.080
 jellicle about emacs i think this this is how i feel about tensorflow vs pie torch yeah having

2:25:48.080 --> 2:25:52.400
 just like we've switched everything to pie torch now put months into the switch i have felt like

2:25:52.400 --> 2:25:58.560
 i've wasted years on tensorflow i can't believe it i can't believe how much better pie torches yeah

2:25:59.440 --> 2:26:04.560
 i've used emacs in vim doesn't matter yeah still just my my heart somehow i fell in love with lisp

2:26:04.560 --> 2:26:09.440
 i don't know why you can't the heart wants with the heart wants i don't i don't understand it but

2:26:09.440 --> 2:26:13.840
 it just connected with me maybe it's the functional language at first i connected with maybe it's because

2:26:13.840 --> 2:26:19.520
 so many of the ai courses before the deep learning revolution were taught with lisp in mind i don't

2:26:19.520 --> 2:26:24.160
 know i don't know what it is but i'm i'm stuck with it but at the same time like why am i not using

2:26:24.160 --> 2:26:28.880
 a modern id for some of these programming i don't know they're not that much better i've used modern

2:26:28.880 --> 2:26:34.000
 id used to but at the same time so like to just we're not to disagree with you but like i like

2:26:34.000 --> 2:26:42.160
 multiple monitors like i have to do work on a laptop and it's a it's a pain in the ass and also i'm

2:26:42.160 --> 2:26:49.360
 addicted to the kinesis weird keyboard that you could you could see there uh yeah so you don't

2:26:49.360 --> 2:26:55.040
 have any of that you can just be in a on a macbook i mean look at work i have three 24 inch monitors

2:26:55.040 --> 2:27:01.200
 i have a happy hacking keyboard i have a razor death header mouse like but it's not essential for you

2:27:01.200 --> 2:27:08.800
 now let's go to uh day in the life of george hotz what is the perfect day productivity wise so we're

2:27:08.800 --> 2:27:16.800
 not talking about like hunter s thompson uh drugs and uh let's let's look at productivity like what

2:27:16.800 --> 2:27:24.000
 what's the day look like i'm like hour by hour is there any regularities that create a magical

2:27:24.000 --> 2:27:30.240
 george hotz experience i can remember three days in my life and i remember these days vividly

2:27:30.240 --> 2:27:38.880
 when i've gone through kind of radical transformations to the way i think and what i would give i would

2:27:38.880 --> 2:27:43.760
 pay a hundred thousand dollars if i could have one of these days tomorrow um the days have been so

2:27:43.760 --> 2:27:50.560
 impactful and one was first discovering leis of utkowski on the singularity and reading that stuff

2:27:50.560 --> 2:27:58.000
 and like you know my mind was blown um and the next was discovering uh the hunter price and that

2:27:58.000 --> 2:28:04.080
 ai is just compression like finally understanding ai xi and what all of that was you know i like

2:28:04.080 --> 2:28:08.320
 read about it when i was 1819 i didn't understand it and then the fact that like lossless compression

2:28:08.320 --> 2:28:14.000
 implies intelligence the day that i was shown that um and then the third one is controversial

2:28:14.000 --> 2:28:19.600
 the day i found a blog called unqualified reservations and uh read that and i was like

2:28:20.400 --> 2:28:26.080
 wait which one is that that's uh what's the guy's name courtesy arvin yeah so many people tell me

2:28:26.080 --> 2:28:33.040
 i'm supposed to talk to him yeah the day he sounds insane or brilliant but insane or both i don't know

2:28:33.040 --> 2:28:37.440
 the day i found that blog was another like this was during like like gamer gate and kind of the

2:28:37.440 --> 2:28:43.840
 run up to the 2016 election and i'm like wow okay the world makes sense now this this like i had a

2:28:43.840 --> 2:28:47.760
 framework now to interpret this just like i got the framework for ai and a framework to

2:28:47.760 --> 2:28:52.960
 interpret technological progress like those days when i discovered these new frameworks or oh

2:28:52.960 --> 2:28:58.720
 interesting it's just not about but what was special about those days how those days come to be

2:28:58.720 --> 2:29:05.760
 is it just you got lucky like sure i like what you just encountered a hotter prize on uh on

2:29:05.760 --> 2:29:12.160
 hack and use or something like that um like what but you see i don't think it's just see i don't

2:29:12.160 --> 2:29:16.320
 think it's just that like i could have gotten lucky at any point i think that in a way you

2:29:16.320 --> 2:29:22.880
 were ready at that moment yeah exactly to receive the information but is there some magic to the

2:29:22.880 --> 2:29:31.680
 day today of like like eating breakfast and it's the mundane things nothing no i drift i drift

2:29:31.680 --> 2:29:37.280
 through life without structure i drift through life hoping and praying that i will get another day

2:29:37.280 --> 2:29:43.680
 like those days and there's nothing in particular you do to uh to be a receptacle for another for

2:29:43.680 --> 2:29:49.600
 day number four no i didn't do anything to get the other ones so i don't think i have to really

2:29:49.600 --> 2:29:55.600
 do anything now i took a month long trip to new york and the ethereum thing was the

2:29:55.600 --> 2:30:00.720
 highlight of it but the rest of it was pretty terrible i did a two week road trip and i got

2:30:00.720 --> 2:30:06.800
 i had to turn around i had to turn around i'm driving in uh in gunnison colorado i passed

2:30:06.800 --> 2:30:12.000
 through gunnison and uh the snow starts coming down there's a pass up there called monarch pass

2:30:12.000 --> 2:30:16.080
 in order to get through to denver you got to get over the rockies and i had to turn my car around

2:30:16.080 --> 2:30:21.680
 i couldn't i watched uh i watched a f150 go off the road i'm like i gotta go back and

2:30:23.120 --> 2:30:27.520
 like that day was meaningful because like like it was real like i actually had to turn my car

2:30:27.520 --> 2:30:33.760
 around um it's rare that anything even real happens in my life even as you know mundane is the fact

2:30:33.760 --> 2:30:38.000
 that yeah there was snow i had to turn around stay in gunnison leave the next day something about

2:30:38.000 --> 2:30:43.680
 that moment felt real okay so actually it's interesting to break apart the three moments

2:30:43.680 --> 2:30:49.040
 you mentioned if it's okay so it'll uh i always have trouble pronouncing his name but

2:30:49.040 --> 2:31:00.160
 allows a yorkowski yeah so what how did your worldview change in starting to consider the

2:31:01.280 --> 2:31:06.880
 the exponential growth of ai and agi that he thinks about and the the threats of artificial

2:31:06.880 --> 2:31:12.400
 intelligence and all that kind of ideas like can you is it just like can you maybe uh break apart

2:31:12.400 --> 2:31:18.240
 like what exactly was so magical to you was a transformational experience today everyone knows

2:31:18.240 --> 2:31:23.520
 him for threats and ai safety um this was pre that stuff there was i don't think a mention of ai

2:31:23.520 --> 2:31:29.120
 safety on the page um this is this is old yorkowski stuff he'd probably denounce it all now he'd

2:31:29.120 --> 2:31:35.840
 probably be like that's exactly what i didn't want to happen sorry man uh is there something

2:31:35.840 --> 2:31:40.960
 specific you can take from his work that you can remember yeah uh it was this realization that

2:31:40.960 --> 2:31:50.000
 uh computers double in power every 18 months and humans do not and they haven't crossed yet

2:31:50.000 --> 2:31:54.720
 but if you have one thing that's doubling every 18 months and one thing that's staying like this

2:31:54.720 --> 2:31:59.600
 you know here's your log graph here's your line you know you calculate that

2:32:01.440 --> 2:32:06.320
 okay and that did that open the door to the exponential thinking like thinking that like

2:32:06.320 --> 2:32:12.320
 you know what with technology we can actually transform the world it opened the door to human

2:32:12.320 --> 2:32:19.360
 obsolescence it it opened the door to realize that in my lifetime humans are going to be replaced

2:32:20.480 --> 2:32:24.480
 and then the matching idea to that of artificial intelligence with a hunter prize

2:32:26.880 --> 2:32:33.520
 you know i'm torn i go back and forth on what i think about it yeah but the the the basic thesis

2:32:33.520 --> 2:32:38.320
 is it's nice it's a nice compelling notion that we can reduce the task of creating an

2:32:38.320 --> 2:32:44.160
 intelligent system a generally intelligent system into the task of compression so you

2:32:44.160 --> 2:32:48.160
 you can think of all of intelligence in the universe in fact is a kind of compression

2:32:50.080 --> 2:32:54.320
 do you find that a was that just at the time you found that as a compelling idea do you still

2:32:54.320 --> 2:33:00.240
 find that a compelling idea i still find that a compelling idea um i think that it's not that

2:33:00.240 --> 2:33:07.600
 useful day to day but actually um one of maybe my quests before that was a search for the definition

2:33:07.600 --> 2:33:13.680
 of the word intelligence and i never had one and i definitely have a definition of the word compression

2:33:14.560 --> 2:33:19.840
 it's a very uh simple uh straightforward one and uh you know what compression is you know what

2:33:19.840 --> 2:33:24.960
 lossless is lossless compression not lossy lossless compression and that that is equivalent to

2:33:24.960 --> 2:33:29.520
 intelligence which i believe i'm not sure how useful that definition is day to day but like i

2:33:29.520 --> 2:33:35.440
 now have a framework to understand what it is and he just 10x the the uh the prize for that

2:33:35.440 --> 2:33:40.880
 competition like recently a few months ago you ever thought of taking a crack at that oh i did

2:33:40.880 --> 2:33:46.960
 oh i did i spent i spent the next after i found the prize i spent the next six months of my life

2:33:46.960 --> 2:33:52.240
 trying it and uh well that's when i started learning everything about ai and then i worked

2:33:52.240 --> 2:33:55.840
 with vicarious for a bit and then i learned read all the deep learning stuff and i'm like okay now

2:33:55.840 --> 2:34:01.280
 i like i'm caught up to modern ai wow and i had i had a really good framework to put it all in

2:34:01.280 --> 2:34:06.640
 from the compression stuff right like some of the first uh some of the first deep learning models

2:34:06.640 --> 2:34:14.240
 i played with were uh or ggpt gpt basically but before transformers before it was still uh rnn's

2:34:15.040 --> 2:34:20.400
 to to do uh character prediction but by the way on the compression side i mean the

2:34:20.400 --> 2:34:26.640
 the especially neural networks what do you make of the lossless requirement with the harder prize so

2:34:27.760 --> 2:34:33.360
 you know human intelligence and neural networks can probably compress stuff pretty well but

2:34:33.360 --> 2:34:38.720
 there would be lossy it's imperfect uh you can turn a lossy compression into a lossless compressor

2:34:38.720 --> 2:34:43.120
 pretty easily using an arithmetic encoder right you can take an arithmetic encoder and you can

2:34:43.120 --> 2:34:48.880
 just encode the noise with maximum efficiency right so even if you can't predict exactly what

2:34:48.880 --> 2:34:54.160
 the next character is the better a probability distribution you can put over the next character

2:34:54.160 --> 2:34:58.400
 you can then use an arithmetic encoder to uh right you don't have to know whether it's an

2:34:58.400 --> 2:35:03.840
 ear and eye you just have to put good probabilities on them and then you know code those and if you

2:35:03.840 --> 2:35:08.800
 have it's a bit of entropy thing right so let me on that topic it'd be interesting as a little

2:35:08.800 --> 2:35:14.960
 side tour what are your thoughts in this year about gpt3 and these language models and these

2:35:14.960 --> 2:35:21.440
 transformers is there something interesting to you as an ai researcher or is there something

2:35:21.440 --> 2:35:27.280
 interesting to you as an autonomous vehicle developer nah i think uh i think it's arrived

2:35:27.280 --> 2:35:31.760
 i mean it's not like it's cool it's cool for what it is but no we're not just going to be able to

2:35:31.760 --> 2:35:37.760
 scale up to gpg12 and get general purpose intelligence like your loss function is literally

2:35:37.760 --> 2:35:42.400
 just you know you know cross entropy loss on the character right like that's not the loss function

2:35:42.400 --> 2:35:52.160
 of general intelligence is that obvious to you yes can you imagine that like to play devil's

2:35:52.160 --> 2:35:58.400
 advocate on yourself is it possible that you can the gpt12 will achieve general intelligence

2:35:58.400 --> 2:36:03.520
 with something as dumb as this kind of loss function i guess it depends what you mean by

2:36:03.520 --> 2:36:09.840
 general intelligence so there's another problem with the gpt's and that's that they don't have a

2:36:09.840 --> 2:36:19.600
 uh they don't have long term memory right all right so like just gpt12 a scaled up version

2:36:19.600 --> 2:36:29.520
 of gpt2 or 3 i find it hard to believe well you can scale it and it's yeah so it's a hard

2:36:29.520 --> 2:36:37.200
 quarter hard coded length but you can make it wider and wider and wider yeah you're gonna get

2:36:37.200 --> 2:36:44.720
 you're gonna get cool things from those systems but i don't think you're ever gonna get something

2:36:44.720 --> 2:36:51.840
 that can like you know build me a rocket ship what about solve driving so you know you can use

2:36:51.840 --> 2:36:58.720
 transformer with video for example you think is there something in there no because i mean look

2:36:59.680 --> 2:37:03.520
 we use we use a grew we use a grew we could change that grew out to a transformer

2:37:03.520 --> 2:37:11.360
 um i think driving is much more marcovian than language so marcovian you mean like the memory

2:37:11.360 --> 2:37:17.360
 which which aspect of marcovian i mean that like most of the information in the state at t minus

2:37:17.360 --> 2:37:22.960
 one is also in the info is in state t right and it kind of like drops off nicely like this whereas

2:37:22.960 --> 2:37:28.000
 sometime with language you have to refer back to the third paragraph on the second page i feel like

2:37:28.000 --> 2:37:32.240
 there's not many like like you can say like speed limit signs but there's really not many things

2:37:32.240 --> 2:37:37.760
 in autonomous driving that look like that but if you look at uh to play devil's advocate is uh the

2:37:37.760 --> 2:37:42.880
 risk estimation thing that you've talked about is kind of interesting is uh it feels like there might

2:37:42.880 --> 2:37:50.880
 be some longer term uh aggregation of context necessary to be able to figure out like the context

2:37:52.000 --> 2:37:57.840
 i'm not even sure i'm i'm believing my my own devil's we have a nice we have a nice like vision

2:37:57.840 --> 2:38:04.400
 model which outputs like a a one or two four dimensional perception space um can i try transformers

2:38:04.400 --> 2:38:08.960
 on it sure i probably will at some point we'll try transformers and then we'll just see do

2:38:08.960 --> 2:38:13.440
 they do better sure i'm but it might not be a game changer you know well i'm not like like

2:38:13.440 --> 2:38:17.760
 might transformers work better than grooves for autonomous driving sure might we switch sure

2:38:17.760 --> 2:38:22.000
 is this some radical change no okay we use a slightly different you know we switch from

2:38:22.000 --> 2:38:28.960
 r and ns to grooves like okay maybe it's grease to transformers but no it's not yeah well on the

2:38:28.960 --> 2:38:33.040
 on the topic of general intelligence i don't know how much i've talked to you about it like what um

2:38:34.720 --> 2:38:41.120
 do you think we'll actually build an agi like if you look at ray korswell with singularity do you

2:38:41.120 --> 2:38:49.040
 do you have like an intuition about you're kind of saying driving is easy yeah and i i tend to

2:38:49.040 --> 2:38:57.360
 personally believe that solving driving will have really deep important impacts on our ability to

2:38:57.360 --> 2:39:03.760
 solve general intelligence like i i think driving doesn't require general intelligence but i think

2:39:03.760 --> 2:39:10.560
 they're going to be neighbors in a way that it's like deeply tied because it's so like driving is so

2:39:10.560 --> 2:39:17.280
 deeply connected to the human experience that i think solving one will help solve the other

2:39:17.280 --> 2:39:22.240
 but but so i don't see i don't see driving is like easy and almost like separate than

2:39:22.240 --> 2:39:27.120
 general intelligence but like what's your vision of the future with a singularity do you see there

2:39:27.120 --> 2:39:31.840
 will be a single moment like a singularity where it'll be a phase shift are we in the singularity

2:39:31.840 --> 2:39:36.320
 now like what do you have crazy ideas about the future in terms of agi we're definitely in the

2:39:36.320 --> 2:39:41.760
 singularity now um we are of course of course look at the bandwidth between people the bandwidth

2:39:41.760 --> 2:39:47.440
 between people goes up all right um the singularity is just you know in the bandwidth but what do you

2:39:47.440 --> 2:39:51.680
 mean by the bandwidth of people communications tools the whole world is networked the whole world

2:39:51.680 --> 2:39:56.240
 is networked and we raise the speed of that network right oh so you think the communication of

2:39:56.240 --> 2:40:02.480
 information in a distributed way is a empowering thing for collective intelligence oh i didn't

2:40:02.480 --> 2:40:05.600
 say it's necessarily a good thing but i think that's like when i think of the definition of the

2:40:05.600 --> 2:40:12.640
 singularity yeah it seems kind of right i see like it's a change in the world beyond which

2:40:13.440 --> 2:40:17.200
 like the world would be transformed in ways that we can't possibly imagine no i mean i think we're

2:40:17.200 --> 2:40:21.120
 in the singularity now in the sense that there's like you know one world in a monoculture and it's

2:40:21.120 --> 2:40:27.600
 also linked yeah i mean i kind of share the intuition that the the singularity will originate

2:40:27.600 --> 2:40:35.120
 from the collective intelligence of us ants versus the like some single system agi type thing

2:40:35.120 --> 2:40:40.400
 oh i totally agree with that yeah i don't i don't really believe in like like a hard takeoff agi

2:40:40.400 --> 2:40:48.880
 kind of thing um yeah i don't think i don't even think ai is all that difference in kind

2:40:49.440 --> 2:40:55.280
 from what we've already been building um with respect to driving i think driving is a subset

2:40:55.280 --> 2:40:59.440
 of general intelligence and i think it's a pretty complete subset i think the tools we develop at

2:40:59.440 --> 2:41:05.120
 comma will also be extremely helpful to solving general intelligence and that's i think the

2:41:05.120 --> 2:41:09.440
 real reason why i'm doing it i don't care about self driving cars it's a cool problem to beat people

2:41:09.440 --> 2:41:16.240
 at but yeah i mean yeah you're kind of you're of two minds so one you do have to have a mission

2:41:16.240 --> 2:41:22.160
 and you want to focus and make sure you get you get there you can't forget that but at the same time

2:41:22.160 --> 2:41:28.720
 there is a thread that's much bigger than uh the kinesis the entirety of your effort that's much

2:41:28.720 --> 2:41:35.040
 bigger than just driving with ai and with general intelligence it is so easy to delude yourself

2:41:35.040 --> 2:41:38.640
 into thinking you've figured something out when you haven't if we build a level five

2:41:38.640 --> 2:41:44.640
 self driving car we have indisputably built something yeah is it general intelligence

2:41:44.640 --> 2:41:49.600
 i'm not going to debate that i will say we've built something that provides huge financial value

2:41:49.600 --> 2:41:54.160
 yeah beautifully put that's the engineer and credo like just just build the thing it's like

2:41:54.160 --> 2:41:59.920
 that's why i'm with uh with uh with with ilan on uh go to mars yeah that's a great one you can

2:41:59.920 --> 2:42:06.560
 argue like who the hell cares about going to mars but the reality is set that as a mission get it

2:42:06.560 --> 2:42:11.760
 done yeah and then you're going to crack some pro problem that you've never even expected in the

2:42:11.760 --> 2:42:17.040
 process of doing that yeah yeah i mean no i think if i had a choice between you managed to go on a

2:42:17.040 --> 2:42:22.000
 mars and solving self driving cars i think going to mars is uh better but i don't know i'm more

2:42:22.000 --> 2:42:25.600
 suited for self driving cars i'm an information guy i'm not a modernist i'm a postmodernist

2:42:26.400 --> 2:42:32.160
 postmodernist all right beautifully put let me let me drag you back to programming for a second

2:42:32.160 --> 2:42:37.120
 what three maybe three to five programming languages should people learn do you think like if you look

2:42:37.120 --> 2:42:44.800
 at yourself what did you get the most out of from learning uh well so everybody should learn

2:42:44.800 --> 2:42:49.920
 see an assembly we'll start with those two right assembly yeah if you can't code an assembly you

2:42:49.920 --> 2:42:54.080
 don't know what the computer's doing you don't understand like you don't have to be great in

2:42:54.080 --> 2:42:59.200
 assembly but you have to code in it and then like you have to appreciate assembly in order to

2:42:59.200 --> 2:43:04.080
 appreciate all the great things see gets you and then you have to code and see in order to appreciate

2:43:04.080 --> 2:43:08.560
 all the great things python gets you so i'll just say assembly see in python we'll start with those three

2:43:09.600 --> 2:43:16.480
 the memory allocation of c and the the the fact that so assembly is to give you a sense of just

2:43:16.480 --> 2:43:21.680
 how many levels of abstraction you get to work on in modern day programming yeah yeah graph coloring

2:43:21.680 --> 2:43:26.080
 for assignment register assignment and compilers yeah like you know you got to do you know the

2:43:26.080 --> 2:43:29.280
 compiler your computer only has a certain number of registers yet you can have all the variables

2:43:29.280 --> 2:43:34.480
 you want to see function you know so you get to start to build intuition about compilation like

2:43:34.480 --> 2:43:41.600
 what a compiler gets you what else um well then there is then there's kind of uh so those are

2:43:41.600 --> 2:43:47.680
 all very imperative programming languages um then there's two other paradigms for programming that

2:43:47.680 --> 2:43:52.720
 everybody should be familiar with i'm one of them is functional uh you should learn haskell and take

2:43:52.720 --> 2:43:58.480
 that all the way through learn a language with dependent types like cock um learn that whole

2:43:58.480 --> 2:44:04.960
 space like the very pl theory heavy languages and haskell is your favorite functional what is that

2:44:04.960 --> 2:44:10.400
 the go to you'd say yeah i'm not a great haskell programmer i wrote a compiler and haskell ones

2:44:10.400 --> 2:44:13.600
 there's another paradigm and actually there's one more paradigm that i'll even talk about

2:44:13.600 --> 2:44:16.880
 after that that i never used to talk about when i would think about this but the next paradigm

2:44:16.880 --> 2:44:22.880
 is learn verilog or hdl um understand this idea of all of the instructions executed once

2:44:25.360 --> 2:44:30.960
 if i have a block in verilog and i write stuff in it it's not sequential they all execute it once

2:44:33.600 --> 2:44:39.280
 and then like think like that that's how hard it works to be so i guess assembly doesn't quite

2:44:39.280 --> 2:44:44.720
 get you that assembly is more about compilation and verilog is more about the hardware like

2:44:44.720 --> 2:44:50.560
 giving a sense of what actually is the hardware is doing assembly see python are straight like

2:44:50.560 --> 2:44:55.760
 they sit right on top of each other in fact c is well let me see it's kind of coded in c but you

2:44:55.760 --> 2:45:00.640
 could imagine the first c was coded in assembly and python is actually coded in c um so you know you

2:45:00.640 --> 2:45:08.320
 can straight up go on that got it and then verilog gives you that's brilliant okay and then i think

2:45:08.320 --> 2:45:15.680
 there's another one now everyone's karpathy calls it programming 2.0 which is learn a i'm not even

2:45:15.680 --> 2:45:20.800
 gonna don't learn tensorflow learn pytorch so machine learning we've got to come up with a

2:45:20.800 --> 2:45:27.440
 better term than programming 2.0 or um but yeah it's a programming language learn it

2:45:29.680 --> 2:45:33.920
 i wonder if it can be formalized a little bit better which we feel like we're in the early days

2:45:33.920 --> 2:45:40.640
 of what that actually entails data driven programming data driven programming yeah

2:45:41.600 --> 2:45:47.360
 but it's so fundamentally different as a paradigm than the others like it almost requires a different

2:45:47.360 --> 2:45:56.560
 skill set but you think it's still yeah and pytorch versus tensorflow pytorch wins it's

2:45:56.560 --> 2:46:00.560
 the fourth paradigm it's the fourth paradigm that i've kind of seen there's like this you know

2:46:00.560 --> 2:46:09.200
 imperative functional hardware i don't know a better word for it and then ml do you have advice

2:46:11.520 --> 2:46:16.640
 for people uh that want to you know get into programming want to learn programming you have a

2:46:17.200 --> 2:46:24.160
 video uh what is programming noob lessons exclamation point and i think the top comment

2:46:24.160 --> 2:46:32.960
 is like warning this is not for noobs uh do you have a noob like uh tldw for that video but also

2:46:34.640 --> 2:46:40.640
 a noob friendly advice on how to get into programming you are never going to learn

2:46:40.640 --> 2:46:46.080
 programming by watching a video called learn programming the only way to learn programming

2:46:46.080 --> 2:46:50.000
 i think and the only one is it the only way everyone i've ever met who can program well

2:46:50.000 --> 2:46:56.400
 learned it all in the same way they had something they wanted to do and then they tried to do it

2:46:56.400 --> 2:47:02.320
 and then they were like oh well okay this is kind of you know be nice as a computer could kind of do

2:47:02.320 --> 2:47:06.160
 this and then you know that's how you learn you just keep pushing on a project

2:47:08.880 --> 2:47:14.480
 so the only advice i have for learning programming is go program somebody wrote to me a question like

2:47:14.480 --> 2:47:19.920
 we don't really they're looking to learn about recurring on networks and saying like my company

2:47:19.920 --> 2:47:24.800
 is thinking of doing recur using recurrent neural networks for time series data but we don't really

2:47:24.800 --> 2:47:29.600
 have an idea of where to use it yet we just want to like do you have any advice on how to learn about

2:47:30.480 --> 2:47:37.200
 these are these kind of general machine learning questions and i think the answer is like actually

2:47:37.200 --> 2:47:41.840
 have a problem that you're trying to solve and and just i see that stuff oh my god when people

2:47:41.840 --> 2:47:46.640
 talk like that they're like i heard machine learning is important could you help us integrate

2:47:46.640 --> 2:47:53.040
 machine learning with macaroni and cheese production you just i don't even you can't help

2:47:53.040 --> 2:47:59.280
 these people like who lets you run anything who lets that kind of person run anything i think we're

2:47:59.280 --> 2:48:06.400
 all we're all beginners at some point so it's not like they're a beginner it's it's like my problem

2:48:06.400 --> 2:48:10.720
 is not that they don't know about machine learning my problem is that they think that machine learning

2:48:10.720 --> 2:48:16.960
 has something to say about macaroni and cheese production or like i heard about this new technology

2:48:16.960 --> 2:48:24.080
 how can i use it for why like i don't know what it is but how can i use it for why that's true

2:48:24.080 --> 2:48:28.240
 you have to build up an intuition of how because you might be able to figure out a way but like

2:48:28.240 --> 2:48:34.160
 the prerequisites you should have a macaroni and cheese problem to solve first exactly and then two

2:48:34.160 --> 2:48:40.880
 you should have more traditional like in the learning process involve more traditionally applicable

2:48:40.880 --> 2:48:46.480
 problems in the space of whatever that is of machine learning and then see if it can be applied

2:48:46.480 --> 2:48:50.960
 to me at least start with tell me about a problem like if you have a problem you're like you know

2:48:50.960 --> 2:48:55.760
 some of my boxes aren't getting enough macaroni in them um can we use machine learning to solve

2:48:55.760 --> 2:49:01.520
 this problem that's much much better than how do i apply machine learning to macaroni and cheese

2:49:01.520 --> 2:49:08.000
 one big thing maybe this is me uh talking to the audience a little bit because i get these days

2:49:08.000 --> 2:49:18.000
 so many messages a device on how to like learn stuff okay my this this this is not me being mean

2:49:18.000 --> 2:49:27.120
 i think this is quite profound actually is you should google it oh yeah like one of the uh

2:49:27.120 --> 2:49:34.000
 like skills that you should really acquire as an engineer as a researcher as a thinker like one

2:49:34.000 --> 2:49:39.760
 there's two two two complementary skills like one is with a blank sheet of paper with no internet

2:49:39.760 --> 2:49:46.400
 to think deeply and then the other is to google the crap out of the questions you have like that's

2:49:46.400 --> 2:49:51.920
 actually a skill i don't people often talk about but like doing research like pulling at the thread

2:49:51.920 --> 2:49:58.640
 like looking up different words going into like github repositories with two stars and like looking

2:49:58.640 --> 2:50:04.320
 how they did stuff like looking at the code or going on twitter seeing like there's little

2:50:04.320 --> 2:50:09.920
 pockets of brilliant people that are like having discussions like if you're a neuroscientist go

2:50:09.920 --> 2:50:16.320
 into signal processing community if you're an ai person going into the psychology community like

2:50:16.320 --> 2:50:23.760
 the switch communities i keep searching searching searching because it's so much better to invest

2:50:23.760 --> 2:50:29.920
 in like finding somebody else who already solved your problem then then is to try to solve the

2:50:29.920 --> 2:50:36.560
 problem and because they've often invested years of their life like entire communities are probably

2:50:36.560 --> 2:50:41.280
 already out there who have tried to solve your problem i think they're the same thing i think

2:50:41.280 --> 2:50:46.320
 you go try to solve the problem and then in trying to solve the problem if you're good at

2:50:46.320 --> 2:50:50.880
 solving problems you'll stumble upon the person who solved it already yeah but the stumbling is

2:50:50.880 --> 2:50:54.560
 really important i think that's the skill that people should really put especially in undergrad

2:50:55.520 --> 2:51:00.640
 like search if you ask me a question how should i get started in deep learning especially like

2:51:00.640 --> 2:51:09.840
 especially like that is just so googleable like the whole point is you google that and you get

2:51:09.840 --> 2:51:15.840
 and you get a million pages and just start looking at them yeah start pulling at the threads start

2:51:15.840 --> 2:51:21.920
 exploring start taking notes start getting advice from a million people that already like spent their

2:51:21.920 --> 2:51:26.640
 life answering that question actually oh well yeah i mean that's definitely also yeah when people

2:51:26.640 --> 2:51:30.400
 like ask me things like that i'm like trust me the top answer on google is much much better than

2:51:30.400 --> 2:51:38.480
 anything i'm going to tell you right yeah people ask it's an interesting question let me know if

2:51:38.480 --> 2:51:44.160
 you have any recommendations what three books technical or fiction or philosophical had an

2:51:44.160 --> 2:51:49.920
 impact on your life or you wouldn't recommend perhaps uh maybe we'll start with the least

2:51:49.920 --> 2:51:59.360
 controversial uh infinite jest um infinite jest is a do you have a foster house yeah it's a book

2:51:59.360 --> 2:52:08.480
 about wireheading really uh very enjoyable to read very well written you know you will you will you

2:52:08.480 --> 2:52:14.320
 will grow as a person reading this book uh it's effort um and i'll set that up for the second

2:52:14.320 --> 2:52:22.480
 book which is pornography that's called atlas shrugged um which um atlas shrugged is pornography

2:52:22.480 --> 2:52:27.520
 i mean it is i will not i will not defend the i will not say atlas shrugged is a well written

2:52:27.520 --> 2:52:32.480
 book um it is entertaining to read certainly just like pornography the production value

2:52:32.480 --> 2:52:37.520
 isn't great um you know there's a 60 page uh monologue in there that anran's editor really

2:52:37.520 --> 2:52:44.480
 wanted to take out and she uh paid she paid out of her pocket to keep that 60 page monologue in

2:52:44.480 --> 2:52:55.200
 the book um but it is a great book for a kind of framework um of human relations and i know a lot

2:52:55.200 --> 2:53:01.440
 of people are like yeah but it's a terrible framework yeah but it's a framework just for context

2:53:01.440 --> 2:53:09.520
 in a couple days i'm speaking with for probably four plus hours with yaron brook who's the main

2:53:09.520 --> 2:53:17.040
 living remaining objectivists objectivist interest uh so i've always found this philosophy quite

2:53:17.040 --> 2:53:25.280
 interesting on many levels one of how repulsive some percent of the large percent of the population

2:53:25.280 --> 2:53:32.880
 find it which is always uh always funny to me when people are like unable to even read a philosophy

2:53:32.880 --> 2:53:41.120
 because um of some i think that says more about their psychological perspective on it yeah but

2:53:41.120 --> 2:53:49.040
 but there is something about objectivism and anran's philosophy that's really deeply connected to this

2:53:49.040 --> 2:53:59.600
 idea of capitalism of uh the ethical life is the productive life um that was always um compelling

2:53:59.600 --> 2:54:04.560
 to me it didn't seem as like i didn't seem to interpret it in the negative sense that some

2:54:04.560 --> 2:54:11.360
 people do to be fair i read the book when i was 19 so you had an impact at that point yeah yeah and

2:54:11.360 --> 2:54:15.840
 the bad guys in the book have this slogan from each according to their ability to each according

2:54:15.840 --> 2:54:21.520
 to their need and i'm looking at this and i'm like these are the most cart this is team rocket level

2:54:21.520 --> 2:54:26.480
 cartoonishness right no bad and then when i realized that was actually the slogan of the communist party

2:54:27.280 --> 2:54:32.960
 i'm like wait a second wait no no no no no just you're telling me this really happened

2:54:32.960 --> 2:54:38.080
 yeah it's interesting i mean one of the criticisms of her work is she has a cartoonish

2:54:38.080 --> 2:54:45.280
 view of good and evil like that there's like the the reality isn't jordan peterson says is that each

2:54:45.280 --> 2:54:50.000
 of us have the capacity for good and evil in us as opposed to like there's some characters who are

2:54:50.000 --> 2:54:53.920
 purely evil and some characters are purely good and that's in a way why it's pornographic

2:54:55.040 --> 2:54:59.680
 the production value i love it well evil is punished and there's very clearly like you know

2:54:59.680 --> 2:55:06.480
 no there's no there's no you know uh just like porn doesn't have uh you know like character

2:55:06.480 --> 2:55:12.160
 growth well you know neither does alish around like brilliant well put but at 19 year old

2:55:12.160 --> 2:55:17.840
 george hotz it was uh it was good enough yeah yeah yeah what uh what's the third you have something um

2:55:18.960 --> 2:55:24.320
 i could give i these these two i'll just throw out uh their sci fi uh perbutation city um great

2:55:24.320 --> 2:55:30.720
 things just try thinking about copies yourself and then um them by science that is uh greg eagan

2:55:31.680 --> 2:55:35.600
 uh he's uh that might not be his real name some australian guy might not be australian

2:55:35.600 --> 2:55:40.720
 i don't know um and then this one's online it's called the metamorphosis of prime intellect

2:55:42.080 --> 2:55:47.920
 um it's a story set in a post singularity world it's interesting is there uh can you

2:55:47.920 --> 2:55:51.760
 either of the worlds do you find something uh philosophy interesting in them that you can

2:55:51.760 --> 2:55:59.840
 come in on i mean it is clear to me that uh metamorphosis of prime intellect is like written by

2:56:00.480 --> 2:56:11.040
 an engineer uh which is it's very it's very almost a pragmatic take on a utopia in a way

2:56:12.480 --> 2:56:18.560
 positive or negative is that well that's up to you to decide reading the book and the ending of it

2:56:18.560 --> 2:56:25.200
 is very interesting as well and i didn't realize what it was i first read that when i was 15 i've

2:56:25.200 --> 2:56:29.840
 reread that book several times in my life and it's short it's 50 pages i want to go read it

2:56:30.720 --> 2:56:35.360
 what's uh sorry it's a little tangent i've been working through the foundation i've been i've

2:56:35.360 --> 2:56:40.080
 haven't read much sci fi my whole life and i'm trying to fix that uh the last few months

2:56:40.080 --> 2:56:46.960
 that's been a little side project what's uh to use the greatest sci fi novel uh that uh people

2:56:46.960 --> 2:56:51.520
 should read or is it or i mean i would i would yeah i would i would say like yeah parametrician

2:56:51.520 --> 2:56:56.800
 city metamorphosis of prime intellect um i don't know i i didn't like foundation i thought it was

2:56:56.800 --> 2:57:02.720
 way too modernist you like dune and like all of those i've never read dune i've never read dune

2:57:02.720 --> 2:57:10.240
 i i have to read it uh fire upon the deep is interesting uh okay i mean look everyone should

2:57:10.240 --> 2:57:13.600
 read everyone should read neuromancer everyone should read snow crash if you haven't read those

2:57:13.600 --> 2:57:18.080
 like start there um yeah i haven't read snow crash never snow crash oh it's i mean it's very

2:57:18.080 --> 2:57:22.960
 interesting go to lesher bach and if you want the controversial one bronze age mindset

2:57:25.120 --> 2:57:30.000
 all right i'll look into that one those aren't sci fi but just to round out books

2:57:31.520 --> 2:57:35.120
 so a bunch of people asked me on twitter and read it and so on

2:57:35.760 --> 2:57:40.000
 for advice so what advice would you give a young person today about life

2:57:40.000 --> 2:57:50.640
 what uh yeah i mean looking back you especially when you're a young younger you did uh and you

2:57:50.640 --> 2:57:57.120
 continued it you've accomplished a lot of interesting things is there some advice from those

2:57:59.600 --> 2:58:04.720
 from that life of yours that you can pass on if college ever opens again i would love

2:58:04.720 --> 2:58:11.200
 to give a graduation speech um at that point i will put a lot of somewhat satirical effort

2:58:11.200 --> 2:58:16.560
 into this question yeah at this uh you haven't written anything at this point oh you know what

2:58:16.560 --> 2:58:23.760
 always wear sunscreen this is water like your plagiarism i mean you know but but that's the

2:58:23.760 --> 2:58:28.800
 that's the like clean your room you know yeah you can plagiarism from from all of this stuff and it's

2:58:28.800 --> 2:58:38.800
 it's there is no self help books aren't designed to help you they're designed to make you feel good

2:58:39.920 --> 2:58:47.120
 like whatever advice i could give you already know everyone already knows sorry it doesn't feel good

2:58:50.080 --> 2:58:55.680
 right like you know you know i don't know what if i tell you that you should you know

2:58:55.680 --> 2:59:03.440
 know eat well and and and read more and it's not going to do anything i think the whole like genre

2:59:03.440 --> 2:59:09.760
 of those kind of questions is is is meaningless i don't know if anything it's don't worry so much

2:59:09.760 --> 2:59:15.520
 about that stuff don't be so caught up in your head right i mean you're yeah in the sense that your

2:59:15.520 --> 2:59:21.920
 whole life if your whole existence is like moving version of that advice i don't know yeah

2:59:21.920 --> 2:59:27.440
 and there's there's something i mean there's something in you that resists that kind of

2:59:27.440 --> 2:59:35.600
 thinking in that in itself is it's just illustrative of who you are and there's something to learn from

2:59:35.600 --> 2:59:43.280
 that i think you're you're clearly not overthinking stuff yeah and you know there's a gut thing i

2:59:43.280 --> 2:59:47.600
 even when i talk about my advice i'm like my advice is only relevant to me yeah it's not

2:59:47.600 --> 2:59:50.880
 relevant to anybody else i'm not saying you should go out if you're the kind of person who

2:59:50.880 --> 2:59:55.360
 overthinks things to stop overthinking things it's not bad it doesn't work for me maybe it works for

2:59:55.360 --> 3:00:03.200
 you i you know i don't know let me ask you about love yeah uh so you i think last time we talked

3:00:03.200 --> 3:00:11.280
 about the meaning of life and it was it was kind of about winning of course uh i don't think i've

3:00:11.280 --> 3:00:17.280
 talked to you about love much whether romantic or just love for the common humanity amongst us all

3:00:17.280 --> 3:00:25.600
 what role has love played in your life in this in this quest for winning where does love fit in

3:00:26.160 --> 3:00:32.560
 well where love i think means several different things there's love in the sense of maybe i could

3:00:32.560 --> 3:00:38.480
 just say there's like love in the sense of opiates and love in the sense of oxytocin and then love

3:00:38.480 --> 3:00:47.840
 in the sense of maybe like a love for math i don't think fits into either those first two paradigms uh

3:00:48.880 --> 3:00:53.200
 so each of those have they uh have they have they given something to you

3:00:55.360 --> 3:01:00.880
 in your life i'm not that big of a fan of the first two um why

3:01:00.880 --> 3:01:08.880
 why the same reason i'm not a fan of you know for the same reason i don't do opiates and don't

3:01:08.880 --> 3:01:16.320
 take ecstasy right and there were times look i've tried both um i like opiates way more than

3:01:16.320 --> 3:01:25.280
 electric ecstasy uh but they're not the ethical life is the productive life so maybe that's my

3:01:25.280 --> 3:01:31.200
 problem with with with those and then like yeah a sense of i don't know like abstract love for humanity

3:01:32.000 --> 3:01:38.720
 i mean the abstract love for humanity i'm like yeah i've always felt that and i guess it's hard

3:01:38.720 --> 3:01:44.000
 for me to imagine not feeling it and maybe people who don't and i don't know but yeah that's just

3:01:44.000 --> 3:01:49.120
 like a background thing that's there i mean since we brought up uh drugs let me ask you

3:01:49.120 --> 3:01:55.360
 you this is becoming more and more part of my life because i'm talking to a few researchers

3:01:55.360 --> 3:02:02.000
 that work on psychedelics i've eaten shrooms a couple times and it was fascinating to me that

3:02:02.000 --> 3:02:08.880
 like the mind can go like just fascinating the mind can go to places i didn't imagine it

3:02:08.880 --> 3:02:14.960
 could go and i was very friendly and positive and exciting and everything was kind of hilarious

3:02:14.960 --> 3:02:19.760
 in in the in the place wherever my mind went that's where what is uh what do you think about

3:02:19.760 --> 3:02:25.840
 psychedelics do you think they have what do you think the mind goes have you done psychedelics

3:02:25.840 --> 3:02:30.560
 what do you where do you think the mind goes uh is there something useful to learn about the places

3:02:30.560 --> 3:02:38.880
 it goes once you come back you know i find it interesting that this idea that psychedelics

3:02:38.880 --> 3:02:44.800
 have something to teach is almost unique to psychedelics right people don't argue this about

3:02:44.800 --> 3:02:52.480
 amphetamines and that's true and i'm not really sure why yeah i think all of the drugs have lessons

3:02:52.480 --> 3:02:56.000
 to teach i think there's things to learn from opiates i think there's things to learn from

3:02:56.000 --> 3:02:59.120
 amphetamines i think there's things to learn from psychedelics things to learn from marijuana

3:03:00.000 --> 3:03:06.960
 um but also at the same time recognize that i don't think you're learning things about the

3:03:06.960 --> 3:03:12.880
 world i think you're learning things about yourself yes um and you know what's the even

3:03:12.880 --> 3:03:17.760
 i might have even been uh might have even been a timothy leary quote i don't know what's

3:03:17.760 --> 3:03:21.680
 called but the idea is basically like you know everybody should look behind the door but then

3:03:21.680 --> 3:03:27.200
 once you've seen behind the door you don't need to keep going back um so i mean and that's my

3:03:27.200 --> 3:03:34.320
 thoughts on on all real drug use too so maybe for caffeine it's a it's a little experience

3:03:34.320 --> 3:03:40.320
 that uh it's good to have but oh yeah no i mean yeah i guess yeah psychedelics are definitely

3:03:41.680 --> 3:03:46.160
 so you're a fan of new experiences i suppose yes because they all contain a little especially the

3:03:46.160 --> 3:03:52.320
 first few times it contains some lessons that can be picked up yeah and i'll i'll i'll revisit

3:03:52.320 --> 3:03:59.520
 psychedelics maybe once a year um usually small smaller doses maybe they turn up the learning

3:03:59.520 --> 3:04:05.440
 rate of your brain i've heard that like that yeah that's cool big learning rates have frozen

3:04:05.440 --> 3:04:11.440
 comms last question this is a little weird one but you've called yourself crazy in the past

3:04:12.640 --> 3:04:18.720
 uh first of all on a scale of one to ten how crazy would you say are you oh i mean it depends

3:04:18.720 --> 3:04:23.760
 how you you know when you compare me to ilan moskin i'd say levandowski not so crazy so like

3:04:23.760 --> 3:04:33.680
 like a seven let's go with six six six six what uh well i like seven seven's a good number seven

3:04:33.680 --> 3:04:40.320
 all right well yeah i'm sure day by day changes right so but you're in that in that area what uh

3:04:42.320 --> 3:04:46.560
 in thinking about that what do you think is the role of madness is that a feature

3:04:46.560 --> 3:04:56.960
 or a bug if you were to dissect your brain so okay from like a like mental health lens on crazy

3:04:56.960 --> 3:05:02.080
 i'm not sure i really believe in that i'm not sure i really believe in like a lot of that stuff

3:05:02.720 --> 3:05:08.720
 right this concept of okay you know when you get over to like like like like like hardcore bipolar

3:05:08.720 --> 3:05:14.000
 and schizophrenia these things are clearly real somewhat biological and then over here on the

3:05:14.000 --> 3:05:19.600
 spectrum you have like add and oppositional defiance disorder and these things that are like

3:05:20.640 --> 3:05:23.440
 wait this is normal spectrum human behavior like this isn't

3:05:25.920 --> 3:05:32.080
 you know where's the the line here and why is this like a problem so there's this whole

3:05:32.880 --> 3:05:37.760
 you know the neurodiversity of humanity is it's huge like people think i'm always on drugs people

3:05:37.760 --> 3:05:41.520
 are saying this to me on my streams and like guys you know like i'm real open with my drug use i'd

3:05:41.520 --> 3:05:46.400
 tell you if i was on drugs and i mean i had like a cup of coffee this morning but other than that

3:05:46.400 --> 3:05:53.680
 this is just me you're witnessing my brain in action you so so the word madness doesn't even

3:05:54.480 --> 3:06:03.200
 make sense and then you're in the rich neurodiversity of humans i think it makes sense but only for

3:06:03.200 --> 3:06:11.440
 our like some insane extremes like if you are actually like visibly hallucinating

3:06:13.200 --> 3:06:17.600
 you know that's okay but there is the kind of spectrum on which you stand out like

3:06:18.640 --> 3:06:23.760
 that that's uh like if i were to look you know at decorations on a christmas tree or

3:06:23.760 --> 3:06:30.000
 something like that like if you were a decoration out that would catch my eye like that thing is

3:06:30.000 --> 3:06:40.320
 sparkly whatever the hell that thing is uh there's something to that just like refusing to be um

3:06:41.040 --> 3:06:50.160
 boring or maybe boring is the wrong word but to um yeah i mean be willing to sparkle

3:06:51.200 --> 3:06:58.960
 you know it's it's like somewhat constructed i mean i am who i choose to be uh i'm gonna say

3:06:58.960 --> 3:07:05.600
 things as true as i can see them i'm not gonna i'm not gonna lie and but that's a really important

3:07:05.600 --> 3:07:11.600
 feature in itself so like whatever the neurodiversity of your whatever your brain is not putting

3:07:12.640 --> 3:07:18.000
 constraints on it that force it to to fit into the mold of what society is like

3:07:18.560 --> 3:07:22.400
 defines what you're supposed to be so you're one of the specimens that

3:07:22.400 --> 3:07:33.360
 that doesn't mind being yourself being right is super important except at the expense of being wrong

3:07:37.040 --> 3:07:41.120
 without breaking that apart i think it's a beautiful way to end it george you're one

3:07:41.120 --> 3:07:45.520
 of the most special humans i know it's truly an honor to talk to you thanks so much for doing it

3:07:45.520 --> 3:07:50.880
 thank you for having me thanks for listening to this conversation with george hotz and thank you

3:07:50.880 --> 3:07:58.480
 to our sponsors for sigmatic which is the maker of delicious mushroom coffee decoding digital

3:07:58.480 --> 3:08:05.680
 which is a tech podcast that i listen to and enjoy and express vpn which is the vpn i've used

3:08:05.680 --> 3:08:11.520
 for many years please check out these sponsors in the description to get a discount and to support

3:08:11.520 --> 3:08:16.880
 this podcast if you enjoy this thing subscribe on youtube review it with five stars and apple

3:08:16.880 --> 3:08:23.680
 podcast follow on spotify support on patreon or connect with me on twitter at lex freedman

3:08:24.320 --> 3:08:29.920
 and now let me leave you with some words from the great and powerful linus torvald

3:08:29.920 --> 3:08:45.840
 talk is cheap show me the code thank you for listening and hope to see you next time

