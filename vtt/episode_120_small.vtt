WEBVTT

00:00.000 --> 00:05.520
 The following is a conversation with Francois Chollet, his second time in the podcast.

00:05.520 --> 00:11.720
 He's both a world class engineer and a philosopher in the realm of deep learning and artificial

00:11.720 --> 00:13.320
 intelligence.

00:13.320 --> 00:19.000
 This time, we talk a lot about his paper titled On the Measure of Intelligence, that discusses

00:19.000 --> 00:24.840
 how we may define and measure general intelligence in our computing machinery.

00:24.840 --> 00:29.640
 Quick summary of the sponsors Babel, Masterclass, and Cash App.

00:29.640 --> 00:34.560
 Take the sponsor links in the description to get a discount and to support this podcast.

00:34.560 --> 00:39.640
 As a side note, let me say that the serious, rigorous, scientific study of artificial general

00:39.640 --> 00:42.280
 intelligence is a rare thing.

00:42.280 --> 00:47.840
 The mainstream machine learning community works on very narrow AI with very narrow benchmarks.

00:47.840 --> 00:53.280
 This is very good for incremental and sometimes big incremental progress.

00:53.280 --> 00:59.920
 On the other hand, the outside the mainstream, renegade, you could say, AGI community, works

00:59.920 --> 01:07.560
 on approaches that verge on the philosophical and even the literary without big public benchmarks.

01:07.560 --> 01:12.320
 Walking the line between the two worlds is a rare breed, but it doesn't have to be.

01:12.320 --> 01:17.840
 I ran the AGI series at MIT as an attempt to inspire more people to walk this line.

01:17.840 --> 01:23.400
 Keep mind and open AI for time and still, on occasion, walk this line.

01:23.400 --> 01:25.840
 Francois Chollet does as well.

01:25.840 --> 01:27.720
 I hope to also.

01:27.720 --> 01:31.600
 It's a beautiful dream to work towards and to make real one day.

01:31.600 --> 01:36.960
 If you enjoy this thing, subscribe on YouTube, review it with 5 stars on Apple Podcast, follow

01:36.960 --> 01:42.080
 on Spotify, support on Patreon, or connect with me on Twitter at Lex Freedman.

01:42.080 --> 01:45.480
 As usual, I'll do a few minutes of ads now and no ads in the middle.

01:45.480 --> 01:50.840
 I try to make these interesting, but I give you time stamps so you can skip.

01:50.840 --> 01:54.720
 But still, please do check out the sponsors by clicking the links in the description.

01:54.720 --> 01:58.040
 It's the best way to support this podcast.

01:58.040 --> 02:03.200
 This show is sponsored by Babel, an app and website that gets you speaking in a new language

02:03.200 --> 02:04.200
 within weeks.

02:04.200 --> 02:08.280
 Go to babel.com and use colex to get three months free.

02:08.280 --> 02:15.360
 They offer 14 languages, including Spanish, French, Italian, German, and yes, Russian.

02:15.360 --> 02:21.000
 Daily lessons are 10 to 15 minutes, super easy, effective, designed by over 100 language

02:21.000 --> 02:22.000
 experts.

02:22.000 --> 02:27.880
 Let me read a few lines from the Russian poem Noch Ulytso Fanaar Abteka by Alexander

02:27.880 --> 02:32.720
 Blok that you'll start to understand if you sign up to Babel.

02:32.720 --> 02:41.600
 Noch Ulytso Fanaar Abteka, Vesmyzynyi Itusklii Svet, Zhevii Shchoh Chetvirt Vekka, Vsebudit

02:41.600 --> 02:44.800
 Tak, Ishoda, Ned.

02:44.800 --> 02:51.360
 Now I say that you'll start to understand this poem because Russian starts with a language

02:51.360 --> 02:54.120
 and ends with a vodka.

02:54.120 --> 02:58.760
 Now the latter part is definitely not endorsed or provided by Babel and will probably lose

02:58.760 --> 03:02.600
 me this sponsorship, although it hasn't yet.

03:02.600 --> 03:06.720
 But once you graduate with Babel, you can enroll in my advanced course of late night

03:06.720 --> 03:09.320
 Russian conversation over vodka.

03:09.320 --> 03:11.400
 No app for that yet.

03:11.400 --> 03:18.280
 So get started by visiting babel.com and use code LEX to get 3 months free.

03:18.280 --> 03:21.200
 This show is also sponsored by Masterclass.

03:21.200 --> 03:26.080
 Sign up at masterclass.com slash lex to get a discount and to support this podcast.

03:26.080 --> 03:30.000
 When I first heard about Masterclass, I thought it was too good to be true.

03:30.000 --> 03:32.440
 I still think it's too good to be true.

03:32.440 --> 03:38.760
 For $180 a year, you get an all access pass to watch courses from to list some of my favorites

03:38.760 --> 03:43.640
 Chris Hatfield on Space Exploration, hope to have him in this podcast one day, Neil

03:43.640 --> 03:49.440
 Lugras Tyson on Scientific Thinking and Communication, Neil too, Will Wright, creator of SimCity

03:49.440 --> 03:55.440
 and Sims on Game Design, Carl Santana on Guitar, Karek Asparov on Chess, Daniel Nagrano on

03:55.440 --> 03:57.280
 Poker and many more.

03:57.280 --> 04:01.240
 Chris Hatfield explaining how rockets work and the experience of being watched at the

04:01.240 --> 04:03.320
 space alone is worth the money.

04:03.320 --> 04:06.640
 By the way, you can watch it on basically any device.

04:06.640 --> 04:13.400
 Once again, sign up at masterclass.com slash lex to get a discount and to support this podcast.

04:13.400 --> 04:18.800
 This show finally is presented by Cash App, the number one finance app in the App Store.

04:18.800 --> 04:24.240
 When you get it, use code LEX Podcast, Cash App lets you send money to friends, buy bitcoin

04:24.240 --> 04:27.360
 and invest in the stock market with as little as $1.

04:27.360 --> 04:31.640
 Since Cash App allows you to send and receive money digitally, let me mention a surprising

04:31.640 --> 04:33.920
 fact related to physical money.

04:33.920 --> 04:39.360
 Of all the currency in the world, roughly 8% of it is actually physical money.

04:39.360 --> 04:45.360
 The other 92% of the money only exists digitally and that's only going to increase.

04:45.360 --> 04:50.760
 So again, if you get Cash App from the App Store, Google Play and use code LEX Podcast,

04:50.760 --> 04:55.720
 you get $10 and Cash App will also donate $10 the first, an organization that is helping

04:55.720 --> 05:00.600
 to advance robotics and STEM education for young people around the world.

05:00.600 --> 05:05.200
 And now, here's my conversation with Francois Chalet.

05:05.200 --> 05:10.760
 What philosophers, thinkers or ideas had a big impact on you growing up and today?

05:10.760 --> 05:17.840
 So one author that had a big impact on me when I read his books as a teenager was Jean

05:17.840 --> 05:25.400
 Piaget, who is a Swiss psychologist, is considered to be the father of developmental psychology

05:25.400 --> 05:33.640
 and he has a large body of work about basically how intelligence develops in children.

05:33.640 --> 05:39.760
 And so it's very old work, like most of it is from the 1930s, 1940s, so it's not quite

05:39.760 --> 05:40.760
 up to date.

05:40.760 --> 05:45.760
 It's actually superseded by many newer developments in developmental psychology.

05:45.760 --> 05:50.920
 But to me, it was very, very interesting, very striking and actually shaped the early

05:50.920 --> 05:55.480
 ways in which I started thinking about the mind and the development of intelligence as

05:55.480 --> 05:56.480
 a teenager.

05:56.480 --> 06:00.120
 His actual ideas or the way he thought about it or just the fact that you could think about

06:00.120 --> 06:01.520
 the developing mind at all?

06:01.520 --> 06:02.520
 I guess both.

06:02.520 --> 06:07.520
 Jean Piaget is the author that's really introduced me to the notion that intelligence and the

06:07.520 --> 06:14.280
 mind is something that you construct throughout your life and that the children construct

06:14.280 --> 06:15.840
 it in stages.

06:15.840 --> 06:20.520
 And I thought that was a very interesting idea, which is, of course, very relevant to AI,

06:20.520 --> 06:23.440
 to building artificial minds.

06:23.440 --> 06:30.520
 Another book that I read around the same time that had a big impact on me, and there was

06:30.520 --> 06:33.920
 actually a little bit of overlap with Jean Piaget as well, and I read it around the same

06:33.920 --> 06:40.040
 time, is Jeff Hawkins on Intelligence, which is a classic.

06:40.040 --> 06:48.040
 And he has this vision of the mind as a multiscale hierarchy of temporal prediction modules.

06:48.040 --> 06:59.440
 And these ideas really resonated with me, the notion of a modular hierarchy of compression

06:59.440 --> 07:01.680
 functions or prediction functions.

07:01.680 --> 07:07.240
 I thought it was really, really interesting, and it shaped the way I started thinking about

07:07.240 --> 07:10.360
 how to build minds.

07:10.360 --> 07:14.000
 The hierarchical nature, which aspect?

07:14.000 --> 07:15.000
 Also he's a neuroscientist.

07:15.000 --> 07:20.360
 He was thinking, he was basically talking about how our mind works.

07:20.360 --> 07:25.080
 Yeah, the notion that cognition is prediction was an idea that was kind of new to me at

07:25.080 --> 07:28.160
 the time, and that I really loved at the time.

07:28.160 --> 07:35.400
 And yeah, the notion that there are multiple scales of processing in the brain.

07:35.400 --> 07:36.400
 The hierarchy.

07:36.400 --> 07:37.400
 Yes.

07:37.400 --> 07:38.400
 This is before deep learning.

07:38.400 --> 07:44.520
 These ideas of hierarchies in AI have been around for a long time, even before on intelligence

07:44.520 --> 07:49.120
 I mean, they've been around since the 1980s.

07:49.120 --> 07:50.560
 And yeah, that was before deep learning.

07:50.560 --> 07:56.760
 But of course, I think these ideas really found their practical implementation in deep

07:56.760 --> 07:57.760
 learning.

07:57.760 --> 07:59.680
 What about the memory side of things?

07:59.680 --> 08:02.240
 I think he was talking about knowledge representation.

08:02.240 --> 08:04.560
 Do you think about memory a lot?

08:04.560 --> 08:10.880
 One way you can think of neural networks as a kind of memory, you're memorizing things,

08:10.880 --> 08:17.440
 but it doesn't seem to be the kind of memory that's in our brains, or it doesn't have the

08:17.440 --> 08:20.880
 same rich complexity, long term nature that's in our brains.

08:20.880 --> 08:21.880
 Yes.

08:21.880 --> 08:27.680
 The brain is more of a sparse access memory so that you can actually retrieve very precisely

08:27.680 --> 08:30.280
 like bits of your experience.

08:30.280 --> 08:36.560
 The retrieval aspect, you can like introspect, you can ask yourself questions, I guess.

08:36.560 --> 08:41.680
 You can program your own memory, and language is actually the tool you used to do that.

08:41.680 --> 08:46.480
 I think language is a kind of operating system for the mind.

08:46.480 --> 08:52.920
 And you use language, well, one of the uses of language is as a query that you run over

08:52.920 --> 08:59.080
 your own memory, use words as keys to retrieve specific experiences or specific concepts,

08:59.080 --> 09:00.080
 specific thoughts.

09:00.080 --> 09:04.680
 Like language is a way you store thoughts, not just in writing, in the physical world,

09:04.680 --> 09:06.280
 but also in your own mind.

09:06.280 --> 09:10.200
 And it's also how you retrieve them, like imagine if you didn't have language, then

09:10.200 --> 09:17.760
 you would have to, you would not really have a self internally triggered way of retrieving

09:17.760 --> 09:18.760
 past thoughts.

09:18.760 --> 09:21.360
 You would have to rely on external experiences.

09:21.360 --> 09:25.800
 For instance, you see a specific site, you smell a specific smell, and that brings up

09:25.800 --> 09:31.800
 memories, but you would not really have a way to deliberately access these memories

09:31.800 --> 09:32.800
 without language.

09:32.800 --> 09:37.520
 Well, the interesting thing you mentioned is you can also program the memory.

09:37.520 --> 09:39.840
 You can change it probably with language.

09:39.840 --> 09:41.320
 Yeah, using language.

09:41.320 --> 09:42.320
 Yes.

09:42.320 --> 09:46.760
 Well, let me ask you a Chomsky question, which is like, first of all, do you think language

09:46.760 --> 09:54.520
 is like fundamental, like there's turtles, what's at the bottom of the turtles?

09:54.520 --> 09:59.240
 They don't go, it can't be turtles all the way down, is language at the bottom of cognition

09:59.240 --> 10:09.200
 of everything is like language, the fundamental aspect of like what it means to be a thinking

10:09.200 --> 10:10.200
 thing.

10:10.200 --> 10:12.040
 No, I don't think so.

10:12.040 --> 10:13.040
 I think language is...

10:13.040 --> 10:14.640
 You disagree with Norm Chomsky?

10:14.640 --> 10:15.640
 Yes.

10:15.640 --> 10:18.040
 In language is a layer on top of cognition.

10:18.040 --> 10:24.560
 So it is fundamental to cognition in the sense that to use a computing metaphor, I see language

10:24.560 --> 10:29.120
 as the operating system of the brain, of the human mind.

10:29.120 --> 10:30.120
 Yeah.

10:30.120 --> 10:33.280
 And the operating system is a layer on top of the computer.

10:33.280 --> 10:37.560
 The computer exists before the operating system, but the operating system is how you

10:37.560 --> 10:39.680
 make it truly useful.

10:39.680 --> 10:45.840
 And the operating system is most likely Windows, not Linux, because its language is messy.

10:45.840 --> 10:53.280
 Yeah, it's messy and it's pretty difficult to inspect it, introspect it.

10:53.280 --> 10:56.280
 How do you think about language?

10:56.280 --> 11:03.200
 We use actually sort of human interpretable language, but is there something deeper that's

11:03.200 --> 11:08.880
 closer to like logical type of statements?

11:08.880 --> 11:16.320
 Yeah, what is the nature of language, do you think?

11:16.320 --> 11:19.200
 Is there something deeper than like the syntactic rules we construct?

11:19.200 --> 11:25.640
 Is there something that doesn't require utterances or writing or so on?

11:25.640 --> 11:31.000
 Now you're asking about the possibility that there could exist languages for thinking that

11:31.000 --> 11:32.920
 are not made of words?

11:32.920 --> 11:35.480
 Yeah, I think so.

11:35.480 --> 11:38.680
 So the mind is layers, right?

11:38.680 --> 11:44.760
 And language is almost like the uttermost, the uppermost layer.

11:44.760 --> 11:51.560
 But before we think in words, I think we think in terms of emotion in space and we think

11:51.560 --> 11:53.520
 in terms of physical actions.

11:53.520 --> 12:00.920
 And I think babies in particular probably express its thoughts in terms of the actions

12:00.920 --> 12:07.320
 that they've seen or that they can perform and in terms of the emotions of objects in

12:07.320 --> 12:10.560
 the environment before they start thinking in terms of words.

12:10.560 --> 12:18.400
 It's amazing to think about that as the building blocks of language, so like the kind of actions

12:18.400 --> 12:25.800
 and ways the babies see the world as like more fundamental than the beautiful Shakespearean

12:25.800 --> 12:28.840
 language you construct on top of it.

12:28.840 --> 12:33.320
 And we probably don't have any idea what that looks like, right?

12:33.320 --> 12:38.480
 Because it's important for them trying to engineer it into AI systems.

12:38.480 --> 12:46.840
 I think visual analogies and motion is a fundamental building block of the mind and you actually

12:46.840 --> 12:51.960
 see it reflected in language, like language is full of special metaphors.

12:51.960 --> 12:57.840
 And when you think about things, I consider myself very much as a visual thinker.

12:57.840 --> 13:08.280
 You often express its thoughts by using things like visualizing concepts into the space or

13:08.280 --> 13:15.040
 like you solve problems by imagining yourself, navigating a concept space.

13:15.040 --> 13:18.080
 I don't know if you have this sort of experience.

13:18.080 --> 13:20.960
 You said visualizing concept space.

13:20.960 --> 13:30.040
 So I certainly think about, I certainly visualized mathematical concepts, but you mean like in

13:30.040 --> 13:37.360
 concept space visually you're embedding ideas into three dimensional space you can explore

13:37.360 --> 13:38.360
 with your mind essentially.

13:38.360 --> 13:40.600
 You should be more like 2D, but yeah.

13:40.600 --> 13:41.600
 2D?

13:41.600 --> 13:42.600
 Yeah.

13:42.600 --> 13:43.600
 You're a flatlander.

13:43.600 --> 13:45.600
 Okay.

13:45.600 --> 13:49.640
 No, I do not.

13:49.640 --> 13:57.120
 I always have to, before I jump from concept to concept, I have to put it back down.

13:57.120 --> 13:58.120
 It has to be on paper.

13:58.120 --> 14:03.480
 I can only travel on 2D paper, not inside my mind.

14:03.480 --> 14:05.080
 You're able to move inside your mind.

14:05.080 --> 14:10.040
 And even if you're writing like a paper, for instance, don't you have like a spatial representation

14:10.040 --> 14:12.160
 of your paper?

14:12.160 --> 14:19.360
 Like you visualize where ideas lie topologically in relationship to other ideas, kind of like

14:19.360 --> 14:22.560
 a software map of the ideas in your paper.

14:22.560 --> 14:23.560
 Yeah.

14:23.560 --> 14:24.560
 That's true.

14:24.560 --> 14:33.240
 I mean, there is, in papers, I don't know about you, but there feels like there's a destination.

14:33.240 --> 14:39.560
 There's a key idea that you want to write that and a lot of it is in the fog and you're

14:39.560 --> 14:40.920
 trying to kind of...

14:40.920 --> 14:45.520
 It's almost like...

14:45.520 --> 14:50.440
 What's that called when you do a path planning search from both directions from the start

14:50.440 --> 14:52.840
 and from the end?

14:52.840 --> 14:58.080
 And then you find, you do like shortest path, but like, in game playing, you do this with

14:58.080 --> 15:00.720
 like A star from both sides.

15:00.720 --> 15:03.240
 And you see where they join.

15:03.240 --> 15:04.240
 Yeah.

15:04.240 --> 15:08.160
 So you kind of do, at least for me, I think like, first of all, just exploring from the

15:08.160 --> 15:12.520
 start from like, first principles, what do I know?

15:12.520 --> 15:15.760
 What can I start proving from that, right?

15:15.760 --> 15:23.840
 And then from the destination, if I use their backtracking, like, if I want to show some

15:23.840 --> 15:27.880
 kind of sets of ideas, what would it take to show them and kind of backtrack?

15:27.880 --> 15:32.160
 But like, yeah, I don't think I'm doing all that in my mind though, like putting it down

15:32.160 --> 15:33.160
 on paper.

15:33.160 --> 15:35.360
 Do you use mind maps to organize your ideas?

15:35.360 --> 15:36.360
 No.

15:36.360 --> 15:37.360
 Yeah.

15:37.360 --> 15:38.360
 Yeah.

15:38.360 --> 15:42.040
 Let's get into this because it's, I've been so jealous of people, I haven't really tried

15:42.040 --> 15:43.040
 it.

15:43.040 --> 15:47.520
 I've been jealous of people that seem to like, they get like this fire of passion in their

15:47.520 --> 15:50.080
 eyes because everything starts making sense.

15:50.080 --> 15:54.520
 It's like Tom Cruise in the movie was like moving stuff around some of the most brilliant

15:54.520 --> 15:55.880
 people I know use mind maps.

15:55.880 --> 15:57.240
 I haven't tried really.

15:57.240 --> 16:00.880
 Can you explain what the hell a mind map is?

16:00.880 --> 16:07.240
 I guess a mind map is the way to make kind of like the mess inside your mind to just

16:07.240 --> 16:10.160
 put it on paper so that you gain more control over it.

16:10.160 --> 16:17.160
 It's the way to organize things on paper and as kind of like a consequence of organizing

16:17.160 --> 16:20.280
 things on paper, it starts being more organized inside your mind.

16:20.280 --> 16:21.600
 So what does that look like?

16:21.600 --> 16:26.360
 You put, like, do you have an example, like, what do you, what's the first thing you write

16:26.360 --> 16:27.360
 on paper?

16:27.360 --> 16:28.760
 What's the second thing you write?

16:28.760 --> 16:34.720
 I mean, typically, you draw a mind map to organize the way you think about the topic.

16:34.720 --> 16:39.880
 So you would start by writing down like the key concept about that topic, like you would

16:39.880 --> 16:42.320
 write intelligence or something.

16:42.320 --> 16:45.680
 And then you would start adding associative connections.

16:45.680 --> 16:48.120
 Like what do you think about when you think about intelligence?

16:48.120 --> 16:50.480
 What do you think are the key elements of intelligence?

16:50.480 --> 16:53.440
 So maybe you would have language, for instance, and you would have motion.

16:53.440 --> 16:55.520
 And so you would start drawing notes with these things.

16:55.520 --> 16:59.160
 And then you would see what do you think about when you think about motion and so on.

16:59.160 --> 17:00.960
 And you would go like that, like a tree.

17:00.960 --> 17:05.720
 Is it a tree or a tree most or is it a graph too, like a tree?

17:05.720 --> 17:09.840
 Oh, it's more of a graph than a tree.

17:09.840 --> 17:13.280
 And it's not limited to just writing down words.

17:13.280 --> 17:16.200
 You can also draw things.

17:16.200 --> 17:19.720
 And it's not supposed to be purely hierarchical, right?

17:19.720 --> 17:24.960
 Like you can, the point is that you can start, once you start writing it down, you can start

17:24.960 --> 17:29.520
 reorganizing it so that it makes more sense, so that it's connected in a more effective

17:29.520 --> 17:30.520
 way.

17:30.520 --> 17:37.000
 See, but I'm so OCD that you just mentioned intelligence and language and motion.

17:37.000 --> 17:42.120
 I'll start becoming paranoid that the categorization isn't perfect.

17:42.120 --> 17:50.080
 Like that I would become paralyzed with the mind map that like this may not be so like

17:50.080 --> 17:56.960
 the, even though you're just doing associative kind of connections, there's an implied hierarchy

17:56.960 --> 17:58.600
 that's emerging.

17:58.600 --> 18:01.880
 And I would start becoming paranoid that it's not the proper hierarchy.

18:01.880 --> 18:07.200
 So you're not just one way to see mind maps is you're putting thoughts on paper.

18:07.200 --> 18:10.680
 It's like a stream of consciousness.

18:10.680 --> 18:12.400
 But then you can also start getting paranoid.

18:12.400 --> 18:17.520
 Well, if it's just the right hierarchy, sure, which is, but it's a mind map, it's your mind

18:17.520 --> 18:20.920
 map, you're free to draw anything you want, you're free to draw any connection you want.

18:20.920 --> 18:25.400
 And you can just make a different mind map if you think the central node is not the right

18:25.400 --> 18:26.400
 node.

18:26.400 --> 18:30.000
 Yeah, I suppose there's a fear of being wrong.

18:30.000 --> 18:36.840
 If you want to organize your ideas by writing down what you think, which I think is very

18:36.840 --> 18:37.840
 effective.

18:37.840 --> 18:43.000
 Like, how do you know what you think about something if you don't write it down, right?

18:43.000 --> 18:50.040
 If you do that, the thing is that it imposes much more syntactic structure over your ideas,

18:50.040 --> 18:51.600
 which is not required with a mind map.

18:51.600 --> 18:58.000
 So a mind map is kind of like a lower level, more freehand way of organizing your thoughts.

18:58.000 --> 19:04.360
 And once you've drawn it, then you can start actually voicing your thoughts in terms of,

19:04.360 --> 19:05.360
 you know, paragraphs.

19:05.360 --> 19:08.400
 It's a two dimensional aspect of layout too, right?

19:08.400 --> 19:09.400
 Yeah.

19:09.400 --> 19:14.240
 And it's a kind of flower, I guess, you start, there's usually, you want to start with a

19:14.240 --> 19:15.240
 central concept?

19:15.240 --> 19:16.240
 Yes.

19:16.240 --> 19:17.240
 And you move out.

19:17.240 --> 19:21.240
 Typically, it ends up more like a subway map, so it ends up more like a graph, a topological

19:21.240 --> 19:22.240
 graph.

19:22.240 --> 19:23.240
 Without a root node.

19:23.240 --> 19:27.160
 Yeah, so like in a subway map, there are some nodes that are more connected than others

19:27.160 --> 19:30.360
 and there are some nodes that are more important than others, right?

19:30.360 --> 19:36.160
 So there are destinations, but it's not going to be purely like a tree, for instance.

19:36.160 --> 19:41.080
 Yeah, it's fascinating to think that if there's something to that about our, about the way

19:41.080 --> 19:42.080
 our mind thinks.

19:42.080 --> 19:48.960
 By the way, I just kind of remembered obvious thing that I have probably thousands of documents

19:48.960 --> 19:56.400
 in Google Doc at this point that are bullet point lists, which is you can probably map

19:56.400 --> 20:01.560
 a mind map to a bullet point list.

20:01.560 --> 20:02.560
 It's the same.

20:02.560 --> 20:03.560
 It's a, no, it's not.

20:03.560 --> 20:04.560
 It's a tree.

20:04.560 --> 20:05.560
 It's a tree.

20:05.560 --> 20:06.560
 Yeah.

20:06.560 --> 20:10.880
 So I create trees, but also they don't have the visual element.

20:10.880 --> 20:13.520
 Like I guess I'm comfortable with the structure.

20:13.520 --> 20:18.320
 It feels like the narrowness, the constraints feel more comforting.

20:18.320 --> 20:23.800
 If you have thousands of documents with your own thoughts in Google Docs, why don't you

20:23.800 --> 20:31.360
 write some kind of search engine, like maybe a mind map, a piece of software, a mind mapping

20:31.360 --> 20:37.440
 software where you write down a concept and then it gives you sentences or paragraphs

20:37.440 --> 20:41.200
 from your thousands Google Docs document that match this concept.

20:41.200 --> 20:48.600
 The problem is it's so deeply unlike mind maps, it's so deeply rooted in natural language.

20:48.600 --> 20:57.280
 So it's not, it's not semantically searchable, I would say, because the categories are very,

20:57.280 --> 21:02.640
 you kind of mention intelligence, language and motion, they're very strong, semantic,

21:02.640 --> 21:09.800
 like it feels like the mind map forces you to be semantically clear and specific.

21:09.800 --> 21:20.240
 The bullet points list I have are sparse, disparate thoughts that poetically represent

21:20.240 --> 21:25.360
 a category like motion as opposed to saying motion.

21:25.360 --> 21:29.800
 So unfortunately, that's the same problem with the internet, that's why the idea of

21:29.800 --> 21:32.480
 semantic web is difficult to get.

21:32.480 --> 21:42.600
 It's most language on the internet is a giant mess of natural language that's hard to interpret.

21:42.600 --> 21:47.320
 So do you think there's something to mind maps as, you actually originally brought it

21:47.320 --> 21:54.360
 up as when we're talking about kind of cognition and language, do you think there's something

21:54.360 --> 22:01.840
 to mind maps about how our brain actually deals, like, think reasons about things?

22:01.840 --> 22:02.840
 It's possible.

22:02.840 --> 22:10.080
 I think it's reasonable to assume that there is some level of topological processing in

22:10.080 --> 22:15.480
 the brain, that the brain is very associative in nature.

22:15.480 --> 22:25.520
 And I also believe that a topological space is a better medium to include thoughts than

22:25.520 --> 22:27.640
 a geometric space.

22:27.640 --> 22:28.640
 So I think...

22:28.640 --> 22:31.240
 What's the difference in a topological and a geometric space?

22:31.240 --> 22:36.280
 Well, if you're talking about topologies, then points are either connected or not.

22:36.280 --> 22:39.000
 So the topology is more like a subway map.

22:39.000 --> 22:44.000
 And geometry is when you're interested in the distance between things.

22:44.000 --> 22:47.280
 In subway maps, you don't really have the concept of distance, you only have the concept

22:47.280 --> 22:53.120
 of whether there is a train going from station A to station B.

22:53.120 --> 22:57.800
 And what we do in deep learning is that we're actually dealing with geometric spaces, we

22:57.800 --> 23:03.800
 are dealing with concept vectors, word vectors, that have a distance between the distance

23:03.800 --> 23:06.680
 and the product.

23:06.680 --> 23:10.720
 We are not really building topological models, usually.

23:10.720 --> 23:12.480
 I think you're absolutely right.

23:12.480 --> 23:16.480
 Distance is a fundamental importance in deep learning.

23:16.480 --> 23:20.120
 I mean, it's the continuous aspect of it.

23:20.120 --> 23:23.120
 Because everything is a vector, and everything has to be a vector because everything has

23:23.120 --> 23:24.120
 to be differentiable.

23:24.120 --> 23:27.760
 If your space is discrete, it's no longer differentiable, you cannot do deep learning

23:27.760 --> 23:28.760
 in it anymore.

23:28.760 --> 23:35.720
 Well, you could, but you could only do it by embedding it in a bigger, continuous space.

23:35.720 --> 23:40.520
 So if you do topology in the context of deep learning, you have to do it by embedding your

23:40.520 --> 23:42.320
 topology in a geometry.

23:42.320 --> 23:46.360
 Well, let me zoom out for a second.

23:46.360 --> 23:52.520
 Let's get into your paper on the measure of intelligence that, did you put it on 2019?

23:52.520 --> 23:53.520
 Yes.

23:53.520 --> 23:54.520
 Okay.

23:54.520 --> 23:55.520
 November.

23:55.520 --> 23:56.520
 November.

23:56.520 --> 23:57.520
 Yeah.

23:57.520 --> 23:58.520
 Remember 2019?

23:58.520 --> 24:01.080
 That was a different time.

24:01.080 --> 24:02.080
 Yeah.

24:02.080 --> 24:03.080
 I remember.

24:03.080 --> 24:06.640
 I still remember.

24:06.640 --> 24:09.680
 It feels like a different world.

24:09.680 --> 24:14.560
 You could travel, or you could actually go outside and see friends.

24:14.560 --> 24:15.560
 Yeah.

24:15.560 --> 24:18.960
 Let me ask the most absurd question.

24:18.960 --> 24:24.320
 I think there's some nonzero probability there'll be a textbook one day, like 200 years from

24:24.320 --> 24:31.080
 now on artificial intelligence, or it'll be called like just intelligence because humans

24:31.080 --> 24:34.680
 will already be gone, and it'll be your picture with a quote.

24:34.680 --> 24:41.840
 This is, you know, one of the early biological systems would consider the nature of intelligence,

24:41.840 --> 24:45.680
 and there'll be like a definition of how they thought about intelligence, which is one of

24:45.680 --> 24:51.800
 the things you do in your paper on measure intelligence is to ask like, well, what is

24:51.800 --> 24:55.680
 intelligence and how to test for intelligence and so on.

24:55.680 --> 25:02.040
 So is there a spiffy quote about what is intelligence?

25:02.040 --> 25:05.760
 What is the definition of intelligence, according to your friends, Warshale?

25:05.760 --> 25:06.760
 Yeah.

25:06.760 --> 25:14.000
 So do you think the superintended AIs of the future will want to remember us?

25:14.000 --> 25:19.480
 The way we remember humans from the past, and do you think they won't be ashamed of

25:19.480 --> 25:21.480
 having a biological origin?

25:21.480 --> 25:24.760
 No, I think it'll be a niche topic.

25:24.760 --> 25:30.960
 It won't be that interesting, but it'll be like the people that study in certain contexts

25:30.960 --> 25:36.680
 like historical civilization that no longer exists, the Aztecs and so on.

25:36.680 --> 25:42.400
 That's how it'll be seen, and it'll be study in also the context on social media.

25:42.400 --> 25:51.280
 There will be hashtags about the atrocity committed to human beings when the robots

25:51.280 --> 25:54.400
 finally got rid of them.

25:54.400 --> 25:55.400
 It was a mistake.

25:55.400 --> 26:00.880
 It'll be seen as a giant mistake, but ultimately in the name of progress, and it created a

26:00.880 --> 26:07.120
 better world because humans were over consuming the resources and they were not very rational

26:07.120 --> 26:12.560
 and were destructive in the end, in terms of productivity, and putting more love in

26:12.560 --> 26:13.880
 the world.

26:13.880 --> 26:17.320
 And so within that context, there'll be a chapter about these biological systems.

26:17.320 --> 26:20.360
 It seems to have a very detailed vision of that feature.

26:20.360 --> 26:22.800
 You should write a sci fi novel about it.

26:22.800 --> 26:27.080
 I'm working on a sci fi novel currently, yes.

26:27.080 --> 26:28.080
 Yeah, so.

26:28.080 --> 26:29.440
 Self published, yeah.

26:29.440 --> 26:37.560
 The definition of intelligence, so intelligence is the efficiency with which you acquire new

26:37.560 --> 26:43.800
 skills at tasks that you did not previously know about, that you did not prepare for,

26:43.800 --> 26:44.800
 right?

26:44.800 --> 26:47.920
 So intelligence is not skill itself.

26:47.920 --> 26:50.880
 It's not what you know, it's not what you can do.

26:50.880 --> 26:54.760
 It's how well and how efficiently you can learn new things.

26:54.760 --> 26:55.760
 New things.

26:55.760 --> 26:56.760
 Yes.

26:56.760 --> 27:00.360
 The idea of newness there seems to be fundamentally important.

27:00.360 --> 27:01.600
 Yes.

27:01.600 --> 27:08.240
 So you would see intelligence on display, for instance, whenever you see a human being

27:08.240 --> 27:14.800
 or an AI creature adapt to a new environment that it does not see before, that its creators

27:14.800 --> 27:16.920
 did not anticipate.

27:16.920 --> 27:22.560
 When you see adaptation, when you see improvisation, when you see generalization, that's intelligence.

27:22.560 --> 27:27.200
 In reverse, if you have a system that when you put it in a slightly new environment,

27:27.200 --> 27:34.920
 it cannot adapt, it cannot improvise, it cannot deviate from what it's hardcoded to do or

27:34.920 --> 27:40.560
 what it has been trained to do, that is a system that is not intelligence.

27:40.560 --> 27:47.200
 There's actually a quote from Einstein that captures this idea, which is, the measure

27:47.200 --> 27:50.800
 of intelligence is the ability to change.

27:50.800 --> 27:51.800
 I like that quote.

27:51.800 --> 27:54.520
 I think it captures at least part of this idea.

27:54.520 --> 27:58.520
 You know, there might be something interesting about the difference between your definition

27:58.520 --> 27:59.520
 of Einstein's.

27:59.520 --> 28:12.040
 I mean, he's just being Einstein and clever, but acquisition of new ability to deal with

28:12.040 --> 28:17.520
 new things versus ability to just change.

28:17.520 --> 28:20.080
 What's the difference between those two things?

28:20.080 --> 28:22.400
 To just change in itself.

28:22.400 --> 28:24.640
 Do you think there's something to that?

28:24.640 --> 28:26.520
 Just being able to change.

28:26.520 --> 28:28.600
 Yes, being able to adapt.

28:28.600 --> 28:33.360
 So not change, but certainly a change in direction.

28:33.360 --> 28:37.760
 Being able to adapt yourself to your environment.

28:37.760 --> 28:38.760
 Whatever the environment is.

28:38.760 --> 28:41.720
 That's a big part of intelligence, yes.

28:41.720 --> 28:46.520
 Intelligence is most precisely how efficiently you're able to adapt, how efficiently you're

28:46.520 --> 28:52.760
 able to basically master your environment, how efficiently you can acquire new skills.

28:52.760 --> 28:59.520
 And I think there's a big distinction to be drawn between intelligence, which is a process

28:59.520 --> 29:04.920
 and the output of that process, which is skill.

29:04.920 --> 29:09.960
 So for instance, if you have a very smart human programmer that considers the game of

29:09.960 --> 29:16.480
 chess and that writes down a static program that can play chess.

29:16.480 --> 29:20.680
 Then the intelligence is the process of developing that program.

29:20.680 --> 29:28.080
 But the program itself is just encoding the output artifact of that process.

29:28.080 --> 29:30.120
 The program itself is not intelligent.

29:30.120 --> 29:34.000
 And the way you tell it's not intelligent is that if you put it in a different context,

29:34.000 --> 29:38.000
 you ask it to play go or something, it's not going to be able to perform well with that

29:38.000 --> 29:43.080
 human involvement because the source of intelligence, the entity that is capable of that process

29:43.080 --> 29:44.440
 is the human programmer.

29:44.440 --> 29:50.120
 So we should be able to tell a difference between the process and its output.

29:50.120 --> 29:53.320
 We should not confuse the output and the process.

29:53.320 --> 30:00.320
 It's the same as do not confuse a road building company and one specific road.

30:00.320 --> 30:04.840
 Because one specific road takes you from point A to point B. But a road building company

30:04.840 --> 30:08.800
 can take you from, can make a path from anywhere to anywhere else.

30:08.800 --> 30:17.120
 Yeah, that's beautifully put, but it's also to play devil's advocate a little bit.

30:17.120 --> 30:21.360
 It's possible that there is something more fundamental than us humans.

30:21.360 --> 30:30.040
 So you said the programmer creates the difference between the choir of the skill and the skill

30:30.040 --> 30:31.040
 itself.

30:31.040 --> 30:37.440
 There could be something like you could argue the universe is more intelligent, like the

30:37.440 --> 30:44.520
 deep, the base intelligence that we should be trying to measure is something that created

30:44.520 --> 30:53.560
 humans should be measuring God or the source of the universe as opposed to like there could

30:53.560 --> 30:57.120
 be a deeper intelligence, there's always deeper intelligence.

30:57.120 --> 31:01.120
 You can argue that, but that does not take anything away from the fact that humans are

31:01.120 --> 31:08.000
 intelligent and you can tell that because they are capable of adaptation and generality.

31:08.000 --> 31:15.920
 And you see that in particular in the fact that humans are capable of handling situations

31:15.920 --> 31:23.000
 and tasks that are quite different from anything that any of our evolutionary ancestors has

31:23.000 --> 31:24.000
 ever encountered.

31:24.000 --> 31:29.760
 So we are capable of generalizing very much out of distribution if you consider our evolutionary

31:29.760 --> 31:32.480
 history as being in a way out of training data.

31:32.480 --> 31:37.800
 Of course evolutionary biologists would argue that we're not going too far out of the distribution.

31:37.800 --> 31:43.240
 We're like mapping the skills we've learned previously, desperately trying to like jam

31:43.240 --> 31:46.120
 them into like these new situations.

31:46.120 --> 31:51.320
 I mean, there's definitely a little bit of that, but it's pretty clear to me that we're

31:51.320 --> 31:58.240
 able to, you know, most of the things we do any given day in our modern civilization are

31:58.240 --> 32:04.040
 things that are very, very different from what our ancestors a million years ago would

32:04.040 --> 32:06.080
 have been doing in a given day.

32:06.080 --> 32:07.680
 And our environment is very different.

32:07.680 --> 32:15.840
 So I agree that everything we do, we do it with cognitive building blocks that we acquire

32:15.840 --> 32:22.840
 over the course of evolution, and that anchors our cognition to certain contexts, which is

32:22.840 --> 32:25.400
 the human condition very much.

32:25.400 --> 32:31.200
 But still, our mind is capable of a pretty remarkable degree of generality far beyond

32:31.200 --> 32:37.080
 anything we can create in artificial systems today, like the degree in which the mind can

32:37.080 --> 32:44.000
 generalize from its evolutionary history, can generalize away from its evolutionary history

32:44.000 --> 32:49.440
 is much greater than the degree to which a deep learning system today can generalize

32:49.440 --> 32:51.240
 away from its training data.

32:51.240 --> 32:57.120
 And the key point you're making, which I think is quite beautiful, is we shouldn't measure,

32:57.120 --> 33:01.720
 if we talk about measurement, we shouldn't measure the skill.

33:01.720 --> 33:06.880
 We should measure the creation of the new skill, the ability to create that new skill.

33:06.880 --> 33:10.560
 But it's tempting.

33:10.560 --> 33:16.520
 It's weird because the skill is a little bit of a small window into the system.

33:16.520 --> 33:21.280
 So whenever you have a lot of skills, it's tempting to measure the skills.

33:21.280 --> 33:22.280
 Yes.

33:22.280 --> 33:26.240
 I mean, the skill is the only thing you can objectively measure.

33:26.240 --> 33:36.680
 But yeah, so the thing to keep in mind is that when you see skill in the human, it gives

33:36.680 --> 33:41.360
 you a strong signal that human is intelligent because you know they weren't born with that

33:41.360 --> 33:42.360
 skill typically.

33:42.360 --> 33:47.520
 Like, you see a very strong chess player, maybe you're a very strong chess player yourself.

33:47.520 --> 33:54.000
 I think you're saying that because I'm Russian and now you're prejudiced, you assume all

33:54.000 --> 33:55.000
 of us are some degree of chess.

33:55.000 --> 33:56.000
 I'm biased.

33:56.000 --> 33:57.000
 Exactly.

33:57.000 --> 33:58.000
 Well, you're dead.

33:58.000 --> 33:59.000
 Bias.

33:59.000 --> 34:04.760
 So if you see a very strong chess player, you know they weren't born knowing how to

34:04.760 --> 34:05.760
 play chess.

34:05.760 --> 34:11.280
 So they had to acquire that skill with their limited resources, with their limited lifetime.

34:11.280 --> 34:15.520
 And you know they did that because they are generally intelligent.

34:15.520 --> 34:19.080
 And so they may as well have acquired any other skill.

34:19.080 --> 34:21.400
 You know they have this potential.

34:21.400 --> 34:27.840
 And on the other hand, if you see a computer playing chess, you cannot make the same assumptions

34:27.840 --> 34:30.920
 because you cannot just assume the computer is generally intelligent.

34:30.920 --> 34:37.320
 The computer may be born knowing how to play chess in the sense that it may have been programmed

34:37.320 --> 34:44.080
 by a human that has understood chess for the computer and that has just encoded the output

34:44.080 --> 34:46.120
 of that understanding in a static program.

34:46.120 --> 34:49.480
 And that program is not intelligent.

34:49.480 --> 34:55.680
 So let's zoom out just for a second and say like, what is the goal of the on the measure

34:55.680 --> 34:57.120
 of intelligence paper?

34:57.120 --> 34:59.160
 Like, what do you hope to achieve with it?

34:59.160 --> 35:05.240
 So the goal of the paper is to clear up some longstanding misunderstandings about the way

35:05.240 --> 35:12.440
 we've been conceptualizing intelligence in the AI community and in the way we've been

35:12.440 --> 35:16.880
 evaluating progress in AI.

35:16.880 --> 35:21.200
 There's been a lot of progress recently in machine learning and people are extrapolating

35:21.200 --> 35:26.680
 from that progress that we are about to solve general intelligence.

35:26.680 --> 35:32.800
 And if you want to be able to evaluate these statements, you need to precisely define what

35:32.800 --> 35:35.680
 you're talking about when you're talking about general intelligence.

35:35.680 --> 35:43.200
 And you need a formal way, a reliable way to measure how much intelligence, how much

35:43.200 --> 35:46.600
 general intelligence a system processes.

35:46.600 --> 35:50.840
 And ideally this measure of intelligence should be actionable.

35:50.840 --> 35:57.200
 So it should not just describe what intelligence is, it should not just be a binary indicator

35:57.200 --> 36:02.200
 that tells you the system is intelligent or it isn't.

36:02.200 --> 36:06.280
 It should be actionable, it should have explanatory power, right?

36:06.280 --> 36:09.120
 So you could use it as a feedback signal.

36:09.120 --> 36:13.640
 It would show you the way towards building more intelligent systems.

36:13.640 --> 36:22.400
 So at the first level, you draw a distinction between two divergent views of intelligence.

36:22.400 --> 36:29.080
 As we just talked about, intelligence is a collection of task specific skills and a general

36:29.080 --> 36:30.440
 learning ability.

36:30.440 --> 36:38.360
 So what's the difference between this memorization of skills and a general learning ability?

36:38.360 --> 36:43.080
 We've talked about it a little bit, but can you try to link around this topic for a bit?

36:43.080 --> 36:49.280
 Yes, so the first part of the paper is an assessment of the different ways we've been

36:49.280 --> 36:54.800
 thinking about intelligence and the different ways we've been evaluating progress in AI.

36:54.800 --> 37:01.400
 And this tree of cognitive sciences has been shaped by two views of the human mind.

37:01.400 --> 37:09.120
 And one view is the evolutionary psychology view in which the mind is a collection of

37:09.120 --> 37:17.800
 fairly static, special purpose ad hoc mechanisms that have been hard coded by evolution over

37:17.800 --> 37:22.880
 our history as a species over a very long time.

37:22.880 --> 37:31.920
 And early AI researchers, people like Marvin Minsky, for instance, they clearly subscribed

37:31.920 --> 37:33.720
 to this view.

37:33.720 --> 37:41.120
 And they saw the mind as a kind of collection of static programs similar to the programs

37:41.120 --> 37:43.720
 they would run on like mainframe computers.

37:43.720 --> 37:49.880
 And in fact, I think they very much understood the mind through the metaphor of the mainframe

37:49.880 --> 37:53.720
 computer because that was the tool they were working with.

37:53.720 --> 37:57.120
 And so you had these static programs, this collection of very different static programs

37:57.120 --> 38:00.200
 operating over a database like memory.

38:00.200 --> 38:03.840
 And in this picture, learning was not very important.

38:03.840 --> 38:05.760
 Learning was considered to be just memorization.

38:05.760 --> 38:15.480
 And in fact, learning is basically not featured in AI textbooks until the 1980s with the rise

38:15.480 --> 38:16.480
 of machine learning.

38:16.480 --> 38:23.400
 It's kind of fun to think about that learning was the outcast, like the weird people working

38:23.400 --> 38:24.400
 on learning.

38:24.400 --> 38:32.000
 Like the mainstream AI world was, I mean, I don't know what the best term is, but it's

38:32.000 --> 38:34.320
 non learning.

38:34.320 --> 38:37.960
 It was seen as like reasoning would not be learning based.

38:37.960 --> 38:45.440
 Yes, it was considered that the mind was a collection of programs that were primarily

38:45.440 --> 38:46.760
 logical in nature.

38:46.760 --> 38:51.000
 And that's all you needed to do to create a mind was to write down these programs.

38:51.000 --> 38:55.280
 And they would operate over knowledge, which will be stored in some kind of database.

38:55.280 --> 39:00.280
 And as long as your database would encompass everything about the world and your logical

39:00.280 --> 39:04.560
 rules were comprehensive, then you would have a mind.

39:04.560 --> 39:10.880
 So the other view of the mind is the brain as sort of blank slate.

39:10.880 --> 39:11.880
 Right?

39:11.880 --> 39:13.240
 This is a very old idea.

39:13.240 --> 39:16.200
 You find it in John Locke's writings.

39:16.200 --> 39:19.560
 This is the Tabulaza.

39:19.560 --> 39:24.800
 And this is this idea that the mind is some kind of like information sponge that starts

39:24.800 --> 39:35.320
 empty, that starts blank, and that absorbs knowledge and skills from experience, right?

39:35.320 --> 39:41.280
 So it's a sponge that reflects the complexity of the world, the complexity of your life

39:41.280 --> 39:47.080
 experience, essentially, that everything you know and everything you can do is a reflection

39:47.080 --> 39:50.520
 of something you found in the outside world, essentially.

39:50.520 --> 39:58.320
 So this is an idea that's very old, that was not very popular, for instance, in the 1970s.

39:58.320 --> 40:02.720
 But that gained a lot of vitality recently with the rise of connectionism in particular

40:02.720 --> 40:04.360
 deep learning.

40:04.360 --> 40:08.560
 And so today, deep learning is the dominant paradigm in AI.

40:08.560 --> 40:15.920
 And I feel like lots of AI researchers are conceptualizing the mind via a deep learning

40:15.920 --> 40:22.120
 metaphor, like they see the mind as a kind of randomly initialized neural network that

40:22.120 --> 40:27.680
 starts blank when you're born, and then that gets trained via exposure to training data

40:27.680 --> 40:30.960
 that requires knowledge and skills, exposure to training data.

40:30.960 --> 40:34.200
 By the way, it's a small tangent.

40:34.200 --> 40:40.080
 I feel like people who are thinking about intelligence are not conceptualizing it that

40:40.080 --> 40:41.280
 way.

40:41.280 --> 40:46.880
 I actually haven't met too many people who believe that a neural network will be able

40:46.880 --> 40:53.080
 to reason, who seriously think that, rigorously, because I think it's an actually interesting

40:53.080 --> 40:54.080
 worldview.

40:54.080 --> 41:00.600
 And we'll talk about it more, but it's been impressive what neural networks have been

41:00.600 --> 41:01.600
 able to accomplish.

41:01.600 --> 41:08.120
 And it's an eye to me, I don't know, you might disagree, but it's an open question whether

41:08.120 --> 41:15.840
 scaling size eventually might lead to incredible results to us mere humans will appear as if

41:15.840 --> 41:16.840
 it's general.

41:16.840 --> 41:22.000
 I mean, if you ask people who are seriously thinking about intelligence, they will definitely

41:22.000 --> 41:27.120
 not say that all you need to do is like the mind is just a neural network.

41:27.120 --> 41:31.920
 However, it's actually a view that's very popular, I think, in the deep learning community,

41:31.920 --> 41:38.480
 that many people are kind of conceptually intellectually lazy about it.

41:38.480 --> 41:45.040
 But I guess what I'm saying exactly right is I haven't met many people, and I think

41:45.040 --> 41:49.680
 it would be interesting to meet a person who is not intellectually lazy about this particular

41:49.680 --> 41:54.400
 topic and still believes that neural networks will go all the way.

41:54.400 --> 42:02.120
 I think Yalla is probably closest to that with self supervisor who argue that currently

42:02.120 --> 42:07.200
 planning techniques are already the way to general artificial intelligence and that all

42:07.200 --> 42:13.040
 you need to do is to scale it up to all the available train data.

42:13.040 --> 42:21.240
 And that's if you look at the waves that open AI is GPT stream model is made, you see echoes

42:21.240 --> 42:22.760
 of this idea.

42:22.760 --> 42:30.880
 So on that topic, GPT three, similar to GPT two actually, have captivated some part of

42:30.880 --> 42:33.160
 the imagination of the public.

42:33.160 --> 42:38.080
 This is just a bunch of hype of different kind that's, I would say it's emergent.

42:38.080 --> 42:39.880
 It's not artificially manufactured.

42:39.880 --> 42:44.080
 It's just like people just get excited for some strange reason.

42:44.080 --> 42:48.520
 In the case of GPT three, which is funny, that there's, I believe a couple of months

42:48.520 --> 42:57.200
 delay from release to hype, maybe I'm not historically correct on that, but it feels

42:57.200 --> 43:04.960
 like there was a little bit of a lack of hype and then there's a phase shift into hype.

43:04.960 --> 43:09.800
 But nevertheless, there's a bunch of cool applications that seem to captivate the imagination

43:09.800 --> 43:16.280
 of the public about what this language model that's trained in unsupervised way without

43:16.280 --> 43:19.720
 any fine tuning is able to achieve.

43:19.720 --> 43:21.040
 So what do you make of that?

43:21.040 --> 43:22.720
 What are your thoughts about GPT three?

43:22.720 --> 43:23.720
 Yeah.

43:23.720 --> 43:29.200
 So I think what's interesting about GPT three is the idea that it may be able to learn new

43:29.200 --> 43:33.760
 tasks in after just being shown a few examples.

43:33.760 --> 43:37.600
 So I think if it's actually capable of doing that, that's novel and that's very interesting

43:37.600 --> 43:40.080
 and that's something we should investigate.

43:40.080 --> 43:46.240
 That said, I must say, I'm not entirely convinced that we have shown it's capable of doing that

43:46.240 --> 43:52.680
 but it's very likely given the amount of data that the model is trained on that what it's

43:52.680 --> 43:59.360
 actually doing is pattern matching a new task you give it with a task that it's been exposed

43:59.360 --> 44:00.360
 to in its train data.

44:00.360 --> 44:05.640
 It's just recognizing the task instead of just developing a model of the task.

44:05.640 --> 44:10.160
 But there's, sorry to interrupt, there's a parallel as to what you said before, which

44:10.160 --> 44:17.640
 is it's possible to see GPT three as like the prompts it's given as a kind of SQL query

44:17.640 --> 44:21.560
 into this thing that it's learned similar to what you said before, which is language

44:21.560 --> 44:23.960
 is used to query the memory.

44:23.960 --> 44:30.840
 So is it possible that neural network is a giant memorization thing, but then if it's

44:30.840 --> 44:38.480
 gets sufficiently giant, it'll memorize sufficiently large amounts of thing in the world or intelligence

44:38.480 --> 44:40.520
 becomes a querying machine.

44:40.520 --> 44:46.720
 I think it's possible that a significant chunk of intelligence is this giant associative

44:46.720 --> 44:48.720
 memory.

44:48.720 --> 44:53.920
 I definitely don't believe that intelligence is just a giant associative memory, but it

44:53.920 --> 44:57.760
 may well be a big component.

44:57.760 --> 45:07.480
 So do you think GPT three, four, five, GPT 10 will eventually like what do you think?

45:07.480 --> 45:08.480
 Where's the ceiling?

45:08.480 --> 45:11.080
 Do you think they'll be able to reason?

45:11.080 --> 45:14.720
 No, that's a bad question.

45:14.720 --> 45:16.760
 Like what is the ceiling is the better question?

45:16.760 --> 45:18.680
 Well, what is going to scale?

45:18.680 --> 45:22.160
 How good is GPT N going to be?

45:22.160 --> 45:31.920
 So I believe GPT N is going to improve on the strength of GPT two and three, which is it

45:31.920 --> 45:37.800
 will be able to generate, you know, ever more plausible text in context.

45:37.800 --> 45:40.360
 Just monetize the performance.

45:40.360 --> 45:48.680
 Yes, if you train a bigger model on more data, then your text will be increasingly more context

45:48.680 --> 45:54.840
 aware and increasingly more plausible in the same way that GPT three is much better at

45:54.840 --> 45:59.120
 generating plausible texts compared to GPT two.

45:59.120 --> 46:04.920
 So that said, I don't think just getting up the model to more transformer layers and

46:04.920 --> 46:10.320
 more train data is going to address the flaws of GPT three, which is that it can generate

46:10.320 --> 46:16.760
 plausible texts, but that text is not constrained by anything else other than plausibility.

46:16.760 --> 46:22.320
 So in particular, it's not constrained by factualness or even consistency, which is

46:22.320 --> 46:28.360
 why it's very easy to get GPT three to generate statements that are factually untrue or to

46:28.360 --> 46:32.400
 generate statements that are even self contradictory, right?

46:32.400 --> 46:39.200
 Because it's only goal is plausibility, and it has no other constraints.

46:39.200 --> 46:42.600
 It's not constrained to be self consistent, right?

46:42.600 --> 46:46.640
 And so for this reason, one thing that I thought was very interesting with GPT three

46:46.640 --> 46:52.520
 is that you can put it in mind the answer it will give you by asking the question in

46:52.520 --> 46:57.280
 a specific way, because it's very responsive to the way you ask the question since it has

46:57.280 --> 47:03.640
 no understanding of the content of the question.

47:03.640 --> 47:10.520
 And if you are the same question in two different ways that are basically adversely engineered

47:10.520 --> 47:15.720
 to produce a certain answer, you will get two different answers, two contradictory answers.

47:15.720 --> 47:18.200
 It's very susceptible to adversarial attacks essentially.

47:18.200 --> 47:19.520
 Potentially, yes.

47:19.520 --> 47:25.160
 So in general, the problem with these models, these generative models is that they are very

47:25.160 --> 47:32.200
 good at generating plausible texts, but that's just not enough, right?

47:32.200 --> 47:38.400
 You need, I think one avenue that would be very interesting to make progress is to make

47:38.400 --> 47:46.000
 it possible to write programs over the latent space that these models operate on, that you

47:46.000 --> 47:53.480
 would rely on these self supervised models to generate a sort of lack pool of knowledge

47:53.480 --> 47:59.600
 and concepts and common sense, and then you would be able to write explicit reasoning

47:59.600 --> 48:01.680
 programs over it.

48:01.680 --> 48:06.640
 Because the current problem with GPT three is that you can be quite difficult to get

48:06.640 --> 48:09.480
 it to do what you want to do.

48:09.480 --> 48:14.960
 If you want to turn GPT three into products, you need to put constraints on it.

48:14.960 --> 48:19.120
 You need to force it to obey certain rules.

48:19.120 --> 48:22.040
 So you need a way to program it explicitly.

48:22.040 --> 48:27.080
 Yeah, so if you look at its ability to do program synthesis, it generates, like you

48:27.080 --> 48:28.760
 said, something that's plausible.

48:28.760 --> 48:36.480
 Yeah, so if you try to make it generate programs, it will perform well for any program that

48:36.480 --> 48:44.520
 it has seen it in its training data, but because a program space is not interpretative, right?

48:44.520 --> 48:48.840
 It's not going to be able to generalize two problems it hasn't seen before.

48:48.840 --> 48:58.400
 Now that's currently, do you think sort of an absurd, but I think useful, I guess, intuition

48:58.400 --> 49:07.720
 builder is, you know, the GPT three has 175 billion parameters.

49:07.720 --> 49:16.480
 Human brain has 100 has about 1000 times that or more in terms of number of synapses.

49:16.480 --> 49:26.560
 Do you think, obviously, very different kinds of things, but there is some degree of similarity.

49:26.560 --> 49:34.520
 Do you think, what do you think GPT will look like when it has 100 trillion parameters?

49:34.520 --> 49:40.640
 You think our conversation might be in nature different, like, because you've criticized

49:40.640 --> 49:43.040
 GPT three very effectively now.

49:43.040 --> 49:44.440
 Do you think?

49:44.440 --> 49:47.080
 No, I don't think so.

49:47.080 --> 49:53.400
 So to begin with, the bottleneck with scaling up GPT three GPT models, genetic pretrained

49:53.400 --> 49:59.000
 transformer models, is not going to be the size of the model or how long it takes to

49:59.000 --> 50:00.000
 train it.

50:00.000 --> 50:05.040
 The bottleneck is going to be the training data because OpenEye is already training GPT

50:05.040 --> 50:08.960
 three on a crawl of basically the entire web, right?

50:08.960 --> 50:09.960
 And that's a lot of data.

50:09.960 --> 50:13.360
 So you could imagine training on more data than that, like Google could train on more

50:13.360 --> 50:17.760
 data than that, but it would still be only incrementally more data.

50:17.760 --> 50:22.800
 And I don't recall exactly how much more data GPT three was trained on compared to GPT

50:22.800 --> 50:28.600
 two, but it's probably at least like 100, maybe even 1000x, don't have the exact number.

50:28.600 --> 50:33.360
 You're not going to be able to train a model on 100 more data than what you're already

50:33.360 --> 50:34.360
 doing.

50:34.360 --> 50:35.360
 So that's brilliant.

50:35.360 --> 50:40.760
 So it's easier to think of compute as a bottleneck and then arguing that we can remove that

50:40.760 --> 50:41.760
 bottleneck.

50:41.760 --> 50:44.720
 We can remove the compute bottleneck, I don't think it's a big problem.

50:44.720 --> 50:51.960
 If you look at the pace at which we've improved the efficiency of deep learning models in

50:51.960 --> 51:00.120
 the past a few years, I'm not worried about training time bottlenecks or model size bottlenecks.

51:00.120 --> 51:04.800
 The bottleneck in the case of these generative transformer models is absolutely the trained

51:04.800 --> 51:05.800
 data.

51:05.800 --> 51:07.800
 What about the quality of the data?

51:07.800 --> 51:08.800
 So yeah.

51:08.800 --> 51:10.960
 So the quality of the data is an interesting point.

51:10.960 --> 51:18.600
 The thing is, if you're going to want to use these models in real products, then you want

51:18.600 --> 51:25.720
 to feed them data that's as high quality as factual, I would say as unbiased as possible,

51:25.720 --> 51:30.640
 but there's not really such a thing as unbiased data in the first place.

51:30.640 --> 51:35.000
 But you probably don't want to train it on Reddit, for instance.

51:35.000 --> 51:37.200
 Sounds like a bad plan.

51:37.200 --> 51:42.840
 So from my personal experience, working with large scale deep learning models.

51:42.840 --> 51:50.640
 So at some point, I was working on a model at Google that's trained on like 350 million

51:50.640 --> 51:51.640
 labeled images.

51:51.640 --> 51:53.800
 It's an image classification model.

51:53.800 --> 51:54.800
 That's a lot of images.

51:54.800 --> 52:01.240
 That's like probably most publicly available images on the web at the time.

52:01.240 --> 52:07.720
 And it was a very noisy data set because the labels were not originally annotated by hand

52:07.720 --> 52:08.720
 by humans.

52:08.720 --> 52:16.320
 They were automatically derived from tags on social media or just keywords in the same

52:16.320 --> 52:18.240
 page as the image was found and so on.

52:18.240 --> 52:19.240
 So it was very noisy.

52:19.240 --> 52:26.360
 And it turned out that you could easily get a better model, not just by training.

52:26.360 --> 52:31.600
 Like if you train on more of the noisy data, you get an incrementally better model, but

52:31.600 --> 52:35.560
 you very quickly eat diminishing returns.

52:35.560 --> 52:39.960
 On the other hand, if you train on a smaller data set with higher quality annotations,

52:39.960 --> 52:46.800
 quality that are annotations that are actually made by humans, you get a better model.

52:46.800 --> 52:49.600
 And it also takes less time to train it.

52:49.600 --> 52:51.640
 Yeah, that's fascinating.

52:51.640 --> 52:53.320
 It's the self supervised learning.

52:53.320 --> 52:58.040
 If there's a way to get better doing the automated labeling.

52:58.040 --> 53:05.960
 Yeah, so you can enrich or refine your labels in an automated way.

53:05.960 --> 53:06.960
 That's correct.

53:06.960 --> 53:12.200
 Do you have a hope for, I don't know if you're familiar with the idea of a semantic web.

53:12.200 --> 53:18.640
 Is this a semantic web, just what people are not familiar, and is the idea of being able

53:18.640 --> 53:27.920
 to convert the internet or be able to attach semantic meaning to the words on the internet?

53:27.920 --> 53:33.960
 The sentences, the paragraphs, to be able to convert information on the internet or

53:33.960 --> 53:38.280
 some fraction of the internet into something that's interpretable by machines.

53:38.280 --> 53:48.240
 That was kind of a dream for, I think the semantic web papers in the 90s, it's kind

53:48.240 --> 53:52.520
 of the dream that the internet is full of rich, exciting information.

53:52.520 --> 53:57.880
 Even just looking at Wikipedia, we should be able to use that as data for machines.

53:57.880 --> 53:58.880
 And so far.

53:58.880 --> 54:01.280
 Information is not in a format that's available to machines.

54:01.280 --> 54:06.640
 So no, I don't think the semantic web will ever work simply because it would be a lot

54:06.640 --> 54:12.240
 of work to provide that information in a structured form.

54:12.240 --> 54:16.440
 And there is not really any incentive for anyone to provide that work.

54:16.440 --> 54:24.200
 So I think the way forward to make the knowledge on the web available to machines is actually

54:24.200 --> 54:27.200
 something closer to unsupervised deep learning.

54:27.200 --> 54:28.200
 Yeah.

54:28.200 --> 54:33.880
 So GBT3 is actually a bigger step in the direction of making the knowledge of the web available

54:33.880 --> 54:36.720
 to machines than the semantic web was.

54:36.720 --> 54:37.720
 Yeah.

54:37.720 --> 54:48.560
 In a human centric sense, it feels like GBT3 hasn't learned anything that could be used

54:48.560 --> 54:50.600
 to reason.

54:50.600 --> 54:52.840
 But that might be just the early days.

54:52.840 --> 54:53.840
 Yeah.

54:53.840 --> 54:54.840
 I think that's correct.

54:54.840 --> 55:00.160
 I think the forms of reasoning that you see it perform are basically just reproducing

55:00.160 --> 55:02.560
 patterns that it has seen in string data.

55:02.560 --> 55:09.360
 So of course, if you're trained on the entire web, then you can produce an illusion of reasoning

55:09.360 --> 55:13.880
 in many different situations, but it will break down if it's presented with a novel

55:13.880 --> 55:14.880
 situation.

55:14.880 --> 55:19.160
 That's the open question between the illusion of reasoning and actual reasoning, yeah.

55:19.160 --> 55:20.160
 Yes.

55:20.160 --> 55:22.960
 The power to adapt to something that is genuinely new.

55:22.960 --> 55:31.320
 Because the thing is, even imagine you had, you could train on every bit of data ever

55:31.320 --> 55:35.600
 generated in the history of humanity.

55:35.600 --> 55:43.280
 It remains, that model would be capable of anticipating many different possible situations,

55:43.280 --> 55:47.560
 but it remains that the future is going to be something different.

55:47.560 --> 55:55.920
 Like, for instance, if you train a GBT3 model on data from the year 2002, for instance,

55:55.920 --> 55:58.920
 and then you use it today, it's going to be missing many things, it's going to be missing

55:58.920 --> 56:02.880
 many common sense facts about the world.

56:02.880 --> 56:05.600
 It's even going to be missing vocabulary and so on.

56:05.600 --> 56:13.040
 Yeah, it's interesting that GBT3 even doesn't have, I think, any information about the coronavirus.

56:13.040 --> 56:15.120
 Yes.

56:15.120 --> 56:21.920
 Which is why a system that's, you tell that the system is intelligent when it's capable

56:21.920 --> 56:22.920
 to adapt.

56:22.920 --> 56:27.480
 So intelligence is going to require some amount of continuous learning.

56:27.480 --> 56:31.280
 It's also going to require some amount of improvisation.

56:31.280 --> 56:36.840
 It's not enough to assume that what you're going to be asked to do is something that

56:36.840 --> 56:41.200
 you've seen before, or something that is a simple interpolation of things you've seen

56:41.200 --> 56:42.200
 before.

56:42.200 --> 56:43.200
 Yeah.

56:43.200 --> 56:51.520
 In fact, that model breaks down for even very tasks that look relatively simple from

56:51.520 --> 56:55.120
 a distance, like L5 self driving, for instance.

56:55.120 --> 57:04.200
 Google at the paper a couple of years back showing that something like 30 million different

57:04.200 --> 57:09.920
 road situations were actually completely insufficient to train a driving model.

57:09.920 --> 57:11.920
 It wasn't even L2, right?

57:11.920 --> 57:12.920
 And that's a lot of data.

57:12.920 --> 57:19.080
 That's a lot more data than the 20 or 30 hours of driving that a human needs to learn to

57:19.080 --> 57:22.240
 drive given the knowledge they've already accumulated.

57:22.240 --> 57:31.120
 Well, let me ask you on that topic, Elon Musk, Tesla autopilot, one of the only companies,

57:31.120 --> 57:35.200
 I believe, is really pushing for a learning based approach.

57:35.200 --> 57:39.680
 You're skeptical that that kind of network can achieve level four?

57:39.680 --> 57:44.640
 L4 is probably achievable, L5 is probably not.

57:44.640 --> 57:46.080
 What's the distinction there?

57:46.080 --> 57:49.080
 Is L5 is completely, you can just fall asleep?

57:49.080 --> 57:51.360
 Yeah, L5 is basically human level.

57:51.360 --> 57:55.440
 Well, it would drive, you have to be careful saying human level because that's the most

57:55.440 --> 57:56.440
 kind of drivers.

57:56.440 --> 58:03.200
 Yeah, that's the clearest example of cars will most likely be much safer than humans

58:03.200 --> 58:06.800
 in many situations where humans fail.

58:06.800 --> 58:09.080
 It's the vice versa question.

58:09.080 --> 58:15.520
 I'll tell you, the thing is the amounts of train data you would need to anticipate for

58:15.520 --> 58:21.280
 pretty much every possible situation you learn content in the real world is such that

58:21.280 --> 58:25.960
 it's not entirely unrealistic to think that at some point in the future we'll develop

58:25.960 --> 58:31.600
 a system that's trying enough data, especially provided that we can simulate a lot of that

58:31.600 --> 58:32.600
 data.

58:32.600 --> 58:40.040
 We don't necessarily need actual cars on the road for everything, but it's a massive effort.

58:40.040 --> 58:44.720
 And it turns out you can create a system that's much more adaptive, that can generalize much

58:44.720 --> 58:53.760
 better if you just add explicit models of the surroundings of the car.

58:53.760 --> 58:59.560
 And if you use deep learning for what it's good at, which is to provide perceptive information.

58:59.560 --> 59:05.080
 So in general, deep learning is a way to encode perception and a way to encode intuition,

59:05.080 --> 59:11.600
 but it is not a good medium for any sort of explicit reasoning.

59:11.600 --> 59:21.320
 And in AI systems today, strong generalization tends to come from explicit models, tend to

59:21.320 --> 59:28.400
 come from abstractions in the human mind that are encoded in program form by a human engineer.

59:28.400 --> 59:33.200
 These are the abstractions that can actually generalize, not the sort of weak abstraction

59:33.200 --> 59:35.280
 that is learned by a neural network.

59:35.280 --> 59:42.600
 And the question is how much reasoning, how much strong abstractions are required to solve

59:42.600 --> 59:45.880
 particular tasks like driving?

59:45.880 --> 59:46.880
 That's the question.

59:46.880 --> 59:53.800
 Or human life, existence, how much strong abstractions does existence require, but more

59:53.800 --> 59:57.240
 specifically on driving?

59:57.240 --> 1:00:03.920
 That seems to be a coupled question about intelligence is like, how much intelligence

1:00:03.920 --> 1:00:07.560
 like, how do you build an intelligent system?

1:00:07.560 --> 1:00:11.520
 And the coupled problem, how hard is this problem?

1:00:11.520 --> 1:00:14.520
 How much intelligence does this problem actually require?

1:00:14.520 --> 1:00:18.160
 So we're, we get to cheat, right?

1:00:18.160 --> 1:00:20.280
 Because we get to look at the problem.

1:00:20.280 --> 1:00:24.840
 Like it's not like you get to close our eyes and completely new to driving.

1:00:24.840 --> 1:00:30.720
 We get to do what we do as human beings, which is for the majority of our life, before we

1:00:30.720 --> 1:00:35.440
 ever learn quote unquote to drive, you get to watch other cars and other people drive.

1:00:35.440 --> 1:00:39.480
 We get to be in cars, we get to watch, we get to go and see movies about cars.

1:00:39.480 --> 1:00:42.800
 We get to, you know, we get to observe all this stuff.

1:00:42.800 --> 1:00:44.840
 And that's similar to what neural networks are doing.

1:00:44.840 --> 1:00:47.400
 It's getting a lot of data.

1:00:47.400 --> 1:00:57.680
 And the question is, yeah, how much is, how many leaps of reasoning genius is required

1:00:57.680 --> 1:00:59.360
 to be able to actually effectively drive?

1:00:59.360 --> 1:01:06.440
 Oh, it's for example, driving, I mean, sure, you've seen a lot of cars in your life before

1:01:06.440 --> 1:01:07.840
 you learn to drive.

1:01:07.840 --> 1:01:14.320
 But let's say you've learned to drive in Silicon Valley and now you rent a car in Tokyo.

1:01:14.320 --> 1:01:17.040
 Well now everyone is driving on the other side of the road.

1:01:17.040 --> 1:01:20.400
 And the signs are different and the roads are more narrow and so on.

1:01:20.400 --> 1:01:22.960
 So it's a very, very different environment.

1:01:22.960 --> 1:01:31.480
 And a smart human, even an average human should be able to just zero shot it to just be operational

1:01:31.480 --> 1:01:38.640
 in this, in this very different environment right away, despite having had no contact

1:01:38.640 --> 1:01:44.480
 with the novel complexity that is contained in this environment, right?

1:01:44.480 --> 1:01:51.680
 And that is novel complexity is not just interpolation over the situations that you've encountered

1:01:51.680 --> 1:01:55.040
 previously, like learning to drive in the US, right?

1:01:55.040 --> 1:01:59.960
 I would say the reason I ask is one of the most interesting tests of intelligence we

1:01:59.960 --> 1:02:06.560
 have today, actively, which is driving in terms of having an impact on the world.

1:02:06.560 --> 1:02:09.920
 Like when do you think we'll pass that test of intelligence?

1:02:09.920 --> 1:02:15.000
 So I don't think driving is that much of a test of intelligence because again, there

1:02:15.000 --> 1:02:23.040
 is no task for which skill at that task demonstrates intelligence unless it's a kind of meta tasks

1:02:23.040 --> 1:02:26.640
 that involves acquiring new skills.

1:02:26.640 --> 1:02:34.120
 So I don't think I think you can actually solve driving without having any, any real

1:02:34.120 --> 1:02:35.120
 amount of intelligence.

1:02:35.120 --> 1:02:41.600
 For instance, if you really did have infinite train there, you could just literally train

1:02:41.600 --> 1:02:45.800
 an end to end deep learning model that does driving provided infinite train data.

1:02:45.800 --> 1:02:53.400
 The only problem with the whole idea is collecting a data sets that's sufficiently comprehensive

1:02:53.400 --> 1:02:57.440
 that covers the very long tail of possible situations you might encounter.

1:02:57.440 --> 1:02:59.400
 And it's really just a scale problem.

1:02:59.400 --> 1:03:06.600
 So I think there's nothing fundamentally wrong with this plan, with this idea.

1:03:06.600 --> 1:03:15.960
 It's just that it strikes me as a fairly inefficient thing to do because you run into this scaling

1:03:15.960 --> 1:03:21.880
 issue with diminishing returns, whereas if instead you took a more manual engineering

1:03:21.880 --> 1:03:31.720
 approach where you use deep learning modules in combination with engineering an explicit

1:03:31.720 --> 1:03:36.280
 model of the surrounding of the cars and you bridge the two in a clever way.

1:03:36.280 --> 1:03:41.160
 Your model will actually start generalizing much earlier and more effectively than the

1:03:41.160 --> 1:03:42.560
 end to end deep learning model.

1:03:42.560 --> 1:03:47.800
 So why would you not go with the more manual engineering oriented approach?

1:03:47.800 --> 1:03:52.240
 Like even if you created that system, either the end to end deep learning model system

1:03:52.240 --> 1:03:58.520
 that's infinite data or the slightly more human system.

1:03:58.520 --> 1:04:04.840
 I don't think achieving alpha would demonstrate general intelligence or intelligence of any

1:04:04.840 --> 1:04:11.280
 generality at all, again, the only possible test of generality in AI would be a test that

1:04:11.280 --> 1:04:14.000
 looks at skill acquisition over unknown tasks.

1:04:14.000 --> 1:04:21.080
 For instance, you could take your L5 driver and ask it to learn to pilot a commercial

1:04:21.080 --> 1:04:25.720
 airplane, for instance, and then you would look at how much human involvement is required

1:04:25.720 --> 1:04:30.120
 and how much training data is required for the system to learn to pilot an airplane.

1:04:30.120 --> 1:04:35.920
 And that gives you a measure of how intelligent that system really is.

1:04:35.920 --> 1:04:37.480
 Yeah, I mean, that's a big leap.

1:04:37.480 --> 1:04:42.000
 I get you, but I'm more interested as a problem.

1:04:42.000 --> 1:04:49.720
 I would see, to me, driving is a black box that can generate novel situations at some

1:04:49.720 --> 1:04:53.520
 rate, what people call edge cases.

1:04:53.520 --> 1:04:59.200
 So it does have newness that keeps being like we're confronted, let's say once a month.

1:04:59.200 --> 1:05:00.640
 It is a very long tail.

1:05:00.640 --> 1:05:01.640
 Yes.

1:05:01.640 --> 1:05:02.640
 It's a long tail.

1:05:02.640 --> 1:05:09.000
 That doesn't mean you cannot solve it just by training as a school model and out of data.

1:05:09.000 --> 1:05:10.000
 Huge amount of data.

1:05:10.000 --> 1:05:12.040
 It's really amount of scale.

1:05:12.040 --> 1:05:18.040
 But I guess what I'm saying is if you have a vehicle that achieves level five, it is

1:05:18.040 --> 1:05:24.240
 going to be able to deal with new situations.

1:05:24.240 --> 1:05:33.360
 Or I mean, the data is so large that the rate of new situations is very low.

1:05:33.360 --> 1:05:34.360
 That's not intelligent.

1:05:34.360 --> 1:05:41.120
 So if we go back to your definition of intelligence, it's the efficiency with which you can adapt

1:05:41.120 --> 1:05:46.320
 to new situations, to truly new situations, not situations you've seen before, not situations

1:05:46.320 --> 1:05:51.040
 that could be anticipated by your creators, by the creators of the system, but true new

1:05:51.040 --> 1:05:52.040
 situations.

1:05:52.040 --> 1:05:55.080
 The efficiency with which you acquire new skills.

1:05:55.080 --> 1:06:03.040
 If you require, in order to pick up a new skill, you require a very extensive training

1:06:03.040 --> 1:06:09.160
 dataset of most possible situations that can occur in the practice of that skill, then

1:06:09.160 --> 1:06:10.600
 the system is not intelligent.

1:06:10.600 --> 1:06:15.240
 It is mostly just a lookup table.

1:06:15.240 --> 1:06:16.240
 Yeah.

1:06:16.240 --> 1:06:17.240
 Well.

1:06:17.240 --> 1:06:23.760
 So if, in order to acquire a skill, you need a human engineer to write down a bunch of

1:06:23.760 --> 1:06:29.640
 rules that cover most or every possible situation, likewise, the system is not intelligent.

1:06:29.640 --> 1:06:38.480
 The system is merely the output artifact of a process that happens in the minds of the

1:06:38.480 --> 1:06:41.000
 engineers that are creating it.

1:06:41.000 --> 1:06:48.000
 It is encoding an abstraction that's produced by the human mind, and intelligence would

1:06:48.000 --> 1:06:55.840
 actually be the process of producing, of autonomously producing this abstraction.

1:06:55.840 --> 1:06:56.840
 Yeah.

1:06:56.840 --> 1:07:02.080
 Not like, if you take an abstraction and you encode it on a piece of paper or in a computer

1:07:02.080 --> 1:07:06.160
 program, the abstraction itself is not intelligent.

1:07:06.160 --> 1:07:11.640
 This intelligent is the agent that's capable of producing these abstractions, right?

1:07:11.640 --> 1:07:12.640
 Yeah.

1:07:12.640 --> 1:07:17.600
 It feels like there's a little bit of a gray area, like, because you're basically saying

1:07:17.600 --> 1:07:25.520
 that deep learning forms abstractions too, but those abstractions do not seem to be effective

1:07:25.520 --> 1:07:30.680
 for generalizing far outside of the things that you've already seen, but generalize a

1:07:30.680 --> 1:07:31.680
 little bit.

1:07:31.680 --> 1:07:32.680
 Yeah.

1:07:32.680 --> 1:07:33.680
 Absolutely.

1:07:33.680 --> 1:07:34.680
 No, deep learning does generalize a little bit.

1:07:34.680 --> 1:07:38.280
 But generalization is not a binary, it's more like a spectrum.

1:07:38.280 --> 1:07:39.280
 Yeah.

1:07:39.280 --> 1:07:42.680
 And there's a certain point, it's a gray area, but there's a certain point where there's

1:07:42.680 --> 1:07:46.720
 an impressive degree of generalization that happens.

1:07:46.720 --> 1:07:56.920
 No, I guess exactly what you were saying is intelligence is how efficiently you're able

1:07:56.920 --> 1:08:03.240
 to generalize far outside of the distribution of things you've seen already.

1:08:03.240 --> 1:08:04.240
 Yes.

1:08:04.240 --> 1:08:09.520
 It's just like the distance of how far you can, like, how new, how radically new something

1:08:09.520 --> 1:08:13.240
 is and how efficiently you're able to deal with that.

1:08:13.240 --> 1:08:18.960
 You can think of intelligence as a measure of an information conversion ratio.

1:08:18.960 --> 1:08:28.160
 Like, imagine a space of possible situations, and you've covered some of them, so you have

1:08:28.160 --> 1:08:32.720
 some amount of information about your space of possible situations that's provided by

1:08:32.720 --> 1:08:38.200
 the situations you already know, and that's, on the other hand, also provided by the prior

1:08:38.200 --> 1:08:42.480
 knowledge that the system brings to the table or the prior knowledge that's embedded in

1:08:42.480 --> 1:08:43.720
 the system.

1:08:43.720 --> 1:08:49.240
 So the system starts with some information, right, about the problem, about the task.

1:08:49.240 --> 1:08:54.880
 And it's about going from that information to a program, what you would call a skill

1:08:54.880 --> 1:09:02.000
 program, a behavioral program that can cover a large area of possible situation space.

1:09:02.000 --> 1:09:06.720
 And essentially, the ratio between that area and the amount of information you start with

1:09:06.720 --> 1:09:09.840
 is intelligence.

1:09:09.840 --> 1:09:17.440
 So a very smart agent can make efficient users of very little information about a new problem

1:09:17.440 --> 1:09:23.200
 and very little prior knowledge as well to cover a very large area of potential situations

1:09:23.200 --> 1:09:31.200
 in that problem, without knowing what these future new situations are going to be.

1:09:31.200 --> 1:09:35.520
 So one of the other big things you talk about in the paper, we've talked about it a little

1:09:35.520 --> 1:09:41.040
 bit already, but let's talk about it some more as the actual tests of intelligence.

1:09:41.040 --> 1:09:48.160
 So if we look at like human and machine intelligence, do you think tests of intelligence should

1:09:48.160 --> 1:09:54.840
 be different for humans and machines, or how we think about testing of intelligence?

1:09:54.840 --> 1:10:01.440
 These fundamentally the same kind of intelligence that we're after, and therefore the tests

1:10:01.440 --> 1:10:03.760
 should be similar?

1:10:03.760 --> 1:10:11.960
 So if your goal is to create AIs that are more human like, then it will be super valuable,

1:10:11.960 --> 1:10:19.640
 obviously, to have a test that's universal, that applies to both AIs and humans, so that

1:10:19.640 --> 1:10:27.440
 you could establish a comparison between the two that you could tell exactly how intelligence,

1:10:27.440 --> 1:10:30.520
 in terms of human intelligence, a given system is.

1:10:30.520 --> 1:10:37.600
 So that said, the constraints that apply to artificial intelligence and to human intelligence

1:10:37.600 --> 1:10:45.240
 are very different, and your test should account for this difference.

1:10:45.240 --> 1:10:50.560
 Because if you look at artificial systems, it's always possible for an experimenter to

1:10:50.560 --> 1:10:59.840
 buy arbitrary levels of skill at arbitrary tasks, either by injecting a hard coded prior

1:10:59.840 --> 1:11:07.120
 knowledge into the system via rules and so on that come from the human mind, from the

1:11:07.120 --> 1:11:14.200
 minds of the programmers, and also buying higher levels of skill just by training on

1:11:14.200 --> 1:11:19.760
 more data, for instance, you could generate an infinity of different Go games, and you

1:11:19.760 --> 1:11:27.520
 could train a Go playing system that way, but you could not directly compare it to human

1:11:27.520 --> 1:11:34.000
 Go playing skills, because a human that plays Go had to develop that skill in a very constrained

1:11:34.000 --> 1:11:35.000
 environment.

1:11:35.000 --> 1:11:40.520
 They had the limited amount of time, they had the limited amount of energy, and of course,

1:11:40.520 --> 1:11:48.720
 they started from a different set of priors, they started from innate human priors.

1:11:48.720 --> 1:11:52.440
 So I think if you want to compare the intelligence of two systems, like the intelligence of an

1:11:52.440 --> 1:11:59.920
 AI and the intelligence of a human, you have to control for priors.

1:11:59.920 --> 1:12:06.880
 You have to start from the same set of knowledge priors about the task, and you have to control

1:12:06.880 --> 1:12:11.280
 for experience, that is to say, for training data.

1:12:11.280 --> 1:12:15.160
 So what's priors?

1:12:15.160 --> 1:12:21.600
 So priors is whatever information you have about a given task before you start learning

1:12:21.600 --> 1:12:23.440
 about this task.

1:12:23.440 --> 1:12:25.960
 And how's the difference from experience?

1:12:25.960 --> 1:12:28.160
 Well experience is acquired, right.

1:12:28.160 --> 1:12:33.840
 For instance, if you're trying to play Go, your experience with Go is all the Go games

1:12:33.840 --> 1:12:39.000
 you've played or you've seen or you've simulated in your mind, let's say.

1:12:39.000 --> 1:12:46.720
 And your priors are things like, well, Go is a game on a 2D grid, and we have lots of

1:12:46.720 --> 1:12:53.440
 hard coded priors about the organization of 2D space.

1:12:53.440 --> 1:13:00.880
 And the rules of how the dynamics of the physics of this game in this 2D space.

1:13:00.880 --> 1:13:04.000
 And the idea that you have what winning is.

1:13:04.000 --> 1:13:05.680
 Yes, exactly.

1:13:05.680 --> 1:13:10.320
 And other board games can also share some similarities with Go, and if you've played

1:13:10.320 --> 1:13:15.080
 these board games, then with respect to the game of Go, that would be part of your priors

1:13:15.080 --> 1:13:16.080
 about the game.

1:13:16.080 --> 1:13:19.880
 Well, it's interesting to think about the game of Go is how many priors are actually

1:13:19.880 --> 1:13:22.760
 brought to the table.

1:13:22.760 --> 1:13:28.960
 When you look at self play, reinforcement learning based mechanisms that do learning,

1:13:28.960 --> 1:13:31.080
 it seems like the number of priors is pretty low.

1:13:31.080 --> 1:13:32.080
 Yes.

1:13:32.080 --> 1:13:33.080
 But you're saying you should be...

1:13:33.080 --> 1:13:35.840
 There is a 2D special priors in the cabinet.

1:13:35.840 --> 1:13:36.840
 Right.

1:13:36.840 --> 1:13:40.640
 But you should be clear at making those priors explicit.

1:13:40.640 --> 1:13:41.640
 Yes.

1:13:41.640 --> 1:13:48.080
 So in part, I think if your goal is to measure a human life form of intelligence, then you

1:13:48.080 --> 1:13:55.280
 should clearly establish that you want the AI you're testing to start from the same set

1:13:55.280 --> 1:13:58.920
 of priors that humans start with.

1:13:58.920 --> 1:14:03.680
 So, I mean, to me personally, but I think to a lot of people, the human side of things

1:14:03.680 --> 1:14:05.400
 is very interesting.

1:14:05.400 --> 1:14:13.480
 So testing intelligence for humans, what do you think is a good test of human intelligence?

1:14:13.480 --> 1:14:19.240
 Well, let's do a question that Psychometrics is interested in.

1:14:19.240 --> 1:14:20.240
 What is?

1:14:20.240 --> 1:14:23.840
 There's an entire subfield of psychology that deals with this question.

1:14:23.840 --> 1:14:25.200
 So what's Psychometrics?

1:14:25.200 --> 1:14:33.120
 The Psychometrics is the subfield of psychology that tries to measure, quantify aspects of

1:14:33.120 --> 1:14:34.120
 the human mind.

1:14:34.120 --> 1:14:39.800
 So in particular, cognitive abilities, intelligence, and personality traits as well.

1:14:39.800 --> 1:14:49.640
 So what are, might be a weird question, but what are the first principles of Psychometrics

1:14:49.640 --> 1:14:55.480
 that operates on, you know, what are the priors it brings to the table?

1:14:55.480 --> 1:15:01.400
 So it's a field with a fairly long history.

1:15:01.400 --> 1:15:08.880
 It's, so you know, psychology sometimes gets a bad reputation for not having very reproducible

1:15:08.880 --> 1:15:14.240
 results and some Psychometrics has actually some fairly slightly reproducible results.

1:15:14.240 --> 1:15:21.320
 So the ideal goals of the field is, you know, tests should be reliable, which is an ocean

1:15:21.320 --> 1:15:23.240
 tide to your productivity.

1:15:23.240 --> 1:15:30.960
 It should be valid, meaning that it should actually measure what you say it measures.

1:15:30.960 --> 1:15:35.720
 So for instance, if you're saying that you're measuring intelligence, then your test results

1:15:35.720 --> 1:15:40.880
 should be correlated with things that you expect to be correlated with intelligence like success

1:15:40.880 --> 1:15:46.400
 in school or success in the workplace and so on, should be standardized, meaning that

1:15:46.400 --> 1:15:51.280
 you can administer your tests to many different people in some conditions, and it should be

1:15:51.280 --> 1:15:57.520
 free from bias, meaning that for instance, if your, if your test involves the English

1:15:57.520 --> 1:16:03.120
 language, then you have to be aware that this creates a bias against people who have English

1:16:03.120 --> 1:16:07.360
 as their second language or people who can't speak English at all.

1:16:07.360 --> 1:16:12.760
 So of course, these, these principles for creating Psychometric tests are very much

1:16:12.760 --> 1:16:13.760
 90 all.

1:16:13.760 --> 1:16:21.400
 I don't think every Psychometric test is, is really either reliable, valid, or offered

1:16:21.400 --> 1:16:26.960
 from bias, but at least the field is aware of these weaknesses and is trying to address

1:16:26.960 --> 1:16:27.960
 them.

1:16:27.960 --> 1:16:32.800
 So it's kind of interesting, ultimately, you're only able to measure like you said previously

1:16:32.800 --> 1:16:38.160
 the skill, but you're trying to do a bunch of measures of different skills that correlate.

1:16:38.160 --> 1:16:39.160
 Yes.

1:16:39.160 --> 1:16:43.120
 You mentioned strongly with some general concept of cognitive ability.

1:16:43.120 --> 1:16:44.120
 Yes, yes.

1:16:44.120 --> 1:16:46.760
 So what's the G factor?

1:16:46.760 --> 1:16:52.920
 So right, there are many different kinds of tests, tests of intelligence and each of them

1:16:52.920 --> 1:16:55.880
 is interested in different aspects of intelligence.

1:16:55.880 --> 1:17:01.080
 You know, some of them will deal with language, some of them will deal with special vision,

1:17:01.080 --> 1:17:04.680
 maybe mental rotations, numbers and so on.

1:17:04.680 --> 1:17:10.920
 When you run these very different tests at scale, what you start seeing is that there

1:17:10.920 --> 1:17:13.720
 are clusters of correlations among test results.

1:17:13.720 --> 1:17:21.000
 So for instance, if you look at homework at school, you will see that people who do well

1:17:21.000 --> 1:17:25.720
 at math are also likely statistically to do well in physics.

1:17:25.720 --> 1:17:31.520
 And what's more, there are also people who do well at math and physics are also statistically

1:17:31.520 --> 1:17:38.760
 likely to do well in things that sound completely unrelated, like writing in English, for instance.

1:17:38.760 --> 1:17:46.040
 And so when you see clusters of correlations in statistical terms, you would explain them

1:17:46.040 --> 1:17:47.840
 with a latent variable.

1:17:47.840 --> 1:17:52.560
 And the latent variable that would, for instance, explain the relationship between being good

1:17:52.560 --> 1:17:57.480
 at math and being good at physics would be cognitive ability, right?

1:17:57.480 --> 1:18:05.560
 And the G factor is the latent variable that explains the fact that every test of intelligence

1:18:05.560 --> 1:18:10.600
 that you can come up with results on this test end up being correlated.

1:18:10.600 --> 1:18:19.000
 So there is some single, unique variable that explains these correlations, that's the G factor.

1:18:19.000 --> 1:18:20.440
 So it's a statistical construct.

1:18:20.440 --> 1:18:25.680
 It's not really something you can directly measure, for instance, in a person.

1:18:25.680 --> 1:18:26.680
 But it's there.

1:18:26.680 --> 1:18:27.680
 But it's there.

1:18:27.680 --> 1:18:28.680
 It's there.

1:18:28.680 --> 1:18:29.680
 It's there at scale.

1:18:29.680 --> 1:18:33.080
 And that's also one thing I want to mention about psychometrics.

1:18:33.080 --> 1:18:38.280
 Like, you know, when you talk about measuring intelligence in humans, for instance, some

1:18:38.280 --> 1:18:42.120
 people get a little bit worried, they will say, you know, that sounds dangerous, maybe

1:18:42.120 --> 1:18:44.560
 that sounds potentially discriminatory and so on.

1:18:44.560 --> 1:18:46.840
 And they are not wrong.

1:18:46.840 --> 1:18:53.240
 And the thing is, personally, I'm not interested in psychometrics as a way to characterize one

1:18:53.240 --> 1:19:01.200
 individual person, like if I get your psychometric personality assessment or your IQ, I don't

1:19:01.200 --> 1:19:05.000
 think that actually tells me much about you as a person.

1:19:05.000 --> 1:19:10.360
 I think psychometrics is most useful as a statistical tool.

1:19:10.360 --> 1:19:12.680
 So it's most useful at scale.

1:19:12.680 --> 1:19:17.640
 It's most useful when you start getting test results for a large number of people and you

1:19:17.640 --> 1:19:23.960
 start cross correlating these test results, because that gives you information about the

1:19:23.960 --> 1:19:29.920
 structure of the human mind, in particular about the structure of human cognitive abilities.

1:19:29.920 --> 1:19:35.720
 So at scale, psychometrics paints a certain picture of the human mind.

1:19:35.720 --> 1:19:37.400
 And that's interesting.

1:19:37.400 --> 1:19:40.760
 And that's what's relevant to AI, the structure of human cognitive abilities.

1:19:40.760 --> 1:19:45.960
 Yeah, it gives you an insight into, I mean, to me, I remember when I learned about GFactor,

1:19:45.960 --> 1:19:56.520
 it seemed like it would be impossible for it to be real, even as a statistical variable.

1:19:56.520 --> 1:20:02.280
 It felt kind of like astrology, like it's like wishful thinking about psychologists.

1:20:02.280 --> 1:20:06.920
 But the more I learned, I realized that there's some, I mean, I'm not sure what to make about

1:20:06.920 --> 1:20:11.880
 human beings, the fact that the GFactor is a thing, that there's a commonality across all

1:20:11.880 --> 1:20:17.000
 of human species, that there does seem to be a strong correlation between cognitive abilities.

1:20:17.000 --> 1:20:18.440
 That's kind of fascinating.

1:20:18.440 --> 1:20:19.080
 Yeah.

1:20:19.080 --> 1:20:25.240
 So human connectivities have a structure, like the most mainstream theory of the structure

1:20:25.240 --> 1:20:28.680
 of connectivities is called a CHC theory.

1:20:28.680 --> 1:20:34.440
 It's a cattle horn, Carol, it's named after the three psychologists who contributed key pieces of it.

1:20:34.440 --> 1:20:40.360
 And it describes cognitive abilities as a hierarchy with three levels.

1:20:40.360 --> 1:20:45.720
 And at the top, you have the GFactor, then you have broad cognitive abilities, for instance,

1:20:45.720 --> 1:20:55.960
 fluid intelligence, that encompass a broad set of possible kinds of tasks that are all related.

1:20:55.960 --> 1:21:02.440
 And then you have narrow cognitive abilities at the last level, which is closer to task specific

1:21:02.440 --> 1:21:09.800
 skill. And there are actually different theories of the structure of cognitive abilities.

1:21:09.800 --> 1:21:13.560
 They just emerge from different statistical analysis of IQ test results.

1:21:14.280 --> 1:21:21.000
 But they all describe a hierarchy with a kind of GFactor at the top.

1:21:21.000 --> 1:21:26.200
 And you're right that the GFactor is, it's not quite real in the sense that it's not

1:21:27.000 --> 1:21:30.120
 something you can observe and measure, like your height, for instance.

1:21:30.120 --> 1:21:36.200
 But it's really in the sense that you see it in a statistical analysis of the data.

1:21:37.640 --> 1:21:41.960
 One thing I want to mention is that the fact that there is a GFactor does not really mean that

1:21:41.960 --> 1:21:48.120
 human intelligence is general in a strong sense, does not mean human intelligence can be applied

1:21:48.120 --> 1:21:53.240
 to any problem at all and that someone who has a high IQ is going to be able to solve any problem

1:21:53.240 --> 1:21:55.400
 at all. That's not quite what it means, I think.

1:21:55.400 --> 1:22:04.680
 One popular analogy to understand it is the sports analogy. If you consider the concept

1:22:04.680 --> 1:22:10.440
 of physical fitness, it's a concept that's very similar to intelligence because it's a useful

1:22:10.440 --> 1:22:17.560
 concept. It's something you can intuitively understand. Some people are fit, maybe like you,

1:22:17.560 --> 1:22:21.640
 some people are not as fit, maybe like me. But none of us can fly.

1:22:21.640 --> 1:22:30.040
 Absolutely. Even if you're very fit, that doesn't mean you can do anything at all in

1:22:30.040 --> 1:22:35.160
 any environment. You obviously cannot fly, you cannot serve at the bottom of the ocean and so

1:22:35.160 --> 1:22:42.760
 on. And if you were a scientist and you wanted to precisely define and measure physical fitness

1:22:42.760 --> 1:22:49.560
 in humans, then you would come up with a battery of tests, like you would have running 100 meter,

1:22:49.560 --> 1:22:57.640
 playing soccer, playing table tennis, swimming, and so on. And if you ran these tests over many

1:22:57.640 --> 1:23:01.720
 different people, you would start seeing correlations and test results. For instance,

1:23:01.720 --> 1:23:08.520
 people who are good at soccer are also good at sprinting. And you would explain these correlations

1:23:08.520 --> 1:23:14.360
 with physical abilities that are strictly analogous to cognitive abilities. And then you would

1:23:14.360 --> 1:23:22.040
 start also observing correlations between biological characteristics, like maybe lung

1:23:22.040 --> 1:23:28.440
 volume is correlated with being a fast runner, for instance. In the same way that there are

1:23:29.080 --> 1:23:38.200
 neurophysical correlates of cognitive abilities. And at the top of the hierarchy of physical

1:23:38.200 --> 1:23:42.920
 abilities that you would be able to observe, you would have a g factor, a physical g factor,

1:23:42.920 --> 1:23:49.720
 which would map to physical fitness. And as you just said, that doesn't mean that people with

1:23:49.720 --> 1:23:55.560
 high physical fitness can't fly. It doesn't mean human morphology and human physiology is universal.

1:23:55.560 --> 1:24:04.600
 It's actually super specialized. We can only do the things that we evolve to do. We are not

1:24:04.600 --> 1:24:11.640
 appropriate to... You could not exist on Venus or Mars or in the void of space or the bottom of the

1:24:11.640 --> 1:24:20.360
 ocean. So that said, one thing that's really striking and remarkable is that our morphology

1:24:22.120 --> 1:24:29.240
 generalizes far beyond the environments that we evolved for. Like in a way, you could say we evolved

1:24:29.240 --> 1:24:36.120
 to run after prey in the savanna, right? That's very much where our human morphology comes from.

1:24:36.120 --> 1:24:43.560
 And that said, we can do a lot of things that are completely unrelated to that. We can climb

1:24:43.560 --> 1:24:50.600
 mountains. We can swim across lakes. We can play table tennis. I mean, table tennis is very different

1:24:50.600 --> 1:24:56.280
 from what we were evolved to do, right? So our morphology, our bodies, our sense and motor

1:24:56.280 --> 1:25:03.480
 affordances have a degree of generality that is absolutely remarkable, right? And I think cognition

1:25:03.480 --> 1:25:09.720
 is very similar to that. Our cognitive abilities have a degree of generality that goes far beyond

1:25:09.720 --> 1:25:15.720
 what the mind was initially supposed to do, which is why we can play music and write novels and go

1:25:15.720 --> 1:25:21.080
 to Mars and do all kinds of crazy things. But it's not universal in the same way that human

1:25:21.080 --> 1:25:27.640
 morphology and our body is not appropriate for actually most of the universe by volume,

1:25:27.640 --> 1:25:31.480
 in the same way you could say that the human mind is not really appropriate for most of

1:25:31.480 --> 1:25:39.080
 problem space, potential problem space by volume. So we have very strong cognitive biases,

1:25:39.080 --> 1:25:44.520
 actually. That means that there are certain types of problems that we handle very well and certain

1:25:44.520 --> 1:25:51.560
 types of problems that we are completely inadapted for. So that's really how we interpret

1:25:51.560 --> 1:25:59.960
 the g factor. It's not a sign of strong generality. It's really just the broadest cognitive ability.

1:25:59.960 --> 1:26:06.200
 But our abilities, whether we are talking about sensory motor abilities or cognitive abilities,

1:26:06.200 --> 1:26:11.160
 they still, they remain very specialized in the human condition, right?

1:26:12.600 --> 1:26:17.320
 Within the constraints of the human cognition, they're general.

1:26:18.280 --> 1:26:19.240
 Yes, absolutely.

1:26:19.240 --> 1:26:21.480
 But the constraints, as you're saying, are very limited.

1:26:21.480 --> 1:26:23.480
 I think what's limiting.

1:26:23.480 --> 1:26:29.400
 So we evolved our cognition and our body evolved in very specific environments

1:26:29.400 --> 1:26:33.800
 because our environment was so valuable, fast changing and so unpredictable.

1:26:34.440 --> 1:26:40.680
 Part of the constraints that drove our evolution is generality itself. So we were in a way evolved

1:26:40.680 --> 1:26:46.040
 to be able to improvise in all kinds of physical or cognitive environments, right?

1:26:47.560 --> 1:26:53.960
 And for this reason, it turns out that the minds and bodies that we ended up with

1:26:53.960 --> 1:26:59.960
 can be applied to much, much broader scope than what they were evolved for, right?

1:26:59.960 --> 1:27:05.400
 And that's truly remarkable. And that goes, that's a degree of generalization that is far beyond

1:27:05.400 --> 1:27:08.360
 anything you can see in artificial systems today, right?

1:27:10.280 --> 1:27:15.720
 That's it. It does not mean that human intelligence is anywhere universal.

1:27:16.280 --> 1:27:22.120
 Yeah, it's not general. You know, it's a kind of exciting topic for people even outside of

1:27:22.120 --> 1:27:30.280
 artificial intelligence IQ tests. I think it's Mensa, whatever, there's different degrees of

1:27:30.280 --> 1:27:35.720
 difficulty for questions. We talked about this offline a little bit too about sort of difficult

1:27:35.720 --> 1:27:43.720
 questions. What makes a question on an IQ test more difficult or less difficult, do you think?

1:27:43.720 --> 1:27:50.440
 So the thing to keep in mind is that there's no such thing as a question that's intrinsically

1:27:50.440 --> 1:27:56.280
 difficult. It has to be difficult to suspect to the things you already know, and the things you

1:27:56.280 --> 1:28:05.240
 cannot really do, right? So in terms of an IQ test question, typically it would be structured,

1:28:05.240 --> 1:28:13.320
 for instance, as a set of demonstration input and output pairs, right? And then you would be given

1:28:13.320 --> 1:28:19.720
 a test input, a prompt, and you would need to recognize or produce the corresponding output.

1:28:19.720 --> 1:28:27.640
 And in that narrow context, you could say a difficult question is a question where

1:28:29.560 --> 1:28:36.360
 the input prompt is very surprising and unexpected given the training example.

1:28:36.360 --> 1:28:40.040
 Just even the nature of the patterns that you're observing in the input prompt.

1:28:40.040 --> 1:28:45.800
 For instance, let's say you have a rotation problem. You must rotate the shape by 90 degrees.

1:28:45.800 --> 1:28:51.720
 If I give you two examples, and then I give you one prompt, which is actually one of the two

1:28:51.720 --> 1:28:57.080
 training examples, then there is zero generalization difficulty for the task. It's actually a trivial

1:28:57.080 --> 1:29:02.280
 task. You just recognize that it's one of the training examples and you produce the same answer.

1:29:02.280 --> 1:29:08.280
 Now, if it's a more complex shape, there is a little bit more generalization, but it remains

1:29:08.280 --> 1:29:14.120
 that you are still doing the same thing at this time as you were being demonstrated at

1:29:14.120 --> 1:29:21.960
 training time. A difficult task does require some amount of test time adaptation, some amount of

1:29:23.320 --> 1:29:30.920
 improvisation, right? So consider, I don't know, you're teaching a class on quantum physics or

1:29:30.920 --> 1:29:41.400
 something. If you wanted to kind of test the understanding that students have of the material,

1:29:41.400 --> 1:29:48.760
 you would come up with an exam that's very different from anything they've seen,

1:29:48.760 --> 1:29:54.680
 like on the Internet when they were cramming. On the other hand, if you wanted to make it easy,

1:29:54.680 --> 1:30:02.360
 you would just give them something that's very similar to the mock exams that they've taken,

1:30:02.360 --> 1:30:06.520
 something that's just a simple interpolation of questions that they've already seen.

1:30:06.520 --> 1:30:12.200
 And so that would be an easy exam. It's very similar to what you've been trained on. And a

1:30:12.200 --> 1:30:18.920
 difficult exam is one that really probes your understanding because it forces you to improvise.

1:30:18.920 --> 1:30:26.440
 It forces you to do things that are different from what you were exposed to before. So that said,

1:30:27.080 --> 1:30:32.600
 it doesn't mean that the exam that requires improvisation is intrinsically hard, right?

1:30:32.600 --> 1:30:38.040
 Because maybe you're a quantum physics expert. So when you take the exam, this is actually stuff

1:30:38.040 --> 1:30:44.600
 that despite being new to the students, it's not new to you, right? So it can only be difficult

1:30:44.600 --> 1:30:51.880
 with respect to what the test taker already knows, and with respect to the information that

1:30:51.880 --> 1:30:58.360
 the test taker has about the task. So that's what I mean by controlling for priors what you,

1:30:58.360 --> 1:31:03.480
 the information you bring to the table. And the experience, which is the training data. So in the

1:31:03.480 --> 1:31:10.360
 case of the quantum physics exam, that would be all the course material itself and all the mock

1:31:10.360 --> 1:31:15.560
 exams that students might have taken online. Yeah, it's interesting because I've also,

1:31:16.200 --> 1:31:22.040
 I sent you an email and asked you, like, I've been, this just this curious question of,

1:31:22.040 --> 1:31:30.680
 you know, what's a really hard IQ test question? And I've been talking to also people who have

1:31:30.680 --> 1:31:35.720
 designed IQ tests as a few folks on the internet. It's like a thing. People are really curious

1:31:35.720 --> 1:31:43.880
 about it. First of all, most of the IQ tests they designed, they like religiously protect against

1:31:43.880 --> 1:31:49.160
 the correct answers. Like you can't find the correct answers anywhere. In fact, the question

1:31:49.160 --> 1:31:54.160
 is ruined once you know, even like the approach you're supposed to take. So they're very

1:31:54.160 --> 1:31:59.320
 that said, the approach is implicit in the training examples. So if you release the train

1:31:59.320 --> 1:32:06.840
 examples, it's over. Well, which is why in arc, for instance, there's a test set that is private

1:32:06.840 --> 1:32:15.160
 and no one has seen it. No, for really tough IQ questions, it's not obvious. It's not because

1:32:15.160 --> 1:32:22.280
 the ambiguity. Like it's, I mean, we'll have to look through them, but like some number sequences

1:32:22.280 --> 1:32:29.000
 and so on, it's not completely clear. So like, you can get a sense, but there's like some,

1:32:30.440 --> 1:32:33.000
 you know, when you look at a number sequence, I don't know,

1:32:35.960 --> 1:32:39.960
 like your Fibonacci number sequence, if you look at the first few numbers, that sequence

1:32:39.960 --> 1:32:45.480
 could be completed in a lot of different ways. And, you know, some are, if you think deeply,

1:32:45.480 --> 1:32:51.800
 are more correct than others. Like there's a kind of intuitive simplicity and elegance

1:32:51.800 --> 1:32:57.880
 to the correct solution. Yes, I am personally not a fan of ambiguity in test questions,

1:32:57.880 --> 1:33:04.280
 actually. But I think you can have difficulty without requiring ambiguity simply by making the

1:33:04.280 --> 1:33:11.480
 test require a lot of extrapolation over the training examples. But the beautiful question

1:33:12.440 --> 1:33:16.040
 is difficult, but gives away everything when you give the training example.

1:33:17.080 --> 1:33:24.920
 Basically, yes. Meaning that, so the tests I'm interested in creating are not necessarily

1:33:24.920 --> 1:33:32.760
 difficult for humans, because human intelligence is the benchmark. They're supposed to be difficult

1:33:32.760 --> 1:33:39.160
 for machines in ways that are easy for humans. Like I think an ideal test of human and machine

1:33:39.160 --> 1:33:47.800
 intelligence is a test that is actionable, that highlights the need for progress, and that

1:33:47.800 --> 1:33:52.760
 highlights the direction in which you should be making progress. I think we'll talk about

1:33:52.760 --> 1:33:57.800
 the RR challenge and the test you've constructed, and you have these elegant examples. I think

1:33:57.800 --> 1:34:03.160
 that highlight, like this is really easy for us humans, but it's really hard for machines.

1:34:03.720 --> 1:34:11.960
 But on the designing an IQ test for IQs of like a higher than 160 and so on,

1:34:12.680 --> 1:34:16.520
 you have to say, you have to take that and put it on steroids, right? You have to think like,

1:34:16.520 --> 1:34:22.840
 what is hard for humans? And that's a fascinating exercise in itself, I think.

1:34:22.840 --> 1:34:29.400
 And it was an interesting question of what it takes to create a really hard question for humans,

1:34:29.400 --> 1:34:40.280
 because you again have to do the same process as you mentioned, which is something basically

1:34:41.000 --> 1:34:46.200
 where the experience that you have likely to have encountered throughout your whole life,

1:34:46.200 --> 1:34:52.200
 even if you've prepared for IQ tests, which is a big challenge, that this will still be novel for

1:34:52.200 --> 1:34:59.160
 you. Yeah, I mean, novelty is a requirement. You should not be able to practice for the questions

1:34:59.160 --> 1:35:03.640
 that you're going to be tested on. That's important. Because otherwise, what you're doing

1:35:03.640 --> 1:35:09.560
 is not exhibiting intelligence, what you're doing is just retrieving what you've been exposed before.

1:35:09.560 --> 1:35:13.640
 It's the same thing as a deep learning model. If you train a deep learning model on

1:35:13.640 --> 1:35:23.720
 all the possible answers, then it will ace your test. In the same way that a stupid student can

1:35:23.720 --> 1:35:32.040
 still ace the test, if they cram for it, they memorize 100 different possible mock exams.

1:35:32.040 --> 1:35:38.120
 And then they hope that the actual exam will be a very simple interpolation of the mock exams.

1:35:38.120 --> 1:35:41.080
 And that student could just be a deep learning model at that point.

1:35:41.080 --> 1:35:45.720
 And that student could just be a deep learning model at that point. But you can actually do that

1:35:45.720 --> 1:35:51.160
 without any understanding of the material. And in fact, many students pass the exams in exactly

1:35:51.160 --> 1:35:56.520
 this way. And if you want to avoid that, you need an exam that's unlike anything they've seen,

1:35:56.520 --> 1:36:04.360
 that really probes their understanding. So how do we design an IQ test for machines?

1:36:04.360 --> 1:36:14.280
 All right, so in the paper, I outline a number of requirements that you expect of such a test.

1:36:15.000 --> 1:36:23.320
 And in particular, we should start by acknowledging the priors that we expect to be required

1:36:23.320 --> 1:36:27.240
 in order to perform the test. So we should be explicit about the priors.

1:36:28.440 --> 1:36:32.680
 And if the goal is to compare machine intelligence and human intelligence,

1:36:32.680 --> 1:36:42.040
 then we should assume human cognitive priors. And secondly, we should make sure that we are testing

1:36:42.040 --> 1:36:47.720
 for skill acquisition ability, skill acquisition efficiency in particular, and not for skill

1:36:47.720 --> 1:36:54.040
 itself, meaning that every task featured in your test should be novel and should not be

1:36:54.040 --> 1:36:58.200
 something that you can anticipate. So for instance, it should not be possible to

1:36:58.200 --> 1:37:05.720
 brute force the space of possible questions to pregenerate every possible question and answer.

1:37:06.840 --> 1:37:12.360
 So it should be tasks that cannot be anticipated, not just by the system itself,

1:37:12.360 --> 1:37:17.880
 but by the creators of the system. Yeah, you know what's fascinating? I mean,

1:37:17.880 --> 1:37:25.320
 one of my favorite aspects of the paper and the work you do, the ARC challenge, is the process

1:37:25.320 --> 1:37:34.680
 of making priors explicit. Just even that act alone is a really powerful one of like, what are,

1:37:35.720 --> 1:37:42.360
 it's a really powerful question, ask of us humans, what are the priors that we bring to the table?

1:37:44.120 --> 1:37:49.640
 So the next step is like, once you have those priors, how do you use them to solve a novel

1:37:49.640 --> 1:37:55.480
 task? But like just even making the priors explicit is a really difficult and really powerful step.

1:37:56.200 --> 1:38:02.440
 And that's like visually beautiful and conceptually philosophically beautiful part of the work you

1:38:02.440 --> 1:38:08.920
 did with, and I guess continue to do probably with the paper and the ARC challenge. Can you

1:38:08.920 --> 1:38:14.760
 talk about some of the priors that we're talking about here? Yes. So a researcher has done a lot

1:38:14.760 --> 1:38:24.360
 of work on what exactly are the knowledge priors that are innate to humans is Elizabeth Spelke

1:38:24.360 --> 1:38:34.440
 from Harvard. So she developed the core knowledge theory, which outlines four different core

1:38:34.440 --> 1:38:41.560
 knowledge systems. So systems of knowledge that we are basically either born with or that we are

1:38:41.560 --> 1:38:51.240
 hardwired to acquire very early on in our development. And there's no strong distinction

1:38:51.240 --> 1:39:01.080
 between the two. Like if you are primed to acquire a certain type of knowledge, in just a few weeks,

1:39:01.080 --> 1:39:07.560
 you might as well just be born with it. It's just part of who you are. And so there are four

1:39:07.560 --> 1:39:15.160
 different core knowledge systems. Like the first one is the notion of objectness and basic physics.

1:39:16.280 --> 1:39:24.200
 Like you recognize that something that moves currently, for instance, is an object. So we

1:39:24.200 --> 1:39:30.440
 intuitively naturally, innately divide the world into objects based on this notion of

1:39:30.440 --> 1:39:38.360
 coherence, physical coherence. And in terms of elementary physics, there's the fact that objects

1:39:38.360 --> 1:39:45.880
 can bump against each other and the fact that they can occlude each other. So these are things that

1:39:45.880 --> 1:39:52.680
 we are essentially born with or at least that we are going to be acquiring extremely early because

1:39:52.680 --> 1:40:01.000
 really hardwired to acquire them. So a bunch of points, pixels that move together on objects

1:40:01.000 --> 1:40:10.120
 are partly the same object. Yes. I mean, I don't smoke weed, but if I did,

1:40:11.160 --> 1:40:15.560
 that's something I could sit all night and just think about. I remember writing in your paper

1:40:15.560 --> 1:40:26.040
 just objectness. I wasn't self aware of that particular prior. That's such a fascinating

1:40:26.040 --> 1:40:35.720
 prior. That's the most basic one. Objectness, just identity, objectness. It's very basic,

1:40:35.720 --> 1:40:40.680
 I suppose, but it's so fundamental. It is fundamental to human cognition.

1:40:40.680 --> 1:40:48.040
 Yeah. And the second prior that's also fundamental is agentness, which is not a real world,

1:40:48.040 --> 1:40:55.240
 a real world, but so agentness. The fact that some of these objects that you segment your

1:40:55.240 --> 1:41:02.040
 environment into, some of these objects are agents. So what's an agent? Basically, it's

1:41:02.040 --> 1:41:08.680
 an object that has goals. That has what? That has goals. There's capable of

1:41:08.680 --> 1:41:15.640
 pursuing goals. So for instance, if you see two dots moving in a roughly synchronized fashion,

1:41:16.200 --> 1:41:24.280
 you will intuitively infer that one of the dots is pursuing the other. So one of the dots is,

1:41:25.160 --> 1:41:30.120
 and one of the dots is an agent, and its goal is to avoid the other dot. And one of the dots,

1:41:30.120 --> 1:41:37.560
 the other dot, is also an agent, and its goal is to catch the first dot. Pelke has shown that

1:41:37.560 --> 1:41:46.280
 babies as young as three months identify agentness and goal directedness in their environment.

1:41:46.280 --> 1:41:53.560
 Another prior is basic geometry and topology, like the notion of distance,

1:41:53.560 --> 1:41:59.480
 the ability to navigate in your environment, and so on. This is something that is fundamentally

1:41:59.480 --> 1:42:07.640
 hardwired into our brain. It's in fact backed by very specific neural mechanisms, like for instance,

1:42:08.440 --> 1:42:16.920
 grid cells and plate cells. So it's something that's literally hardcoded at the neural level

1:42:17.800 --> 1:42:24.760
 in our hippocampus. And the last prior would be the notion of numbers, like numbers are not

1:42:24.760 --> 1:42:32.840
 actually a cultural construct. We are intuitively, innately able to do some basic counting and to

1:42:32.840 --> 1:42:40.120
 compare quantities. So it doesn't mean we can do arbitrary arithmetic. Counting, the actual counting.

1:42:40.120 --> 1:42:46.280
 Like counting one, two, three, then maybe more than three. You can also compare quantities if I give

1:42:46.280 --> 1:42:53.720
 you three dots and five dots, you can tell the side with five dots as more dots. So this is

1:42:53.720 --> 1:43:02.280
 actually an innate prior. So that said, the list may not be exhaustive. So Spelki is still

1:43:03.720 --> 1:43:10.360
 pursuing the potential existence of new knowledge systems, for instance,

1:43:11.160 --> 1:43:15.480
 knowledge systems that we deal with social relationships.

1:43:15.480 --> 1:43:24.040
 Yeah. Which is much less relevant to something like ARC or IQ test.

1:43:24.040 --> 1:43:30.440
 Right. There could be stuff that's, like you said, rotation or symmetry. Is it really interesting?

1:43:31.000 --> 1:43:36.440
 It's very likely that there is, speaking about rotation, that there is in the brain

1:43:37.560 --> 1:43:40.520
 a hardcoded system that is capable of performing rotations.

1:43:40.520 --> 1:43:48.520
 One famous experiment that people did in the, I don't remember who it was exactly, but in the

1:43:48.520 --> 1:43:56.840
 70s was that people found that if you asked people, if you give them two different shapes,

1:43:57.560 --> 1:44:02.680
 and one of the shapes is a rotated version of the first shape, and you ask them,

1:44:03.240 --> 1:44:09.400
 is that shape a rotated version of the first shape or not? What you see is that the time it

1:44:09.400 --> 1:44:16.760
 takes people to answer is linearly proportional, right, to the angle of rotation. So it's almost

1:44:16.760 --> 1:44:24.760
 like you have it somewhere in your brain, like a turntable with a fixed speed. And if you want to

1:44:24.760 --> 1:44:31.000
 know if two objects are rotated versions of each other, you put the object on the turntable,

1:44:32.040 --> 1:44:36.760
 you let it move around a little bit, and then you stop when you have a match.

1:44:36.760 --> 1:44:41.720
 And that's really interesting. So what's the arc challenge?

1:44:42.680 --> 1:44:49.560
 So in the paper, I outlined all these principles that a good test of machine

1:44:49.560 --> 1:44:54.280
 intelligence and human intelligence should follow. And the arc challenge is one attempt

1:44:55.160 --> 1:45:00.440
 to embody as many of these principles as possible. So I don't think it's anywhere near

1:45:00.440 --> 1:45:06.920
 a perfect attempt, right? It does not actually follow every principle, but it is

1:45:07.560 --> 1:45:14.760
 what I was able to do given the constraints. So the format of arc is very similar to classic

1:45:14.760 --> 1:45:20.440
 IQ tests, in particular Raven's Progessive Metruses. Yeah, Raven's Progessive Metruses.

1:45:20.440 --> 1:45:24.040
 I mean, if you've done IQ tests in the past, you know where that is probably,

1:45:24.040 --> 1:45:31.240
 at least you've seen it, even if you don't know what it's called. And so you have a set of tasks,

1:45:31.240 --> 1:45:38.840
 that's what they're called. And for each task, you have training data, which is a set of input

1:45:38.840 --> 1:45:46.600
 and output pairs. So an input or output pair is a grid of colors, basically. The size of the

1:45:46.600 --> 1:45:54.920
 grid is variable, is the size of the grid is variable. And you're given an input and you

1:45:54.920 --> 1:46:01.960
 must transform it into the proper output, right? And so you're shown a few demonstrations

1:46:01.960 --> 1:46:06.840
 of a task in the form of existing input output pairs, and then you're given a new input,

1:46:06.840 --> 1:46:16.680
 and you must provide, you must produce the correct output. And the assumptions

1:46:18.840 --> 1:46:28.600
 in arc is that every task should only require core knowledge priors, should not require any

1:46:28.600 --> 1:46:38.520
 outside knowledge. So for instance, no language, no English, nothing like this, no concepts taken

1:46:38.520 --> 1:46:48.600
 from our human experience, like trees, dogs, cats, and so on. So only tasks that are, reasoning tasks

1:46:48.600 --> 1:46:56.120
 that are built on top of core knowledge priors. And some of the tasks are actually explicitly

1:46:56.120 --> 1:47:04.840
 trying to probe specific forms of abstraction, right? Part of the reason why I wanted to create arc

1:47:05.480 --> 1:47:16.440
 is I'm a big believer in, you know, when you're faced with a problem as murky as

1:47:17.240 --> 1:47:21.560
 understanding how to autonomously generate abstraction in a machine,

1:47:21.560 --> 1:47:28.520
 you have to co evolve the solution and the problem. And so part of the reason why I designed arc

1:47:28.520 --> 1:47:35.160
 was to clarify my ideas about the nature of abstraction, right? And some of the tasks are

1:47:35.160 --> 1:47:41.800
 actually designed to probe bits of that theory. And there are things that are turned out to be

1:47:41.800 --> 1:47:47.000
 very easy for humans to perform, including young kids, right? But turned out to be

1:47:47.000 --> 1:47:52.680
 not to be near impossible for machines. So what have you learned from the nature of abstraction

1:47:54.280 --> 1:48:00.920
 from designing that? Can you clarify what you mean? One of the things you wanted to try to

1:48:00.920 --> 1:48:10.520
 understand was this idea of abstraction? Yes. So clarifying my own ideas about abstraction by

1:48:10.520 --> 1:48:17.480
 forcing myself to produce tasks that would require the ability to produce that form of

1:48:17.480 --> 1:48:23.000
 abstraction in order to solve them. Got it. Okay. So, and by the way, just to, I mean,

1:48:23.000 --> 1:48:28.360
 people should check out, I'll probably overlay if you're watching the video part, but the grid input

1:48:28.360 --> 1:48:35.560
 output with the different colors on the grid. That's it. That's that means a very simple world.

1:48:35.560 --> 1:48:40.840
 But it's kind of beautiful. It's very similar to classic acutest. Like, it's not very original

1:48:40.840 --> 1:48:46.680
 in that sense. The main difference with acutest is that we make the priors explicit, which is not

1:48:46.680 --> 1:48:51.560
 usually the case in acutest. So you might get explicit that everything should only be built

1:48:51.560 --> 1:48:59.000
 out of core knowledge priors. I also think it's generally more diverse than acutest in general.

1:48:59.000 --> 1:49:05.960
 And it's, it perhaps requires a bit more manual work to produce solutions because you have to

1:49:05.960 --> 1:49:11.880
 click around on a grid for a while. Sometimes the grades can be as large as cell by cell cells.

1:49:11.880 --> 1:49:18.760
 So how did you come up? If you can reveal with the questions, like what's the process

1:49:18.760 --> 1:49:23.240
 of the questions? Was it mostly you? Yeah, they came up with the questions. What,

1:49:23.240 --> 1:49:30.600
 how difficult is it to come up with a question? Like, is this scalable to a much larger number?

1:49:30.600 --> 1:49:35.720
 If we think, you know, with acutest, you might not necessarily want it to or need it to be scalable

1:49:36.280 --> 1:49:41.480
 with machines. It's possible you could argue that it needs to be scalable.

1:49:41.480 --> 1:49:48.200
 So there are a thousand questions, a thousand tasks, including the test set, the private test set.

1:49:48.200 --> 1:49:54.600
 I think it's fairly difficult in the sense that a big requirement is that every task should be

1:49:55.320 --> 1:50:04.040
 novel and unique and unpredictable, right? Like you don't want to create your own little world

1:50:04.040 --> 1:50:10.200
 that is simple enough that it would be possible for a human to reverse and generate

1:50:10.920 --> 1:50:16.840
 and write down an algorithm that could generate every possible arc task and their solution.

1:50:16.840 --> 1:50:21.000
 So in a sense, that would completely invalidate the test. So you're constantly coming up with new

1:50:21.000 --> 1:50:29.400
 stuff. Yeah, you need a source of novelty, of unfakeable novelty. And one thing I found is that

1:50:30.120 --> 1:50:37.320
 as a human, you are not a very good source of unfakeable novelty. And so you have to

1:50:38.040 --> 1:50:43.080
 base the creation of these tasks quite a bit. There are only so many unique tasks that you

1:50:43.080 --> 1:50:51.080
 can do in a given day. So that means coming up with truly original new ideas. Did psychedelics

1:50:51.080 --> 1:50:56.840
 help you at all? No, it's okay. But I mean, that's fascinating to think about. So you would be like

1:50:56.840 --> 1:51:02.040
 walking or something like that. Are you constantly thinking of something totally new?

1:51:02.840 --> 1:51:10.440
 Yes. I mean, this is hard. This is hard. Yeah, I mean, I'm not saying I've done

1:51:10.440 --> 1:51:15.880
 anywhere near perfect job at it. There is some amount of redundancy, and there are many imperfections

1:51:15.880 --> 1:51:22.600
 in arc. So that said, you should consider arc as a work in progress. It is not the definitive

1:51:22.600 --> 1:51:29.960
 state where the arc tasks today are not the definitive state of the test. I want to keep

1:51:29.960 --> 1:51:37.560
 refining it. In the future, I also think it should be possible to open up the creation of tasks

1:51:37.560 --> 1:51:43.960
 to broad audience to do crowdsourcing. That would involve several levels of filtering, obviously.

1:51:43.960 --> 1:51:50.120
 But I think it's possible to apply crowdsourcing to develop a much bigger and much more diverse

1:51:50.120 --> 1:51:55.720
 arc data set that would also be free of potentially some of my own personal biases.

1:51:56.440 --> 1:52:02.120
 Is there always need to be a part of arc that the test is hidden?

1:52:02.120 --> 1:52:11.880
 Yes, absolutely. It is impressive that the test that you're using to actually benchmark algorithms

1:52:11.880 --> 1:52:16.040
 is not accessible to the people developing these algorithms. Because otherwise, what's

1:52:16.040 --> 1:52:21.720
 going to happen is that the human engineers are just going to solve the tasks themselves

1:52:21.720 --> 1:52:28.120
 and encode their solution in program form. But that again, what you're seeing here is

1:52:28.120 --> 1:52:33.880
 the process of intelligence happening in the mind of the human. And then you're just capturing

1:52:33.880 --> 1:52:38.920
 its crystallized output. But that crystallized output is not the same thing as the process

1:52:38.920 --> 1:52:44.600
 generated. It's not intelligent. So by the way, the idea of crowdsourcing it is fascinating.

1:52:46.040 --> 1:52:52.840
 I think the creation of questions is really exciting for people. I think there's a lot

1:52:52.840 --> 1:52:56.200
 of really brilliant people out there that love to create these kinds of stuff.

1:52:56.200 --> 1:53:03.080
 Yeah. One thing that surprised me that I wasn't expecting is that lots of people seem to actually

1:53:03.080 --> 1:53:12.600
 enjoy arc as a kind of game. And I was really seeing it as a test, as a benchmark of fluid

1:53:12.600 --> 1:53:19.080
 general intelligence. And lots of people, including kids, are just enjoying it as a game. So I think

1:53:19.080 --> 1:53:24.440
 that's encouraging. Yeah, I'm fascinated by it. There's a world of people who create IQ questions.

1:53:24.440 --> 1:53:35.320
 I think that's a cool activity for machines and for humans. And humans are themselves fascinated

1:53:35.320 --> 1:53:44.280
 by taking the questions, measuring their own intelligence. That's just really compelling.

1:53:44.280 --> 1:53:48.600
 It's really interesting to me too. It helps. One of the cool things about arc, you said,

1:53:48.600 --> 1:53:54.520
 it's kind of inspired by IQ tests or whatever. It follows a similar process. But because of its

1:53:54.520 --> 1:54:00.680
 nature, because of the context in which it lives, it immediately forces you to think about the nature

1:54:00.680 --> 1:54:06.120
 of intelligence as opposed to just the test of your own. It forces you to really think. I don't

1:54:06.120 --> 1:54:12.520
 know if it's within the question, inherent in the question, or just the fact that it lives

1:54:12.520 --> 1:54:17.720
 in the test that's supposed to be a test of machine intelligence. Absolutely. As you solve

1:54:17.720 --> 1:54:26.920
 arc tasks as a human, you will be forced to basically introspect how you come up with solutions,

1:54:26.920 --> 1:54:35.640
 and that forces you to reflect on the human problem solving process and the way your own mind

1:54:36.200 --> 1:54:46.760
 generates abstract representations of the problems it's exposed to. I think it's due to the fact that

1:54:46.760 --> 1:54:55.080
 the set of core knowledge priors that arc is built upon is so small. It's all a recombination of a

1:54:55.080 --> 1:55:04.440
 very, very small set of assumptions. Okay. So what's the future of arc? So you held arc as a

1:55:04.440 --> 1:55:12.200
 challenge as part of a Kegel competition. Yes. Kegel competition. And what do you think? Do

1:55:12.200 --> 1:55:17.720
 you think that's something that continues for five years, 10 years, just continues growing?

1:55:17.720 --> 1:55:22.600
 Yes, absolutely. So arc itself will keep evolving. So I've talked about crowd sourcing,

1:55:22.600 --> 1:55:30.200
 I think that's a good avenue. Another thing I'm starting is I'll be collaborating with

1:55:30.200 --> 1:55:37.480
 folks from the psychology department at NYU to do human testing on arc. And I think there are

1:55:37.480 --> 1:55:44.280
 lots of interesting questions you can start asking, especially as you start coordinating machine

1:55:44.280 --> 1:55:51.720
 solutions to arc tasks and the human characteristics of solutions. Like for instance, you can try to

1:55:51.720 --> 1:55:58.840
 see if there's a relationship between the human perceived difficulty of a task and the machine

1:55:58.840 --> 1:56:03.480
 perceived. Yes, and exactly some measure of machine perceived difficulty. Yeah, it's a nice

1:56:03.480 --> 1:56:07.320
 playground in which to explore this very difference. It's the same thing as we talked

1:56:07.320 --> 1:56:11.240
 about the autonomous vehicles. The things that could be difficult for humans might be very

1:56:11.240 --> 1:56:17.160
 different than the things that are absolutely and formalizing or making explicit that difference

1:56:17.160 --> 1:56:22.040
 and difficulty will teach us something may teach us something fundamental about intelligence.

1:56:22.040 --> 1:56:29.400
 So one thing I think we did well with arc is that it's proving to be a very

1:56:29.400 --> 1:56:37.720
 actionable test in the sense that machine performance on arc started at very much zero

1:56:37.720 --> 1:56:47.000
 initially, while humans found actually the task very easy. And that alone was like a big red

1:56:47.000 --> 1:56:52.920
 flashing light saying that something is going on and that we are missing something. And at the

1:56:52.920 --> 1:56:58.920
 same time, machine performance did not stay at zero for very long actually within two weeks

1:56:58.920 --> 1:57:04.600
 of the Kaggle competition, we started having a non zero number. And now the state of the art is

1:57:04.600 --> 1:57:14.920
 around 20% of the test set solved. And so arc is actually a challenge where our capabilities

1:57:14.920 --> 1:57:20.120
 start at zero, which indicates the need for progress. But it's also not an impossible

1:57:20.120 --> 1:57:25.560
 challenge. It's not accessible. You can start making progress basically right away. At the

1:57:25.560 --> 1:57:32.680
 same time, we are still very far from having solved it. And that's actually a very positive outcome

1:57:32.680 --> 1:57:40.280
 of the competition is that the competition has proven that there was no obvious shortcut to

1:57:40.280 --> 1:57:45.560
 solve these tasks. Right. Yeah, so the test held up. Yeah, exactly. That was the primary reason

1:57:45.560 --> 1:57:51.880
 to do the Kaggle competition is to check if some some, you know, clever person was was going to

1:57:51.880 --> 1:57:57.880
 hack the benchmark. And that did not happen, right? Like people who are solving the task are

1:57:57.880 --> 1:58:05.720
 essentially doing it. Well, in a way, they're actually exploring some flaws of arc that we

1:58:05.720 --> 1:58:11.480
 will need to address in the future, especially they're essentially anticipating what sort of tasks

1:58:11.480 --> 1:58:18.280
 may be contained in the test set, right? Right. Which is kind of, yeah, that's the kind of hacking.

1:58:18.280 --> 1:58:24.120
 It's human hacking of the test. Yes. That said, you know, with the state of the art, that's like

1:58:24.120 --> 1:58:31.720
 a 20% versus very, very far from human level, which is closer to 100 person. And so, and I do

1:58:31.720 --> 1:58:40.600
 believe that, you know, it will it will take a while until we reach a human parity on arc. And

1:58:40.600 --> 1:58:47.640
 that by the time we have human parity, we will have AI systems that are probably pretty close to

1:58:47.640 --> 1:58:53.560
 human level in terms of general fluid intelligence, which is, I mean, it's they're not going to be

1:58:53.560 --> 1:58:59.560
 necessarily human like, they're not necessarily, you would not necessarily recognize them as,

1:58:59.560 --> 1:59:07.800
 you know, being an AI. But they would be capable of a degree of generalization that matches the

1:59:07.800 --> 1:59:12.920
 generalization performed by human fluid intelligence. Sure. I mean, this is a good point

1:59:12.920 --> 1:59:18.760
 in terms of general fluid intelligence to mention in your paper, you describe different kinds of

1:59:18.760 --> 1:59:26.280
 generalizations, local, broad, extreme, and there's a kind of hierarchy that you form. So when we say

1:59:27.240 --> 1:59:36.040
 generalizations, what, what are we talking about? What kinds are there? Right. So generalization is

1:59:36.040 --> 1:59:40.840
 very old idea. I mean, it's even older than machine learning. In the context of machine learning,

1:59:40.840 --> 1:59:48.600
 you say a system generalizes if it can make sense of an input it has, it has not yet seen.

1:59:49.480 --> 1:59:55.560
 And that's what I would call a system centric generalization, you generalization

1:59:56.840 --> 2:00:04.360
 with respect to novelty for the specific system you're considering. So I think a good test of

2:00:04.360 --> 2:00:11.240
 intelligence should actually deal with developer aware generalization, which is slightly stronger

2:00:11.800 --> 2:00:15.880
 than system centric generalization. So developer generalization developer aware generalization

2:00:15.880 --> 2:00:23.800
 would be the ability to generalize to novelty or uncertainty that not only the system itself

2:00:23.800 --> 2:00:28.360
 has not access to, but the developer of the system could not have access to either.

2:00:28.360 --> 2:00:36.280
 Yeah. That's a fascinating, that's a fascinating meta definition. So like the system is, it's

2:00:36.280 --> 2:00:40.680
 basically the edge case thing we're talking about with autonomous vehicles, neither the developer

2:00:40.680 --> 2:00:47.720
 nor the system know about the edge cases. So it's up to the system should be able to generalize the

2:00:47.720 --> 2:00:56.440
 thing that, that nobody expected, neither the designer of the training data, nor obviously

2:00:56.440 --> 2:01:00.280
 the contents of the training data. That's a fascinating definition.

2:01:00.280 --> 2:01:03.560
 So you can see generalization degrees of generalization as a spectrum.

2:01:04.360 --> 2:01:10.920
 And the lowest level is what machine learning is trying to do is the assumption that

2:01:11.960 --> 2:01:18.200
 any new situation is going to be sampled from a static distribution of possible situations.

2:01:18.200 --> 2:01:23.720
 And that you already have a representative sample of the distribution that's your training data.

2:01:23.720 --> 2:01:28.120
 And so in machine learning, you generalize to a new sample from a known distribution.

2:01:28.680 --> 2:01:36.760
 And the ways in which your new sample will be new or different are ways that are already understood

2:01:36.760 --> 2:01:44.200
 by the developers of the system. So you are generalizing to known unknowns for one specific task.

2:01:45.000 --> 2:01:49.160
 That's what you would call robustness. You are robust to things like noise,

2:01:49.160 --> 2:01:58.440
 small variations and so on. For one fixed known distribution that you know through your training

2:01:58.440 --> 2:02:08.040
 data. And a higher degree would be flexibility in machine intelligence. So flexibility would be

2:02:08.040 --> 2:02:15.080
 something like an L5 cell driving car, or maybe a robot that can pass the

2:02:15.080 --> 2:02:20.600
 the coffee cup test, which is the notion that you would be given a random kitchen

2:02:21.400 --> 2:02:26.440
 somewhere in the country and you would have to go make a cup of coffee in that kitchen.

2:02:28.360 --> 2:02:34.280
 So flexibility would be the ability to deal with unknown unknowns. So things that could not,

2:02:35.240 --> 2:02:41.000
 dimensions of variability that could not have been possibly foreseen by the creators of the system

2:02:41.000 --> 2:02:46.920
 within one specific task. So generalizing to the long tail of situations in cell driving,

2:02:46.920 --> 2:02:51.560
 for instance, would be flexibility. So you have robustness, flexibility. And finally,

2:02:51.560 --> 2:02:58.040
 we'd have extreme generalization, which is basically flexibility, but instead of just

2:02:58.040 --> 2:03:04.360
 considering one specific domain like driving or domestic robotics, you're considering an

2:03:04.360 --> 2:03:12.680
 open ended range of possible domains. So a robot would be capable of extreme generalization if

2:03:13.400 --> 2:03:22.360
 let's say it's designed and trained for cooking, for instance. And if I buy the robots,

2:03:22.360 --> 2:03:29.960
 and if I'm able, if it's able to teach itself gardening in a couple weeks, it would be capable

2:03:29.960 --> 2:03:34.760
 of extreme generalization, for instance. So the ultimate goal is extreme generalization. Yes.

2:03:34.760 --> 2:03:43.800
 So creating a system that is so general that it could essentially achieve human skill parity over

2:03:44.920 --> 2:03:52.120
 arbitrary tasks and arbitrary domains with the same level of improvisation and adaptation power

2:03:52.120 --> 2:03:58.920
 as humans when it encounters new situations. And it would do so over basically the same range

2:03:58.920 --> 2:04:04.840
 of possible domains and tasks as humans, and using essentially the same amount of training

2:04:04.840 --> 2:04:08.760
 experience of practice as humans would require. That would be human level

2:04:08.760 --> 2:04:14.920
 of extreme generalization. So I don't actually think humans are anywhere near the

2:04:16.760 --> 2:04:22.600
 optimal intelligence bound if there is such a thing. So I think for humans or in general?

2:04:22.600 --> 2:04:31.800
 In general. I think it's quite likely that there is a hard limit to how intelligent

2:04:32.760 --> 2:04:36.840
 any system can be. But at the same time, I don't think humans are anywhere near that limit.

2:04:39.080 --> 2:04:44.600
 Yeah, last time I think we talked, I think you had this idea that we're only as intelligent as

2:04:44.600 --> 2:04:53.960
 the problems we face. We are bounded by the problem. In a way, yes. We are bounded by our

2:04:53.960 --> 2:05:01.000
 environments and we are bounded by the problems we try to solve. Yeah. What do you make of Neuralink

2:05:01.000 --> 2:05:08.280
 and outsourcing some of the brain power, like brain computer interfaces? Do you think we can expand

2:05:08.280 --> 2:05:18.600
 our, augment our intelligence? I am fairly skeptical of Neuralink interfaces because

2:05:19.240 --> 2:05:28.040
 they're trying to fix one specific bottleneck in human mission cognition, which is the bandwidth

2:05:28.040 --> 2:05:35.640
 bottleneck input and output of information in the brain. And my perception of the problem is that

2:05:35.640 --> 2:05:43.000
 bandwidth is not, at this time, a bottleneck at all, meaning that we already have senses that

2:05:43.000 --> 2:05:50.760
 enable us to take in far more information than what we can actually process. Well, to push back

2:05:50.760 --> 2:05:55.880
 on that a little bit, to sort of play devil's advocate a little bit, is if you look at the

2:05:55.880 --> 2:06:01.960
 internet, the Wikipedia, let's say Wikipedia, I would say that humans, after the advent of Wikipedia,

2:06:01.960 --> 2:06:10.520
 are much more intelligent. Yes. I think that's a good one. But that's also not about, that's about

2:06:12.040 --> 2:06:18.840
 externalizing our intelligence via information processing systems, external information

2:06:18.840 --> 2:06:24.920
 processing systems, which is very different from brain computer interfaces. Right. But the question

2:06:24.920 --> 2:06:32.200
 is whether if we have direct access, if our brain has direct access to Wikipedia, would our brain

2:06:32.200 --> 2:06:38.360
 already has direct access to Wikipedia, it's on your phone, and you have your hands and your eyes

2:06:38.360 --> 2:06:43.720
 and your ears and so on to access that information and the speed at which you can access it.

2:06:44.280 --> 2:06:49.800
 Is bottlenecked by the cognition? I think it's already close, fairly close to optimal, which is

2:06:49.800 --> 2:06:55.880
 why speed reading, for instance, does not work. The faster you read, the less you understand.

2:06:55.880 --> 2:07:01.800
 But maybe it's because it uses the eyes. So maybe, so I don't believe so. I think, you know,

2:07:01.800 --> 2:07:08.200
 the brain is very slow. It's speaking operates, you know, the fastest things that happen in the

2:07:08.200 --> 2:07:15.320
 brain at the level of 50 milliseconds, forming a conscious start can potentially take entire

2:07:15.320 --> 2:07:20.440
 seconds. Right. And you can already read pretty fast. So I think the speed at which you can

2:07:21.800 --> 2:07:27.800
 take information in and even the speed at which you can output information can only be very

2:07:28.520 --> 2:07:34.280
 incrementally improved. Maybe if you're very fast typer, if you're a very trained typer,

2:07:34.280 --> 2:07:39.160
 the speed at which you can express your thoughts is already a speed at which you can form your

2:07:39.160 --> 2:07:46.680
 thoughts. Right. So that's kind of an idea that there are fundamental bottlenecks to the human

2:07:46.680 --> 2:07:53.080
 mind. But it's possible that everything we have in the human mind is just to be able to survive

2:07:53.080 --> 2:08:01.720
 in the environment. And there's a lot more to expand. Maybe, you know, you said the speed of the

2:08:01.720 --> 2:08:09.000
 thought. So yeah, I think augmenting human intelligence is a very valid and very powerful

2:08:09.000 --> 2:08:14.040
 avenue. Right. And that's what computers are about. In fact, that's what, you know, all of

2:08:14.040 --> 2:08:21.400
 culture and civilization is about. They are culture is externalized cognition. And we rely

2:08:21.400 --> 2:08:27.160
 on culture to think constantly. Yeah. Yeah. I mean, that's another yeah, that's not just not

2:08:27.160 --> 2:08:32.360
 just computers, not just forms on the internet. I mean, all of culture, like language, for instance,

2:08:32.360 --> 2:08:36.600
 is a form of externalized cognition. Books are obviously externalized cognition.

2:08:37.400 --> 2:08:42.680
 Yeah, that's great. And you can scale that externalized cognition, you know, far beyond

2:08:42.680 --> 2:08:49.160
 the capability of the human brain. And you could see, you know, civilization itself is

2:08:50.760 --> 2:08:55.400
 it has capabilities that are far beyond any individual brain and we keep scaling it because

2:08:55.400 --> 2:09:02.600
 it's not rebound by individual brains. It's a different kind of system. Yeah. And and that

2:09:02.600 --> 2:09:08.520
 system includes non human, non humans. First of all, includes all the other biological systems,

2:09:08.520 --> 2:09:12.280
 which are probably contributing to the overall intelligence of the organism.

2:09:12.840 --> 2:09:17.400
 And then computers are part of it on non human systems, probably not contributing much, but

2:09:17.400 --> 2:09:26.280
 AI is definitely contributing to that. Like Google search, for instance, part of it. Yeah. Yeah.

2:09:27.160 --> 2:09:32.280
 A huge part, a part that we can't probably introspect. Like how the world has changed in

2:09:32.280 --> 2:09:38.440
 the past 20 years. It's probably very difficult for us to be able to understand until, of course,

2:09:38.440 --> 2:09:43.720
 whoever created the simulation wherein is probably do metrics measuring the progress.

2:09:43.720 --> 2:09:50.120
 Yes. There was probably a big spike in performance. They're enjoying, they're enjoying this.

2:09:51.560 --> 2:09:57.880
 So what are your thoughts on the Turing test and the Lobner Prize, which is the,

2:09:59.240 --> 2:10:04.760
 you know, one of the most famous attempts at the test of human intelligence, sorry,

2:10:04.760 --> 2:10:12.920
 of artificial intelligence by doing a natural language open dialogue test that's test that's

2:10:14.360 --> 2:10:17.480
 judged by humans as far as how well the machine did.

2:10:18.760 --> 2:10:24.600
 So I'm not a fan of the Turing test itself or any of its variants for two reasons.

2:10:24.600 --> 2:10:38.120
 So first of all, it's really coping out of trying to define and measure intelligence because it's

2:10:38.120 --> 2:10:46.680
 entirely outsourcing that to a panel of human judges. And these human judges, they may not

2:10:46.680 --> 2:10:52.760
 themselves have any proper methodology. They may not themselves have any proper definition of

2:10:52.760 --> 2:10:57.960
 intelligence. They may not be reliable. So the Turing test already failing one of the core

2:10:57.960 --> 2:11:05.000
 psychometrics principles, which is reliability because you have biased human judges. It's also

2:11:05.000 --> 2:11:10.840
 violating the standardization requirement and the freedom from bias requirement. And so it's

2:11:10.840 --> 2:11:16.120
 really a coop out because you are outsourcing everything that matters, which is precisely

2:11:16.120 --> 2:11:23.320
 describing intelligence and finding a standard on test to measure it. You are sourcing everything

2:11:23.320 --> 2:11:31.080
 to people. So it's really a coop out. And by the way, we should keep in mind that when Turing

2:11:31.080 --> 2:11:39.640
 proposed the imitation game, it was not meaning for the imitation game to be an actual goal for

2:11:39.640 --> 2:11:47.560
 the field of AI and actual tests of intelligence. It was using the imitation game as a thought

2:11:47.560 --> 2:11:58.520
 experiment in a philosophical discussion in his 1950 paper. He was trying to argue that theoretically,

2:11:58.520 --> 2:12:05.480
 it should be possible for something very much like the human mind indistinguishable from the

2:12:05.480 --> 2:12:14.440
 human mind to be encoded in a Turing machine. And at the time, that was a very daring idea.

2:12:14.440 --> 2:12:21.320
 It was stretching credulity. But nowadays, I think it's fairly well accepted that the mind is an

2:12:21.320 --> 2:12:26.280
 information processing system and that you could probably encode it into a computer. So another

2:12:26.280 --> 2:12:35.400
 reason why I'm not a fan of this type of test is that the incentives that it creates are incentives

2:12:35.400 --> 2:12:45.160
 that are not conducive to proper scientific research. If your goal is to convince a panel of

2:12:45.160 --> 2:12:54.040
 human judges that they're talking to a human, then you have an incentive to rely on tricks and

2:12:54.040 --> 2:13:00.040
 prestidigitation in the same way that, let's say, you're doing physics and you want to solve

2:13:00.040 --> 2:13:06.760
 teleportation. And what if the test that you set out to pass is you need to convince a panel of

2:13:06.760 --> 2:13:11.960
 judges that teleportation took place and they're just sitting there and watching what you're doing.

2:13:12.520 --> 2:13:20.200
 And that is something that you can achieve with David Copperfield could achieve it in his show

2:13:20.200 --> 2:13:29.320
 at Vegas. And what he's doing is very elaborate. But it's not actually, it's not physics. It's

2:13:29.320 --> 2:13:34.680
 not making any progress in our understanding of the universe. To push back on that as possible,

2:13:34.680 --> 2:13:41.000
 that's the hope with these kinds of subjective evaluations is that it's easier to solve it

2:13:41.000 --> 2:13:47.240
 generally than it is to come up with tricks that convince a large number of judges. That's the hope.

2:13:47.240 --> 2:13:52.680
 In practice, it turns out that it's very easy to deceive people in the same way that you can do

2:13:52.680 --> 2:13:58.440
 magic in Vegas. You can actually very easily convince people that they're talking to a human

2:13:58.440 --> 2:14:03.400
 when they're actually talking to an algorithm. I disagree with that. I think it's easy.

2:14:05.160 --> 2:14:10.680
 It's not easy. It's doable. It's very easy because I wouldn't say it's very easy though.

2:14:10.680 --> 2:14:17.720
 We are biased. We have theory of mind. We are constantly projecting emotions, intentions,

2:14:19.960 --> 2:14:26.200
 agentness. Agentness is one of our core innate priors. We are projecting these things on everything

2:14:26.200 --> 2:14:33.960
 around us. If you paint a smiley on a rock, the rock becomes happy in our eyes. Because

2:14:33.960 --> 2:14:40.920
 we have this extreme bias that permits everything we see around us, it's actually pretty easy to

2:14:40.920 --> 2:14:49.320
 trick people. I disagree with that. I totally disagree with that. You brilliantly put the

2:14:49.320 --> 2:14:53.880
 anthropomorphization that we naturally do, the agentness of that word. Is that a real word?

2:14:53.880 --> 2:14:57.640
 No, it's not a real word. I like it. But it's a good word. It's a useful word.

2:14:57.640 --> 2:15:02.520
 It's a useful word. Let's make it real. It's a huge help. But I still think it's really difficult

2:15:02.520 --> 2:15:09.880
 to convince. If you do like the Alexa Prize formulation where you talk for an hour,

2:15:09.880 --> 2:15:13.640
 like there's formulations of the test you can create where it's very difficult.

2:15:13.640 --> 2:15:18.920
 So I like the Alexa Prize better because it's more pragmatic. It's more practical.

2:15:18.920 --> 2:15:27.160
 It's actually incentivizing developers to create something that's useful as a human

2:15:27.160 --> 2:15:31.560
 mission interface. So that's slightly better than just the imitation.

2:15:31.560 --> 2:15:38.920
 So your idea is like a test which hopefully will help us in creating intelligent systems

2:15:38.920 --> 2:15:43.240
 as a result. If you create a system that passes it, it'll be useful for creating

2:15:43.240 --> 2:15:50.920
 further intelligent systems. Yes, at least. I'm a little bit surprised

2:15:51.720 --> 2:15:58.280
 how little inspiration people draw from the Turing test today. The media and the popular

2:15:58.280 --> 2:16:03.400
 press might write about it every once in a while. The philosophers might talk about it.

2:16:03.400 --> 2:16:10.680
 But most engineers are not really inspired by it. I know you don't like the Turing test,

2:16:10.680 --> 2:16:16.920
 but we'll have this argument another time. There's something inspiring it about it,

2:16:16.920 --> 2:16:21.640
 I think. As a philosophical device in a philosophical discussion,

2:16:21.640 --> 2:16:26.040
 I think there is something very interesting about it. I don't think it is in practical terms.

2:16:26.040 --> 2:16:32.680
 I don't think it's conducive to progress. And one of the reasons why is that I think

2:16:33.320 --> 2:16:38.520
 being very human like being undistinguishable from a human is actually the very last step

2:16:38.520 --> 2:16:43.720
 in the creation of machine intelligence. That the first AI is that will show strong

2:16:44.520 --> 2:16:53.000
 generalization that will actually implement human like broad cognitive abilities.

2:16:53.000 --> 2:16:56.920
 They will not actually be able to look anything like humans.

2:16:58.360 --> 2:17:03.720
 Human likeness is the very last step in that process. And so a good test is a test that

2:17:03.720 --> 2:17:08.760
 points you towards the first step on the ladder, not towards the top of the ladder, right?

2:17:08.760 --> 2:17:14.040
 So to push back on that, I usually agree with you on most things. I remember you,

2:17:14.040 --> 2:17:19.000
 I think, at some point tweeting something about the Turing test not being counterproductive or

2:17:19.000 --> 2:17:29.080
 something like that. And I think a lot of very smart people agree with that. A computation

2:17:29.080 --> 2:17:33.880
 speaking not very smart person disagree with that because I think there's some magic to the

2:17:33.880 --> 2:17:39.000
 interactivity interactivity with other humans. So to push to play devil's advocate on your

2:17:39.000 --> 2:17:45.400
 statement, it's possible that in order to demonstrate the generalization abilities of a system,

2:17:45.400 --> 2:17:54.680
 you have to show your in conversation show your ability to adjust, adapt to the conversation

2:17:54.680 --> 2:18:01.160
 through not just like as a standalone system, but through the process of like the interaction

2:18:01.160 --> 2:18:09.560
 that game theoretic, where you're you really are changing the environment by your actions. So

2:18:09.560 --> 2:18:15.400
 in the art challenge, for example, you're an observer, you can't you can't scare the test

2:18:15.400 --> 2:18:22.360
 into into changing, you can't talk to the test, you can't play with it. So there's some aspect

2:18:22.360 --> 2:18:28.200
 of that interactivity that becomes highly subjective, but it feels like it could be conducive

2:18:28.200 --> 2:18:33.960
 to yeah, I think you make a great point. The interactivity is very good setting to force

2:18:33.960 --> 2:18:42.680
 a system to show adaptation to show generalization. That said, you're at the same time. It's not

2:18:42.680 --> 2:18:47.720
 something very scalable because you rely on human judges. It's not something reliable because the

2:18:47.720 --> 2:18:53.880
 human judges may not you don't like human judges. Basically, yes. And I think so. I love the idea

2:18:53.880 --> 2:19:01.640
 of interactivity. I initially wanted an artist that had some amount of interactivity where your

2:19:01.640 --> 2:19:07.640
 score on a task would not be one or zero if you can solve it or not, but would be the number

2:19:09.240 --> 2:19:16.280
 of attempts that you can make before you hit the right solution, which means that now you can start

2:19:16.280 --> 2:19:22.280
 applying the scientific method as you sort of arc tasks that you can start formulating hypothesis

2:19:22.280 --> 2:19:27.960
 and probing the system to see whether the idea of this is the observation will match the hypothesis

2:19:27.960 --> 2:19:33.960
 or not. It would be amazing if you could also even higher level than that, measure the quality of

2:19:33.960 --> 2:19:39.080
 your attempts, which of course is impossible. But again, that gets subjective. How good was

2:19:39.080 --> 2:19:47.880
 your thinking? How efficient was? So one thing that's interesting about this notion of scoring

2:19:47.880 --> 2:19:53.080
 you as how many attempts you need is that you can start producing tasks that are way more ambiguous,

2:19:53.640 --> 2:20:01.240
 right? Because with the different attempts, you can actually probe that

2:20:01.240 --> 2:20:11.320
 ambiguity, right? Right. So that's in a sense, which is how good can you adapt to the uncertainty

2:20:12.200 --> 2:20:21.080
 and reduce the uncertainty? Yes. It's half fast. Is the efficiency with which to reduce uncertainty

2:20:21.080 --> 2:20:24.840
 in program space? Exactly. Very difficult to come up with that kind of test though.

2:20:24.840 --> 2:20:30.520
 Yeah. So I would love to be able to create something like this. In practice, it would be very,

2:20:30.520 --> 2:20:36.840
 very difficult. But yes. What you're doing, what you've done with the ARC challenge is brilliant.

2:20:37.400 --> 2:20:42.040
 I'm also not, I'm surprised that it's not more popular, but I think it's picking up like that.

2:20:42.040 --> 2:20:47.320
 It does its niche. It does its niche. Yeah. What are your thoughts about another test that I talked

2:20:47.320 --> 2:20:52.120
 with Marcus Hutter? He has the harder prize for compression of human knowledge and the idea is

2:20:52.920 --> 2:20:58.280
 really sort of quantify and reduce the test of intelligence purely to just the ability to

2:20:58.280 --> 2:21:05.720
 compress. What's your thoughts about this intelligence as compression? I mean, it's a very

2:21:06.440 --> 2:21:13.720
 fun test because it's such a simple idea. Like you're given Wikipedia, basically English Wikipedia,

2:21:13.720 --> 2:21:21.080
 and you must compress it. And so it stems from the idea that cognition is compression,

2:21:21.080 --> 2:21:27.480
 that the brain is basically a compression algorithm. This is a very old idea. It's a very, I think,

2:21:27.480 --> 2:21:34.920
 striking and beautiful idea. I used to believe it. I eventually had to realize that it was,

2:21:34.920 --> 2:21:40.920
 it was very much a flawed idea. So I no longer believe that compression is cognition is compression.

2:21:40.920 --> 2:21:47.960
 So, but I can tell you what's the difference. So it's very easy to believe that cognition

2:21:47.960 --> 2:21:53.480
 and compression are the same thing because, so Jeff Hawkins, for instance, says that

2:21:53.480 --> 2:21:58.600
 cognition is prediction. And of course, prediction is basically the same thing as compression, right?

2:21:58.600 --> 2:22:05.960
 It's just including the temporal axis. And it's very easy to believe this because compression

2:22:05.960 --> 2:22:12.200
 is something that we do all the time very naturally. We are constantly compressing information. We are

2:22:14.280 --> 2:22:20.040
 constantly trying, we have this bias towards simplicity. We're constantly trying to organize

2:22:20.040 --> 2:22:26.920
 things in our mind and around us to be more regular, right? So it's a beautiful idea. It's

2:22:26.920 --> 2:22:33.880
 very easy to believe. There is a big difference between what we do with our brains and compression.

2:22:33.880 --> 2:22:41.400
 So compression is actually kind of a tool in the human cognitive tool kit that is used in many

2:22:41.400 --> 2:22:47.640
 ways. But it's just a tool. It is not, it is a tool for cognition. It is not cognition itself.

2:22:47.640 --> 2:22:55.560
 And the big fundamental difference is that cognition is about being able to operate in

2:22:55.560 --> 2:23:02.920
 future situations that include fundamental uncertainty and novelty. So for instance,

2:23:03.720 --> 2:23:10.840
 consider a child at age 10. And so they have 10 years of life experience. They've gotten,

2:23:10.840 --> 2:23:18.120
 you know, pain, pleasure, rewards, and punishment at a period of time. If you were to generate

2:23:18.760 --> 2:23:26.680
 the shortest behavioral program that would have basically run that child over these 10 years

2:23:26.680 --> 2:23:33.640
 in an optimal way, right? The shortest optimal behavioral program given the experience of that

2:23:33.640 --> 2:23:39.400
 child so far. Well, that program, that compressed program, this is what you would get if the mind

2:23:39.400 --> 2:23:48.920
 of the child was a compression algorithm essentially, would be utterly unable, inappropriate to process

2:23:48.920 --> 2:23:59.400
 the next 70 years in the life of that child. So in the models we build of the world, we are not

2:23:59.400 --> 2:24:07.160
 trying to make them actually optimally compressed. We are using compression as a tool to promote

2:24:07.160 --> 2:24:13.080
 simplicity and efficiency in our models. But they are not perfectly compressed because they need to

2:24:13.080 --> 2:24:20.600
 include things that are seemingly useless today, that have seemingly been useless so far. But that

2:24:20.600 --> 2:24:26.680
 may turn out to be useful in the future because you just don't know the future. And that's the

2:24:26.680 --> 2:24:32.920
 fundamental principle that cognition, that intelligence arises from is that you need to be

2:24:32.920 --> 2:24:38.440
 able to run appropriate behavioral programs, except you have absolutely no idea what sort of

2:24:38.440 --> 2:24:43.640
 context, environment, and situation they're going to be running in. And you have to deal with that,

2:24:43.640 --> 2:24:52.440
 with that uncertainty, with that future novelty. So an analogy that you can make is with investing,

2:24:52.440 --> 2:25:01.000
 for instance. If I look at the past 20 years of stock market data, and I use a compression

2:25:01.000 --> 2:25:06.680
 algorithm to figure out the best trading strategy, it's going to be you buy Apple stock, then maybe

2:25:06.680 --> 2:25:13.560
 the past few years you buy Tesla stock or something. But is that strategy still going to be true for

2:25:13.560 --> 2:25:21.320
 the next 20 years? Well, actually, probably not. Which is why if you're a smart investor, you're

2:25:21.320 --> 2:25:28.120
 not just going to be following the strategy that corresponds to compression of the past.

2:25:28.120 --> 2:25:35.320
 You're going to be following, you're going to have a balanced spot for you, right? Because you

2:25:35.320 --> 2:25:41.240
 just don't know what's going to happen. I mean, I guess in that same sense, the compression is

2:25:41.240 --> 2:25:46.840
 analogous to what you talked about, which is like local or robust generalization versus extreme

2:25:46.840 --> 2:25:53.320
 generalization. It's much closer to that side of being able to generalize in the local sense.

2:25:53.320 --> 2:26:01.480
 That's why as humans, when we are children, in our education, so a lot of it is driven by place,

2:26:01.480 --> 2:26:09.080
 driven by curiosity, we are not efficiently compressing things. We're actually exploring.

2:26:09.640 --> 2:26:19.560
 We are retaining all kinds of things from our environment that seem to be completely useless,

2:26:19.560 --> 2:26:26.760
 because they might turn out to be eventually useful. That's what cognition is really about,

2:26:26.760 --> 2:26:33.320
 and what makes it antagonistic to compression is that it is about hedging for future uncertainty.

2:26:38.360 --> 2:26:43.720
 Cognition leverages compression as a tool to promote efficiency.

2:26:43.720 --> 2:26:50.760
 So in that sense, in our models. It's like Einstein said, make it simpler,

2:26:50.760 --> 2:26:56.760
 but not however that quote goes, but not too simple. So you want to compression,

2:26:56.760 --> 2:26:58.840
 simplifies things, but you don't want to make it too simple.

2:27:00.040 --> 2:27:05.400
 Yes. So a good model of the world is going to include all kinds of things that are completely

2:27:05.400 --> 2:27:11.000
 useless, actually, just in case. Because you need diversity in the same way that in your portfolio,

2:27:11.000 --> 2:27:14.840
 you need all kinds of stocks that may not have performed well so far, but you need

2:27:14.840 --> 2:27:18.680
 diversity. And the reason you need diversity is because, fundamentally, you don't know what

2:27:18.680 --> 2:27:25.400
 you're doing. And the same is true of the human mind, is that it needs to behave appropriately

2:27:26.040 --> 2:27:30.520
 in the future. And it has no idea what the future is going to be like. But it's not going to be

2:27:30.520 --> 2:27:35.160
 like the past. So compressing the past is not appropriate, because the past is not

2:27:35.160 --> 2:27:43.000
 predictive of the future. Yeah. History repeats itself, but not perfectly.

2:27:44.600 --> 2:27:49.720
 I don't think I asked you last time the most inappropriately absurd question.

2:27:51.080 --> 2:27:58.920
 We've talked a lot about intelligence, but the bigger question from intelligence is of meaning.

2:27:58.920 --> 2:28:03.400
 You know, intelligence systems are kind of goal oriented. They're always optimizing for goal.

2:28:03.400 --> 2:28:08.840
 You look at the hotter prize, actually. I mean, there's always a clean formulation of a goal.

2:28:08.840 --> 2:28:15.000
 But the natural questions for us humans, since we don't know our objective function, is what is

2:28:15.000 --> 2:28:22.840
 the meaning of it all? So the absurd question is, what Francois Chalet do you think is the meaning

2:28:22.840 --> 2:28:27.080
 of life? What's the meaning of life? Yeah, that's a big question.

2:28:27.080 --> 2:28:37.080
 And I think I can give you my answer, at least one of my answers.

2:28:37.960 --> 2:28:46.440
 And so you know, the one thing that's very important in understanding who we are is that

2:28:47.720 --> 2:28:52.440
 everything that makes up ourselves, that makes up who we are,

2:28:52.440 --> 2:29:00.920
 even your most personal thoughts is not actually your own. Even your most personal thoughts

2:29:01.640 --> 2:29:09.080
 are expressed in words that you did not invent and are built on concepts and images that you did

2:29:09.080 --> 2:29:17.720
 not invent. We are very much cultural beings. We are made of culture. What makes us different

2:29:17.720 --> 2:29:26.440
 from animals, for instance. So everything about ourselves is an echo of the past, an echo of

2:29:26.440 --> 2:29:35.960
 people who lived before us. That's who we are. And in the same way, if we manage to contribute

2:29:35.960 --> 2:29:44.360
 something to the collective edifice of culture, a new idea, maybe a beautiful piece of music,

2:29:44.360 --> 2:29:54.840
 a work of art, a grand theory, and new words, maybe, that something is going to become a part

2:29:55.800 --> 2:30:03.800
 of the minds of future humans, essentially forever. So everything we do creates repulse

2:30:03.800 --> 2:30:11.800
 that propagates into the future. And that's in a way, this is our path to immortality,

2:30:11.800 --> 2:30:21.560
 is that as we contribute things to culture, culture in turn becomes future humans. And

2:30:22.280 --> 2:30:30.680
 we keep influencing people thousands of years from now. So our actions today create repulse.

2:30:30.680 --> 2:30:38.680
 And these repulse, I think, basically sum up the meaning of life. Like in the same way that we are,

2:30:38.680 --> 2:30:46.520
 the sum of the interactions between many different repulse that came from our past,

2:30:47.080 --> 2:30:51.320
 we are ourselves creating repulse that will propagate into the future. And that's why

2:30:52.360 --> 2:30:59.240
 we should be, this seems like perhaps an eighth thing to say, but we should be kind to others

2:30:59.240 --> 2:31:06.680
 during our time on Earth, because every act of kindness creates repulse. And in reverse,

2:31:06.680 --> 2:31:13.160
 every act of violence also creates repulse. And you want to carefully choose which kind of repulse

2:31:13.160 --> 2:31:18.280
 you want to create, and you want to propagate into the future. And in your case, first of all,

2:31:18.280 --> 2:31:26.600
 beautifully put, but in your case, creating repulse into the future human and future AGI systems.

2:31:27.800 --> 2:31:30.280
 Yes. It's fascinating. Our successors.

2:31:30.280 --> 2:31:37.640
 I don't think there's a better way to end it. Francois has always, for a second time, and I'm

2:31:37.640 --> 2:31:43.240
 sure many times in the future, it's been a huge honor. You're one of the most brilliant people

2:31:43.240 --> 2:31:49.080
 in the machine learning computer science, science world. Again, it's a huge honor. Thanks for talking

2:31:49.080 --> 2:31:54.520
 to me. It's been a pleasure. Thanks a lot for having me. We appreciate it. Thanks for listening

2:31:54.520 --> 2:32:00.280
 to this conversation with Francois Chollet. And thank you to our sponsors, Babel, Masterclass,

2:32:00.280 --> 2:32:05.400
 and Cash App. Click the sponsor links in the description to get a discount and to support

2:32:05.400 --> 2:32:10.200
 this podcast. If you enjoy this thing, subscribe on YouTube, review it with five stars on our

2:32:10.200 --> 2:32:16.680
 podcast, follow on Spotify, support on Patreon, or connect with me on Twitter at Lex Freedman.

2:32:17.640 --> 2:32:23.720
 And now let me leave you with some words from Renee Descartes in 1668, an excerpt of which

2:32:23.720 --> 2:32:28.680
 Francois includes in his On the Measure of Intelligence paper. If there were machines

2:32:28.680 --> 2:32:34.280
 which bore a resemblance to our bodies and imitated our actions as closely as possible

2:32:34.280 --> 2:32:39.800
 for all practical purposes, we should still have two very certain means of recognizing

2:32:39.800 --> 2:32:45.240
 that they were not real men. The first is that they could never use words or put together

2:32:45.240 --> 2:32:51.480
 signs as we do in order to declare our thoughts to others. For we can certainly conceive of a

2:32:51.480 --> 2:32:57.000
 machine so constructed that it utters words and even utters words that correspond to bodily

2:32:57.000 --> 2:33:02.760
 actions causing a change in its organs. But it is not conceivable that such a machine should

2:33:02.760 --> 2:33:07.880
 produce different arrangements of words so as to give it an appropriately meaningful answer

2:33:07.880 --> 2:33:13.960
 to whatever is said in its presence as the dullest of men can do. Here Descartes is anticipating

2:33:13.960 --> 2:33:20.840
 the touring test, and the argument still continues to this day. Secondly, he continues,

2:33:20.840 --> 2:33:26.360
 even though some machines might do some things as well as we do them, or perhaps even better,

2:33:26.360 --> 2:33:31.160
 they would inevitably fail in others, which would reveal that they are acting not from

2:33:31.160 --> 2:33:37.880
 understanding but only from the disposition of their organs. This is an incredible quote.

2:33:38.600 --> 2:33:46.440
 For whereas reason is a universal instrument which can be used in all kinds of situations,

2:33:46.440 --> 2:33:51.960
 these organs need some particular action. Hence it is for all practical purposes impossible

2:33:51.960 --> 2:33:57.720
 for a machine to have enough different organs to make it act in all the contingencies of life

2:33:57.720 --> 2:34:04.920
 in the way in which our reason makes us act. That's the debate between mimicry memorization

2:34:04.920 --> 2:34:20.200
 versus understanding. So, thank you for listening and hope to see you next time.

