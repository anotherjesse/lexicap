WEBVTT

00:00.000 --> 00:03.360
 The following is a conversation with Christos Goodrow,

00:03.360 --> 00:08.320
 vice president of engineering at Google and head of search and discovery at YouTube,

00:08.320 --> 00:15.120
 also known as the YouTube algorithm. YouTube has approximately 1.9 billion users,

00:15.120 --> 00:21.680
 and every day people watch over 1 billion hours of YouTube video. It is the second most popular

00:21.680 --> 00:27.200
 search engine behind Google itself. For many people, it is not only a source of entertainment,

00:27.200 --> 00:33.120
 but also how we learn new ideas from math and physics videos to podcasts to debates,

00:33.120 --> 00:38.640
 opinions, ideas from out of the box thinkers and activists on some of the most tense,

00:38.640 --> 00:44.880
 challenging and impactful topics in the world today. YouTube and other content platforms

00:44.880 --> 00:51.200
 receive criticism from both viewers and creators, as they should. Because the engineering task

00:51.200 --> 00:56.720
 before them is hard, and they don't always succeed, and the impact of their work is truly

00:56.720 --> 01:03.040
 world changing. To me, YouTube has been an incredible wellspring of knowledge. I've watched

01:03.040 --> 01:08.960
 hundreds if not thousands of lectures that change the way I see many fundamentals ideas in math,

01:08.960 --> 01:15.360
 science, engineering, and philosophy. But it does put a mirror to ourselves, and keeps the

01:15.360 --> 01:21.200
 responsibility of the steps we take in each of our online educational journeys into the hands of

01:21.200 --> 01:27.600
 each of us. The YouTube algorithm has an important role in that journey of helping us find new exciting

01:27.600 --> 01:32.960
 ideas to learn about. That's a difficult and an exciting problem for an artificial intelligence

01:32.960 --> 01:38.640
 system. As I've said in lectures and other forums, recommendation systems will be one of the most

01:38.640 --> 01:45.120
 impactful areas of AI in the 21st century, and YouTube is one of the biggest recommendation

01:45.120 --> 01:51.760
 systems in the world. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe

01:51.760 --> 01:57.440
 on YouTube, give it five stars on Apple Podcasts, follow on Spotify, support it on Patreon, or

01:57.440 --> 02:04.480
 simply connect with me on Twitter. Alex Friedman spelled F R I D M A N. I recently started doing

02:04.480 --> 02:09.520
 ads at the end of the introduction. I'll do one or two minutes after introducing the episode and

02:09.520 --> 02:14.560
 never any ads in the middle that can break the flow of the conversation. I hope that works for you

02:14.560 --> 02:20.640
 and doesn't hurt the listening experience. This show is presented by Cash App, the number one

02:20.640 --> 02:25.600
 finance app in the App Store. I personally use Cash App to send money to friends, but you can

02:25.600 --> 02:31.680
 also use it to buy, sell, and deposit Bitcoin in just seconds. Cash App also has a new investing

02:31.680 --> 02:36.800
 feature. You can buy fractions of a stock, say $1 worth, no matter what the stock price is.

02:37.360 --> 02:42.880
 Broker services are provided by Cash App Investing, a subsidiary of Square and member SIPC.

02:42.880 --> 02:48.640
 I'm excited to be working with Cash App to support one of my favorite organizations called First,

02:48.640 --> 02:54.080
 best known for their first robotics and Lego competitions. They educate and inspire hundreds

02:54.080 --> 02:59.920
 of thousands of students in over 110 countries and have a perfect rating and charity navigator,

02:59.920 --> 03:05.360
 which means that donated money is used to maximum effectiveness. When you get Cash App from the

03:05.360 --> 03:12.400
 App Store, Google Play, and use code LEX Podcast, you'll get $10 and Cash App will also donate to

03:12.400 --> 03:18.800
 $10 to First, which again is an organization that I've personally seen inspire girls and boys

03:18.800 --> 03:25.440
 to dream of engineering a better world. And now here's my conversation with Christos Godro.

03:26.720 --> 03:30.480
 YouTube is the world's second most popular search engine behind Google, of course.

03:31.280 --> 03:36.800
 We watch more than 1 billion hours of YouTube videos a day, more than Netflix and Facebook

03:36.800 --> 03:44.720
 video combined. YouTube creators upload over 500,000 hours of video every day. Average lifespan

03:44.720 --> 03:52.640
 of a human being just for comparison is about 700,000 hours. So what's uploaded every single date

03:53.280 --> 03:58.560
 is just enough for a human to watch in a lifetime. So let me ask an absurd philosophical question.

03:59.440 --> 04:03.680
 If from birth, when I was born, and there's many people born today with the internet,

04:03.680 --> 04:09.760
 I watched YouTube videos nonstop. Do you think there are trajectories through

04:09.760 --> 04:16.400
 YouTube video space that can maximize my average happiness or maybe education or

04:17.120 --> 04:23.440
 my growth as a human being? I think there are some great trajectories through YouTube videos,

04:24.000 --> 04:29.440
 but I wouldn't recommend that anyone spend all of their waking hours or all of their hours

04:29.440 --> 04:34.560
 watching YouTube. I mean, I think about the fact that YouTube has been really great for my kids,

04:34.560 --> 04:41.920
 for instance. My oldest daughter, you know, she's been watching YouTube for several years,

04:41.920 --> 04:48.720
 she watches Tyler Oakley and the vlogbrothers. And I know that it's had a very profound and

04:48.720 --> 04:53.920
 positive impact on her character. And my younger daughter, she's a ballerina and her teachers tell

04:53.920 --> 05:00.720
 her that YouTube is a huge advantage for her because she can practice a routine and watch

05:01.520 --> 05:07.040
 like professional dancers do that same routine and stop it and back it up and rewind and all

05:07.040 --> 05:12.640
 that stuff, right? So it's been really good for them. And then even my son is a sophomore in

05:12.640 --> 05:19.600
 college. He got through his linear algebra class because of a channel called Three Blue One Brown,

05:19.600 --> 05:26.240
 which, you know, helps you understand linear algebra, but in a way that would be very hard

05:26.240 --> 05:33.920
 for anyone to do on a whiteboard or a chalkboard. And so I think that those experiences, from my

05:33.920 --> 05:38.720
 point of view, were very good. And so I can imagine really good trajectories through YouTube. Yes.

05:38.720 --> 05:43.680
 Have you looked at, do you think of broadly about that trajectory over a period because

05:43.680 --> 05:50.640
 YouTube has grown up now. So over a period of years, you just kind of gave a few anecdotal

05:50.640 --> 05:56.240
 examples. But, you know, I used to watch certain shows on YouTube. I don't anymore. I've moved on

05:56.240 --> 06:01.680
 to other shows. And ultimately, you want people to, from YouTube's perspective, to stay on YouTube,

06:01.680 --> 06:08.400
 to grow as human beings on YouTube. So you have to think not just what makes them engage

06:08.400 --> 06:14.080
 today or this month, but also over a period of years. Absolutely. That's right. I mean,

06:14.080 --> 06:20.560
 if YouTube is going to continue to enrich people's lives, then, you know, then it has to grow with

06:20.560 --> 06:30.400
 them. And people's interests change over time. And so I think we've been working on this problem.

06:30.400 --> 06:37.040
 And I'll just say it broadly as like, how to introduce diversity and introduce people who

06:37.040 --> 06:41.280
 are watching one thing to something else they might like. We've been working on that problem

06:41.920 --> 06:48.560
 all the eight years I've been at YouTube. It's a hard problem because, I mean, of course,

06:48.560 --> 06:54.080
 it's trivial to introduce diversity that doesn't help. Yeah, just had a random video.

06:54.080 --> 06:59.840
 I could just randomly select a video from the billions that we have. It's likely not to even

06:59.840 --> 07:06.400
 be in your language. So the likelihood that you would watch it and develop a new interest is

07:06.400 --> 07:13.840
 very, very low. And so what you want to do when you're trying to increase diversity is find something

07:13.840 --> 07:21.760
 that is not too similar to the things that you've watched, but also something that you might be

07:21.760 --> 07:28.720
 likely to watch. And that balance, finding that spot between those two things is quite challenging.

07:28.720 --> 07:36.000
 So the diversity of content, diversity of ideas, it's a really difficult,

07:36.000 --> 07:40.560
 it's the thing like that's almost impossible to define, right? Like what's different?

07:41.680 --> 07:48.800
 So how do you think about that? So two examples is I'm a huge fan of Three Blue One Brown, say,

07:48.800 --> 07:54.240
 and then one diversity, I wasn't even aware of a channel called Veritasium,

07:54.240 --> 08:00.560
 which is a great science, physics, whatever channel. So one version of diversity is showing me

08:01.120 --> 08:05.840
 Derek's Veritasium channel, which I was really excited to discover actually and now watch a lot

08:05.840 --> 08:12.240
 of his videos. Okay, so you're a person who's watching some math channels and you might be

08:12.240 --> 08:17.360
 interested in some other science or math channels. So like you mentioned, the first kind of diversity

08:17.360 --> 08:25.360
 is just show you some, some things from other channels that are related, but not just, you know,

08:25.360 --> 08:31.600
 not all the Three Blue One Brown channel throw in a couple others. So that's the, maybe the first

08:31.600 --> 08:40.320
 kind of diversity that we started with many, many years ago. Taking a bigger leap is about,

08:40.320 --> 08:46.960
 I mean, the mechanisms we do, we use for that is, is we basically cluster videos and channels

08:46.960 --> 08:51.520
 together, mostly videos, we do every, almost everything at the video level. And so we'll,

08:51.520 --> 08:58.560
 we'll make some kind of a cluster via some embedding process. And then, and then measure,

08:58.560 --> 09:04.480
 you know, what is the likelihood that a, that users who watch one cluster might also watch

09:04.480 --> 09:11.680
 another cluster that's very distinct. So we may come to find that, that people who watch science

09:11.680 --> 09:20.240
 videos also like jazz. This is possible, right? And so, and so because of that relationship that

09:20.240 --> 09:26.720
 we've identified through the measure, through the embeddings and then the measurement of the

09:26.720 --> 09:32.160
 people who watch both, we might recommend a jazz video once in a while. So there's this

09:32.160 --> 09:38.640
 cluster in the embedding space of jazz videos and science videos. And so you kind of try to look at

09:38.640 --> 09:45.920
 aggregate statistics where if a lot of people that jump from science cluster to the jazz

09:45.920 --> 09:53.440
 cluster tend to remain as engaged or become more engaged, then that's, that means those two

09:54.240 --> 09:57.280
 are, they should hop back and forth and they'll be, they'll be happy.

09:57.280 --> 10:03.200
 Right. There's a higher likelihood that a person from who's watching science would like jazz than

10:03.840 --> 10:08.320
 the person watching science would like, I don't know, backyard railroads or, or something else,

10:08.320 --> 10:15.280
 right? And so we can try to measure these likelihoods and use that to make the best recommendation we

10:15.280 --> 10:21.040
 can. So, okay. So we'll talk about the machine learning of that, but I have to linger on things

10:21.040 --> 10:29.040
 that neither you or anyone have an answer to. There's gray areas of truth, which is, for example,

10:29.840 --> 10:37.200
 now I can't believe I'm going there, but politics, it, it, it happens so that certain people believe

10:37.200 --> 10:42.400
 certain things and they're very certain about them. Let's move outside the red versus blue

10:42.400 --> 10:48.560
 politics of today's world. But there's different ideologies. For example, in college, I read quite

10:48.560 --> 10:54.160
 a lot of Vyn Rand I studied and that's a particular philosophical ideology I find, I found it interesting

10:54.160 --> 10:58.640
 to explore. Okay. So that was that kind of space. I've kind of moved on from that cluster,

10:59.200 --> 11:02.960
 intellectually, but it nevertheless is an interesting cluster. There's, I was born in

11:02.960 --> 11:08.000
 the Soviet Union, socialism, communism is a certain kind of political ideology that's

11:08.000 --> 11:13.440
 really interesting to explore. Again, objectively, just there's a set of beliefs about how the economy

11:13.440 --> 11:19.440
 should work and so on. And so it's hard to know what's true or not in terms of people within

11:19.440 --> 11:24.400
 those communities are often advocating that this is how we achieve utopia in this world.

11:25.120 --> 11:34.080
 And they're pretty certain about it. So how do you try to manage politics in this chaotic,

11:34.080 --> 11:39.520
 divisive world, not positive or any kind of ideas in terms of filtering what people should

11:39.520 --> 11:46.880
 watch next and in terms of also not letting certain things be on YouTube? This is exceptionally

11:47.520 --> 11:53.360
 difficult responsibility. Right. Well, the responsibility to get this right is our top

11:53.360 --> 12:01.920
 priority. And the first comes down to making sure that we have good, clear rules of the road.

12:03.360 --> 12:07.600
 Just because we have freedom of speech doesn't mean that you can literally say anything. Like

12:07.600 --> 12:15.280
 we as a society have accepted certain restrictions on our freedom of speech. There are things like

12:15.280 --> 12:23.440
 libel laws and things like that. And so where we can draw a clear line, we do and we continue to

12:23.440 --> 12:30.720
 evolve that line over time. However, as you pointed out, wherever you draw the line, there's going to

12:30.720 --> 12:40.160
 be a borderline. And in that borderline area, we are going to maybe not remove videos, but we will

12:40.160 --> 12:47.280
 try to reduce the recommendations of them or the proliferation of them by demoting them. And then

12:47.280 --> 12:54.000
 alternatively, in those situations, try to raise what we would call authoritative or credible

12:54.000 --> 13:05.680
 sources of information. You mentioned Iran and communism. Those are two valid points of view

13:05.680 --> 13:12.240
 that people are going to debate and discuss. And of course, people who believe in one or the other

13:12.240 --> 13:16.720
 of those things are going to try to persuade other people to their point of view. And so

13:18.240 --> 13:23.040
 we're not trying to settle that or choose a side or anything like that. What we're trying

13:23.040 --> 13:30.240
 to do is make sure that the people who are expressing those point of view and offering

13:30.240 --> 13:38.320
 those positions are authoritative and credible. So let me ask a question about people I don't like

13:38.320 --> 13:45.120
 personally. You heard me. I don't care if you leave comments on this. But sometimes they're

13:45.120 --> 13:53.520
 brilliantly funny, which is trolls. So people who kind of mock, I mean, the internet is full,

13:53.520 --> 14:00.880
 the Reddit of mock style comedy, where people just kind of make fun of, point out that the emperor

14:00.880 --> 14:05.600
 has no clothes. And there's brilliant comedy in that. But sometimes you can get cruel and mean.

14:06.880 --> 14:13.920
 So on that, on the mean point, and sorry to linger on these things that have no good answers,

14:13.920 --> 14:19.920
 but actually, I totally hear you that this is really important that you're trying to solve it.

14:19.920 --> 14:24.800
 But how do you reduce the meanness of people on YouTube?

14:27.120 --> 14:33.600
 I understand that anyone who uploads YouTube videos has to become resilient to a certain

14:33.600 --> 14:43.600
 amount of meanness. I've heard that from many creators. And we are trying in various ways,

14:43.600 --> 14:52.320
 comment ranking, allowing certain features to block people to reduce or make that meanness or

14:52.320 --> 15:04.080
 that trolling behavior less effective on YouTube. And so, I mean, it's very important. But it's

15:04.080 --> 15:09.920
 something that we're going to keep having to work on. And as we improve it, maybe we'll get

15:09.920 --> 15:16.800
 to a point where people don't have to suffer this sort of meanness when they upload YouTube videos.

15:16.800 --> 15:24.320
 I hope we do. But it just does seem to be something that you have to be able to deal with

15:24.320 --> 15:28.400
 as a YouTube creator nowadays. Do you have a hope that, so you mentioned two things that

15:28.400 --> 15:33.440
 kind of agree with this. So there's like a machine learning approach of ranking

15:34.320 --> 15:39.760
 comments based on whatever, based on how much they contribute to the healthy conversation.

15:39.760 --> 15:46.400
 Let's put it that way. And the other is almost an interface question of how do you,

15:47.200 --> 15:54.480
 how does the creator filter, so block or, how do humans themselves, the users of

15:54.480 --> 15:59.280
 YouTube manage their own conversation? Do you have hope that these two tools will

15:59.280 --> 16:05.440
 create a better society without limiting freedom of speech too much? Without sort of

16:05.440 --> 16:11.920
 attacking, even like saying that, people like, what do you mean limiting sort of curating speech?

16:12.560 --> 16:16.960
 I mean, I think that that overall is our whole project here at YouTube.

16:16.960 --> 16:23.440
 Right. Like, we fundamentally believe and I personally believe very much that YouTube can

16:23.440 --> 16:30.640
 be great. It's been great for my kids. I think it can be great for society. But it's absolutely

16:30.640 --> 16:36.000
 critical that we get this responsibility part right. And that's why it's our top priority.

16:37.040 --> 16:42.960
 Susan Wojcicki, who's the CEO of YouTube, she says something that I personally find very inspiring,

16:42.960 --> 16:51.520
 which is that we want to do our jobs today in a manner so that people 20 and 30 years from now

16:51.520 --> 16:56.560
 will look back and say, you know, YouTube, they really figured this out. They really found a way

16:56.560 --> 17:03.200
 to strike the right balance between the openness and the value that the openness has, and also

17:03.200 --> 17:07.600
 making sure that we are meeting our responsibility to users in society.

17:09.040 --> 17:14.000
 So the burden on YouTube actually is quite incredible. And the one thing that people don't

17:15.360 --> 17:19.360
 give enough credit to the seriousness and the magnitude of the problem, I think.

17:19.360 --> 17:27.200
 So I personally hope that you do solve it because a lot is in your hand. A lot is riding on your

17:27.200 --> 17:32.880
 success or failure. So it's besides, of course, running a successful company, you're also curating

17:33.680 --> 17:38.960
 the content of the internet and the conversation on the internet. That's a powerful thing.

17:40.160 --> 17:48.880
 So one thing that people wonder about is how much of it can be solved with pure machine learning?

17:48.880 --> 17:55.280
 So looking at the data, studying the data and creating algorithms that curate the comments,

17:55.280 --> 18:02.640
 curate the content, and how much of it needs human intervention, meaning people here at

18:02.640 --> 18:11.680
 YouTube in a room sitting and thinking about what is the nature of truth? What is, what are the

18:11.680 --> 18:17.760
 ideals that we should be promoting, that kind of thing? So algorithm versus human input,

18:17.760 --> 18:24.480
 input. What's your sense? I mean, my own experience has demonstrated that you need both of those

18:24.480 --> 18:30.720
 things. Algorithms, I mean, you're familiar with machine learning algorithms. And the thing they

18:30.720 --> 18:39.600
 need most is data. And the data is generated by humans. And so for instance, when we're building

18:39.600 --> 18:47.280
 a system to try to figure out which are the videos that are misinformation or borderline

18:47.280 --> 18:54.720
 policy violations, well, the first thing we need to do is get human beings to make decisions about

18:54.720 --> 19:02.640
 which of those videos are in which category. And then we use that data and basically take

19:02.640 --> 19:10.800
 that information that's determined and governed by humans and extrapolate it or apply it to the

19:10.800 --> 19:19.360
 entire set of billions of YouTube videos. And we couldn't get to all the videos on YouTube well

19:19.360 --> 19:24.320
 without the humans. And we couldn't use the humans to get to all the videos of YouTube.

19:24.320 --> 19:32.320
 So there's no world in which you have only one or the other of these things. And just as you said,

19:32.320 --> 19:41.040
 a lot of it comes down to people at YouTube spending a lot of time trying to figure out what

19:41.040 --> 19:47.040
 are the right policies? What are the outcomes based on those policies? Are they the kinds of

19:47.040 --> 19:54.640
 things we want to see? And then once we kind of get an agreement or build some consensus around

19:55.120 --> 20:00.400
 what the policies are, well, then we've got to find a way to implement those policies across all

20:00.400 --> 20:07.280
 of YouTube. And that's where both the human beings, we call them evaluators or reviewers,

20:07.280 --> 20:12.800
 come into play to help us with that. And then once we get a lot of training data from them,

20:12.800 --> 20:16.240
 then we apply the machine learning techniques to take it even further.

20:16.240 --> 20:22.800
 Do you have a sense that these human beings have a bias in some kind of direction?

20:22.800 --> 20:30.800
 I mean, that's an interesting question. We do sort of in autonomous vehicles and computer vision

20:30.800 --> 20:42.000
 in general, a lot of annotation. And we rarely ask what bias do the annotators have? Even in the

20:42.000 --> 20:48.560
 sense that they're better at annotating certain things than others. For example, people are much

20:48.560 --> 20:57.200
 better at for annotating segmentation at segmenting cars in a scene versus segmenting bushes or trees.

20:58.880 --> 21:04.960
 You know, there's specific mechanical reasons for that, but also because it's semantic gray area.

21:04.960 --> 21:09.520
 And just for a lot of reasons, people are just terrible at annotating trees.

21:09.520 --> 21:15.840
 Okay. So in the same kind of sense, do you think of in terms of people reviewing videos or annotating

21:15.840 --> 21:24.160
 the content of videos, is there some kind of bias that you're aware of or seek out in that human input?

21:24.160 --> 21:31.040
 Well, we take steps to try to overcome these kinds of biases or biases that we think would be

21:31.040 --> 21:38.560
 problematic. So for instance, we ask people to have a bias towards scientific consensus. That's

21:38.560 --> 21:46.720
 something that we instruct them to do. We ask them to have a bias towards demonstration of

21:46.720 --> 21:52.960
 expertise or credibility or authoritativeness. But there are other biases that we want to

21:52.960 --> 21:59.280
 make sure to try to remove. And there's many techniques for doing this. One of them is you

21:59.280 --> 22:06.880
 send the same thing to be reviewed to many people. And so that's one technique. Another is that you

22:06.880 --> 22:13.520
 make sure that the people that are doing these sorts of tasks are from different backgrounds and

22:13.520 --> 22:19.440
 different areas of the United States or of the world. But then even with all of that, it's possible

22:19.440 --> 22:27.600
 for certain kinds of what we would call unfair biases to creep into machine learning systems,

22:27.600 --> 22:33.760
 primarily as you said, because maybe the training data itself comes in in a biased way. And so

22:33.760 --> 22:41.760
 we also have worked very hard on improving the machine learning systems to remove and reduce

22:41.760 --> 22:50.000
 unfair biases when it's when it goes against or is involved some protected class, for instance.

22:50.000 --> 22:56.000
 Thank you for exploring with me some of the more challenging things. I'm sure there's a few more

22:56.000 --> 23:02.800
 that we'll jump back to. But let me jump into the fun part, which is maybe the basics

23:02.800 --> 23:09.760
 of the quote unquote YouTube algorithm. What does the YouTube algorithm look at to make

23:09.760 --> 23:15.360
 recommendation for what to watch next from a machine learning perspective? Or when you

23:16.080 --> 23:21.440
 search for a particular term, how does it know what to show you next? Because it seems to, at

23:21.440 --> 23:27.440
 least for me, do an incredible job of both. Well, that's kind of you to say. It didn't

23:27.440 --> 23:34.080
 used to do a very good job. But it's gotten better over the years. Even I observe that it's

23:34.080 --> 23:39.040
 improved quite a bit. Those are two different situations. Like when you search for something,

23:40.320 --> 23:48.800
 YouTube uses the best technology we can get from Google to make sure that the YouTube search system

23:48.800 --> 23:54.640
 finds what someone's looking for. And of course, the very first things that one thinks about is,

23:54.640 --> 24:03.440
 okay, well, does the word occur in the title? For instance, but there are much more sophisticated

24:03.440 --> 24:11.280
 things where we're mostly trying to do some syntactic match or maybe a semantic match based on

24:12.400 --> 24:19.920
 words that we can add to the document itself. For instance, maybe is this video

24:19.920 --> 24:28.480
 watched a lot after this query? That's something that we can observe. And then as a result,

24:29.120 --> 24:36.240
 make sure that that document would be retrieved for that query. Now, when you talk about what kind

24:36.240 --> 24:43.760
 of videos would be recommended to watch next, that's something, again, we've been working on for

24:43.760 --> 24:55.760
 many years. And probably the first real attempt to do that well was to use collaborative filtering.

24:56.960 --> 24:59.520
 So you can describe what collaborative filtering is?

24:59.520 --> 25:07.680
 Sure. It's just basically what we do is we observe which videos get watched close together

25:07.680 --> 25:16.160
 by the same person. And if you observe that, and if you can imagine creating a graph where the videos

25:16.160 --> 25:21.760
 that get watched close together by the most people are sort of very close to one another in this

25:21.760 --> 25:27.120
 graph and videos that don't frequently get watched close to close together by the same person or

25:27.120 --> 25:34.320
 the same people are far apart, then you end up with this graph that we call the related

25:34.320 --> 25:41.040
 graph that basically represents videos that are very similar or related in some way. And

25:42.080 --> 25:48.800
 what's amazing about that is that it puts all the videos that are in the same language together,

25:48.800 --> 25:55.360
 for instance. And we didn't even have to think about language. It just doesn't, right? And it

25:55.360 --> 25:59.920
 puts all the videos that are about sports together, and it puts most of the music videos together,

25:59.920 --> 26:06.560
 and it puts all of these sorts of videos together just because that's sort of the way the people

26:06.560 --> 26:14.240
 using YouTube behave. So that already cleans up a lot of the problem. It takes care of the

26:14.240 --> 26:20.720
 lowest hanging fruit, which happens to be a huge one of just managing these millions of videos.

26:20.720 --> 26:25.840
 That's right. I remember a few years ago, I was talking to someone who was

26:25.840 --> 26:37.280
 trying to propose that we do a research project concerning people who are bilingual. And this

26:37.280 --> 26:44.560
 person was making this proposal based on the idea that YouTube could not possibly be good

26:45.200 --> 26:53.280
 at recommending videos well to people who are bilingual. And so she was telling me

26:53.280 --> 26:58.480
 about this, and I said, well, can you give me an example of what problem do you think we have on

26:58.480 --> 27:05.840
 YouTube with the recommendations? And so she said, well, I'm a researcher in the U.S., and when I'm

27:05.840 --> 27:11.680
 looking for academic topics, I want to see them in English. And so she searched for one,

27:11.680 --> 27:15.920
 found a video, and then looked at the watch next suggestions, and they were all in English.

27:16.560 --> 27:21.760
 And so she said, oh, I see. YouTube must think that I speak only English. And so she said,

27:21.760 --> 27:26.080
 now I'm actually originally from Turkey, and sometimes when I'm cooking, let's say I want to

27:26.080 --> 27:31.520
 make some baklava, I really like to watch videos that are in Turkish. And so she searched for a

27:31.520 --> 27:36.560
 video about making the baklava, and then selected it, and it was in Turkish. And the watch next

27:36.560 --> 27:42.960
 recommendations were in Turkish. And she just couldn't believe how this was possible. And how

27:42.960 --> 27:47.200
 is it that you know that I speak both these two languages and put all the videos together? And

27:47.200 --> 27:52.960
 it's just sort of an outcome of this related graph that's created through collaborative filtering.

27:54.000 --> 28:00.320
 So for me, one of my huge interests is just human psychology, right? And that's such a powerful

28:00.320 --> 28:06.560
 platform on which to utilize human psychology to discover what individual people want to watch

28:06.560 --> 28:15.120
 next. But it's also be just fascinating to me. You know, I've Google search has ability to look

28:15.120 --> 28:21.440
 at your own history. And I've done that before. Just, just what I've searched three years for

28:21.440 --> 28:28.240
 many, many years. And it's fascinating picture of who I am actually. And I don't think anyone's

28:28.240 --> 28:34.560
 ever summarized. I personally would love that a summary of who I am as a person on the internet

28:35.200 --> 28:42.320
 to me, because I think it reveals, I think it puts a mirror to me or to others, you know,

28:42.320 --> 28:49.520
 that's actually quite revealing and interesting. You know, just maybe the number of it's a joke,

28:49.520 --> 28:54.800
 but not really is the number of cap videos I've watched or videos of people falling,

28:54.800 --> 29:00.720
 you know, stuff that's absurd, that kind of stuff. It's really interesting. And of course,

29:00.720 --> 29:06.960
 it's really good for the machine learning aspect to, to show, to figure out what to show next.

29:06.960 --> 29:14.480
 But it's interesting. Have you just as a tangent played around with the idea of giving a map to

29:14.480 --> 29:22.240
 people sort of as opposed to just using this information to show us next, showing them here

29:22.240 --> 29:27.840
 are the clusters you've loved over the years kind of thing. Well, we do provide the history of all

29:27.840 --> 29:32.160
 the videos that you've watched. Yes. So you can definitely search through that and look through

29:32.160 --> 29:36.720
 it and search through it to see what it is that you've been watching on YouTube. We have actually,

29:38.000 --> 29:46.080
 in various times, experimented with this sort of cluster idea, finding ways to demonstrate or show

29:46.080 --> 29:51.920
 people what topics they've been interested in or what what clusters they've watched from.

29:52.480 --> 29:59.760
 It's interesting that you bring this up because in some sense, the way the recommendation system

29:59.760 --> 30:06.080
 of YouTube sees a user is exactly as the history of all the videos they've watched on YouTube.

30:06.800 --> 30:18.080
 And so you can think of yourself or any user on YouTube as kind of like a DNA strand of all

30:18.080 --> 30:24.640
 your videos, right? That sort of represents you. You can also think of it as maybe a vector in

30:24.640 --> 30:31.920
 the space of all the videos on YouTube. And so now, once you think of it as a vector in the

30:31.920 --> 30:37.600
 space of all the videos on YouTube, then you can start to say, okay, well, which other vectors

30:37.600 --> 30:44.880
 are close to me and to my vector? And that's one of the ways that we generate some diverse

30:44.880 --> 30:50.960
 recommendations is because you're like, okay, well, these people seem to be close with respect

30:50.960 --> 30:56.560
 to the videos they've watched on YouTube. But here's a topic or a video that one of them has

30:56.560 --> 31:02.000
 watched and enjoyed, but the other one hasn't. That could be an opportunity to make a good

31:02.000 --> 31:07.440
 recommendation. I gotta tell you, I'm gonna ask for things that are impossible, but I would love

31:07.440 --> 31:13.040
 to cluster than human beings. I would love to know who has similar trajectories as me,

31:13.040 --> 31:17.040
 because you probably would want to hang out. There's a social aspect there.

31:17.040 --> 31:22.000
 Like actually finding some of the most fascinating people I find on YouTube have like no followers,

31:22.000 --> 31:27.200
 and I start following them and they create incredible content. And you know, and on that topic,

31:27.200 --> 31:33.440
 I just love to ask, there's some videos that just blow my mind in terms of quality and depth.

31:34.080 --> 31:43.200
 And just in every regard are amazing videos and they have like 57 views. Okay. How do you get

31:43.200 --> 31:50.800
 videos of quality to be seen by many eyes? So the measure of quality, is it just something?

31:50.800 --> 31:56.400
 Yeah. How do you know that something is good? Well, I mean, I think it depends initially on

31:57.040 --> 32:04.640
 what sort of video we're talking about. So in the realm of let's say, you mentioned politics and news,

32:04.640 --> 32:16.240
 in that realm, quality news or quality journalism relies on having a journalism

32:17.360 --> 32:21.920
 department, right? Like you have to have actual journalists and fact checkers and people like

32:21.920 --> 32:30.800
 that. And so in that situation, and in others, maybe science or in medicine, quality has a lot

32:30.800 --> 32:35.920
 to do with the authoritativeness and the credibility and the expertise of the people who make the video.

32:37.360 --> 32:39.280
 Now, if you think about the other end of the spectrum,

32:40.880 --> 32:44.960
 you know, what is the highest quality prank video? Or what is the highest quality

32:46.160 --> 32:53.360
 Minecraft video, right? That might be the one that people enjoy watching the most and watch to the

32:53.360 --> 33:01.200
 end. Or it might be the one that when we ask people the next day after they watched it,

33:02.160 --> 33:08.000
 were they satisfied with it? And so we, especially in the realm of entertainment,

33:09.120 --> 33:17.120
 have been trying to get at better and better measures of quality or satisfaction or enrichment

33:17.120 --> 33:22.880
 since I came to YouTube. And we started with, well, you know, the first approximation is the one that

33:22.880 --> 33:31.520
 gets more views. But, you know, we both know that things can get a lot of views and not really be

33:31.520 --> 33:36.160
 that high quality, especially if people are clicking on something and then immediately

33:36.160 --> 33:43.360
 realizing that it's not that great and abandoning it. And that's why we moved from views to thinking

33:43.360 --> 33:48.000
 to thinking about the amount of time people spend watching it with the premise that like,

33:48.800 --> 33:55.840
 you know, in some sense, the time that someone spends watching a video is related to the value

33:55.840 --> 34:00.480
 that they get from that video. It may not be perfectly related, but it has something to say

34:00.480 --> 34:07.920
 about how much value they get. But even that's not good enough, right? Because I myself have spent

34:07.920 --> 34:14.480
 time clicking through channels on television late at night and ended up watching under siege too,

34:14.480 --> 34:18.880
 for some reason, I don't know. And if you were to ask me the next day, are you glad that you

34:18.880 --> 34:26.000
 watched that show on TV last night? I'd say, yeah, I wish I would have gone to bed or read a book or

34:26.000 --> 34:33.520
 almost anything else really. And so that's why some people got the idea a few years ago to try

34:33.520 --> 34:42.720
 to survey users afterwards. And so we get feedback data from those surveys and then use that in the

34:42.720 --> 34:46.320
 machine learning system to try to not just predict what you're going to click on right now,

34:47.120 --> 34:52.800
 what you might watch for a while, but what when we ask you tomorrow, you'll give four or five stars

34:52.800 --> 34:59.120
 to. So just to summarize, what are the signals from the machine learning perspective that the

34:59.120 --> 35:04.560
 user can provide? So you mentioned just clicking on the video views, the time watch, maybe the

35:04.560 --> 35:12.400
 relative time watch, the clicking like and dislike on the video, maybe commenting on the video,

35:13.440 --> 35:18.560
 all those things, all those things. And then the one I wasn't actually quite aware of,

35:18.560 --> 35:24.400
 even though I might have engaged in it is a survey afterwards, which is a brilliant idea.

35:24.400 --> 35:30.480
 Is there other signals? I mean, that's already a really rich space of signals to learn from.

35:30.480 --> 35:36.720
 Is there something else? Well, you mentioned commenting, also sharing the video. If you

35:36.720 --> 35:40.800
 think it's worthy to be shared with someone else, you know, within YouTube or outside of YouTube

35:40.800 --> 35:46.400
 as well. Either. Let's see, you mentioned like, dislike. Like and dislike. How important is that?

35:46.400 --> 35:53.680
 It's very important, right? We want, it's predictive of satisfaction. But it's not, it's

35:53.680 --> 36:00.640
 not perfectly predictive. Subscribe. If you subscribe to the channel of the person who

36:00.640 --> 36:07.360
 made the video, then that also is a piece of information that signals satisfaction. Although

36:08.800 --> 36:14.720
 over the years, we've learned that people have a wide range of attitudes about what it means to

36:14.720 --> 36:23.920
 subscribe. We would ask some users who didn't subscribe very much, but they watched a lot from

36:23.920 --> 36:27.840
 a few channels. We'd say, well, why didn't you subscribe? And they would say, well, I can't

36:27.840 --> 36:33.840
 afford to pay for anything. And, you know, we tried to let them understand like, actually,

36:33.840 --> 36:39.200
 it doesn't cost anything. It's free. It just helps us know that you are very interested in this

36:39.200 --> 36:46.320
 creator. But then we've asked other people who subscribed to many things and don't really watch

36:47.040 --> 36:53.040
 any of the videos from those channels. And we say, well, why did you subscribe to this if you

36:53.040 --> 36:56.960
 weren't really interested in any more videos from that channel? And they might tell us,

36:57.840 --> 37:00.800
 well, I just, you know, I thought the person did a great job and I just want to kind of

37:00.800 --> 37:06.960
 give them a high five. Yeah. And so, yeah, that's where I said, I actually subscribed to channels

37:06.960 --> 37:15.040
 where I just, this person is amazing. I like this person. But then I like this person,

37:15.040 --> 37:21.200
 I really want to support them. That's how I click subscribe, even though I mean, never actually want

37:21.200 --> 37:25.680
 to click on their videos when they're releasing it. I just love what they're doing. And it's maybe

37:25.680 --> 37:31.520
 outside of my interest area and so on, which is probably the wrong way to use the subscribe button.

37:31.520 --> 37:37.040
 Well, I just want to say congrats. This is a great work. Well, so you have to deal with all the

37:37.040 --> 37:41.840
 space of people that see the subscribe button is totally different. That's right. And so, you know,

37:41.840 --> 37:47.280
 we, we can't just close our eyes and say, sorry, you're using it wrong. You know, we're not going

37:47.280 --> 37:52.640
 to pay attention to what you've done. We need to embrace all the ways in which all the different

37:52.640 --> 37:58.800
 people in the world use the subscribe button or the like and the dislike button. So in terms of

37:58.800 --> 38:06.240
 signals of machine learning, using for the search and for the recommendation, you've mentioned title,

38:06.240 --> 38:12.720
 so like metadata, like text data that people provide description and title and maybe keywords.

38:13.360 --> 38:19.840
 So maybe you can speak to the value of those things in search and also this incredible,

38:19.840 --> 38:25.200
 fascinating area of the content itself. So the video content itself, trying to understand what's

38:25.200 --> 38:29.440
 happening in the video. So YouTube released a data set that, you know, in the machine learning

38:29.440 --> 38:35.920
 computer vision world, this is just an exciting space. How much is that currently? How much are

38:35.920 --> 38:40.160
 you playing with that currently? How much is your hope for the future of being able to analyze the

38:40.160 --> 38:46.000
 content of the video itself? Well, we have been working on that also since I came to YouTube.

38:46.000 --> 38:52.400
 Analyzing the content. Analyzing the content of the video, right. And what I can tell you is that

38:52.400 --> 39:02.560
 our ability to do it well is still somewhat crude. We can, we can tell if it's a music video. We can

39:02.560 --> 39:09.920
 tell if it's a sports video. We can probably tell you that people are playing soccer. We probably

39:09.920 --> 39:16.800
 can't tell whether it's Manchester United or my daughter's soccer team. So these things are kind

39:16.800 --> 39:22.960
 of difficult and using them, we can use them in some ways. So for instance, we use that kind of

39:22.960 --> 39:31.360
 information to understand and inform these clusters that I talked about. And also maybe to add some

39:31.360 --> 39:36.800
 words like soccer, for instance, to the video if it doesn't occur in the title or the description,

39:36.800 --> 39:44.640
 which is remarkable that often it doesn't. One of the things that I ask creators to do is please

39:44.640 --> 39:52.800
 help us out with the title in the description. For instance, we were a few years ago having a

39:52.800 --> 40:00.960
 live stream of some competition for World of Warcraft on YouTube. And it was a very important

40:00.960 --> 40:05.840
 competition. But if you typed World of Warcraft in search, you wouldn't find it. World of Warcraft

40:05.840 --> 40:11.280
 wasn't in the title? World of Warcraft wasn't in the title. It was match four, seven, eight, you know,

40:11.280 --> 40:16.480
 A team versus B team. And World of Warcraft wasn't in the title. Just like, come on, give me.

40:17.680 --> 40:22.160
 Being literal on the internet is actually very uncool, which is the problem.

40:22.160 --> 40:22.960
 Oh, is that right?

40:23.760 --> 40:29.200
 Well, I mean, in some sense, some of the greatest videos, I mean, there's a humor to just being

40:29.200 --> 40:36.080
 indirect, being witty and so on. And actually, machine learning algorithms want you to be

40:36.080 --> 40:44.080
 literal. You just want to say what's in the thing, be very, very simple. And in some sense,

40:44.080 --> 40:50.000
 that gets away from wit and humor. So you have to play with both. But you're saying that for

40:50.000 --> 40:56.560
 now, the content of the title, the content of the description, the actual text is one of the

40:56.560 --> 41:02.800
 best ways for the algorithm to find your video and put them in the right cluster.

41:02.800 --> 41:09.280
 That's right. And I would go further and say that if you want people, human beings, to select

41:09.280 --> 41:14.800
 your video in search, then it helps to have, let's say, World of Warcraft in the title. Because

41:16.240 --> 41:19.840
 why would a person's, you know, if they're looking at a bunch, they type World of Warcraft,

41:19.840 --> 41:24.800
 and they have a bunch of videos, all of whom say World of Warcraft, except the one that you uploaded,

41:24.800 --> 41:29.040
 well, even the person is going to think, well, maybe this isn't somehow search made a mistake.

41:29.040 --> 41:33.760
 This isn't really about World of Warcraft. So it's important not just for the machine

41:33.760 --> 41:38.320
 learning systems, but also for the people who might be looking for this sort of thing. They get a

41:39.280 --> 41:45.360
 clue that it's what they're looking for by seeing that same thing prominently in the title of the

41:45.360 --> 41:50.560
 video. Okay, let me push back on that. So I think from the algorithm perspective, yes, but if they

41:50.560 --> 41:59.040
 typed in World of Warcraft and saw a video that with the title simply winning, and the thumbnail

41:59.040 --> 42:09.840
 has like a sad orc or something, I don't know, right? Like, I think that's much, it gets your

42:09.840 --> 42:15.680
 curiosity up. And then if they could trust that the algorithm was smart enough to figure out somehow

42:15.680 --> 42:19.760
 that this is indeed a World of Warcraft video, that would have created the most beautiful

42:19.760 --> 42:25.120
 experience. I think in terms of just the wit and the humor and the curiosity that we human beings

42:25.120 --> 42:29.440
 naturally have, but you're saying, I mean, realistically speaking, it's really hard for

42:29.440 --> 42:34.480
 the algorithm to figure out that the content of that video will be a World of Warcraft video.

42:34.480 --> 42:39.600
 And you have to accept that some people are going to skip it. Yeah, right. I mean, and so you're

42:39.600 --> 42:47.040
 right. The people who don't skip it and select it are going to be delighted. Yeah. But other people

42:47.040 --> 42:52.240
 might say, yeah, this is not what I was looking for. And making stuff discoverable, I think,

42:52.240 --> 42:59.200
 is what you're really working on and hoping. So yeah, so from your perspective, put stuff in

42:59.200 --> 43:04.080
 the title of the description. And remember, the collaborative filtering part of the system

43:04.080 --> 43:11.520
 starts by the same user watching videos together, right? So the way that they're probably going

43:11.520 --> 43:16.320
 to do that is by searching for them. That's a fascinating aspect of it. It's like ant colonies.

43:16.320 --> 43:23.600
 That's how they find stuff. So I mean, what degree for collaborative filtering in general

43:24.560 --> 43:31.440
 is one curious ant, one curious user essential? So just a person who is more willing to click on

43:31.440 --> 43:37.840
 random videos and sort of explore these cluster spaces. In your sense, how many people are just

43:37.840 --> 43:41.680
 like watching the same thing over and over and over and over? And how many are just like the

43:41.680 --> 43:48.160
 explorers that just kind of like click on stuff and then help the other ant in the ants colony

43:48.160 --> 43:52.720
 discover the cool stuff. Do you have a sense of that at all? I really don't think I have a sense

43:52.720 --> 43:58.800
 for the relative sizes of those groups. But I would say that people come to YouTube with

43:58.800 --> 44:06.720
 some certain amount of intent. And as long as they, to the extent to which they try to satisfy

44:06.720 --> 44:12.880
 that intent, that certainly helps our systems, right? Because our systems rely on kind of a

44:12.880 --> 44:19.440
 faithful amount of behavior, right? And there are people who try to trick us, right? There are people

44:19.440 --> 44:26.080
 and machines that try to associate videos together that really don't belong together,

44:26.080 --> 44:32.000
 but they're trying to get that association made because it's profitable for them. And so we have

44:32.000 --> 44:39.040
 to always be resilient to that sort of attempt at gaming the systems. So speaking to that,

44:39.040 --> 44:44.080
 there's a lot of people that in a positive way, perhaps, I don't know, I don't like it, but

44:44.080 --> 44:48.720
 like to gain, want to try to gain the system to get more attention. Everybody, creators,

44:48.720 --> 44:55.040
 in a positive sense, want to get attention, right? So how do you, how do you work in this space when

44:55.040 --> 45:03.840
 people create more and more sort of click baity titles and thumbnails? Sort of very

45:03.840 --> 45:09.360
 to ask him, Derek has made a video where basically describes that it seems what works is to create

45:09.360 --> 45:14.560
 a high quality video, really good video where people would want to watch and wants to click on it,

45:14.560 --> 45:19.680
 but have click baity titles and thumbnails to get them to click on it in the first place.

45:19.680 --> 45:24.320
 And he's saying, I'm embracing this fact, I'm just going to keep doing it. And I hope

45:24.320 --> 45:28.880
 you forgive me for doing it. And you will enjoy my videos once you click on them.

45:28.880 --> 45:38.080
 So in what sense do you see this kind of click bait style attempt to manipulate to get people in

45:38.080 --> 45:42.400
 the door to manipulate the algorithm or play with the algorithm or game the algorithm?

45:43.280 --> 45:47.440
 I think that that you can look at it as an attempt to game the algorithm. But

45:48.640 --> 45:53.120
 even if you were to take the algorithm out of it and just say, okay, well, all these videos

45:53.120 --> 45:57.760
 happen to be lined up, which the algorithm didn't make any decision about which one to

45:57.760 --> 46:02.480
 put at the top or the bottom, but they're all lined up there, which one are the people going to

46:02.480 --> 46:08.960
 choose? And I'll tell you the same thing that I told Derek is, you know, I have a bookshelf

46:08.960 --> 46:15.040
 and they have two kinds of books on them, science books. I have my math books from when I was a

46:15.040 --> 46:21.920
 student and they all look identical except for the titles on the covers. They're all yellow,

46:21.920 --> 46:26.800
 they're all from Springer, and they're every single one of them. The cover is totally the same.

46:28.160 --> 46:34.560
 Right? On the other hand, I have other more pop science type books, and they all have very

46:34.560 --> 46:40.800
 interesting covers, right? And they have provocative titles and things like that. I mean, I wouldn't

46:40.800 --> 46:46.560
 say that they're click baity because they are indeed good books. And I don't think that they

46:46.560 --> 46:54.800
 cross any line, but that's just a decision you have to make, right? Like the people who write

46:54.800 --> 47:01.680
 classical recursion theory by P. R. O. D. Freddie, he was fine with the yellow title and nothing

47:01.680 --> 47:10.240
 more. Whereas I think other people who wrote a more popular type book understand that they need

47:10.240 --> 47:16.880
 to have a compelling cover and a compelling title. And, you know, I don't think there's anything

47:16.880 --> 47:24.400
 really wrong with that. We do take steps to make sure that there is a line that you don't cross.

47:24.400 --> 47:31.920
 And if you go too far, maybe your thumbnail is especially racy or, you know, it's all caps with

47:31.920 --> 47:40.720
 too many exclamation points. We observe that users are kind of, you know, sometimes offended

47:40.720 --> 47:48.960
 by that. And so for the users who are offended by that, we will then depress or suppress those

47:49.520 --> 47:55.760
 videos. And which reminds me, there's also another signal where users can say, I don't know if I was

47:55.760 --> 48:00.960
 recently added, but I really enjoy it. I'm just saying I didn't, something like I don't want to

48:00.960 --> 48:07.360
 see this video anymore or something like, like this is a, like there's certain videos that just

48:08.000 --> 48:12.800
 cut me the wrong way. Like just jump out at me. It's like, I don't want this. And it feels really

48:12.800 --> 48:19.040
 good to clean that up. To be like, I don't, that's not, that's not for me. I don't know. I think

48:19.040 --> 48:23.600
 that might have been recently added, but that's also a really strong signal. Yes, absolutely.

48:23.600 --> 48:29.920
 Right. We don't want to make a recommendation that people are unhappy with. And that makes me,

48:29.920 --> 48:35.040
 that particular one makes me feel good as a user in general, and as a machine learning person,

48:35.040 --> 48:39.840
 because I feel like I'm helping the algorithm. My interaction on YouTube don't always feel like

48:39.840 --> 48:46.640
 I'm helping the algorithm. Like I'm not reminded of that fact. Like for example, Tesla and Autopilot

48:46.640 --> 48:51.600
 and, you know, Musk create a feeling for their customers, for people that own Tesla's, that

48:51.600 --> 48:56.000
 they're helping the algorithm of Tesla. Like they're all like a really proud, they're helping

48:56.000 --> 49:01.120
 the fleet learn. I think YouTube doesn't always remind people that you're helping the algorithm

49:01.120 --> 49:07.280
 get smarter. And for me, I love that idea. Like we're all collaboratively, like Wikipedia gives

49:07.280 --> 49:13.520
 that sense that we're all together creating a beautiful thing. YouTube doesn't always remind

49:13.520 --> 49:19.360
 me of that. This conversation is reminding me of that, but... Well, that's a good tip. We should

49:19.360 --> 49:24.480
 keep that fact in mind when we design these features. I'm not sure I really thought about it

49:24.480 --> 49:29.760
 that way, but that's a very interesting perspective. It's an interesting question of personalization

49:30.800 --> 49:37.280
 that I feel like when I click like on a video, I'm just improving my experience.

49:39.200 --> 49:43.840
 It would be great. It would make me personally, people are different, but make me feel great

49:43.840 --> 49:48.000
 if I was helping also the YouTube algorithm broadly say something. You know what I'm saying?

49:48.000 --> 49:55.040
 Like I don't know if that's human nature, but the products you love, and I certainly love

49:55.040 --> 50:00.000
 YouTube, you want to help it get smarter and smarter and smarter because there's some kind

50:00.000 --> 50:06.400
 of coupling between our lives together being better. If YouTube was better, then my life

50:06.400 --> 50:09.840
 will be better. And that's that kind of reasoning. I'm not sure what that is. And I'm not sure how

50:09.840 --> 50:15.360
 many people share that feeling. That could be just a machine learning feeling. But on that point,

50:15.360 --> 50:23.600
 how much personalization is there in terms of next video recommendations? So is it kind of

50:23.600 --> 50:30.800
 all really boiling down to clustering? Like if I'm in your clusters to me and so on and

50:31.680 --> 50:35.840
 that kind of thing, or how much is personalized to me, the individual completely?

50:35.840 --> 50:44.080
 It's very, very personalized. So your experience will be quite a bit different from anybody else's

50:44.080 --> 50:51.760
 who's watching that same video, at least when they're logged in. And the reason is that we found

50:51.760 --> 50:58.720
 that users often want two different kinds of things when they're watching a video. Sometimes

50:58.720 --> 51:06.480
 they want to keep watching more on that topic or more in that genre. And other times they just

51:06.480 --> 51:10.560
 are done and they're ready to move on to something else. And so the question is,

51:10.560 --> 51:16.400
 well, what is the something else? And one of the first things one can imagine is, well,

51:16.400 --> 51:22.640
 maybe something else is the latest video from some channel to which you've subscribed. And

51:22.640 --> 51:28.800
 that's going to be very different for you than it is for me, right? And even if it's not something

51:28.800 --> 51:32.720
 that you subscribe to, it's something that you watch a lot. And again, that'll be very different

51:32.720 --> 51:41.040
 on a person by person basis. And so even the watch next, as well as the homepage, of course,

51:41.040 --> 51:47.280
 is quite personalized. So what we mentioned some of the signals, but what does success look like?

51:47.280 --> 51:52.160
 What does success look like in terms of the algorithm creating a great long term experience

51:52.160 --> 51:59.120
 for a user? Or put another way, if you look at the videos I've watched this month,

51:59.120 --> 52:06.240
 how do you know the algorithm succeeded for me? I think, first of all, if you come back and watch

52:06.240 --> 52:11.040
 more YouTube, then that's one indication that you found some value from it. So just the number of

52:11.040 --> 52:19.280
 hours is a powerful indicator? Well, I mean, not the hours themselves, but the fact that you return

52:19.280 --> 52:26.880
 on another day. So that's probably the most simple indicator. People don't come back to things that

52:26.880 --> 52:32.560
 they don't find value in, right? There's a lot of other things that they could do. But like I said,

52:32.560 --> 52:38.400
 I mean, ideally, we would like everybody to feel that YouTube enriches their lives and that every

52:38.400 --> 52:43.440
 video they watched is the best one they've ever watched since they've started watching YouTube.

52:44.080 --> 52:53.360
 And so that's why we survey them and ask them, like, is this one to five stars? And so our version

52:53.360 --> 53:00.480
 of success is every time someone takes that survey, they say it's five stars. And if we ask them,

53:00.480 --> 53:07.040
 is this the best video you've ever seen on YouTube? They say yes, every single time. So it's hard to

53:07.040 --> 53:11.360
 imagine that we would actually achieve that. Maybe asymptotically, we would get there. But

53:12.320 --> 53:19.360
 that would be what we think success is. It's funny. I've recently said somewhere, I don't know,

53:19.360 --> 53:27.600
 maybe tweeted, but that Ray Dalio has this video on the economic machine. I forget what it's called,

53:27.600 --> 53:31.760
 but it's a 30 minute video. And I said, it's the greatest video I've ever watched on YouTube.

53:32.560 --> 53:38.560
 It's like, I watched the whole thing and my mind was blown as a very crisp, clean description of

53:38.560 --> 53:44.240
 how at least the American economic system works. It's a beautiful video. And I was just, I wanted

53:44.240 --> 53:50.080
 to click on something to say, this is the best thing ever. Please let me, I can't believe I

53:50.080 --> 53:56.960
 discovered it. I mean, the views and the likes reflect its quality. But I was almost upset that

53:56.960 --> 54:02.080
 I haven't found it earlier and wanted to find other things like it. I don't think I've ever felt

54:02.080 --> 54:08.400
 that this is the best video I've ever watched. And that was that. And to me, the ultimate Utopia,

54:08.400 --> 54:13.440
 the best experiences were every single video where I don't see any of the videos I regret and

54:13.440 --> 54:19.920
 every single video I watched is one that actually helps me grow, helps me enjoy life, be happy,

54:19.920 --> 54:29.840
 and so on. Well, so that's, that's, that's a heck of a, that's a, that's one of the most beautiful

54:29.840 --> 54:34.560
 and ambitious, I think, machine learning tasks. So when you look at a society as opposed to an

54:34.560 --> 54:40.880
 individual user, do you think of how YouTube is changing society when you have these millions

54:40.880 --> 54:47.440
 of people watching videos, growing, learning, changing, having debates? Do you have a sense

54:47.440 --> 54:52.800
 of, yeah, what the big impact on society is? Because I think it's huge, but do you have a

54:52.800 --> 54:59.600
 sense of what direction we're taking this world? Well, I mean, I think, you know, openness has had

54:59.600 --> 55:06.720
 an impact on society already. There's a lot of... What do you mean by openness? Well, the fact that

55:06.720 --> 55:15.200
 unlike other mediums, there's not someone sitting at YouTube who decides before you can upload your

55:15.200 --> 55:22.160
 video, whether it's worth having you upload it, or, or worth anybody seeing it really, right? And so,

55:23.360 --> 55:31.440
 you know, there are some creators who say, like, I wouldn't have this opportunity to, to reach an

55:31.440 --> 55:37.920
 audience. Tyler Oakley often said that, you know, he wouldn't have had this opportunity to reach this

55:37.920 --> 55:47.840
 audience if it weren't for YouTube. And, and so I think that's one way in which YouTube has changed

55:47.840 --> 55:54.800
 society. I know that there are people that I work with from outside the United States, especially

55:54.800 --> 56:04.160
 from places where literacy is low. And they think that YouTube can help in those places because

56:04.720 --> 56:09.680
 you don't need to be able to read and write in order to learn something important for your life,

56:09.680 --> 56:16.960
 maybe, you know, how to do some job or how to fix something. And so that's another way in which I

56:16.960 --> 56:24.480
 think YouTube is possibly changing society. So I've, I've worked at YouTube for eight, almost nine

56:24.480 --> 56:31.920
 years now. And it's fun because I meet people and, you know, you tell them where they, where you work,

56:31.920 --> 56:37.280
 you say you work on YouTube, and they immediately say, I love YouTube. Yeah. Right. Which is great,

56:37.280 --> 56:41.920
 makes me feel great. But then, of course, when I ask them, well, what is it that you love about

56:41.920 --> 56:50.080
 YouTube? Not one time ever has anybody said that the search works outstanding or that the recommendations

56:50.080 --> 56:57.840
 are great. What they always say when I ask them, what do you love about YouTube is they immediately

56:57.840 --> 57:04.320
 start talking about some channel or some creator or some topic or some community that they found on

57:04.320 --> 57:13.520
 YouTube and that they just love. Yeah. And so that has made me realize that YouTube is really about

57:13.520 --> 57:21.440
 the video and connecting the people with the videos and then everything else kind of gets out of the

57:21.440 --> 57:29.120
 way. So beyond the video, it's an interesting, because you kind of mentioned creator. What about

57:29.120 --> 57:35.600
 the connection with just the individual creators as opposed to just individual video? So like,

57:35.600 --> 57:43.360
 I gave the example of Dalya video that the video itself is incredible. But there's some people

57:43.360 --> 57:49.680
 who are just creators that I love that they're one of the cool things about people who call

57:49.680 --> 57:54.800
 themselves YouTubers or whatever is they have a journey. They usually almost all of them are

57:54.800 --> 57:59.760
 or they suck horribly in the beginning and then they kind of grow and then there's that

57:59.760 --> 58:06.720
 genuineness in their growth. So YouTube clearly wants to help creators connect with their audience

58:06.720 --> 58:11.280
 in this kind of way. So how do you think about that process of helping creators grow,

58:11.280 --> 58:16.000
 helping them connect with their audience, develop not just individual videos, but the

58:16.000 --> 58:21.920
 entirety of a creator's life on YouTube? Well, I mean, we're trying to help creators find the

58:21.920 --> 58:28.640
 biggest audience that they can find. And the reason why that's you brought up creator versus

58:28.640 --> 58:37.760
 video, the reason why creator channel is so important is because if we have a hope of people

58:37.760 --> 58:45.120
 coming back to YouTube, well, they have to have in their minds some sense of what they're going to

58:45.120 --> 58:53.280
 find when they come back to YouTube. If YouTube were just the next viral video, and I have no

58:53.280 --> 58:58.240
 concept of what the next viral video could be one time it's a cat playing a piano and the next day

58:58.240 --> 59:04.480
 it's some children interrupting a reporter and the next day it's some other thing happening,

59:05.360 --> 59:13.520
 then it's hard for me to when I'm not watching YouTube say, gosh, I really would like to see

59:13.520 --> 59:20.400
 something from someone or about something. And so that's why I think this connection between

59:20.400 --> 59:30.080
 fans and creators is so important for both because it's a way of fostering a relationship

59:30.080 --> 59:37.600
 that can play out into the future. Let me talk about kind of a dark and interesting question

59:37.600 --> 59:45.040
 in general. And again, a topic that you or nobody has an answer to, but social media has a sense of

59:46.800 --> 59:53.120
 you know, it gives us highs and it gives us lows in the sense that sort of creators often speak

59:53.120 --> 59:59.520
 about having sort of burn out and having psychological ups and downs and challenges

59:59.520 --> 1:00:04.320
 mentally in terms of continuing the creation process. There's a momentum. There's a huge

1:00:04.320 --> 1:00:10.160
 excited audience that makes everybody feel that makes creators feel great. And I think it's more

1:00:10.160 --> 1:00:16.560
 than just financial. I think it's literally just they love that sense of community. It's part of

1:00:16.560 --> 1:00:21.840
 the reason I upload to YouTube. I don't care about money. Never will. What I care about is the the

1:00:21.840 --> 1:00:27.520
 community, but some people feel like this momentum and even when there's times in their life when

1:00:27.520 --> 1:00:32.800
 they don't feel, you know, for some reason don't feel like creating. So how do you think about

1:00:32.800 --> 1:00:38.960
 burn out this mental exhaustion that some YouTube creators go through? Is that something we have

1:00:38.960 --> 1:00:42.960
 an answer for? Is it something? How do we even think about that? Well, the first thing is we

1:00:42.960 --> 1:00:48.800
 want to make sure that the YouTube systems are not contributing to this sense, right? And so

1:00:49.760 --> 1:00:57.760
 we've done a fair amount of research to demonstrate that you can absolutely take a break. If you are

1:00:57.760 --> 1:01:03.680
 if you are a creator and you've been uploading a lot, we have just as many examples of people who

1:01:03.680 --> 1:01:09.440
 took a break and came back more popular than they were before as we have examples of going the other

1:01:09.440 --> 1:01:14.640
 way. Yeah, can we pause on that for a second? So the feeling that people have, I think, is if I take

1:01:14.640 --> 1:01:22.080
 a break, everybody, the party will leave, right? So if you could just linger on that. So in your

1:01:22.080 --> 1:01:28.560
 sense that taking a break is okay. Yes, taking a break is absolutely okay. And the reason I say

1:01:28.560 --> 1:01:37.600
 that is because we have we can observe many examples of being of creators coming back very

1:01:37.600 --> 1:01:43.520
 strong and even stronger after they have taken some sort of break. And so I just want to dispel the

1:01:43.520 --> 1:01:53.120
 myth that this somehow necessarily means that your channel is going to go down or lose views.

1:01:53.120 --> 1:02:01.200
 That is not the case. We know for sure that this is not a necessary outcome. And so we want to

1:02:01.200 --> 1:02:06.160
 encourage people to make sure that they take care of themselves. That is job one, right? You have

1:02:06.160 --> 1:02:14.000
 to look after yourself and your mental health. And you know, I think that it probably in some of

1:02:14.000 --> 1:02:20.880
 these cases contributes to better videos once they come back, right? Because a lot of people,

1:02:21.600 --> 1:02:25.920
 I know myself, if I burn out on something, then I'm probably not doing my best work,

1:02:25.920 --> 1:02:33.360
 even though I can keep working until I pass out. And so I think that the taking a break

1:02:33.360 --> 1:02:38.160
 may even improve the creative ideas that someone has.

1:02:39.280 --> 1:02:44.880
 Okay, I think it's a really important thing to dispel. I think it applies to all of social media.

1:02:45.520 --> 1:02:52.720
 Like literally, I've taken a break for a day every once in a while. Sorry, sorry if that sounds

1:02:52.720 --> 1:02:59.520
 like a short time. But even like email, just taking a break from email or only checking email once a

1:02:59.520 --> 1:03:04.880
 day, especially when you're going through something psychologically in your personal life or so on,

1:03:04.880 --> 1:03:10.240
 or really not sleeping much because of work deadlines, it can refresh you in a way that's

1:03:10.240 --> 1:03:15.520
 profound. And so the same applies. It was there when you came back, right? It's there. And it looks

1:03:15.520 --> 1:03:20.560
 different actually when you come back. You're sort of brighter eyed with some coffee, everything,

1:03:20.560 --> 1:03:24.880
 the world looks better. So it's important to take a break when you need it.

1:03:24.880 --> 1:03:32.480
 So you've mentioned kind of the the YouTube algorithm isn't, you know, e equals mc squared.

1:03:32.480 --> 1:03:37.760
 It's not the single equation. It's potentially sort of more than a million lines of code.

1:03:40.560 --> 1:03:46.240
 Sort of is it more akin to what autonomous successful autonomous vehicles today are,

1:03:46.240 --> 1:03:53.520
 which is they're just basically patches on top of patches of heuristics and human experts really

1:03:53.520 --> 1:04:01.200
 tuning the algorithm and have some machine learning modules? Or is it becoming more and more a giant

1:04:01.200 --> 1:04:06.240
 machine learning system with humans just doing a little bit of tweaking here and there? What's

1:04:06.240 --> 1:04:11.120
 your sense? First off, do you even have a sense of what is the YouTube algorithm at this point?

1:04:11.120 --> 1:04:14.800
 And whichever, however much you do have a sense, what does it look like?

1:04:15.680 --> 1:04:22.000
 Well, we don't usually think about it as the algorithm because it's a bunch of systems that

1:04:22.000 --> 1:04:28.160
 work on different services. The other thing that I think people don't understand is that

1:04:28.880 --> 1:04:34.240
 what you might refer to as the YouTube algorithm from outside of YouTube is actually

1:04:35.280 --> 1:04:42.400
 a bunch of code and machine learning systems and heuristics, but that's married with the behavior

1:04:42.400 --> 1:04:46.480
 of all the people who come to YouTube every day. So the people part of the code essentially.

1:04:46.480 --> 1:04:50.960
 Exactly, right? Like if there were no people who came to YouTube tomorrow, then the algorithm

1:04:50.960 --> 1:04:56.320
 wouldn't work anymore, right? So that's a critical part of the algorithm. And so when people talk

1:04:56.320 --> 1:05:00.960
 about, well, the algorithm does this, the algorithm does that, it's sometimes hard to understand.

1:05:00.960 --> 1:05:07.360
 Well, you know, it could be the viewers are doing that and the algorithm is mostly just keeping track

1:05:07.360 --> 1:05:15.440
 of what the viewers do and then reacting to those things in sort of more fine grained situations.

1:05:15.440 --> 1:05:21.680
 And I think that this is the way that the recommendation system and the search system and

1:05:21.680 --> 1:05:28.400
 probably many machine learning systems evolve is you start trying to solve a problem and the

1:05:28.400 --> 1:05:35.280
 first way to solve a problem is often with a simple heuristic, right? And you want to say,

1:05:35.280 --> 1:05:38.880
 what are the videos we're going to recommend? Well, how about the most popular ones?

1:05:38.880 --> 1:05:47.120
 Right? And that's where you start. And over time, you collect some data and you refine

1:05:47.120 --> 1:05:52.320
 your situation so that you're making less heuristics and you're building a system that can

1:05:52.320 --> 1:05:58.240
 actually learn what to do in different situations based on some observations of those situations

1:05:58.240 --> 1:06:04.480
 in the past. And you keep chipping away at these heuristics over time. And so I think that

1:06:04.480 --> 1:06:11.360
 that just like with diversity, you know, I think the first diversity measure we took was, okay,

1:06:11.360 --> 1:06:16.880
 not more than three videos in a row from the same channel, right? It's a pretty simple heuristic

1:06:16.880 --> 1:06:23.040
 to encourage diversity, but it worked, right? Who needs to see four, five, six videos in a row

1:06:23.040 --> 1:06:29.840
 from the same channel? And over time, we try to chip away at that and make it more fine grained

1:06:29.840 --> 1:06:39.120
 and basically have it remove the heuristics in favor of something that can react to individuals

1:06:39.120 --> 1:06:45.600
 and individual situations. So how do you, you mentioned, you know, we know that something

1:06:45.600 --> 1:06:52.000
 worked. How do you get a sense when decisions are the kind of A B testing that this idea was a good

1:06:52.000 --> 1:06:59.200
 one, this was not so good? What's, how do you measure that? And across which time scale, across

1:06:59.200 --> 1:07:04.720
 how many users that kind of, that kind of thing? Well, you mentioned that A B experiments. And so

1:07:06.000 --> 1:07:12.960
 just about every single change we make to YouTube, we do it only after we've run a A B experiment.

1:07:13.520 --> 1:07:24.320
 And so in those experiments, which run from one week to months, we measure hundreds, literally

1:07:24.320 --> 1:07:30.720
 hundreds of different variables and, and measure changes with confidence intervals in all of them,

1:07:30.720 --> 1:07:37.520
 because we really are trying to get a sense for ultimately, does this improve the experience

1:07:37.520 --> 1:07:43.200
 for viewers? That's the question we're trying to answer. And an experiment is one way because we

1:07:43.200 --> 1:07:49.200
 can see certain things go up and down. So for instance, if we noticed in the experiment, people

1:07:49.200 --> 1:07:56.720
 are dismissing videos less frequently, or they're saying that they're more satisfied,

1:07:56.720 --> 1:08:01.360
 they're giving more videos five stars after they watch them, then those would be indications of

1:08:02.640 --> 1:08:06.320
 that the experiment is successful, that it's improving the situation for viewers.

1:08:07.920 --> 1:08:13.040
 But we can also look at other things, like we might do user studies where we invite some

1:08:13.040 --> 1:08:17.040
 people in and ask them, like, what do you think about this? What do you think about that? How do

1:08:17.040 --> 1:08:23.840
 you feel about this? And other various kinds of user research. But ultimately, before we launch

1:08:23.840 --> 1:08:28.640
 something, we're going to want to run an experiment. So we get a sense for what the impact is going to

1:08:28.640 --> 1:08:34.560
 be, not just to the viewers, but also to the different channels and all of that.

1:08:36.400 --> 1:08:40.720
 An absurd question. Nobody knows. Well, actually, it's interesting. Maybe there's an answer, but

1:08:40.720 --> 1:08:48.480
 if I want to make a viral video, how do I do it? I don't know how you make a viral video. I know

1:08:48.480 --> 1:08:56.160
 that we have, in the past, tried to figure out if we could detect when a video was going to go

1:08:56.160 --> 1:09:02.800
 viral. And those were, you take the first and second derivatives of the view count and maybe

1:09:02.800 --> 1:09:10.960
 use that to do some prediction. But I can't say we ever got very good at that. Oftentimes,

1:09:10.960 --> 1:09:17.040
 we look at where the traffic was coming from. If a lot of the viewership is coming from

1:09:17.040 --> 1:09:23.680
 something like Twitter, then maybe it has a higher chance of becoming viral than if it

1:09:23.680 --> 1:09:29.600
 were coming from search or something. But that was just trying to detect a video that might be

1:09:29.600 --> 1:09:35.040
 viral. How to make one? Like, I have no idea. I mean, you get your kids to interrupt you while

1:09:35.040 --> 1:09:41.280
 you're on the news or something. Absolutely. But after the fact, on one individual video,

1:09:42.080 --> 1:09:49.120
 sort of ahead of time predicting is a really hard task. But after the video went viral in analysis,

1:09:49.920 --> 1:09:55.440
 can you sometimes understand why it went viral from the perspective of YouTube broadly?

1:09:55.440 --> 1:10:02.000
 Firstly, is it even interesting for YouTube that a particular video is viral? Or does that not

1:10:02.000 --> 1:10:09.120
 matter for the experience of people? Well, I think people expect that if a video is going

1:10:09.120 --> 1:10:15.120
 viral and it's something they would be interested in, then I think they would expect YouTube to

1:10:15.120 --> 1:10:21.760
 recommend it to them. Right. So if someone's going viral, it's good to just let people ride the wave

1:10:21.760 --> 1:10:26.800
 of it. It's viral. Well, I mean, we want to meet people's expectations in that way, of course.

1:10:27.520 --> 1:10:32.560
 So like I mentioned, I hung out with Derek Mueller a while ago, a couple months back.

1:10:33.840 --> 1:10:38.080
 He's actually the person who suggested I talk to you on this podcast.

1:10:38.080 --> 1:10:43.280
 All right. Well, thank you, Derek. At that time, he just recently posted

1:10:43.280 --> 1:10:49.520
 an awesome science video titled, Why Are 96 Million Black Balls on This Reservoir?

1:10:49.520 --> 1:10:54.800
 And in a matter of, I don't know how long, but like a few days, you got 38 million views

1:10:54.800 --> 1:11:02.160
 and it's still growing. Is this something you can analyze and understand why it happened

1:11:02.160 --> 1:11:04.480
 this video and you want a particular video like it?

1:11:05.360 --> 1:11:12.160
 I mean, we can surely see where it was recommended, where it was found, who watched it,

1:11:12.160 --> 1:11:17.760
 and those sorts of things. So it's actually starting to interrupt. It is the video which

1:11:17.760 --> 1:11:23.760
 helped me discover who Derek is. I didn't know who he is before. So I remember, you know,

1:11:23.760 --> 1:11:29.520
 usually I just have all of these technical boring MIT Stanford talks in my recommendation

1:11:29.520 --> 1:11:34.160
 because that's how I watch. And then all of a sudden there's this Black Balls in Reservoir

1:11:34.160 --> 1:11:40.720
 video with like an excited nerd and with like just, why is this being recommended to me?

1:11:40.720 --> 1:11:44.560
 So I clicked on it and watched the whole thing. It was awesome. But, and then a lot of people

1:11:44.560 --> 1:11:49.200
 had that experience like, why was I recommended this? But they all, of course, watched it and

1:11:49.200 --> 1:11:55.040
 enjoyed it, which is, what's your sense of this just wave of recommendation that comes

1:11:55.040 --> 1:11:59.360
 with this viral video that ultimately people get enjoy after they click on it?

1:12:00.160 --> 1:12:04.640
 Well, I think it's the system, you know, basically doing what anybody who's recommending

1:12:04.640 --> 1:12:09.040
 something would do, which is you show it to some people and if they like it, you say,

1:12:09.040 --> 1:12:13.200
 okay, well, can I find some more people who are a little bit like them? Okay, I'm gonna

1:12:13.200 --> 1:12:17.040
 try it with them. Oh, they like it too. Let me expand the circle some more, find some more

1:12:17.040 --> 1:12:21.280
 people. Oh, it turns out they like it too. And you just keep going until you get some

1:12:21.280 --> 1:12:24.960
 feedback that says, no, now you've gone too far. These people don't like it anymore.

1:12:25.680 --> 1:12:32.240
 And so I think that's basically what happened. Now, you asked me about how to make a video go

1:12:32.240 --> 1:12:40.160
 viral or make a viral video. I don't think that if you or I decided to make a video about 96

1:12:40.160 --> 1:12:47.280
 million balls, that it would also go viral. It's possible that Derek made like the canonical video

1:12:47.280 --> 1:12:54.000
 about those black balls in the lake. Exactly. He did actually. Right. And so I don't know whether

1:12:54.000 --> 1:13:01.120
 or not just following along is the secret. Yeah, but it's fascinating. I mean, just like you said,

1:13:01.120 --> 1:13:05.520
 the algorithm sort of expanding that circle and then figuring out that more and more people did

1:13:05.520 --> 1:13:11.840
 enjoy it. And that sort of phase shift of just a huge number of people enjoying it in the algorithm

1:13:11.840 --> 1:13:18.000
 quickly, automatically, I assume, figuring that out. That's a, I don't know, the dynamics of

1:13:18.000 --> 1:13:24.080
 psychology, that is a beautiful thing. And so what do you think about the idea of clipping,

1:13:24.080 --> 1:13:30.000
 like too many people annoyed me into doing it, which is they were requesting it. They said it

1:13:30.000 --> 1:13:37.200
 would be very beneficial to add clips in like the coolest points and actually have explicit

1:13:37.200 --> 1:13:42.480
 videos. Like I'm reuploading a video, like a short clip, which is what the podcasts are doing.

1:13:44.160 --> 1:13:48.080
 Do you see, as opposed to like I also add timestamps for the topics, you know,

1:13:48.080 --> 1:13:53.920
 do you want the clip? Do you see YouTube somehow helping creators with that process or helping

1:13:53.920 --> 1:13:59.840
 connect clips to the original videos? Or is that just on a long list of amazing features to work

1:13:59.840 --> 1:14:06.080
 towards? Yeah, I mean, it's not something that I think we've done yet. But I can tell you that

1:14:07.840 --> 1:14:12.400
 I think clipping is great. And I think it's actually great for you as a creator.

1:14:12.400 --> 1:14:18.480
 And here's the reason. If you think about, I mean, let's, let's say the NBA is uploading

1:14:20.080 --> 1:14:27.920
 videos of, of its games. Well, people might search for warriors versus rockets,

1:14:27.920 --> 1:14:33.600
 or they might search for Steph Curry. And so a highlight from the game in which Steph Curry

1:14:33.600 --> 1:14:41.120
 makes an amazing shot is an opportunity for someone to find a portion of that video. And so I think

1:14:41.120 --> 1:14:48.400
 that you never know how people are going to search for something that you've created. And so you want

1:14:48.400 --> 1:14:54.480
 to, I would say you want to make clips and, and add titles and things like that so that they can

1:14:54.480 --> 1:15:00.800
 find it as easily as possible. Do you have a dream of a future, perhaps a distant future,

1:15:00.800 --> 1:15:09.360
 when the YouTube algorithm figures that out, sort of automatically detects the parts of the video

1:15:09.360 --> 1:15:14.480
 that are really interesting, exciting, potentially exciting for people, and sort of clip them out

1:15:14.480 --> 1:15:19.520
 in this incredibly rich space? Because if you talk about, if you talk, even just this conversation,

1:15:19.520 --> 1:15:26.880
 we probably covered 30, 40 little topics. And there's a huge space of users that would find,

1:15:27.680 --> 1:15:33.120
 you know, 30% of those topics really interesting. And that space is very different. It's something

1:15:33.120 --> 1:15:39.760
 that's beyond my ability to clip out, right? But the algorithm might be able to figure all that

1:15:39.760 --> 1:15:45.680
 out, sort of expand into clips. Do you have a, do you think about this kind of thing? Do you have

1:15:45.680 --> 1:15:50.160
 a hope, a dream that one day the algorithm will be able to do that kind of deep content analysis?

1:15:50.160 --> 1:15:57.200
 Well, we've actually had projects that attempt to achieve this. But it really does depend on

1:15:58.000 --> 1:16:03.360
 understanding the video well. And our understanding of the video right now is quite crude. And so

1:16:04.320 --> 1:16:11.680
 I think it would be especially hard to do it with a conversation like this. One might be able to do

1:16:11.680 --> 1:16:19.600
 it with, let's say, a soccer match more easily, right? You could probably find out where the goals

1:16:19.600 --> 1:16:25.600
 were scored. And then of course, you need to figure out who it was that scored the goal. And

1:16:25.600 --> 1:16:31.600
 that might require a human to do some annotation. But I think that trying to identify coherent

1:16:31.600 --> 1:16:40.720
 topics in a transcript, like the one of our conversation, is not something that we're going

1:16:40.720 --> 1:16:45.760
 to be very good at right away. And I was speaking more to the general problem, actually, of being

1:16:45.760 --> 1:16:52.320
 able to do both a soccer match and our conversation without explicit sort of almost, my hope was that

1:16:52.320 --> 1:17:03.440
 there exists an algorithm that's able to find exciting things in video. So Google now on Google

1:17:03.440 --> 1:17:08.960
 search will help you find the segment of the video that you're interested in. So if you search for

1:17:08.960 --> 1:17:14.720
 or something like how to change the filter in my dishwasher, then if there's a long video about

1:17:14.720 --> 1:17:18.880
 your dishwasher, and this is the part where the person shows you how to change the filter, then

1:17:19.760 --> 1:17:26.880
 it will highlight that area and provide a link directly to it. And from your recollection,

1:17:26.880 --> 1:17:31.440
 do you know if the thumbnail reflects? What's the difference between showing the full video and

1:17:31.440 --> 1:17:35.680
 the shorter clip? Do you know how it's presented in search results? I don't remember how it's

1:17:35.680 --> 1:17:42.000
 presented. And the other thing I would say is that right now it's based on creator annotations.

1:17:42.000 --> 1:17:47.280
 Ah, got it. So it's not the thing we're talking about. But folks are working on the more

1:17:48.080 --> 1:17:54.800
 automatic version. It's interesting, people might not imagine this, but a lot of our systems start

1:17:55.520 --> 1:18:01.920
 by using almost entirely the audience behavior. And then as they get better,

1:18:01.920 --> 1:18:10.480
 the refinement comes from using the content. And I wish, I know there's privacy concerns, but

1:18:11.200 --> 1:18:18.400
 I wish YouTube explored the space, which is sort of putting a camera on the users if they allowed

1:18:18.400 --> 1:18:24.720
 it, right? To study their, like I did a lot of emotion recognition work and so on, to study

1:18:24.720 --> 1:18:31.600
 actual sort of richer signal. One of the cool things when you upload 360 like VR video to YouTube,

1:18:31.600 --> 1:18:38.080
 and I've done this a few times. So I've uploaded myself. It's a horrible idea. Some people enjoyed

1:18:38.080 --> 1:18:44.160
 it, but whatever. The video of me giving a lecture in 360, 360 camera, and it's cool because YouTube

1:18:44.160 --> 1:18:50.160
 allows you to then watch, where did people look at? There's a heat map of where, you know,

1:18:50.160 --> 1:18:55.360
 of where the center of the VR experience was. And it's interesting because that reveals to you,

1:18:55.360 --> 1:19:00.480
 like, what people looked at. And it's, it's not always what you were expecting. It's not,

1:19:00.480 --> 1:19:05.600
 in the case of the lecture is pretty boring. It is what we're expecting, but we did a few funny

1:19:05.600 --> 1:19:10.000
 videos where there's a bunch of people doing things and they, everybody tracks those people.

1:19:10.000 --> 1:19:14.000
 You know, in the beginning, they all look at the main person and they start spreading around and

1:19:14.000 --> 1:19:18.000
 looking at the other people. It's fascinating. So that kind of, that's a really strong signal

1:19:18.640 --> 1:19:23.760
 of what people found exciting in the video. I don't know how you get that from people just

1:19:23.760 --> 1:19:31.440
 watching, except they tuned out at this point. Like, it's hard to measure this moment was super

1:19:31.440 --> 1:19:35.520
 exciting for people. I don't know how you get that signal. Maybe comment, is there a way to get

1:19:35.520 --> 1:19:40.720
 that signal where this was like, this is when their eyes opened up and they're like, like,

1:19:40.720 --> 1:19:44.560
 for me with the Ray Dalio video, right? Like, at first I was like, oh, okay, this is another one

1:19:44.560 --> 1:19:50.000
 of these like dumb it down for you videos. And then you like start watching, it's like, okay,

1:19:50.000 --> 1:19:55.120
 there's really crisp, clean, deep explanation of how the economy works. That's where I like set up

1:19:55.120 --> 1:20:00.320
 and started watching right that moment. Is there a way to detect that moment? The only way I can

1:20:00.320 --> 1:20:07.360
 think of is by asking people to label it. You mentioned that we're quite far away in terms

1:20:07.360 --> 1:20:15.920
 of doing video analysis, deep video analysis. Of course, Google, YouTube, you know, we're

1:20:15.920 --> 1:20:21.040
 quite far away from solving a time was driving problem too. I don't know. I think we're closer

1:20:21.040 --> 1:20:28.160
 to that. You never know. And the Wright brothers thought they're never they're not going to fly

1:20:28.160 --> 1:20:34.720
 for 50 years, three years before they flew. So what are the biggest challenges, would you say?

1:20:34.720 --> 1:20:40.960
 Is it the broad challenge of understanding video, understanding natural language, understand the

1:20:40.960 --> 1:20:45.040
 challenge before the entire machine learning community or just being able to understand

1:20:45.040 --> 1:20:50.080
 it? Or is there something specific to video that's even more challenging than understanding

1:20:50.960 --> 1:20:54.160
 natural language understanding? What's your sense of what the biggest challenge is?

1:20:54.160 --> 1:21:01.280
 I mean, video is just so much information. And so precision becomes a real problem. It's like

1:21:02.720 --> 1:21:11.200
 you're trying to classify something and you've got a million classes. And the distinctions

1:21:11.200 --> 1:21:23.200
 among them, at least from a machine learning perspective, are often pretty small. You need

1:21:23.200 --> 1:21:29.520
 to see this person's number in order to know which player it is. And there's a lot of players.

1:21:30.800 --> 1:21:38.320
 Or you need to see the logo on their chest in order to know which team they play for.

1:21:38.320 --> 1:21:43.280
 And so, and that's just figuring out who's who, right? And then you go further and saying, okay,

1:21:43.280 --> 1:21:48.080
 well, you know, was that a goal? Was it not a goal? Like, is that an interesting moment,

1:21:48.080 --> 1:21:52.800
 as you said? Or is that not an interesting moment? These things can be pretty hard.

1:21:52.800 --> 1:21:59.040
 So, okay, so Yan Likun, I'm not sure if you're familiar sort of with this current thinking

1:21:59.040 --> 1:22:04.800
 and work. So he believes that self, what is referring to as self supervised learning

1:22:04.800 --> 1:22:10.160
 will be the solution sort of to achieving this kind of greater level of intelligence. In fact,

1:22:10.160 --> 1:22:15.360
 the thing he's focusing on is watching video and predicting the next frame. So predicting

1:22:15.360 --> 1:22:22.000
 the future of video, right? So for now, we're very far from that. But his thought is because

1:22:22.000 --> 1:22:28.640
 it's unsupervised, or as he refers to as self supervised. You know, if you watch enough video,

1:22:28.640 --> 1:22:34.000
 essentially, if you watch YouTube, you'll be able to learn about the nature of reality,

1:22:34.000 --> 1:22:39.120
 the physics, the common sense reasoning required by just teaching a system to predict

1:22:39.120 --> 1:22:44.720
 the next frame. So he's confident this is the way to go. So for you, from the perspective of just

1:22:45.360 --> 1:22:52.000
 working with this video, how do you think an algorithm that just watches all of YouTube

1:22:52.960 --> 1:22:59.280
 stays up all day and night watching YouTube would be able to understand enough of the

1:22:59.280 --> 1:23:03.680
 physics of the world about the way this world works, be able to do common sense reasoning and

1:23:03.680 --> 1:23:10.080
 so on? Well, I mean, we have systems that already watch all the videos on YouTube, right?

1:23:10.640 --> 1:23:15.920
 But they're just looking for very specific things, right? They're supervised learning systems that

1:23:15.920 --> 1:23:23.920
 are trying to identify something or classify something. And I don't know if predicting the

1:23:23.920 --> 1:23:30.880
 next frame is really going to get there because I'm not an expert on compression algorithms,

1:23:30.880 --> 1:23:34.640
 but I understand that that's kind of what compression, video compression algorithms do,

1:23:34.640 --> 1:23:40.560
 is they basically try to predict the next frame and then fix up the places where they got it

1:23:40.560 --> 1:23:46.400
 wrong. And that leads to higher compression than if you actually put all the bits for the next frame

1:23:46.400 --> 1:23:53.280
 there. So I don't know if I believe that just being able to predict the next frame is going to be

1:23:53.280 --> 1:24:00.480
 enough because there's so many frames and even a tiny bit of error on a per frame basis can lead

1:24:00.480 --> 1:24:07.760
 to wildly different videos. So the thing is the idea of compression is one way to do compression

1:24:07.760 --> 1:24:11.520
 is to describe through text what's contained in the video. That's the ultimate high level of

1:24:11.520 --> 1:24:16.880
 compression. So the idea is a tradition when you think of video image compression, you're trying

1:24:16.880 --> 1:24:24.400
 to maintain the same visual quality while reducing the size. But if you think of deep learning from

1:24:24.400 --> 1:24:30.000
 a bigger perspective of what compression is, is you're trying to summarize the video. And the idea

1:24:30.000 --> 1:24:36.080
 there is if you have a big enough neural network by watching the next, by trying to predict the

1:24:36.080 --> 1:24:41.600
 next frame, you'll be able to form a compression of actually understanding what's going on in the

1:24:41.600 --> 1:24:47.600
 scene. If there's two people talking, you can just reduce that entire video into the fact that two

1:24:47.600 --> 1:24:53.840
 people are talking and maybe the content of what they're saying and so on. That's kind of the open

1:24:53.840 --> 1:25:00.800
 ended dream. So I just wanted to sort of express that because it's an interesting compelling notion, but

1:25:00.800 --> 1:25:07.760
 it is nevertheless true that video, our world is a lot more complicated than we get credit for.

1:25:07.760 --> 1:25:13.280
 I mean, in terms of search and discovery, we have been working on trying to summarize videos

1:25:13.280 --> 1:25:24.080
 in text or with some kind of labels for eight years, at least. And we're kind of so so.

1:25:24.080 --> 1:25:32.160
 So if you were to say the problem is 100% solved and eight years ago, was 0% solved?

1:25:34.640 --> 1:25:36.960
 Where are we on that timeline, would you say?

1:25:36.960 --> 1:25:44.240
 Yeah, to summarize a video well, maybe less than a quarter of the way.

1:25:44.240 --> 1:25:50.000
 So on that topic, what does YouTube look like 10, 20, 30 years from now?

1:25:51.040 --> 1:25:59.840
 I mean, I think that YouTube is evolving to take the place of TV. I grew up as a kid in the 70s

1:25:59.840 --> 1:26:06.640
 and I watched a tremendous amount of television. And I feel sorry for my poor mom because

1:26:07.600 --> 1:26:11.840
 people told her at the time that it was going to rot my brain and that she should kill her

1:26:11.840 --> 1:26:18.080
 television. But anyway, I mean, I think that YouTube is, at least for my family,

1:26:19.680 --> 1:26:26.240
 a better version of television, right? It's one that is on demand. It's more tailored to the things

1:26:26.240 --> 1:26:33.120
 that my kids want to watch. And also, they can find things that they would never have found on

1:26:33.120 --> 1:26:39.840
 television. And so I think that at least from just observing my own family, that's where we're

1:26:39.840 --> 1:26:45.200
 headed is that people watch YouTube kind of in the same way that I watched television when I was

1:26:45.200 --> 1:26:53.200
 younger. So from a search and discovery perspective, what are you excited about in the 5, 10, 20,

1:26:53.200 --> 1:27:00.640
 30 years? It's already really good. I think it's achieved a lot of, of course, we don't know what's

1:27:00.640 --> 1:27:08.080
 possible. So it's the task of search of typing in the text or discovering new videos by the next

1:27:08.080 --> 1:27:14.160
 recommendation. I personally am really happy with the experience. Continuously, I rarely watch a video

1:27:14.160 --> 1:27:21.200
 that's not awesome from my own perspective. But what else is possible? What are you excited about?

1:27:21.200 --> 1:27:29.200
 Well, I think introducing people to more of what's available on YouTube is not only very important

1:27:29.200 --> 1:27:35.600
 to YouTube and to creators, but I think it will help enrich people's lives. Because there's a lot

1:27:35.600 --> 1:27:40.960
 that I'm still finding out is available on YouTube that I didn't even know. I've been working

1:27:40.960 --> 1:27:48.400
 YouTube eight years, and it wasn't until last year that I learned that, that I could watch

1:27:48.400 --> 1:27:54.000
 USC football games from the 1970s. Like, I didn't even know that was possible until last year. And

1:27:54.000 --> 1:27:58.480
 I've been working here quite some time. So, you know, what was broken about, about that, that it

1:27:58.480 --> 1:28:04.080
 took me seven years to learn that this stuff was already on YouTube, even when I got here. So I

1:28:04.080 --> 1:28:10.480
 think there's a big opportunity there. And then, as I said before, you know, we want to make sure

1:28:10.480 --> 1:28:21.520
 that YouTube finds a way to ensure that it's acting responsibly with respect to society and

1:28:21.520 --> 1:28:26.480
 enriching people's lives. So we want to take all of the great things that it does and make sure

1:28:26.480 --> 1:28:33.920
 that we are eliminating the negative consequences that might happen. And then lastly, if we could

1:28:33.920 --> 1:28:38.960
 get to a point where all the videos people watch are the best ones they've ever watched, that would

1:28:38.960 --> 1:28:44.480
 be outstanding too. Do you see, in many senses, becoming a window into the world for people?

1:28:45.360 --> 1:28:51.680
 It's, especially with live video, you get to watch events. I mean, it's really, it's the way you

1:28:51.680 --> 1:28:57.840
 experience a lot of the world that's out there is better than TV in many, many ways. So do you see

1:28:57.840 --> 1:29:05.360
 becoming more than just video? Do you see creators creating visual experiences and virtual worlds?

1:29:05.360 --> 1:29:09.520
 So if I'm talking crazy now, but sort of virtual reality and entering that space,

1:29:09.520 --> 1:29:13.920
 or is that, at least for now, totally outside what YouTube is thinking about?

1:29:13.920 --> 1:29:20.000
 I mean, I think Google is thinking about virtual reality. I don't think about virtual reality too

1:29:20.000 --> 1:29:28.880
 much. I know that we would want to make sure that YouTube is there when virtual reality becomes

1:29:28.880 --> 1:29:34.720
 something or if virtual reality becomes something that a lot of people are interested in. But I

1:29:34.720 --> 1:29:42.320
 haven't seen it really take off yet. Take off. Well, the future is wide open. Christos, I've been

1:29:42.320 --> 1:29:46.000
 really looking forward to this conversation. It's been a huge honor. Thank you for answering some

1:29:46.000 --> 1:29:51.680
 of the more difficult questions I've asked. I'm really excited about what YouTube has in store

1:29:51.680 --> 1:29:55.840
 for us. It's one of the greatest products I've ever used and continues. So thank you so much for

1:29:55.840 --> 1:30:01.200
 talking to it. It's my pleasure. Thanks for asking me. Thanks for listening to this conversation.

1:30:01.200 --> 1:30:07.040
 And thank you to our presenting sponsor, Cash App. Download it, use code LEX Podcast.

1:30:07.040 --> 1:30:12.880
 You'll get $10 and $10 will go to FIRST, a STEM education nonprofit that inspires hundreds of

1:30:12.880 --> 1:30:18.640
 thousands of young minds to become future leaders and innovators. If you enjoy this podcast,

1:30:18.640 --> 1:30:23.360
 subscribe on YouTube, give it five stars on Apple Podcasts, follow on Spotify,

1:30:23.360 --> 1:30:26.640
 support on Patreon, or simply connect with me on Twitter.

1:30:26.640 --> 1:30:32.800
 And now, let me leave you with some words of wisdom from Marcel Proust. The real

1:30:32.800 --> 1:30:38.560
 voyage of discovery consists not in seeking new landscapes, but in having new eyes.

1:30:38.560 --> 1:30:56.240
 Thank you for listening, and I hope to see you next time.

