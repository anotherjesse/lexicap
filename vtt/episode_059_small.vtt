WEBVTT

00:00.000 --> 00:03.520
 The following is a conversation with Sebastian Thrun.

00:03.520 --> 00:06.200
 He's one of the greatest roboticists, computer scientists,

00:06.200 --> 00:08.160
 and educators of our time.

00:08.160 --> 00:10.600
 He led the development of the autonomous vehicles

00:10.600 --> 00:14.360
 at Stanford that won the 2005 DARPA Grand Challenge

00:14.360 --> 00:18.200
 and placed second in the 2007 DARPA Urban Challenge.

00:18.200 --> 00:21.280
 He then led the Google Self Driving Car Program, which

00:21.280 --> 00:24.680
 launched the Self Driving Car Revolution.

00:24.680 --> 00:26.520
 He taught the popular Stanford course

00:26.520 --> 00:29.080
 on artificial intelligence in 2011,

00:29.080 --> 00:32.440
 which was one of the first massive open online courses,

00:32.440 --> 00:35.080
 or MOOCs, as they're commonly called.

00:35.080 --> 00:37.600
 That experience led him to cofound Udacity,

00:37.600 --> 00:39.800
 an online education platform.

00:39.800 --> 00:41.640
 If you haven't taken courses on it yet,

00:41.640 --> 00:43.320
 I highly recommend it.

00:43.320 --> 00:47.160
 Their Self Driving Car Program, for example, is excellent.

00:47.160 --> 00:50.720
 He's also the CEO of Kitty Hawk, a company working

00:50.720 --> 00:54.000
 on building flying cars, or more technically,

00:54.000 --> 00:56.920
 EV TALLS, which stands for Electric Vertical Takeoff

00:56.920 --> 00:58.680
 and Landing Aircraft.

00:58.680 --> 01:00.600
 He has launched several revolutions

01:00.600 --> 01:02.720
 and inspired millions of people.

01:02.720 --> 01:06.840
 But also, as many know, he's just a really nice guy.

01:06.840 --> 01:10.560
 It was an honor and a pleasure to talk with him.

01:10.560 --> 01:12.800
 This is the Artificial Intelligence Podcast.

01:12.800 --> 01:15.920
 If you enjoy it, subscribe on YouTube, give it five stars

01:15.920 --> 01:18.520
 on Apple Podcasts, follow it on Spotify,

01:18.520 --> 01:21.880
 support it on Patreon, or simply connect with me on Twitter.

01:21.880 --> 01:25.840
 And Lex Friedman, spelled F R I D M A N.

01:25.840 --> 01:28.280
 If you leave a review on Apple Podcasts or YouTube

01:28.280 --> 01:30.600
 or Twitter, consider mentioning ideas,

01:30.600 --> 01:32.880
 people, topics you find interesting.

01:32.880 --> 01:35.840
 It helps guide the future of this podcast.

01:35.840 --> 01:38.840
 But in general, I just love comments with kindness

01:38.840 --> 01:40.160
 and thoughtfulness in them.

01:40.160 --> 01:43.640
 This podcast is a side project for me, as many people know,

01:43.640 --> 01:45.880
 but I still put a lot of effort into it.

01:45.880 --> 01:47.720
 So the positive words of support

01:47.720 --> 01:52.200
 from an amazing community, from you, really help.

01:52.200 --> 01:55.240
 I recently started doing ads at the end of the introduction.

01:55.240 --> 01:58.120
 I'll do one or two minutes after introducing the episode,

01:58.120 --> 01:59.640
 and never any ads in the middle

01:59.640 --> 02:01.800
 that can break the flow of the conversation.

02:01.800 --> 02:03.120
 I hope that works for you

02:03.120 --> 02:05.400
 and doesn't hurt the listening experience.

02:05.400 --> 02:08.000
 I provide timestamps for the start of the conversation

02:08.000 --> 02:11.440
 that you can skip to, but it helps if you listen to the ad

02:11.440 --> 02:13.840
 and support this podcast by trying out the product

02:13.840 --> 02:15.440
 or service being advertised.

02:16.480 --> 02:18.760
 This show is presented by Cash App,

02:18.760 --> 02:21.440
 the number one finance app in the App Store.

02:21.440 --> 02:24.040
 I personally use Cash App to send money to friends,

02:24.040 --> 02:25.880
 but you can also use it to buy, sell,

02:25.880 --> 02:28.200
 and deposit Bitcoin in just seconds.

02:28.200 --> 02:31.040
 Cash App also has a new investing feature.

02:31.040 --> 02:34.440
 You can buy fractions of a stock, say $1 worth,

02:34.440 --> 02:36.560
 no matter what the stock price is.

02:36.560 --> 02:39.520
 Brokerage services are provided by Cash App Investing,

02:39.520 --> 02:42.920
 a subsidiary of Square and member SIPC.

02:42.920 --> 02:44.680
 I'm excited to be working with Cash App

02:44.680 --> 02:47.880
 to support one of my favorite organizations called First,

02:47.880 --> 02:51.280
 best known for their first robotics and Lego competitions.

02:51.280 --> 02:54.640
 They educate and inspire hundreds of thousands of students

02:54.640 --> 02:56.480
 in over 110 countries

02:56.480 --> 02:59.000
 and have a perfect rating on charity navigator,

02:59.000 --> 03:00.680
 which means the donated money

03:00.680 --> 03:03.080
 is used to maximum effectiveness.

03:03.080 --> 03:06.040
 When you get Cash App from the App Store or Google Play

03:06.040 --> 03:09.280
 and use code LEGS Podcast, you'll get $10,

03:09.280 --> 03:12.080
 and Cash App will also donate $10 the first,

03:12.080 --> 03:13.920
 which again is an organization

03:13.920 --> 03:16.640
 that I've personally seen inspire girls and boys

03:16.640 --> 03:19.720
 to dream of engineering a better world.

03:19.720 --> 03:24.880
 And now here's my conversation with Sebastian Thrun.

03:24.880 --> 03:28.960
 You mentioned that The Matrix may be your favorite movie.

03:28.960 --> 03:32.160
 So let's start with a crazy philosophical question.

03:32.160 --> 03:34.800
 Do you think we're living in a simulation

03:34.800 --> 03:40.000
 and in general, do you find the thought experiment interesting?

03:40.000 --> 03:42.240
 Define simulation, I would say.

03:42.240 --> 03:43.720
 Maybe we are, maybe we are not,

03:43.720 --> 03:47.160
 but it's completely irrelevant to the way we should act.

03:47.160 --> 03:50.960
 Putting aside for a moment the fact

03:50.960 --> 03:52.320
 that it might not have any impact

03:52.320 --> 03:54.920
 on how we should act as human beings,

03:54.920 --> 03:57.320
 for people studying theoretical physics,

03:57.320 --> 03:59.600
 these kinds of questions might be kind of interesting,

03:59.600 --> 04:03.800
 looking at the universe's information processing system.

04:03.800 --> 04:05.960
 The universe is an information processing system.

04:05.960 --> 04:07.720
 It is. It's a huge physical, biological,

04:07.720 --> 04:11.000
 chemical computer. There's no question.

04:11.000 --> 04:12.920
 But I live here and now.

04:12.920 --> 04:15.640
 I care about people. I care about us.

04:15.640 --> 04:17.680
 What do you think is trying to compute?

04:17.680 --> 04:18.840
 I don't think there's an intention.

04:18.840 --> 04:22.080
 I think the world evolves the way it evolves.

04:22.080 --> 04:25.400
 And it's beautiful. It's unpredictable.

04:25.400 --> 04:28.040
 And I'm really, really grateful to be alive.

04:28.040 --> 04:30.480
 I've spoken like a true human.

04:30.480 --> 04:33.400
 Which last time I checked I was.

04:33.400 --> 04:35.280
 Well, in fact, this whole conversation

04:35.280 --> 04:40.280
 is just a touring test to see if indeed you are.

04:40.280 --> 04:42.760
 You've also said that one of the first programs

04:42.760 --> 04:45.120
 or the first few programs you've written

04:45.120 --> 04:50.040
 was a wait for a TI 57 calculator.

04:50.040 --> 04:52.040
 Maybe that's early 80s.

04:52.040 --> 04:54.280
 I don't want to date calculators or anything.

04:54.280 --> 04:55.640
 That's early 80s, correct.

04:55.640 --> 04:56.480
 Yeah.

04:56.480 --> 04:59.800
 So if you were to place yourself back into that time,

04:59.800 --> 05:02.160
 into the mindset you were in,

05:02.160 --> 05:05.600
 because you have predicted the evolution of computing,

05:05.600 --> 05:10.760
 AI, the internet, technology in the decades that followed.

05:10.760 --> 05:13.160
 I was super fascinated by Silicon Valley,

05:13.160 --> 05:15.680
 which I'd seen on television once and thought, my God,

05:15.680 --> 05:16.520
 this is so cool.

05:16.520 --> 05:19.600
 They built like DRAMs there and CPUs.

05:19.600 --> 05:20.440
 How cool is that?

05:20.440 --> 05:23.960
 And as a college student, a few years later,

05:23.960 --> 05:26.920
 I decided to study intelligence and study human beings

05:26.920 --> 05:30.520
 and found that even back then in the 80s and 90s,

05:30.520 --> 05:33.320
 artificial intelligence is what fascinated me the most.

05:33.320 --> 05:35.840
 What's missing is that back in the day,

05:35.840 --> 05:37.680
 the computers are really small.

05:37.680 --> 05:39.120
 They're like the brains you could build

05:39.120 --> 05:41.600
 were not anywhere bigger as a cockroach.

05:41.600 --> 05:43.760
 And cockroaches aren't very smart.

05:43.760 --> 05:46.320
 So we weren't at the scale yet where we are today.

05:46.320 --> 05:49.240
 Did you dream at that time to achieve

05:49.240 --> 05:51.080
 the kind of scale we have today?

05:51.080 --> 05:52.680
 Did that seem possible?

05:52.680 --> 05:54.320
 I always wanted to make robots smart.

05:54.320 --> 05:57.960
 I felt it was super cool to build an artificial human.

05:57.960 --> 05:59.680
 And the best way to build an artificial human

05:59.680 --> 06:00.680
 would be to build a robot,

06:00.680 --> 06:03.080
 because that's kind of the closest we could do.

06:03.080 --> 06:04.960
 Unfortunately, we aren't there yet.

06:04.960 --> 06:07.200
 The robots today are still very brittle,

06:07.200 --> 06:09.440
 but it's fascinating to study intelligence

06:09.440 --> 06:12.880
 from a constructive perspective where you build something.

06:12.880 --> 06:15.120
 To understand you build,

06:15.120 --> 06:19.480
 what do you think it takes to build an intelligent system

06:19.480 --> 06:20.880
 and an intelligent robot?

06:20.880 --> 06:22.680
 I think the biggest innovation that we've seen

06:22.680 --> 06:23.760
 is machine learning.

06:23.760 --> 06:26.120
 And it's the idea that their computers

06:26.120 --> 06:27.600
 can basically teach themselves.

06:28.640 --> 06:29.720
 Let's give an example.

06:29.720 --> 06:33.080
 I'd say everybody pretty much knows how to walk.

06:33.080 --> 06:35.160
 And we learn how to walk in the first year,

06:35.160 --> 06:36.840
 two of our lives.

06:36.840 --> 06:38.800
 But no scientist has ever been able

06:38.800 --> 06:41.120
 to write on the rules of human gait.

06:41.120 --> 06:42.840
 We don't understand it.

06:42.840 --> 06:45.160
 We have it in our brains some, or we can practice it.

06:45.160 --> 06:47.760
 We understand it, but we can't articulate it.

06:47.760 --> 06:50.280
 We can't pass it on by language.

06:50.280 --> 06:52.080
 And that to me is kind of the deficiency

06:52.080 --> 06:53.360
 of today's computer programming.

06:53.360 --> 06:55.240
 Even you program a computer,

06:55.240 --> 06:56.720
 they're so insanely dumb

06:56.720 --> 06:59.840
 that you have to give them rules for every contingencies.

06:59.840 --> 07:01.720
 Very unlike the way people learn,

07:01.720 --> 07:03.440
 but learn from data and experience,

07:03.440 --> 07:05.480
 computers are being instructed.

07:05.480 --> 07:07.840
 And because it's so hard to get this instruction set right,

07:07.840 --> 07:11.520
 we pay software engineers $200,000 a year.

07:11.520 --> 07:13.160
 Now, the most recent innovation,

07:13.160 --> 07:15.920
 which has been in the make for like 30, 40 years,

07:15.920 --> 07:18.520
 is an idea that computers can find their own rules.

07:18.520 --> 07:20.680
 So they can learn from falling down and getting up

07:20.680 --> 07:22.000
 the same way children can learn

07:22.000 --> 07:23.840
 from falling down and getting up.

07:23.840 --> 07:26.240
 And that revolution has led to a capability

07:26.240 --> 07:28.760
 that's completely unmatched.

07:28.760 --> 07:31.800
 Today's computers can watch experts do their jobs,

07:31.800 --> 07:33.920
 whether you're a doctor or a lawyer,

07:33.920 --> 07:36.960
 pick up the regularities, learn those rules,

07:36.960 --> 07:39.440
 and then become as good as the best experts.

07:39.440 --> 07:42.720
 So the dream of in the 80s of expert systems, for example,

07:42.720 --> 07:46.600
 had at its core the idea that humans could boil down

07:46.600 --> 07:49.400
 their expertise on a sheet of paper.

07:49.400 --> 07:50.800
 So to sort of reduce,

07:50.800 --> 07:53.280
 sort of be able to explain to machines

07:53.280 --> 07:55.520
 how to do something explicitly.

07:55.520 --> 07:57.320
 So do you think,

07:57.320 --> 08:00.120
 what's the use of human expertise into this whole picture?

08:00.120 --> 08:01.800
 Do you think most of the intelligence

08:01.800 --> 08:04.320
 will come from machines learning from experience

08:04.320 --> 08:06.520
 without human expertise input?

08:06.520 --> 08:08.400
 So the question for me is much more,

08:08.400 --> 08:10.720
 how do you express expertise?

08:10.720 --> 08:13.000
 You can express expertise by writing a book.

08:13.000 --> 08:15.160
 You can express expertise by showing someone

08:15.160 --> 08:16.280
 what you're doing.

08:16.280 --> 08:18.400
 You can express expertise by applying it

08:18.400 --> 08:20.040
 by many different ways.

08:20.040 --> 08:23.760
 And I think the expert systems was our best attempt in AI

08:23.760 --> 08:26.040
 to capture expertise and rules.

08:26.040 --> 08:27.160
 But someone sat down and say,

08:27.160 --> 08:28.640
 here are the rules of human gate.

08:28.640 --> 08:31.360
 Here's when you put your big toe forward

08:31.360 --> 08:34.720
 and your heel backwards and here how it stops stumbling.

08:34.720 --> 08:37.120
 And as we now know, the set of rules,

08:37.120 --> 08:39.440
 the set of language that we can command

08:39.440 --> 08:41.200
 is incredibly limited.

08:41.200 --> 08:43.760
 The majority of the human brain doesn't deal with language.

08:43.760 --> 08:47.240
 It deals with like subconscious numerical,

08:47.240 --> 08:50.120
 perceptual things that we don't even have self aware of.

08:51.320 --> 08:55.760
 Now, when a AI system watches an expert do their job

08:55.760 --> 08:57.880
 and practice their job,

08:57.880 --> 09:01.640
 it can pick up things that people can't even put into writing

09:01.640 --> 09:03.040
 into books or rules.

09:03.040 --> 09:04.520
 And that's what the real power is.

09:04.520 --> 09:07.160
 We now have AI systems that, for example,

09:07.160 --> 09:10.560
 look over the shoulders of highly paid human doctors

09:10.560 --> 09:12.840
 like dermatologists or radiologists

09:12.840 --> 09:15.320
 and they can somehow pick up those skills

09:15.320 --> 09:17.120
 that no one can express in words.

09:18.440 --> 09:22.160
 So you were a key person in launching three revolutions,

09:22.160 --> 09:25.160
 online education, autonomous vehicles

09:25.160 --> 09:28.200
 and fine cars or vtals.

09:28.200 --> 09:32.600
 So high level and I apologize

09:32.600 --> 09:34.680
 for all the philosophical questions.

09:34.680 --> 09:36.920
 That's no apology necessary.

09:36.920 --> 09:40.680
 How do you choose what problems to try and solve?

09:40.680 --> 09:43.400
 What drives you to make those solutions a reality?

09:43.400 --> 09:44.880
 I have two desires in life.

09:44.880 --> 09:48.520
 I want to literally make the lives of others better

09:48.520 --> 09:50.520
 or as we often say,

09:50.520 --> 09:53.000
 maybe jokingly make the world a better place.

09:53.000 --> 09:55.040
 I actually believe in this.

09:55.040 --> 09:56.440
 It's as funny as it sounds.

09:57.720 --> 09:59.160
 And second, I want to learn.

09:59.160 --> 10:00.480
 I want to get in the skills.

10:00.480 --> 10:01.880
 I don't want to be in the drop I'm good at

10:01.880 --> 10:04.320
 because if I'm in a job that I'm good at,

10:04.320 --> 10:05.880
 the chance for me to learn something interesting

10:05.880 --> 10:06.720
 is actually minimized.

10:06.720 --> 10:09.400
 So when I be in a job, I'm bad at.

10:09.400 --> 10:10.240
 That's really important to me.

10:10.240 --> 10:11.600
 So in a build, for example,

10:11.600 --> 10:13.040
 what people often call flying cars,

10:13.040 --> 10:16.560
 these are electrical, vertical takeoff and landing vehicles.

10:17.960 --> 10:19.720
 I'm just no expert in any of this.

10:19.720 --> 10:22.280
 And it's so much fun to learn on the job

10:22.280 --> 10:24.920
 what it actually means to build something like this.

10:24.920 --> 10:27.560
 Now I'd say the stuff that I done lately

10:27.560 --> 10:31.120
 after I finished my professorship at Stanford,

10:31.120 --> 10:32.560
 they really focused on

10:32.560 --> 10:34.880
 like what has the maximum impact on society.

10:34.880 --> 10:37.560
 Like transportation is something that has transformed

10:37.560 --> 10:38.960
 the 21st or 20th century

10:38.960 --> 10:40.520
 more than any other invention I'm happy in

10:40.520 --> 10:42.560
 and even more than communication.

10:42.560 --> 10:45.800
 And cities are different workers, different women's rights

10:45.800 --> 10:47.880
 are different because of transportation.

10:47.880 --> 10:51.360
 And yet we still have a very suboptimal transportation

10:51.360 --> 10:56.080
 solution where we kill 1.2 or so million people

10:56.080 --> 10:57.680
 every year in traffic.

10:57.680 --> 10:58.920
 It's like the leading cause of death

10:58.920 --> 11:01.160
 for young people in many countries

11:01.160 --> 11:03.640
 where we are extremely inefficient resource wise,

11:03.640 --> 11:06.800
 just go to your other neighborhood city

11:06.800 --> 11:08.320
 and look at the number of parked cars

11:08.320 --> 11:10.400
 that's a travesty in my opinion,

11:10.400 --> 11:13.840
 or where we spend endless hours in traffic jams.

11:13.840 --> 11:15.720
 And very, very simple innovations

11:15.720 --> 11:18.840
 like a self driving car or what people call a flying car

11:18.840 --> 11:21.040
 could completely change this and it's there.

11:21.040 --> 11:23.320
 I mean, the technology is basically there.

11:23.320 --> 11:25.480
 You have to close your eyes not to see it.

11:25.480 --> 11:29.480
 So lingering on autonomous vehicles,

11:29.480 --> 11:32.120
 a fascinating space, some incredible work

11:32.120 --> 11:33.640
 you've done throughout your career there.

11:33.640 --> 11:37.120
 So let's start with DARPA.

11:37.120 --> 11:40.400
 I think the DARPA challenge through the desert

11:40.400 --> 11:42.920
 and then urban to the streets.

11:42.920 --> 11:45.800
 I think that inspired an entire generation of roboticists

11:45.800 --> 11:49.520
 and obviously sprung this whole excitement

11:49.520 --> 11:52.680
 about this particular kind of four wheeled robots

11:52.680 --> 11:55.560
 who called autonomous cars, self driving cars.

11:55.560 --> 11:58.280
 So you led the development of Stanley,

11:58.280 --> 12:01.320
 the autonomous car that won the race of the desert,

12:01.320 --> 12:04.000
 the DARPA challenge in 2005.

12:04.000 --> 12:07.400
 And junior, the car that finished second

12:07.400 --> 12:11.120
 in the DARPA urban challenge also did incredibly well

12:11.120 --> 12:13.440
 in 2007, I think.

12:14.440 --> 12:18.680
 What are some painful inspiring or enlightening experiences

12:18.680 --> 12:20.680
 from that time that stand out to you?

12:20.680 --> 12:25.680
 Oh my God, painful were all these incredibly complicated

12:28.200 --> 12:30.520
 stupid bugs that had to be found.

12:30.520 --> 12:33.080
 We had a phase where the Stanley,

12:33.080 --> 12:36.280
 our car that eventually won the DARPA run challenge

12:36.280 --> 12:39.360
 would every 30 miles just commit suicide.

12:39.360 --> 12:40.960
 And we didn't know why.

12:40.960 --> 12:43.600
 And it ended up to be that in the sinking

12:43.600 --> 12:47.760
 of two computer clocks, occasionally a clock went backwards

12:47.760 --> 12:50.880
 and that negative time elapsed, screwed up

12:50.880 --> 12:54.440
 the entire internal logic, but it took ages to find this.

12:54.440 --> 12:56.360
 It was like bugs like that.

12:56.360 --> 12:59.360
 I'd say enlightening is the Stanford team

12:59.360 --> 13:02.520
 immediately focused on machine learning and on software

13:02.520 --> 13:03.800
 whereas everybody else seemed to focus

13:03.800 --> 13:05.240
 on building better hardware.

13:05.240 --> 13:07.680
 Our analysis had been a human being

13:07.680 --> 13:10.240
 with an existing rental car can perfectly drive the course

13:10.240 --> 13:12.160
 by having to build a better rental car.

13:12.160 --> 13:14.880
 I just thought it should replace the human being.

13:14.880 --> 13:18.840
 And the human being to me was a conjunction of three steps.

13:18.840 --> 13:21.040
 We had like sensors, eyes and ears,

13:21.040 --> 13:23.800
 mostly eyes, we had brains in the middle

13:23.800 --> 13:26.320
 and then we had actuators, our hands and our feet.

13:26.320 --> 13:28.200
 Now the extras are easy to build.

13:28.200 --> 13:29.720
 The sensors are actually also easy to build

13:29.720 --> 13:30.680
 what was missing was the brain.

13:30.680 --> 13:32.600
 So we had to build a human brain

13:32.600 --> 13:35.200
 and nothing clearer than to me

13:35.200 --> 13:36.960
 that the human brain is a learning machine.

13:36.960 --> 13:38.200
 So why not just train our robot?

13:38.200 --> 13:42.280
 So you would build a massive machine learning into our machine

13:42.280 --> 13:45.640
 and with that we're able to not just learn from human drivers.

13:45.640 --> 13:47.960
 We had the entire speed control of the vehicle

13:47.960 --> 13:49.840
 was copied from human driving

13:49.840 --> 13:51.640
 but also have the robot learn from experience

13:51.640 --> 13:53.680
 where it made a mistake and go to recover from it

13:53.680 --> 13:54.520
 and learn from it.

13:55.560 --> 14:00.560
 You mentioned the pain point of software and clocks.

14:00.680 --> 14:04.600
 Synchronization seems to be a problem

14:04.600 --> 14:06.040
 that continues with robotics.

14:06.040 --> 14:08.040
 It's a tricky one with drones and so on.

14:08.040 --> 14:13.040
 So what does it take to build a thing,

14:13.400 --> 14:16.640
 a system with so many constraints?

14:16.640 --> 14:20.280
 You have a deadline, no time,

14:20.280 --> 14:22.080
 you're unsure about anything really.

14:22.080 --> 14:25.000
 It's the first time that people really even explain.

14:25.000 --> 14:26.800
 It's not even sure that anybody can finish

14:26.800 --> 14:28.880
 when we're talking about the race or the desert

14:28.880 --> 14:30.640
 the year before nobody finished.

14:30.640 --> 14:33.440
 What does it take to scramble and finish a product

14:33.440 --> 14:35.880
 that actually, a system that actually works?

14:35.880 --> 14:38.360
 We were lucky, we were a really small team.

14:38.360 --> 14:40.480
 The core of the team were four people.

14:40.480 --> 14:43.160
 It was four because five couldn't comfortably sit

14:43.160 --> 14:45.360
 inside a car but four could.

14:45.360 --> 14:47.760
 And I as a team leader, my job was to get pizza

14:47.760 --> 14:51.200
 for everybody and wash the car and stuff like this

14:51.200 --> 14:52.880
 and repair the radiator and it broke

14:52.880 --> 14:54.920
 and debug the system.

14:54.920 --> 14:56.920
 And we were very kind of open minded.

14:56.920 --> 14:58.320
 We had like no egos involved.

14:58.320 --> 15:00.880
 We just wanted to see how far we can get.

15:00.880 --> 15:03.320
 What we did really, really well was time management.

15:03.320 --> 15:06.240
 We were done with everything a month before the race

15:06.240 --> 15:08.760
 and we froze the entire software a month before the race.

15:08.760 --> 15:11.440
 And it turned out, looking at other teams,

15:11.440 --> 15:14.120
 every other team complained if they had just one more week

15:14.120 --> 15:15.440
 they would have won.

15:15.440 --> 15:18.760
 And we decided that's gonna fall into that mistake.

15:18.760 --> 15:19.880
 We're gonna be early.

15:19.880 --> 15:22.680
 And we had an entire month to shake the system.

15:22.680 --> 15:24.920
 And we actually found two or three minor bucks

15:24.920 --> 15:27.080
 in the last month that we had to fix

15:27.080 --> 15:30.000
 and we were completely prepared when the race occurred.

15:30.000 --> 15:33.880
 Okay, so first of all, that's such an incredibly rare

15:33.880 --> 15:37.720
 achievement in terms of being able to be done on time

15:37.720 --> 15:39.000
 or ahead of time.

15:39.000 --> 15:43.080
 What do you, how do you do that in your future work?

15:43.080 --> 15:44.760
 What advice do you have in general?

15:44.760 --> 15:46.320
 Because it seems to be so rare,

15:46.320 --> 15:49.280
 especially in highly innovative projects like this.

15:49.280 --> 15:50.840
 People worked till the last second.

15:50.840 --> 15:52.560
 Well, the nice thing about the Topper Grand Challenge

15:52.560 --> 15:55.320
 is that the problem was incredibly well defined.

15:55.320 --> 15:57.160
 We were able for a while to drive

15:57.160 --> 15:58.800
 the old Topper Grand Challenge course

15:58.800 --> 16:00.840
 which had been used the year before.

16:00.840 --> 16:04.080
 And then at some reason we were kicked out of the region.

16:04.080 --> 16:05.520
 So we had to go to different deserts,

16:05.520 --> 16:06.360
 the snorren deserts,

16:06.360 --> 16:08.920
 and we were able to drive desert trails

16:08.920 --> 16:10.640
 just of the same type.

16:10.640 --> 16:12.400
 So there was never any debate about like,

16:12.400 --> 16:13.320
 what's actually the problem?

16:13.320 --> 16:14.440
 We didn't sit down and say,

16:14.440 --> 16:16.600
 hey, should we build a car or a plane

16:16.600 --> 16:18.320
 that we had to build a car?

16:18.320 --> 16:20.480
 That made it very, very easy.

16:20.480 --> 16:23.880
 Then I studied my own life and life of others.

16:23.880 --> 16:26.440
 And we asked that the typical mistake that people make

16:26.440 --> 16:29.680
 is that there is this kind of crazy bug left

16:29.680 --> 16:31.320
 that they haven't found yet.

16:32.280 --> 16:34.400
 And it's just, they regret it

16:34.400 --> 16:36.240
 and the bug would have been trivial to fix.

16:36.240 --> 16:37.800
 They just haven't fixed it yet.

16:37.800 --> 16:39.640
 And they didn't want to fall into that trap.

16:39.640 --> 16:41.160
 So I built a testing team.

16:41.160 --> 16:43.800
 We had a testing team that built a testing booklet

16:43.800 --> 16:46.840
 of 160 pages of tests we had to go through

16:46.840 --> 16:49.800
 just to make sure we shake out the system appropriately.

16:49.800 --> 16:51.840
 And the testing team was with us all the time

16:51.840 --> 16:53.480
 and dictated to us today,

16:53.480 --> 16:55.560
 we do railroad crossings.

16:55.560 --> 16:58.480
 Tomorrow we do, we practice the start of the event.

16:58.480 --> 17:01.120
 And in all of these, we thought, oh my God,

17:01.120 --> 17:03.200
 this long solved trivial and then we tested it out.

17:03.200 --> 17:05.160
 Oh my God, it doesn't do a railroad crossing, why not?

17:05.160 --> 17:09.680
 Oh my God, it mistakes the rails for metal barriers.

17:09.680 --> 17:10.600
 We have to fix this.

17:10.600 --> 17:11.600
 Yes.

17:11.600 --> 17:14.480
 So it was really a continuous focus

17:14.480 --> 17:16.400
 on improving the weakest part of the system.

17:16.400 --> 17:19.160
 And as long as you focus on improving

17:19.160 --> 17:20.560
 the weakest part of the system,

17:20.560 --> 17:23.120
 you eventually build a really great system.

17:23.120 --> 17:25.880
 Let me just pause in that, to me as an engineer,

17:25.880 --> 17:28.280
 it's just super exciting that you were thinking like that,

17:28.280 --> 17:30.440
 especially at that stage as brilliant

17:30.440 --> 17:33.400
 that testing was such a core part of it.

17:33.400 --> 17:35.760
 It may be to linger on the point of leadership.

17:36.720 --> 17:39.120
 I think it's one of the first times

17:39.120 --> 17:42.000
 you were really a leader

17:42.000 --> 17:45.440
 and you've led many very successful teams since then.

17:46.440 --> 17:48.480
 What does it take to be a good leader?

17:48.480 --> 17:51.120
 I would say most of them just take credit.

17:51.120 --> 17:54.320
 Yeah, for the work of others.

17:54.320 --> 17:55.320
 Right.

17:55.320 --> 17:57.560
 That's very convenient, turns out,

17:57.560 --> 18:00.200
 because I can't do all these things myself.

18:00.200 --> 18:03.760
 I'm an engineer at heart, so I care about engineering.

18:03.760 --> 18:06.160
 So I don't know what the chicken and the egg is,

18:06.160 --> 18:07.880
 but as a kid, I loved computers

18:07.880 --> 18:09.560
 because you could tell them to do something.

18:09.560 --> 18:10.720
 And they actually did it.

18:10.720 --> 18:11.560
 It was very cool.

18:11.560 --> 18:12.760
 And you could, like, in the middle of the night,

18:12.760 --> 18:15.200
 wake up at one in the morning and switch on your computer.

18:15.200 --> 18:18.160
 And what you told you to do yesterday would still do.

18:18.160 --> 18:19.400
 That was really cool.

18:19.400 --> 18:21.320
 Unfortunately, they didn't quite work with people.

18:21.320 --> 18:22.840
 So you go to people and tell them what to do

18:22.840 --> 18:25.800
 and they don't do it and they hate you for it.

18:25.800 --> 18:28.080
 Or you do it today and then they go a day later

18:28.080 --> 18:30.240
 and they'll stop doing it, so you have to.

18:30.240 --> 18:31.480
 So then the question really became,

18:31.480 --> 18:34.120
 how can you put yourself in the brain of people

18:34.120 --> 18:35.120
 as opposed to computers?

18:35.120 --> 18:36.920
 And in terms of computers, it's super dumb.

18:36.920 --> 18:38.240
 They're just so dumb.

18:38.240 --> 18:39.640
 If people were as dumb as computers,

18:39.640 --> 18:41.280
 I wouldn't want to work with them.

18:41.280 --> 18:43.600
 But people are smart and people are emotional

18:43.600 --> 18:45.920
 and people have pride and people have aspirations.

18:45.920 --> 18:49.880
 So how can I connect to that?

18:49.880 --> 18:52.600
 And that's the thing where most of leadership just fails

18:52.600 --> 18:56.240
 because many, many engineers, turn manager,

18:56.240 --> 18:58.440
 believe they can treat their team just the same way

18:58.440 --> 19:00.440
 they can treat your computer and it just doesn't work this way.

19:00.440 --> 19:02.360
 It's just really bad.

19:02.360 --> 19:05.120
 So how can I connect to people?

19:05.120 --> 19:07.680
 And it turns out, as a college professor,

19:07.680 --> 19:10.000
 the wonderful thing you do all the time

19:10.000 --> 19:11.040
 is to empower other people.

19:11.040 --> 19:14.760
 Like, your job is to make your students look great.

19:14.760 --> 19:15.560
 That's all you do.

19:15.560 --> 19:16.920
 You're the best coach.

19:16.920 --> 19:18.760
 And it turns out, if you do a fantastic job

19:18.760 --> 19:20.840
 with making your students look great,

19:20.840 --> 19:22.720
 they actually love you and their parents love you.

19:22.720 --> 19:24.760
 And they give you all the credit for stuff you don't deserve.

19:24.760 --> 19:27.200
 Turns out, all my students were smarter than me.

19:27.200 --> 19:28.760
 All the great stuff invented at Stanford

19:28.760 --> 19:30.040
 was their stuff, not my stuff.

19:30.040 --> 19:32.520
 And they give me credit and say, oh, Sebastian,

19:32.520 --> 19:35.160
 we're just making them feel good about themselves.

19:35.160 --> 19:38.080
 So the question really is, can you take a team of people

19:38.080 --> 19:40.400
 and what does it take to make them,

19:40.400 --> 19:43.360
 to connect to what they actually want in life

19:43.360 --> 19:45.760
 and turn this into productive action?

19:45.760 --> 19:48.520
 It turns out, every human being that I know

19:48.520 --> 19:50.120
 has incredibly good intentions.

19:50.120 --> 19:54.080
 I've really rarely met a person with bad intentions.

19:54.080 --> 19:55.920
 I believe every person wants to contribute.

19:55.920 --> 19:59.440
 I think every person I've met wants to help others.

19:59.440 --> 20:02.560
 It's amazing how much of an urge we have not to just help

20:02.560 --> 20:04.440
 ourselves, but to help others.

20:04.440 --> 20:06.480
 So how can we empower people and give them

20:06.480 --> 20:10.640
 the right framework that they can accomplish this?

20:10.640 --> 20:12.440
 In moments when it works, it's magical

20:12.440 --> 20:17.200
 because you'd see the confluence of people

20:17.200 --> 20:19.200
 being able to make the world a better place

20:19.200 --> 20:22.880
 and deriving enormous confidence and pride out of this.

20:22.880 --> 20:27.200
 And that's when my environment works the best.

20:27.200 --> 20:29.440
 These are moments where I can disappear for a month

20:29.440 --> 20:31.600
 and come back and things still work.

20:31.600 --> 20:35.080
 It's very hard to accomplish, but when it works, it's amazing.

20:35.080 --> 20:37.960
 So I agree with you very much, and it's not often

20:37.960 --> 20:43.560
 heard that most people in the world have good intentions.

20:43.560 --> 20:45.960
 At the core, their intentions are good,

20:45.960 --> 20:47.400
 and they're good people.

20:47.400 --> 20:48.880
 That's a beautiful message.

20:48.880 --> 20:50.200
 It's not often heard.

20:50.200 --> 20:52.640
 We make this mistake, and this is a friend of mine,

20:52.640 --> 20:56.400
 Alex Zorta, talking this, that we judge ourselves

20:56.400 --> 21:00.120
 by our intentions and others by their actions.

21:00.120 --> 21:02.800
 And I think the biggest skill, I mean, here in Silicon Valley,

21:02.800 --> 21:05.160
 we're full of engineers who have very little empathy

21:05.160 --> 21:09.240
 and are kind of befuddled by why it doesn't work for them.

21:09.240 --> 21:13.120
 The biggest skill, I think, that people should acquire

21:13.120 --> 21:16.920
 is to put themselves into the position of the other

21:16.920 --> 21:20.040
 and listen, and listen to what the other has to say.

21:20.040 --> 21:23.480
 And they'd be shocked how similar they are to themselves.

21:23.480 --> 21:26.440
 And they might even be shocked how their own actions don't

21:26.440 --> 21:28.640
 reflect their intentions.

21:28.640 --> 21:31.480
 I often have conversations with engineers where I say, look,

21:31.480 --> 21:33.400
 hey, I love you, you're doing a great job.

21:33.400 --> 21:37.360
 And by the way, what you just did has the following effect.

21:37.360 --> 21:38.880
 Are you aware of that?

21:38.880 --> 21:41.320
 And then people would say, oh, my god, not I wasn't,

21:41.320 --> 21:43.120
 because my intention was that.

21:43.120 --> 21:45.040
 And they say, yeah, I trust your intention,

21:45.040 --> 21:46.360
 you're a good human being.

21:46.360 --> 21:48.520
 But just to help you in the future,

21:48.520 --> 21:51.360
 if you keep expressing it that way,

21:51.360 --> 21:53.440
 then people will just hate you.

21:53.440 --> 21:55.680
 And I've had many instances where you would say, oh, my god,

21:55.680 --> 21:58.320
 thank you for telling me this, because it wasn't my intention

21:58.320 --> 21:59.320
 to look like an idiot.

21:59.320 --> 22:00.920
 It wasn't my intention to help other people.

22:00.920 --> 22:02.520
 I just didn't know how to do it.

22:02.520 --> 22:04.040
 Very simple, by the way.

22:04.040 --> 22:07.480
 There's a book, Dale Carnegie, 1936,

22:07.480 --> 22:10.440
 How to Make Friends and How to Influence Others,

22:10.440 --> 22:11.480
 has the entire Bible.

22:11.480 --> 22:12.760
 You just read it, and you're done,

22:12.760 --> 22:14.000
 and you apply it every day.

22:14.000 --> 22:16.760
 And I wish I was good enough to apply it every day.

22:16.760 --> 22:19.800
 But it's just simple things, right?

22:19.800 --> 22:22.600
 Be positive, remember people's names, smile,

22:22.600 --> 22:24.480
 and eventually have empathy.

22:24.480 --> 22:27.440
 Really think that the person that you hate

22:27.440 --> 22:30.440
 and you think is an idiot is actually just like yourself.

22:30.440 --> 22:32.080
 It's a person who's struggling, who

22:32.080 --> 22:34.240
 means well, and who might need help.

22:34.240 --> 22:35.080
 And guess what?

22:35.080 --> 22:36.640
 You need help.

22:36.640 --> 22:39.960
 I've recently spoken with Stephen Schwartzman.

22:39.960 --> 22:41.640
 I'm not sure if you know who that is.

22:41.640 --> 22:42.920
 I do.

22:42.920 --> 22:44.280
 So he said.

22:44.280 --> 22:45.880
 It's on my list.

22:45.880 --> 22:47.400
 I know this.

22:47.400 --> 22:52.760
 But he said to expand on what you're saying,

22:52.760 --> 22:56.040
 that one of the biggest things you can do

22:56.040 --> 23:00.040
 is hear people when they tell you what their problem is,

23:00.040 --> 23:02.360
 and then help them with that problem.

23:02.360 --> 23:06.400
 He says it's surprising how few people actually

23:06.400 --> 23:09.320
 listen to what troubles others.

23:09.320 --> 23:12.640
 And because it's right there in front of you,

23:12.640 --> 23:15.240
 and you can benefit the world the most.

23:15.240 --> 23:18.040
 And in fact, yourself and everybody around you

23:18.040 --> 23:20.840
 by just hearing the problems and solving them.

23:20.840 --> 23:24.000
 I mean, that's my little history of engineering.

23:24.000 --> 23:28.240
 That is, while I was engineering with computers,

23:28.240 --> 23:32.400
 I didn't care at all what their computer's problems were.

23:32.400 --> 23:34.800
 I just told them what to do and they do it.

23:34.800 --> 23:37.560
 And it just doesn't work this way with people.

23:37.560 --> 23:38.480
 It doesn't work with me.

23:38.480 --> 23:43.600
 If you come to me and say, do A, I do the opposite.

23:43.600 --> 23:47.120
 But let's return to the comfortable world of engineering.

23:47.120 --> 23:50.800
 And can you tell me in broad strokes

23:50.800 --> 23:52.160
 in how you see it?

23:52.160 --> 23:53.840
 Because you're at the core of starting it,

23:53.840 --> 23:56.520
 the core of driving it, the technical evolution

23:56.520 --> 24:00.400
 of autonomous vehicles from the first DARPA Grand Challenge

24:00.400 --> 24:04.200
 to the incredible success we see with the program you started

24:04.200 --> 24:05.960
 with Google Self Driving Car and Waymo

24:05.960 --> 24:08.320
 and the entire industry that sprung up

24:08.320 --> 24:11.160
 of different kinds of approaches, debates, and so on.

24:11.160 --> 24:13.840
 Well, the idea of self driving car goes back to the 80s.

24:13.840 --> 24:16.160
 There was a team in Germany and another team in Carnegie Mellon

24:16.160 --> 24:18.680
 that did some very pioneering work.

24:18.680 --> 24:20.800
 But back in the day, I'd say the computers

24:20.800 --> 24:24.640
 were so deficient that even the best professors

24:24.640 --> 24:28.200
 and engineers in the world basically stood no chance.

24:28.200 --> 24:32.120
 It then folded into a phase where the US government spent

24:32.120 --> 24:34.640
 at least half a billion dollars that I could count

24:34.640 --> 24:36.120
 on research projects.

24:36.120 --> 24:38.920
 But the way the procurement works,

24:38.920 --> 24:42.760
 a successful stack of paper describing lots of stuff

24:42.760 --> 24:46.240
 that no one's ever going to read was a successful product

24:46.240 --> 24:47.640
 of a research project.

24:47.640 --> 24:52.560
 So we trained our researchers to produce lots of paper.

24:52.560 --> 24:54.280
 That all changed with the DARPA Grand Challenge.

24:54.280 --> 24:58.480
 And I really got a credit the ingenious people at DARPA

24:58.480 --> 25:00.400
 and the US government in Congress

25:00.400 --> 25:02.960
 that took a complete new funding model where they said,

25:02.960 --> 25:05.600
 let's not fund effort, let's fund outcomes.

25:05.600 --> 25:08.800
 And it sounds very trivial, but there was no tax code

25:08.800 --> 25:13.680
 that allowed the use of congressional tax money for a price.

25:13.680 --> 25:15.080
 It was all effort based.

25:15.080 --> 25:17.440
 So if you put in 100 hours in, you could charge 100 hours.

25:17.440 --> 25:20.640
 If you put in 1,000 hours in, you could build 1,000 hours.

25:20.640 --> 25:22.840
 By changing the focus and so you're making the price,

25:22.840 --> 25:26.320
 we don't pay you for development, we pay you for the accomplishment.

25:26.320 --> 25:30.200
 They automatically drew out all these contractors

25:30.200 --> 25:33.400
 who are used to the drug of getting money per hour.

25:33.400 --> 25:35.720
 And they drew in a whole bunch of new people.

25:35.720 --> 25:37.600
 And these people are mostly crazy people.

25:37.600 --> 25:40.760
 There were people who had a car and a computer

25:40.760 --> 25:42.400
 and they wanted to make a million bucks.

25:42.400 --> 25:44.200
 The million bucks was their visual price money,

25:44.200 --> 25:45.480
 it was then doubled.

25:45.480 --> 25:49.400
 And they felt if I put my computer in my car and program it,

25:49.400 --> 25:50.880
 I can be rich.

25:50.880 --> 25:52.040
 And that was so awesome.

25:52.040 --> 25:55.440
 Like half the teams, there was a team that was surfer dudes

25:55.440 --> 25:58.520
 and they had like two surfboards on their vehicle

25:58.520 --> 26:01.560
 and brought like these fashion girls, super cute girls,

26:01.560 --> 26:03.720
 like twin sisters.

26:03.720 --> 26:07.280
 And you could tell these guys were not your common felt way

26:07.280 --> 26:10.880
 bandit who gets all these big, multi million

26:10.880 --> 26:13.520
 and billion dollar countries from the US government.

26:13.520 --> 26:16.320
 And there was a great reset.

26:16.320 --> 26:18.560
 Universities moved in.

26:18.560 --> 26:21.800
 I was very fortunate at Stanford that I just received 10 year.

26:21.800 --> 26:23.360
 So I couldn't get fired, no matter what I do.

26:23.360 --> 26:25.120
 Otherwise I wouldn't have done it.

26:25.120 --> 26:28.240
 And I had enough money to finance this thing.

26:28.240 --> 26:31.160
 And I was able to attract a lot of money from third parties.

26:31.160 --> 26:32.520
 And even car companies moved in.

26:32.520 --> 26:34.040
 They kind of moved in very quietly

26:34.040 --> 26:36.560
 because they were super scared to be embarrassed

26:36.560 --> 26:38.520
 that their car would flip over.

26:38.520 --> 26:40.640
 But Ford was there and Volkswagen was there

26:40.640 --> 26:43.360
 and a few others and GM was there.

26:43.360 --> 26:46.320
 So it kind of reset the entire landscape of people.

26:46.320 --> 26:48.200
 And if you look at who's a big name

26:48.200 --> 26:49.480
 in self living cars today,

26:49.480 --> 26:53.400
 these are mostly people who participated in those challenges.

26:53.400 --> 26:54.320
 OK, that's incredible.

26:54.320 --> 26:58.680
 Can you just comment quickly on your sense of lessons

26:58.680 --> 27:01.240
 learned from that kind of funding model

27:01.240 --> 27:04.400
 and the research that's going on in academia

27:04.400 --> 27:06.120
 in terms of producing papers.

27:06.120 --> 27:09.840
 Is there something to be learned and scaled up bigger,

27:09.840 --> 27:11.720
 having these kinds of grand challenges

27:11.720 --> 27:14.560
 that could improve outcomes?

27:14.560 --> 27:17.200
 So I'm a big believer in focusing on kind of an end

27:17.200 --> 27:19.640
 to end system.

27:19.640 --> 27:21.920
 I'm a really big believer in systems building.

27:21.920 --> 27:23.680
 I've always built systems in my academic career,

27:23.680 --> 27:27.040
 even though I do a lot of math and abstract stuff.

27:27.040 --> 27:29.640
 But it's all derived from the idea of let's solve a real problem.

27:29.640 --> 27:34.160
 And it's very hard for me to be in academic and say,

27:34.160 --> 27:35.760
 let me solve a component of a problem.

27:35.760 --> 27:38.640
 Like if someone feels like non monetary logic

27:38.640 --> 27:41.760
 or AI planning systems where people believe

27:41.760 --> 27:44.320
 that a certain style of problem solving

27:44.320 --> 27:47.280
 is the ultimate end objective.

27:47.280 --> 27:51.000
 And I would always turn around and say, hey, what problem

27:51.000 --> 27:53.000
 would my grandmother care about that

27:53.000 --> 27:54.680
 doesn't understand computer technology

27:54.680 --> 27:56.480
 and doesn't want to understand?

27:56.480 --> 27:58.480
 And how can I make her love what I do?

27:58.480 --> 28:01.320
 Because only then do I have an impact on the world.

28:01.320 --> 28:02.920
 I can easily impress my colleagues.

28:02.920 --> 28:04.720
 That is much easier.

28:04.720 --> 28:07.600
 But impressing my grandmother is very, very hard.

28:07.600 --> 28:10.760
 So I would always thought if I can build a self driving car

28:10.760 --> 28:13.840
 and my grandmother can use it even after she loses

28:13.840 --> 28:16.160
 her driving privileges or children can use it

28:16.160 --> 28:20.560
 or we save maybe a million lives a year,

28:20.560 --> 28:22.160
 that would be very impressive.

28:22.160 --> 28:23.920
 And then there's so many problems like these.

28:23.920 --> 28:26.000
 Like there's a problem with Q and cancer or whatever it is.

28:26.000 --> 28:27.880
 Live twice as long.

28:27.880 --> 28:29.960
 Once a problem is defined, of course,

28:29.960 --> 28:31.440
 I can't solve it in its entirety.

28:31.440 --> 28:34.200
 Like it takes sometimes tens of thousands of people

28:34.200 --> 28:35.360
 to find a solution.

28:35.360 --> 28:39.400
 There's no way you can fund an army of 10,000 at Stanford.

28:39.400 --> 28:41.080
 So you've got to build a prototype.

28:41.080 --> 28:42.520
 Let's build a meaningful prototype.

28:42.520 --> 28:44.320
 And the DARPA Grand Challenge was beautiful because it

28:44.320 --> 28:46.080
 told me what this prototype had to do.

28:46.080 --> 28:47.720
 I didn't even think about what it had to do.

28:47.720 --> 28:48.840
 It just had to read the rules.

28:48.840 --> 28:51.080
 And it was really, really beautiful.

28:51.080 --> 28:52.840
 And it's most beautiful, you think,

28:52.840 --> 28:57.160
 what academia could aspire to is to build a prototype that's

28:57.160 --> 29:01.400
 the systems level that solves or gives you an inkling

29:01.400 --> 29:03.520
 that this problem could be solved with this prototype.

29:03.520 --> 29:06.560
 First of all, I want to emphasize what academia really is.

29:06.560 --> 29:08.600
 And I think people misunderstand it.

29:08.600 --> 29:13.360
 First and foremost, academia is a way to educate young people.

29:13.360 --> 29:15.440
 First and foremost, a professor is an educator,

29:15.440 --> 29:18.600
 no matter where you are at a small suburban college

29:18.600 --> 29:22.840
 or whether you are a Harvard or Stanford professor.

29:22.840 --> 29:25.000
 That's not the way most people think of themselves

29:25.000 --> 29:28.240
 in academia because we have this kind of competition going

29:28.240 --> 29:31.440
 on for citations and publication.

29:31.440 --> 29:32.840
 That's a measurable thing.

29:32.840 --> 29:35.480
 But that is secondary to the primary purpose

29:35.480 --> 29:37.840
 of educating people to think.

29:37.840 --> 29:42.880
 Now, in terms of research, most of the great science,

29:42.880 --> 29:45.560
 the great research comes out of universities.

29:45.560 --> 29:47.880
 You can trace almost everything back, including Google,

29:47.880 --> 29:48.880
 to universities.

29:48.880 --> 29:52.160
 So there's nothing really fundamentally broken here.

29:52.160 --> 29:53.440
 It's a good system.

29:53.440 --> 29:55.960
 And I think America has the finest university system

29:55.960 --> 29:57.640
 on the planet.

29:57.640 --> 30:01.000
 We can talk about reach and how to reach people outside

30:01.000 --> 30:01.480
 the system.

30:01.480 --> 30:02.360
 It's a different topic.

30:02.360 --> 30:04.840
 But the system itself is a good system.

30:04.840 --> 30:08.360
 If I had one wish, I would say it'd be really great

30:08.360 --> 30:14.240
 if there was more debate about what the great big problems

30:14.240 --> 30:18.800
 are in society and focus on those.

30:18.800 --> 30:21.640
 And most of them are interdisciplinary.

30:21.640 --> 30:24.640
 Unfortunately, it's very easy to fall

30:24.640 --> 30:28.960
 into an interdisciplinary viewpoint where

30:28.960 --> 30:32.360
 your problem is dictated, but your closest colleagues

30:32.360 --> 30:33.720
 believe the problem is.

30:33.720 --> 30:35.280
 It's very hard to break out and say,

30:35.280 --> 30:37.960
 well, there's an entire new field of problems.

30:37.960 --> 30:39.840
 So give an example.

30:39.840 --> 30:41.640
 Prior to me working on self driving cars,

30:41.640 --> 30:44.600
 I was a roboticist and a machine learning expert.

30:44.600 --> 30:47.280
 And I wrote books on robotics, something called

30:47.280 --> 30:48.480
 probabilistic robotics.

30:48.480 --> 30:51.520
 It's a very methods driven kind of view point of the world.

30:51.520 --> 30:54.600
 I built robots that acted in museums as tour guides that

30:54.600 --> 30:55.600
 led children around.

30:55.600 --> 31:00.040
 It is something that at the time was moderately challenging.

31:00.040 --> 31:03.360
 When I started working on cars, several colleagues

31:03.360 --> 31:06.080
 told me, Sebastian, you're destroying your career

31:06.080 --> 31:08.160
 because in our field of robotics,

31:08.160 --> 31:10.400
 cars are looked like as a gimmick.

31:10.400 --> 31:11.760
 And they're not expressive enough.

31:11.760 --> 31:15.080
 They can only put this bottle in the brakes.

31:15.080 --> 31:16.440
 There's no dexterity.

31:16.440 --> 31:18.240
 There's no complexity.

31:18.240 --> 31:19.480
 It's just too simple.

31:19.480 --> 31:21.600
 And no one came to me and said, wow,

31:21.600 --> 31:25.000
 if you solve that problem, you can save a million lives.

31:25.000 --> 31:27.240
 Among all robotic problems that I've seen in my life,

31:27.240 --> 31:29.760
 I would say the self living car transportation

31:29.760 --> 31:32.080
 is the one that has the most hope for society.

31:32.080 --> 31:35.120
 So how come the robotics community wasn't all over the place?

31:35.120 --> 31:37.920
 And it was become because we focused on methods and solutions

31:37.920 --> 31:39.880
 and not on problems.

31:39.880 --> 31:42.400
 Like if you go around today and ask your grandmother,

31:42.400 --> 31:43.240
 what bugs you?

31:43.240 --> 31:45.240
 What really makes you upset?

31:45.240 --> 31:48.760
 I challenge any academic to do this

31:48.760 --> 31:52.600
 and then realize how far your research is probably

31:52.600 --> 31:54.840
 away from that today.

31:54.840 --> 31:57.520
 At the very least, that's a good thing for academics

31:57.520 --> 31:58.960
 to deliberate on.

31:58.960 --> 32:00.880
 The other thing that's really nice in Silicon Valley

32:00.880 --> 32:04.400
 is Silicon Valley is full of smart people outside academia.

32:04.400 --> 32:06.320
 So there's the Larry Pages and Mark Zuckerbergs

32:06.320 --> 32:09.040
 in the world who are anywhere smarter, smarter

32:09.040 --> 32:11.720
 than the best academics I've met in my life.

32:11.720 --> 32:15.480
 And what they do is they are at a different level.

32:15.480 --> 32:16.720
 They build the systems.

32:16.720 --> 32:19.320
 They build the customer facing systems.

32:19.320 --> 32:21.920
 They build things that people can use

32:21.920 --> 32:23.800
 without technical education.

32:23.800 --> 32:25.840
 And they are inspired by research.

32:25.840 --> 32:27.520
 They're inspired by scientists.

32:27.520 --> 32:30.320
 They hire the best PhDs from the best universities

32:30.320 --> 32:32.040
 for a reason.

32:32.040 --> 32:34.800
 So I think this kind of vertical integration

32:34.800 --> 32:37.800
 that between the real product, the real impact,

32:37.800 --> 32:39.880
 and the real thought, the real ideas,

32:39.880 --> 32:41.720
 that's actually working surprisingly well in Silicon

32:41.720 --> 32:42.760
 Valley.

32:42.760 --> 32:45.000
 It did not work as well in other places in this nation.

32:45.000 --> 32:46.720
 So when I worked at Carnegie Mellon,

32:46.720 --> 32:49.800
 we had the world's finest computer science university.

32:49.800 --> 32:52.760
 But there wasn't those people in Pittsburgh

32:52.760 --> 32:55.720
 that would be able to take these very fine computer science

32:55.720 --> 33:00.600
 ideas and turn them into massively impactful products.

33:00.600 --> 33:04.320
 That symbiosis seemed to exist pretty much only in Silicon

33:04.320 --> 33:06.600
 Valley and maybe a bit in Boston and Austin.

33:06.600 --> 33:07.240
 Yeah.

33:07.240 --> 33:11.200
 With Stanford, that's really interesting.

33:11.200 --> 33:14.080
 So if we look a little bit further on

33:14.080 --> 33:17.800
 from the DARPA Grand Challenge and the launch

33:17.800 --> 33:21.080
 of the Google self driving car, what do you see

33:21.080 --> 33:24.520
 as the state, the challenges of autonomous vehicles

33:24.520 --> 33:29.160
 as they are now, is actually achieving that huge scale

33:29.160 --> 33:31.640
 and having a huge impact on society?

33:31.640 --> 33:35.200
 I'm extremely proud of what has been accomplished.

33:35.200 --> 33:38.280
 And again, I'm taking a lot of quality for the work for others.

33:38.280 --> 33:40.000
 And I'm actually very optimistic.

33:40.000 --> 33:42.360
 And people have been kind of worrying,

33:42.360 --> 33:45.880
 is it too fast, is it slow, why is it not there yet, and so on.

33:45.880 --> 33:48.800
 It is actually quite an interesting hard problem

33:48.800 --> 33:54.480
 in that a self driving car, to build one that manages 90%

33:54.480 --> 33:57.280
 of the problems encountered in everyday driving is easy.

33:57.280 --> 33:59.520
 We can literally do this over a weekend.

33:59.520 --> 34:03.280
 To do 99% might take a month, then there's 1% left.

34:03.280 --> 34:08.160
 So 1% would mean that you still have a fatal accident every week.

34:08.160 --> 34:09.120
 Very unacceptable.

34:09.120 --> 34:11.000
 So now you work on this 1%.

34:11.000 --> 34:13.720
 And the 99% of that, the remaining 1%,

34:13.720 --> 34:15.800
 is actually still relatively easy.

34:15.800 --> 34:18.280
 But now you're down to like a 100% or 1%.

34:18.280 --> 34:21.640
 And it's still completely unacceptable in terms of safety.

34:21.640 --> 34:24.280
 So the variety of things you encounter are just enormous.

34:24.280 --> 34:26.480
 And that gives me enormous respect for a human being

34:26.480 --> 34:30.520
 that we're able to deal with the couch on the highway,

34:30.520 --> 34:33.520
 or the deer in the headlight, or the blown tire

34:33.520 --> 34:34.960
 that we've never been trained for.

34:34.960 --> 34:37.160
 And all of a sudden, I have to handle an emergency situation

34:37.160 --> 34:38.800
 and often do very, very successfully.

34:38.800 --> 34:39.880
 It's amazing.

34:39.880 --> 34:42.240
 From that perspective, how safe driving actually is,

34:42.240 --> 34:45.280
 given how many millions of miles we drive every year

34:45.280 --> 34:47.680
 in this country.

34:47.680 --> 34:50.120
 We are now at a point where I believe the technology is there.

34:50.120 --> 34:51.680
 And I've seen it.

34:51.680 --> 34:52.560
 I've seen it in way more.

34:52.560 --> 34:55.480
 I've seen it in app, I've seen it in cruise,

34:55.480 --> 34:58.360
 and in a number of companies and in voyage,

34:58.360 --> 35:02.400
 where vehicles not driving around and basically

35:02.400 --> 35:04.280
 flawlessly are able to drive people around

35:04.280 --> 35:06.160
 in limited scenarios.

35:06.160 --> 35:08.080
 In fact, you can go to Vegas today

35:08.080 --> 35:09.960
 and order a summoner lift.

35:09.960 --> 35:13.560
 And if you get the right setting off your app,

35:13.560 --> 35:15.880
 you'll be picked up by a driverless car.

35:15.880 --> 35:18.120
 Now, there's still safety drivers in there.

35:18.120 --> 35:21.360
 But that's a fantastic way to kind of learn

35:21.360 --> 35:23.000
 what the limits are of technology today.

35:23.000 --> 35:24.760
 And there's still some glitches.

35:24.760 --> 35:26.600
 But the glitches have become very, very rare.

35:26.600 --> 35:29.720
 I think the next step is going to be to down cost it,

35:29.720 --> 35:31.280
 to harden it.

35:31.280 --> 35:34.280
 The entrapment, the sensors are not quite

35:34.280 --> 35:36.240
 an automotive grade standard yet.

35:36.240 --> 35:37.840
 And then to really build the business models,

35:37.840 --> 35:41.000
 to really kind of go somewhere and make the business case.

35:41.000 --> 35:42.600
 And the business case is hard work.

35:42.600 --> 35:44.600
 It's not just, oh my god, we have this capability.

35:44.600 --> 35:45.560
 People are just going to buy it.

35:45.560 --> 35:46.760
 You have to make it affordable.

35:46.760 --> 35:51.320
 You have to find the social acceptance

35:51.320 --> 35:52.240
 of people.

35:52.240 --> 35:55.360
 None of the teams yet has been able to, or gutsy enough,

35:55.360 --> 35:59.360
 to drive around without a person inside the car.

35:59.360 --> 36:01.320
 And that's the next magical hurdle.

36:01.320 --> 36:04.280
 We'll be able to send these vehicles around completely

36:04.280 --> 36:05.760
 empty in traffic.

36:05.760 --> 36:08.120
 And I think, I mean, I wait every day,

36:08.120 --> 36:11.840
 wait for the news that Waymo has just done this.

36:11.840 --> 36:14.680
 So interesting, you mentioned gutsy.

36:14.680 --> 36:20.200
 Let me ask some maybe unanswerable question,

36:20.200 --> 36:21.480
 maybe edgy questions.

36:21.480 --> 36:26.840
 But in terms of how much risk is required,

36:26.840 --> 36:30.360
 some guts, in terms of leadership style,

36:30.360 --> 36:32.600
 it would be good to contrast approaches.

36:32.600 --> 36:34.680
 And I don't think anyone knows what's right.

36:34.680 --> 36:38.560
 But if we compare Tesla and Waymo, for example,

36:38.560 --> 36:44.880
 Elon Musk and the Waymo team, there's slight differences

36:44.880 --> 36:45.720
 in approach.

36:45.720 --> 36:49.600
 So on the Elon side, there's more,

36:49.600 --> 36:50.880
 I don't know what the right word to use,

36:50.880 --> 36:53.960
 but aggression in terms of innovation.

36:53.960 --> 37:01.320
 And on Waymo's side, there's more cautious, safety focused

37:01.320 --> 37:03.520
 approach to the problem.

37:03.520 --> 37:06.240
 What do you think it takes?

37:06.240 --> 37:07.520
 What's leadership?

37:07.520 --> 37:09.160
 But which moment is right?

37:09.160 --> 37:11.640
 Which approach is right?

37:11.640 --> 37:13.960
 Look, I don't sit in either of those teams.

37:13.960 --> 37:18.040
 So I'm unable to even verify, like somebody says, correct.

37:18.040 --> 37:21.280
 In the end of the day, every innovator in that space

37:21.280 --> 37:23.200
 will face a fundamental dilemma.

37:23.200 --> 37:27.160
 And I would say you could put aerospace tightens

37:27.160 --> 37:29.360
 into the same bucket, which is you

37:29.360 --> 37:34.360
 have to balance public safety with your drive to innovate.

37:34.360 --> 37:36.840
 And this country in particular in the States

37:36.840 --> 37:40.680
 has a 100 plus year history of doing this very successfully.

37:40.680 --> 37:43.920
 Air travel is what 100 times is safe per mile,

37:43.920 --> 37:47.560
 then ground travel, then cars.

37:47.560 --> 37:49.520
 And there's a reason for it, because people

37:49.520 --> 37:54.320
 have found ways to be very methodological about ensuring

37:54.320 --> 37:56.960
 public safety while still being able to make progress

37:56.960 --> 38:00.280
 on important aspects, for example, like yell and noise

38:00.280 --> 38:03.680
 and fuel consumption.

38:03.680 --> 38:06.200
 So I think that those practices are proven,

38:06.200 --> 38:07.880
 and they actually work.

38:07.880 --> 38:09.880
 We live in a world safer than ever before.

38:09.880 --> 38:11.880
 And yes, there will always be the provision

38:11.880 --> 38:12.760
 that something goes wrong.

38:12.760 --> 38:15.280
 There's always the possibility that someone makes a mistake,

38:15.280 --> 38:17.160
 or there's an unexpected failure.

38:17.160 --> 38:21.000
 We can never guarantee to 100% absolute safety

38:21.000 --> 38:23.280
 other than just not doing it.

38:23.280 --> 38:27.080
 But I think I'm very proud of the history of the United States.

38:27.080 --> 38:30.120
 I mean, we've dealt with much more dangerous technology,

38:30.120 --> 38:33.720
 like nuclear energy, and kept that safe too.

38:33.720 --> 38:36.360
 We have nuclear weapons, and we keep those safe.

38:36.360 --> 38:39.400
 So we have methods and procedures

38:39.400 --> 38:42.920
 that really balance these two things very, very successfully.

38:42.920 --> 38:46.280
 You've mentioned a lot of great autonomous vehicle companies

38:46.280 --> 38:48.720
 that are taking sort of the level four, level five.

38:48.720 --> 38:51.840
 They jump in full autonomy with a safety driver

38:51.840 --> 38:53.600
 and take that kind of approach, and also

38:53.600 --> 38:55.760
 through simulation and so on.

38:55.760 --> 38:59.520
 There's also the approach that Tesla autopilot is doing,

38:59.520 --> 39:03.680
 which is kind of incrementally taking a level two vehicle

39:03.680 --> 39:07.080
 and using machine learning and learning from the driving

39:07.080 --> 39:10.520
 of human beings, and trying to creep up,

39:10.520 --> 39:12.320
 trying to incrementally improve the system

39:12.320 --> 39:15.520
 until it's able to achieve level four autonomy.

39:15.520 --> 39:19.720
 So perfect autonomy in certain kind of geographical regions.

39:19.720 --> 39:23.120
 What are your thoughts on these contrasting approaches?

39:23.120 --> 39:25.520
 Well, first of all, I'm a very proud Tesla owner,

39:25.520 --> 39:27.840
 and I literally use the autopilot every day,

39:27.840 --> 39:30.760
 and it literally has kept me safe.

39:30.760 --> 39:33.880
 It is a beautiful technology specifically

39:33.880 --> 39:37.560
 for highway driving when I'm slightly tired,

39:37.560 --> 39:42.160
 because then it turns me into a much safer driver,

39:42.160 --> 39:46.440
 and I'm 100% confident that's the case.

39:46.440 --> 39:49.440
 In terms of the right approach, I think the biggest change

39:49.440 --> 39:51.040
 I've seen since I went in the Waymo team

39:51.040 --> 39:54.520
 is this thing called deep learning.

39:54.520 --> 39:57.960
 Deep learning was not a hot topic when I started Waymo,

39:57.960 --> 39:59.360
 or Google self having cars.

39:59.360 --> 40:01.720
 It was there, but in fact, we started Google Brain

40:01.720 --> 40:04.760
 at the same time in Google X, so I invested in deep learning,

40:04.760 --> 40:06.280
 but people didn't talk about it.

40:06.280 --> 40:07.800
 It wasn't a hot topic.

40:07.800 --> 40:08.480
 And now it is.

40:08.480 --> 40:11.760
 There's a shift of emphasis from a more geometric

40:11.760 --> 40:14.280
 perspective, where you use geometric sensors.

40:14.280 --> 40:15.840
 They give you a full 3D view, and you

40:15.840 --> 40:18.200
 do a geometric reasoning about, oh, this box over here

40:18.200 --> 40:19.600
 might be a car.

40:19.600 --> 40:24.080
 Towards a more human like, oh, let's just learn about it.

40:24.080 --> 40:26.520
 This looks like the thing I've seen 10,000 times before,

40:26.520 --> 40:28.520
 so maybe it's the same thing.

40:28.520 --> 40:30.280
 Machine learning perspective.

40:30.280 --> 40:32.240
 And that has really put, I think,

40:32.240 --> 40:35.960
 all these approaches on steroids.

40:35.960 --> 40:38.720
 At Udacity, we teach a course in self driving cars.

40:38.720 --> 40:43.800
 In fact, I think we've credited over 20,000 or so people

40:43.800 --> 40:44.960
 on self driving cars skills.

40:44.960 --> 40:47.440
 So every self driving car team in the world

40:47.440 --> 40:49.280
 now uses our engineers.

40:49.280 --> 40:51.880
 And in this course, the very first homework assignment

40:51.880 --> 40:54.920
 is to do lane finding on images.

40:54.920 --> 40:57.440
 And lane finding images, for laymen, what this means

40:57.440 --> 40:59.880
 is you put a camera into your car, or you open your eyes,

40:59.880 --> 41:02.440
 and you wouldn't know where the lane is.

41:02.440 --> 41:05.000
 So you can stay inside the lane with your car.

41:05.000 --> 41:06.520
 Humans can do this super easily.

41:06.520 --> 41:08.480
 You just look and you know where the lane is.

41:08.480 --> 41:10.120
 Intuitively.

41:10.120 --> 41:12.160
 For machines, for a long time, it was super hard

41:12.160 --> 41:14.600
 because people would write these kind of crazy rules.

41:14.600 --> 41:16.080
 If there's like wine lane markers,

41:16.080 --> 41:17.600
 and these are what white really means,

41:17.600 --> 41:20.320
 this is not quite white enough, so let's all, it's not white.

41:20.320 --> 41:22.280
 Or maybe the sun is shining, so when the sun shines,

41:22.280 --> 41:24.600
 and this is white, and this is a straight line.

41:24.600 --> 41:25.680
 Or maybe it's not quite a straight line

41:25.680 --> 41:27.200
 because the road is curved.

41:27.200 --> 41:30.080
 And do we know that there's a 6 feet between lane markings

41:30.080 --> 41:33.840
 or not, or 12 feet, whatever it is?

41:33.840 --> 41:36.280
 And now, what the students are doing,

41:36.280 --> 41:37.560
 they would take machine learning.

41:37.560 --> 41:39.600
 Instead of like writing these crazy rules

41:39.600 --> 41:40.720
 for the lane marker, they would say,

41:40.720 --> 41:42.680
 hey, let's take an hour of driving,

41:42.680 --> 41:44.400
 and label it and tell the vehicle,

41:44.400 --> 41:45.760
 this is actually the lane by hand.

41:45.760 --> 41:47.320
 And then these are examples,

41:47.320 --> 41:49.360
 and have the machine find its own rules

41:49.360 --> 41:51.400
 what lane markings are.

41:51.400 --> 41:53.760
 And within 24 hours, now every student

41:53.760 --> 41:56.040
 that's never done any programming before in this space

41:56.040 --> 41:58.280
 can write a perfect lane finder

41:58.280 --> 42:00.840
 as good as the best commercial lane finders.

42:00.840 --> 42:02.760
 And that's completely amazing to me.

42:02.760 --> 42:05.480
 We've seen progress using machine learning

42:05.480 --> 42:08.120
 that completely dwarfs anything

42:08.120 --> 42:09.920
 that I saw 10 years ago.

42:10.920 --> 42:12.840
 Yeah, and just as a side note,

42:12.840 --> 42:15.200
 the self driving car nanodegree,

42:15.200 --> 42:18.880
 the fact that you launched that many years ago now,

42:18.880 --> 42:21.360
 maybe four years ago, three years ago,

42:21.360 --> 42:23.720
 is incredible that that's a great example

42:23.720 --> 42:24.720
 of system level thinking.

42:24.720 --> 42:27.120
 Sort of just taking an entire course

42:27.120 --> 42:29.320
 that teaches how to solve the entire problem.

42:29.320 --> 42:31.200
 I definitely recommend people.

42:31.200 --> 42:32.360
 It's been super popular,

42:32.360 --> 42:34.240
 and it's become actually incredibly high quality

42:34.240 --> 42:37.360
 related with Mercedes and various other companies

42:37.360 --> 42:38.200
 in that space.

42:38.200 --> 42:40.120
 And we find that engineers from Tesla

42:40.120 --> 42:41.960
 and Waymo are taking it today.

42:43.120 --> 42:45.480
 The insight was that two things.

42:45.480 --> 42:49.200
 One is existing universities will be very slow to move

42:49.200 --> 42:50.480
 because the departmentalized

42:50.480 --> 42:52.360
 and there's no department for self driving cars.

42:52.360 --> 42:56.240
 So between Mackey and EE and computer science,

42:56.240 --> 42:57.680
 getting those folks together into one room

42:57.680 --> 42:59.640
 is really, really hard.

42:59.640 --> 43:01.280
 And every professor listening here will know,

43:01.280 --> 43:02.920
 we'll probably agree to that.

43:02.920 --> 43:06.400
 And secondly, even if all the great universities

43:06.400 --> 43:08.400
 just did this, which none so far

43:08.400 --> 43:11.120
 has developed a curriculum in this field,

43:11.120 --> 43:13.720
 it is just a few thousand students that can partake

43:13.720 --> 43:16.280
 because all the great universities are super selective.

43:16.280 --> 43:18.160
 So how about people in India?

43:18.160 --> 43:20.680
 How about people in China or in the Middle East

43:20.680 --> 43:23.480
 or Indonesia or Africa?

43:23.480 --> 43:25.200
 Why should those be excluded

43:25.200 --> 43:27.280
 from the skill of building self driving cars?

43:27.280 --> 43:28.520
 Are they any dumber than we are?

43:28.520 --> 43:30.240
 Are they any less privileged?

43:30.240 --> 43:34.880
 And the answer is we should just give everybody the skill

43:34.880 --> 43:35.920
 to build a self driving car.

43:35.920 --> 43:37.480
 Because if we do this,

43:37.480 --> 43:40.400
 then we have like a thousand self driving car startups.

43:40.400 --> 43:42.960
 And if 10% succeed, that's like a hundred,

43:42.960 --> 43:44.200
 that means a hundred countries now

43:44.200 --> 43:46.840
 will have self driving cars and be safer.

43:46.840 --> 43:50.400
 It's kind of interesting to imagine impossible to quantify,

43:50.400 --> 43:55.080
 but the number, over a period of several decades,

43:55.080 --> 43:57.960
 the impact that has, like a single course,

43:57.960 --> 44:00.800
 like a ripple effect to society.

44:00.800 --> 44:03.040
 If you, I just recently talked to Andrew

44:03.040 --> 44:06.560
 and who was creator of Cosmos, so it's a show.

44:06.560 --> 44:08.200
 It's interesting to think about

44:08.200 --> 44:10.680
 how many scientists that show launched.

44:10.680 --> 44:15.600
 And so it's really, in terms of impact,

44:15.600 --> 44:17.200
 I can't imagine a better course

44:17.200 --> 44:18.680
 than the self driving car course.

44:18.680 --> 44:21.840
 That's, you know, there's other more specific disciplines

44:21.840 --> 44:22.880
 like deep learning and so on

44:22.880 --> 44:24.120
 that Udacity is also teaching,

44:24.120 --> 44:26.840
 but self driving cars, it's really, really interesting course.

44:26.840 --> 44:28.400
 In the end, it came at the right moment.

44:28.400 --> 44:31.680
 It came at a time when there were a bunch of acquires.

44:31.680 --> 44:34.160
 Acquire is acquisition of a company,

44:34.160 --> 44:36.360
 not for its technology or its products or business,

44:36.360 --> 44:38.280
 but for its people.

44:38.280 --> 44:40.600
 So acquire means maybe the company of 70 people,

44:40.600 --> 44:41.520
 they have no product yet,

44:41.520 --> 44:43.120
 but they're super smart people

44:43.120 --> 44:44.320
 and they pay a certain amount of money.

44:44.320 --> 44:48.400
 So I took acquires like GM Cruise and Uber and others

44:48.400 --> 44:50.040
 and did the math and said,

44:50.040 --> 44:52.200
 hey, how many people are there

44:52.200 --> 44:53.720
 and how much money was paid?

44:53.720 --> 44:55.600
 And as a lower bound,

44:55.600 --> 44:58.520
 I estimated the value of a self driving car engineer

44:58.520 --> 45:02.240
 in these acquisitions to be at least $10 million, right?

45:02.240 --> 45:05.040
 So think about this, you get yourself a skill

45:05.040 --> 45:06.680
 and you team up and build a company

45:06.680 --> 45:09.760
 and your worth now is $10 million.

45:09.760 --> 45:10.800
 I mean, that's kind of cool.

45:10.800 --> 45:13.400
 I mean, what other thing could you do in life

45:13.400 --> 45:15.920
 to be worth $10 million within a year?

45:15.920 --> 45:17.600
 Yeah, amazing.

45:17.600 --> 45:21.000
 But to come back for a moment onto deep learning

45:21.000 --> 45:23.720
 and its application in autonomous vehicles,

45:23.720 --> 45:28.520
 you know, what are your thoughts on Elon Musk's statement,

45:28.520 --> 45:31.160
 provocative statement, perhaps that light air is a crutch.

45:31.160 --> 45:34.080
 So there's geometric way of thinking about the world,

45:34.080 --> 45:39.000
 maybe holding us back if what we should instead be doing

45:39.000 --> 45:39.960
 in this robotics space,

45:39.960 --> 45:42.600
 in this particular space of autonomous vehicles

45:42.600 --> 45:46.520
 is using camera as a primary sensor

45:46.520 --> 45:48.280
 and using computer vision and machine learning

45:48.280 --> 45:49.760
 as the primary way to...

45:49.760 --> 45:50.600
 Okay, I have two comments.

45:50.600 --> 45:55.440
 I think first of all, we all know that people can drive cars

45:55.440 --> 45:56.920
 without lighters in their heads

45:56.920 --> 45:59.040
 because we only have eyes

45:59.040 --> 46:02.120
 and we mostly just use eyes for driving.

46:02.120 --> 46:04.600
 Maybe we use some other perception about our bodies,

46:04.600 --> 46:06.720
 accelerations, occasionally our ears,

46:08.040 --> 46:09.560
 certainly not our noses.

46:10.720 --> 46:12.480
 So the existence proof is there,

46:12.480 --> 46:14.640
 that eyes must be sufficient.

46:15.600 --> 46:17.960
 In fact, we could even drive a car

46:17.960 --> 46:19.480
 if someone put a camera out

46:19.480 --> 46:23.480
 and then gave us the camera image with no latency,

46:23.480 --> 46:26.400
 we would be able to drive a car that way the same way.

46:26.400 --> 46:28.800
 So a camera is also sufficient.

46:28.800 --> 46:31.840
 Secondly, I really love the idea that in the Western world,

46:31.840 --> 46:33.640
 we have many, many different people

46:33.640 --> 46:35.720
 trying different hypotheses.

46:35.720 --> 46:36.880
 It's almost like an anthill,

46:36.880 --> 46:39.600
 like if an anthill tries to forge for food, right?

46:39.600 --> 46:40.840
 You can sit there as two ants

46:40.840 --> 46:42.560
 and agree what the perfect path is

46:42.560 --> 46:44.040
 and then every single ant marches

46:44.040 --> 46:46.360
 for the most likely location of food is,

46:46.360 --> 46:48.000
 or you can even just spread out.

46:48.000 --> 46:50.480
 And I promise you the spread out solution will be better

46:50.480 --> 46:52.960
 because if they're discussing

46:52.960 --> 46:55.600
 philosophical intellectual ants get it wrong

46:55.600 --> 46:56.960
 and they're all moving the wrong direction,

46:56.960 --> 46:58.280
 they're gonna waste the day

46:58.280 --> 47:00.560
 and then they're gonna discuss again for another week.

47:00.560 --> 47:02.480
 Whereas if all these ants go in the right direction,

47:02.480 --> 47:04.280
 someone's gonna succeed and they're gonna come back

47:04.280 --> 47:06.840
 and claim victory and get the Nobel Prize

47:06.840 --> 47:08.720
 or whatever the ant equivalent is.

47:08.720 --> 47:10.560
 And then they all march in the same direction.

47:10.560 --> 47:11.840
 And that's great about society.

47:11.840 --> 47:13.200
 That's great about the Western society.

47:13.200 --> 47:15.520
 We're not plant based, we're not central based,

47:15.520 --> 47:18.880
 we don't have a Soviet Union style central government

47:18.880 --> 47:21.000
 that tells us where to forge.

47:21.000 --> 47:24.040
 We just forge, we started in the C Corp.

47:24.040 --> 47:25.840
 We get investor money, go out and try it out.

47:25.840 --> 47:27.520
 And who knows who's gonna win?

47:28.720 --> 47:29.560
 I like it.

47:30.680 --> 47:35.200
 When you look at the longterm vision of autonomous vehicles,

47:35.200 --> 47:37.840
 do you see machine learning as fundamentally

47:37.840 --> 47:39.600
 being able to solve most of the problems?

47:39.600 --> 47:42.280
 So learning from experience.

47:42.280 --> 47:44.240
 I'd say we should be very clear

47:44.240 --> 47:46.120
 about what machine learning is and is not.

47:46.120 --> 47:48.160
 And I think there's a lot of confusion.

47:48.160 --> 47:50.880
 What is today is a technology

47:50.880 --> 47:54.680
 that can go through large databases

47:54.680 --> 47:59.680
 of repetitive patterns and find those patterns.

48:00.880 --> 48:03.560
 So in example, we did a study at Stanford two years ago

48:03.560 --> 48:05.440
 where we applied machine learning

48:05.440 --> 48:07.880
 to detecting skin cancer in images.

48:07.880 --> 48:12.880
 And we harvested or built a data set of 129,000 skin

48:12.880 --> 48:15.800
 photo shots that were all had been biopsied

48:15.800 --> 48:18.240
 for what the actual situation was.

48:18.240 --> 48:21.320
 And those included melanomas and carcinomas,

48:21.320 --> 48:25.320
 also included rashes and other skin conditions, lesions.

48:26.320 --> 48:29.600
 And then we had a network find those patterns

48:29.600 --> 48:33.360
 and it was by and large able to then detect skin cancer

48:33.360 --> 48:37.360
 with an iPhone as accurately as the best board certified

48:37.360 --> 48:40.080
 Stanford level dermatologist.

48:40.080 --> 48:41.440
 We proved that.

48:41.440 --> 48:42.840
 We proved that.

48:42.840 --> 48:45.920
 Now this thing was great in this one thing

48:45.920 --> 48:48.600
 and finding skin cancer, but it couldn't drive a car.

48:49.720 --> 48:51.640
 So the difference to human intelligence

48:51.640 --> 48:53.320
 is we do all these many, many things

48:53.320 --> 48:56.760
 and we can often learn from a very small data set

48:56.760 --> 48:59.640
 of experiences where as machines still need

48:59.640 --> 49:03.360
 very large data sets and things that will be very repetitive.

49:03.360 --> 49:04.720
 Now that's still super impactful

49:04.720 --> 49:06.480
 because almost everything we do is repetitive.

49:06.480 --> 49:10.040
 So that's gonna really transform human labor.

49:10.040 --> 49:13.160
 But it's not this almighty general intelligence.

49:13.160 --> 49:15.320
 We're really far away from a system

49:15.320 --> 49:17.320
 that would exhibit general intelligence.

49:18.760 --> 49:21.360
 To that end, I actually commiserate the naming a little bit

49:21.360 --> 49:24.480
 because artificial intelligence, if you believe Hollywood,

49:24.480 --> 49:27.360
 is immediately mixed into the idea of human suppression

49:27.360 --> 49:30.400
 and machine superiority.

49:30.400 --> 49:33.000
 I don't think that we're gonna see this in my lifetime.

49:33.000 --> 49:35.520
 I don't think human suppression is a good idea.

49:36.480 --> 49:37.480
 I don't see it coming.

49:37.480 --> 49:39.760
 I don't see the technology being there.

49:39.760 --> 49:42.360
 What I see instead is a very pointed,

49:42.360 --> 49:44.360
 focused pattern recognition technology

49:44.360 --> 49:48.480
 that's able to extract patterns from large data sets.

49:48.480 --> 49:51.560
 And in doing so, it can be super impactful.

49:51.560 --> 49:53.560
 Super impactful.

49:53.560 --> 49:55.880
 Let's take the impact of artificial intelligence

49:55.880 --> 49:57.680
 on human work.

49:57.680 --> 50:00.600
 We all know that it takes something like 10,000 hours

50:00.600 --> 50:01.600
 to become an expert.

50:02.560 --> 50:04.280
 If you're gonna be a doctor or a lawyer

50:04.280 --> 50:06.360
 or even a really good driver,

50:06.360 --> 50:09.600
 it takes a certain amount of time to become experts.

50:09.600 --> 50:12.240
 Machines now are able and have been shown

50:12.240 --> 50:16.720
 to observe people become experts and observe experts

50:16.720 --> 50:18.480
 and then extract those rules from experts

50:18.480 --> 50:19.760
 in some interesting way.

50:19.760 --> 50:23.160
 They could go from law to sales,

50:23.160 --> 50:28.160
 to driving cars, to diagnosing cancer

50:29.280 --> 50:30.960
 and then giving that capability

50:30.960 --> 50:33.080
 to people who are completely new in their job.

50:33.080 --> 50:35.520
 We now can, and that's been done.

50:35.520 --> 50:38.520
 It's been done commercially in many, many instantiations.

50:38.520 --> 50:40.840
 That means we can use machine learning

50:40.840 --> 50:43.000
 to make people an expert

50:43.000 --> 50:45.520
 on their very first day of their work.

50:45.520 --> 50:46.560
 Like think about the impact.

50:46.560 --> 50:51.040
 If your doctor is still in their first 10,000 hours,

50:51.040 --> 50:53.800
 you have a doctor who's not quite an expert yet.

50:53.800 --> 50:55.240
 Who would not want a doctor

50:55.240 --> 50:57.360
 who's the world's best expert?

50:57.360 --> 50:59.200
 And now we can leverage machines

50:59.200 --> 51:02.720
 to really eradicate error in decision making,

51:02.720 --> 51:06.240
 error in lack of expertise for human doctors.

51:06.240 --> 51:08.360
 That could save your life.

51:08.360 --> 51:10.360
 If we can link on that for a little bit,

51:10.360 --> 51:14.800
 in which way do you hope machines in the medical field

51:14.800 --> 51:16.360
 could help assist doctors?

51:16.360 --> 51:21.360
 You mentioned this sort of accelerating the learning curve

51:21.360 --> 51:24.520
 or people, if they start a job

51:24.520 --> 51:27.360
 or in the first 10,000 hours can be assisted by machines.

51:27.360 --> 51:29.720
 How do you envision that assistance looking?

51:29.720 --> 51:32.320
 So we built this app for an iPhone

51:32.320 --> 51:36.320
 that can detect and classify and diagnose skin cancer.

51:36.320 --> 51:39.440
 And we proved two years ago

51:39.440 --> 51:40.920
 that it does pretty much as good

51:40.920 --> 51:42.200
 or better than the best human doctor.

51:42.200 --> 51:43.600
 So let me tell you a story.

51:43.600 --> 51:45.480
 So there's a friend of mine that's called Ben.

51:45.480 --> 51:47.680
 Ben is a very famous venture capitalist.

51:47.680 --> 51:50.720
 He goes to his doctor and the doctor looks at a mole

51:50.720 --> 51:55.360
 and says, hey, that mole is probably harmless.

51:55.360 --> 51:58.680
 And for some very funny reason,

51:58.680 --> 52:00.440
 he pulls out that phone with our app.

52:00.440 --> 52:02.640
 He's a collaborator in our study.

52:02.640 --> 52:04.640
 And the app says, no, no, no, no.

52:04.640 --> 52:06.320
 This is a melanoma.

52:06.320 --> 52:07.240
 And for background,

52:07.240 --> 52:10.800
 melanomas are skin cancer is the most common cancer

52:10.800 --> 52:12.400
 in this country.

52:12.400 --> 52:16.640
 Melanomas can go from stage zero to stage four

52:16.640 --> 52:18.120
 within less than a year.

52:18.120 --> 52:20.880
 Stage zero means you can basically cut it out yourself

52:20.880 --> 52:23.200
 with a kitchen knife and be safe.

52:23.200 --> 52:25.000
 And stage four means your chances

52:25.000 --> 52:28.000
 of leaving five more years than less than 20%.

52:28.000 --> 52:31.160
 So it's a very serious, serious, serious condition.

52:31.160 --> 52:36.160
 So this doctor who took out the iPhone

52:36.160 --> 52:37.680
 looked at the iPhone and was a little bit puzzled.

52:37.680 --> 52:39.720
 He said, I mean, what, just to be safe,

52:39.720 --> 52:41.600
 let's cut it out and biopsy it.

52:41.600 --> 52:45.440
 That's the technical term for let's get an in depth diagnostics

52:45.440 --> 52:47.720
 that is more than just looking at it.

52:47.720 --> 52:50.800
 And it came back as cancerous as a melanoma.

52:50.800 --> 52:52.240
 And it was then removed.

52:52.240 --> 52:54.960
 And my friend Ben, I was hiking with him

52:54.960 --> 52:56.280
 and we were talking about AI.

52:56.280 --> 52:58.880
 And he said, I'm talking to this vocal skin cancer.

52:58.880 --> 53:00.720
 And he said, oh, funny.

53:00.720 --> 53:03.800
 My doctor just had an iPhone that found my cancer.

53:05.480 --> 53:06.920
 So I was like completely intrigued.

53:06.920 --> 53:08.200
 I didn't even know about this.

53:08.200 --> 53:09.040
 So here's a person.

53:09.040 --> 53:11.640
 I mean, this is a real human life, right?

53:11.640 --> 53:13.520
 Now, who doesn't know somebody who has been affected

53:13.520 --> 53:14.360
 by cancer?

53:14.360 --> 53:16.160
 Cancer is cause of death number two.

53:16.160 --> 53:19.440
 Cancer is this kind of disease that is mean.

53:19.440 --> 53:21.960
 And in the following way, most cancers

53:21.960 --> 53:24.520
 can actually be cured relatively easily

53:24.520 --> 53:25.880
 if we catch them early.

53:25.880 --> 53:28.360
 And the reason why we don't tend to catch them early

53:28.360 --> 53:30.560
 is because they have no symptoms.

53:30.560 --> 53:33.840
 Like your very first symptom of a gallbladder cancer

53:33.840 --> 53:37.040
 or a pancreate cancer might be a headache.

53:37.040 --> 53:38.680
 And when you finally go to your doctor

53:38.680 --> 53:41.600
 because of these headaches or your back pain

53:41.600 --> 53:45.880
 and you're being imaged, it's usually stage four plus.

53:45.880 --> 53:48.200
 And that's the time when your curing chances

53:48.200 --> 53:50.880
 might be dropped to a single digital percentage.

53:50.880 --> 53:54.520
 So if you could leverage AI to inspect your body

53:54.520 --> 53:58.080
 on a regular basis without even a doctor in the room,

53:58.080 --> 54:00.360
 maybe when you take a shower or what have you,

54:00.360 --> 54:03.040
 I know this sounds creepy, but then we might be able

54:03.040 --> 54:05.040
 to save millions and millions of lives.

54:06.280 --> 54:09.440
 You've mentioned there's a concern that people have

54:09.440 --> 54:12.800
 about near term impacts of AI in terms of job loss.

54:12.800 --> 54:15.480
 So you've mentioned being able to assist doctors,

54:15.480 --> 54:17.880
 being able to assist people in their jobs.

54:17.880 --> 54:21.120
 Do you have a worry of people losing their jobs

54:22.240 --> 54:25.440
 or the economy being affected by the improvements in AI?

54:25.440 --> 54:27.680
 Yeah, anybody concerned about job losses,

54:27.680 --> 54:32.320
 please come to Udacity.com, we teach contemporary tech skills

54:32.320 --> 54:35.800
 and we have a kind of implicit job promise.

54:36.680 --> 54:40.400
 We often, when we measure, we spend way over 50%

54:40.400 --> 54:42.920
 of our graduates in new jobs and they're very satisfied

54:42.920 --> 54:44.800
 about it and it costs almost nothing

54:44.800 --> 54:47.120
 cause like 1,500 max or something like that.

54:47.120 --> 54:48.920
 And I saw there's a cool new program

54:48.920 --> 54:51.120
 that you agreed with the U.S. government

54:51.120 --> 54:54.880
 guaranteeing that you will help give scholarships

54:54.880 --> 54:57.840
 that educate people in this kind of situation.

54:57.840 --> 54:59.960
 Yeah, we're working with the U.S. government

54:59.960 --> 55:03.880
 on the idea of basically rebuilding the American dream.

55:03.880 --> 55:07.440
 So Udacity has just dedicated 100,000 scholarships

55:07.440 --> 55:12.080
 for citizens of America for various levels of courses

55:12.080 --> 55:15.640
 that eventually will get you a job.

55:16.720 --> 55:19.480
 And those courses all somewhat relate to the tech sector

55:19.480 --> 55:21.520
 because the tech sector is kind of the hottest sector

55:21.520 --> 55:25.000
 right now and they range from interlevel digital marketing

55:25.000 --> 55:28.120
 to very advanced self driving car engineering.

55:28.120 --> 55:29.480
 And we're doing this with the White House

55:29.480 --> 55:30.920
 because we think it's bipartisan.

55:30.920 --> 55:34.240
 It's an issue that is that if you wanna really make

55:34.240 --> 55:39.240
 America great, being able to be part of the solution

55:40.120 --> 55:43.840
 and live the American dream requires us to be proactive

55:43.840 --> 55:45.840
 about our education and our skillset.

55:45.840 --> 55:47.760
 It's just the way it is today.

55:47.760 --> 55:48.760
 And it's always been this way.

55:48.760 --> 55:50.000
 And we always had this American dream

55:50.000 --> 55:51.200
 to send our kids to college

55:51.200 --> 55:53.280
 and now the American dream has to be

55:53.280 --> 55:54.640
 to send ourselves to college.

55:54.640 --> 55:58.240
 We can do this very, very efficiently

55:58.240 --> 56:00.920
 and very, very, we can squeeze in in the evenings

56:00.920 --> 56:03.160
 and things to online at all ages.

56:03.160 --> 56:04.000
 All ages.

56:04.000 --> 56:09.000
 So our learners go from age 11 to age 80.

56:11.240 --> 56:14.120
 I just traveled Germany and the guy

56:14.120 --> 56:17.400
 in the train compartment next to me was one of my students.

56:17.400 --> 56:19.680
 It's like, wow, that's amazing.

56:19.680 --> 56:21.280
 I don't think about impact.

56:21.280 --> 56:23.320
 We've become the educator of choice

56:23.320 --> 56:25.520
 for now, I believe officially six countries

56:25.520 --> 56:27.360
 or five countries is the most in the Middle East

56:27.360 --> 56:30.080
 like Saudi Arabia and in Egypt.

56:30.080 --> 56:33.440
 In Egypt, we just had a cohort graduate

56:33.440 --> 56:37.280
 where we had 1100 high school students

56:37.280 --> 56:39.800
 that went through programming skills,

56:39.800 --> 56:42.920
 proficient at the level of a computer science undergrad.

56:42.920 --> 56:45.200
 And we had a 95% graduation rate

56:45.200 --> 56:46.280
 even though everything's online.

56:46.280 --> 56:48.240
 It's kind of tough, but we kind of trying to figure out

56:48.240 --> 56:50.120
 how to make this effective.

56:50.120 --> 56:52.520
 The vision is, the vision is very, very simple.

56:52.520 --> 56:57.520
 The vision is education ought to be a basic human right.

56:58.320 --> 57:02.320
 It cannot be locked up behind ivory tower walls

57:02.320 --> 57:04.400
 only for the rich people, for the parents

57:04.400 --> 57:06.920
 who might be bright themselves into the system

57:06.920 --> 57:09.240
 and only for young people and only for people

57:09.240 --> 57:11.880
 from the right demographics and the right geography

57:11.880 --> 57:14.200
 and possibly even the right race.

57:14.200 --> 57:15.800
 It has to be opened up to everybody.

57:15.800 --> 57:18.720
 If we are truthful to the human mission,

57:18.720 --> 57:20.640
 if we are truthful to our values,

57:20.640 --> 57:23.440
 we're gonna open up education to everybody in the world.

57:23.440 --> 57:27.200
 So Udacity's pledge of 100,000 scholarships,

57:27.200 --> 57:29.160
 I think is the biggest pledge of scholarships ever

57:29.160 --> 57:30.720
 in terms of numbers.

57:30.720 --> 57:33.000
 And we're working, as I said, with the White House

57:33.000 --> 57:36.040
 and with very accomplished CEOs like Tim Cook

57:36.040 --> 57:39.000
 from Apple and others to really bring education

57:39.000 --> 57:41.040
 to everywhere in the world.

57:41.040 --> 57:44.600
 Not to ask you to pick the favorite of your children,

57:44.600 --> 57:46.680
 but at this point. Oh, that's Jasper.

57:46.680 --> 57:48.840
 I only have one that I know of.

57:49.720 --> 57:50.560
 Okay, good.

57:52.680 --> 57:55.800
 In this particular moment, what nano good degree,

57:55.800 --> 58:00.040
 what set of courses are you most excited about Udacity

58:00.040 --> 58:02.000
 or is that too impossible to pick?

58:02.000 --> 58:03.800
 I've been super excited about something

58:03.800 --> 58:05.480
 we haven't launched yet in the building,

58:05.480 --> 58:09.120
 which is when we talk to our partner companies,

58:09.120 --> 58:12.680
 we have now a very strong footing in the enterprise world.

58:12.680 --> 58:14.560
 In order to our students,

58:14.560 --> 58:17.240
 we've kind of always focused on these hard skills

58:17.240 --> 58:19.720
 like the programming skills or math skills

58:19.720 --> 58:22.200
 or building skills or design skills.

58:22.200 --> 58:25.160
 And a very common ask is soft skills.

58:25.160 --> 58:26.880
 Like how do you behave in your work?

58:26.880 --> 58:28.280
 How do you develop empathy?

58:28.280 --> 58:29.600
 How do you work in a team?

58:30.440 --> 58:32.400
 What are the very basics of management?

58:32.400 --> 58:33.680
 How do you do time management?

58:33.680 --> 58:36.240
 How do you advance your career

58:36.240 --> 58:39.280
 in the context of a broader community?

58:39.280 --> 58:41.400
 And that's something that we haven't done

58:41.400 --> 58:43.840
 very well at Udacity, and I would say most universities

58:43.840 --> 58:45.160
 are doing very poorly as well

58:45.160 --> 58:47.880
 because we're so obsessed with individual test scores

58:47.880 --> 58:49.480
 and so little,

58:49.480 --> 58:52.600
 pays a little attention to teamwork in education.

58:52.600 --> 58:55.480
 So that's something I see us moving into as a company

58:55.480 --> 58:56.920
 because I'm excited about this.

58:56.920 --> 59:00.120
 And I think, look, we can teach people tech skills

59:00.120 --> 59:00.960
 and they're gonna be great.

59:00.960 --> 59:02.720
 But if you teach people empathy,

59:02.720 --> 59:04.960
 that's gonna have the same impact.

59:04.960 --> 59:08.720
 Maybe harder than self driving cars, but I don't think so.

59:08.720 --> 59:11.320
 I think the rules are really simple.

59:11.320 --> 59:14.400
 You just have to, you have to want to engage.

59:14.400 --> 59:18.200
 It's, we literally went in school and in K through 12,

59:18.200 --> 59:20.480
 we teach kids like get the highest math score.

59:20.480 --> 59:22.920
 And if you are a rational human being,

59:22.920 --> 59:25.640
 you might evolve from this education, say,

59:25.640 --> 59:28.080
 having the best math score and the best English scores,

59:28.080 --> 59:29.680
 making me the best leader.

59:29.680 --> 59:31.080
 And it turns out not to be the case.

59:31.080 --> 59:34.360
 It's actually really wrong because making the,

59:34.360 --> 59:35.840
 first of all, in terms of math scores,

59:35.840 --> 59:37.680
 I think it's perfectly fine to hire somebody

59:37.680 --> 59:38.520
 with great math skills.

59:38.520 --> 59:40.640
 You don't have to do it yourself.

59:40.640 --> 59:42.760
 You can't hire some of the great empathy for you.

59:42.760 --> 59:43.880
 That's much harder,

59:43.880 --> 59:46.360
 but it can always hire some of the great math skills.

59:46.360 --> 59:49.000
 But we live in a fluent world

59:49.000 --> 59:51.040
 where we constantly deal with other people

59:51.040 --> 59:51.920
 and that's a beauty.

59:51.920 --> 59:53.360
 It's not a nuisance, it's a beauty.

59:53.360 --> 59:55.960
 So if we somehow develop that muscle

59:55.960 --> 59:59.920
 that we can do that well and empower others

59:59.920 --> 1:00:00.920
 in the workplace,

1:00:00.920 --> 1:00:02.920
 I think we're gonna be super successful.

1:00:02.920 --> 1:00:07.280
 And I know many fellow robot assistant computer scientists

1:00:07.280 --> 1:00:09.840
 that I will insist to take this course.

1:00:09.840 --> 1:00:12.200
 Not to be named you.

1:00:12.200 --> 1:00:13.760
 Not to be named.

1:00:13.760 --> 1:00:17.960
 Many, many years ago, 1903,

1:00:17.960 --> 1:00:22.600
 the Wright Brothers flew in Kitty Hawk for the first time.

1:00:22.600 --> 1:00:26.920
 And you've launched a company of the same name, Kitty Hawk,

1:00:26.920 --> 1:00:31.920
 with the dream of building flying cars, EV Talls.

1:00:32.320 --> 1:00:34.560
 So at the big picture,

1:00:34.560 --> 1:00:36.640
 what are the big challenges of making this thing

1:00:36.640 --> 1:00:40.000
 that actually inspired generations of people

1:00:40.000 --> 1:00:41.760
 about what the future looks like?

1:00:41.760 --> 1:00:42.600
 What does it take?

1:00:42.600 --> 1:00:43.680
 What are the biggest challenges?

1:00:43.680 --> 1:00:47.240
 So flying cars has always been a dream.

1:00:47.240 --> 1:00:49.720
 Every boy, every girl wants to fly.

1:00:49.720 --> 1:00:51.040
 Let's be honest.

1:00:51.040 --> 1:00:52.360
 And let's go back in our history

1:00:52.360 --> 1:00:53.800
 of your dreaming of flying.

1:00:53.800 --> 1:00:57.440
 I think honestly, my single most remembered childhood dream

1:00:57.440 --> 1:00:59.440
 has been a dream where I was sitting on a pillow

1:00:59.440 --> 1:01:00.760
 and I could fly.

1:01:00.760 --> 1:01:02.080
 I was like five years old.

1:01:02.080 --> 1:01:04.160
 I remember like maybe three dreams of my childhood,

1:01:04.160 --> 1:01:07.560
 but that's the one that we remember most vividly.

1:01:07.560 --> 1:01:09.400
 And then Peter Thiel famously said,

1:01:09.400 --> 1:01:10.720
 they promised us flying cars

1:01:10.720 --> 1:01:12.840
 and they gave us 140 characters,

1:01:12.840 --> 1:01:15.280
 pointing as Twitter at the time,

1:01:15.280 --> 1:01:18.400
 limited message size to 140 characters.

1:01:18.400 --> 1:01:20.240
 So we're coming back now to really go

1:01:20.240 --> 1:01:23.280
 for these super impactful stuff like flying cars.

1:01:23.280 --> 1:01:25.920
 And to be precise, they're not really cars.

1:01:25.920 --> 1:01:27.200
 They don't have wheels.

1:01:27.200 --> 1:01:28.640
 They're actually much closer to a helicopter

1:01:28.640 --> 1:01:29.680
 than anything else.

1:01:29.680 --> 1:01:32.120
 They take off vertically in their flight horizontally,

1:01:32.120 --> 1:01:34.400
 but they have important differences.

1:01:34.400 --> 1:01:37.760
 One difference is that they are much quieter.

1:01:37.760 --> 1:01:41.600
 We just released a vehicle called Project Heavy Sight

1:01:41.600 --> 1:01:43.560
 that can fly over you as low as a helicopter

1:01:43.560 --> 1:01:45.240
 and you basically can't hear.

1:01:45.240 --> 1:01:46.720
 It's like 38 decibels.

1:01:46.720 --> 1:01:49.280
 It's like, if you were inside the library,

1:01:49.280 --> 1:01:50.240
 you might be able to hear it,

1:01:50.240 --> 1:01:53.040
 but anywhere outdoors, your ambient noise is higher.

1:01:54.600 --> 1:01:57.080
 Secondly, they're much more affordable.

1:01:57.080 --> 1:01:59.000
 They're much more affordable than helicopters.

1:01:59.000 --> 1:02:01.960
 And the reason is helicopters are expensive

1:02:01.960 --> 1:02:03.080
 for many reasons.

1:02:04.440 --> 1:02:07.040
 There's lots of single point of figures in a helicopter.

1:02:07.040 --> 1:02:09.160
 There's a bolt between the blades

1:02:09.160 --> 1:02:10.840
 that's caused Jesus bolt.

1:02:10.840 --> 1:02:13.080
 And the reason why it's called Jesus bolt is

1:02:13.080 --> 1:02:16.400
 if this bolt breaks, you will die.

1:02:16.400 --> 1:02:19.560
 There is no second solution in helicopter flight.

1:02:19.560 --> 1:02:21.520
 Whereas we have these distributed mechanism.

1:02:21.520 --> 1:02:23.760
 When you go from gasoline to electric,

1:02:23.760 --> 1:02:25.880
 you can now have many, many, many small motors

1:02:25.880 --> 1:02:27.360
 as opposed to one big motor.

1:02:27.360 --> 1:02:28.840
 And that means if you lose one of those motors,

1:02:28.840 --> 1:02:29.680
 not a big deal.

1:02:29.680 --> 1:02:31.360
 Heavy Sight, if it loses a motor,

1:02:31.360 --> 1:02:32.840
 it has eight of those.

1:02:32.840 --> 1:02:34.040
 If it loses one of those eight motors,

1:02:34.040 --> 1:02:35.200
 so it's seven left,

1:02:35.200 --> 1:02:37.320
 you can take off just like before

1:02:37.320 --> 1:02:38.880
 and land just like before.

1:02:40.160 --> 1:02:42.080
 We are now also moving into a technology

1:02:42.080 --> 1:02:44.200
 that doesn't require a commercial pilot.

1:02:44.200 --> 1:02:45.560
 Because in some level,

1:02:45.560 --> 1:02:49.000
 flight is actually easier than ground transportation.

1:02:49.000 --> 1:02:51.440
 Like in self driving cars,

1:02:51.440 --> 1:02:54.520
 the world is full of like children and bicycles

1:02:54.520 --> 1:02:57.600
 and other cars and mailboxes and curbs and shrubs

1:02:57.600 --> 1:02:58.440
 and whatever you,

1:02:58.440 --> 1:03:00.520
 all these things you have to avoid.

1:03:00.520 --> 1:03:03.760
 When you go above the buildings and tree lines,

1:03:03.760 --> 1:03:04.600
 there's nothing there.

1:03:04.600 --> 1:03:06.120
 I mean, you can do the test right now,

1:03:06.120 --> 1:03:09.440
 look outside and count the number of things you see flying.

1:03:09.440 --> 1:03:11.480
 I'd be shocked if you could see more than two things.

1:03:11.480 --> 1:03:12.840
 It's probably just zero.

1:03:13.840 --> 1:03:16.960
 In the Bay Area, the most I've ever seen was six.

1:03:16.960 --> 1:03:20.400
 And maybe it's 15 or 20, but not 10,000.

1:03:20.400 --> 1:03:24.000
 So the sky is very ample and very empty and very free.

1:03:24.000 --> 1:03:25.160
 So the vision is,

1:03:25.160 --> 1:03:29.960
 can we build a socially acceptable mass transit solution

1:03:29.960 --> 1:03:34.280
 for daily transportation that is affordable?

1:03:34.280 --> 1:03:36.320
 And we have an existence proof.

1:03:36.320 --> 1:03:39.800
 Heavy sites can fly 100 miles in range

1:03:39.800 --> 1:03:43.280
 with still 30% electric reserves.

1:03:43.280 --> 1:03:46.080
 It can fly up to like 180 miles an hour.

1:03:46.080 --> 1:03:48.880
 We know that that solution at scale

1:03:48.880 --> 1:03:52.640
 would make your ground transportation 10 times as fast

1:03:52.640 --> 1:03:57.520
 as a car based on use sensors or statistics data,

1:03:57.520 --> 1:04:01.960
 which means we would take your 300 hours of daily commute

1:04:01.960 --> 1:04:05.160
 down to 30 hours and give you 270 hours back.

1:04:05.160 --> 1:04:07.640
 Who wouldn't want, I mean, who doesn't hate traffic?

1:04:07.640 --> 1:04:10.760
 Like I hate, give me the person who doesn't hate traffic.

1:04:10.760 --> 1:04:13.920
 I hate traffic every time I'm in traffic, I hate it.

1:04:13.920 --> 1:04:17.520
 And if we could free the world from traffic,

1:04:17.520 --> 1:04:20.000
 we have technology, we can free the world from traffic.

1:04:20.000 --> 1:04:21.320
 We have the technology.

1:04:21.320 --> 1:04:23.040
 It's there, we have an existence proof.

1:04:23.040 --> 1:04:25.400
 It's not a technological problem anymore.

1:04:25.400 --> 1:04:29.320
 Do you think there is a future where tens of thousands,

1:04:29.320 --> 1:04:34.360
 maybe hundreds of thousands of both delivery drones

1:04:34.360 --> 1:04:39.920
 and flying cars of this kind, EV talls, fill the sky?

1:04:39.920 --> 1:04:40.920
 I absolutely believe this.

1:04:40.920 --> 1:04:43.600
 And there's obviously the societal acceptance

1:04:43.600 --> 1:04:46.920
 is a major question and of course safety is.

1:04:46.920 --> 1:04:49.520
 I believe in safety, we only exceed ground transportation

1:04:49.520 --> 1:04:52.840
 safety, as has happened for aviation already,

1:04:52.840 --> 1:04:54.480
 commercial aviation.

1:04:54.480 --> 1:04:56.880
 And in terms of acceptance, I think

1:04:56.880 --> 1:04:58.280
 one of the key things is noise.

1:04:58.280 --> 1:05:00.920
 That's why we are focusing relentlessly on noise

1:05:00.920 --> 1:05:05.600
 and we built perhaps the crisis electric V tall vehicle

1:05:05.600 --> 1:05:07.600
 ever built.

1:05:07.600 --> 1:05:09.720
 The nice thing about the sky is it's three dimensional.

1:05:09.720 --> 1:05:11.880
 So any mathematician will immediately

1:05:11.880 --> 1:05:13.400
 recognize the difference between 1D

1:05:13.400 --> 1:05:17.240
 of like a regular highway to 3D of a sky.

1:05:17.240 --> 1:05:20.200
 But to make it clear for the layman,

1:05:20.200 --> 1:05:23.880
 say you want to make 100 vertical lanes of highway 101

1:05:23.880 --> 1:05:26.160
 in San Francisco, because you believe building

1:05:26.160 --> 1:05:28.920
 a hundred vertical lanes is the right solution.

1:05:28.920 --> 1:05:30.720
 Imagine how much it would cost to stack

1:05:30.720 --> 1:05:33.360
 100 vertical lanes physically onto 101.

1:05:33.360 --> 1:05:34.280
 They would be prohibitive.

1:05:34.280 --> 1:05:37.720
 They would be consuming the world's GDP for an entire year

1:05:37.720 --> 1:05:39.160
 just for one highway.

1:05:39.160 --> 1:05:41.200
 It's amazingly expensive.

1:05:41.200 --> 1:05:43.640
 In the sky, it would just be a recompilation

1:05:43.640 --> 1:05:46.520
 of a piece of software because all these lanes are virtual.

1:05:46.520 --> 1:05:49.880
 That means any vehicle that is in conflict with another vehicle

1:05:49.880 --> 1:05:51.800
 would just go to different altitudes

1:05:51.800 --> 1:05:53.280
 and the conflict is gone.

1:05:53.280 --> 1:05:55.560
 And if you don't believe this, that's

1:05:55.560 --> 1:05:58.520
 exactly how commercial aviation works.

1:05:58.520 --> 1:06:01.400
 When you fly from New York to San Francisco,

1:06:01.400 --> 1:06:04.200
 another plane flies from San Francisco to New York,

1:06:04.200 --> 1:06:06.720
 there are different altitudes so they don't hit each other.

1:06:06.720 --> 1:06:10.320
 It's a solved problem for the jet space.

1:06:10.320 --> 1:06:12.680
 And it will be a solved problem for the urban space.

1:06:12.680 --> 1:06:15.280
 There's companies like Google Wing and Amazon

1:06:15.280 --> 1:06:17.000
 working on very innovative solutions.

1:06:17.000 --> 1:06:18.520
 How do we have space management?

1:06:18.520 --> 1:06:20.160
 They use exactly the same principles

1:06:20.160 --> 1:06:23.280
 as we use today to route today's jets.

1:06:23.280 --> 1:06:25.920
 There's nothing hard about this.

1:06:25.920 --> 1:06:28.960
 Do you envision autonomy being a key part of it

1:06:28.960 --> 1:06:35.400
 so that the flying vehicles are either semi autonomous

1:06:35.400 --> 1:06:36.920
 or fully autonomous?

1:06:36.920 --> 1:06:37.880
 100% autonomous.

1:06:37.880 --> 1:06:40.440
 You don't want idiots like me flying in the sky.

1:06:40.440 --> 1:06:41.960
 I promise you.

1:06:41.960 --> 1:06:46.000
 And if you have 10,000, watch the movie, The Fifth Element,

1:06:46.000 --> 1:06:49.480
 to get a pee for what would happen if it's not autonomous.

1:06:49.480 --> 1:06:51.720
 And a centralized, that's a really interesting idea

1:06:51.720 --> 1:06:56.320
 of a centralized management system for lanes and so on.

1:06:56.320 --> 1:07:00.280
 So actually just being able to have

1:07:00.280 --> 1:07:03.000
 similar as we have in the current commercial aviation,

1:07:03.000 --> 1:07:05.560
 but scale it up to much more vehicles.

1:07:05.560 --> 1:07:07.680
 That's a really interesting optimization problem.

1:07:07.680 --> 1:07:11.120
 It is mathematically very, very straightforward.

1:07:11.120 --> 1:07:13.560
 Like the gap we leave between jets is gargantuanous.

1:07:13.560 --> 1:07:16.440
 And part of the reason is there isn't that many jets.

1:07:16.440 --> 1:07:18.840
 So it just feels like a good solution.

1:07:18.840 --> 1:07:22.360
 Today, when you get vectored by air traffic control,

1:07:22.360 --> 1:07:23.920
 someone talks to you.

1:07:23.920 --> 1:07:26.960
 So an ATC controller might have up to maybe 20 planes

1:07:26.960 --> 1:07:28.200
 on the same frequency.

1:07:28.200 --> 1:07:29.160
 And then they talk to you.

1:07:29.160 --> 1:07:30.360
 You have to talk back.

1:07:30.360 --> 1:07:32.720
 And it feels right because there isn't more than 20 planes

1:07:32.720 --> 1:07:34.920
 around anyhow, so you can talk to everybody.

1:07:34.920 --> 1:07:36.720
 But if there's 20,000 things around,

1:07:36.720 --> 1:07:38.120
 you can't talk to everybody anymore.

1:07:38.120 --> 1:07:40.240
 So we have to do something that's called digital,

1:07:40.240 --> 1:07:42.120
 like text messaging.

1:07:42.120 --> 1:07:43.040
 We do have solutions.

1:07:43.040 --> 1:07:45.520
 Like we have what, four, five billion smartphones

1:07:45.520 --> 1:07:47.720
 in the world now, and they're all connected.

1:07:47.720 --> 1:07:50.720
 And some of us solve the scale problem for smartphones.

1:07:50.720 --> 1:07:51.960
 We know where they all are.

1:07:51.960 --> 1:07:53.600
 They can talk to somebody.

1:07:53.600 --> 1:07:54.880
 And they're very reliable.

1:07:54.880 --> 1:07:56.480
 They're amazingly reliable.

1:07:56.480 --> 1:08:00.080
 We could use the same system, the same scale,

1:08:00.080 --> 1:08:01.080
 for air traffic control.

1:08:01.080 --> 1:08:04.080
 So instead of me as a pilot talking to a human being

1:08:04.080 --> 1:08:06.800
 in the middle of the conversation receiving

1:08:06.800 --> 1:08:09.680
 a new frequency, like how ancient is that,

1:08:09.680 --> 1:08:13.480
 we could digitize the stuff and digitally transmit

1:08:13.480 --> 1:08:15.280
 the right flight coordinates.

1:08:15.280 --> 1:08:18.680
 And that solution will automatically scale to 10,000

1:08:18.680 --> 1:08:20.080
 vehicles.

1:08:20.080 --> 1:08:22.480
 We talked about empathy a little bit.

1:08:22.480 --> 1:08:25.840
 Do you think we'll one day build an AI system

1:08:25.840 --> 1:08:28.000
 that a human being can love and that

1:08:28.000 --> 1:08:31.360
 loves that human back, like in the movie Her?

1:08:31.360 --> 1:08:33.960
 Look, I'm a pragmatist.

1:08:33.960 --> 1:08:35.640
 For me, AI is a tool.

1:08:35.640 --> 1:08:37.080
 It's like a shovel.

1:08:37.080 --> 1:08:39.400
 And the ethics of using the shovel

1:08:39.400 --> 1:08:41.880
 are always with us, the people.

1:08:41.880 --> 1:08:44.240
 And it has to be this way.

1:08:44.240 --> 1:08:49.880
 In terms of emotions, I would hate to come into my kitchen

1:08:49.880 --> 1:08:54.240
 and see that my refrigerator spoiled all my food,

1:08:54.240 --> 1:08:56.560
 then have it explained to me that it fell in love

1:08:56.560 --> 1:08:58.000
 with a dishwasher.

1:08:58.000 --> 1:08:59.680
 And I wasn't as nice as the dishwasher.

1:08:59.680 --> 1:09:02.200
 So as a result, it neglected me.

1:09:02.200 --> 1:09:05.160
 That would just be a bad experience.

1:09:05.160 --> 1:09:07.080
 And it would be a bad product.

1:09:07.080 --> 1:09:09.560
 I would probably not recommend this refrigerator

1:09:09.560 --> 1:09:11.760
 to my friends.

1:09:11.760 --> 1:09:13.040
 And that's where I draw the line.

1:09:13.040 --> 1:09:16.280
 To me, technology has to be reliable

1:09:16.280 --> 1:09:17.360
 and has to be predictable.

1:09:17.360 --> 1:09:19.520
 I want my car to work.

1:09:19.520 --> 1:09:22.480
 I don't want to fall in love with my car.

1:09:22.480 --> 1:09:24.320
 I just want it to work.

1:09:24.320 --> 1:09:26.840
 I want it to complement me, not to replace me.

1:09:26.840 --> 1:09:30.280
 I have very unique human properties.

1:09:30.280 --> 1:09:35.360
 And I want the machines to make me turn me into a superhuman.

1:09:35.360 --> 1:09:37.440
 Like, I'm already a superhuman today,

1:09:37.440 --> 1:09:38.960
 thanks to the machines that surround me.

1:09:38.960 --> 1:09:40.440
 And I'll give you examples.

1:09:40.440 --> 1:09:45.360
 I can run across the Atlantic near the speed of sound

1:09:45.360 --> 1:09:48.120
 at 36,000 feet today.

1:09:48.120 --> 1:09:49.240
 That's kind of amazing.

1:09:49.240 --> 1:09:54.320
 I can, my voice now carries me all the way to Australia

1:09:54.320 --> 1:09:56.280
 using a smartphone today.

1:09:56.280 --> 1:09:59.720
 And it's not the speed of sound, which would take hours.

1:09:59.720 --> 1:10:00.960
 It's the speed of light.

1:10:00.960 --> 1:10:03.480
 My voice travels at the speed of light.

1:10:03.480 --> 1:10:04.280
 How cool is that?

1:10:04.280 --> 1:10:05.960
 That makes me superhuman.

1:10:05.960 --> 1:10:10.280
 I would even argue my flushing toilet makes me superhuman.

1:10:10.280 --> 1:10:13.560
 Just think of the time before flushing toilets.

1:10:13.560 --> 1:10:16.280
 And maybe you have a very old person in your family

1:10:16.280 --> 1:10:18.240
 that you can ask about this.

1:10:18.240 --> 1:10:23.160
 Or take a trip to rural India to experience it.

1:10:23.160 --> 1:10:25.600
 It makes me superhuman.

1:10:25.600 --> 1:10:28.680
 So to me, what technology does, it complements me.

1:10:28.680 --> 1:10:30.680
 It makes me stronger.

1:10:30.680 --> 1:10:37.520
 Therefore, words like love and compassion have very little interest

1:10:37.520 --> 1:10:38.400
 in this for machines.

1:10:38.400 --> 1:10:40.560
 I have interest in people.

1:10:40.560 --> 1:10:44.080
 You don't think, first of all, beautifully put,

1:10:44.080 --> 1:10:45.480
 beautifully argued.

1:10:45.480 --> 1:10:50.280
 But do you think love has use in our tools, compassion?

1:10:50.280 --> 1:10:53.120
 I think love is a beautiful human concept.

1:10:53.120 --> 1:10:55.240
 And if you think of what love really is,

1:10:55.240 --> 1:11:03.080
 love is a means to convey safety, to convey trust.

1:11:03.080 --> 1:11:07.280
 I think trust has a huge need in technology as well,

1:11:07.280 --> 1:11:09.040
 not just people.

1:11:09.040 --> 1:11:13.120
 We want to trust our technology in a similar way

1:11:13.120 --> 1:11:15.840
 we trust people.

1:11:15.840 --> 1:11:19.240
 In human interaction, standards have emerged.

1:11:19.240 --> 1:11:22.400
 And feelings, emotions have emerged, maybe genetically,

1:11:22.400 --> 1:11:24.960
 maybe ideologically, that are able to convey

1:11:24.960 --> 1:11:26.400
 a sense of trust, sense of safety,

1:11:26.400 --> 1:11:28.760
 a sense of passion, of love, of dedication.

1:11:28.760 --> 1:11:30.680
 That makes the human fabric.

1:11:30.680 --> 1:11:33.600
 And I'm a big slacker for love.

1:11:33.600 --> 1:11:34.480
 I want to be loved.

1:11:34.480 --> 1:11:35.240
 I want to be trusted.

1:11:35.240 --> 1:11:36.720
 I want to be admired.

1:11:36.720 --> 1:11:38.760
 All these wonderful things.

1:11:38.760 --> 1:11:42.080
 And because all of us, we have this beautiful system,

1:11:42.080 --> 1:11:44.800
 I wouldn't just blindly copy this to the machines.

1:11:44.800 --> 1:11:46.120
 Here's why.

1:11:46.120 --> 1:11:49.280
 When you look at, say, transportation,

1:11:49.280 --> 1:11:54.480
 you could have observed that up to the end of the 19th century,

1:11:54.480 --> 1:11:58.080
 almost all transportation used any number of legs,

1:11:58.080 --> 1:12:01.600
 from one leg to two legs to 1,000 legs.

1:12:01.600 --> 1:12:03.800
 And you could have concluded that is the right way

1:12:03.800 --> 1:12:06.680
 to move about the environment.

1:12:06.680 --> 1:12:08.920
 We've made the exception of birds who is flapping wings.

1:12:08.920 --> 1:12:10.800
 In fact, there are many people in aviation

1:12:10.800 --> 1:12:13.680
 that flap wings to their arms and jump from cliffs.

1:12:13.680 --> 1:12:16.920
 Most of them didn't survive.

1:12:16.920 --> 1:12:19.880
 Then the interesting thing is that the technology solutions

1:12:19.880 --> 1:12:21.560
 are very different.

1:12:21.560 --> 1:12:23.840
 Like, in technology, it's really easy to build a wheel.

1:12:23.840 --> 1:12:25.680
 In biology, it's super hard to build a wheel.

1:12:25.680 --> 1:12:30.040
 There's very few perpetually rotating things in biology.

1:12:30.040 --> 1:12:34.160
 And they usually run cells and things.

1:12:34.160 --> 1:12:37.200
 In engineering, we can build wheels.

1:12:37.200 --> 1:12:41.280
 And those wheels gave rise to cars.

1:12:41.280 --> 1:12:44.360
 Similar wheels gave rise to aviation.

1:12:44.360 --> 1:12:46.760
 Like, there's no thing that flies that

1:12:46.760 --> 1:12:48.800
 wouldn't have something that rotates,

1:12:48.800 --> 1:12:52.400
 like a jet engine or helicopter blades.

1:12:52.400 --> 1:12:55.480
 So the solutions have used very different physical laws

1:12:55.480 --> 1:12:56.480
 than nature.

1:12:56.480 --> 1:12:58.040
 And that's great.

1:12:58.040 --> 1:13:00.400
 So for me to be too much focused on, oh, this

1:13:00.400 --> 1:13:03.360
 is how nature does it, let's just replicate it,

1:13:03.360 --> 1:13:06.160
 if you really believed that the solution to the agricultural

1:13:06.160 --> 1:13:09.200
 evolution was a humanoid robot, you would still

1:13:09.200 --> 1:13:10.920
 be waiting today.

1:13:10.920 --> 1:13:14.680
 Again, beautifully put, you said that you don't take yourself

1:13:14.680 --> 1:13:15.920
 too seriously.

1:13:15.920 --> 1:13:18.160
 You don't say that?

1:13:18.160 --> 1:13:19.160
 You want me to say that?

1:13:19.160 --> 1:13:19.640
 Maybe.

1:13:19.640 --> 1:13:20.960
 You don't take me seriously.

1:13:20.960 --> 1:13:21.560
 I'm not.

1:13:21.560 --> 1:13:22.800
 Yeah, that's right.

1:13:22.800 --> 1:13:23.280
 Good.

1:13:23.280 --> 1:13:23.960
 You're right.

1:13:23.960 --> 1:13:24.480
 I don't want to.

1:13:24.480 --> 1:13:25.720
 I just made that up.

1:13:25.720 --> 1:13:29.120
 But you have a humor and a likeness about life

1:13:29.120 --> 1:13:33.480
 that I think is beautiful and inspiring to a lot of people.

1:13:33.480 --> 1:13:35.040
 Where does that come from?

1:13:35.040 --> 1:13:38.400
 The smile, the humor, the likeness

1:13:38.400 --> 1:13:42.560
 amidst all the chaos of the hard work that you're in.

1:13:42.560 --> 1:13:43.640
 Where does that come from?

1:13:43.640 --> 1:13:44.560
 I just love my life.

1:13:44.560 --> 1:13:45.960
 I love the people around me.

1:13:45.960 --> 1:13:49.720
 I love, I'm just so glad to be alive.

1:13:49.720 --> 1:13:52.320
 Like, I'm, what, 52?

1:13:52.320 --> 1:13:53.640
 How to believe?

1:13:53.640 --> 1:13:55.480
 People say 52 is a new 51.

1:13:55.480 --> 1:13:58.520
 So now I feel better.

1:13:58.520 --> 1:14:03.280
 But in looking around the world, looking,

1:14:03.280 --> 1:14:06.160
 just go back 200, 300 years.

1:14:06.160 --> 1:14:09.320
 Humanity is what, 300,000 years old.

1:14:09.320 --> 1:14:13.960
 But for the first 300,000 years minus the last 100,

1:14:13.960 --> 1:14:18.320
 our life expectancy would have been plus or minus 30 years,

1:14:18.320 --> 1:14:20.240
 roughly, give or take.

1:14:20.240 --> 1:14:23.480
 So I would be long dead now.

1:14:23.480 --> 1:14:26.800
 Like, that makes me just enjoy every single day of my life.

1:14:26.800 --> 1:14:28.040
 Because I don't deserve this.

1:14:28.040 --> 1:14:32.480
 Like, why am I born today when so many of my ancestors

1:14:32.480 --> 1:14:34.520
 died of horrible deaths?

1:14:34.520 --> 1:14:39.880
 Like, famines, massive wars that ravaged Europe

1:14:39.880 --> 1:14:43.160
 for the last 1,000 years, mystically disappeared

1:14:43.160 --> 1:14:46.520
 after World War II when the Americans and the Allies

1:14:46.520 --> 1:14:48.280
 did something amazing to my country

1:14:48.280 --> 1:14:51.440
 that didn't deserve it, the country of Germany.

1:14:51.440 --> 1:14:52.600
 This is so amazing.

1:14:52.600 --> 1:14:56.960
 And then when you're alive and feel this every day,

1:14:56.960 --> 1:15:02.040
 then it's just so amazing what we can accomplish,

1:15:02.040 --> 1:15:03.520
 what we can do.

1:15:03.520 --> 1:15:06.840
 We live in a world that is so incredibly

1:15:06.840 --> 1:15:09.840
 changing every day, almost everything

1:15:09.840 --> 1:15:12.920
 that we cherish from your smartphone

1:15:12.920 --> 1:15:16.240
 to your flushing toilet, to all these basic inventions,

1:15:16.240 --> 1:15:19.640
 your new clothes you're wearing, your watch, your plane,

1:15:19.640 --> 1:15:25.800
 penicillin, I don't know, anesthesia for surgery,

1:15:25.800 --> 1:15:30.080
 penicillin, have been invented in the last 150 years.

1:15:30.080 --> 1:15:32.400
 So in the last 150 years, something magical happened.

1:15:32.400 --> 1:15:34.920
 And I would trace it back to Gutenberg and the printing

1:15:34.920 --> 1:15:37.680
 press that has been able to disseminate information

1:15:37.680 --> 1:15:40.160
 more efficiently than before, that all of a sudden we

1:15:40.160 --> 1:15:43.840
 were able to invent agriculture and nitrogen

1:15:43.840 --> 1:15:46.360
 fertilization that made agriculture so much more

1:15:46.360 --> 1:15:48.800
 potent that we didn't have to work with farms anymore.

1:15:48.800 --> 1:15:50.160
 And we could start reading and writing,

1:15:50.160 --> 1:15:52.840
 and we could become all these wonderful things we are today,

1:15:52.840 --> 1:15:56.800
 from airline pilot to massage therapist to software engineer.

1:15:56.800 --> 1:16:00.960
 It's just amazing, living in that time is such a blessing.

1:16:00.960 --> 1:16:04.440
 We should sometimes really think about this.

1:16:04.440 --> 1:16:07.360
 Steven Pinker, who is a very famous author and philosopher

1:16:07.360 --> 1:16:09.920
 whom I really adore, wrote a great book called Enlightenment

1:16:09.920 --> 1:16:11.920
 Now, and that's maybe the one book I would recommend.

1:16:11.920 --> 1:16:14.080
 And he asked the question, if there

1:16:14.080 --> 1:16:17.200
 was only a single article written in the 20th century,

1:16:17.200 --> 1:16:19.120
 only one article, what would it be?

1:16:19.120 --> 1:16:21.160
 What's the most important innovation,

1:16:21.160 --> 1:16:23.120
 the most important thing that happened?

1:16:23.120 --> 1:16:27.080
 And he would say this article would credit a guy named Carl Bosch.

1:16:27.080 --> 1:16:30.360
 And I'd challenge anybody, have you ever heard of the name Carl

1:16:30.360 --> 1:16:31.240
 Bosch?

1:16:31.240 --> 1:16:33.000
 I haven't.

1:16:33.000 --> 1:16:35.440
 There's a Bosch corporation in Germany,

1:16:35.440 --> 1:16:38.480
 but it's not associated with Carl Bosch.

1:16:38.480 --> 1:16:39.920
 So I looked it up.

1:16:39.920 --> 1:16:42.720
 Carl Bosch invented nitrogen fertilization.

1:16:42.720 --> 1:16:47.560
 And in doing so, together with an older invention of irrigation,

1:16:47.560 --> 1:16:50.880
 was able to increase the yield per agricultural land

1:16:50.880 --> 1:16:57.720
 by a factor of 26, so a 2,500% increase in fertility of land.

1:16:57.720 --> 1:17:00.560
 And that, so Steve Pinker argues,

1:17:00.560 --> 1:17:04.680
 saved over 2 billion lives today, 2 billion people

1:17:04.680 --> 1:17:08.440
 who would be dead if this man hadn't done what he had done.

1:17:08.440 --> 1:17:12.200
 Think about that impact and what that means to society.

1:17:12.200 --> 1:17:14.200
 That's the way I look at the world.

1:17:14.200 --> 1:17:16.960
 I mean, it's so amazing to be alive and to be part of this.

1:17:16.960 --> 1:17:21.360
 And I'm so glad I lived after Carl Bosch and not before.

1:17:21.360 --> 1:17:23.520
 I don't think there's a better way to end this.

1:17:23.520 --> 1:17:25.480
 Sebastian, it's an honor to talk to you,

1:17:25.480 --> 1:17:27.400
 to have had the chance to learn from you.

1:17:27.400 --> 1:17:28.800
 Thank you so much for talking to us.

1:17:28.800 --> 1:17:29.800
 Thanks for coming out.

1:17:29.800 --> 1:17:31.000
 It's a real pleasure.

1:17:31.000 --> 1:17:34.400
 Thank you for listening to this conversation with Sebastian Thrun.

1:17:34.400 --> 1:17:37.480
 And thank you to our presenting sponsor, Cash App.

1:17:37.480 --> 1:17:40.240
 Download it, use code LEX Podcast.

1:17:40.240 --> 1:17:41.520
 You'll get $10.

1:17:41.520 --> 1:17:44.920
 And $10 will go to first, a STEM education nonprofit

1:17:44.920 --> 1:17:47.480
 that inspires hundreds of thousands of young minds

1:17:47.480 --> 1:17:50.560
 to learn and to dream of engineering our future.

1:17:50.560 --> 1:17:53.360
 If you enjoy this podcast, subscribe on YouTube,

1:17:53.360 --> 1:17:56.640
 get 5 stars on Apple Podcast, support on Patreon,

1:17:56.640 --> 1:17:58.840
 or connect with me on Twitter.

1:17:58.840 --> 1:18:01.280
 And now, let me leave you with some words of wisdom

1:18:01.280 --> 1:18:03.280
 from Sebastian Thrun.

1:18:03.280 --> 1:18:05.400
 It's important to celebrate your failures

1:18:05.400 --> 1:18:07.720
 as much as your successes.

1:18:07.720 --> 1:18:09.800
 If you celebrate your failures really well,

1:18:09.800 --> 1:18:13.920
 if you say, wow, I failed, I tried, I was wrong,

1:18:13.920 --> 1:18:15.600
 but I learned something,

1:18:15.600 --> 1:18:18.280
 then you realize you have no fear.

1:18:18.280 --> 1:18:22.520
 And when your fear goes away, you can move the world.

1:18:22.520 --> 1:18:44.480
 Thank you for listening and hope to see you next time.

