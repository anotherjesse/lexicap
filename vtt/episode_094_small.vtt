WEBVTT

00:00.000 --> 00:05.360
 The following is a conversation with Ilya Satskeva, cofounder and chief scientist of Open AI,

00:06.080 --> 00:12.800
 one of the most cited computer scientists in history with over 165,000 citations,

00:13.440 --> 00:19.040
 and to me, one of the most brilliant and insightful minds ever in the field of deep learning.

00:19.920 --> 00:24.240
 There are very few people in this world who I would rather talk to and brainstorm with about

00:24.240 --> 00:31.920
 deep learning, intelligence, and life in general than Ilya, on and off the mic. This was an honor

00:31.920 --> 00:37.120
 and a pleasure. This conversation was recorded before the outbreak of the pandemic,

00:37.120 --> 00:41.360
 for everyone feeling the medical, psychological, and financial burden of this crisis,

00:41.360 --> 00:46.160
 I'm sending love your way. Stay strong, we're in this together, we'll beat this thing.

00:47.120 --> 00:51.680
 This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,

00:51.680 --> 00:56.880
 review it with 5 stars on Apple Podcasts, support on Patreon, or simply connect with me on Twitter

00:56.880 --> 01:03.440
 at Lex Freedman, spelled F R I D M A N. As usual, I'll do a few minutes of ads now and never any

01:03.440 --> 01:07.840
 ads in the middle that can break the flow of the conversation. I hope that works for you

01:07.840 --> 01:13.840
 and doesn't hurt the listening experience. This show is presented by Cash App, the number one

01:13.840 --> 01:20.000
 finance app in the App Store. When you get it, use code Lex Podcast. Cash App lets you send money

01:20.000 --> 01:26.480
 to friends, buy Bitcoin, invest in the stock market with as little as $1. Since Cash App allows you

01:26.480 --> 01:31.680
 to buy Bitcoin, let me mention that cryptocurrency in the context of the history of money is

01:31.680 --> 01:37.520
 fascinating. I recommend Ascent of Money as a great book on this history. Both the book and

01:37.520 --> 01:44.640
 audiobook are great. Debits and credits on ledgers started around 30,000 years ago. The US dollar

01:44.640 --> 01:50.800
 created over 200 years ago and Bitcoin, the first decentralized cryptocurrency released just over

01:50.800 --> 01:56.800
 10 years ago. So given that history, cryptocurrency is still very much in its early days of development,

01:56.800 --> 02:03.440
 but it's still aiming to just might redefine the nature of money. So again, if you get Cash App

02:03.440 --> 02:10.400
 from the App Store, Google Play and use the code Lex Podcast, you get $10 and Cash App will also

02:10.400 --> 02:15.920
 donate $10 to FIRST, an organization that is helping advance robotics and STEM education

02:15.920 --> 02:22.160
 for young people around the world. And now, here's my conversation with Ilya Setskever.

02:23.280 --> 02:30.000
 You were one of the three authors with Alex Kyshevsky, Jeff Hinton of the famed AlexNet paper

02:30.000 --> 02:36.000
 that is arguably the paper that marked the big catalytic moment that launched the deep learning

02:36.000 --> 02:42.160
 revolution. At that time, take us back to that time. What was your intuition about neural networks,

02:42.160 --> 02:47.840
 about the representational power of neural networks? And maybe you could mention how did

02:47.840 --> 02:54.480
 that evolve over the next few years, up to today, over the 10 years? Yeah, I can answer that question.

02:55.120 --> 03:03.200
 At some point in about 2010 or 2011, I connected two facts in my mind. Basically,

03:03.200 --> 03:11.680
 the realization was this. At some point, we realized that we can train very large, I shouldn't say very

03:11.680 --> 03:17.280
 you know, tiny by today's standards, but large and deep neural networks end to end with back

03:17.280 --> 03:23.760
 propagation. At some point, different people obtained this result. I obtained this result.

03:23.760 --> 03:29.200
 The first, the first moment in which I realized that deep neural networks are powerful was when

03:29.200 --> 03:36.240
 James Martens invented the Hessian free optimizer in 2010. And he trained a 10 layer neural network

03:36.240 --> 03:43.840
 end to end without pre training from scratch. And when that happened, I thought this is it.

03:43.840 --> 03:48.560
 Because if you can train a big neural network, a big neural network can represent very complicated

03:48.560 --> 03:54.480
 function. Because if you have a neural network with 10 layers, it says, though, you allow the

03:54.480 --> 04:02.480
 human brain to run for some number of milliseconds, neuron firings are slow. And so in maybe 100

04:02.480 --> 04:07.120
 milliseconds, your neurons only fire 10 times. So it's also kind of like 10 layers. And in 100

04:07.120 --> 04:12.720
 milliseconds, you can perfectly recognize any object. So I thought, so I already had the idea

04:12.720 --> 04:18.480
 then that we need to train a very big neural network on lots of supervised data. And then it

04:18.480 --> 04:22.560
 must succeed, because we can find the best neural network. And then there's also theory

04:22.560 --> 04:26.720
 that if you have more data than parameters, you won't overfit. Today, we know that actually,

04:26.720 --> 04:30.240
 this theory is very incomplete, and you won't overfit even if you have less data than parameters.

04:30.240 --> 04:33.200
 But definitely, if you have more data than parameters, you won't overfit.

04:33.200 --> 04:38.960
 So the fact that neural networks were heavily overparameterized, wasn't discouraging to you.

04:38.960 --> 04:42.880
 So you were thinking about the theory that the number of parameters,

04:42.880 --> 04:45.840
 the fact there's a huge number of parameters is okay. Is it going to be okay?

04:45.840 --> 04:49.360
 I mean, there was some evidence before that it was okay, but the theory was most,

04:49.360 --> 04:52.880
 the theory was that if you had a big data set and a big neural net, it was going to work.

04:52.880 --> 04:57.280
 The overparameterization just didn't really figure much as a problem. I thought, well,

04:57.280 --> 05:00.240
 with images, you're just going to add some data augmentation, and it's going to be okay.

05:00.240 --> 05:04.320
 So where was any doubt coming from? The main doubt was can we train a big,

05:04.320 --> 05:07.440
 really have enough compute to train a big enough neural net with back propagation.

05:07.440 --> 05:11.120
 Back propagation, I thought was would work. The thing which wasn't clear would was whether

05:11.120 --> 05:14.720
 there would be enough compute to get a very convincing result. And then at some point,

05:14.720 --> 05:19.040
 Alex Krzewski wrote these insanely fast UDA kernels for training convolutional neural nets,

05:19.040 --> 05:22.640
 and that was bam, let's do this. Let's get image net, and it's going to be the greatest thing.

05:23.280 --> 05:29.520
 Was your intuition, most of your intuition from empirical results by you and by others?

05:29.520 --> 05:34.560
 So like just actually demonstrating that a piece of program can train a 10 layer neural network?

05:34.560 --> 05:40.400
 Or was there some pen and paper or marker and whiteboard thinking intuition?

05:41.600 --> 05:46.080
 Because you just connected a 10 layer large neural network to the brain. So you just mentioned

05:46.080 --> 05:51.840
 the brain. So in your intuition about neural networks, does the human brain come into play

05:51.840 --> 05:57.360
 as an intuition builder? Definitely. I mean, you know, you got to be precise with these analogies

05:57.360 --> 06:02.720
 between neural artificial neural networks and the brain. But there's no question that the brain

06:02.720 --> 06:08.400
 is a huge source of intuition and inspiration for deep learning researchers since all the way

06:08.400 --> 06:14.240
 from Rosenblatt in the 60s. Like if you look at the whole idea of a neural network is directly

06:14.240 --> 06:18.640
 inspired by the brain. You had people like McCallum and Pitts who were saying, hey, you got these

06:20.080 --> 06:24.240
 neurons in the brain. And hey, we recently learned about the computer and automata.

06:24.240 --> 06:28.240
 Can we use some ideas from the computer and automata to design some kind of computational

06:28.240 --> 06:33.280
 object that's going to be simple, computational, and kind of like the brain, and they invented the

06:33.280 --> 06:37.280
 neuron. So they were inspired by it back then. Then you had the convolutional neural network

06:37.280 --> 06:42.000
 from Fukushima, and then later young Lacan, who said, hey, if you limit the receptive fields of

06:42.000 --> 06:46.800
 a neural network, it's going to be especially suitable for images as it turned out to be true.

06:46.800 --> 06:52.240
 So there was a very small number of examples where analogies to the brain were successful.

06:52.240 --> 06:56.720
 And then I thought, well, probably an artificial neuron is not that different from the brain if

06:56.720 --> 07:02.160
 it's quite hard enough. So let's just assume it is and roll with it. So we're now at a time where

07:02.160 --> 07:09.600
 deep learning is very successful. So let us squint less and say, let's open our eyes and say, what

07:09.600 --> 07:14.960
 to you is an interesting difference between the human brain? Now, I know you're probably not an

07:14.960 --> 07:19.920
 expert, neither in your scientists and your biologists, but loosely speaking, what's the

07:19.920 --> 07:24.000
 difference between the human brain and artificial neural networks? That's interesting to you for

07:24.000 --> 07:28.880
 the next decade or two. That's a good question to ask. What is an interesting difference between

07:28.880 --> 07:33.920
 the neural between the brain and our artificial neural networks? So I feel like today,

07:33.920 --> 07:39.920
 artificial neural networks, so we all agree that there are certain dimensions in which the human

07:39.920 --> 07:45.200
 brain vastly outperforms our models. But I also think that there are some ways in which artificial

07:45.200 --> 07:51.360
 neural networks have a number of very important advantages over the brain. Looking at the advantages

07:51.360 --> 07:56.640
 versus disadvantages is a good way to figure out what is the important difference. So the brain

07:57.360 --> 08:01.680
 uses spikes, which may or may not be important. Yes, that's a really interesting question. Do you

08:01.680 --> 08:07.200
 think it's important or not? That's one big architectural difference between artificial

08:07.200 --> 08:13.520
 neural networks. It's hard to tell, but my prior is not very high. And I can say why. There are

08:13.520 --> 08:17.920
 people who are interested in spiking neural networks. And basically, what they figured out is

08:17.920 --> 08:23.200
 that they need to simulate the non spiking neural networks in spikes. And that's how they're going

08:23.200 --> 08:27.280
 to make them work. If you don't simulate the non spiking neural networks in spikes, it's not going

08:27.280 --> 08:30.720
 to work because the question is, why should it work? And that connects to questions around back

08:30.720 --> 08:37.200
 propagation and questions around deep learning. You've got this giant neural network. Why should

08:37.200 --> 08:44.560
 it work at all? Why should that learning rule work at all? It's not a self evident question,

08:44.560 --> 08:48.000
 especially if you, let's say if you were just starting in the field and you read the very

08:48.000 --> 08:54.400
 early papers, you can say, Hey, people are saying, let's build neural networks. That's a great idea

08:54.400 --> 08:58.480
 because the brain is a neural network. So it would be useful to build neural networks. Now,

08:58.480 --> 09:02.640
 let's figure out how to train them. It should be possible to train them probably, but how?

09:03.360 --> 09:11.200
 And so the big idea is the cost function. That's the big idea. The cost function is a way of measuring

09:11.200 --> 09:16.560
 the performance of the system according to some measure. By the way, that is a big, actually,

09:16.560 --> 09:22.640
 let me think. Is that, is that a one, a difficult idea to arrive at? And how big of an idea is that

09:22.640 --> 09:31.360
 that there's a single cost function? Sorry, let me take a pause. Is supervised learning a difficult

09:31.360 --> 09:37.040
 concept to come to? I don't know. All concepts are very easy in retrospect. Yeah, that's what it

09:37.040 --> 09:42.000
 seems trivial now, but I, because the reason I asked that, and we'll talk about it, because is there

09:42.000 --> 09:48.080
 other things? Is there things that don't necessarily have a cost function, maybe have many cost

09:48.080 --> 09:54.000
 functions, or maybe have dynamic cost functions, or maybe a totally different kind of architectures?

09:54.000 --> 09:58.560
 Because we have to think like that in order to arrive at something new, right? So the only,

09:58.560 --> 10:02.000
 so the good examples of things which don't have clear cost functions are GANs.

10:03.840 --> 10:07.200
 Right. And again, you have a game. So instead of thinking of a cost function,

10:08.080 --> 10:11.920
 where you want to optimize, where you know that you have an algorithm gradient descent,

10:11.920 --> 10:16.240
 which will optimize the cost function. And then you can reason about the behavior of your system

10:16.240 --> 10:21.680
 in terms of what it optimizes. With GAN, you say, I have a game, and I'll reason about the behavior

10:21.680 --> 10:25.600
 of the system in terms of the equilibrium of the game. But it's all about coming up with these

10:25.600 --> 10:30.640
 mathematical objects that help us reason about the behavior of our system. Right. That's really

10:30.640 --> 10:35.600
 interesting. Yeah. So GAN is the only one. It's kind of a, the cost function is emergent from the

10:35.600 --> 10:40.160
 comparison. I don't know if it has a cost function. I don't know if it's meaningful to talk about the

10:40.160 --> 10:44.160
 cost function of a GAN. It's kind of like the cost function of biological evolution or the cost

10:44.160 --> 10:51.840
 function of the economy. It's, you can talk about regions to which it will go towards, but I don't

10:51.840 --> 10:58.560
 think, I don't think the cost function analogy is the most useful. So evolution doesn't,

10:59.120 --> 11:03.920
 that's really interesting. So if evolution doesn't really have a cost function, like a cost function

11:03.920 --> 11:11.840
 based on it's something akin to our mathematical conception of a cost function, then do you think

11:11.840 --> 11:17.520
 cost functions in deep learning are holding us back? Yeah. So you just kind of mentioned that

11:17.520 --> 11:23.680
 cost function is a nice first profound idea. Do you think that's a good idea? Do you think it's

11:23.680 --> 11:30.160
 an idea will go past? So self play starts to touch on that a little bit in reinforcement

11:30.160 --> 11:35.840
 learning systems. That's right. Self play and also ideas around exploration where you're trying to

11:35.840 --> 11:41.120
 take action. That's surprise a predictor. I'm a big fan of cost functions. I think cost functions

11:41.120 --> 11:44.560
 are great and they serve us really well. And I think that whenever we can do things with

11:44.560 --> 11:50.240
 cost functions, we should. And you know, maybe there is a chance that we will come up with some

11:50.240 --> 11:55.440
 yet another profound way of looking at things that will involve cost functions in a less central way.

11:55.440 --> 11:57.280
 But I don't know. I think cost functions are I mean,

11:59.840 --> 12:04.640
 I would not bet against against cost functions. Is there other things about the brain

12:04.640 --> 12:11.200
 that pop into your mind that might be different and interesting for us to consider in designing

12:11.200 --> 12:16.880
 artificial neural networks? So we talked about spiking a little bit. I mean, one thing which may

12:16.880 --> 12:20.960
 potentially be useful, I think people, neuroscientists figured out something about the learning rule of

12:20.960 --> 12:25.120
 the brain, or I'm talking about spike time independent plasticity. And it would be nice

12:25.120 --> 12:30.160
 if some people were to study that in simulation. Wait, sorry, spike time independent plasticity.

12:30.160 --> 12:36.640
 Yeah, that's that STD. It's a particular learning rule that uses spike timing to figure out how to

12:36.640 --> 12:42.800
 determine how to update the synapses. So it's kind of like, if a synapse fires into the neuron before

12:42.800 --> 12:47.440
 the neuron fires, then it's strengthened the synapse. And if the synapse fires into the

12:47.440 --> 12:52.080
 neuron shortly after the neuron fired, then it becomes the synapse something along this line.

12:52.080 --> 12:58.320
 I'm 90% sure it's right. So if I said something wrong here, don't don't get too angry.

12:58.320 --> 13:03.360
 But you saw me brilliant while saying it. But the timing, that's one thing that's missing.

13:04.080 --> 13:10.000
 The temporal dynamics is not captured. I think that's like a fundamental property of the brain,

13:10.000 --> 13:14.400
 is the timing of the signals. Well, you're recording neural networks.

13:15.280 --> 13:21.920
 But you think of that as, I mean, that's a very crude simplified, what's that called?

13:21.920 --> 13:29.600
 There's a clock, I guess, to recurrent neural networks. This seems like the brain is the

13:29.600 --> 13:35.440
 general, the continuous version of that, the generalization where all possible timings are

13:35.440 --> 13:41.520
 possible. And then within those timings is contained some information. You think recurrent neural

13:41.520 --> 13:48.160
 networks, the recurrence in recurrent neural networks can capture the same kind of phenomena

13:48.160 --> 13:55.680
 as the timing that seems to be important for the brain in the firing of neurons in the brain?

13:55.680 --> 14:04.160
 I mean, I think recurrent neural networks are amazing. And I think they can do anything we'd

14:04.160 --> 14:09.760
 want a system to do. Right now, recurrent neural networks have been superseded by

14:09.760 --> 14:13.920
 transformers, but maybe one day they'll make a comeback, maybe it'll be back. We'll see.

14:13.920 --> 14:20.720
 Let me, in a small tangent, say, do you think they'll be back? So so much of the breakthroughs

14:20.720 --> 14:26.080
 recently that we'll talk about on natural language processing and language modeling has been with

14:26.640 --> 14:32.320
 transformers that don't emphasize recurrence. Do you think recurrence will make a comeback?

14:33.200 --> 14:39.120
 Well, some kind of recurrence, I think, very likely. Recurrent neural networks, as they're

14:39.120 --> 14:43.600
 typically thought of for processing sequences, I think it's also possible.

14:43.600 --> 14:49.520
 What is, to you, a recurrent neural network? And generally speaking, I guess, what is a

14:49.520 --> 14:53.440
 recurrent neural network? You have a neural network which maintains a high dimensional

14:53.440 --> 14:59.200
 hidden state. And then when an observation arrives, it updates its high dimensional hidden state

14:59.200 --> 15:06.960
 through its connections in some way. So do you think, you know, that's what like expert systems

15:06.960 --> 15:15.920
 did, right? Symbolic AI, the knowledge based, growing a knowledge base is maintaining a

15:15.920 --> 15:20.320
 hidden state, which is its knowledge base and is growing it by sequentially processing. Do you

15:20.320 --> 15:28.320
 think of it more generally in that way? Or is it simply, is it the more constrained form of

15:29.120 --> 15:33.520
 a hidden state with certain kind of gating units that we think of as today with LSDMs and that?

15:33.520 --> 15:38.000
 I mean, the hidden state is technically what you described there, the hidden state that goes

15:38.000 --> 15:43.040
 inside the LSDM or the RNN or something like this. But then what should be contained, you know,

15:43.040 --> 15:49.440
 if you want to make the expert system analogy, I'm not, I mean, you could say that the knowledge

15:49.440 --> 15:55.040
 is stored in the connections and then the short term processing is done in the hidden state.

15:56.160 --> 16:02.720
 Yes. Could you say that? So sort of, do you think there's a future of building large scale

16:02.720 --> 16:05.920
 knowledge bases within the neural networks? Definitely.

16:08.880 --> 16:14.080
 So we're going to pause on that confidence because I want to explore that. But let me zoom back out

16:14.080 --> 16:21.200
 and ask back to the history of ImageNet. Neural networks have been around for many decades,

16:21.200 --> 16:26.720
 as you mentioned. What do you think were the key ideas that led to their success, that ImageNet

16:26.720 --> 16:33.840
 moment and beyond the success in the past 10 years? Okay, so the question is to make sure I

16:33.840 --> 16:39.280
 didn't miss anything. The key ideas that led to the success of deep learning over the past 10 years.

16:39.280 --> 16:44.720
 Exactly. Even though the fundamental thing behind deep learning has been around for much longer.

16:44.720 --> 16:55.920
 So the key idea about deep learning or rather the key fact about deep learning before deep learning

16:55.920 --> 17:02.800
 started to be successful is that it was underestimated. People who worked in machine learning

17:02.800 --> 17:08.320
 simply didn't think that neural networks could do much. People didn't believe that large neural

17:08.320 --> 17:14.320
 networks could be trained. People thought that, well, there was lots of, there was a lot of debate

17:14.320 --> 17:19.200
 going on in machine learning about what are the right methods and so on. And people were arguing

17:19.200 --> 17:25.440
 because there was no way to get hard facts. And by that, I mean, there were no benchmarks which

17:25.440 --> 17:31.360
 were truly hard, that if you do really well on them, then you can say, look, here's my system.

17:32.400 --> 17:38.240
 That's when you switch from, that's when this field becomes a little bit more of an engineering

17:38.240 --> 17:43.360
 field. So in terms of deep learning, to answer the question directly, the ideas were all there.

17:43.360 --> 17:47.600
 The thing that was missing was a lot of supervised data and a lot of compute.

17:49.600 --> 17:53.600
 Once you have a lot of supervised data and a lot of compute, then there is a third thing which is

17:53.600 --> 17:59.520
 needed as well. And that is conviction, conviction that if you take the right stuff, which already

17:59.520 --> 18:04.640
 exists, and apply and mixed with a lot of data and a lot of compute, that it will in fact work.

18:05.840 --> 18:11.440
 And so that was the missing piece. It was you had the, you needed the data, you needed the compute

18:11.440 --> 18:16.560
 which showed up in terms of GPUs, and you needed the conviction to realize that you need to mix

18:16.560 --> 18:23.600
 them together. So that's really interesting. So I guess the presence of compute and the presence

18:23.600 --> 18:30.720
 supervised data allowed the empirical evidence to do the convincing of the majority of the computer

18:30.720 --> 18:40.160
 science community. So I guess there's a key moment with Jitendra Malik and Alex Alyosha Efros,

18:40.160 --> 18:46.480
 who were very skeptical, right? And then there's a Jeffrey Hinton that was the opposite of skeptical.

18:46.480 --> 18:50.080
 And there was a convincing moment. And I think Emission had served as that moment.

18:50.080 --> 18:55.760
 That's right. And they represented this kind of, or the big pillars of computer vision community,

18:55.760 --> 19:01.600
 kind of the wizards got together. And then all of a sudden there was a shift. And

19:02.960 --> 19:06.320
 it's not enough for the ideas to all be there and the computer to be there. It's

19:06.320 --> 19:12.960
 for it to convince the cynicism that existed. That's interesting. That people just didn't

19:12.960 --> 19:20.480
 believe for a couple of decades. Yeah. Well, but it's more than that. It's kind of, when put this

19:20.480 --> 19:24.880
 way, it sounds like, well, you know, those silly people who didn't believe what were they, what

19:24.880 --> 19:28.720
 were they missing. But in reality, things were confusing because neural networks really did

19:28.720 --> 19:32.640
 not work on anything. And they were not the best method on pretty much anything as well.

19:32.640 --> 19:37.600
 Well, and it was pretty rational to say, yeah, this stuff doesn't have any traction.

19:39.520 --> 19:44.800
 And that's why you need to have these very hard tasks, which are, which produce undeniable evidence.

19:44.800 --> 19:48.480
 And that's how we make progress. And that's why the field is making progress today,

19:48.480 --> 19:53.840
 because we have these hard benchmarks, which represent true progress. And so, and this is

19:53.840 --> 20:00.800
 why we were able to avoid endless debate. So incredibly, you've contributed some of the

20:00.800 --> 20:07.600
 biggest recent ideas in AI in computer vision, language, natural language processing, reinforcement

20:07.600 --> 20:15.840
 learning, sort of everything in between, maybe not GANs. There may not be a topic you haven't

20:15.840 --> 20:21.520
 touched. And of course, the fundamental science of deep learning. What is the difference to you

20:21.520 --> 20:28.160
 between vision, language, and as in reinforcement learning, action, as learning problems,

20:28.160 --> 20:32.400
 and what are the commonalities? Do you see them as all interconnected? Are they fundamentally

20:32.400 --> 20:39.520
 different domains that require different approaches? Okay, that's a good question.

20:39.520 --> 20:42.960
 Machine learning is a field with a lot of unity, a huge amount of unity.

20:44.000 --> 20:47.280
 What do you mean by unity? Like overlap of ideas?

20:48.240 --> 20:52.560
 Overlap of ideas, overlap of principles. In fact, there's only one or two or three principles,

20:52.560 --> 20:58.160
 which are very, very simple. And then they apply in almost the same way, in almost the

20:58.160 --> 21:02.480
 same way to the different modalities to the different problems. And that's why today,

21:02.480 --> 21:07.040
 when someone writes a paper on improving optimization of deep learning and vision,

21:07.040 --> 21:10.400
 it improves the different NLP applications, and it improves the different reinforcement

21:10.400 --> 21:16.640
 learning applications. Reinforcement learning. So I would say that computer vision and NLP are

21:16.640 --> 21:22.080
 very similar to each other. Today, they differ in that they have slightly different architectures.

21:22.080 --> 21:26.320
 We use transformers in NLP, and we use convolutional neural networks in vision.

21:26.320 --> 21:30.560
 But it's also possible that one day this will change and everything will be unified with a

21:30.560 --> 21:35.120
 single architecture. Because if you go back a few years ago in natural language processing,

21:36.400 --> 21:41.840
 there were a huge number of architectures for every different tiny problem had its own architecture.

21:43.200 --> 21:48.960
 Today, there's just one transformer for all those different tasks. And if you go back in time even

21:48.960 --> 21:54.320
 more, you had even more and more fragmentation and every little problem in AI had its own

21:54.320 --> 21:59.040
 little subspecialization and sub, you know, little set of collection of skills, people who would

21:59.040 --> 22:03.120
 know how to engineer the features. Now it's all been subsumed by deep learning. We have this

22:03.120 --> 22:08.640
 unification. And so I expect vision to become unified with natural language as well. Or rather,

22:08.640 --> 22:12.800
 I just expect I think it's possible. I don't want to be too sure because I think on the

22:12.800 --> 22:17.600
 convolutional neural net is very computationally efficient. Arrel is different. Arrel does require

22:17.600 --> 22:21.920
 slightly different techniques because you really do need to take action. You really do need to do

22:21.920 --> 22:27.040
 something about exploration, your variance is much higher. But I think there is a lot of unity

22:27.040 --> 22:30.400
 even there. And I would expect, for example, that at some point, there will be some

22:32.400 --> 22:36.560
 broader unification between Arrel and supervised learning where somehow the Arrel will be making

22:36.560 --> 22:41.200
 decisions to make the supervised learning go better. And it will be, I imagine one big black

22:41.200 --> 22:45.440
 box and you just throw every, you know, you shovel, shovel things into it. And it just

22:45.440 --> 22:49.200
 figures out what to do with whatever you shovel in it. I mean, reinforcement learning has

22:49.760 --> 22:56.560
 some aspects of language and vision combined, almost. There's elements of a long term

22:57.200 --> 23:03.200
 memory that you should be utilizing. And there's elements of a really rich sensory space. So it

23:03.200 --> 23:09.040
 seems like the, it's like the union of the two or something like that. I'd say something

23:09.040 --> 23:14.720
 slightly differently. I'd say that reinforcement learning is neither, but it naturally interfaces

23:14.720 --> 23:19.600
 and integrates with the two of them. Do you think action is fundamentally different? So yeah,

23:19.600 --> 23:26.800
 what is interesting about what is unique about policy of learning to act? Well, so one example,

23:26.800 --> 23:32.400
 for instance, is that when you learn to act, you are fundamentally in a non stationary world.

23:33.120 --> 23:38.240
 Because as your actions change, the things you see start changing. You,

23:39.440 --> 23:43.040
 you experience the world in a different way. And this is not the case for

23:43.040 --> 23:47.040
 the more traditional static problem where you have some distribution and you just apply a model to

23:47.040 --> 23:52.800
 that distribution. Do you think it's a fundamentally different problem or is it just a more difficult

23:53.840 --> 23:58.560
 it's a generalization of the problem of understanding? I mean, it's a question of

23:58.560 --> 24:02.880
 definitions almost. There is a huge amount of commonality for sure. You take gradients,

24:02.880 --> 24:06.480
 you take gradients, we try to approximate gradients in both cases. In some case,

24:06.480 --> 24:10.960
 in the case of reinforcement learning, you have some tools to reduce the variance of the gradients.

24:10.960 --> 24:16.080
 You do that. There's lots of commonalities, the same neural net in both cases,

24:16.080 --> 24:18.240
 you compute the gradient, you apply Adam in both cases.

24:20.640 --> 24:25.120
 So I mean, there's lots in common for sure, but there are some small

24:26.160 --> 24:30.800
 differences which are not completely insignificant. It's really just a matter of your point of view,

24:30.800 --> 24:36.400
 what frame of reference you what how much do you want to zoom in or out as you look at these

24:36.400 --> 24:42.000
 problems? Which problem do you think is harder? So people like Noam Chomsky believe that language

24:42.000 --> 24:47.920
 is fundamental to everything. So it underlies everything. Do you think language understanding

24:47.920 --> 24:54.480
 is harder than visual scene understanding or vice versa? I think that asking if a problem is hard

24:54.480 --> 24:58.400
 is slightly wrong. I think the question is a little bit wrong and I want to explain why.

24:58.400 --> 25:06.800
 Okay. So what does it mean for a problem to be hard? Okay, the non interesting dumb answer to

25:06.800 --> 25:14.000
 that is there's a benchmark and there's a human level performance on that benchmark. And how

25:15.120 --> 25:20.160
 is the effort required to reach the human level? Okay, benchmark. So from the perspective of how

25:20.160 --> 25:28.080
 much until we get to human level on a very good benchmark. Yeah, like some I understand what you

25:28.080 --> 25:32.720
 mean by that. So what I was going to say that a lot of it depends on, you know, once you solve a

25:32.720 --> 25:37.680
 problem, it stops being hard. And that's, that's always true. And so but something is hard or not

25:37.680 --> 25:44.000
 depends on what our tools can do today. So you know, you say today, through human level, language

25:44.000 --> 25:49.520
 understanding and visual perception are hard in the sense that there is no way of solving the

25:49.520 --> 25:54.160
 problem completely in the next three months. Right. So I agree with that statement. Beyond

25:54.160 --> 25:59.120
 that, I'm just I'd be my guess would be as good as yours. I don't know. Okay, so you don't have a

25:59.120 --> 26:04.160
 fundamental intuition about how hard language understanding is. I think I not change my mind.

26:04.160 --> 26:09.280
 I'd say language is probably going to be hard. I mean, it depends on how you define it. Like if

26:09.280 --> 26:15.360
 you mean absolute top notch 100% language understanding, I'll go with language. And so

26:16.000 --> 26:20.720
 but then if I show you a piece of paper with letters on it, is that you see what I mean?

26:20.720 --> 26:26.000
 So you have a vision system, you say it's the best human level vision system. I show you I open

26:26.000 --> 26:31.360
 a book, and I show you letters. Will it understand how these letters form into word and sentences

26:31.360 --> 26:35.200
 and meaning is this part of the vision problem? Where does vision and the language begin?

26:36.000 --> 26:40.480
 Yeah, so Chomsky would say it starts at language. So vision is just a little example of the kind of

26:41.840 --> 26:48.080
 structure and, you know, fundamental hierarchy of ideas that's already represented in our brain

26:48.080 --> 26:56.080
 somehow that's represented through language. But where does vision stop and language begin?

26:57.840 --> 27:03.200
 That's a really interesting question.

27:07.680 --> 27:11.600
 So one possibility is that it's impossible to achieve really deep understanding

27:11.600 --> 27:17.680
 in either images or language without basically using the same kind of system.

27:18.240 --> 27:22.960
 So you're going to get the other for free. I think I think it's pretty likely that yes,

27:22.960 --> 27:27.200
 if we can get one we probably our machine learning is probably that good that we can get the other

27:27.200 --> 27:34.400
 but it's not 100 I'm not 100% sure. And also, I think a lot, a lot of it really does depend on

27:34.400 --> 27:41.840
 your definitions. Definitions of like perfect vision. Because, you know, reading is vision,

27:41.840 --> 27:47.920
 but should it count? Yeah, to me, so my definition is if a system looked at an image,

27:48.640 --> 27:55.200
 and then a system looked at a piece of text, and then told me something about that,

27:55.840 --> 28:01.280
 and I was really impressed. That's relative. You'll be impressed for half an hour and then

28:01.280 --> 28:04.960
 you're going to say, well, I mean, all the systems do that. But here's the thing they don't do.

28:04.960 --> 28:08.720
 Yeah, but I don't have that with humans. Humans continue to impress me.

28:08.720 --> 28:09.280
 Is that true?

28:10.400 --> 28:16.080
 Well, the ones, okay, so I'm a fan of monogamy. So I like the idea of marrying somebody being

28:16.080 --> 28:21.360
 with them for several decades. So I believe in the fact that yes, it's possible to have somebody

28:21.360 --> 28:29.920
 continuously giving you pleasurable, interesting, witty new ideas, friends. Yeah, I think so. They

28:29.920 --> 28:41.840
 continue to surprise you. The surprise, it's that injection of randomness seems to be a nice source

28:41.840 --> 28:52.800
 of, yeah, continued inspiration, like the wit, the humor. I think, yeah, that would be,

28:53.520 --> 28:57.600
 it's a very subjective test, but I think if you have enough humans in the room.

28:57.600 --> 29:02.480
 Yeah, I understand what you mean. Yeah, I feel like I misunderstood what you meant by

29:02.480 --> 29:08.080
 impressing you. I thought you meant to impress you with its intelligence, with how good,

29:08.080 --> 29:12.080
 valid understands an image. I thought you meant something like, I'm going to show you

29:12.080 --> 29:14.880
 a really complicated image and it's going to get it right and you're going to say, wow,

29:14.880 --> 29:19.760
 that's really cool. The systems of January 2020 have not been doing that.

29:19.760 --> 29:25.520
 Yeah, no, I think it all boils down to the reason people click like on stuff on the

29:25.520 --> 29:31.600
 internet, which is like it makes them laugh. So it's like humor or wit or insight.

29:32.720 --> 29:40.320
 I'm sure we'll get that as well. So forgive the romanticized question, but looking back to you,

29:40.320 --> 29:46.640
 what is the most beautiful or surprising idea in deep learning or AI in general you've come across?

29:46.640 --> 29:50.400
 So I think the most beautiful thing about deep learning is that it actually works.

29:51.520 --> 29:54.960
 And I mean it because you got these ideas, you got the little neural network, you got the back

29:54.960 --> 30:01.200
 propagation algorithm. And then you got some theories as to, you know, this is kind of like

30:01.200 --> 30:04.880
 the brain. So maybe if you make it large, if you make the neural network large and you're

30:04.880 --> 30:09.520
 trained on a lot of data, then it will do the same function that the brain does.

30:09.520 --> 30:14.160
 And it turns out to be true. That's crazy. And now we just train these neural networks and you

30:14.160 --> 30:18.720
 make them larger and they keep getting better. And I find it unbelievable. I find it unbelievable

30:18.720 --> 30:24.960
 that this whole AI stuff with neural networks works. Have you built up an intuition of why are

30:24.960 --> 30:30.160
 there a little bits and pieces of intuitions of insights of why this whole thing works?

30:31.200 --> 30:36.320
 I mean, some definitely, while we know that optimization, we now have good, you know,

30:37.280 --> 30:43.280
 we've had lots of empirical, huge amounts of empirical reasons to believe that optimization

30:43.280 --> 30:49.280
 should work on all most problems we care about. Do you have insights of what, so you just said

30:49.280 --> 30:58.240
 empirical evidence is most of your sort of empirical evidence kind of convinces you,

30:58.240 --> 31:03.280
 it's like evolution is empirical, it shows you that look, this evolutionary process seems to be

31:03.280 --> 31:10.480
 a good way to design organisms that survive in their environment. But it doesn't really get you

31:10.480 --> 31:16.880
 to the insides of how the whole thing works. I think it's a good analogy is physics. You know how

31:16.880 --> 31:20.640
 you say, Hey, let's do some physics calculation and come up with some new physics theory and make

31:20.640 --> 31:24.960
 some prediction. But then you got around the experiment. You know, you got around the experiment,

31:24.960 --> 31:29.760
 it's important. So it's a bit the same here, except that maybe sometimes the experiment came

31:29.760 --> 31:34.240
 before the theory. But it still is the case, you know, you have some data and you come up with

31:34.240 --> 31:37.520
 some prediction, you say, Yeah, let's make a big neural network, let's train it, and it's going to

31:37.520 --> 31:41.680
 work much better than anything before it. And it will in fact continue to get better as you make

31:41.680 --> 31:46.560
 it larger. And it turns out to be true. That's, that's amazing when a theory is validated like

31:46.560 --> 31:51.760
 this, you know, it's not a mathematical theory, it's more of a biological theory almost. So I

31:51.760 --> 31:56.160
 think there are not terrible analogies between deep learning and biology. I would say it's like

31:56.160 --> 32:02.800
 the geometric mean of biology and physics, that's deep learning. The geometric mean of biology and

32:02.800 --> 32:08.880
 physics, I think I'm going to need a few hours to wrap my head around that. Because just to find

32:08.880 --> 32:17.920
 the geometric, just to find the set of what biology represents. Well, biology, in biology,

32:17.920 --> 32:21.920
 things are really complicated. The theories are really, really, it's really hard to have good

32:21.920 --> 32:26.480
 predictive theory. And in physics, the theories are too good. In theory, in physics, people make

32:26.480 --> 32:29.840
 these super precise theories, which make these amazing predictions. And in machine learning,

32:29.840 --> 32:35.120
 they're kind of in between. Kind of in between. But it'd be nice if machine learning somehow

32:35.120 --> 32:39.040
 helped us discover the unification of the two as opposed to serve the in between.

32:40.800 --> 32:46.240
 But you're right, that's, you're kind of trying to juggle both. So do you think there are still

32:46.240 --> 32:51.200
 beautiful and mysterious properties in neural networks that are yet to be discovered? Definitely.

32:51.200 --> 32:54.000
 I think that we are still massively underestimating deep learning.

32:54.000 --> 32:59.040
 What do you think it will look like? Like what if I knew I would have done it?

33:01.120 --> 33:06.240
 So, but if you look at all the progress from the past 10 years, I would say most of it,

33:06.960 --> 33:12.080
 I would say there have been a few cases where some were things that felt like really new ideas

33:12.080 --> 33:17.120
 showed up. But by and large, it was every year, we thought, okay, deep learning goes this far.

33:17.120 --> 33:21.680
 Nope, it actually goes further. And then the next year, okay, now you know, this is this is

33:21.680 --> 33:25.440
 big deep learning. We are really done. Nope, it goes further. It just keeps going further each

33:25.440 --> 33:30.160
 year. So that means that we keep underestimating, we keep not understanding it as surprising properties

33:30.160 --> 33:35.120
 all the time. Do you think it's getting harder and harder to make progress, need to make progress?

33:35.840 --> 33:39.840
 It depends on what we mean. I think the field will continue to make very robust progress

33:39.840 --> 33:44.320
 for quite a while. I think for individual researchers, especially people who are doing

33:45.040 --> 33:49.040
 research, it can be harder because there is a very large number of researchers right now.

33:49.040 --> 33:53.360
 I think that if you have a lot of compute, then you can make a lot of very interesting discoveries,

33:53.360 --> 33:59.440
 but then you have to deal with the challenge of managing a huge computer, a huge class,

33:59.440 --> 34:01.840
 huge computer cluster to run your experiments. It's a little bit harder.

34:01.840 --> 34:06.640
 So I'm asking all these questions that nobody knows the answer to, but you're one of the smartest

34:06.640 --> 34:11.760
 people I know. So I'm going to keep asking the, so let's imagine all the breakthroughs that happen

34:11.760 --> 34:16.720
 in the next 30 years in deep learning. Do you think most of those breakthroughs can be done by

34:16.720 --> 34:23.600
 one person with one computer? Sort of in the space of breakthroughs, do you think compute

34:23.600 --> 34:32.720
 will be, compute and large efforts will be necessary? I mean, I can't be sure. When you say

34:32.720 --> 34:47.440
 one computer, you mean how large? You're clever. I mean, one GPU. I see. I think it's pretty unlikely.

34:47.440 --> 34:52.400
 I think it's pretty unlikely. I think that there are many, the stack of deep learning is starting

34:52.400 --> 35:00.640
 to be quite deep. If you look at it, you've got all the way from the ideas, the systems to build

35:00.640 --> 35:07.360
 the datasets, the distributed programming, the building the actual cluster, the GPU programming,

35:08.000 --> 35:11.440
 putting it all together. So the stack is getting really deep. And I think it becomes,

35:12.160 --> 35:16.960
 it can be quite hard for a single person to become, to be world class in every single layer of the

35:16.960 --> 35:24.240
 stack. What about what like Vladimir Vapnik really insists on is taking MNIST and trying to learn

35:24.240 --> 35:30.800
 from very few examples. So being able to learn more efficiently. Do you think that there'll be

35:30.800 --> 35:36.720
 breakthroughs in that space that would may not need this huge compute? I think there will be a

35:36.720 --> 35:40.960
 large number of breakthroughs in general that will not need a huge amount of compute. So maybe I

35:40.960 --> 35:46.720
 should clarify that. I think that some breakthroughs will require a lot of compute. And I think building

35:46.720 --> 35:51.200
 systems which actually do things will require a huge amount of compute. That one is pretty obvious.

35:51.200 --> 35:55.600
 If you want to do X, and X requires a huge neural net, you got to get a huge neural net.

35:56.480 --> 36:02.640
 But I think there will be lots of, I think there is lots of room for very important work being

36:02.640 --> 36:08.400
 done by small groups and individuals. Can you maybe sort of on the topic of the science of

36:08.400 --> 36:14.640
 deep learning, talk about one of the recent papers that you've released, the deep double descent,

36:15.600 --> 36:19.520
 where bigger models and more data hurt. I think it's a really interesting paper.

36:19.520 --> 36:26.400
 Can you can describe the main idea? And yeah, definitely. So what happened is that some over

36:26.400 --> 36:30.720
 the years, some small number of researchers noticed that it is kind of weird that when you make the

36:30.720 --> 36:33.840
 neural network larger, it works better. And it seems to go in contradiction with statistical

36:33.840 --> 36:38.240
 ideas. And then some people made an analysis showing that actually you got this double descent

36:38.240 --> 36:44.560
 bump. And what we've done was to show that double descent occurs for pretty much all practical

36:44.560 --> 36:53.440
 deep learning systems. And that it'll be also so can you step back? What's the X axis and the Y

36:53.440 --> 37:01.840
 axis of a double descent plot? Okay, great. So you can you can look you can do things like

37:02.480 --> 37:06.800
 you can take your neural network. And you can start increasing its size slowly,

37:07.440 --> 37:13.840
 while keeping your data set fixed. So if you increase the size of the neural network slowly,

37:13.840 --> 37:21.360
 and if you don't do early stopping, that's a pretty important detail. Then when the

37:21.360 --> 37:25.040
 neural network is really small, you make it larger, you get a very rapid increase in performance.

37:25.920 --> 37:28.560
 Then you continue to make it larger. And at some point performance will get worse.

37:30.000 --> 37:35.760
 And it gets and it gets the worst exactly at the point at which it achieves zero training

37:35.760 --> 37:40.480
 error precisely zero training loss. And then as you make it larger, it starts to get better again.

37:40.480 --> 37:44.400
 And it's kind of counterintuitive, because you'd expect deep learning phenomena to be

37:44.400 --> 37:51.520
 monotonic. And it's hard to be sure what it means. But it also occurs in the case of linear

37:51.520 --> 37:58.000
 classifiers. And the intuition basically boils down to the following. When you when you have a lot

37:58.000 --> 38:05.120
 when you have a large data set, and a small model, then small, tiny, random. So basically,

38:05.120 --> 38:13.040
 what is overfitting? Overfitting is when your model is somehow very sensitive to the small, random,

38:14.000 --> 38:19.200
 unimportant stuff in your data set in the training data in the training data set precisely. So if

38:19.200 --> 38:24.640
 you have a small model, and you have a big data set, and there may be some random thing, you know,

38:24.640 --> 38:29.760
 some training cases are randomly in the data set, and others may not be there. But the small model,

38:29.760 --> 38:34.640
 but the small model is kind of insensitive to this randomness, because it's the same you there is

38:34.640 --> 38:39.520
 pretty much no uncertainty about the model, when the data set is large. So okay, so at the very

38:39.520 --> 38:47.760
 basic level, to me, it is the most surprising thing that neural networks don't overfit every time,

38:48.560 --> 38:56.560
 very quickly, before ever being able to learn anything, the huge number of parameters. So here

38:56.560 --> 39:01.360
 is so there is one way okay, so maybe so let me try to give the explanation, maybe that will be

39:01.360 --> 39:06.720
 that will work. So you got a huge neural network, let's suppose you got them. You are you have a

39:06.720 --> 39:10.880
 huge neural network, you have a huge number of parameters. And now let's pretend everything is

39:10.880 --> 39:16.160
 linear, which is not let's just pretend. Then there is this big subspace, where your network

39:16.160 --> 39:22.480
 achieves zero error. And SGT is going to find approximately the point really that's right,

39:22.480 --> 39:29.040
 approximately the point with the smallest norm in that subspace. Okay, and that can also be proven

39:29.040 --> 39:35.680
 to be insensitive to the small randomness in the data, when the dimensionality is high. But when

39:35.680 --> 39:39.680
 the dimensionality of the data is equal to the dimensionality of the model, then there is a

39:39.680 --> 39:45.360
 one to one correspondence between all the data sets and the models. So small changes in the data

39:45.360 --> 39:48.960
 set actually lead to large changes in the model. And that's why performance gets worse. So this

39:48.960 --> 39:56.080
 is the best explanation more or less. So then it would be good for the model to have more parameters

39:56.080 --> 40:01.280
 so to be bigger than the data. That's right. But only if you don't really stop. If you introduce

40:01.280 --> 40:05.280
 early stop in your regularization, you can make a double as a descent pump almost completely

40:05.280 --> 40:10.960
 disappear. What is early stop early stopping is when you train your model, and you monitor your

40:10.960 --> 40:15.760
 test validation performance. And then if at some point validation performance starts to get worse,

40:15.760 --> 40:19.200
 you say, Okay, let's stop training. We are good. We are good. We are good enough.

40:19.840 --> 40:24.880
 So the magic happens after after that moment. So you don't want to do the early stopping.

40:24.880 --> 40:28.480
 Well, if you don't do the early stopping, you get this very, you get a very pronounced double

40:28.480 --> 40:35.360
 descent. Do you have any intuition why this happens? Double descent or sorry, are you stopping?

40:35.360 --> 40:40.000
 No, the double descent. So the Well, yeah, so I try it, let's see the intuition is basically

40:40.000 --> 40:48.240
 is this that when the data set has as many degrees of freedom as the model, then there is a one to

40:48.240 --> 40:53.840
 one correspondence between them. And so small changes to the data set lead to noticeable changes

40:53.840 --> 40:58.720
 in the model. So your model is very sensitive to all the randomness, it is unable to discard it.

40:59.440 --> 41:05.600
 Whereas, it turns out that when you have a lot more data than parameters, or a lot more parameters

41:05.600 --> 41:10.400
 than data, the resulting solution will be insensitive to small changes in the data set.

41:10.400 --> 41:18.720
 So it's able to nicely put discard the small changes, the randomness. Exactly. The spurious

41:18.720 --> 41:23.360
 correlation which you don't want. Jeff Hinton suggested we need to throw back propagation.

41:23.360 --> 41:27.200
 We already kind of talked about this a little bit, but he suggested we need to throw away

41:27.200 --> 41:31.840
 back propagation and start over. I mean, of course, some of that is a little bit

41:33.760 --> 41:39.520
 wit and humor. But what do you think? What could be an alternative method of training neural networks?

41:39.520 --> 41:44.000
 Well, the thing that he said precisely is that to the extent that you can't find back propagation

41:44.000 --> 41:49.360
 in the brain, it's worth seeing if we can learn something from how the brain learns. But back

41:49.360 --> 41:54.560
 propagation is very useful and we should keep using it. Oh, you're saying that once we discover the

41:54.560 --> 41:59.600
 mechanism of learning in the brain or any aspects of that mechanism, we should also try to implement

41:59.600 --> 42:03.600
 that in neural networks? If it turns out that you can't find back propagation in the brain?

42:03.600 --> 42:11.920
 If we can't find back propagation in the brain? Well, so I guess your answer to that is back

42:11.920 --> 42:17.440
 propagation is pretty damn useful. So why are we complaining? I mean, I personally am a big fan

42:17.440 --> 42:21.840
 of back propagation. I think it's a great algorithm because it solves an extremely fundamental problem

42:21.840 --> 42:29.840
 which is finding a neural circuit subject to some constraints. And I don't see that problem going

42:29.840 --> 42:36.160
 away. So that's why I really, I think it's pretty unlikely that you'll have anything which is going

42:36.160 --> 42:41.120
 to be dramatically different. It could happen. But I wouldn't bet on it right now.

42:41.120 --> 42:51.040
 So let me ask a sort of big picture question. Do you think neural networks can be made to reason?

42:51.600 --> 42:59.760
 Why not? Well, if you look, for example, at AlphaGo or AlphaZero, the neural network of AlphaZero

43:00.320 --> 43:08.400
 plays Go, which we all agree is a game that requires reasoning, better than 99.9% of all humans,

43:08.400 --> 43:11.360
 just the neural network without the search, just the neural network itself.

43:12.240 --> 43:16.320
 Doesn't that give us an existence proof that neural networks can reason?

43:17.680 --> 43:24.800
 To push back and disagree a little bit, we all agree that Go is reasoning. I think I agree. I

43:24.800 --> 43:32.000
 don't think it's at trivial. So obviously reasoning like intelligence is a loose gray area term

43:32.000 --> 43:37.920
 a little bit. Maybe you disagree with that. But yes, I think it has some of the same elements

43:37.920 --> 43:45.360
 of reasoning. Reasoning is almost akin to search. There's a sequential element of

43:46.720 --> 43:54.320
 stepwise consideration of possibilities and sort of building on top of those possibilities in a

43:54.320 --> 44:00.400
 sequential manner until you arrive at some insight. So yeah, I guess playing Go is kind of like that.

44:00.400 --> 44:04.720
 And when you have a single neural network doing that without search, that's kind of like that.

44:04.720 --> 44:10.080
 So there's an existence proof in a particular constrained environment that a process akin to

44:10.880 --> 44:18.720
 what many people call reasoning exists, but more general kind of reasoning. So off the board.

44:18.720 --> 44:24.560
 There is one other existence proof. Oh boy, which one? Us humans? Yes. Okay. All right. So

44:26.000 --> 44:34.480
 do you think the architecture that will allow neural networks to reason will look similar

44:34.480 --> 44:41.520
 to the neural network architectures we have today? I think it will. I think, well, I don't want to make

44:41.520 --> 44:47.600
 two overly definitive statements. I think it's definitely possible that the neural networks

44:47.600 --> 44:51.760
 that will produce the reasoning breakthroughs of the future will be very similar to the

44:51.760 --> 44:56.160
 architectures that exist today, maybe a little bit more recurrent, maybe a little bit deeper.

44:56.160 --> 45:04.560
 But these neural nets are so insanely powerful. Why wouldn't they be able to learn to reason?

45:05.440 --> 45:10.800
 Humans can reason. So why can't neural networks? So do you think the kind of stuff we've seen

45:11.520 --> 45:15.840
 neural networks do is a kind of just weak reasoning? So it's not a fundamentally different

45:15.840 --> 45:21.120
 process. Again, this is stuff we don't nobody knows the answer to. So when it comes to our

45:21.120 --> 45:26.960
 neural networks, I would think which I would say is that neural networks are capable of reasoning.

45:28.160 --> 45:31.760
 But if you train a neural network on a task which doesn't require reasoning,

45:32.400 --> 45:37.040
 it's not going to reason. This is a well known effect where the neural network will solve

45:37.040 --> 45:43.040
 exactly the it will solve the problem that you pose in front of it in the easiest way possible.

45:43.040 --> 45:53.120
 Right. That takes us to one of the brilliant ways you describe neural networks, which is

45:54.000 --> 45:57.040
 you've referred to neural networks as the search for small circuits,

45:57.840 --> 46:02.960
 and maybe general intelligence as the search for small programs,

46:04.320 --> 46:08.560
 which I found is a metaphor very compelling. Can you elaborate on that difference?

46:08.560 --> 46:17.520
 Yeah. So the thing which I said precisely was that if you can find the shortest program that

46:17.520 --> 46:23.520
 outputs the data at your disposal, then you will be able to use it to make the best prediction

46:23.520 --> 46:29.680
 possible. And that's a theoretical statement which can be proved mathematically. Now,

46:29.680 --> 46:34.800
 you can also prove mathematically that it is that finding the shortest program which generates

46:34.800 --> 46:41.600
 some data is not a computable operation. No finite amount of compute can do this.

46:42.720 --> 46:48.800
 So then with neural networks, neural networks are the next best thing that actually works in

46:48.800 --> 46:54.640
 practice. We are not able to find the best, the shortest program which generates our data,

46:55.600 --> 47:02.720
 but we are able to find a small, but now that statement should be amended. Even a large circuit

47:02.720 --> 47:06.640
 which fits our data in some way. Well, I think what you meant by the small

47:06.640 --> 47:12.240
 circuit is the smallest needed circuit. Well, the thing which I would change now,

47:12.240 --> 47:18.400
 back then I really haven't fully internalized the overparameterized results. The things we know

47:18.400 --> 47:25.360
 about overparameterized neural nets, now I would phrase it as a large circuit whose weights contain

47:25.360 --> 47:30.320
 a small amount of information, which I think is what's going on. If you imagine the training

47:30.320 --> 47:36.080
 process of a neural network as you slowly transmit entropy from the data set to the parameters,

47:36.880 --> 47:42.720
 then somehow the amount of information in the weights ends up being not very large,

47:42.720 --> 47:48.800
 which would explain why the general is so well. So the large circuit might be one that's

47:48.800 --> 47:59.680
 helpful for the generalization. Yeah, something like this. But do you see it important to be able

47:59.680 --> 48:05.920
 to try to learn something like programs? I mean, if we can, definitely. I think it's kind of,

48:05.920 --> 48:10.400
 the answer is kind of yes, if we can do it. We should do things that we can do it.

48:11.840 --> 48:19.040
 The reason we are pushing on deep learning, the fundamental reason, the root cause is that we

48:19.040 --> 48:25.440
 are able to train them. So in other words, training comes first. We've got our pillar,

48:25.440 --> 48:30.080
 which is the training pillar. And now if you're trying to contort our neural networks around

48:30.080 --> 48:36.800
 the training pillar, we got to stay trainable. This is an invariant we cannot violate. And so

48:38.160 --> 48:42.720
 being trainable means starting from scratch, knowing nothing, you can actually pretty quickly

48:42.720 --> 48:48.480
 converge towards knowing a lot or even slowly. But it means that given the resources at your

48:48.480 --> 48:55.920
 disposal, you can train the neural net and get it to achieve useful performance. Yeah, that's a

48:55.920 --> 48:59.520
 pillar we can't move away from. That's right. Because if you can, whereas if you say, Hey,

48:59.520 --> 49:04.800
 let's find the shortest program, we can't do that. So it doesn't matter how useful that would be.

49:05.920 --> 49:10.640
 We can do it. So we want. So do you think you kind of mentioned that the neural networks are

49:10.640 --> 49:16.400
 good at finding small circuits or large circuits? Do you think then the matter of finding small

49:16.400 --> 49:28.800
 programs is just the data? No, sorry, not the size or the type of data. Ask giving it programs.

49:28.800 --> 49:35.440
 Well, I think the thing is that right now, there are no good precedents of people successfully

49:35.440 --> 49:42.320
 finding programs really well. And so the way you'd find programs is you'd train a deep neural

49:42.320 --> 49:48.800
 network to do it basically. Right. Which is the right way to go about it. But there's not good

49:49.600 --> 49:53.920
 illustrations of that. Yes, hasn't been done yet. But in principle, it should be possible.

49:56.320 --> 50:01.120
 Can you elaborate a little bit? What's your insight in principle? And put another way,

50:01.120 --> 50:07.440
 you don't see why it's not possible. Well, it's kind of like more, it's more a statement of

50:07.440 --> 50:13.440
 I think that it's, I think that it's unwise to bet against deep learning. And

50:14.880 --> 50:18.880
 if it's a, if it's a cognitive function that humans seem to be able to do, then

50:20.320 --> 50:24.320
 it doesn't take too long for some deep neural net to pop up that can do it too.

50:25.680 --> 50:32.880
 Yeah, I'm there with you. I can, I've stopped betting against neural networks at this point

50:32.880 --> 50:38.320
 because they continue to surprise us. What about long term memory? Can neural networks have long

50:38.320 --> 50:45.360
 term memory or something like knowledge basis? So being able to aggregate important information

50:45.360 --> 50:53.200
 over long periods of time, that would then serve as useful sort of representations of

50:54.000 --> 51:00.400
 state that you can make decisions by. So have a long term context based on what you make in the

51:00.400 --> 51:07.600
 decision. So in some sense, the parameters already do that. The parameters are an aggregation of the

51:07.600 --> 51:12.080
 day of the neural of the entirety of the neural nets experience. And so they count as the long

51:12.080 --> 51:18.240
 as long form long term knowledge. And people have trained various neural nets to act as

51:18.240 --> 51:22.480
 knowledge basis and, you know, investigated with invest people have investigated language

51:22.480 --> 51:28.480
 models as knowledge basis. So there is work, there is work there. Yeah, but in some sense,

51:28.480 --> 51:35.760
 do you think in every sense, do you think there's a, it's all just a matter of coming up with a

51:35.760 --> 51:40.480
 better mechanism of forgetting the useless stuff and remembering the useful stuff? Because right

51:40.480 --> 51:46.160
 now, I mean, there's not been mechanisms that do remember really long term information.

51:46.720 --> 51:47.920
 What do you mean by that precisely?

51:48.800 --> 51:51.920
 Precisely. I like the word precisely. So

51:51.920 --> 51:59.760
 I'm thinking of the kind of compression of information the knowledge bases represent,

52:00.400 --> 52:07.040
 sort of creating a, now I apologize for my sort of human centric thinking about

52:07.680 --> 52:13.040
 what knowledge is because neural networks aren't interpretable necessarily with the

52:13.040 --> 52:18.800
 kind of knowledge they have discovered. But a good example for me is knowledge bases being

52:18.800 --> 52:23.120
 able to build up over time something like the knowledge that Wikipedia represents.

52:23.920 --> 52:26.480
 It's a really compressed structured

52:29.680 --> 52:35.600
 knowledge base, obviously not the actual Wikipedia or the language, but like a semantic web,

52:35.600 --> 52:40.240
 the dream that semantic web represented. So it's a really nice compressed knowledge base

52:40.240 --> 52:46.800
 or something akin to that in the noninterpretable sense as neural networks would have.

52:46.800 --> 52:50.720
 Well, the neural networks would be noninterpretable if you look at their weights, but their outputs

52:50.720 --> 52:55.920
 should be very interpretable. Okay, so yeah, how do you make very smart neural networks like

52:55.920 --> 53:00.720
 language models interpretable? Well, you ask them to generate some text and the text will

53:00.720 --> 53:05.840
 generally be interpretable. Do you find that the epitome of interpretability, like can you do better?

53:07.600 --> 53:11.360
 Because you can't, okay, I'd like to know what does it know and what doesn't know.

53:11.360 --> 53:17.200
 I would like the neural network to come up with examples where it's completely dumb

53:17.200 --> 53:22.480
 and examples where it's completely brilliant. And the only way I know how to do that now is to

53:22.480 --> 53:27.360
 generate a lot of examples and use my human judgment. But it would be nice if the neural

53:27.360 --> 53:34.320
 network had some self awareness about it. Yeah, 100%. I'm a big believer in self awareness. And I

53:34.320 --> 53:41.760
 think that I think, I think neural net self awareness will allow for things like the capabilities,

53:41.760 --> 53:45.840
 like the ones you described, like for them to know what they know and what they don't know,

53:45.840 --> 53:50.640
 and for them to know where to invest to increase their skills most optimally. And to your question

53:50.640 --> 53:54.880
 of interpretability, there are actually two answers to that question. One answer is, you know,

53:54.880 --> 53:59.040
 we have the neural net, so we can analyze the neurons and we can try to understand what the

53:59.040 --> 54:03.680
 different neurons and different layers mean. And you can actually do that and OpenAI has done

54:03.680 --> 54:10.000
 some work on that. But there is a different answer which is that I would say that's the human

54:10.000 --> 54:15.920
 centric answer where you say, you know, you look at a human being, you can't read, you know,

54:15.920 --> 54:19.680
 how do you know what a human being is thinking? You ask them, you say, Hey, what do you think about

54:19.680 --> 54:24.960
 this? What do you think about that? And you get some answers. The answers you get are sticky. In

54:24.960 --> 54:30.800
 the sense, you already have a mental model, you already have an, yeah, mental model of that human

54:30.800 --> 54:37.760
 being. You already have an understanding of like a big conception of what it of that human being,

54:37.760 --> 54:42.400
 how they think, what they know, how they see the world, and then everything you ask, you're

54:42.400 --> 54:50.800
 adding onto that. And that stickiness seems to be, that's one of the really interesting qualities

54:50.800 --> 54:56.640
 of the human being is that information is sticky. You don't, you seem to remember the useful stuff,

54:56.640 --> 55:02.000
 aggregate it well, and forget most of the information that's not useful. That process,

55:02.000 --> 55:06.960
 but that's also pretty similar to the process that neural networks do. It's just that neural

55:06.960 --> 55:12.480
 networks are much crappier at this time. It doesn't seem to be fundamentally that different.

55:12.480 --> 55:18.880
 But just to stick on reasoning for a little longer, you said, why not? Why can't that reason?

55:18.880 --> 55:28.000
 What's a good, impressive feat benchmark to you of reasoning that you'll be impressed by if

55:28.000 --> 55:32.960
 neural networks were able to do? Is that something you already have in mind? Well, I think writing,

55:32.960 --> 55:40.800
 writing really good code, I think, proving really hard theorems, solving open ended problems

55:40.800 --> 55:49.760
 without the box solutions. And sort of theorem type mathematical problems. Yeah, I think those

55:49.760 --> 55:53.920
 ones are a very natural example as well. You know, if you can prove an unproven theorem,

55:53.920 --> 55:58.960
 then it's hard to argue it on reason. And so by the way, and this comes back to the point about

55:58.960 --> 56:04.480
 the hard results, you know, if you've got a hard, if you have machine learning, deep learning as a

56:04.480 --> 56:09.040
 field is very fortunate, because we have the ability to sometimes produce these unambiguous

56:09.040 --> 56:15.280
 results. And when they happen, the debate changes, the conversation changes. It's a converse, you

56:15.280 --> 56:20.800
 have the ability to produce conversation changing results. Conversation. And then of course,

56:20.800 --> 56:24.960
 just like you said, people kind of take that for granted, say that wasn't actually a hard problem.

56:24.960 --> 56:27.680
 Well, I mean, at some point, we'll probably run out of hard problems.

56:29.280 --> 56:34.720
 Yeah, that whole mortality thing is kind of a sticky problem that we haven't quite figured

56:34.720 --> 56:40.720
 out. Maybe we'll solve that one. I think one of the fascinating things in your entire body of work,

56:40.720 --> 56:45.680
 but also the work at OpenAI recently, one of the conversation changes has been in the world of

56:45.680 --> 56:51.600
 language models. Can you briefly kind of try to describe the recent history of using neural

56:51.600 --> 56:57.280
 networks in the domain of language and text? Well, there's been lots of history. I think the

56:57.280 --> 57:03.040
 Elman network was a small, tiny recurrent neural network applied to language back in the 80s.

57:03.040 --> 57:11.120
 So the history is really, you know, fairly long, at least. And the thing that started the thing

57:11.120 --> 57:17.040
 that changed the trajectory of neural networks and language is the thing that changed the trajectory

57:17.040 --> 57:22.560
 of all deep learning and that's data and compute. So suddenly you move from small language models,

57:22.560 --> 57:27.600
 which learn a little bit. And with language models, in particular, you can, there's a very clear

57:27.600 --> 57:32.560
 explanation for why they need to be large, to be good. Because they're trying to predict the

57:32.560 --> 57:40.800
 next word. So when you don't know anything, you'll notice very, very broad strokes, surface level

57:40.800 --> 57:46.320
 patterns, like sometimes there are characters and there is space between those characters,

57:46.320 --> 57:50.640
 you'll notice this pattern. And you'll notice that sometimes there is a comma and then the next

57:50.640 --> 57:54.960
 character is a capital letter, you'll notice that pattern. Eventually, you may start to notice that

57:54.960 --> 57:59.840
 there are certain words occur often, you may notice that spellings are a thing, you may notice

57:59.840 --> 58:04.880
 syntax. And when you get really good at all these, you start to notice the semantics,

58:05.680 --> 58:10.400
 you start to notice the facts. But for that to happen, the language model needs to be larger.

58:11.360 --> 58:16.240
 So that's, let's linger on that, is that's where you and Noam Chomsky disagree.

58:18.640 --> 58:25.600
 See, you think we're actually taking incremental steps, sort of larger network, larger compute

58:25.600 --> 58:34.640
 will be able to get to the semantics, be able to understand language without what Noam likes to

58:34.640 --> 58:42.000
 sort of think of as a fundamental understandings of the structure of language, like imposing

58:42.000 --> 58:49.280
 your theory of language onto the learning mechanism. So you're saying the learning you can learn from

58:49.280 --> 58:57.040
 raw data, the mechanism that underlies language? Well, I think it's pretty likely. But I also

58:57.040 --> 59:05.520
 want to say that I don't really know precisely what Chomsky means when he talks about him.

59:05.520 --> 59:11.040
 You said something about imposing your structural language. I'm not 100% sure what he means, but

59:11.040 --> 59:15.520
 empirically, it seems that when you inspect those larger language models, they exhibit signs of

59:15.520 --> 59:19.200
 understanding the semantics, whereas the smaller language models do not. We've seen that a few

59:19.200 --> 59:25.200
 years ago when we did work on the sentiment neuron, we trained a small, you know, smallish LSTM

59:25.200 --> 59:30.800
 to predict the next character in Amazon reviews. And we noticed that when you increase the size

59:30.800 --> 59:37.120
 of the LSTM from 500 LSTM cells to 4,000 LSTM cells, then one of the neurons starts to represent

59:37.120 --> 59:43.840
 the sentiment of the article of sorry, of their view. Now, why is that sentiment is a pretty

59:43.840 --> 59:48.560
 semantic attribute? It's not a syntactic attribute. And for people who might not know, I don't know

59:48.560 --> 59:51.920
 if that's a standard term, but sentiment is whether it's a positive or negative review.

59:51.920 --> 59:56.000
 That's right. Like, is the person happy with something or is the person unhappy with something?

59:56.000 --> 1:00:01.760
 And so here we had very clear evidence that a small neural net does not capture sentiment

1:00:01.760 --> 1:00:07.280
 while a large neural net does. And why is that? Well, our theory is that at some point,

1:00:07.280 --> 1:00:10.320
 you run out of syntax to models, you start to got to focus on something else.

1:00:10.960 --> 1:00:17.360
 And besides, you quickly run out of syntax to model, and then you really start to focus on

1:00:17.360 --> 1:00:22.080
 the semantics. This would be the idea. That's right. And so I don't want to imply that our models

1:00:22.080 --> 1:00:27.760
 have complete semantic understanding, because that's not true. But they definitely are showing

1:00:27.760 --> 1:00:32.480
 signs of semantic understanding, partial semantic understanding, but the smaller models do not show

1:00:32.480 --> 1:00:39.920
 that those signs. Can you take a step back and say, what is GPT2, which is one of the big language

1:00:39.920 --> 1:00:46.320
 models that was the conversation changer in the past couple of years? Yeah, so GPT2 is a

1:00:46.320 --> 1:00:54.880
 transformer with one and a half billion parameters that was trained on about 40 billion tokens of

1:00:54.880 --> 1:01:01.600
 text, which were obtained from webpages that were linked to from Reddit articles with more than three

1:01:01.600 --> 1:01:06.560
 uploads. And what's the transformer? The transformer, it's the most important advance

1:01:06.560 --> 1:01:11.280
 in neural network architectures in recent history. What is the tension maybe two,

1:01:11.280 --> 1:01:15.360
 because I think that's an interesting idea, not necessarily sort of technically speaking, but

1:01:15.360 --> 1:01:21.040
 the idea of attention versus maybe what recurrent neural networks represent.

1:01:21.040 --> 1:01:25.760
 Yeah. So the thing is, the transformer is a combination of multiple ideas simultaneously

1:01:25.760 --> 1:01:31.760
 of which attention is one. Do you think attention is the key? No, it's a key, but it's not the key.

1:01:32.320 --> 1:01:37.680
 The transformer is successful because it is the simultaneous combination of multiple ideas. And

1:01:37.680 --> 1:01:43.040
 if you were to remove either idea, it would be much less successful. So the transformer uses a

1:01:43.040 --> 1:01:47.280
 lot of attention, but attention exists for a few years. So that can't be the main innovation.

1:01:48.320 --> 1:01:54.880
 The transformer is designed in such a way that it runs really fast on the GPU.

1:01:56.000 --> 1:02:00.240
 And that makes a huge amount of difference. This is one thing. The second thing is that

1:02:00.240 --> 1:02:06.320
 transformer is not recurrent. And that is really important too, because it is more shallow and

1:02:06.320 --> 1:02:12.160
 therefore much easier to optimize. So in other words, uses attention. It is, it is a really

1:02:12.160 --> 1:02:17.760
 great fit to the GPU. And it is not recurrent. So therefore, less deep and easier to optimize.

1:02:17.760 --> 1:02:22.640
 And the combination of those factors make it successful. So now it makes, it makes great

1:02:22.640 --> 1:02:27.440
 use of your GPU. It allows you to achieve better results for the same amount of compute.

1:02:28.560 --> 1:02:33.520
 And that's why it's successful. Were you surprised how well transformers worked?

1:02:34.160 --> 1:02:40.480
 And GPT2 worked? So you worked on language. You've had a lot of great ideas before

1:02:40.480 --> 1:02:45.280
 transformers came about in language. So you got to see the whole set of revolutions before and

1:02:45.280 --> 1:02:51.280
 after. Were you surprised? Yeah, a little. A little. Yeah. I mean, it's hard, it's hard to

1:02:51.280 --> 1:02:56.800
 remember because you adapt really quickly. But it definitely was surprising. It definitely was,

1:02:56.800 --> 1:03:01.760
 in fact, I'll, you know what, I'll, I'll retract my statement. It was, it was pretty amazing.

1:03:02.320 --> 1:03:07.680
 It was just amazing to see generate this text of this. And you know, I gotta keep in mind that

1:03:07.680 --> 1:03:12.000
 we've seen, at that time, we've seen all this progress in GANs and improving, you know, the

1:03:12.000 --> 1:03:16.720
 samples produced by GANs were just amazing. You have these realistic faces, but text hasn't really

1:03:16.720 --> 1:03:23.760
 moved that much. And suddenly we moved from, you know, whatever GANs were in 2015, to the best,

1:03:23.760 --> 1:03:29.040
 most amazing GANs in one step. And I was really stunning. Even though theory predicted, yeah,

1:03:29.040 --> 1:03:32.560
 you train a big language model, of course, you should get this, but then to see it with your

1:03:32.560 --> 1:03:39.840
 own eyes, it's something else. And yet we adapt really quickly. And now there's sort of

1:03:41.520 --> 1:03:48.240
 some cognitive scientists, right, articles saying that GPT two models don't really understand

1:03:48.240 --> 1:03:54.240
 language. So we adapt quickly to how amazing the fact that they're able to model the language so

1:03:54.240 --> 1:04:03.920
 well is. So what do you think is the bar for what for impressing us that I don't know, do you think

1:04:03.920 --> 1:04:10.160
 that bar will continuously be moved? Definitely. I think when you start to see really dramatic

1:04:10.160 --> 1:04:14.480
 economic impact, that's when I think that's in some sense the next barrier. Because right now,

1:04:14.480 --> 1:04:20.640
 if you think about the work in AI, it's really confusing. It's really hard to know what to make

1:04:20.640 --> 1:04:26.720
 of all these advances. It's kind of like, okay, you got an advance and now you can do more things

1:04:26.720 --> 1:04:34.400
 and you got another improvement and you got another cool demo. At some point, I think people who are

1:04:34.400 --> 1:04:40.160
 outside of AI, they can no longer distinguish this progress anymore. So we were talking offline about

1:04:40.160 --> 1:04:45.440
 translating Russian to English and how there's a lot of brilliant work in Russian that the rest

1:04:45.440 --> 1:04:50.080
 of the world doesn't know about. That's true for Chinese. It's true for a lot of scientists and

1:04:50.080 --> 1:04:54.240
 just artistic work in general. Do you think translation is the place where we're going

1:04:54.240 --> 1:04:59.600
 to see sort of economic big impact? I don't know. I think there is a huge number of

1:04:59.600 --> 1:05:05.360
 applications. First of all, I want to point out that translation already today is huge.

1:05:05.360 --> 1:05:10.960
 I think billions of people interact with big chunks of the internet primarily through translation.

1:05:10.960 --> 1:05:18.000
 So translation is already huge and it's hugely positive too. I think self driving is going to be

1:05:18.000 --> 1:05:26.320
 hugely impactful. It's unknown exactly when it happens, but again, I would not bet against

1:05:26.320 --> 1:05:31.120
 deep learning. So that's deep learning in general. Deep learning for self driving.

1:05:31.680 --> 1:05:35.040
 Yes, deep learning for self driving. But I was talking about sort of language models.

1:05:35.040 --> 1:05:39.760
 I see. Just to check. I beard off a little bit. Just to check. You're not seeing a connection

1:05:39.760 --> 1:05:44.000
 between driving and language. No, no. Okay. I'd rather both use neural nets.

1:05:44.000 --> 1:05:48.240
 That'd be a poetic connection. I think there might be some, like you said, there might be some kind

1:05:48.240 --> 1:05:56.000
 of unification towards a kind of multitask transformers that can take on both language

1:05:56.000 --> 1:06:02.640
 and vision tasks. That'd be an interesting unification. Let's see. What can I ask about

1:06:02.640 --> 1:06:09.760
 GPT2 more? It's simple. It's not much to ask. You take a transform, you make it bigger,

1:06:09.760 --> 1:06:12.480
 give it more data, and suddenly it does all those amazing things.

1:06:12.480 --> 1:06:17.200
 Yeah. One of the beautiful things is that GPT, the transformers are fundamentally simple to

1:06:17.200 --> 1:06:25.920
 explain, to train. Do you think bigger will continue to show better results in language?

1:06:26.960 --> 1:06:31.280
 Probably. Sort of like what are the next steps with GPT2? Do you think?

1:06:31.280 --> 1:06:38.000
 I mean, I think for sure seeing what larger versions can do is one direction. Also,

1:06:38.000 --> 1:06:42.880
 I mean, there are many questions. There's one question which I'm curious about and that's

1:06:42.880 --> 1:06:47.200
 the following. Right now, GPT2, so we feed it all this data from the internet, which means that

1:06:47.200 --> 1:06:53.680
 it needs to memorize all those random facts about everything in the internet. It would be nice if

1:06:55.280 --> 1:07:01.600
 the model could somehow use its own intelligence to decide what data it wants to start, accept,

1:07:01.600 --> 1:07:07.040
 and what data it wants to reject, just like people. People don't learn all data indiscriminately.

1:07:07.040 --> 1:07:11.760
 We are super selective about what we learn. I think this kind of active learning I think

1:07:11.760 --> 1:07:18.320
 would be very nice to have. Yeah. Listen, I love active learning. Let me ask,

1:07:19.280 --> 1:07:23.840
 does the selection of data, can you just elaborate that a little bit more? Do you think the selection

1:07:23.840 --> 1:07:33.680
 of data is, I have this kind of sense that the optimization of how you select data,

1:07:33.680 --> 1:07:39.520
 so the active learning process is going to be a place for a lot of breakthroughs,

1:07:40.720 --> 1:07:44.880
 even in the near future, because there hasn't been many breakthroughs there that are public.

1:07:44.880 --> 1:07:49.120
 I feel like there might be private breakthroughs that companies keep to themselves,

1:07:49.120 --> 1:07:52.800
 because the fundamental problem has to be solved if you want to solve self driving,

1:07:52.800 --> 1:07:57.680
 if you want to solve a particular task. What do you think about the space in general?

1:07:57.680 --> 1:08:02.320
 Yeah, so I think that for something like active learning, or in fact for any kind of capability,

1:08:02.320 --> 1:08:06.880
 like active learning, the thing that it really needs is the problem. It needs a problem that

1:08:06.880 --> 1:08:12.880
 requires it. It's very hard to do research about the capability if you don't have a task,

1:08:12.880 --> 1:08:16.640
 because then what's going to happen is you will come up with an artificial task,

1:08:16.640 --> 1:08:23.120
 get good results, but not really convince anyone. Right. We're now past the stage where

1:08:24.000 --> 1:08:30.720
 getting a result on MNIST, some clever formulation of MNIST will convince people.

1:08:30.720 --> 1:08:34.800
 That's right. In fact, you could quite easily come up with a simple active learning scheme

1:08:34.800 --> 1:08:44.880
 on MNIST and get a 10x speed up, but then so what? I think that active learning will naturally arise

1:08:45.440 --> 1:08:51.600
 as problems that require it to pop up. That's my take on it.

1:08:52.720 --> 1:08:58.720
 There's another interesting thing that OpenAS brought up with GPT2, which is when you create a

1:08:58.720 --> 1:09:04.880
 powerful artificial intelligence system, and it was unclear what kind of detrimental, once you

1:09:04.880 --> 1:09:11.600
 release GPT2, what kind of detrimental effect it will have. Because if you have a model that can

1:09:11.600 --> 1:09:18.480
 generate pretty realistic text, you can start to imagine that it would be used by bots in some

1:09:20.080 --> 1:09:24.320
 way that we can't even imagine. There's this nervousness about what it's possible to do.

1:09:24.320 --> 1:09:30.000
 So you did a really brave and profound thing, which just started a conversation about this.

1:09:30.000 --> 1:09:38.320
 How do we release powerful artificial intelligence models to the public? If we do it all, how do we

1:09:38.320 --> 1:09:45.920
 privately discuss with other even competitors about how we manage the use of the systems and so on?

1:09:45.920 --> 1:09:51.120
 So from this whole experience, you've released a report on it, but in general, are there any

1:09:51.120 --> 1:09:57.520
 insights that you've gathered from just thinking about this, about how you release models like this?

1:09:57.520 --> 1:10:04.240
 I mean, I think that my take on this is that the field of AI has been in a state of childhood,

1:10:04.880 --> 1:10:08.160
 and now it's exiting that state and it's entering a state of maturity.

1:10:09.440 --> 1:10:15.280
 What that means is that AI is very successful and also very impactful, and its impact is not only

1:10:15.280 --> 1:10:22.960
 large, but it's also growing. And so for that reason, it seems wise to start thinking about

1:10:22.960 --> 1:10:27.920
 the impact of our systems before releasing them, maybe a little bit too soon, rather than a little

1:10:27.920 --> 1:10:34.160
 bit too late. And with the case of GPT2, like I mentioned earlier, the results really were stunning,

1:10:34.960 --> 1:10:41.680
 and it seemed plausible. It didn't seem certain. It seemed plausible that something like GPT2 could

1:10:41.680 --> 1:10:49.200
 easily use to reduce the cost of disinformation. And so there was a question of what's the best

1:10:49.200 --> 1:10:54.800
 way to release it? And a staged release seemed logical. A small model was released, and there

1:10:54.800 --> 1:11:00.080
 was time to see the many people use these models in lots of cool ways. There have been lots of

1:11:00.080 --> 1:11:06.800
 really cool applications. There haven't been any negative applications we know of. And so eventually

1:11:06.800 --> 1:11:10.880
 it was released, but also other people replicated similar models. That's an interesting question,

1:11:10.880 --> 1:11:19.040
 though, that we know of. So in your view, staged release is at least part of the answer to the

1:11:19.040 --> 1:11:27.040
 question of what do we do once we create a system like this? It's part of the answer, yes.

1:11:28.240 --> 1:11:33.120
 Is there any other insights? Say you don't want to release the model at all, because it's useful

1:11:33.120 --> 1:11:38.160
 to you for whatever the business is. Well, there are plenty of people who don't release models

1:11:38.160 --> 1:11:45.040
 already, right? Of course. But is there some moral ethical responsibility when you have a

1:11:45.040 --> 1:11:51.760
 very powerful model to sort of communicate? Just as you said, when you had GPT2, it was

1:11:51.760 --> 1:11:57.280
 unclear how much it could be used for misinformation. It's an open question. And getting an answer to

1:11:57.280 --> 1:12:03.280
 that might require that you talk to other really smart people that are outside of your particular

1:12:03.280 --> 1:12:10.400
 group. Have you please tell me there's some optimistic pathway for people across the world

1:12:10.400 --> 1:12:17.840
 to collaborate on these kinds of cases? Or is it still really difficult from one company to

1:12:17.840 --> 1:12:23.280
 talk to another company? So it's definitely possible. It's definitely possible to discuss

1:12:24.560 --> 1:12:31.280
 these kind of models with colleagues elsewhere and to get their take on what to do.

1:12:31.280 --> 1:12:34.000
 How hard is it though? I mean,

1:12:36.320 --> 1:12:41.440
 do you see that happening? I think that's the place where it's important to gradually build

1:12:41.440 --> 1:12:47.760
 trust between companies. Because ultimately, all the AI developers are building technology,

1:12:47.760 --> 1:12:51.360
 which is going to be increasingly more powerful. And so it's

1:12:54.480 --> 1:12:56.880
 the way to think about it is that ultimately, we're only together.

1:12:56.880 --> 1:13:06.320
 Yeah, I tend to believe in the better angels of our nature, but I do hope that

1:13:08.960 --> 1:13:13.760
 when you build a really powerful AI system in a particular domain, that you also think about

1:13:14.880 --> 1:13:23.920
 the potential negative consequences of AI. It's an interesting and scary possibility

1:13:23.920 --> 1:13:30.240
 that there will be a race for AI development that would push people to close that development

1:13:30.240 --> 1:13:36.480
 and not share ideas with others. I don't love this. I've been in a pure academic for 10 years.

1:13:36.480 --> 1:13:44.400
 I really like sharing ideas and it's fun. It's exciting. Let's talk about AGI a little bit.

1:13:44.400 --> 1:13:49.360
 What do you think it takes to build a system of human level intelligence? We talked about reasoning,

1:13:49.360 --> 1:13:53.600
 we talked about long term memory, but in general, what does it take, do you think?

1:13:53.600 --> 1:14:02.640
 Well, I can't be sure. But I think the deep learning plus maybe another small idea.

1:14:03.600 --> 1:14:08.960
 Do you think self play will be involved? You've spoken about the powerful mechanism of self play,

1:14:08.960 --> 1:14:18.400
 where systems learn by exploring the world in a competitive setting against other entities that

1:14:18.400 --> 1:14:23.760
 are similarly skilled as them and so incrementally improve in this way. Do you think self play will

1:14:23.760 --> 1:14:30.880
 be a component of building an AGI system? Yeah. What I would say to build AGI, I think it's going

1:14:30.880 --> 1:14:39.120
 to be deep learning plus some ideas. I think self play will be one of those ideas. I think that

1:14:39.120 --> 1:14:49.760
 that is a very self play has this amazing property that it can surprise us in truly novel ways. For

1:14:49.760 --> 1:14:59.280
 example, like we, I mean, pretty much every self play system, both are dot a bot. I don't know if

1:14:59.280 --> 1:15:05.120
 openly I had a release about multi agents where you had two little agents who were playing hide and

1:15:05.120 --> 1:15:11.520
 seek. And of course, also alpha zero, they will all produce surprising behaviors. They all produce

1:15:11.520 --> 1:15:17.200
 behaviors that we didn't expect. They are creative solutions to problems. And that seems like an

1:15:17.200 --> 1:15:23.840
 important part of AGI that our systems don't exhibit routinely right now. And so that's why I

1:15:23.840 --> 1:15:27.440
 like this area, I like this direction because of its ability to surprises.

1:15:27.440 --> 1:15:32.720
 To surprises. And AGI system would surprise us fundamentally. Yes. But and to be precise, not

1:15:32.720 --> 1:15:38.240
 just a not just a random surprise, but to find the surprising solution to a problem that's also

1:15:38.240 --> 1:15:45.520
 useful. Right. Now, a lot of the self playing mechanisms have been used in the game context,

1:15:45.520 --> 1:15:55.120
 or at least in the simulation context. How much, how much, how far along the path to AGI do you

1:15:55.120 --> 1:16:02.160
 think will be done in simulation? How much faith promise do you have in simulation versus having

1:16:02.160 --> 1:16:09.280
 to have a system that operates in the real world, whether it's the real world of digital real world

1:16:09.280 --> 1:16:14.240
 data or real world, like actual physical world with robotics? I don't think it's in either war.

1:16:14.800 --> 1:16:19.520
 I think simulation is a tool. And it helps it has certain its strengths and certain weaknesses.

1:16:19.520 --> 1:16:24.640
 And we should use it. Yeah, but okay. I understand that that's

1:16:24.640 --> 1:16:34.320
 that's true. But one of the criticisms of self play, one of the criticisms and reinforcement

1:16:34.320 --> 1:16:42.160
 learning is one of the the its current power, its current results, while amazing have been

1:16:42.160 --> 1:16:46.240
 demonstrated in a simulated environments, or very constrained physical environments,

1:16:46.240 --> 1:16:51.600
 do you think it's possible to escape them, escape the simulator environments and be able to learn

1:16:51.600 --> 1:16:57.520
 in non simulated environments? Or do you think it's possible to also just simulate in a photo

1:16:57.520 --> 1:17:03.680
 realistic, and physics realistic way, the real world in a way that we can solve real problems

1:17:03.680 --> 1:17:10.400
 with self play in simulation. So I think that's transfer from simulation to the real world is

1:17:10.400 --> 1:17:16.240
 definitely possible, and has been exhibited many times in by many different groups. It's been

1:17:16.240 --> 1:17:22.640
 especially successful in vision. Also open AI in the summer has demonstrated a robot hand which

1:17:22.640 --> 1:17:28.160
 was trained entirely in simulation, in a certain way that allowed for seem to real transfer to occur.

1:17:29.680 --> 1:17:34.560
 This is for the Rubik's cube. That's right. And I wasn't aware that was trained in simulation

1:17:34.560 --> 1:17:39.680
 was trained in simulation entirely. Really? So what it wasn't in the physical that the hand

1:17:39.680 --> 1:17:45.840
 wasn't trained? No, 100% of the training was done in simulation. And the policy that was

1:17:45.840 --> 1:17:50.800
 learned in simulation was trained to be very adaptive. So adaptive that when you transfer it,

1:17:50.800 --> 1:17:55.360
 it could very quickly adapt to the physical to the physical world. So the kind of perturbations

1:17:55.360 --> 1:18:01.680
 with the giraffe or whatever the heck it was, those weren't were those part of the simulation?

1:18:01.680 --> 1:18:07.280
 Well, the simulation was generally so the simulation was trained to be robust to many

1:18:07.280 --> 1:18:11.680
 different things, but not the kind of perturbations we've had in the video. So it's never been

1:18:11.680 --> 1:18:18.160
 trained with a glove. It's never been trained with a stuff giraffe. So in theory, these are

1:18:18.160 --> 1:18:23.680
 novel perturbations? Correct. It's not a theory in practice. And that those are novel perturbations?

1:18:23.680 --> 1:18:30.480
 Well, that's okay. That's a clean, small scale, but clean example of a transfer from the simulated

1:18:30.480 --> 1:18:35.440
 world to the physical world. Yeah. And I will also say that I expect the transfer capabilities

1:18:35.440 --> 1:18:40.320
 of deep learning to increase in general. And the better the transfer capabilities are,

1:18:40.320 --> 1:18:45.440
 the more useful simulation will become. Because then you could take you could

1:18:46.240 --> 1:18:50.880
 experience something in simulation, and then learn a moral of the story, which you could then

1:18:50.880 --> 1:18:55.200
 carry with you to the real world, right? As humans do all the time in the computer games.

1:18:56.880 --> 1:19:06.000
 So let me ask sort of an embodied question, staying an AGI for a sec. Do you think AGI system

1:19:06.000 --> 1:19:12.880
 would need to have a body? We need to have some of those human elements of self awareness, consciousness,

1:19:12.880 --> 1:19:19.120
 sort of fear of mortality, sort of self preservation in the physical space, which comes with having

1:19:19.120 --> 1:19:24.720
 a body? I think having a body will be useful. I don't think it's necessary. But I think it's

1:19:24.720 --> 1:19:30.160
 very useful to have a body for sure, because you can learn a whole new you can learn things which

1:19:30.160 --> 1:19:34.960
 cannot be learned without a body. But at the same time, I think that you can go if you don't have

1:19:34.960 --> 1:19:40.480
 a body, you could compensate for it and still succeed. You think so? Yes. Well, there is evidence

1:19:40.480 --> 1:19:45.920
 for this. For example, there are many people who were born deaf and blind, and they were able to

1:19:45.920 --> 1:19:52.080
 compensate for the lack of modalities. I'm thinking about Helen Kahler specifically. So even if you're

1:19:52.080 --> 1:19:57.760
 not able to physically interact with the world, and if you're not able to, I mean, I actually was

1:19:57.760 --> 1:20:04.800
 getting at maybe let me ask on the more particular, I'm not sure if it's connected to having a body

1:20:04.800 --> 1:20:11.120
 or not, but the idea of consciousness and a more constrained version of that is self awareness.

1:20:11.120 --> 1:20:17.680
 Do you think an AGI system should have consciousness? It's what we can't define kind of whatever the

1:20:17.680 --> 1:20:22.800
 heck you think consciousness is. Yeah, hard question to answer, given how hard is to define it.

1:20:24.560 --> 1:20:28.960
 Do you think it's useful to think about? I mean, it's definitely interesting. It's fascinating.

1:20:29.680 --> 1:20:32.880
 I think it's definitely possible that our systems will be conscious.

1:20:32.880 --> 1:20:37.360
 Do you think that's an emergent thing that just comes from? Do you think consciousness could

1:20:37.360 --> 1:20:42.400
 emerge from the representation that's stored within your networks? So like that it naturally just

1:20:42.400 --> 1:20:46.960
 emerges when you become more and more able to represent more and more of the world?

1:20:46.960 --> 1:20:54.480
 Well, let's say I'd make the following argument, which is humans are conscious. And if you believe

1:20:54.480 --> 1:21:01.200
 that artificial neural nets are sufficiently similar to the brain, then there should at least

1:21:01.200 --> 1:21:06.000
 exist artificial neural nets you should be conscious to. You're leaning on that existence proof pretty

1:21:06.000 --> 1:21:16.320
 heavily. Okay. But that's the best answer I can give. No, I know. I know. I know. There's still

1:21:16.320 --> 1:21:21.680
 an open question if there's not some magic in the brain that we're not. I mean, I don't mean

1:21:21.680 --> 1:21:28.080
 a non materialistic magic, but that the brain might be a lot more complicated and interesting

1:21:28.080 --> 1:21:33.440
 than we give it credit for. If that's the case, then it should show up. And at some point,

1:21:33.440 --> 1:21:37.360
 at some point, we will find out that we can't continue to make progress. But I think it's

1:21:37.360 --> 1:21:42.400
 unlikely. So we talk about consciousness, but let me talk about another poorly defined concept of

1:21:42.400 --> 1:21:48.400
 intelligence. Again, we've talked about reasoning. We've talked about memory. What do you think is

1:21:48.400 --> 1:21:55.520
 a good test of intelligence for you? Are you impressed by the test that Alan Turing formulated

1:21:55.520 --> 1:22:01.680
 with the imitation game of natural language? Is there something in your mind that you will be

1:22:02.800 --> 1:22:09.040
 deeply impressed by if a system was able to do? I mean, lots of things. There's certain

1:22:09.040 --> 1:22:15.360
 frontier. There is a certain frontier of capabilities today. And there exists things

1:22:15.360 --> 1:22:20.720
 outside of that frontier. And I would be impressed by any such thing. For example, I would be

1:22:20.720 --> 1:22:27.120
 impressed by a deep learning system, which solves a very pedestrian, you know, pedestrian task

1:22:27.120 --> 1:22:33.680
 like machine translation or computer vision task or something, which never makes mistake a human

1:22:33.680 --> 1:22:39.920
 wouldn't make under any circumstances. I think that is something which have not yet been demonstrated.

1:22:39.920 --> 1:22:44.720
 And I would find it very impressive. Yes. So right now, they make mistakes in different,

1:22:44.720 --> 1:22:49.040
 they might be more accurate than human beings, but they still make a different set of mistakes.

1:22:49.040 --> 1:22:55.680
 So my, my, I would guess that a lot of the skepticism that some people have about deep learning

1:22:55.680 --> 1:22:58.160
 is when they look at their mistakes and they say, well, those mistakes,

1:22:59.200 --> 1:23:02.720
 they make no sense. Like if you understood the concept, you wouldn't make that mistake us.

1:23:03.920 --> 1:23:09.600
 And I think that changing that would be would would that would inspire me that would be,

1:23:09.600 --> 1:23:15.280
 yes, this is this is this is this is progress. Yeah, that's a really nice way to put it.

1:23:15.280 --> 1:23:21.600
 But I also just don't like that human instinct to criticize a model is not intelligent. That's

1:23:21.600 --> 1:23:29.040
 the same instinct as we do when we criticize any group of creatures as the other. Because

1:23:31.280 --> 1:23:36.320
 it's very possible that GPT two is much smarter than human beings at many things.

1:23:36.320 --> 1:23:39.280
 That's definitely true. It has a lot more breadth of knowledge.

1:23:39.280 --> 1:23:46.400
 Yes, breadth of knowledge and even, and even perhaps depth on certain topics. It's kind of

1:23:46.400 --> 1:23:52.560
 hard to judge what depth means, but there's definitely a sense in which humans don't make

1:23:52.560 --> 1:23:58.160
 mistakes that these models do. Yes, the same is applied to autonomous vehicles. The same is

1:23:58.160 --> 1:24:02.240
 probably going to continue being applied to a lot of artificial intelligence systems. We find

1:24:02.880 --> 1:24:07.840
 this is the annoying thing. This is the process of in the 21st century, the process of analyzing

1:24:07.840 --> 1:24:15.760
 the progress of AI is the search for one case where the system fails in a big way where humans

1:24:15.760 --> 1:24:23.600
 would not. And then many people writing articles about it. And then broadly, as the public

1:24:23.600 --> 1:24:28.800
 generally gets convinced that the system is not intelligent. And we like pacify ourselves by

1:24:28.800 --> 1:24:33.520
 thinking it's not intelligent because of this one anecdotal case. And this can seems to continue

1:24:33.520 --> 1:24:38.400
 happening. Yeah, I mean, there is truth to that. Although I'm sure that plenty of people are also

1:24:38.400 --> 1:24:42.160
 extremely impressed by the systems that exist today. But I think this connects to the earlier

1:24:42.160 --> 1:24:49.440
 point we discussed that it's just confusing to judge progress in AI. And you have a new robot

1:24:49.440 --> 1:24:55.360
 demonstrating something. How impressed should you be? And I think that people will start to be

1:24:55.360 --> 1:25:01.440
 impressed once AI starts to really move the needle on the GDP. So you're one of the people that

1:25:01.440 --> 1:25:07.920
 might be able to create an AGS system here, not you, but you and open AI. If you do create an

1:25:07.920 --> 1:25:15.680
 AGS system, and you get to spend sort of the evening with it, him, her, what would you talk

1:25:15.680 --> 1:25:20.720
 about do you think? The very first time? The first time? Well, the first time was just,

1:25:21.680 --> 1:25:26.000
 we would just ask all kinds of questions and try to make it to get it to make a mistake. And that

1:25:26.000 --> 1:25:34.000
 would be amazed that it doesn't make mistakes and just keep asking broad. What kind of questions do

1:25:34.000 --> 1:25:41.120
 you think? Would they be factual or would they be personal, emotional, psychological? What do you

1:25:41.120 --> 1:25:51.440
 think? All of the above. Would you ask for advice? Definitely. I mean, why would I limit myself

1:25:51.440 --> 1:25:57.280
 talking to a system like this? Now, again, let me emphasize the fact that you truly are one of

1:25:57.280 --> 1:26:04.400
 the people that might be in the room where this happens. So let me ask a sort of a profound

1:26:04.400 --> 1:26:11.360
 question about, I've just talked to Stalin historian, been talking to a lot of people who

1:26:11.360 --> 1:26:18.240
 are studying power. Abraham Lincoln said, nearly all men can stand adversity. But if you want to

1:26:18.240 --> 1:26:26.320
 test a man's character, give him power. I would say the power of the 21st century, maybe the 22nd,

1:26:26.320 --> 1:26:32.640
 but hopefully the 21st would be the creation of an AGI system and the people who have control,

1:26:33.360 --> 1:26:39.920
 direct possession and control of the AGI system. So what do you think after spending that evening

1:26:41.120 --> 1:26:45.040
 having a discussion with the AGI system? What do you think you would do?

1:26:45.040 --> 1:26:56.160
 Well, the ideal world I'd like to imagine is one where humanity are like the board,

1:26:56.960 --> 1:27:03.760
 the board members of a company where the AGI is the CEO. So it would be,

1:27:05.520 --> 1:27:09.760
 I would like the picture of which I would imagine is you have some kind of different

1:27:09.760 --> 1:27:17.520
 entities, different countries or cities and the people that leave their vote for what the AGI

1:27:17.520 --> 1:27:22.160
 that represents them should do and the AGI that represents them goes and does it. I think a picture

1:27:22.160 --> 1:27:28.560
 like that, I find very appealing. You could have multiple AGI, you would have an AGI for a city,

1:27:28.560 --> 1:27:35.920
 for a country and it would be trying to in effect take the democratic process to the next level.

1:27:35.920 --> 1:27:40.480
 And the board can almost fire the CEO. Essentially, press the reset button, say.

1:27:40.480 --> 1:27:42.800
 Press the reset. Rerandomize the parameters.

1:27:42.800 --> 1:27:48.240
 Well, let me sort of, that's actually, okay, that's a beautiful vision, I think,

1:27:48.960 --> 1:27:54.960
 as long as it's possible to press the reset button. Do you think it will always be possible to

1:27:54.960 --> 1:28:00.000
 press the reset button? So I think that it's definitely really possible to build.

1:28:00.000 --> 1:28:11.600
 So you're talking, so the question that I really understand from you is, will humans or humans

1:28:11.600 --> 1:28:16.720
 people have control over the AI systems that they build? Yes. And my answer is, it's definitely

1:28:16.720 --> 1:28:22.640
 possible to build AI systems which will want to be controlled by their humans. Wow, that's part of

1:28:22.640 --> 1:28:32.560
 their, so it's not that just they can't help but be controlled, but that's one of the objectives

1:28:32.560 --> 1:28:37.360
 of their existence is to be controlled. In the same way that human parents

1:28:39.760 --> 1:28:45.200
 generally want to help their children, they want their children to succeed. It's not a burden for

1:28:45.200 --> 1:28:51.360
 them. They are excited to help the children and to feed them and to dress them and to take care of

1:28:51.360 --> 1:28:59.040
 them. And I believe with high conviction that the same will be possible for an AGI. It will be

1:28:59.040 --> 1:29:04.800
 possible to program an AGI, to design it in such a way that it will have a similar deep drive that

1:29:04.800 --> 1:29:12.160
 it will be delighted to fulfill and the drive will be to help humans flourish. But let me take a step

1:29:12.160 --> 1:29:16.880
 back to that moment where you create the AGI system. I think this is a really crucial moment.

1:29:16.880 --> 1:29:27.360
 And between that moment and the Democratic Board members with the AGI at the head,

1:29:28.800 --> 1:29:35.680
 there has to be a relinquishing of power. So it's George Washington, despite all the bad

1:29:35.680 --> 1:29:40.240
 things he did, one of the big things he did is he relinquished power. He first of all didn't want

1:29:40.240 --> 1:29:46.240
 to be president. And even when he became president, he gave, he didn't keep just serving as most

1:29:46.240 --> 1:29:56.240
 dictators do for indefinitely. Do you see yourself being able to relinquish control over an AGI system

1:29:56.240 --> 1:30:01.840
 given how much power you can have over the world at first financial, just make a lot of money,

1:30:02.640 --> 1:30:09.200
 and then control by having possession as a AGI system? I'd find it trivial to do that. I'd find

1:30:09.200 --> 1:30:14.960
 it trivial to relinquish this kind of power. I mean, the kind of scenario you are describing

1:30:14.960 --> 1:30:21.200
 sounds terrifying to me. That's all. I would absolutely not want to be in that position.

1:30:22.320 --> 1:30:28.640
 Do you think you represent the majority or the minority of people in the AGI community?

1:30:29.280 --> 1:30:35.600
 Well, I mean, it's an open question, an important one. Are most people good is another way to ask it.

1:30:35.600 --> 1:30:44.240
 So I don't know if most people are good. But I think that when it really counts,

1:30:44.240 --> 1:30:48.240
 people can be better than we think. That's beautifully put. Yeah.

1:30:49.040 --> 1:30:53.840
 Are there specific mechanisms you can think of aligning AI gene values to human values?

1:30:54.400 --> 1:31:00.160
 Is that do you think about these problems of continued alignment as we develop the AI systems?

1:31:00.160 --> 1:31:07.200
 Yeah, definitely. In some sense, the kind of question which you are asking is,

1:31:07.200 --> 1:31:11.840
 so if you have to translate the question to today's terms, it would be a question about

1:31:13.280 --> 1:31:20.160
 how to get an RL agent that's optimizing a value function, which itself is learned.

1:31:21.040 --> 1:31:24.880
 And if you look at humans, humans are like that because the reward function,

1:31:24.880 --> 1:31:28.560
 the value function of humans is not external, it is internal.

1:31:28.560 --> 1:31:36.720
 That's right. And there are definite ideas of how to train a value function,

1:31:36.720 --> 1:31:41.280
 basically an objective, an as objective as possible perception system

1:31:42.400 --> 1:31:50.160
 that will be trained separately to recognize, to internalize human judgments on different

1:31:50.160 --> 1:31:56.000
 situations. And then that component wouldn't be integrated as the value as the base value

1:31:56.000 --> 1:32:00.400
 function for some more capable RL system. You could imagine a process like this.

1:32:00.400 --> 1:32:05.360
 I'm not saying this is the process. I'm saying this is an example of the kind of thing you could do.

1:32:07.360 --> 1:32:13.120
 So on that topic of the objective functions of human existence, what do you think is the

1:32:13.120 --> 1:32:28.960
 objective function that's implicit in human existence? What's the meaning of life? I think

1:32:28.960 --> 1:32:34.720
 the question is wrong in some way. I think that the question implies that there is an

1:32:34.720 --> 1:32:39.120
 objective answer, which is an external answer, you know, your meaning of life is X. I think

1:32:39.120 --> 1:32:45.280
 what's going on is that we exist and that's amazing. And we should try to make the most

1:32:45.280 --> 1:32:52.560
 of it and try to maximize our own value and enjoyment of a very short time while we do exist.

1:32:53.360 --> 1:32:57.280
 It's funny because action does require an objective function. It's definitely there

1:32:57.280 --> 1:33:02.720
 in some form, but it's difficult to make it explicit and maybe impossible to make it explicit.

1:33:02.720 --> 1:33:08.000
 I guess is what you're getting at. And that's an interesting fact of an RL environment.

1:33:08.000 --> 1:33:13.920
 Well, what I was making a slightly different point is that humans want things and their wants

1:33:13.920 --> 1:33:19.680
 create the drives that cause them to, you know, our wants are our objective functions,

1:33:19.680 --> 1:33:24.080
 our individual objective functions. We can later decide that we want to change,

1:33:24.080 --> 1:33:27.040
 that what we wanted before is no longer good and we want something else.

1:33:27.040 --> 1:33:32.000
 Yeah, but they're so dynamic. There's got to be some underlying sort of Freud.

1:33:32.000 --> 1:33:37.040
 There's things, there's like sexual stuff. There's people who think it's the fear of death.

1:33:37.040 --> 1:33:41.920
 And there's also the desire for knowledge and, you know, all these kinds of things,

1:33:41.920 --> 1:33:46.880
 procreation, sort of all the evolutionary arguments, it seems to be,

1:33:46.880 --> 1:33:53.040
 there might be some kind of fundamental objective function from which everything else emerges.

1:33:53.920 --> 1:33:56.400
 But it seems like it's very difficult to make it explicit.

1:33:56.400 --> 1:33:59.520
 I think that probably is an evolutionary objective function, which is to survive and

1:33:59.520 --> 1:34:06.160
 procreate and make your students succeed. That would be my guess. But it doesn't give an answer

1:34:06.160 --> 1:34:11.760
 to the question of what's the meaning of life. I think you can see how humans are part of this

1:34:11.760 --> 1:34:19.520
 big process, this ancient process we are, we are, we exist on a small planet. And that's it.

1:34:20.720 --> 1:34:25.040
 So given that we exist, try to make the most of it and try to

1:34:25.040 --> 1:34:30.880
 enjoy more and suffer less as much as we can. Let me ask two silly questions about life.

1:34:30.880 --> 1:34:39.280
 One, do you have regrets, moments that if you went back, you would do differently? And two,

1:34:39.920 --> 1:34:43.280
 are there moments that you're especially proud of that made you truly happy?

1:34:44.640 --> 1:34:51.840
 So I can answer both questions. Of course, there's a huge number of choices and decisions that

1:34:51.840 --> 1:34:56.160
 have made that with the benefit of hindsight, I wouldn't have made them. And I do experience

1:34:56.160 --> 1:35:01.360
 some regret, but, you know, I try to take solace in the knowledge that at the time I did the best

1:35:01.360 --> 1:35:06.880
 I could. And in terms of things that I'm proud of, I'm very fortunate to have done things I'm

1:35:06.880 --> 1:35:12.720
 proud of. And they made me happy for some time, but I don't think that that is the source of

1:35:12.720 --> 1:35:18.720
 happiness. So your academic accomplishments, all the papers, you're one of the most cited people

1:35:18.720 --> 1:35:24.720
 in the world, all the breakthroughs I mentioned in computer vision and language and so on is

1:35:24.720 --> 1:35:30.960
 what is the source of happiness and pride for you? I mean, all those things are a source of pride,

1:35:30.960 --> 1:35:36.320
 for sure. I'm very grateful for having done all those things. And it was very fun to do them.

1:35:37.280 --> 1:35:42.800
 But happiness comes, you know, you can, happiness, well, my current view is that happiness comes from

1:35:42.800 --> 1:35:48.480
 our to a lot to a very large degree from the way we look at things. You know, you can have a

1:35:48.480 --> 1:35:53.760
 simple meal and be quite happy as a result, or you can talk to someone and be happy as a result

1:35:53.760 --> 1:35:59.440
 as well. Or conversely, you can have a meal and be disappointed that the meal wasn't a better meal.

1:36:00.240 --> 1:36:03.840
 So I think a lot of happiness comes from that, but I'm not sure. I don't want to be too confident.

1:36:05.440 --> 1:36:12.000
 Being humble in the face of the uncertainty seems to be also a part of this whole happiness thing.

1:36:12.000 --> 1:36:17.760
 Well, I don't think there's a better way to end it than meaning of life and discussions of happiness.

1:36:17.760 --> 1:36:23.600
 So, Ilya, thank you so much. You've given me a few incredible ideas. You've given the world

1:36:23.600 --> 1:36:27.280
 many incredible ideas. I really appreciate it. And thanks for talking today.

1:36:27.280 --> 1:36:29.280
 Yeah, thanks for stopping by. I really enjoyed it.

1:36:30.320 --> 1:36:34.000
 Thanks for listening to this conversation with Ilya Setskever. And thank you to our

1:36:34.000 --> 1:36:39.440
 presenting sponsor, Cash App. Please consider supporting the podcast by downloading Cash App

1:36:39.440 --> 1:36:45.200
 and using the code Lex Podcast. If you enjoy this podcast, subscribe on YouTube,

1:36:45.200 --> 1:36:50.320
 review it with Five Stars and Apple Podcasts. Support on Patreon or simply connect with me

1:36:50.320 --> 1:36:58.160
 on Twitter at Lex Freedman. And now let me leave you with some words from Alan Turing on Machine

1:36:58.160 --> 1:37:04.800
 Learning. Instead of trying to produce a program to simulate the adult mind, why not rather try

1:37:04.800 --> 1:37:11.200
 to produce one which simulates the child? If this were then subjected to an appropriate course of

1:37:11.200 --> 1:37:27.120
 education, one would obtain the adult brain. Thank you for listening and hope to see you next time.

