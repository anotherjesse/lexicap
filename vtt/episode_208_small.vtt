WEBVTT

00:00.000 --> 00:05.440
 The following is a conversation with Jeff Hawkins, a neuroscientist seeking to understand

00:05.440 --> 00:09.360
 the structure, function, and the origin of intelligence in the human brain.

00:10.080 --> 00:15.760
 He previously wrote a seminal book on the subject titled On Intelligence, and recently

00:15.760 --> 00:21.120
 a new book called A Thousand Brains, which presents a new theory of intelligence

00:21.120 --> 00:26.800
 that Richard Dawkins, for example, has been raving about, calling the book, quote,

00:26.800 --> 00:33.280
 brilliant and exhilarating. I can't read those two words and not think of him saying it in his

00:33.280 --> 00:40.400
 British accent. Quick mention of our sponsors, Code Academy, Biooptimizers, ExpressVPN,

00:40.400 --> 00:44.560
 ASleep, and Blinkist. Check them out in the description to support this podcast.

00:45.200 --> 00:50.320
 As a side note, let me say that one small but powerful idea that Jeff Hawkins mentions in his

00:50.320 --> 00:57.360
 new book is that if human civilization were to destroy itself, all of knowledge, all our creations

00:57.360 --> 01:03.120
 will go with us. He proposes that we should think about how to save that knowledge in a way that

01:03.120 --> 01:10.320
 long outlives us, whether that's on Earth, in orbit around Earth, or in deep space. And then

01:10.320 --> 01:15.200
 to send messages that advertise this backup of human knowledge to other intelligent alien

01:15.200 --> 01:24.800
 civilizations. The main message of this advertisement is not that we are here, but that we were once

01:24.800 --> 01:32.240
 here. This little difference somehow was deeply humbling to me, that we may with some nonzero

01:32.240 --> 01:37.600
 likelihood destroy ourselves, and that an alien civilization, thousands or millions of years

01:37.600 --> 01:43.760
 from now may come across this knowledge store, and they would only with some low probability even

01:43.760 --> 01:49.840
 notice it, not to mention be able to interpret it. And the deeper question here for me is what

01:49.840 --> 01:55.040
 information in all of human knowledge is even essential? Does Wikipedia capture it or not at

01:55.040 --> 02:00.800
 all? This thought experiment forces me to wonder what are the things we've accomplished and are

02:00.800 --> 02:07.200
 hoping to still accomplish that will outlive us? Is it things like complex buildings, bridges,

02:07.200 --> 02:15.120
 cars, rockets? Is it ideas like science, physics, and mathematics? Is it music and art? Is it

02:15.120 --> 02:20.320
 computers, computational systems, or even artificial intelligence systems? I personally

02:20.320 --> 02:26.960
 can't imagine that aliens wouldn't already have all of these things. In fact, much more and much

02:26.960 --> 02:33.120
 better. To me, the only unique thing we may have is consciousness itself, and the actual

02:33.120 --> 02:40.400
 subjective experience of suffering, of happiness, of hatred, of love. If we can record these experiences

02:40.400 --> 02:45.040
 in the highest resolution directly from the human brain, such that aliens will be able to replay

02:45.040 --> 02:52.080
 them, that is what we should store and send as a message. Not Wikipedia, but the extremes of

02:52.080 --> 02:58.720
 conscious experiences, the most important of which, of course, is love. This is the Lex

02:58.720 --> 03:06.400
 Friedman podcast, and here is my conversation with Jeff Hawkins. We previously talked over two

03:06.400 --> 03:11.600
 years ago. Do you think there's still neurons in your brain that remember that conversation,

03:12.160 --> 03:18.240
 that remember me and got excited? There's a Lex neuron in your brain that just finally has a purpose?

03:18.240 --> 03:23.520
 I do remember our conversation, or I have some memories of it, and I formed additional memories

03:23.520 --> 03:29.360
 of you in the meantime. I wouldn't say there's a neuron or a neuron in my brain that know you,

03:29.360 --> 03:35.520
 but there are synapses in my brain that have formed that reflect my knowledge of you and

03:35.520 --> 03:40.640
 the model I have of you in the world. Whether the exact same synapses were formed two years ago,

03:40.640 --> 03:45.600
 it's hard to say because these things come and go all the time. One thing to note about

03:45.600 --> 03:49.040
 brains is that when you think of things, you often erase the memory and rewrite it again.

03:49.040 --> 03:54.880
 So yes, but I have a memory of you, and that's instantiated in synapses. There's a simpler way

03:54.880 --> 04:01.200
 to think about it, Lex. We have a model of the world in your head, and that model is continually

04:01.200 --> 04:06.080
 being updated. I updated it this morning. You offered me this water, you said it was from the

04:06.080 --> 04:12.400
 refrigerator. I remember these things, and so the model includes where we live, the places we know,

04:12.400 --> 04:16.560
 the words, the objects in the world, but it's just monstrous model, and it's constantly being

04:16.560 --> 04:22.080
 updated, and people are just part of that model. So are animals, so are other physical objects,

04:22.080 --> 04:29.760
 so are events we've done. So it's no special in my mind, special place for the memories of humans.

04:29.760 --> 04:39.440
 I mean, obviously, I know a lot about my wife, but and friends, and so on, but it's not like a

04:39.440 --> 04:43.920
 special place for humans were over here, but we model everything, and we model other people's

04:43.920 --> 04:48.480
 behaviors, too. So if I said, there's a copy of your mind in my mind, it's just because I know

04:48.480 --> 04:55.200
 how humans, I've learned how humans behave, and I learned some things about you, and that's part

04:55.200 --> 05:01.680
 of my world model. Well, I just also mean the collective intelligence of the human species.

05:02.320 --> 05:09.440
 I wonder if there's something fundamental to the brain that enables that, so modeling other

05:09.440 --> 05:14.560
 humans with their ideas. You're actually jumping into a lot of big profits, like collective

05:14.560 --> 05:18.240
 intelligence is a separate topic that a lot of people like to talk about, we can talk about that.

05:19.360 --> 05:25.120
 But so that's interesting, like, you know, we're not just individuals, we live in society and so on.

05:25.920 --> 05:30.960
 But from our research point of view, and so again, let's just talk, we study the neocortex,

05:30.960 --> 05:36.960
 it's a sheet of neural tissue, it's about 75% of your brain, it runs on this very repetitive

05:36.960 --> 05:44.000
 algorithm. It's a very repetitive circuit. And so you can apply that algorithm to lots of different

05:44.000 --> 05:47.200
 problems, but it's all underneath, it's the same thing, we're just building this model.

05:47.760 --> 05:52.320
 So from our point of view, we wouldn't look for these special circuits someplace buried in your

05:52.320 --> 05:58.240
 brain that might be related to understanding other humans. It's more like, how do we build a

05:58.240 --> 06:02.000
 model of anything? How do we understand anything in the world? And humans are just another part

06:02.000 --> 06:08.720
 of the things we understand. So there's nothing, there's nothing to the brain that knows the

06:08.720 --> 06:13.120
 emergent phenomenon of collective intelligence? Well, I certainly know about that. I've heard

06:13.120 --> 06:17.760
 the terms I've read. No, but that's, right. Well, okay, right. As an idea. Well, I think we have

06:17.760 --> 06:22.480
 language, which is sort of built into our brains, and that's a key part of collective intelligence.

06:22.480 --> 06:27.920
 So there are some, you know, prior assumptions about the world we're going to live in, when we're

06:27.920 --> 06:34.960
 born, we're not just a blank slate. And so, you know, did we evolve to take advantage of those

06:34.960 --> 06:38.960
 situations? Yes. But again, we study only part of the brain, the neocortex. There's other parts

06:38.960 --> 06:46.160
 of the brain are very much involved in societal interactions and human emotions and how we interact

06:46.160 --> 06:53.520
 and even societal issues about, you know, how we interact with other people when we support them,

06:53.520 --> 06:58.240
 when we're greedy and things like that. I mean, certainly the brain is a great place

07:00.240 --> 07:06.960
 where to study intelligence. I wonder if it's the fundamental atom of intelligence? Well,

07:06.960 --> 07:12.000
 I would say it's absolutely an essential component, even if you believe in collective

07:12.000 --> 07:16.320
 intelligence as, hey, that's where it's all happening. That's what we need to study,

07:16.320 --> 07:19.360
 which I don't believe that, by the way. I think it's really important, but I don't think that is

07:19.360 --> 07:26.080
 the thing. But even if you do believe that, then you have to understand how the brain works in

07:26.080 --> 07:31.360
 doing that. It's, you know, it's more like we are intelligent, we are intelligent individuals,

07:31.360 --> 07:35.360
 and together we are much more magnified, our intelligence. We can do things that we couldn't

07:35.360 --> 07:40.640
 do individually. But even as individuals, we're pretty damn smart. And we can model things and

07:40.640 --> 07:45.440
 understand the world and interact with it. So to me, if you're going to start someplace,

07:45.440 --> 07:50.160
 you need to start with the brain, then you could say, well, how do brains interact with each other?

07:50.160 --> 07:55.200
 And what is the nature of language? And how do we share models that I've learned something about

07:55.200 --> 07:59.280
 the world? How do I share it with you? Which is really what, you know, sort of communal

07:59.280 --> 08:03.520
 intelligence is. I know something, you know something. We've had different experiences in

08:03.520 --> 08:07.040
 the world. I've learned something about brains. Maybe I can impart that to you. You've learned

08:07.040 --> 08:13.440
 something about, you know, whatever physics and you can part that to me. But it all comes down to

08:13.440 --> 08:17.840
 even just the epistemological question of, well, what is knowledge and how do you represent it in

08:17.840 --> 08:23.200
 the brain? Right? And it's not, that's where it's going to reside, right? Or in our writings.

08:23.200 --> 08:29.920
 It's obvious that human collaboration, human interaction is how we build societies. But

08:29.920 --> 08:37.760
 some of the things you talk about and work on, some of those elements of what makes up an intelligent

08:37.760 --> 08:44.240
 entity is there with a single person. Absolutely. I mean, we can't deny that the brain is the core

08:44.240 --> 08:50.000
 element here in, at least I think it's obvious, the brain is the core element in all theories of

08:50.000 --> 08:56.000
 intelligence. It's where knowledge is represented. It's where knowledge is created. We interact,

08:56.000 --> 09:01.920
 we share, we build upon each other's work. But without a brain, you'd have nothing. You know,

09:01.920 --> 09:08.080
 there would be no intelligence without brains. And so that's where we start. I got into this field

09:08.080 --> 09:13.040
 because I just was curious as to who I am. You know, how do I think? What's going on in my head

09:13.040 --> 09:17.840
 when I'm thinking? What does it mean to know something? I can ask what it means for me to

09:17.840 --> 09:22.880
 know something independent of how I learned it from you or from someone else or from society.

09:22.880 --> 09:26.400
 What does it mean for me to know that I have a model of you in my head? What does it mean to

09:26.400 --> 09:29.920
 know I know what this microphone does and how it works physically, even when I can't see it right

09:29.920 --> 09:35.760
 now? How do I know that? What does it mean? How do the neurons do that at the fundamental level

09:35.760 --> 09:41.520
 of neurons and synapses and so on? Those are really fascinating questions. And I'm happy to

09:42.560 --> 09:52.080
 just happy to understand those if I could. So in your new book, you talk about our brain,

09:52.080 --> 09:57.920
 our mind as being made up of many brains. So the book is called A Thousand Brains,

09:57.920 --> 10:01.680
 A Thousand Brain Theory of Intelligence. What is the key idea of this book?

10:02.800 --> 10:09.360
 The book has three sections and it has sort of maybe three big ideas. So the first section is

10:09.360 --> 10:13.760
 all about what we've learned about the neocortex and that's the thousand brains theory. Just to

10:13.760 --> 10:17.040
 complete the picture, the second section is all about AI and the third section is about the future

10:17.040 --> 10:26.720
 of humanity. So the thousand brains theory, the big idea there, if I had to summarize into one

10:26.720 --> 10:32.880
 one big idea, is that we think of the brain, the neocortex is learning this model of the world.

10:33.440 --> 10:38.560
 But what we learned is actually there's tens of thousands of independent modeling systems going

10:38.560 --> 10:44.080
 on. And so each, what we call a column in the cortex with about 150,000 of them is a complete

10:44.080 --> 10:50.080
 modeling system. So it's a collective intelligence in your head in some sense. So the thousand brains

10:50.080 --> 10:54.400
 theory says about where do I have knowledge about, you know, this coffee cup or where is the model

10:54.400 --> 10:59.280
 of this cell phone? It's not in one place. It's in thousands of separate models that are complementary

10:59.280 --> 11:03.440
 and they communicate with each other through voting. So this idea that we have, we feel like

11:03.440 --> 11:08.640
 we're one person, you know, that's our experience, we can explain that. But reality, there's lots of

11:08.640 --> 11:13.680
 these like, it's almost like little brains, like, but they're sophisticated modeling systems,

11:13.680 --> 11:19.760
 about 150,000 of them in each human brain. And that's a totally different way of thinking about

11:19.760 --> 11:24.240
 how the neocortex is structured than we or anyone else thought of even just five years ago.

11:25.040 --> 11:30.960
 So you mentioned you started this journey just looking in the mirror and trying to understand

11:30.960 --> 11:35.360
 who you are. So if you have many brains, who are you then?

11:35.920 --> 11:40.000
 So it's interesting, we have a singular perception, right? You know, we think, oh, I'm just here,

11:40.000 --> 11:43.840
 I'm looking at you. But it's, it's composed of all these things, like there's sounds and there's,

11:43.840 --> 11:48.800
 and there's this vision and there's touch and all kinds of inputs yet we have the singular

11:48.800 --> 11:52.720
 perception. And what the 1000 brain theory says, we have these models that are visual models,

11:52.720 --> 11:56.880
 we have audit models, auditory models, models of talk to models and so on, but they vote.

11:57.680 --> 12:02.480
 And so they send in the cortex, you can think about that these columns as about like little

12:02.480 --> 12:09.040
 grains of rice, 150,000 stacked next to each other. And each one is its own little modeling system.

12:09.600 --> 12:14.480
 But they have these long range connections that go between them. And we call those voting

12:14.480 --> 12:20.960
 connections or voting neurons. And so the different columns try to reach a consensus,

12:20.960 --> 12:24.800
 like, what am I looking at? Okay, you know, each one has some ambiguity, but they come to

12:24.800 --> 12:30.640
 a consensus. Oh, there's a water bottle, I'm looking at. We are only consciously able to

12:30.640 --> 12:36.000
 perceive the voting. We're not able to perceive anything that goes under the hood. So the voting

12:36.000 --> 12:41.760
 is what we're aware of. The results of the vote. Yeah, the voting. Well, it's, you can imagine

12:41.760 --> 12:45.280
 it this way. We were just talking about eye movements a moment ago. So as I'm looking at

12:45.280 --> 12:49.120
 something, my eyes are moving about three times a second. And with each movement,

12:49.120 --> 12:53.360
 a completely new input is coming into the brain. It's not repetitive. It's not shifting around.

12:53.360 --> 12:58.320
 It's completely new. I'm totally unaware of it. I can't perceive it. But yet if I looked at the

12:58.320 --> 13:02.080
 neurons in your brain, they're going on and off, on and off, on and off, on up. But the voting neurons

13:02.080 --> 13:05.760
 are not. The voting neurons are saying, you know, we all agree, even though I'm looking at different

13:05.760 --> 13:10.720
 parts of this is a water bottle right now. And that's not changing. And it's in some position

13:10.720 --> 13:15.520
 and pose relative to me. So I have this perception of the water bottle about two feet away from me

13:15.520 --> 13:20.480
 at a certain pose to me. That is not changing. That's the only part I'm aware of. I can't be

13:20.480 --> 13:25.040
 aware of the fact of the inputs from the eyes are moving and changing and all this other tapping.

13:25.040 --> 13:31.200
 So these long range connections are the part we can be conscious of. The individual activity in

13:31.200 --> 13:36.560
 each column is doesn't go anywhere else. It doesn't get shared anywhere else. It doesn't,

13:36.560 --> 13:42.480
 there's no way to extract it and talk about it or extract it and even remember it to say, oh,

13:42.480 --> 13:47.760
 yes, I can recall that. So, but these long range connections are the things that are accessible

13:47.760 --> 13:52.960
 to language. And to our, you know, it's like the hippocampus or our memories, you know, our

13:52.960 --> 13:59.840
 short term memory systems and so on. So we're not aware of 95% or maybe it's even 98% of what's

13:59.840 --> 14:06.800
 going on in your brain. We're only aware of this sort of stable, somewhat stable voting outcome

14:06.800 --> 14:09.280
 of all these things that are going on underneath the hood.

14:09.280 --> 14:15.520
 So what would you say is the basic element in the 1000 brains theory of intelligence,

14:16.160 --> 14:21.360
 of intelligence? Like, what's the atom of intelligence when you think about it?

14:21.360 --> 14:24.480
 Is it the individual brains and then what is a brain?

14:24.480 --> 14:28.400
 Well, let's, let's, can we just talk about what intelligence is first and then,

14:28.400 --> 14:33.760
 and then we can talk about the elements are. So in my, in my book, intelligence is the ability

14:33.760 --> 14:40.240
 to learn a model of the world, to build the internal to your head, a model that represents

14:40.240 --> 14:44.800
 the structure of everything, you know, to know what this is a table and that's a coffee cup and

14:44.800 --> 14:48.720
 this is a goose neck lamp and all these things. To know these things, I have to have a model

14:48.720 --> 14:53.520
 in my head. I just don't look at them and go, what is that? I already have internal representations

14:53.520 --> 14:57.360
 of these things in my head and I had to learn them. I wasn't born of any of that knowledge.

14:57.360 --> 15:01.680
 You were, you know, we have some lights in the room here. I, you know, that's not part of my

15:01.680 --> 15:07.120
 evolutionary heritage, right? It's not in my genes. So we have this incredible model and the model

15:07.120 --> 15:10.800
 includes not only what things look like and feel like, but where they are relative to each other

15:10.800 --> 15:14.880
 and how they behave. I've never picked up this water bottle before, but I know that if I took

15:14.880 --> 15:18.320
 my hand on that blue thing and I turn it, it'll probably make a funny little sound as the little

15:18.320 --> 15:22.480
 plastic things detach and then it'll rotate and it'll rotate a certain way and it'll come off.

15:22.480 --> 15:27.120
 How do I know that? Because I have this model in my head. So the essence of intelligence

15:27.120 --> 15:32.000
 is our ability to learn a model and the more sophisticated our model is, the smarter we are.

15:33.040 --> 15:36.800
 Not that there is a single intelligence because you can know about, you know a lot about things

15:36.800 --> 15:40.400
 that I don't know and I know about things you don't know and we can both be very smart,

15:40.400 --> 15:44.240
 but we both learned a model of the world through interacting with it. So that is the

15:44.240 --> 15:48.960
 essence of intelligence. Then we can ask ourselves, what are the mechanisms in the brain that allow

15:48.960 --> 15:52.320
 us to do that? And what are the mechanisms of learning, not just the neural mechanisms,

15:52.320 --> 15:56.240
 what are the general process by how we learn a model? So that was a big insight for us.

15:56.240 --> 16:00.800
 It's like, what are the, what are the actual things that, how do you learn this stuff?

16:00.800 --> 16:04.480
 It turns out you have to learn it through movement. You can't learn it just by,

16:04.480 --> 16:09.120
 that's how we learn, we learn through movement, we learn. So you build up this model by observing

16:09.120 --> 16:12.080
 things and touching them and moving them and walking around the world and so on.

16:12.080 --> 16:16.720
 So either you move or the thing moves. Somehow. Yeah, obviously you can learn

16:16.720 --> 16:20.160
 things just by reading a book, something like that, but think about if I were to say, oh,

16:20.160 --> 16:23.920
 here's a new house. I want you to learn, you know, what do you do? You have to walk,

16:23.920 --> 16:27.760
 you have to walk from room to room. You have to open the doors, look around,

16:27.760 --> 16:32.000
 see what's on the left, what's on the right. As you do this, you're building a model in your head.

16:32.000 --> 16:35.600
 It's just, that's what you're doing. You can't just sit there and say, I'm going to grok the

16:35.600 --> 16:39.760
 house. No, you know, or you don't even want to just sit down and read some description of it,

16:39.760 --> 16:43.440
 right? You literally physically interact with them. The same with like a smartphone. If I'm

16:43.440 --> 16:47.360
 going to learn a new app, I touch it and I move things around. I see what happens when I,

16:47.360 --> 16:50.640
 when I do things with it. So that's the basic way we learn in the world.

16:50.640 --> 16:55.360
 And by the way, when you say model, you mean something that can be used for prediction

16:55.360 --> 17:00.400
 in the future. It's, it's used for prediction and for behavior and planning.

17:00.960 --> 17:04.640
 Right. And does a pretty good job at doing so.

17:04.640 --> 17:08.320
 Yeah. Here's the way to think about the model. A lot of people get hung up on this. So

17:10.160 --> 17:15.120
 you can imagine an architect making a model of a house, right? So there's a physical model

17:15.120 --> 17:19.120
 that's small. And why don't they do that? Well, we do that because you can imagine what it would

17:19.120 --> 17:22.640
 look like from different angles. Okay, look from here, look from there. And you can also say,

17:22.640 --> 17:27.440
 well, how far to get from the garage to the, to the swimming pool or something like that,

17:27.440 --> 17:30.480
 right? You can imagine looking at this. And so what would be the view from this location?

17:30.480 --> 17:35.120
 So we build these physical models to let you imagine the future and imagine behaviors.

17:35.920 --> 17:40.320
 Now we can take that same model and put it in a computer. So we now, today, they all build

17:41.040 --> 17:44.720
 models of houses in a computer and they, and they do that using a set of,

17:44.720 --> 17:49.840
 we'll come back to this term in a moment, reference frames, but eventually you assign a

17:49.840 --> 17:52.560
 reference frame to the house and you assign different things for the house in different

17:52.560 --> 17:56.720
 locations. And then the computer can generate an image and say, okay, this is what it looks

17:56.720 --> 18:00.640
 like in this direction. The brain is doing something remarkably similar to this.

18:00.640 --> 18:05.280
 Surprising. It's using reference frames. It's building these, it's similar to a model in a

18:05.280 --> 18:09.280
 computer, which has the same benefits of building a physical model. It allows me to say, what would

18:09.280 --> 18:14.080
 this thing look like if it was in this orientation? What would likely happen if I pushed this button?

18:14.080 --> 18:20.720
 I've never pushed this button before. Or how would I accomplish something? I want to convey

18:20.720 --> 18:25.360
 a new idea of learned. How would I do that? I can imagine in my head, well, I could talk about it.

18:25.360 --> 18:32.000
 I could write a book. I could do some podcasts. I could, you know, maybe tell my neighbor,

18:32.000 --> 18:36.080
 you know, and I can imagine the outcomes of all these things before I do any of them.

18:36.080 --> 18:41.840
 That's the model that you do. Let's just plan the future and imagine the consequences or actions.

18:41.840 --> 18:47.600
 Prediction, you asked about prediction. Prediction is not the goal of the model. Prediction is an

18:47.600 --> 18:51.520
 inherent property of it. And it's how the model corrects itself.

18:52.320 --> 18:57.760
 So prediction is fundamental to intelligence. It's fundamental to building a model and the

18:57.760 --> 19:02.000
 model's intelligent. And let me go back and be very precise about this. Prediction,

19:02.000 --> 19:05.200
 you can think of prediction two ways. One is like, hey, what would happen if I did this?

19:05.200 --> 19:09.600
 That's the type of prediction. That's a key part of intelligence. But it isn't prediction. It's like,

19:09.600 --> 19:15.200
 oh, what's this water bottle going to feel like when I pick it up? And that doesn't seem very

19:15.200 --> 19:21.520
 intelligent. But the way to think, one way to think about prediction is it's a way for us to learn

19:21.520 --> 19:27.520
 where our model is wrong. So if I picked up this water bottle and it felt hot, I'd be very surprised.

19:27.520 --> 19:32.400
 Or if I picked up it was very light, it would be very, I'd be surprised. Or if I turned this top

19:32.400 --> 19:37.040
 and it didn't, I had to turn it the other way, I'd be surprised. And so all those might have

19:37.040 --> 19:40.480
 a prediction like, okay, I'm going to do it. I'm going to drink some water. Okay, I do this.

19:40.480 --> 19:44.000
 There it is. I feel opening, right? What if I had to turn it the other way? Or what if it's

19:44.000 --> 19:48.880
 split in two? Then I say, oh my gosh, I misunderstood this. I didn't have the right model. This thing,

19:48.880 --> 19:52.720
 my attention would be drawn to, I'll be looking at it going, well, how did that happen? Why did it

19:52.720 --> 19:57.520
 open up that way? And I would update my model by doing it. Just by looking at it and playing around

19:57.520 --> 20:02.320
 that update and saying, this is a new type of water bottle. So you're talking about sort of

20:02.320 --> 20:08.560
 complicated things like a water bottle. But this also applies for just basic vision, just like

20:08.560 --> 20:14.240
 seeing things. That's almost like a precondition of just perceiving the world as predicting.

20:15.840 --> 20:20.960
 Everything that you see is first passed through your prediction.

20:20.960 --> 20:26.480
 Everything you see and feel, in fact, this is the insight I had back in the late 80s,

20:26.480 --> 20:33.440
 and excuse me, early 80s. And another people have reached the same idea is that every sensory input

20:33.440 --> 20:41.360
 you get, not just vision, but touch and hearing, you have an expectation about it and a prediction.

20:41.360 --> 20:45.120
 Sometimes you can predict very accurately. Sometimes you can't. I can't predict what next

20:45.120 --> 20:48.400
 word is going to come out of your mouth. But as you start talking, I was better and better

20:48.400 --> 20:53.600
 predictions. And if you talked about some topics, I'd be very surprised. So I have this sort of

20:53.600 --> 20:59.600
 background prediction that's going on all the time for all of my senses. Again, the way I think

20:59.600 --> 21:06.560
 about that is this is how we learn. It's more about how we learn. It's a test of our understanding,

21:06.560 --> 21:10.960
 our predictions, our test. Is this really a water bottle? If it is, I shouldn't see

21:11.840 --> 21:15.200
 a little finger sticking out the side. And if I saw a little finger sticking out, I was like,

21:15.200 --> 21:17.680
 what the hell's going on? That's not normal.

21:17.680 --> 21:26.800
 I mean, that's fascinating. Let me linger on this for a second. It really honestly feels

21:26.800 --> 21:33.280
 that prediction is fundamental to everything, to the way our mind operates, to intelligence.

21:33.280 --> 21:39.840
 So it's just a different way to see intelligence, which is like everything starts a prediction.

21:39.840 --> 21:45.040
 And prediction requires a model. You can't predict something unless you have a model of it.

21:45.040 --> 21:50.080
 Right. But the action is prediction. So the thing the model does is prediction.

21:51.200 --> 21:59.040
 But you can then extend it to things like, what would happen if I took this today? I went and

21:59.040 --> 22:04.320
 did this. What would be likely? Or you can extend prediction to like, oh, I want to get a promotion

22:04.320 --> 22:09.040
 at work. What action should I take? And you can say, if I did this, I could predict what might

22:09.040 --> 22:13.360
 happen. If I spoke to someone, I predict what would happen. So it's not just low level predictions.

22:13.360 --> 22:17.440
 Yeah, it's all predictions. It's all predictions. It's like this black box, so you can ask basically

22:17.440 --> 22:21.200
 any question, low level or high level. So we started off with that observation. It's all,

22:21.200 --> 22:26.480
 it's this nonstop prediction. And I write about this in the book about, and then we asked,

22:26.480 --> 22:30.240
 how do neurons actually make predictions? And physically, like, what does the neuron do when

22:30.240 --> 22:35.360
 it makes a prediction? And, or the neural tissue does when it makes prediction. And then we asked,

22:35.360 --> 22:39.520
 what are the mechanisms by how we build a model that allows you to make prediction? So we started

22:39.520 --> 22:46.720
 with prediction as sort of the fundamental research agenda, if in some sense, like, and say, well,

22:46.720 --> 22:50.800
 we understand how the brain makes predictions. We will understand how it builds these models and how

22:50.800 --> 22:55.680
 it learns. And that's the core of intelligence. So it was like, it was the key that got us in the door

22:55.680 --> 23:00.480
 to say, that is our research agenda. Understand predictions. So in this whole process,

23:00.480 --> 23:10.880
 this, where does intelligence originate, would you say? So if we look at things that are much

23:10.880 --> 23:15.360
 less intelligence to humans, and you start to build up a human, the process of evolution,

23:16.080 --> 23:23.680
 where's this magic thing that has a prediction model or a model that's able to predict that

23:23.680 --> 23:29.360
 starts to look a lot more like intelligence? Is there a place where Richard Dawkins wrote an

23:29.360 --> 23:34.880
 introduction to your, to your book, an excellent introduction? I mean, it puts a lot of things

23:34.880 --> 23:40.320
 into context. And it's funny, just looking at parallels for your book and Darwin's origin of

23:40.320 --> 23:48.640
 species. So Darwin wrote about the origin of species. So what is the origin of intelligence?

23:48.640 --> 23:53.520
 Yeah, well, we have a theory about it. And it's just that is the theory. Theory goes as follows.

23:54.160 --> 23:59.200
 As soon as living things started to move, they're not just floating in sea, they're not just

23:59.200 --> 24:04.960
 a plant, you know, grounded someplace. As soon as they started to move, there was an advantage to

24:04.960 --> 24:10.000
 moving intelligently, to moving in certain ways. And there's some very simple things you can do,

24:10.000 --> 24:16.000
 you know, bacteria or single cell organisms can move towards a source of gradient of food or

24:16.000 --> 24:20.720
 something like that. But an animal that might know where it is and know where it's been and how to

24:20.720 --> 24:25.120
 get back to that place, or an animal that might say, oh, there was a source of food someplace,

24:25.120 --> 24:29.840
 how do I get to it? Or there was a danger, how do I get to it? There was a mate, how do I get to them?

24:31.280 --> 24:35.440
 There was a big evolutionary advantage to that. So early on, there was a pressure to start

24:35.440 --> 24:41.360
 understanding your environment, like, where am I? And where have I been? And what happened in those

24:41.360 --> 24:50.160
 different places? So we still have this neural mechanism in our brains. It's in the mammals,

24:50.160 --> 24:56.400
 it's in the hippocampus and enterinocortex, these are older parts of the brain. And these are very

24:56.400 --> 25:03.120
 well studied. We build a map of our environment. So these neurons in these parts of the brain know

25:03.120 --> 25:08.880
 where I am in this room and where the door was and things like that. So a lot of other mammals

25:08.880 --> 25:14.240
 have this kind of animal? All mammals have this, right? And almost any animal that knows where it

25:14.240 --> 25:20.000
 is and get around must have some mapping system, must have some way of saying, I've learned a map

25:20.000 --> 25:24.800
 of my environment. I have hummingbirds in my backyard. And they go to the same places all the

25:24.800 --> 25:29.360
 time. They must know where they are. They just know where they are. They're not just randomly

25:29.360 --> 25:35.600
 flying around. They know particular flowers they come back to. So we all have this. And it turns

25:35.600 --> 25:41.920
 out it's very tricky to get neurons to do this, to build a map of an environment. And so we now

25:41.920 --> 25:47.200
 know there's these famous studies that's still very active about place cells and grid cells and

25:47.200 --> 25:51.440
 these other types of cells in the older parts of the brain and how they build these maps of the

25:51.440 --> 25:55.600
 world. It's really clever. It's obviously been under a lot of evolutionary pressure over a long

25:55.600 --> 26:01.040
 period of time to get good at this. So animals know where they are. What we think has happened,

26:01.920 --> 26:06.080
 and there's a lot of evidence that suggests this, is that mechanism we learned to map

26:06.080 --> 26:16.480
 a space is was repackaged. The same type of neurons was repackaged into a more compact form.

26:17.840 --> 26:22.080
 And that became the cortical column. And it was in some sense,

26:22.080 --> 26:26.800
 genericized, if that's a word. It was turned into a very specific thing about learning maps

26:26.800 --> 26:32.000
 of environments to learning maps of anything, learning a model of anything, not just your

26:32.000 --> 26:39.200
 space, but coffee cups and so on. And it got sort of repackaged into a more compact version,

26:39.200 --> 26:45.760
 a more universal version, and then replicated. So the reason we're so flexible is we have a very

26:45.760 --> 26:52.160
 generic version of this mapping algorithm, and we have 150,000 copies of it. Sounds a lot like

26:52.160 --> 26:59.600
 the progress of deep learning. How so? To take neural networks that seem to work well for a

26:59.600 --> 27:09.120
 specific task, compress them, and multiply it by a lot. And then you just stack them on top of it.

27:09.120 --> 27:13.680
 It's like the story of transformers in natural language processing.

27:13.680 --> 27:18.080
 Deep learning networks, they end up, you're replicating an element, but you still need

27:18.080 --> 27:24.560
 the entire network to do anything. Here, what's going on, each individual element is a complete

27:24.560 --> 27:30.880
 learning system. This is why I can take a human brain, cut it in half, and it still works. It's

27:30.880 --> 27:34.960
 pretty amazing. It's fundamentally distributed. It's fundamentally distributed, complete modeling

27:34.960 --> 27:42.880
 systems. But that's our story we like to tell. I would guess it's likely largely right,

27:43.760 --> 27:47.920
 but there's a lot of evidence supporting that story, this evolutionary story.

27:47.920 --> 27:55.120
 The thing which brought me to this idea is that the human brain got big very quickly,

27:55.920 --> 28:01.680
 so that led to the proposal a long time ago that, well, there's this common element just

28:01.680 --> 28:06.720
 instead of creating new things, it just replicated something. We also are extremely flexible. We

28:06.720 --> 28:14.000
 can learn things that we had no history about. So that tells it that the learning algorithm is

28:14.000 --> 28:19.760
 very generic. It's very universal because it doesn't assume any prior knowledge about what it's

28:19.760 --> 28:26.000
 learning. So you combine those things together and you say, okay, well, how did that come about?

28:26.000 --> 28:29.600
 Where did that universal algorithm come from? It had to come from something that wasn't universal.

28:29.600 --> 28:34.160
 It came from something that was more specific. Anyway, this led to our hypothesis that you

28:34.160 --> 28:39.840
 would find grid cells and play cell equivalents in the New York Cortex. When we first published our

28:39.840 --> 28:44.240
 first papers on this theory, we didn't know of evidence for that. It turns out there was some,

28:44.240 --> 28:49.200
 but we didn't know about it. And since then, so then we became aware of evidence for grid cells

28:49.200 --> 28:53.040
 in certain parts of the New York Cortex. And then now there's been new evidence coming out. There's

28:53.040 --> 28:58.800
 some interesting papers that came out just January of this year. So one of our predictions was if

28:58.800 --> 29:03.760
 this evolutionary hypothesis is correct, we would see grid cell play cell equivalents, cells that

29:03.760 --> 29:07.440
 work like them through every column in the New York Cortex, and that's starting to be seen.

29:07.440 --> 29:11.760
 And what does it mean that why is it important that they're present?

29:11.760 --> 29:16.000
 Because it tells us, well, we're asking about the evolutionary origin of intelligence, right?

29:16.000 --> 29:21.840
 So our theory is that these columns in the Cortex are working on the same principles,

29:22.640 --> 29:27.840
 the modeling systems. And it's hard to imagine how neurons do this. And so we said, hey,

29:28.640 --> 29:32.000
 it's really hard to imagine how neurons could learn these models of things. We can talk about

29:32.000 --> 29:37.920
 the details of that if you want. But let's, but there's this other part of the brain, we know

29:37.920 --> 29:43.280
 the learned models of environments. So could that mechanism to learn to model this room be

29:43.280 --> 29:48.320
 used to learn to model the water bottle? Is it the same mechanism? So we said it's much more

29:48.320 --> 29:53.520
 likely the brain's using the same mechanism, which case it would have these equivalent cell types.

29:54.160 --> 29:59.360
 So it's basically the whole theory is built on the idea that these columns have reference frames

29:59.360 --> 30:04.000
 and they're learning these models and these grid cells create these reference frames. So it's

30:04.000 --> 30:10.080
 basically the major, in some sense, the major predictive part of this theory is that we will

30:10.080 --> 30:15.760
 find these equivalent mechanisms in each column in the near Cortex, which tells us that that's

30:15.760 --> 30:20.560
 what they're doing. They're learning these sensory model models of the world. So just

30:21.120 --> 30:23.840
 we're pretty confident that would happen. But now we're seeing the evidence.

30:23.840 --> 30:28.560
 So the evolutionary process nature does a lot of copy and paste and see what happens.

30:28.560 --> 30:34.320
 Yeah. Yeah, there's no direction to it. But, but it just found out like, Hey, if I took this,

30:34.320 --> 30:38.400
 these elements and made more of them, what happens? And let's hook them up to the eyes and

30:38.400 --> 30:44.880
 let's hook them to ears. And that seems to work pretty well for us. Again, just to take a quick

30:44.880 --> 30:51.120
 step back to our conversation of collective intelligence. Do you sometimes see that as just

30:51.120 --> 31:00.400
 another copy and paste aspect is copying pasting these brains and humans and making a lot of them

31:00.400 --> 31:06.240
 and then creating social structures that then almost operates as a single brain?

31:06.240 --> 31:08.320
 I wouldn't have said it, but you said it sounded pretty good.

31:10.160 --> 31:15.040
 So to you, the brain is fundamental is like, is it something?

31:15.040 --> 31:20.880
 Yeah. I mean, our goal is to understand how the neocortex works. We can argue how essential that

31:20.880 --> 31:25.440
 is to understanding human brain because it's not the entire human brain. You can argue how

31:25.440 --> 31:29.920
 essential that is to understanding human intelligence. You can argue how essential this

31:29.920 --> 31:38.080
 to, to, you know, sort of communal intelligence. I'm not, I didn't, our goal was to understand

31:38.080 --> 31:43.680
 the neocortex. Yeah. So what is the neocortex and where does it fit in the various aspect of

31:43.680 --> 31:48.800
 what the brain does? Like, how important is it to you? Well, obviously, again, we, I mentioned

31:48.800 --> 31:55.360
 again, in the beginning, it's, it's about 70 to 75% of the volume of a human brain. So it, you know,

31:55.360 --> 31:59.840
 it dominates our brain in terms of size, not in terms of number of neurons, but in terms of size.

32:00.640 --> 32:06.560
 Size isn't everything, Jeff. I know. But it's, it's nothing, it's nothing to be, it's not that.

32:06.560 --> 32:11.760
 We know that all high level vision, hearing and touch happens in neocortex. We know that

32:11.760 --> 32:16.480
 all language occurs and is understood in the neocortex, whether that's spoken language, written

32:16.480 --> 32:21.040
 language, sign language, whether language of mathematics, language of physics, music, math,

32:21.040 --> 32:25.920
 you know, we know that all high level planning and thinking occurs in the neocortex. If I were to

32:25.920 --> 32:30.480
 say, you know, what part of your brain designed a computer and understands programming and, and

32:30.480 --> 32:36.240
 creates music, it's all the neocortex. So then that's a kind of undeniable fact.

32:36.240 --> 32:41.360
 If, but then there's other parts of our brain are important too, right? Our emotional states,

32:41.360 --> 32:48.800
 our body regulating our body. So the way I like to look at it is, you know, could you, can you

32:49.760 --> 32:53.120
 understand the neocortex without the rest of the brain? And some people say you can't,

32:53.120 --> 32:58.400
 and I think absolutely you can. It's not that they're not interacting, but you can understand it.

32:58.400 --> 33:01.840
 Can you understand the neocortex without understanding the emotions of fear? Yes,

33:01.840 --> 33:05.920
 you can. You can understand how the system works. It's just a modeling system. I make the,

33:05.920 --> 33:11.040
 the analogy in the book that it's, it's like a map of the world and how that map is used

33:11.040 --> 33:17.040
 depends on who's using it. So how our map of our world in our neocortex, how we, how we

33:17.040 --> 33:21.360
 manifest as a human depends on the rest of our brain. What are our motivations? You know,

33:21.360 --> 33:25.680
 what are my desires? Am I a nice guy or not a nice guy? Am I a cheater or am I, you know,

33:25.680 --> 33:35.200
 not a cheater? You know, how important different things are in my life. So, so, but the neocortex

33:35.200 --> 33:40.800
 can be understood on its own. And, and I say that as a neuroscientist, I know there's all these

33:40.800 --> 33:45.840
 interactions and I want to say I don't know them and we don't think about them. But a layperson's

33:45.840 --> 33:51.520
 point of view, you can say it's a modeling system. I don't generally think too much about the communal

33:51.520 --> 33:55.840
 aspect of intelligence, which you brought up a number of times already. So that's not really

33:55.840 --> 34:00.560
 been my concern. I just wonder if there's a continuum from the origin of the universe, like

34:00.560 --> 34:10.400
 this pockets of complexities that form living organisms. I wonder if we're just, if you look

34:10.400 --> 34:16.560
 at humans, we feel like we're at the top. And I wonder if there's like just where everybody

34:16.560 --> 34:24.000
 probably, every living type pocket of complexity is probably thinks they're the part in the French,

34:24.000 --> 34:27.760
 they're the shit. They're at the top of the pyramid. Well, if they're thinking.

34:27.760 --> 34:35.680
 Well, and then what is thinking? Well, that in a sense, the whole point is in their sense of the

34:35.680 --> 34:41.840
 world, they, their sense is that they're at the top of it. I think what is the turtle?

34:41.840 --> 34:46.800
 But you're, you're, you're bringing up, you know, the problems of complexity and complexity theory

34:46.800 --> 34:53.600
 are, you know, it's a huge, interesting problem in science. And, you know, I think we've made

34:53.600 --> 35:00.080
 surprisingly little progress in understanding complex systems in general. And so, you know,

35:00.080 --> 35:04.240
 the Santa Fe Institute was founded to, to study this and even the scientists there will say it's

35:04.240 --> 35:09.040
 really hard. We haven't really been able to figure out exactly, you know, that science

35:09.040 --> 35:12.960
 hasn't really congealed yet. We're still trying to figure out the basic elements of that science.

35:13.760 --> 35:17.840
 What, you know, where does complexity come from and what is it and how you define it,

35:17.840 --> 35:24.400
 whether it's DNA, creating bodies or phenotypes or individuals creating societies or ants and,

35:24.400 --> 35:29.680
 you know, markets and so on. It's, it's a very complex thing. I'm not a complexity theorist

35:29.680 --> 35:36.000
 person, right? I, I think you need to ask, well, the brain itself is a complex system. So,

35:36.000 --> 35:40.560
 can we understand that? I think we've made a lot of progress understanding how the brain works.

35:40.560 --> 35:45.920
 So, but I haven't brought it out to like, oh, well, where are we on the complexity spectrum?

35:45.920 --> 35:53.600
 You know, it's like, it's a great question. I prefer for that answer to be, we're not special.

35:54.560 --> 36:00.240
 It seems like if we're honest, most likely we're not special. So, if there is a spectrum,

36:00.240 --> 36:04.000
 we're probably not in some kind of significant place. I think there's one thing we could say

36:04.000 --> 36:09.760
 that we are special. And again, only here on earth, I'm not saying I'm bad, is that if we

36:09.760 --> 36:20.240
 think about knowledge, what we know, we clearly, human brains have, the only brains have a certain

36:20.240 --> 36:24.720
 types of knowledge. We're the only brains on, on this earth to understand what the earth is,

36:24.720 --> 36:30.400
 how old it is, that the universe is a picture as a whole with the only organisms understand DNA and

36:30.400 --> 36:37.200
 the origins of, you know, of species. No other species on, on this planet has that knowledge.

36:37.200 --> 36:43.440
 So, if we think about, I like to think about, you know, one of the endeavors of humanity is to

36:43.440 --> 36:49.920
 understand the universe as much as we can. I think our species is further along in that,

36:49.920 --> 36:54.800
 undeniably, whether our theories are right or wrong, we can debate, but at least we have theories.

36:54.800 --> 37:00.480
 You know, we, we know that what the sun is and how fusion is and how what black holes are and,

37:00.480 --> 37:05.120
 you know, we know general theory and relativity and no other animal has any of this knowledge.

37:05.120 --> 37:11.840
 So, from that sense, we're special. Are we special in terms of the, the hierarchy of complexity in,

37:11.840 --> 37:13.120
 in the universe? Probably not.

37:16.560 --> 37:22.800
 Can we look at a neuron? Yeah, you say that prediction happens in the neuron. What does

37:22.800 --> 37:27.440
 that mean? So, the neuron tradition is seen as the basic element of the, the brain.

37:27.440 --> 37:31.760
 So, we, I mentioned this earlier, that prediction was our research agenda.

37:31.760 --> 37:37.840
 Yeah. We said, okay, how does the brain make a prediction? Like, I'm about to grab this water

37:37.840 --> 37:42.720
 bottle and my brain is predicting what I'm going to feel on, on all my parts of my fingers. If I

37:42.720 --> 37:46.560
 felt something really odd on any part here, I'd notice it. So, my brain is predicting what it's

37:46.560 --> 37:51.360
 going to feel as I grab this thing. So, what is that? How does that manifest itself in neural

37:51.360 --> 37:57.360
 tissue, right? We got brains made of neurons and there's chemicals and there's neurons and there's

37:57.360 --> 38:03.360
 spikes and the connect, you know, where is, where is the prediction going on? And one argument could

38:03.360 --> 38:09.360
 be that, well, when I'm predicting something, a neuron must be firing in advance. It's like, okay,

38:09.360 --> 38:14.080
 this neuron represents what you're going to feel and it's firing. It's sending a spike. And certainly,

38:14.080 --> 38:19.840
 that happens to some extent. But our predictions are so ubiquitous that we're making so many of them,

38:19.840 --> 38:23.280
 which we're totally unaware of. Just the vast majority of them have no idea that you're doing

38:23.280 --> 38:29.520
 this. That it, there wasn't really, we were trying to figure how could this be? Where are these,

38:29.520 --> 38:34.880
 where are these happening, right? And I won't walk you through the whole story unless you insist on

38:34.880 --> 38:42.480
 it, but we came to the realization that most of your predictions are occurring inside individual

38:42.480 --> 38:47.600
 neurons, especially these, the most common neuron, the pyramidal cells. And there are, there's a

38:47.600 --> 38:52.400
 property of neurons. I mean, everyone knows, or most people know that a neuron is a cell and it

38:52.400 --> 38:57.760
 has this spike called an action potential and it sends information. But we now note that there's

38:57.760 --> 39:02.480
 these spikes internal to the neuron. They're called dendritic spikes. They travel along the

39:02.480 --> 39:07.200
 branches of the neuron and they don't leave the neuron. They're just internal only. They're far

39:07.200 --> 39:11.920
 more dendritic spikes than there are action potentials, far more. They're happening all the

39:11.920 --> 39:17.680
 time. And what we came to understand that those dendritic spikes, the ones that are occurring,

39:17.680 --> 39:23.280
 are actually a form of prediction. They're telling the neuron, the neuron is saying, I expect that

39:23.280 --> 39:28.720
 I might become active shortly. And that internal, so the internal spike is a way of saying,

39:29.440 --> 39:33.760
 you're going to, you might be generating external spikes soon. I predicted you're going to become

39:33.760 --> 39:40.800
 active. And we've, we've, we wrote a paper in 2016, which explained how this manifests itself

39:40.800 --> 39:47.040
 in neural tissue and how it is that this all works together. But the vast, we think it's,

39:47.040 --> 39:51.600
 there's a lot of evidence supporting it. So we, that's where we think that most of these predictions

39:51.600 --> 39:55.440
 are internal. That's why you can't be, the internal neuron, you can't perceive them.

39:56.960 --> 40:01.680
 From understanding the, the prediction mechanism of a single neuron, do you think there's deep

40:01.680 --> 40:06.640
 insights to be gained about the prediction capabilities of the mini brains within the

40:06.640 --> 40:09.600
 bigger brain and the brain? Oh yeah. Yeah. Yeah. So having a prediction

40:09.600 --> 40:13.200
 side of their individual neuron is not that useful. You know, what, so what?

40:13.200 --> 40:22.240
 The way it manifests itself in neural tissue is that when a neuron, a neuron emits these spikes

40:22.240 --> 40:26.720
 are a very singular type of vent. If a neuron is predicting that it's going to be active,

40:26.720 --> 40:31.280
 it emits its spike very a little bit sooner, just a few milliseconds sooner than it would

40:31.280 --> 40:35.200
 have otherwise. It's like, I give the analogy to the book, it's like a sprinter on a starting

40:35.200 --> 40:41.920
 block in a race. And if someone says, get ready, set, you get up and you're ready to go. And then

40:41.920 --> 40:45.920
 when your race starts, you get a little bit earlier start. So that, it's that, that ready set is like

40:45.920 --> 40:50.400
 the prediction and the neurons like ready to go quicker. And what happens is when you have a whole

40:50.400 --> 40:55.120
 bunch of neurons together and they're all getting these inputs, the ones that are in the predictive

40:55.120 --> 40:59.520
 state, the ones that are anticipating to become active, if they do become active, they, they

40:59.520 --> 41:03.520
 happen sooner, they disable everything else and it leads to different representations in the brain.

41:03.520 --> 41:09.600
 So you have to, it's not isolated just to the neuron, the prediction occurs with the neuron,

41:09.600 --> 41:14.880
 but the network behavior changes. So what happens under different predictions, different inputs

41:14.880 --> 41:20.800
 have different representations. So how I, what I predict is going to be different under different

41:20.800 --> 41:24.960
 contexts, you know, what my input will be is different under different contexts. So this is,

41:24.960 --> 41:31.200
 this is a key to the whole theory, how this works. So the theory of the 1000 brains, if you were to

41:31.200 --> 41:36.880
 count the number of brains, how would you do it? The 1000 main theory says that basically every

41:36.880 --> 41:43.600
 cortical column in the, in your neocortex is a complete modeling system. And that when I ask

41:43.600 --> 41:47.360
 where do I have a model of something like a coffee cup, it's not in one of those models,

41:47.360 --> 41:51.200
 it's in thousands of those models. There's thousands of models of coffee cups. That's what

41:51.200 --> 41:54.960
 the 1000 brains, there's a voting mechanism, then there's a voting mechanism, which you

41:54.960 --> 41:58.240
 leads, which you're, which is the thing you're, which you're conscious of, which leads to your

41:58.240 --> 42:04.880
 singular perception. That's why you, you perceive something. So that's the 1000 brain theory.

42:04.880 --> 42:11.680
 The details of how we got to that theory are complicated. It wasn't, we just thought of it

42:11.680 --> 42:16.320
 one day. And one of those details that we had to ask, how does a model make predictions? And we

42:16.320 --> 42:20.800
 talked about just these predictive neurons. That's part of this theory. That's like saying, oh,

42:20.800 --> 42:24.080
 it's a detail, but it was like a crack in the door. It's like, how are we going to figure out how

42:24.080 --> 42:28.400
 these neurons build, do this? You know, what is going on here? So we just looked at prediction

42:28.400 --> 42:32.480
 as like, well, we know that's ubiquitous. We know that every part of the cortex is making

42:32.480 --> 42:37.280
 predictions. Therefore, whatever the predictive system is, it's going to be everywhere. We know

42:37.280 --> 42:41.200
 there's a gazillion predictions happening at once. So this is, we can start teasing apart,

42:41.760 --> 42:46.560
 you know, ask questions about, you know, how can neurons be making these predictions? And that

42:46.560 --> 42:50.720
 sort of built up to now what we have this 1000 brain theory, which is complex, you know, which

42:50.720 --> 42:55.920
 is, I can state it simply, but we just didn't think of it. We had to get there step by step,

42:55.920 --> 43:04.560
 very, it took years to get there. And where does reference frames fit in? So yeah.

43:04.560 --> 43:10.800
 Okay. So again, a reference frame, I mentioned earlier about the, you know, model of a house.

43:10.800 --> 43:14.160
 And I said, if you're going to build a model of a house in a computer, they have a reference

43:14.160 --> 43:18.480
 frame. And you can think of referencing like Cartesian coordinates, like X, Y and Z axes.

43:19.200 --> 43:24.000
 So I can say, oh, I'm going to design a house. I can say, well, the front door is at this location,

43:24.000 --> 43:28.560
 XYZ and the roof is at this location, XYZ and so on. That's the type of reference frame.

43:29.440 --> 43:33.440
 So it turns out for you to make a prediction and then I walk you through the thought experiment

43:33.440 --> 43:37.360
 in the book where I was predicting what my finger was going to feel when I touched a coffee cup,

43:37.920 --> 43:45.200
 was a ceramic coffee cup, but this one will do. And what I realized is that to make a prediction

43:45.200 --> 43:48.480
 when my finger is going to feel like it's going to feel different than this, which it feel different

43:48.480 --> 43:53.520
 if I touch the hole or the thing on the bottom, make that prediction. The cortex needs to know

43:53.520 --> 43:59.520
 where the finger is, the tip of the finger relative to the coffee cup and exactly relative to the

43:59.520 --> 44:03.280
 coffee cup. And to do that, I have to have a reference frame for the coffee cup. There has

44:03.280 --> 44:08.080
 to have a way of representing the location of my finger to the coffee cup. And then we realized,

44:08.080 --> 44:11.360
 of course, every part of your skin has to have a reference frame relative to things that touch.

44:11.360 --> 44:16.240
 And then we did the same thing with vision. But so the idea that a reference frame is necessary

44:16.240 --> 44:19.520
 to make a prediction when you're touching something or when you're seeing something

44:19.520 --> 44:24.160
 and you're moving your eyes or you're moving your fingers, it's just a requirement to know what to

44:24.160 --> 44:28.960
 predict. If I have a structure, I'm going to make a prediction. I have to know where it is.

44:28.960 --> 44:34.160
 I'm looking or touching it. So then we say, well, how do neurons make reference frames?

44:34.160 --> 44:39.840
 It's not obvious. You know, XYZ coordinates don't exist in the brain. It's just not the way it works.

44:39.840 --> 44:43.760
 So that's when we looked at the older part of the brain, the hippocampus and the enteronocortex,

44:43.760 --> 44:49.520
 where we knew that in that part of the brain, there's a reference frame for a room or a reference

44:49.520 --> 44:53.440
 frame for environment. Remember, I talked earlier about how you could make a map of this room.

44:54.320 --> 45:00.240
 So we said, oh, they are implementing reference frames there. So we knew that a reference

45:00.240 --> 45:06.320
 frames needed to exist in every quarter of a column. And so that was a deductive thing. We

45:06.320 --> 45:14.800
 just deduced it. So you take the old mammalian ability to know where you are in a particular

45:14.800 --> 45:19.600
 space and you start applying that to higher and higher levels. Yeah. You first you apply it to

45:19.600 --> 45:23.600
 like where your finger is. So here's what I think about it. The old part of the brain says,

45:23.600 --> 45:29.040
 where's my body in this room? Yeah. The new part of the brain says, where's my finger relative to

45:29.040 --> 45:36.880
 this object? Yeah. Where is a section of my retina relative to this object? I'm looking at one

45:36.880 --> 45:42.160
 little corner. Where is that relative to this patch of my retina? Yeah. And then we take the same

45:42.160 --> 45:48.080
 thing and apply the concepts, mathematics, physics, you know, humanity, whatever you want to think

45:48.080 --> 45:52.160
 of. And eventually you're pondering your own mortality. Well, whatever. But the point is,

45:52.880 --> 45:56.800
 when we think about the world, when we have knowledge about the world, how is that knowledge

45:56.800 --> 46:02.400
 organized, Lex? Where is it in your head? The answer is, it's in reference frames. So the way I

46:02.400 --> 46:07.360
 learned the structure of this water bottle, where the features are relative to each other,

46:07.920 --> 46:12.960
 when I think about history or democracy or mathematics, there's same basic underlying

46:12.960 --> 46:16.480
 structures happening. There's reference frames for where the knowledge that you're signing

46:16.480 --> 46:21.040
 things to. So in the book, I go through examples like mathematics and language and politics.

46:21.040 --> 46:26.400
 But the evidence is very clear in the neuroscience. The same mechanism that we used to model this

46:26.400 --> 46:32.080
 coffee cup, we're going to use to model high level thoughts. You're the demise of the humanity,

46:32.080 --> 46:36.640
 whatever you want to think about. It's interesting to think about how different are the representations

46:36.640 --> 46:43.760
 of those higher dimensional concepts, higher level concepts, how different the representation

46:43.760 --> 46:49.920
 there is in terms of reference frames versus spatial. But interesting thing, it's a different

46:49.920 --> 46:57.120
 application, but it's the exact same mechanism. But isn't there some aspect to higher level

46:57.120 --> 47:02.720
 concepts that they seem to be hierarchical? They just seem to integrate a lot of information

47:02.720 --> 47:10.320
 into them. So is our physical objects. So take this water bottle. I'm not particular to this

47:10.320 --> 47:16.480
 brand, but this is a Fiji water bottle, and it has a logo on it. I use this example in my book,

47:16.480 --> 47:21.920
 in my book, our company's coffee cup has a logo on it. But this object is hierarchical.

47:23.680 --> 47:27.600
 It's got like a cylinder and a cap, but then it has this logo on it, and the logo has a word.

47:27.600 --> 47:32.720
 The word has letters, the letters have different features. And so I don't have to remember,

47:32.720 --> 47:36.160
 I don't have to think about this. So I say, oh, there's a Fiji logo on this water bottle. I don't

47:36.160 --> 47:40.560
 have to go through and say, oh, what is the Fiji logo? It's the F and I and the J and I, and there's

47:40.560 --> 47:45.600
 a hibiscus flower. And oh, it has a, you know, the stamen on it. I don't have to do that. I just

47:45.600 --> 47:51.600
 incorporate all of that in some sort of hierarchical representation. I say, you know, put this logo on

47:51.600 --> 47:57.440
 this water bottle. And, and, and then the logo has a word and the word has letters, all hierarchical.

47:57.440 --> 48:02.000
 It's all that stuff is big. It's amazing that the brain instantly just does all that. The idea

48:02.000 --> 48:08.080
 that there's, there's water, it's liquid, and the idea that you can drink it when you're thirsty,

48:08.080 --> 48:13.840
 the idea that there's brands. And then there's like, all of that information is instantly

48:13.840 --> 48:18.240
 like built into the whole thing once you proceed. So I wanted to get back to your point about

48:18.240 --> 48:23.040
 hierarchical representation. The world itself is hierarchical, right? And I can take this

48:23.040 --> 48:26.240
 microphone in front of me. I know inside there's going to be some electronics. I know there's

48:26.240 --> 48:29.280
 going to be some wires and I know there's going to be a little dive from them was back and forth.

48:30.400 --> 48:35.840
 I don't see that, but I know it. So everything in the world is hierarchical. Just go into room.

48:35.840 --> 48:39.840
 It's composed of other components of kitchen has a refrigerator, you know, the refrigerator has a

48:39.840 --> 48:46.080
 door, the door has a hinge, the hinge has screws and pin. So anyway, the modeling system that

48:46.080 --> 48:52.000
 exists in every cortical column learns the hierarchical structure of objects. So it's a

48:52.000 --> 48:55.360
 very sophisticated modeling system in this grain of rice. It's hard to imagine, but this

48:55.360 --> 48:58.880
 grain of rice can do really sophisticated things. It's got a hundred thousand neurons in it.

49:00.240 --> 49:07.440
 It's very sophisticated. So that same mechanism that can model a water bottle or a coffee cup

49:07.440 --> 49:13.600
 can model conceptual objects as well. That's the beauty of this discovery that this guy,

49:13.600 --> 49:18.720
 Vernon Mountcastle, made many, many years ago, which is that there's a single cortical algorithm

49:18.720 --> 49:24.960
 underlying everything we're doing. So common sense concepts and higher level concepts are

49:24.960 --> 49:29.840
 all represented in the same way. They're set in the same mechanisms. It's a little bit like

49:29.840 --> 49:34.640
 computers, right? All computers are universal Turing machines. Even the little teeny one

49:34.640 --> 49:38.960
 that's in my toaster and the big one that's running some cloud servers someplace.

49:40.080 --> 49:44.080
 They're all running on the same principle. They can apply different things. So the brain is all

49:44.080 --> 49:48.400
 built on the same principle. It's all about learning these models, structured models using

49:48.960 --> 49:54.480
 movement and reference frames. And it can be applied to something as simple as a water bottle

49:54.480 --> 49:58.240
 and a coffee cup. And it can be like just thinking like, what's the future of humanity? And, you

49:58.240 --> 50:05.840
 know, why do you have a hedgehog on your desk? I don't know. Nobody knows. Well, I think it's

50:05.840 --> 50:12.080
 a hedgehog. That's right. It's a hedgehog in the fog. It's a Russian reference. Does it give you any

50:14.080 --> 50:18.480
 inclination or hope about how difficult that is to engineer common sense reasoning?

50:19.200 --> 50:26.480
 So how complicated is this whole process? So looking at the brain, is this a marvel of

50:26.480 --> 50:32.240
 engineering? Or is it pretty dumb stuff stacked on top of each other over a pretty extensive

50:32.240 --> 50:39.760
 copy? Can it be both? Can it be both, right? I don't know if it can be both, because if it's

50:39.760 --> 50:44.720
 an incredible engineering job, that means it's so evolution did a lot of work.

50:46.480 --> 50:52.960
 Yeah, but then it just copied that, right? So as I said earlier, the figuring out how to model

50:52.960 --> 50:57.760
 something like a space is really hard. And evolution had to go through a lot of trick

50:57.760 --> 51:01.760
 and these cells I was talking about, these grid cells and place cells, they're really complicated.

51:01.760 --> 51:07.120
 This is not simple stuff. This neural tissue works on these really unexpected weird mechanisms.

51:08.800 --> 51:13.680
 But it did it. It figured it out. But now you could just make lots of copies of it.

51:13.680 --> 51:18.320
 But then finding, yeah, so it's a very interesting idea that it's a lot of copies

51:18.320 --> 51:25.440
 of a basic mini brain. But the question is, how difficult it is to find that mini brain that

51:25.440 --> 51:34.880
 you can copy and paste effectively? Well, today, we know enough to build this. I'm sitting here with,

51:34.880 --> 51:38.800
 you know, I know the steps we have to go. There's still some engineering problems to solve,

51:38.800 --> 51:44.400
 but we know enough. And it's not like, Oh, this is an interesting idea, we have to go think about

51:44.400 --> 51:50.000
 it in other few decades. No, we actually understand it in pretty well details. So not all the details,

51:50.000 --> 51:56.480
 but most of them. So it's complicated, but it is an engineering problem. So in my company,

51:56.480 --> 52:02.240
 we are working on that. We are basically the roadmap, how we do this. It's not going to take

52:02.240 --> 52:10.480
 decades. It's better a few years, optimistically, but I think that's possible. It's, you know,

52:10.480 --> 52:14.640
 complex things. If you understand them, you can build them. So in which domain do you think it's

52:14.640 --> 52:23.200
 best to build them? Are we talking about robotics, like entities that operate in the physical world

52:23.200 --> 52:26.880
 that are able to interact with that world? Are we talking about entities that operate in the

52:26.880 --> 52:33.200
 digital world? Are we talking about something more like, more specific, like is done in the

52:33.200 --> 52:37.520
 machine learning community, where you look at natural language or computer vision?

52:37.520 --> 52:43.520
 Where do you think is easiest? It's the first, it's the first two more than the third one,

52:43.520 --> 52:52.000
 I would say. Again, let's just use computers as an analogy. The pioneers are computing people

52:52.000 --> 52:56.880
 like John Van Norman on Turing, they created this thing, you know, we now call the universal

52:56.880 --> 53:00.800
 Turing machine, which is a computer, right? Did they know how it was going to be applied,

53:00.800 --> 53:04.320
 where it was going to be used, you know, could they envision any of the future? No,

53:04.320 --> 53:10.320
 they just said, this is like a really interesting computational idea about algorithms and how you

53:10.320 --> 53:17.680
 can implement them in a machine. And we're doing something similar to that today, like we are,

53:17.680 --> 53:22.960
 we are building this sort of universal learning principle that can be applied to many, many

53:22.960 --> 53:27.520
 different things. But the robotics piece of that, the interactive.

53:27.520 --> 53:31.920
 Okay, all right. Let us be specific. You can think of this cortical column as what we call a

53:31.920 --> 53:36.000
 sensory motor learning system. It has the idea that there's a sensor, and then it's moving.

53:36.720 --> 53:41.200
 That sensor can be physical. It could be like my finger, and it's moving in the world. It could

53:41.200 --> 53:48.080
 be like my eye, and it's physically moving. It can also be virtual. So it could be, an example

53:48.080 --> 53:54.080
 would be, I could have a system that lives in the internet that that actually samples information

53:54.080 --> 53:58.320
 on the internet and moves by following links. That's, that's a sensory motor system. So

53:58.320 --> 54:02.800
 it's just something that echoes the process of a finger moving along.

54:02.800 --> 54:08.720
 But in a very, very loose sense, it's, it's like, again, learning is inherently about

54:08.720 --> 54:11.440
 the subring, the structure in the world and discover the structure in the world,

54:11.440 --> 54:16.240
 you have to move through the world, even if it's a virtual world, even if it's a conceptual world,

54:16.240 --> 54:20.800
 you have to move through it. You don't, it doesn't exist in one, it has some structure to it.

54:21.360 --> 54:25.840
 So here's, here's a couple of predictions that getting what you're talking about.

54:25.840 --> 54:32.240
 So in humans, the same algorithm does robotics, right? It moves my arms, my eyes, my body, right?

54:34.560 --> 54:39.040
 And so in my, in the future, to me, robotics and AI will merge. They're not going to be

54:39.040 --> 54:44.160
 separate fields because they're going to, the, the, the algorithms to really controlling robots

54:44.160 --> 54:47.280
 are going to be the same algorithms we have in our brain, that the brain, that these

54:47.280 --> 54:50.400
 sensory motor algorithms, today we're not there, but I think that's going to happen.

54:50.400 --> 54:58.320
 And, and then, so, but not all AI systems will have to be robotics. You can have systems that

54:58.320 --> 55:01.920
 have very different types of embodiments. Some will have physical movements, some will have

55:01.920 --> 55:07.680
 not have physical movements. It's a very generic learning system. Again, it's like computers,

55:07.680 --> 55:10.880
 the Turing machine is, it's like, it doesn't say how it's supposed to be implemented. It doesn't

55:10.880 --> 55:14.000
 tell you how big it is. It doesn't tell you what you can apply it to, but it's an interesting,

55:14.000 --> 55:19.360
 it's a computational principle. Cortical column equivalent is a computational principle about

55:19.360 --> 55:23.760
 learning. It's about how you learn and it can be applied to a gazillion things. This is, I think

55:23.760 --> 55:29.200
 this is, I think this impact of AI is going to be as large, if not larger than computing has been

55:29.200 --> 55:34.480
 in the last century, by far, because it's, it's getting at a fundamental thing. It's not a vision

55:34.480 --> 55:39.360
 system or a learning system. It's a, it's not a vision system or a hearing system. It is a learning

55:39.360 --> 55:42.960
 system. It's a fundamental principle, how you learn the structure in the world, how you gain

55:42.960 --> 55:47.520
 knowledge and be intelligent. And that's what the thousand brain says was going on. And we have a

55:47.520 --> 55:51.280
 particular implementation in our head, but it doesn't have to be like that at all. Do you think

55:51.280 --> 55:58.720
 there's going to be some kind of impact? Okay, let me ask it another way. What do increasingly

55:58.720 --> 56:06.480
 intelligent AI systems do with us humans in the following way? Like, how hard is the human in a

56:06.480 --> 56:13.600
 loop problem? How hard is it to interact the finger on the coffee cup equivalent of having a

56:13.600 --> 56:19.120
 conversation with a human being? So how hard is it to fit into our little human world?

56:20.160 --> 56:25.200
 I don't, I think it's a lot of engineering problems. I don't think it's a fundamental problem.

56:25.200 --> 56:29.440
 I could ask you the same question. How hard is it for computers to fit into a human world?

56:29.440 --> 56:35.040
 Right. That, I mean, that's essentially what I'm asking. Like, how much are we,

56:36.000 --> 56:41.600
 elitist are we as humans? Like, we tried to keep out systems?

56:41.600 --> 56:47.440
 I don't know. I'm not sure. I think I'm not sure that's the right question. Let's look at

56:47.440 --> 56:52.080
 computers as an analogy. Computers are million times faster than us. They do things we can't

56:52.080 --> 56:56.480
 understand. Most people have no idea what's going on when they use computers, right? How

56:56.480 --> 57:02.320
 we integrate them in our society? Well, we don't think of them as their own entity. They're not

57:02.320 --> 57:12.160
 living things. We don't afford them rights. We, we rely on them. Our survival as seven billion

57:12.160 --> 57:14.800
 people or something like that is relying on computers now.

57:15.920 --> 57:21.120
 Don't you think that's a fundamental problem that we see them as something we can't,

57:21.120 --> 57:23.280
 we don't give rights to? Computers?

57:23.280 --> 57:28.320
 So yeah, computers. So robots, computers, intelligent systems, it feels like for them to

57:28.320 --> 57:36.160
 operate successfully, they would need to have a lot of the elements that we would start having

57:36.160 --> 57:39.760
 to think about, like, should this entity have rights?

57:39.760 --> 57:45.520
 I don't think so. I think it's tempting to think that way. First of all, I don't think anyone,

57:45.520 --> 57:49.840
 hardly anyone thinks that's for computers today. No one says, oh, this thing needs a right. I

57:49.840 --> 57:53.760
 shouldn't be able to turn it off. Or, you know, if I throw it in the trash can, you know, and hit

57:53.760 --> 57:59.920
 it with a sledgehammer, my, my, for my criminal act, no, no one thinks that. And now we think

57:59.920 --> 58:06.800
 about intelligent machines, which is where you're going. And, and all of a sudden we're like, well,

58:06.800 --> 58:11.040
 now we can't do that. I think the basic problem we have here is that people think intelligent

58:11.040 --> 58:15.360
 machines will be like us. They're going to have the same emotions as we do, the same feelings as

58:15.360 --> 58:20.320
 we do. What if I can build an intelligent machine that have absolutely could care less about whether

58:20.320 --> 58:24.000
 it was on or off or destroyed or not? It just doesn't care. It's just like a map. It's just

58:24.000 --> 58:31.760
 a modeling system. It has no desires to live, nothing. Is it possible to create a system that

58:31.760 --> 58:38.080
 can model the world deeply and not care about whether it lives or dies? Absolutely. No question

58:38.080 --> 58:43.760
 about it. To me, that's not 100% obvious. It's obvious to me. So we can, we can debate if you

58:43.760 --> 58:51.200
 want. Where does your, where does your desire to live come from? It's an old evolutionary design.

58:51.760 --> 58:57.200
 I mean, we can argue, does it really matter if we live or not? Objectively no, right? We're all

58:57.200 --> 59:04.640
 going to die eventually. But evolution makes us want to live. Evolution makes us want to fight

59:04.640 --> 59:09.600
 to live. Evolutionists want to care and love one another and to care for our children and our,

59:09.600 --> 59:16.160
 and our relatives and our family and, and so on. And those are all good things. But they come about

59:16.160 --> 59:21.120
 not because we're smart, because we're animals that grew up. You know, the hummingbird in my

59:21.120 --> 59:26.400
 backyard cares about its offspring. You know, they, every living thing in some sense cares about,

59:26.400 --> 59:31.600
 you know, surviving. But when we talk about creating intelligent machines, we're not creating

59:31.600 --> 59:36.640
 life. We're not creating evolving creatures. We're not creating living things. We're just

59:36.640 --> 59:41.280
 creating a machine that can learn really sophisticated stuff. And that machine, it may even be able to

59:41.280 --> 59:47.120
 talk to us, but it doesn't, it's not going to have a desire to live unless somehow we put it

59:47.120 --> 59:52.720
 into that system. Well, there's learning, right? The thing is, but you don't learn to like want to

59:52.720 --> 1:00:00.400
 live. It's built into you. It's, well, people like Ernest Becker argue. So, okay, there's the fact

1:00:00.400 --> 1:00:08.560
 of finiteness of life. The way we think about it is something we learned, perhaps. So, okay.

1:00:08.560 --> 1:00:12.880
 Yeah. And some people decide they don't want to live. And some people decide, you know,

1:00:12.880 --> 1:00:18.240
 you can, but the desire to live is built in DNA, right? But I think what I'm trying to get to is,

1:00:18.240 --> 1:00:23.280
 in order to accomplish goals, it's useful to have the urgency of mortality. So what the Stoics

1:00:23.280 --> 1:00:31.440
 talked about is meditating in your mortality. It might be a very useful thing to do, to die

1:00:31.440 --> 1:00:38.560
 and have the urgency of death. And to realize the, to conceive yourself as an entity that operates

1:00:38.560 --> 1:00:43.040
 in this world that eventually will no longer be a part of this world. And actually conceive of

1:00:43.040 --> 1:00:49.600
 yourself as a conscious entity might be very useful for you to be a system that makes sense of the

1:00:49.600 --> 1:00:54.960
 world. Otherwise, you might get lazy. Well, okay. We're going to build these machines, right?

1:00:55.760 --> 1:01:03.440
 So, are we talking about building AI? But we're building the equivalent of the

1:01:03.440 --> 1:01:11.200
 cortical columns. The neocortex. The neocortex. And the question is, where do they arrive at?

1:01:11.200 --> 1:01:17.120
 Because we're not hard coding everything in. Well, in terms of, if you build the neocortex

1:01:17.120 --> 1:01:23.520
 equivalent, it will not have any of these desires or emotional states. Now, you could argue that

1:01:23.520 --> 1:01:28.240
 that neocortex won't be useful unless I give it some agency, unless I give it some desire,

1:01:28.240 --> 1:01:31.760
 unless I give it some motivation. Otherwise, you'll be just lazy to do nothing, right? You

1:01:31.760 --> 1:01:36.960
 could argue that. But on its own, it's not going to do those things. It's just not, it's just not

1:01:36.960 --> 1:01:41.280
 going to sit there and say, I understand the world. Therefore, I care to live. No, it's not going to

1:01:41.280 --> 1:01:46.320
 do that. It's just going to say, I understand the world. Why is that obvious to you? Do you think

1:01:46.320 --> 1:01:53.040
 it's possible? Okay, let me ask it this way. Do you think it's possible it will at least assign to

1:01:53.040 --> 1:02:04.240
 itself agency and perceive itself in this world as being a conscious entity as a useful way to

1:02:04.240 --> 1:02:09.360
 operate in the world and to make sense of the world? I think intelligent machine can be conscious,

1:02:09.360 --> 1:02:16.800
 but that does not, again, imply any of these desires and goals that you're worried about.

1:02:16.800 --> 1:02:20.560
 We can talk about what it means for a machine to be conscious.

1:02:20.560 --> 1:02:24.640
 By the way, not worry about, but get excited about. It's not necessarily that we should worry

1:02:24.640 --> 1:02:29.760
 about it. I think there's a legitimate problem or not a problem. A question asks, if you build

1:02:29.760 --> 1:02:36.320
 this modeling system, what's it going to model? What's its desire? What's its goal? What are we

1:02:36.320 --> 1:02:43.520
 applying it to? That's an interesting question. One thing, and it depends on the application.

1:02:44.080 --> 1:02:48.000
 It's not something that inherent to the modeling system. It's something we apply to the modeling

1:02:48.000 --> 1:02:54.560
 system in a particular way. If I wanted to make a really smart car, it would have to know about

1:02:54.560 --> 1:02:59.680
 driving in cars and what's important in driving in cars. It's not going to figure that out on its

1:02:59.680 --> 1:03:03.760
 own. It's not going to sit there and say, you know, I've understood the world and I've decided,

1:03:03.760 --> 1:03:06.720
 you know, no, no, no, we're going to have to tell it. We're going to have to say like,

1:03:06.720 --> 1:03:10.880
 so I imagine I make this car really smart. It learns about your driving habits. It learns

1:03:10.880 --> 1:03:16.320
 about the world. It's just, you know, is it one day going to wake up and say, you know what,

1:03:16.320 --> 1:03:21.040
 I'm tired of driving and doing what you want. I think I have better ideas about how to spend my

1:03:21.040 --> 1:03:25.760
 time. Okay. No, it's not going to do that. Well, part of me is playing a little bit of devil's

1:03:25.760 --> 1:03:32.000
 advocate, but part of me is also trying to think through this because I've studied cars quite a

1:03:32.000 --> 1:03:36.800
 bit and I've studied pedestrians and cyclists quite a bit. And as part of me that thinks

1:03:38.560 --> 1:03:45.120
 that there needs to be more intelligence than we realize in order to drive successfully,

1:03:46.080 --> 1:03:54.800
 that game theory of human interaction seems to require some deep understanding of human nature.

1:03:54.800 --> 1:04:04.560
 Okay. When a pedestrian crosses the street, there's some sense. They look at a car usually

1:04:05.600 --> 1:04:11.600
 and then they look away. There's some sense in which they say, I believe that you're not going

1:04:11.600 --> 1:04:16.960
 to murder me. You don't have the guts to murder me. This is the little dance of pedestrian car

1:04:16.960 --> 1:04:22.640
 interaction is saying, I'm going to look away and I'm going to put my life in your hands

1:04:22.640 --> 1:04:28.080
 because I think you're human. You're not going to kill me. And then the car in order to successfully

1:04:28.080 --> 1:04:34.080
 operate in like Manhattan streets has to say, no, no, no, no, I am going to kill you like a

1:04:34.080 --> 1:04:39.680
 little bit. There's a little bit of this weird inkling of mutual murder and that's a dance

1:04:39.680 --> 1:04:43.120
 and then somehow successfully operate through that. Do you think you were born of that?

1:04:43.120 --> 1:04:50.000
 Did you learn that social interaction? I think it might have a lot of the same elements that

1:04:50.000 --> 1:04:55.760
 you're talking about, which is we're leveraging things we were born with and applying them in the

1:04:55.760 --> 1:05:03.600
 context that I would have said that that kind of interaction is learned because people in

1:05:03.600 --> 1:05:06.800
 different cultures have different interactions like that. If you cross the street in different

1:05:06.800 --> 1:05:10.240
 cities and different parts of the world, they have different ways of interacting. I would say

1:05:10.240 --> 1:05:14.720
 that's learned and I would say an intelligent system can learn that too, but that does not

1:05:14.720 --> 1:05:23.040
 lead and the intelligent system can understand humans. It could understand that just like I can

1:05:23.040 --> 1:05:27.440
 study an animal and learn something about that animal. I could study apes and learn something

1:05:27.440 --> 1:05:32.640
 about their culture and so on. I don't have to be an ape to know that. I may not be completely,

1:05:32.640 --> 1:05:36.400
 but I can understand something. So intelligent machine can model that. That's just part of

1:05:36.400 --> 1:05:40.560
 the world. It's just part of the interactions. The question we're trying to get at, will the

1:05:40.560 --> 1:05:46.400
 intelligent machine have its own personal agency that's beyond what we assigned to it or its own

1:05:46.400 --> 1:05:53.280
 personal goals or will evolve and create these things? My confidence comes from understanding

1:05:53.280 --> 1:05:58.880
 the mechanisms I'm talking about creating. This is not hand wavy stuff. It's down in the details.

1:05:59.520 --> 1:06:02.880
 I'm going to build it and I know what it's going to look like and I know what's it going to behave.

1:06:02.880 --> 1:06:06.400
 I know what the kind of things it could do and the kind of things it can't do. Just like when I

1:06:06.400 --> 1:06:11.280
 build a computer, I know it's not going to on its own decide to put another register inside

1:06:11.280 --> 1:06:15.520
 of it. It can't do that. It's no way. No matter what your software does, it can't add a register

1:06:15.520 --> 1:06:25.600
 to the computer. So in this way, when we build AI systems, we have to make choices about how we

1:06:25.600 --> 1:06:30.880
 embed them. So I talk about this in the book. I said, intelligent system is not just a neocortex

1:06:30.880 --> 1:06:36.800
 equivalent. You have to have that, but it has to have some kind of embodiment, physical, virtual.

1:06:36.800 --> 1:06:41.280
 It has to have some sort of goals. It has to have some sort of ideas about dangers, about

1:06:41.280 --> 1:06:47.760
 things it shouldn't do like we build in safeguards in the systems. We have them in our bodies. We

1:06:47.760 --> 1:06:53.280
 have put them in the cars. My car follows my directions until the day it sees I'm about to

1:06:53.280 --> 1:06:58.240
 hit something and it ignores my directions and puts the brakes on. So we can build those things in.

1:06:58.240 --> 1:07:06.160
 So that's a very interesting problem, how to build those in. I think my differing opinion

1:07:06.160 --> 1:07:10.960
 about the risks of AI for most people is that people assume that somehow those things will

1:07:10.960 --> 1:07:16.560
 just appear automatically and it will evolve. And intelligence itself begets that stuff or

1:07:16.560 --> 1:07:20.640
 requires it. But it's not. Intelligence of the neocortex equivalent doesn't require this. The

1:07:20.640 --> 1:07:25.520
 neocortex equivalent just says, I'm a learning system. Tell me what you want me to learn. And

1:07:25.520 --> 1:07:31.440
 I'll ask me questions and I'll tell you the answers. But in that, again, it's like a map.

1:07:31.440 --> 1:07:37.360
 It doesn't, a map has no intent about things, but you can use it to solve problems.

1:07:37.360 --> 1:07:44.560
 Okay. So the building, engineering, the neocortex in itself is just creating an intelligent

1:07:44.560 --> 1:07:50.480
 prediction system. Modeling system. Sorry, modeling system. You can use it to then make

1:07:50.480 --> 1:07:56.240
 predictions. But you can also put it inside a thing that's actually acting in this world.

1:07:56.800 --> 1:08:02.000
 You have to put it inside something. Again, think of the map analogy. A map on its own

1:08:02.000 --> 1:08:07.280
 doesn't do anything. It's just inert. It can learn, but it's inert. So we have to embed

1:08:07.280 --> 1:08:13.200
 it somehow in something to do something. So what's your intuition here? You had a conversation

1:08:13.200 --> 1:08:19.920
 with Sam Harris recently that was sort of, you've had a bit of a disagreement and you're

1:08:19.920 --> 1:08:27.680
 sticking on this point. Elon Musk, Stuart Russell kind of have us worry existential

1:08:28.560 --> 1:08:35.360
 threats of AI. What's your intuition? Why, if we engineer an increasingly intelligent

1:08:35.360 --> 1:08:40.960
 neocortex type of system in the computer, why that shouldn't be a thing that we...

1:08:40.960 --> 1:08:44.880
 It was interesting to use the word intuition and Sam Harris used the word intuition too.

1:08:45.680 --> 1:08:48.640
 And when he used that intuition, that word immediately stopped and said,

1:08:48.640 --> 1:08:53.760
 that's the cut to the problem. He's using intuition. I'm not speaking about my intuition.

1:08:53.760 --> 1:08:56.720
 I'm speaking about something I understand, something I'm going to build, something I am

1:08:56.720 --> 1:09:01.440
 building, something I understand completely or at least well enough to know what it's all

1:09:01.440 --> 1:09:07.840
 I'm guessing. I know what this thing's going to do. And I think most people who are worried,

1:09:07.840 --> 1:09:12.560
 they have trouble separating out. They don't have the unknowledge or the understanding about

1:09:12.560 --> 1:09:16.240
 like, what is intelligence? How's it manifest in the brain? How's it separate from these other

1:09:16.240 --> 1:09:20.560
 functions in the brain? And so they imagine it's going to be human like or animal like.

1:09:20.560 --> 1:09:26.320
 It's going to have the same sort of drives and emotions we have, but there's no reason for that.

1:09:27.120 --> 1:09:31.280
 That's just because there's an unknown. If the unknown is like, oh my God,

1:09:31.280 --> 1:09:33.520
 I don't know what this is going to do. We have to be careful. It could be like us,

1:09:33.520 --> 1:09:37.520
 but really smarter. I'm saying, no, it won't be like us. It'll be really smarter,

1:09:37.520 --> 1:09:43.040
 but it won't be like us at all. But I'm coming from that not because I just

1:09:43.040 --> 1:09:47.760
 guessing I'm not using intuition. I'm basically like, okay, I understand this thing works.

1:09:47.760 --> 1:09:54.560
 This is what it does. It makes money to you. Okay. But to push back, so I also disagree with

1:09:54.560 --> 1:10:01.920
 the intuitions that Sam has, but so disagree with what you just said, which, you know,

1:10:01.920 --> 1:10:07.840
 what's a good analogy. So if you look at the Twitter algorithm in the early days,

1:10:07.840 --> 1:10:13.760
 just recommender systems, you can understand how recommender systems work. What you can't

1:10:13.760 --> 1:10:18.480
 understand in the early days is when you apply that recommender system at scale to thousands

1:10:18.480 --> 1:10:25.440
 and millions of people, how that can change societies. So the question is, yes, you're just

1:10:25.440 --> 1:10:31.840
 saying this is how an engineer in your cortex works, but when you have a very useful

1:10:31.840 --> 1:10:39.280
 TikTok type of service that goes viral, when your neural cortex goes viral, and then millions of

1:10:39.280 --> 1:10:43.760
 people start using it, can that destroy the world? No. Well, first of all, this is back,

1:10:43.760 --> 1:10:48.480
 one thing I want to say is that AI is a dangerous technology. I'm not denying that.

1:10:48.480 --> 1:10:52.320
 All technologies dangerous. Well, and AI, maybe particularly so. Okay. So

1:10:53.600 --> 1:10:58.160
 am I worried about it? Yeah, I'm totally worried about it. The thing where the narrow component

1:10:58.160 --> 1:11:03.360
 we're talking about now is the existential risk of AI. So I want to make that distinction because

1:11:03.360 --> 1:11:09.360
 I think AI can be applied poorly. It can be applied in ways that people are going to understand

1:11:09.360 --> 1:11:18.400
 the consequences of it. These are all potentially very bad things, but they're not the AI system

1:11:18.400 --> 1:11:22.800
 creating this existential risk on its own. And that's the only place that I disagree with other

1:11:22.800 --> 1:11:29.280
 people. Right. So I think the existential risk thing is humans are really damn good at surviving.

1:11:29.280 --> 1:11:36.880
 So to kill off the human race would be very, very different. Yes, but I'll go further. I don't think

1:11:36.880 --> 1:11:41.520
 AI systems are ever going to try to. I don't think AI systems are ever going to like say,

1:11:41.520 --> 1:11:46.240
 I'm going to ignore you. I'm going to do what I think is best. I don't think that's going to

1:11:46.240 --> 1:11:53.680
 happen, at least not in the way I'm talking about it. So the Twitter recommendation algorithm

1:11:53.680 --> 1:12:00.480
 is an interesting example. Let's use computers as an analogy again. I build a computer. It's a

1:12:00.480 --> 1:12:04.160
 universal computing machine. I can't predict what people are going to use it for. They can build

1:12:04.160 --> 1:12:09.120
 all kinds of things. They can even create computer viruses. It's all kinds of stuff.

1:12:10.000 --> 1:12:14.560
 So there's some unknown about its utility and about where it's going to go. But in the other

1:12:14.560 --> 1:12:19.520
 hand, I pointed out that once I build a computer, it's not going to fundamentally change how it

1:12:19.520 --> 1:12:23.280
 computes. It's like, I use the example of a register, which is a part internal part of a

1:12:23.280 --> 1:12:28.880
 computer. You know, I say it can't just say it because computers don't evolve. They don't replicate.

1:12:28.880 --> 1:12:32.400
 They don't evolve. They don't, you know, the physical manifestation of the computer itself

1:12:32.400 --> 1:12:37.200
 is not going to, there's certain things that can't do. Right. So we can break into things like

1:12:37.200 --> 1:12:41.200
 things that are possible to happen we can't predict and things that are just impossible to

1:12:41.200 --> 1:12:45.280
 happen. Unless we go out of our way to make them happen, they're not going to happen unless somebody

1:12:45.280 --> 1:12:49.760
 makes them happen. Yeah. So there's, there's a bunch of things to say. One is the physical

1:12:49.760 --> 1:12:55.200
 aspect, which you're absolutely right. We have to build a thing for it to operate in the physical

1:12:55.200 --> 1:13:02.240
 world and you can just stop building them. You know, the moment they're not doing the thing you

1:13:02.240 --> 1:13:06.400
 want them to do or just change the design or change the design. The question is, I mean,

1:13:06.400 --> 1:13:11.360
 there's, it's possible in the physical world, this is probably longer term is you automate the

1:13:11.360 --> 1:13:15.920
 building. It makes, it makes a lot of sense to automate the building. There's a lot of factories

1:13:15.920 --> 1:13:21.360
 that are doing more and more and more automation to go from raw resources to the final product.

1:13:21.360 --> 1:13:26.800
 It's possible to imagine that obviously much more efficient to keep, to create a factory

1:13:26.800 --> 1:13:32.880
 that's creating robots that do something, you know, they do something extremely useful for society.

1:13:32.880 --> 1:13:38.240
 It could be personal assistance. It could be, it could be, it could be your toaster, but a toaster

1:13:38.240 --> 1:13:44.480
 that's much has a deeper knowledge of your culinary preferences. And that could. Well,

1:13:44.480 --> 1:13:47.920
 I think now you've hit on the right thing. The real thing we need to be worried about next is

1:13:47.920 --> 1:13:53.360
 self replication. Right. That is the thing that we're in the physical world or even the virtual

1:13:53.360 --> 1:13:58.560
 world. Self replication, because self replication is dangerous. It's probably more likely to be

1:13:58.560 --> 1:14:03.920
 killed by a virus, you know, or a human engineered virus. Anybody can create a, you know, this

1:14:03.920 --> 1:14:08.240
 technology is getting so almost anybody, well, not anybody, but a lot of people could create

1:14:08.240 --> 1:14:13.760
 a human engineered virus that could wipe out humanity. That is really dangerous. No intelligence

1:14:13.760 --> 1:14:22.400
 required. Just self replication. So, so we need to be careful about that. So when I think about,

1:14:22.400 --> 1:14:27.280
 you know, AI, I'm not thinking about robots building robots. Don't do that. Don't build a,

1:14:27.280 --> 1:14:32.000
 you know, just. Well, that's because you're interested in creating intelligence. It seems

1:14:32.000 --> 1:14:38.880
 like self replication is a good way to make a lot of money. Well, fine. But so is, you know,

1:14:38.880 --> 1:14:44.240
 maybe editing viruses is a good way too. I don't know. The point is, if as a society,

1:14:44.240 --> 1:14:51.200
 when we want to look at existential risks, the existential risks we face that we can control

1:14:51.200 --> 1:14:58.480
 almost all evolve around self replication. Yes. The question is, I don't see a good way to make

1:14:58.480 --> 1:15:02.640
 a lot of money by engineering viruses and deploying them on the world. There could be,

1:15:02.640 --> 1:15:07.200
 there could be applications that are useful. But let's separate out. Let's separate out. I mean,

1:15:07.200 --> 1:15:10.080
 you don't need to. You only need some, you know, terrorists who want to do it because it doesn't

1:15:10.080 --> 1:15:15.920
 take a lot of money to make viruses. Let's just separate out what's risky and what's not risky.

1:15:15.920 --> 1:15:21.280
 I'm arguing that the intelligence side of this equation is not risky. It's not risky at all.

1:15:21.280 --> 1:15:26.320
 It's the self replication side of the equation that's risky. And I'm not dismissing that. I'm

1:15:26.320 --> 1:15:33.520
 scared as hell. It's like the paperclip maximizer thing. Those are often like talked about in the

1:15:33.520 --> 1:15:40.720
 same conversation. I think you're right. Like creating ultra intelligence, super intelligent

1:15:40.720 --> 1:15:46.800
 systems is not necessarily coupled with a self replicating, arbitrarily self replicating

1:15:46.800 --> 1:15:51.840
 systems. Yeah. And you don't get evolution unless you're self replicating. Yeah. And so I think

1:15:51.840 --> 1:15:56.960
 that's just this argument that people have trouble separating those two out. They just think, oh,

1:15:56.960 --> 1:16:00.960
 yeah, intelligence looks like us. And look how, look at the damage we've done to this planet.

1:16:00.960 --> 1:16:04.640
 Like how we've, you know, destroyed all these other species. Yeah. Well, we replicate,

1:16:04.640 --> 1:16:12.240
 which 8 billion of us or 7 billion of us now. I think the idea is that the more intelligent

1:16:12.240 --> 1:16:18.160
 we're able to build systems, the more tempting it becomes from a capitalist perspective of creating

1:16:18.160 --> 1:16:22.400
 products, the more tempting it becomes to create self reproducing systems.

1:16:22.400 --> 1:16:26.880
 All right. So let's say that's true. So does that mean we don't build intelligent systems? No,

1:16:26.880 --> 1:16:34.400
 that means we regulate, we understand the risks, we regulate them. You know, look, there's a lot

1:16:34.400 --> 1:16:38.320
 of things we could do as society, which have some sort of financial benefit to someone,

1:16:38.320 --> 1:16:42.560
 which could do a lot of harm. And we have to learn how to regulate those things.

1:16:42.560 --> 1:16:46.080
 We have to learn how to deal with those things. I will argue this. I would say the opposite.

1:16:46.080 --> 1:16:52.000
 Like I would say having intelligent machines at our disposal will actually help us in the end more

1:16:52.000 --> 1:16:55.440
 because it'll help us understand these risks better, help us mitigate these risks better.

1:16:55.440 --> 1:16:59.040
 There might be ways of saying, oh, well, how do we solve climate change problems? You know,

1:16:59.040 --> 1:17:05.680
 how do we do this or how do we do that? That just like computers are dangerous in the hands of the

1:17:05.680 --> 1:17:09.840
 wrong people, but they've been so great for so many other things, we live with those dangers.

1:17:09.840 --> 1:17:13.520
 And I think we have to do the same with intelligent machines. But we have to be

1:17:13.520 --> 1:17:18.720
 constantly vigilant about this idea of A, bad actors doing bad things with them and B,

1:17:20.080 --> 1:17:25.600
 don't ever, ever create a self replicating system. And by the way, I don't even know

1:17:25.600 --> 1:17:30.320
 if you could create a self replicating system that uses a factory that's really dangerous.

1:17:30.320 --> 1:17:33.520
 You know, nature's way of self replicating is so amazing.

1:17:34.560 --> 1:17:39.360
 You know, it doesn't require anything. It just, you know, the thing and resources and it goes,

1:17:39.360 --> 1:17:45.360
 right? Yeah. If I said to you, you know what, we have to build, our goal is to build a factory

1:17:45.360 --> 1:17:51.600
 that can make, that builds new factories. And it has to end the end supply chain. It has to

1:17:51.600 --> 1:17:59.360
 bind the resources, get the energy. I mean, that's really hard. You know, no one's doing that in the

1:17:59.360 --> 1:18:05.760
 next, you know, 100 years. I've been extremely impressed by the efforts of Elon Musk and Tesla

1:18:05.760 --> 1:18:12.160
 to try to do exactly that, not from raw resource. Well, he actually, I think states the goal is

1:18:12.160 --> 1:18:19.440
 to go from raw resource to the final car in one factory. That's the main goal. Of course,

1:18:19.440 --> 1:18:24.160
 it's not currently possible, but they're taking huge leaps. Well, he's not the only one to do that.

1:18:24.160 --> 1:18:27.120
 This has been a goal for many industries for a long, long time.

1:18:27.920 --> 1:18:32.160
 It's difficult to do. Well, a lot of people, what they do is instead they have like

1:18:32.160 --> 1:18:36.240
 a million suppliers and then they, like there's everybody's management.

1:18:36.240 --> 1:18:40.560
 They all co locate them and they kind of systems together.

1:18:40.560 --> 1:18:45.040
 It's a fundamental distributed system. I think that's, that also is not getting at the issue

1:18:45.040 --> 1:18:51.760
 I was just talking about, which is self replication. I mean, self replication means

1:18:51.760 --> 1:18:56.400
 there's no entity involved other than the entity that's replicating.

1:18:57.680 --> 1:19:02.160
 Right. And so if there are humans in this, in the loop, that's not really self replicating, right?

1:19:02.160 --> 1:19:09.520
 It's, unless somehow we're duped into it. But it's also don't necessarily

1:19:09.520 --> 1:19:15.600
 agree with you because you've kind of mentioned that AI will not say no to us.

1:19:16.400 --> 1:19:23.280
 I just think they will. Yeah. Yeah. So like, I think it's a useful feature to build in.

1:19:23.280 --> 1:19:28.960
 I'm just trying to like put myself in the mind of engineers to sometimes say no,

1:19:29.760 --> 1:19:35.760
 you know, if you, I get an example earlier, right? I get an example of my car, right?

1:19:35.760 --> 1:19:41.200
 My car turns the wheel and applies the accelerator and the brake as I say,

1:19:41.200 --> 1:19:45.360
 until it decides there's something dangerous. Yes. And then it doesn't do that.

1:19:45.360 --> 1:19:51.760
 Yeah. Now, that was something it didn't decide to do. It's something we programmed into the car.

1:19:52.880 --> 1:19:57.600
 And so good. It's a good idea, right? The question again, isn't like,

1:19:57.600 --> 1:20:03.200
 if we create an intelligent system or ever ignore our commands, of course we will sometimes.

1:20:03.200 --> 1:20:08.960
 Is it going to do it because it came up with its own goals that serve its purposes and it

1:20:08.960 --> 1:20:11.680
 doesn't care about our purposes? No, I don't think that's going to happen.

1:20:12.480 --> 1:20:16.960
 Okay. So let me ask you about these super intelligent cortical systems that we engineer

1:20:16.960 --> 1:20:24.160
 and us humans. Do you think with these entities operating out there in the world,

1:20:24.160 --> 1:20:29.680
 what is the future, most promising future look like? Is it us merging with them?

1:20:29.680 --> 1:20:37.440
 Or is it us? Like, how do we keep us humans around when you have increasingly intelligent

1:20:37.440 --> 1:20:43.040
 beings? Is it one of the dreams is to upload our minds in the digital space? So can we just

1:20:44.320 --> 1:20:49.920
 give our minds to these systems so they can operate on them? Is there some kind of more

1:20:49.920 --> 1:20:54.400
 interesting merger or is there more? In the third part of my book, I talked about all these scenarios

1:20:54.400 --> 1:21:00.160
 and let me just walk through them. Sure. The uploading the mind one. Yes.

1:21:00.880 --> 1:21:06.560
 Extremely, really difficult to do. Like, we have no idea how to do this even remotely right now.

1:21:08.080 --> 1:21:12.720
 So it would be a very long way away, but I make the argument you wouldn't like the result.

1:21:14.320 --> 1:21:17.760
 And you wouldn't be pleased with the result. It's really not what you think it's going to be.

1:21:18.480 --> 1:21:22.000
 Imagine I could upload your brain into a computer right now and now the computer's

1:21:22.000 --> 1:21:25.840
 sitting there going, Hey, I'm over here. Great. Get rid of that old bio person. I don't need

1:21:25.840 --> 1:21:30.560
 them. You're still sitting here. Yeah. What are you going to do? No, no, that's not me. I'm here.

1:21:30.560 --> 1:21:34.880
 Right. Yeah. Are you going to feel satisfied? Then you, but people imagine, look, I'm on my

1:21:34.880 --> 1:21:39.840
 deathbed and I'm about to, you know, expire and I pushed the button and now I'm uploaded. But

1:21:39.840 --> 1:21:44.320
 think about it a little differently. And so I don't think it's going to be a thing because

1:21:44.320 --> 1:21:49.760
 people by the time we're able to do this, if ever, because you have to replicate the entire body,

1:21:49.760 --> 1:21:54.320
 not just the brain, it's, it's really, it's, I walk through the issues. It's really substantial.

1:21:56.000 --> 1:22:01.920
 Do you have a sense of what makes us us? Is there, is there a shortcut to it can only save a certain

1:22:01.920 --> 1:22:06.720
 part that makes us truly arts? No, but I think that machine would feel like it's you too.

1:22:07.280 --> 1:22:11.600
 Right. Right. If you people just like, I have a child, right? I have two daughters.

1:22:12.400 --> 1:22:15.600
 They're independent people. I created them. Well, partly. Yeah. And

1:22:15.600 --> 1:22:22.720
 I don't, just because they're somewhat like me, I don't feel on them and they don't feel

1:22:22.720 --> 1:22:26.160
 like I'm me. So if you split the part, you have two people. So we can tell them to come back to

1:22:26.160 --> 1:22:30.800
 what makes, what consciousness we want. We can talk about that, but we don't have a remote

1:22:30.800 --> 1:22:35.280
 consciousness. I'm not sitting there going, oh, I'm conscious of that. I'm in that system over there.

1:22:35.280 --> 1:22:40.080
 So let's, let's stay on our topic. So one was uploading a brain. Yeah.

1:22:40.800 --> 1:22:44.960
 It ain't going to happen in a hundred years, maybe a thousand, but I don't think people are going to

1:22:44.960 --> 1:22:50.800
 want to do it. The merging your mind with, uh, you know, the neural link thing, right? Like,

1:22:51.840 --> 1:22:56.720
 again, really, really difficult. It's, it's one thing to make progress to control a prosthetic

1:22:56.720 --> 1:23:01.440
 arm. It's another to have like a billion or several billion, you know, things and understanding what

1:23:01.440 --> 1:23:06.160
 those signals mean. Like it's the one thing to like, okay, I can learn to think some patterns

1:23:06.160 --> 1:23:10.960
 to make something happen. It's quite another thing to have a system, a computer, which actually

1:23:10.960 --> 1:23:14.720
 knows exactly what cells it's talking to and how it's talking to them and interacting in a way

1:23:14.720 --> 1:23:18.400
 like that. Very, very difficult. We're not getting anywhere closer to that.

1:23:19.440 --> 1:23:26.960
 Interesting. Can I, can I ask a question here? So for me, what makes that merger very difficult

1:23:26.960 --> 1:23:33.440
 practically in the next 10, 20, 50 years is like literally the biology side of it, which is like,

1:23:34.080 --> 1:23:39.840
 it's just hard to do that kind of surgery in a safe way. But your intuition is even the machine

1:23:39.840 --> 1:23:45.440
 learning part of it, where the machine has to learn what the heck it's talking to. That's even

1:23:45.440 --> 1:23:50.080
 hard. I think it's even harder. And it's not, it's, it's easy to do when you're talking about

1:23:50.080 --> 1:23:55.600
 hundreds of signals. It's, it's a totally different thing to say talking about billions of signals.

1:23:55.600 --> 1:23:59.680
 So you don't think it's the raw, it's a machine learning problem. You don't think it could be

1:23:59.680 --> 1:24:04.080
 learned? Well, I'm just saying, no, I think you'd have to have detailed knowledge. You'd have to

1:24:04.080 --> 1:24:08.320
 know exactly what the types of neurons you're connecting to. I mean, in the brain, there's these

1:24:08.320 --> 1:24:11.360
 neurons that do all different types of things. It's not like a neural network. It's a very

1:24:11.360 --> 1:24:15.600
 complex organism system up here. We talked about the grid cells or the place cells, you know,

1:24:15.600 --> 1:24:18.320
 you have to know what kind of cells you're talking to and what they're doing and how their

1:24:18.320 --> 1:24:23.360
 timing works and all, all this stuff, which you can't today is no way of doing that, right?

1:24:23.360 --> 1:24:27.680
 But I think it's, I think it's a, I think the problem, you're right that the biological aspect

1:24:27.680 --> 1:24:31.760
 of like who wants to have a surgery and have this stuff inserted in your brain, that's a problem.

1:24:31.760 --> 1:24:36.880
 But this is when we solve that problem. I think the, the information coding aspect is much worse.

1:24:36.880 --> 1:24:41.040
 I think that's much worse. It's not like what they're doing today. Today, it's simple machine

1:24:41.040 --> 1:24:45.360
 learning stuff because you're doing simple things. But if you want to merge your brain,

1:24:45.360 --> 1:24:49.920
 like I'm thinking on the internet, I'm merged my brain with the machine and we're both doing,

1:24:49.920 --> 1:24:54.320
 that's a totally different issue. That's interesting. I tend to think if the, okay,

1:24:54.320 --> 1:25:01.040
 yeah, if you have a super clean signal from a bunch of neurons at the start, you don't know

1:25:01.040 --> 1:25:07.600
 what those neurons are. I think that's much easier than the getting of the clean signal.

1:25:07.600 --> 1:25:12.240
 I think if you think about today's machine learning, that's what you would conclude.

1:25:13.040 --> 1:25:16.640
 I'm thinking about what's going on in the brain and I don't reach that conclusion. So we'll have

1:25:16.640 --> 1:25:21.920
 to see. Sure. But I don't think even, even then, I think there's kind of a sad future.

1:25:22.560 --> 1:25:27.840
 Like, you know, do I, do I have to like plug my brain into a computer? I'm still a biologic

1:25:27.840 --> 1:25:32.480
 organism. I assume I'm still going to die. So what, what have I achieved? Right? You know,

1:25:32.480 --> 1:25:37.680
 what have I achieved to doing some sort of? Oh, I disagree. We don't know what those are, but it

1:25:37.680 --> 1:25:42.480
 seems like there could be a lot of different applications. It's like virtual reality is

1:25:42.480 --> 1:25:49.120
 to expand your brain's capability to like read Wikipedia. Yeah, but fine. But you're still

1:25:49.120 --> 1:25:53.280
 a biologic organism. Yes. Yes. You're still, you're still mortal. You're still all right. So,

1:25:53.280 --> 1:25:57.200
 so what are you accomplishing? You're making your life in this short period of time better,

1:25:57.200 --> 1:26:03.280
 right? Just like having the internet made our life better. Yeah. Yeah. Okay. So I think that's

1:26:03.280 --> 1:26:08.320
 of, of, if I think about all the possible gains we can have here, that's a marginal one. It's

1:26:08.320 --> 1:26:15.280
 an individual, hey, I'm better, you know, I'm smarter. But you'll find I'm not against it.

1:26:15.280 --> 1:26:20.240
 I just don't think it's earth changing. I, but, but so this is the true of the internet.

1:26:20.240 --> 1:26:24.800
 When each of us individuals are smarter, we get a chance to then share our smartness.

1:26:24.800 --> 1:26:28.480
 We get smarter and smarter together as like, as a collective. This is kind of like the

1:26:28.480 --> 1:26:32.480
 same colony. Why don't I just create an intelligent machine that doesn't have any of this biological

1:26:32.480 --> 1:26:37.920
 nonsense? This is all the same. It's, it's everything except don't burden it with my brain.

1:26:38.720 --> 1:26:43.120
 Yeah. Right. It has a brain. It is smart. It's like my child, but it's much, much smarter than

1:26:43.120 --> 1:26:47.760
 me. So I have a choice between doing some implant, doing some hybrid weird, you know,

1:26:47.760 --> 1:26:53.360
 biological thing that's bleeding and all these problems and limited by my brain or creating

1:26:53.360 --> 1:26:58.320
 a system which is super smart that I can talk to that helps me understand the world that can read

1:26:58.320 --> 1:27:03.760
 the internet, you know, read Wikipedia and talk to me. I guess my, the open questions there are

1:27:04.640 --> 1:27:10.640
 what does the manifestation of superintelligence look like? So like, what are we going to,

1:27:10.640 --> 1:27:15.840
 you talked about, why do I want to merge with AI? Like, what, what's the actual marginal benefit

1:27:15.840 --> 1:27:24.560
 here? If I, if we have a super intelligent system, yeah, how will it make our life better?

1:27:24.560 --> 1:27:28.720
 So let's, let's, that's a great question, but let's break it onto little pieces. All right.

1:27:28.720 --> 1:27:33.760
 On the one hand, it can make our life better in lots of simple ways. You mentioned like a care robot

1:27:33.760 --> 1:27:37.360
 or something that helps me do things, a cook side, I don't know what it does, right? Little things

1:27:37.360 --> 1:27:43.520
 like that. We have super better, smarter cars. We can have, you know, better agents, aids helping

1:27:43.520 --> 1:27:47.840
 us in our work environment and things like that. To me, that's like the easy stuff, the simple stuff

1:27:47.840 --> 1:27:54.480
 in the beginning. And so in the same way that computers made our lives better in ways, many,

1:27:54.480 --> 1:28:01.120
 many ways, I will have those kind of things. To me, the really exciting thing about AI is

1:28:01.120 --> 1:28:06.400
 sort of its transcendent, transcendent quality in terms of humanity. We're still biological

1:28:06.400 --> 1:28:10.400
 organisms. We're still stuck here on earth. It's going to be hard for us to live anywhere else.

1:28:10.400 --> 1:28:18.160
 I don't think you and I are going to want to live on Mars anytime soon. And, and we're flawed,

1:28:18.720 --> 1:28:24.960
 you know, we may end up destroying ourselves. It's totally possible. We, if not completely,

1:28:24.960 --> 1:28:29.360
 we could destroy our civilizations. You know, it's just face the fact that we have issues here,

1:28:30.080 --> 1:28:34.000
 but we can create intelligent machines that can help us in various ways. For example,

1:28:34.000 --> 1:28:38.480
 one example I gave, and that sounds a little sci fi, but I believe this, if we really want to

1:28:38.480 --> 1:28:43.760
 live on Mars, we'd have to have intelligent systems that go there and build the habitat for us,

1:28:43.760 --> 1:28:49.200
 not humans. Humans are never going to do this. It's just too hard. But could we have a thousand or

1:28:49.200 --> 1:28:54.000
 10,000, you know, engineer workers up there doing this stuff, building things, terraforming Mars?

1:28:54.000 --> 1:28:58.640
 Sure. Maybe we can move Mars. But then if we want to, if we want to go around the universe,

1:28:58.640 --> 1:29:02.400
 should I send my children around the universe? Or should I send some intelligent machine,

1:29:02.400 --> 1:29:07.520
 which is like a child that represents me and understands our needs here on earth that could

1:29:07.520 --> 1:29:13.920
 travel through space? So it sort of, in some sense, intelligence allows us to transcend the

1:29:13.920 --> 1:29:20.800
 limitations of our biology. And don't think of it as a negative thing. It's in some sense,

1:29:20.800 --> 1:29:28.720
 my children transcend my biology too, because they live beyond me. And they represent me,

1:29:28.720 --> 1:29:31.840
 and they also have their own knowledge, and I can impart knowledge to them. So intelligent

1:29:31.840 --> 1:29:37.840
 machines would be like that too, but not limited like us. But the question is, there's so many

1:29:37.840 --> 1:29:44.320
 ways that transcendence can happen. And the merger with AI and humans is one of those ways. So you

1:29:44.320 --> 1:29:51.360
 said intelligent, basically beings or systems propagating throughout the universe representing

1:29:51.360 --> 1:29:56.560
 us humans. They represent us humans in the sense they represent our knowledge and our history,

1:29:56.560 --> 1:30:04.720
 not us individually. Right, right. But I mean, the question is, is it just a database

1:30:05.600 --> 1:30:11.200
 with the really damn good model of the world? No, no, they're conscious, just like us.

1:30:11.760 --> 1:30:16.720
 Okay. But just different. They're different. Just like my children are different. They're like me,

1:30:16.720 --> 1:30:23.440
 but they're different. These are more different. I guess maybe I've already, I kind of, I take

1:30:23.440 --> 1:30:29.120
 a very broad view of our life here on Earth. I say, you know, why are we living here? Are we

1:30:29.120 --> 1:30:33.920
 just living because we live? Are we surviving because we can survive? Are we fighting just

1:30:33.920 --> 1:30:39.680
 because we want to just keep going? What's the point of it? Right? So to me, the point,

1:30:39.680 --> 1:30:46.800
 if I ask myself, what's the point of life is, what transcends that ephemeral sort of biological

1:30:46.800 --> 1:30:54.080
 experience is to me, this is my answer, is the acquisition of knowledge to understand more

1:30:54.080 --> 1:31:01.920
 about the universe and to explore. And that's partly to learn more, right? I don't view it as

1:31:02.720 --> 1:31:09.920
 a terrible thing if the ultimate outcome of humanity is we create systems that are intelligent,

1:31:09.920 --> 1:31:14.800
 that are offspring, but they're not like us at all. And we stay here and live on Earth as long

1:31:14.800 --> 1:31:21.920
 as we can, which won't be forever, but as long as we can. And, but that would be a great thing

1:31:21.920 --> 1:31:31.120
 to do. It's not, it's not like a negative thing. Well, would you be okay then if the human

1:31:31.840 --> 1:31:38.560
 species vanishes, but our knowledge is preserved and keeps being expanded by intelligent systems?

1:31:38.560 --> 1:31:45.600
 I want our knowledge to be preserved and expanded. Yeah. Am I okay with humans dying? No, I don't

1:31:45.600 --> 1:31:51.040
 want that to happen. But if it does happen, what if we were sitting here and this is the

1:31:51.040 --> 1:31:56.000
 last two people on Earth who were saying, Lex, we blew it, it's all over, right? Wouldn't I feel

1:31:56.000 --> 1:32:01.520
 better if I knew that our knowledge was preserved and that we had agents that knew about that,

1:32:01.520 --> 1:32:06.640
 that were trans, you know, that left Earth? I would want that. It's better than not having that.

1:32:06.640 --> 1:32:10.080
 You know, I make the analogy of like, you know, the dinosaurs, the poor dinosaurs, they live for,

1:32:10.080 --> 1:32:13.520
 you know, tens of millions of years. They raised their kids. They, you know, they,

1:32:13.520 --> 1:32:18.560
 they fought to survive. They were hungry. They, they did everything we do. And then they're all

1:32:18.560 --> 1:32:24.960
 gone. Yeah. Like, you know, and, and if we didn't discover their bones, nobody would ever know that

1:32:24.960 --> 1:32:30.000
 they ever existed, right? Do we want to be like that? I don't want to be like that. There's a sad

1:32:30.000 --> 1:32:37.680
 aspect to it. And it kind of is jarring to think about that it's possible that a human like intelligent

1:32:37.680 --> 1:32:44.640
 civilization has previously existed on Earth. The reason I say this is like, it is jarring to think

1:32:44.640 --> 1:32:49.040
 that we would not, if they weren't extinct, we wouldn't be able to find evidence of them.

1:32:49.040 --> 1:32:54.000
 After a sufficient amount of time. After a sufficient amount of time. Of course, there's like,

1:32:54.000 --> 1:32:59.920
 like basically humans, like if we destroy ourselves now, human civilization destroy ourselves now,

1:32:59.920 --> 1:33:04.480
 after a sufficient amount of time, we would not be, we'd find the evidence of the dinosaurs.

1:33:04.480 --> 1:33:09.040
 We would not find evidence of us humans. Yeah. That's kind of an odd thing to think about. Although

1:33:09.840 --> 1:33:14.880
 I'm not sure if we have enough knowledge about species going back through billions of years,

1:33:14.880 --> 1:33:18.960
 but we could, we could, we might be able to eliminate that possibility. But it's an interesting

1:33:18.960 --> 1:33:23.200
 question. Of course, this is a similar question to, you know, there were lots of intelligent

1:33:23.200 --> 1:33:31.280
 species about our galaxy that have all disappeared. Yeah. That's super sad that they're exactly that

1:33:31.280 --> 1:33:36.800
 there may have been much more intelligent alien civilizations in our galaxy. There are no longer

1:33:36.800 --> 1:33:44.720
 there. Yeah. You actually talked about this, that humans might destroy ourselves. Yeah. And how we

1:33:44.720 --> 1:33:54.480
 might preserve our knowledge and advertise that knowledge to other. Advertise is a funny word

1:33:54.480 --> 1:33:58.080
 to use. From a PR perspective. There's no financial gain in this.

1:34:00.720 --> 1:34:04.560
 You know, like make it like from a tourism perspective, make it interesting. Can you

1:34:04.560 --> 1:34:09.280
 describe how? Well, there's a couple things. I broke it down into two parts, actually three

1:34:09.280 --> 1:34:17.040
 parts. One is, you know, there's a lot of things we know that what if we were, what if we ended,

1:34:17.040 --> 1:34:21.280
 what if our civilization collapsed? Yeah, I'm not talking tomorrow. Yeah, we could be a thousand

1:34:21.280 --> 1:34:25.440
 years from that. Like, you know, we don't really know. But historically, it would be likely at

1:34:25.440 --> 1:34:33.040
 some point. Time flies when you're having fun. Yeah. You know, could we, and then intelligent

1:34:33.040 --> 1:34:37.440
 life evolved again on this planet? Wouldn't they want to know a lot about us and what we knew?

1:34:37.440 --> 1:34:41.120
 Wouldn't they wouldn't be able to ask us questions? So one very simple thing I said,

1:34:41.120 --> 1:34:44.960
 how would we archive what we know? That was a very simple idea. I said, you know what,

1:34:44.960 --> 1:34:48.960
 it wouldn't be that hard to put a few satellites, you know, going around the sun and we upload

1:34:48.960 --> 1:34:53.920
 Wikipedia every day and that kind of thing. So, you know, if we end up killing ourselves,

1:34:53.920 --> 1:34:57.280
 well, it's up there and the next intelligence piece will find it and learn something. They

1:34:57.280 --> 1:35:03.040
 would like that. They would appreciate that. So that's one thing. The next thing I said, well,

1:35:03.040 --> 1:35:08.720
 what if, you know, how we're outside of our solar system? We have the SETI program. We're

1:35:08.720 --> 1:35:12.640
 looking for these intelligent signals from everybody. And if you do a little bit of math,

1:35:12.640 --> 1:35:17.680
 which I did in the book, and you say, well, what if intelligent species only live for 10,000 years

1:35:18.240 --> 1:35:21.840
 before, you know, technologically intelligent species? Like, ones are really able to do the

1:35:21.840 --> 1:35:26.320
 task we're just starting to be able to do. Well, the chances are we wouldn't be able to see any of

1:35:26.320 --> 1:35:31.040
 them because they would have all been disappeared by now. They've lived for 10,000 years and now

1:35:31.040 --> 1:35:35.440
 they're gone. And so we're not going to find these signals being sent from these people because

1:35:36.080 --> 1:35:40.560
 if I say, what kind of signal could you create that would last a million years or a billion years?

1:35:41.120 --> 1:35:46.400
 That someone would say, damn it, someone smart lived there. We know that. That would be a life

1:35:46.400 --> 1:35:50.080
 changing event for us to figure that out. Well, what we're looking for today in the SETI program

1:35:50.080 --> 1:35:54.800
 isn't that. We're looking for very coded signals in some sense. And so I asked myself, what would

1:35:54.800 --> 1:35:58.960
 be a different type of signal one could create? I've always thought about this throughout my life

1:35:58.960 --> 1:36:06.400
 and in the book I gave one possible suggestion, which was we now detect planets going around

1:36:06.400 --> 1:36:13.600
 other suns, other stars, excuse me. And we do that by seeing this slight dimming of the light as

1:36:13.600 --> 1:36:18.880
 the planets move in front of them. That's how we detect planets elsewhere in our galaxy.

1:36:20.000 --> 1:36:26.480
 What if we created something like that that just rotated around the sun and it blocked out a little

1:36:26.480 --> 1:36:31.760
 bit of light in a particular pattern that someone said, hey, that's not a planet. That is a sign

1:36:31.760 --> 1:36:36.480
 that someone was once there. You can say, what if it's beating up pi, three point, whatever.

1:36:37.840 --> 1:36:45.360
 So I did it from a distance, broadly broadcast, takes no continue activation on our part. This

1:36:45.360 --> 1:36:49.040
 is the key, right? No one has to be seeing a running computer and supplying it with power.

1:36:49.040 --> 1:36:55.200
 It just goes on. So we go, it's continuous. And I argued that part of the SETI program

1:36:55.200 --> 1:36:58.960
 should be looking for signals like that. And to look for signals like that, you ought to figure

1:36:58.960 --> 1:37:03.840
 out how would we create a signal? Like, what would we create that would be like that, that would

1:37:03.840 --> 1:37:07.760
 persist for millions of years, that would be broadcast broadly that you could see from a

1:37:07.760 --> 1:37:13.760
 distance that was unequivocal, came from an intelligent species. And so I gave that one

1:37:13.760 --> 1:37:18.560
 example because they don't know what to know of actually. And then finally, right,

1:37:18.560 --> 1:37:26.640
 if, if our, ultimately our solar system will die at some point in time, you know, how do we go

1:37:26.640 --> 1:37:31.600
 beyond that? And I think it's possible, if at all possible, we'll have to create intelligent machines

1:37:31.600 --> 1:37:36.880
 that travel throughout the solar system or throughout the galaxy. And I don't think that's

1:37:36.880 --> 1:37:40.720
 going to be humans. I don't think it's going to be biological organisms. So these are just

1:37:40.720 --> 1:37:44.560
 things to think about, you know, like, what's the, you know, I don't want to be like the dinosaur.

1:37:44.560 --> 1:37:48.480
 I don't want to just live in, okay, that was it. We're done, you know. Well, there is a kind of

1:37:48.480 --> 1:37:55.600
 presumption that we're going to live forever, which I think it is a bit sad to imagine that the

1:37:55.600 --> 1:38:04.000
 message we send as, as he talked about is that we were once here instead of we are here. Well,

1:38:04.000 --> 1:38:10.080
 it could be we are still here. But it's more of a, it's more of an insurance policy in case we're

1:38:10.080 --> 1:38:17.280
 not here, you know? Well, I don't know, but there's something I think about, we humans don't often

1:38:17.280 --> 1:38:26.080
 think about this, but it's like, like, whenever I record a video, I've done this a couple of times

1:38:26.080 --> 1:38:30.640
 in my life, I've recorded a video for my future self, just for personal, just for fun. And it's

1:38:30.640 --> 1:38:40.160
 always just fascinating to think about that, preserving yourself for future civilizations. For

1:38:40.160 --> 1:38:46.320
 me, it was preserving myself for a future me, but that's a little, that's a little fun example

1:38:46.320 --> 1:38:53.600
 of archival. Well, these podcasts are preserving you and I in a way, for future, hopefully well

1:38:53.600 --> 1:39:00.880
 after we're gone. But you don't often, we're sitting here talking about this. You are not

1:39:00.880 --> 1:39:07.600
 thinking about the fact that you and I are going to die, and there'll be like 10 years after somebody

1:39:07.600 --> 1:39:13.520
 watching this, and we're still alive. You know, in some sense, I do. I'm here because I want to

1:39:13.520 --> 1:39:21.520
 talk about ideas. And these ideas transcend me, and they transcend this time and on our planet.

1:39:21.520 --> 1:39:27.440
 We're talking here about ideas that could be around a thousand years from now or a million years

1:39:27.440 --> 1:39:34.000
 from now. When I wrote my book, I had an audience in mind, and one of the clearest audiences was

1:39:34.000 --> 1:39:39.760
 aliens. No, were people reading this a hundred years from now? Yes. I said to myself, how do I

1:39:39.760 --> 1:39:44.000
 make this book relevant to somebody reading this a hundred years from now? What would they want to

1:39:44.000 --> 1:39:48.880
 know that we were thinking back then? What would make it like that was an interesting, it's still

1:39:48.880 --> 1:39:54.080
 an interesting book. I'm not sure I can achieve that, but that was how I thought about it because

1:39:54.080 --> 1:39:57.600
 these ideas, especially in the third part of the book, the ones we were just talking about,

1:39:58.160 --> 1:40:01.920
 you know, these crazy, it sounds like crazy ideas about, you know, storing our knowledge and,

1:40:01.920 --> 1:40:06.640
 and, you know, merging our brains or computers and sending, you know, our machines out into space,

1:40:06.640 --> 1:40:11.440
 is not going to happen in my lifetime. And they may not, and they may not happen in the next

1:40:11.440 --> 1:40:17.280
 hundred years. They may not happen for a thousand years. Who knows? But we have the unique opportunity

1:40:17.280 --> 1:40:24.160
 right now, we, you, me, and other people like this, to sort of at least propose the agenda

1:40:25.200 --> 1:40:30.400
 that might impact the future like that. It's a fascinating way to think, both like writing or

1:40:30.400 --> 1:40:39.680
 creating, try to make, try to create ideas, try to create things that hold up in time. Yeah. You

1:40:39.680 --> 1:40:43.200
 know, understanding how the brain works, we're going to figure that out once. That's it. It's

1:40:43.200 --> 1:40:48.000
 going to be figured out once. And after that, that's the answer. And people will, people will study

1:40:48.000 --> 1:40:53.920
 that thousands of years now. We still, we still, you know, venerate Newton and, and Einstein and,

1:40:54.960 --> 1:41:02.080
 and, you know, because, because ideas are exciting even well into the future. Well, the interesting

1:41:02.080 --> 1:41:10.560
 thing is like big ideas, even if they're wrong, are still useful. Like, yeah, especially if they're

1:41:10.560 --> 1:41:14.480
 not completely wrong. Like you're right, right, right. Right. Noons laws are not wrong. They're

1:41:14.480 --> 1:41:20.960
 just Einstein's are better. So, um, let's see. Yeah. I mean, but we're talking with Newton and

1:41:20.960 --> 1:41:25.760
 Einstein. We're talking about physics. I wonder if we'll ever achieve that kind of clarity about

1:41:25.760 --> 1:41:32.560
 understanding, um, like complex systems and the, this particular manifestation of complex systems,

1:41:32.560 --> 1:41:37.200
 which is the human brain. I'm, I'm totally optimistic we can do that. I mean, we're making

1:41:37.200 --> 1:41:42.400
 progress at it. I don't see any reason why we can't completely, I mean, completely understand in the

1:41:42.400 --> 1:41:47.120
 sense, um, you know, we don't really completely understand what all the molecules in this water

1:41:47.120 --> 1:41:52.400
 bottle are doing, but, you know, we have laws that sort of capture it pretty good. Um, and, uh,

1:41:52.400 --> 1:41:55.680
 so we'll have that kind of understanding. I mean, it's not like you're going to have to know what

1:41:55.680 --> 1:42:02.880
 every neuron in your brain is doing. Um, but enough to, um, first of all, to build it and second of

1:42:02.880 --> 1:42:08.480
 all, to do, you know, do what physics does, which is like have concrete experiments where we can

1:42:08.480 --> 1:42:13.520
 validate. We're, we're, we're, this is happening right now. Like it's not, this is not some future

1:42:13.520 --> 1:42:18.640
 thing. Um, you know, I'm very optimistic about, I'm, I know about our, our work and what we're

1:42:18.640 --> 1:42:28.320
 doing. We'll have to prove it to people. Um, but, um, I consider myself a rational person and, um,

1:42:28.320 --> 1:42:32.080
 you know, until fairly recently, I wouldn't have said that, but right now I'm, where I'm sitting

1:42:32.080 --> 1:42:36.320
 right now, I'm saying, you know, we, this is going to happen. There's, there's no big obstacles to

1:42:36.320 --> 1:42:42.560
 it. Um, we finally have a framework for understanding what's going on in the cortex and, um, and

1:42:42.560 --> 1:42:48.320
 that's liberating. It's, it's like, oh, it's happening. So I can't see why we wouldn't be able

1:42:48.320 --> 1:42:53.360
 to understand it. I just can't. Okay. Oh, so, I mean, on that topic, let me ask you to play devil's

1:42:53.360 --> 1:43:02.400
 advocate. Is it possible for you to imagine luck, look a hundred years from now and looking at your

1:43:02.400 --> 1:43:10.320
 book, uh, in which ways might your ideas be wrong? Oh, I worry about this all the time. Um,

1:43:12.320 --> 1:43:16.320
 yeah, it's still useful. Yeah. Yeah.

1:43:16.320 --> 1:43:24.800
 Um, I think there's, you know, um, well, I can, I can best relate it to like things I'm worried

1:43:24.800 --> 1:43:29.600
 about right now. So we talk about this voting idea, right? It's happening. There's, there's no

1:43:29.600 --> 1:43:35.680
 question it's happening, but it could be far more, um, uh, there's, there's enough things I

1:43:35.680 --> 1:43:40.480
 don't know about it that it might be working in different ways differently than I'm thinking about

1:43:40.480 --> 1:43:44.560
 the kind of what's voting, who's voting, you know, where are representations. I talked about,

1:43:44.560 --> 1:43:49.840
 like you have a thousand models of a coffee cup like that. That could turn out to be wrong, um,

1:43:49.840 --> 1:43:54.800
 because it may be, maybe there are a thousand models that are sub models, but not really a

1:43:54.800 --> 1:43:59.600
 single model of the coffee cup. Um, I mean, there's things that these are all sort of on the edges,

1:44:00.480 --> 1:44:04.480
 things that I present as like, oh, it's so simple and clean. Well, that's not that. It's always going

1:44:04.480 --> 1:44:11.600
 to be more complex. And, um, and there's parts of the theory, which I don't understand the

1:44:11.600 --> 1:44:17.680
 complexity well. So I think, I think the idea that the brain is a distributed modeling system is

1:44:17.680 --> 1:44:22.400
 not controversial at all, right? That's not, that's well understood by many people. The question then

1:44:22.400 --> 1:44:28.800
 is, are each quarterly column an independent modeling system? Right. Um, I could be wrong about

1:44:28.800 --> 1:44:35.840
 that. Um, I don't think so, but I worry about it. My intuition, not even thinking why you could be

1:44:35.840 --> 1:44:43.280
 wrong is the same intuition I have about any sort of physicist, uh, like strength theory, that we,

1:44:43.280 --> 1:44:51.200
 as humans, desire for a clean explanation. And, uh, a hundred years from now, uh, intelligent systems

1:44:51.200 --> 1:44:57.760
 might look back at us and laugh at how we try to get rid of the whole mess by having simple

1:44:57.760 --> 1:45:04.320
 explanation. When the reality is, it's, it's way messier. And in fact, it's impossible to understand

1:45:04.320 --> 1:45:09.040
 you can only build it. It's like this idea of complex systems and cellular automata,

1:45:09.040 --> 1:45:13.840
 you can only launch the thing, you cannot understand it. Yeah. I think that, you know,

1:45:13.840 --> 1:45:19.760
 the history of science suggests that's not likely to occur. Um, the history of science suggests that

1:45:19.760 --> 1:45:25.920
 like as a theorist and we're theorists, you look for simple explanations, right? Fully knowing

1:45:25.920 --> 1:45:30.720
 that whatever simple explanation you're going to come up with is not going to be completely correct.

1:45:30.720 --> 1:45:35.840
 I mean, it can't be. I mean, it's just, it's just more complexity. But that's the role of theorists

1:45:35.840 --> 1:45:41.600
 play. They, they sort of, they give you a framework on which you now can talk about a problem and

1:45:41.600 --> 1:45:46.560
 figure out, okay, now we can start digging more details. The best frameworks stick around while

1:45:46.560 --> 1:45:53.360
 the details change. You know, again, you know, the classic example is Newton and Einstein, right?

1:45:53.360 --> 1:45:59.920
 You know, um, Newton's theories are still used. They're still valuable. They're still practical.

1:45:59.920 --> 1:46:05.120
 They're not like wrong. Just they've been refined. Yeah. But that's in physics. It's not obvious,

1:46:05.120 --> 1:46:10.400
 by the way, it's not obvious for physics either that the universe should be such that's amenable

1:46:10.400 --> 1:46:18.080
 to these simple, but it's so far it appears to be as far as we can tell. Um, yeah. I mean, but

1:46:18.640 --> 1:46:23.040
 as far as we could tell, and, but it's also an open question whether the brain is amenable to

1:46:23.040 --> 1:46:28.960
 such clean theories. That's the brain, but intelligence. Well, I, I, I don't know. I would

1:46:28.960 --> 1:46:37.280
 take intelligence out of it. Just say, you know, um, well, okay. Um, the evidence we have suggested

1:46:37.280 --> 1:46:42.800
 that the human brain is, is, is a, at the one time extremely messy and complex, but there's

1:46:42.800 --> 1:46:47.200
 some parts that are very regular and structured. That's why we started the neocortex. It's

1:46:47.200 --> 1:46:52.640
 extremely regular in its structure. Yeah. And unbelievably so. And then I mentioned earlier,

1:46:52.640 --> 1:46:59.760
 the other thing is it's, it's universal abilities. It is so flexible to learn so many things. We

1:46:59.760 --> 1:47:03.120
 don't, we haven't figured out what it can't learn yet. We don't know, but we haven't figured out

1:47:03.120 --> 1:47:08.960
 yet, but he learns things that never was evolved to learn. So those give us hope. Um, that's why

1:47:08.960 --> 1:47:14.880
 I went into this field because I said, you know, this regular structure, it's doing this amazing

1:47:14.880 --> 1:47:18.240
 number of things. There's got to be some underlying principles that are, that are common

1:47:18.240 --> 1:47:24.400
 and other, other scientists have come up with the same conclusions. Um, and so it's promising.

1:47:24.400 --> 1:47:30.720
 It's promising. And, um, and that's, and whether the theories play out exactly this way or not,

1:47:31.440 --> 1:47:36.480
 that is the role that theorists play. And so far it's worked out well, even though, you know,

1:47:36.480 --> 1:47:41.040
 maybe, you know, we don't understand all the laws of physics, but so far it's been pretty damn

1:47:41.040 --> 1:47:48.160
 useful. The ones we have are, our theories are pretty useful. You mentioned that, uh, we should

1:47:48.160 --> 1:47:53.360
 not necessarily be at least to the degree that we are worried about the existential risks of

1:47:53.360 --> 1:48:02.080
 artificial intelligence relative to, uh, human risks from human nature being existential risk.

1:48:02.720 --> 1:48:07.600
 What aspect of human nature worries you the most in terms of the survival of the human species?

1:48:07.600 --> 1:48:15.440
 I mean, I'm disappointed in humanity as humans. I mean, all of us, I'm, I'm one, so I'm disappointed

1:48:15.440 --> 1:48:23.200
 myself too. Um, it's kind of a sad state. There's, there's two things that disappoint me. One is

1:48:24.880 --> 1:48:30.640
 how it's difficult for us to separate our rational component of ourselves from our evolutionary

1:48:30.640 --> 1:48:39.040
 heritage, which is, you know, not always pretty. You know, rape is a, is an evolutionary good

1:48:39.040 --> 1:48:45.760
 strategy for reproduction. Murder can be at times too. You know, making other people miserable

1:48:45.760 --> 1:48:51.040
 at times is a good strategy for reproduction. It's just, and it's just, and, and so now that we know

1:48:51.040 --> 1:48:55.040
 that, and yet we have this sort of, you know, we, you and I can have this very rational discussion

1:48:55.040 --> 1:49:00.720
 talking about, you know, intelligence and brains and life and so on. Some, it seems like it's so hard.

1:49:00.720 --> 1:49:06.480
 It's just a big transition to get humans, all humans to, to make the transition from like,

1:49:06.480 --> 1:49:11.600
 let's pay no attention to all that ugly stuff over here. Let's just focus on the

1:49:11.600 --> 1:49:15.600
 incident. What's unique about humanity is our knowledge and our intellect.

1:49:16.160 --> 1:49:21.200
 But the fact that we're striving isn't itself amazing, right? The fact that we're able to

1:49:21.200 --> 1:49:28.240
 overcome that part and it seems like we are more and more becoming successful at overcoming that

1:49:28.240 --> 1:49:32.560
 part. That is the optimistic view and I agree with you. Yeah. But I worry about it. I'm not saying,

1:49:33.120 --> 1:49:37.040
 I'm worrying about it. I think that was your question. I still worry about it. Yes. You know,

1:49:37.040 --> 1:49:40.800
 we could be, and tomorrow because some terrorists could get nuclear bombs and, you know,

1:49:40.800 --> 1:49:45.920
 blow us all up. Who knows, right? The other thing I think I'm disappointed is, and it's just,

1:49:45.920 --> 1:49:49.120
 I understand it. It's, I guess you can't really be disappointed. It's just a fact,

1:49:49.120 --> 1:49:54.400
 is that we're so prone to false beliefs. We, you know, we have a model in our head,

1:49:55.360 --> 1:50:01.440
 the things we can interact with directly, physical objects, people, that model is pretty good.

1:50:01.440 --> 1:50:05.600
 And we can test it all the time, right? I touch something, I look at it, talk to you, see

1:50:05.600 --> 1:50:10.640
 my model is correct. But so much of what we know is stuff I can't directly interact with. I can't,

1:50:10.640 --> 1:50:16.800
 I don't even know because someone told me about it. And so, so we're prone, inherently prone to

1:50:16.800 --> 1:50:21.280
 having false beliefs because if I'm told something, how am I going to know it's right or wrong, right?

1:50:21.920 --> 1:50:27.840
 And so then we have the scientific process, which says we are inherently flawed. So the only way we

1:50:27.840 --> 1:50:37.920
 can get closer to the truth is by looking for contrary evidence. Yeah. Like this conspiracy

1:50:37.920 --> 1:50:42.480
 theory, this, this theory that scientists keep telling me about that the earth is round.

1:50:42.480 --> 1:50:49.280
 As far as I can tell, when I look out, it looks pretty flat. Yeah. So yeah, there's, there's

1:50:49.280 --> 1:50:56.960
 attention, but it's also, I tend to believe that we haven't figured out most of this thing, right?

1:50:58.480 --> 1:51:04.000
 Most of nature around us is a mystery. And so it, but that doesn't work. Does that worry you?

1:51:04.000 --> 1:51:07.600
 I mean, it's like, oh, that's, that's like a pleasure more to figure out, right? Yeah,

1:51:07.600 --> 1:51:13.520
 that's exciting. But I'm saying like, there's going to be a lot of quote unquote, wrong ideas.

1:51:13.520 --> 1:51:19.600
 I mean, I've been thinking a lot about engineering systems like social networks and so on. And I've

1:51:19.600 --> 1:51:24.320
 been worried about censorship and thinking through all that kind of stuff because there's a lot of

1:51:24.320 --> 1:51:32.400
 wrong ideas. There's a lot of dangerous ideas, but then I also read a history, read history and see

1:51:32.400 --> 1:51:38.880
 when you censor ideas that are wrong. Now, this could be a small scale censorship, like a young

1:51:38.880 --> 1:51:45.040
 grad student who comes up, who like raises their hand and says some crazy idea. It's a form of

1:51:45.040 --> 1:51:51.280
 censorship could be, I shouldn't use the word censorship, but like de incentivize them from,

1:51:51.280 --> 1:51:55.040
 no, no, no, no, this is the way it's been done. Yeah, you're foolish kid, don't think so. Yeah,

1:51:55.040 --> 1:52:04.320
 you're foolish. So in some sense, those wrong ideas most of the time end up being wrong,

1:52:04.320 --> 1:52:07.840
 but sometimes end up being. I agree with you. So I don't like the word censorship.

1:52:09.360 --> 1:52:16.800
 At the very end of the book, I ended up with a sort of a plea or a recommended force of action.

1:52:17.520 --> 1:52:22.640
 And the best way I could, I know how to deal with this issue that you bring up

1:52:22.640 --> 1:52:28.640
 is if everybody understood, as part of your upbringing life, something about how your brain

1:52:28.640 --> 1:52:34.000
 works, that it builds a model of the world, how it worked, how basic it builds that model of the

1:52:34.000 --> 1:52:39.600
 world, and that the model is not the real world. It's just a model. And it's never going to reflect

1:52:39.600 --> 1:52:44.080
 the entire world, and it can be wrong, and it's easy to be wrong. And here's all the ways you

1:52:44.080 --> 1:52:49.520
 can get the wrong model in your head, right? It's not to prescribe what's right or wrong,

1:52:49.520 --> 1:52:54.480
 it's just to understand that process. If we all understood the process, and then I got together

1:52:54.480 --> 1:52:58.560
 and you said, I disagree with you, Jeff, and I said, Lex, I disagree with you, that at least we

1:52:58.560 --> 1:53:03.440
 understand that we're both trying to model something. We both have different information

1:53:03.440 --> 1:53:06.560
 which leads to our different models. And therefore, I shouldn't hold it against you,

1:53:06.560 --> 1:53:11.040
 and you shouldn't hold it against me. And we can at least agree that, well, what can we look for

1:53:11.040 --> 1:53:18.640
 in its common ground to test our beliefs, as opposed to so much as we raise our kids on dogma,

1:53:18.640 --> 1:53:26.400
 which is this is a fact, and this is a fact, and these people are bad. And if everyone knew just

1:53:27.200 --> 1:53:33.360
 to be skeptical of every belief and why and how their brains do that, I think we might have a

1:53:33.360 --> 1:53:41.520
 better world. Do you think the human mind is able to comprehend reality? So you talk about

1:53:41.520 --> 1:53:49.520
 this creating models that are better and better. How close do you think we get to reality? So

1:53:49.520 --> 1:53:55.040
 the wildest ideas is like Donald Hoffman saying, we're very far away from reality. Do you think

1:53:55.040 --> 1:54:01.280
 we're getting close to reality? Well, I guess it depends on what you define reality. We have a

1:54:01.280 --> 1:54:07.440
 model of the world that's very useful for basic goals of survival. Well, for our survival and

1:54:07.440 --> 1:54:15.040
 our pleasure, right? So that's useful. That means really useful. Oh, we can build planes,

1:54:15.040 --> 1:54:20.960
 we can build computers, we can do these things. I don't think, I don't know the answer to that

1:54:20.960 --> 1:54:27.120
 question. I think that's part of the question we're trying to figure out. Obviously, if we

1:54:27.120 --> 1:54:32.000
 end up with a theory of everything that really is a theory of everything, and all of a sudden,

1:54:32.000 --> 1:54:35.760
 everything comes into play and there's no room for something else, then you might feel like we

1:54:35.760 --> 1:54:40.160
 have a good model of the world. Yeah, but if we have a theory of everything and somehow, first of

1:54:40.160 --> 1:54:44.480
 all, you'll never be able to really conclusively say it's a theory of everything, but say somehow

1:54:44.480 --> 1:54:50.480
 we are very damn sure it's a theory of everything. We understand what happened at the Big Bang and how

1:54:50.480 --> 1:54:55.920
 just the entirety of the physical process. I'm still not sure that gives us an understanding of

1:54:56.560 --> 1:55:03.280
 the next many layers of the hierarchy of abstractions that form. Well, also, what if string

1:55:03.280 --> 1:55:08.960
 theory turns out to be true, and then you say, well, we have no reality, no modeling, what's

1:55:08.960 --> 1:55:14.240
 going on in those other dimensions that are wrapped into it on each other, right? Or the multiverse,

1:55:14.880 --> 1:55:21.520
 you know? I honestly don't know how for us, for human interaction, for ideas of intelligence,

1:55:21.520 --> 1:55:25.520
 how it helps us to understand that we're made up of vibrating strings that are

1:55:26.720 --> 1:55:32.960
 like 10 to the whatever times smaller than us. You could probably build better

1:55:32.960 --> 1:55:36.640
 weapons of better rockets, but you're not going to be able to understand intelligence.

1:55:36.640 --> 1:55:41.680
 I guess maybe better computers. No, you won't be able to. I think it's just more purely knowledge.

1:55:41.680 --> 1:55:45.440
 You might lead to a better understanding of the beginning of the universe,

1:55:46.160 --> 1:55:52.720
 right? It might lead to a better understanding of, I don't know. I think the acquisition of

1:55:52.720 --> 1:56:01.840
 knowledge has always been one where you pursue it for its own pleasure and you don't always know

1:56:01.840 --> 1:56:07.680
 what is going to make a difference. You're pleasantly surprised by the weird things you find.

1:56:07.680 --> 1:56:13.200
 Do you think for the for the New York Cortex in general, do you think there's a lot of innovation

1:56:13.200 --> 1:56:19.760
 to be done on the machine side? You use the computer as a metaphor quite a bit. Is there

1:56:19.760 --> 1:56:23.600
 different types of computer that would help us build intelligence? What are the physical

1:56:23.600 --> 1:56:30.880
 manifestations of intelligent machines? Oh, no, it's going to be totally crazy. We have no idea

1:56:30.880 --> 1:56:36.640
 how this is going to look out yet. You can already see this. Today, of course, we modeled these things

1:56:36.640 --> 1:56:44.000
 on traditional computers, and now GPUs are really popular with neural networks and so on.

1:56:44.960 --> 1:56:48.960
 But there are companies coming up with fundamentally new physical substrates

1:56:50.400 --> 1:56:53.040
 that are just really cool. I don't know if they're going to work or not,

1:56:54.240 --> 1:56:58.560
 but I think there'll be decades of innovation here, totally.

1:56:58.560 --> 1:57:03.920
 Do you think the final thing will be messy, like our biology is messy? Or do you think

1:57:05.120 --> 1:57:09.600
 it's the old bird versus airplane question? Or do you think we could just

1:57:12.000 --> 1:57:18.000
 build airplanes that fly way better than birds in the same way we can build

1:57:20.640 --> 1:57:26.000
 electrical in New York Cortex? Can I riff on the bird thing a bit? Because I think it's

1:57:26.000 --> 1:57:33.040
 interesting. People really misunderstand this. The Wright brothers, the problem they were trying

1:57:33.040 --> 1:57:38.320
 to solve was controlled flight, how to turn an airplane, not how to propel an airplane.

1:57:38.320 --> 1:57:42.880
 They weren't worried about that. They already had, at that time, there was already wing shapes,

1:57:42.880 --> 1:57:46.640
 which they had from studying birds. There was already gliders that carried people.

1:57:46.640 --> 1:57:50.160
 The problem was, if you put a rudder on the back of a glider and you turn it, the plane falls out

1:57:50.160 --> 1:57:57.360
 of the sky. The problem was, how do you control flight? They studied birds. They actually had

1:57:57.360 --> 1:58:01.040
 birds in captivity. They watched birds in wind tunnels. They observed them in the wild. They

1:58:01.040 --> 1:58:06.240
 discovered the secret was the birds twist their wings when they turn. That's what they did on

1:58:06.240 --> 1:58:10.240
 the Wright brothers fly. They had these sticks that you would twist the wing. That was their

1:58:10.240 --> 1:58:16.000
 innovation, not their propeller. Today, airplanes still twist their wings. We don't twist the entire

1:58:16.000 --> 1:58:21.520
 wing. We just the tail end of it. The flaps, which is the same thing. Today's airplanes

1:58:21.520 --> 1:58:25.680
 fly on the same principles as birds, which we observe. Everyone get that analogy wrong.

1:58:26.720 --> 1:58:31.520
 Let's step back from that. Once you understand the principles of flight,

1:58:31.520 --> 1:58:36.720
 you can choose how to implement them. No one's going to use bones and feathers and muscles,

1:58:37.680 --> 1:58:43.040
 but they do have wings. We don't flap them. We have propellers. When we have the principles

1:58:43.040 --> 1:58:48.880
 of computation that goes on to modeling the world in a brain, we understand those principles

1:58:48.880 --> 1:58:53.760
 very clearly. We have choices on how to implement them. Some of them be biological like and some

1:58:53.760 --> 1:59:00.000
 won't. I do think there's going to be a huge amount of innovation here. Do you think about

1:59:00.000 --> 1:59:05.680
 the innovation when in the computer? They had invented the transistor. They invented the silicon

1:59:05.680 --> 1:59:11.280
 chip. They had invented software. It's the things they had to do, memory systems.

1:59:11.280 --> 1:59:19.760
 It's going to be similar. It's interesting that the effectiveness of deep learning for

1:59:19.760 --> 1:59:24.480
 specific tasks is driving a lot of innovation in the hardware, which may have effects

1:59:25.520 --> 1:59:31.360
 for actually allowing us to discover intelligent systems that operate very differently or

1:59:31.360 --> 1:59:37.680
 much bigger than deep learning. Ultimately, it's good to have an application that's making our

1:59:37.680 --> 1:59:45.200
 life better now because the capitalist process, if you can make money, that works. The other way,

1:59:46.240 --> 1:59:50.320
 Neil deGrasse Tyson writes about this, is the other way we fund science, of course, is through

1:59:50.320 --> 1:59:57.920
 military conquests. It's an interesting thing that we're doing on this regard. We used to have

1:59:57.920 --> 2:00:01.360
 a series of biological principles. We can see how to build these intelligent machines,

2:00:01.360 --> 2:00:08.240
 but we've decided to apply some of these principles to today's machine learning techniques. One

2:00:08.240 --> 2:00:12.880
 that we didn't talk about this principle, one is sparsity in the brain. Most of the neurons

2:00:12.880 --> 2:00:16.080
 are inactive at any point in time. It's sparse and the connectivity is sparse. That's different

2:00:16.080 --> 2:00:21.920
 than deep learning networks. We've already shown that we can speed up existing deep learning

2:00:21.920 --> 2:00:29.680
 networks anywhere from 10 to a factor of 100, literally 100, and make them more robust at the

2:00:29.680 --> 2:00:38.160
 same time. This is commercially very, very valuable. If we can prove this actually in the

2:00:38.160 --> 2:00:43.760
 largest systems that are commercially applied today, there's a big commercial desire to do this.

2:00:43.760 --> 2:00:50.240
 Well, sparsity is something that doesn't run really well on existing hardware. It doesn't

2:00:50.240 --> 2:01:00.000
 really run really well on GPUs and on CPUs. That would be a way of bringing more brain

2:01:00.000 --> 2:01:04.640
 principles into the existing system on a commercially valuable basis. Another thing we

2:01:04.640 --> 2:01:11.760
 can think we can do is we're going to use these dendrites. I talked earlier about the prediction

2:01:11.760 --> 2:01:17.120
 occurring from inside and around. That basic property can be applied to existing neural networks

2:01:17.120 --> 2:01:20.720
 and allow them to learn continuously with something they don't do today.

2:01:20.720 --> 2:01:23.520
 The dendritic spikes that you were talking about.

2:01:23.520 --> 2:01:28.960
 Yeah. Well, we wouldn't model the spikes, but the idea that today's neural networks have to

2:01:28.960 --> 2:01:34.320
 go to point neurons is a very simple model of a neuron. By adding dendrites to them,

2:01:34.320 --> 2:01:39.280
 at just one more level of complexity that's in biological systems, you can solve problems

2:01:39.280 --> 2:01:46.720
 in continuous learning and rapid learning. We're trying to bring the existing

2:01:46.720 --> 2:01:50.640
 field. We'll see if we can do it. We're trying to bring the existing field of machine learning

2:01:52.240 --> 2:01:56.720
 commercially along with us. You brought up this idea of keeping paying for it commercially along

2:01:56.720 --> 2:02:01.520
 with us as we move towards the ultimate goal of a true AI system. Even small innovations on

2:02:01.520 --> 2:02:08.160
 neural networks are really, really exciting. It seems like such a trivial model of the brain

2:02:08.160 --> 2:02:17.520
 and applying different insights that just even, like you said, continuous learning or making it

2:02:17.520 --> 2:02:25.760
 more asynchronous or maybe making more dynamic or incentivizing. Or more robust. Even just

2:02:25.760 --> 2:02:33.760
 one more robust. And making it somehow much better, incentivizing sparsity somehow.

2:02:33.760 --> 2:02:37.920
 Yeah. Well, if you can make things 100 times faster, then there's plenty of incentive.

2:02:40.080 --> 2:02:43.440
 People spending millions of dollars just training some of these networks now,

2:02:44.320 --> 2:02:45.440
 these transforming networks.

2:02:46.800 --> 2:02:53.520
 Let me ask you a big question. For young people listening to this today in high school and college,

2:02:53.520 --> 2:03:01.520
 what advice would you give them in terms of which career path to take and maybe just about life in

2:03:01.520 --> 2:03:09.680
 general? Well, in my case, I didn't start life with any kind of goals. I was, when I was going to

2:03:09.680 --> 2:03:12.960
 college, I was like, oh, what do I study? Well, maybe I'll do intellectual engineering stuff.

2:03:15.120 --> 2:03:18.880
 I wasn't like, today you see some of these young kids are so motivated, change the world. I was like,

2:03:18.880 --> 2:03:26.880
 hey, whatever. But then I did fall in love with something besides my wife. But I fell in love

2:03:26.880 --> 2:03:30.240
 with this like, oh my God, it would be so cool to understand how the brain works.

2:03:30.240 --> 2:03:34.480
 And then I said to myself, that's the most important thing I could work on. I can't

2:03:34.480 --> 2:03:37.360
 imagine anything more important because if you understand how the brain's working,

2:03:37.360 --> 2:03:40.400
 build tells machines and they could figure out all the other big questions in the world.

2:03:41.120 --> 2:03:45.440
 So, and then I said, I want to understand how I work. So I fell in love with this idea and I

2:03:45.440 --> 2:03:52.160
 became passionate about it. And this is a trope people say this, but it's true.

2:03:52.880 --> 2:03:57.680
 Because I was passionate about it, I was able to put up with almost so much crap.

2:03:57.680 --> 2:04:02.080
 You know, I was in that, you know, I was like,

2:04:02.080 --> 2:04:05.600
 person said, you can't do this. I was a graduate student at Berkeley when they said,

2:04:05.600 --> 2:04:09.920
 you can't study this problem. You know, no one's can solve this or you can't get funded for it.

2:04:09.920 --> 2:04:13.040
 You know, then I went into do, you know, mobile computing and there's like people say,

2:04:13.040 --> 2:04:18.880
 you can't do that. You can't build a cell phone. You know, so, but all along I kept being motivated

2:04:18.880 --> 2:04:22.320
 because I wanted to work on this problem. I said, I want to understand the brain works.

2:04:22.320 --> 2:04:26.240
 I got myself, you know, I got one lifetime, I'm going to figure it out, do as best I can.

2:04:26.240 --> 2:04:31.680
 So by having that, because you know, these things, it's really, as you point out, Lex,

2:04:31.680 --> 2:04:36.080
 it's really hard to do these things. People, it's just, there's so many downers along the way.

2:04:36.880 --> 2:04:40.080
 So many way obstacles are getting your way. Yeah, I'm sitting here happy all the time,

2:04:40.080 --> 2:04:42.080
 but trust me, it's not always like that.

2:04:42.080 --> 2:04:47.520
 That's, I guess, the happiness that the passion is a prerequisite for surviving the whole thing.

2:04:47.520 --> 2:04:53.120
 Yeah, I think so. I think that's right. And so I don't want to sit to someone say, you know,

2:04:53.120 --> 2:04:57.920
 you need to find a passion and do it. No, maybe you don't. But if you do find something you're

2:04:57.920 --> 2:05:04.000
 passionate about, then, then you can follow it as far as your passion will let you put up with it.

2:05:04.000 --> 2:05:07.440
 Do you remember how you found it? This is how the spark happened.

2:05:08.960 --> 2:05:10.800
 Why, specifically for me?

2:05:10.800 --> 2:05:14.800
 Yeah, like, because you said, it's such an interesting, so like almost like later in life,

2:05:14.800 --> 2:05:20.560
 by later, I mean, like not when you were five, you, you didn't really know. And then all of a

2:05:20.560 --> 2:05:24.160
 sudden you fell in love with it. Yeah, yeah. There was there was two separate events that

2:05:24.160 --> 2:05:29.920
 compounded one another. One, when I was probably a teenager, it might have been 17 or 18,

2:05:29.920 --> 2:05:35.360
 I made a list of the most interesting problems I could think of. First was, why does the universe

2:05:35.360 --> 2:05:40.320
 exist? Seems like not existing is more likely. Yeah. The second one was, well, given exists,

2:05:40.320 --> 2:05:44.000
 why does it behave the way it does? You know, laws of physics, why is it equal to MC squared,

2:05:44.000 --> 2:05:47.760
 not MC cubed? You know, that's interesting question. I don't know. Third one was like,

2:05:47.760 --> 2:05:53.840
 what's the origin of life? And the fourth one was what's intelligence? And I stopped there.

2:05:53.840 --> 2:05:57.040
 I said, well, that's probably the most interesting one. And I put that aside

2:05:58.160 --> 2:06:05.520
 as a teenager. But then when I was 22, and I was reading the, no, I was, excuse me, I was 70,

2:06:05.520 --> 2:06:12.000
 it was 1979, excuse me, 1979. I was reading, so I was at that time was 22. I was reading

2:06:12.960 --> 2:06:16.880
 the September issue of Scientific American, which is all about the brain. And then the

2:06:16.880 --> 2:06:24.000
 final essay was by Francis Crick, who of DNA fame, and he had taken his interest

2:06:24.000 --> 2:06:29.280
 to studying the brain now. And he said, you know, there's something wrong here. He says,

2:06:29.280 --> 2:06:34.960
 we got all this data, all this fact, this is 1979, all these facts about the brain,

2:06:34.960 --> 2:06:40.000
 tons and tons of facts about the brain. Do we need more facts? Or do we just need to think

2:06:40.000 --> 2:06:43.680
 about a way of rearranging the facts we have? Maybe we're just not thinking about the problem

2:06:43.680 --> 2:06:49.520
 correctly. You know, he says, this shouldn't be, this shouldn't be like this, you know?

2:06:50.160 --> 2:06:55.840
 So I read that and I said, wow. I said, I don't have to become like an experimental

2:06:55.840 --> 2:07:02.000
 neuroscientist. I could just look at all those facts and try to become a theoretician and try

2:07:02.000 --> 2:07:07.360
 to figure it out. And I said, that, I felt like it was something I would be good at. I said,

2:07:07.360 --> 2:07:12.320
 I wouldn't be a good experimentalist. I don't have the patience for it. But I'm a good thinker

2:07:12.320 --> 2:07:16.560
 and I love puzzles. And this is like the biggest puzzle in the world. This is the biggest puzzle

2:07:16.560 --> 2:07:21.520
 of all time. And I got all the puzzle pieces in front of me. Damn, that was exciting.

2:07:21.520 --> 2:07:25.040
 And there's something obviously you can't convert into words. They just kind of

2:07:25.920 --> 2:07:30.080
 sparked this passion. And I have that a few times in my life, just something

2:07:32.000 --> 2:07:38.160
 just, just like you, it grabs you. Yeah. I thought it was something that was both important

2:07:38.160 --> 2:07:42.640
 and that I could make a contribution to. And so all of a sudden it felt like, oh, it gave me purpose

2:07:42.640 --> 2:07:47.440
 in life, you know? I honestly don't think it has to be as big as one of those four questions.

2:07:48.800 --> 2:07:54.400
 You can find those things in the smallest. Oh, absolutely. David Foster Wallace said,

2:07:54.400 --> 2:08:00.480
 like, the key to life is to be unboreable. I think it's very possible to find that

2:08:00.480 --> 2:08:04.640
 intensity of joy in the smallest thing. Absolutely. I'm just, you asked me my story.

2:08:04.640 --> 2:08:08.960
 Yeah. No, but I'm actually speaking to the audience. It doesn't have to be those four.

2:08:08.960 --> 2:08:14.480
 You happen to get excited by one of the bigger questions in the universe. But

2:08:16.160 --> 2:08:21.680
 even the smallest things and watching the Olympics now, just giving yourself life,

2:08:22.480 --> 2:08:27.600
 giving your life over to the study and the mastery of a particular sport is fascinating.

2:08:27.600 --> 2:08:33.680
 And if it sparks joy and passion, you're able to, in the case of the Olympics,

2:08:33.680 --> 2:08:37.920
 basically suffer for like a couple of decades to achieve. I mean, you can find joy and passion

2:08:37.920 --> 2:08:44.400
 just being a parent. I mean, yeah, the parenting one is funny. So I was not always, but for a long

2:08:44.400 --> 2:08:49.600
 time, wanted kids and get married and stuff. And especially it has to do with the fact that

2:08:49.600 --> 2:08:57.920
 I've seen a lot of people that I respect get a whole other level of joy from kids. And at,

2:08:57.920 --> 2:09:04.240
 you know, at first is like, you're thinking is, well, like, I don't have enough time in the day,

2:09:05.040 --> 2:09:08.000
 right? If I have this passion to solve intelligence. Which is true.

2:09:09.280 --> 2:09:14.080
 But like, if I want to solve intelligence, how's this kid situation going to help me?

2:09:14.080 --> 2:09:22.640
 But then you realize that, you know, like you said, the things that sparks joy and it's very

2:09:22.640 --> 2:09:29.280
 possible that kids can provide even a greater or deeper, more meaningful joy than those bigger

2:09:29.280 --> 2:09:34.560
 questions when they enrich each other. And that seemed like a, obviously, when I was younger,

2:09:34.560 --> 2:09:39.120
 it's probably a counterintuitive notion because there's only so many hours in the day. But then

2:09:39.120 --> 2:09:43.680
 life is finite and you have to pick the things that give you joy.

2:09:44.720 --> 2:09:50.800
 But you also understand you can be patient too. I mean, it's finite, but we do have, you know,

2:09:50.800 --> 2:09:55.920
 whatever, 50 years or something. It's us alone. Yeah. So in my case, you know, in my case,

2:09:55.920 --> 2:10:00.560
 I had to give up on my dream of the neuroscience because I was a graduate student at Berkeley

2:10:00.560 --> 2:10:03.920
 and they told me I couldn't do this and I couldn't get funded. And, you know, and,

2:10:04.480 --> 2:10:09.200
 and so I went back in, and went back into the computing industry for a number of years. I

2:10:09.200 --> 2:10:12.880
 thought it would be four, but it turned out to be more. But I said, but I said, I'll come back.

2:10:13.440 --> 2:10:16.400
 You know, I definitely, I'm definitely going to come back. I know I'm going to do this computer

2:10:16.400 --> 2:10:19.920
 stuff for a while, but I'm definitely coming back. Everyone knows that. And it's the same

2:10:19.920 --> 2:10:23.600
 as raising kids. Well, yeah, you still, you have to spend a lot of time with your kids. It's fun,

2:10:23.600 --> 2:10:29.120
 enjoyable. But that doesn't mean you have to give up on other dreams. It just means that you have

2:10:29.120 --> 2:10:37.680
 to wait a week or two to work on that next idea. You talk about the darker side of me,

2:10:37.680 --> 2:10:42.880
 disappointing sides of human nature that we're hoping to overcome so that we don't destroy

2:10:42.880 --> 2:10:52.480
 ourselves. I tend to put a lot of value in the broad general concept of love, of the human capacity

2:10:52.480 --> 2:10:59.520
 to, of compassion towards each other, of just kindness, whatever that longing of like just the

2:10:59.520 --> 2:11:05.600
 human, human to human connection, it connects back to our initial discussion. I tend to see a lot

2:11:05.600 --> 2:11:10.080
 of value in this collective intelligence aspect. I think some of the magic of human civilization

2:11:10.080 --> 2:11:16.720
 happens when there's a party is not as fun when you're alone. I totally agree with you on these

2:11:16.720 --> 2:11:23.840
 issues. Do you think from a New York Cortex perspective, what role does love play in the

2:11:23.840 --> 2:11:29.040
 human condition? Well, those are two separate things from the New York Cortex. I don't think it

2:11:29.040 --> 2:11:33.680
 doesn't impact our thinking about the New York Cortex. From a human condition point of view,

2:11:33.680 --> 2:11:42.240
 I think it's core. I mean, we get so much pleasure out of loving people and helping people.

2:11:45.600 --> 2:11:50.320
 I'll rack it up to old brain stuff, and maybe we can throw it under the bus of evolution,

2:11:50.320 --> 2:11:57.120
 if you want. That's fine. It doesn't impact how we think about how we model the world,

2:11:58.320 --> 2:12:00.800
 but from a humanity point of view, I think it's essential.

2:12:00.800 --> 2:12:06.480
 Well, I tend to give it to the new brain, and also I tend to think the sum of aspects of that

2:12:06.480 --> 2:12:14.960
 need to be engineered into AI systems, both in their ability to have compassion for other humans

2:12:16.080 --> 2:12:24.560
 and their ability to maximize love in the world between humans. I'm more thinking about the social

2:12:24.560 --> 2:12:29.680
 network. Whenever there's a deep integration between AI systems and humans, there's specific

2:12:29.680 --> 2:12:36.640
 applications where it's AI and humans. I think that's something that's often not talked about in

2:12:36.640 --> 2:12:47.120
 terms of metrics over which you try to maximize, which metric to maximize in a system. It seems

2:12:47.120 --> 2:12:55.120
 like one of the most powerful things in societies is the capacity to love.

2:12:55.120 --> 2:13:01.840
 It's a great way of thinking about it. I have been thinking more of these fundamental

2:13:01.840 --> 2:13:07.600
 mechanisms in the brain as opposed to the social interaction between humans and AI systems in

2:13:07.600 --> 2:13:14.080
 the future. If you think about that, you're absolutely right, but that's a complex system.

2:13:14.080 --> 2:13:17.360
 I can have intelligent systems that don't have that component, but they're not interacting

2:13:17.360 --> 2:13:21.200
 with people. They're just running something or building a building someplace or something,

2:13:21.200 --> 2:13:28.080
 I don't know. If you think about interacting with humans, it has to be engineered in there.

2:13:28.080 --> 2:13:31.680
 I don't think it's going to appear on its own. That's a good question.

2:13:35.040 --> 2:13:42.720
 In terms of, from a reinforcement learning perspective, whether the darker

2:13:43.840 --> 2:13:49.520
 size of human nature or the better angels of our nature win out, statistically speaking,

2:13:49.520 --> 2:13:53.280
 I don't know. I tend to be optimistic and hope that love wins out in the end.

2:13:54.480 --> 2:14:02.880
 You've done a lot of incredible stuff. Your book is driving towards this fourth question

2:14:02.880 --> 2:14:08.080
 that you started with on the nature of intelligence. What do you hope your legacy

2:14:08.880 --> 2:14:14.880
 for people reading 100 years from now? How do you hope they remember your work?

2:14:14.880 --> 2:14:20.800
 How do you hope they remember this book? Well, I think as an entrepreneur or a scientist or

2:14:21.520 --> 2:14:28.320
 any human who's trying to accomplish some things, I have a view that really all you can do is

2:14:28.320 --> 2:14:34.640
 accelerate the inevitable. It's like, if we didn't figure out, if we didn't study the brain,

2:14:34.640 --> 2:14:38.560
 someone else would study the brain. If Elon just didn't make electric cars, someone else would do

2:14:38.560 --> 2:14:44.800
 it eventually. If Thomas Edison didn't invent a light bulb, we wouldn't be using candles today.

2:14:44.800 --> 2:14:51.200
 What you can do as an individual is you can accelerate something that's beneficial and make

2:14:51.200 --> 2:14:56.240
 it happen sooner than whatever. That's really it. That's all you can do. You can't create a new

2:14:56.240 --> 2:15:04.720
 reality that it wasn't going to happen. From that perspective, I would hope that our work,

2:15:04.720 --> 2:15:10.480
 not just me, but our work in general, people would look back and said, hey, they really helped make

2:15:10.480 --> 2:15:17.680
 this better future happen sooner. They helped us understand the nature of false beliefs sooner

2:15:17.680 --> 2:15:21.920
 than what I made up. Now, we're so happy that we have these intelligent machines doing these things,

2:15:21.920 --> 2:15:27.360
 helping us that maybe that solved the climate change problem, and they made it happen sooner.

2:15:28.240 --> 2:15:33.760
 I think that's the best I would hope for. Some would say, those guys just moved the needle forward

2:15:33.760 --> 2:15:40.080
 a little bit in time. Well, it feels like the progress of human civilization is not,

2:15:42.000 --> 2:15:50.000
 there's a lot of trajectories. If you have individuals that accelerate towards one direction

2:15:50.000 --> 2:15:56.960
 that helps steer human civilization. I think in those long stretch of time, all trajectories will

2:15:56.960 --> 2:16:01.920
 be traveled, but I think it's nice for this particular civilization on earth to travel down

2:16:01.920 --> 2:16:06.320
 one that's not. Yeah. Well, I think you're right. We have to take the whole period of World War II

2:16:06.320 --> 2:16:10.400
 and Nazism or something like that. Well, that was a bad side step. We've been on with that for a

2:16:10.400 --> 2:16:17.360
 while, but there is the optimistic view about life that ultimately it does converge in a positive

2:16:17.360 --> 2:16:26.080
 way. It progresses ultimately, even if we have years of darkness. I think you could perhaps,

2:16:26.080 --> 2:16:31.040
 that's accelerating the positive. It could also mean eliminating some bad missteps along the way,

2:16:31.040 --> 2:16:38.240
 too. But I'm an optimistic in that way. Despite we talked about the end of civilization,

2:16:39.200 --> 2:16:43.680
 I think we're going to live for a long time. I hope we are. I think our society in the future

2:16:43.680 --> 2:16:46.400
 is going to be better. We're going to have less discord. We're going to have less people killing

2:16:46.400 --> 2:16:52.080
 each other. We'll make them live in some way that's compatible with the carrying capacity of the

2:16:52.080 --> 2:16:57.760
 earth. I'm optimistic these things will happen. All we can do is try to get there sooner.

2:16:57.760 --> 2:17:06.400
 At the very least, if we do destroy ourselves, we'll have a few satellites that will tell alien

2:17:06.400 --> 2:17:14.160
 civilization that we were once here. Or maybe our future inhabitants of earth. Imagine the

2:17:14.160 --> 2:17:18.000
 planet of the ape scenario. We kill ourselves a million years from now or a billion years from

2:17:18.000 --> 2:17:21.520
 now. There's another species on the planet. There's curious creatures who are once here.

2:17:22.560 --> 2:17:27.440
 Jeff, thank you so much for your work and thank you so much for talking to me once again.

2:17:27.440 --> 2:17:31.440
 Well, it's great. I love what you do. I love your podcast. You have those interesting people

2:17:31.440 --> 2:17:40.320
 me aside. It's a real service I think you do for a very broader sense for humanity, I think.

2:17:40.320 --> 2:17:44.880
 Thanks, Jeff. All right. It's a pleasure. Thanks for listening to this conversation with Jeff

2:17:44.880 --> 2:17:52.880
 Hawkins. And thank you to Code Academy, Bio Optimizers, ExpressVPN, A Sleep, and Blinkist.

2:17:52.880 --> 2:17:58.720
 Check them out in the description to support this podcast. And now let me leave you with some words

2:17:58.720 --> 2:18:07.120
 from Albert Camus. An intellectual is someone whose mind watches itself. I like this because I'm

2:18:07.120 --> 2:18:13.600
 happy to be both halves, the watcher and the watched. Can they be brought together? This

2:18:13.600 --> 2:18:23.600
 is a practical question we must try to answer. Thank you for listening. I hope to see you next time.

