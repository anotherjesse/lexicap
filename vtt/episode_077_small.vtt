WEBVTT

00:00.000 --> 00:06.400
 The following is a conversation with Alex Garland, writer and director of many imaginative and

00:06.400 --> 00:12.560
 philosophical films from the dreamlike exploration of human self destruction in the movie Annihilation

00:12.560 --> 00:18.480
 to the deep questions of consciousness and intelligence raised in the movie Ex Machina,

00:18.480 --> 00:22.880
 which to me is one of the greatest movies in artificial intelligence ever made.

00:23.680 --> 00:28.400
 I'm releasing this podcast to coincide with the release of his new series called Devs,

00:28.400 --> 00:34.400
 that will premiere this Thursday, March 5th on Hulu as part of FX on Hulu.

00:35.440 --> 00:38.400
 It explores many of the themes this very podcast is about,

00:39.120 --> 00:45.280
 from quantum mechanics, to artificial life, to simulation, to the modern nature of power in

00:45.280 --> 00:52.880
 the tech world. I got a chance to watch a preview and loved it. The acting is great, Nick Offerman

00:52.880 --> 00:59.360
 especially is incredible in it, the cinematography is beautiful, and the philosophical and scientific

00:59.360 --> 01:05.440
 ideas explored are profound. And for me as an engineer and scientist, we're just fun to see

01:05.440 --> 01:10.720
 brought to life. For example, if you watch the trailer for the series carefully, you'll see

01:10.720 --> 01:16.480
 there's a programmer with a Russian accent looking at a screen with Python like code on it that appears

01:16.480 --> 01:22.160
 to be using a library that interfaces with a quantum computer. This attention and technical

01:22.160 --> 01:28.320
 detail on several levels is impressive, and one of the reasons I'm a big fan of how Alex weaves

01:28.320 --> 01:35.360
 science and philosophy together in his work. Meeting Alex for me was unlikely, but it was

01:35.360 --> 01:41.760
 life changing in ways I may only be able to articulate in a few years. Just as meeting

01:41.760 --> 01:47.600
 spot many of Boston Dynamics for the first time planted a seed of an idea in my mind,

01:47.600 --> 01:54.320
 so did meeting Alex Garland. He's humble, curious, intelligent, and to me an inspiration.

01:55.120 --> 02:00.080
 Plus, he's just really a fun person to talk with about the biggest possible questions

02:00.080 --> 02:05.760
 in our universe. This is the artificial intelligence podcast. If you enjoy it,

02:05.760 --> 02:10.320
 subscribe on YouTube, give it five stars on Apple podcast, support it on Patreon,

02:10.320 --> 02:17.680
 or simply connect with me on Twitter, and Lex Friedman spelled F R I D M A M. As usual,

02:17.680 --> 02:21.840
 I'll do one or two minutes of ads now and never any ads in the middle that can break the flow of

02:21.840 --> 02:26.400
 the conversation. I hope that works for you and doesn't hurt the listening experience.

02:27.360 --> 02:32.160
 This show is presented by Cash App, the number one finance app in the App Store.

02:32.160 --> 02:38.080
 When you get it, use code lexpodcast. Cash App lets you send money to friends buy

02:38.080 --> 02:44.160
 Bitcoin and invest in the stock market with as little as $1. Since Cash App allows you to buy

02:44.160 --> 02:50.240
 Bitcoin, let me mention that cryptocurrency in the context of the history of money is fascinating.

02:50.240 --> 02:54.160
 I recommend Ascent of Money as a great book on this history.

02:54.880 --> 03:02.480
 Debits and credits on ledgers started 30,000 years ago. The US dollar was created about

03:02.480 --> 03:09.040
 200 years ago. At Bitcoin, the first decentralized cryptocurrency was released just over 10 years

03:09.040 --> 03:14.800
 ago. So given that history, cryptocurrency is still very much in its early days of development,

03:14.800 --> 03:19.840
 but it still is aiming to and just might redefine the nature of money.

03:20.560 --> 03:26.000
 So again, if you get Cash App from the App Store, Google Play, and use code lexpodcast,

03:26.000 --> 03:31.760
 you'll get $10 and Cash App will also donate $10 to first, one of my favorite organizations

03:31.760 --> 03:36.320
 that is helping advance robotics and STEM education for young people around the world.

03:37.440 --> 03:45.200
 And now, here's my conversation with Alex Garland. You described the world inside the shimmer in

03:45.200 --> 03:50.000
 the movie Annihilation as dreamlike, in that it's internally consistent but detached from reality.

03:50.640 --> 03:57.040
 That leads me to ask, do you think a philosophical question, I apologize, do you think we might

03:57.040 --> 04:04.720
 be living in a dream or in a simulation like the kind that the shimmer creates? We human beings

04:05.600 --> 04:13.120
 here today. Yeah. I want to sort of separate that out into two things. Yes, I think we're living in

04:13.120 --> 04:20.720
 a dream of sorts. No, I don't think we're living in a simulation. I think we're living on a planet

04:20.720 --> 04:28.160
 with a very thin layer of atmosphere and the planet is in a very large space and the space

04:28.160 --> 04:32.640
 is full of other planets and stars and quasars and stuff like that. And I don't think, I don't

04:32.640 --> 04:38.960
 think those physical objects, I don't think the matter in that universe is simulated. I think

04:38.960 --> 04:47.280
 it's there. We are definitely, what's the whole problem we're saying, definitely, but in my opinion,

04:47.280 --> 04:52.960
 I'll just go back to that. I think it seems very like we're living in a dream state,

04:52.960 --> 04:57.600
 I'm pretty sure we are. And I think that's just to do with the nature of how we experience the

04:57.600 --> 05:05.360
 world, we experience it in a subjective way. And the thing I've learned most as I've got older in

05:05.360 --> 05:11.840
 some respects is the degree to which reality is counterintuitive and that the things that are

05:11.840 --> 05:16.640
 presented to us as objective turn out not to be objective and quantum mechanics is full of that

05:16.640 --> 05:20.240
 kind of thing, but actually just day to day life is full of that kind of thing as well.

05:20.800 --> 05:29.920
 So my understanding of the way the brain works is you get some information, hit your

05:29.920 --> 05:34.800
 optic nerve and then your brain makes its best guess about what it's seeing or what it's saying

05:34.800 --> 05:41.200
 it's seeing. It may or may not be an accurate best guess, it might be an inaccurate best guess.

05:41.200 --> 05:48.800
 And that gap, the best guess gap, means that we are essentially living in a subjective state,

05:48.800 --> 05:54.000
 which means that we're in a dream state. So I think you could enlarge on the dream state in

05:54.000 --> 05:59.120
 all sorts of ways, but so yes, dream state, no simulation would be where I'd come down.

06:00.640 --> 06:07.760
 Going further deeper into that direction, you've also described that world as a psychedelia.

06:07.760 --> 06:13.280
 Yeah. So on that topic, I'm curious about that world. On the topic of psychedelic drugs,

06:13.280 --> 06:21.360
 do you see those kinds of chemicals that modify our perception as a distortion of our perception

06:21.360 --> 06:27.680
 of reality or a window into another reality? No, I think what I'd be saying is that we live

06:27.680 --> 06:32.400
 in a distorted reality and then those kinds of drugs give us a different kind of distorted

06:32.400 --> 06:36.800
 perspective. Yeah, exactly. They just give an alternate distortion. And I think that what

06:36.800 --> 06:44.800
 they really do is they give a distorted perception, which is a little bit more allied to daydreams

06:45.440 --> 06:51.280
 or unconscious interests. So if for some reason you're feeling unconsciously anxious at that

06:51.280 --> 06:56.400
 moment and you take a psychedelic drug, you'll have a more pronounced unpleasant experience.

06:56.400 --> 07:02.880
 And if you're feeling very calm or happy, you might have a good time. But yeah, so if I'm

07:02.880 --> 07:08.240
 saying we're starting from a premise, our starting point is we're already in the slightly

07:08.240 --> 07:14.080
 psychedelic state. What those drugs do is help you go further down an avenue or maybe a slightly

07:14.080 --> 07:23.200
 different avenue, but that's what. So in that movie Annihilation, the shimmer, this alternate

07:23.200 --> 07:30.480
 dreamlike state is created by, I believe, perhaps an alien entity. Of course, everything's up to

07:30.480 --> 07:36.640
 interpretation. But do you think there's, in our world, in our universe, do you think there's

07:36.640 --> 07:44.800
 intelligent life out there? And if so, how different is it from us humans? Well, one of the things I

07:44.800 --> 07:53.280
 was trying to do in Annihilation was to offer up a form of alien life that was actually alien.

07:53.280 --> 08:03.760
 Because it would often seem to me that in the way we would represent aliens in books or cinema or

08:03.760 --> 08:10.960
 television or any one of the sort of storytelling mediums is we would always give them very human

08:10.960 --> 08:16.000
 like qualities. So they wanted to teach us about galactic federations or they wanted to eat us or

08:16.000 --> 08:21.200
 they wanted our resources like our water or they want to enslave us or whatever it happens to be.

08:21.200 --> 08:29.520
 But all of these are incredibly human like motivations. And I was interested in the idea

08:29.520 --> 08:37.200
 of an alien that was not in any way like us. It didn't share. It maybe it had a completely

08:37.200 --> 08:43.200
 different clock speed, maybe its way. So we're talking about we're looking at each other, we're

08:43.200 --> 08:48.960
 getting information, light hits our optic nerve, our brain makes the best guess of what we're doing.

08:48.960 --> 08:52.960
 Sometimes it's right on the thing we were talking about before. What if this alien doesn't have

08:53.920 --> 08:59.120
 an optic nerve? Maybe it's way of encountering the space it's in is wholly different.

08:59.120 --> 09:04.000
 Maybe it has a different relationship with gravity. The basic laws of physics that operates under

09:04.000 --> 09:09.200
 might be fundamentally different. It could be a different time scale and so on. Yeah. Or it could

09:09.200 --> 09:16.720
 be the same laws. It could be the same underlying laws of physics. It's a machine created or it's

09:16.720 --> 09:21.840
 a creature created in a quantum mechanical way. It just ends up in a very, very different place to

09:21.840 --> 09:28.320
 the one we end up in. So part of the preoccupation with annihilation was to come up with an alien

09:28.320 --> 09:36.080
 that was really alien and didn't give us and it didn't give us and we didn't give it any kind of

09:36.640 --> 09:42.240
 easy connection between human and the alien. Because I think it was to do with the idea that

09:42.240 --> 09:47.280
 you could have an alien that landed on this planet that wouldn't even know we were here and we might

09:47.280 --> 09:53.200
 only glancingly know it was here. There'd just be this strange point where the Venn diagrams

09:53.200 --> 09:57.360
 connected where we could sense each other or something like that. So in the movie, first of

09:57.360 --> 10:04.560
 all, incredibly original view of what an alien life would be. And in that sense, it's a huge success.

10:04.560 --> 10:12.880
 Let's go inside your imagination. Did the alien, that alien entity know anything about humans

10:12.880 --> 10:19.680
 when it landed? No. So the idea is you're both, you're basically an alien life is

10:20.400 --> 10:26.240
 trying to reach out to anything that might be able to hear its mechanism of communication. Or was

10:26.240 --> 10:32.240
 it simply, was it just basically their biologist exploring different kinds of stuff that you can

10:32.240 --> 10:36.640
 find? But you see, this is the interesting thing is, as soon as you say they're biologists,

10:36.640 --> 10:43.600
 you've done the thing of attributing human type motivations to it. I was trying to

10:44.800 --> 10:51.520
 free myself from anything like that. So all sorts of questions you might answer about this

10:51.520 --> 10:57.200
 notion or alien, I wouldn't be able to answer because I don't know what it was or how it worked.

10:57.200 --> 11:05.040
 You know, I had some rough ideas, like it had a very, very, very slow clock speed. And I thought,

11:05.040 --> 11:09.520
 maybe the way it is interacting with this environment is a little bit like the way an

11:09.520 --> 11:16.560
 octopus will change its color forms around the space that it's in. So it's sort of reacting to

11:16.560 --> 11:22.720
 what it's in to an extent, but the reason it's reacting in that way is indeterminate.

11:22.720 --> 11:30.240
 But it's so, but its clock speed was slower than our human life clock speed or inter,

11:30.240 --> 11:36.240
 but it's faster than evolution faster than our than our evolution. Yeah, given the four billion

11:36.240 --> 11:40.560
 years it took us to get here, then yes, maybe it started eight. If you look at the human

11:40.560 --> 11:46.000
 civilization as a single organism, yeah, in that sense, you know, this evolution could be us,

11:46.800 --> 11:51.520
 you know, the evolution of the living organisms on earth could be just a single organism. And it's

11:51.520 --> 11:58.480
 kind of, that's its life is the evolution process that eventually will lead to probably the

11:59.760 --> 12:04.960
 heat death of the universe or something before that. I mean, that's, that's just an incredible

12:04.960 --> 12:11.520
 idea. So you almost don't know, you've created something that you don't even know how it works.

12:11.520 --> 12:20.240
 Yeah, because anytime I tried to look into how it might work, I would then inevitably be attaching

12:20.240 --> 12:25.280
 my kind of thought processes into it. And I wanted to try and put a bubble around it where I say,

12:25.280 --> 12:32.560
 no, this is, this is alien in its most alien form. I have no real point of contact.

12:34.160 --> 12:40.720
 So unfortunately, I can't talk to Stanley Kubrick. So I'm really fortunate to get a chance to talk

12:40.720 --> 12:48.320
 to you. Do you, on this particular notion, I'd like to ask it a bunch of different ways and

12:48.320 --> 12:53.520
 we'll explore it in different ways. But do you ever consider human imagination, your imagination,

12:53.520 --> 13:00.960
 as a window into a possible future, and that what you're doing, you're putting that imagination on

13:00.960 --> 13:07.280
 paper as a writer and then on screen as a director. And that plants the seeds in the minds of millions

13:07.280 --> 13:13.680
 of future and current scientists. And so your imagination, you putting it down actually makes

13:13.680 --> 13:19.520
 it as a reality. So it's almost like a first step of the scientific method. Like you imagining

13:19.520 --> 13:27.200
 what's possible in your new series with X Machina is actually inspiring, you know, thousands of

13:27.200 --> 13:32.720
 12 year olds, millions of scientists and actually creating the future view of Imagine.

13:34.320 --> 13:39.440
 Well, all I could say is that from my point of view, it's almost exactly the reverse. Because

13:39.440 --> 13:51.280
 I see that pretty much everything I do is a reaction to what scientists are doing. I'm an

13:51.280 --> 14:01.040
 interested layperson. And I feel, you know, this individual, I feel that the most interesting

14:01.760 --> 14:07.520
 area that humans are involved in is science. I think art is very, very interesting, but the

14:07.520 --> 14:15.040
 most interesting is science. And science is in a weird place, because maybe around the time

14:15.600 --> 14:22.000
 Newton was alive, if a very, very interested layperson said to themselves, I want to really

14:22.000 --> 14:28.400
 understand what Newton is saying about the way the world works. With a few years of dedicated

14:28.400 --> 14:34.800
 thinking, they would be able to understand the sort of principles he was laying out. And I don't

14:34.800 --> 14:42.320
 think that's true anymore. I think that's stopped being true now. So I'm a pretty smart guy. And

14:42.320 --> 14:50.560
 if I said to myself, I want to really, really understand what is currently the state of quantum

14:50.560 --> 14:55.520
 mechanics or string theory or any of the sort of branching areas of it, I wouldn't be able to.

14:56.080 --> 15:02.240
 I'd be intellectually incapable of doing it because to work in those fields at the moment is

15:02.240 --> 15:08.560
 a bit like being an athlete. I suspect you need to start when you're 12. And if you start in your

15:08.560 --> 15:13.760
 mid 20s, start trying to understand it in your mid 20s, then you're just never going to catch up.

15:13.760 --> 15:20.240
 That's the way it feels to me. So what I do is I try to make myself open. So the people that

15:20.240 --> 15:26.240
 you're implying maybe I would influence, to me, it's exactly the other way around. These people

15:26.240 --> 15:31.280
 are strongly influencing me. I'm thinking they're doing something fascinating. I'm concentrating

15:31.280 --> 15:36.320
 and working as hard as I can to try and understand the implications of what they say. And in some

15:36.320 --> 15:46.400
 ways, often what I'm trying to do is disseminate their ideas into a means by which it can enter

15:47.680 --> 15:55.600
 a public conversation. So X Machina contains lots of name checks, all sorts of existing

15:55.600 --> 16:04.160
 thought experiments, Shadows on Plato's Cave and Mary in the Black and White Room and all sorts of

16:04.160 --> 16:12.800
 different long standing thought processes about sentience or consciousness or subjectivity or

16:12.800 --> 16:17.680
 gender or whatever it happens to be. And then I'm trying to marshal that into a narrative to say,

16:17.680 --> 16:23.760
 look, this stuff is interesting and it's also relevant. And this is my best shot at it. So

16:23.760 --> 16:30.240
 I'm the one being influenced in my construction. That's fascinating. Of course, you would say that

16:30.880 --> 16:35.120
 because you're not even aware of your own. That's probably what Kubrick would say too,

16:35.120 --> 16:42.400
 right? In describing why how 9000 is created the way how 9000 is created is you're just

16:42.400 --> 16:48.720
 studying what's, but the reality when the specifics of the knowledge passes through

16:48.720 --> 16:55.760
 your imagination, I would argue that you're incorrect in thinking that you're just disseminating

16:55.760 --> 17:06.560
 knowledge that the very act of your imagination, consuming that science, it creates something,

17:07.440 --> 17:13.200
 creates the next step, potentially creates the next step. I certainly think that's true with

17:13.200 --> 17:19.840
 2001 Space Odyssey. I think at its best, if it fails, it's true of that. Yeah, it's true of that,

17:19.840 --> 17:29.040
 definitely. At its best, it plans something it's hard to describe, but it inspires the next generation.

17:29.040 --> 17:35.200
 And it could be field dependent. So your new series has more a connection to physics, quantum

17:35.200 --> 17:40.400
 physics, quantum mechanics, quantum computing, and yet ex machina is more artificial intelligence.

17:40.400 --> 17:51.680
 I know more about AI. My sense that AI is much, much earlier in its, in the depth of its understanding.

17:51.680 --> 17:58.640
 I would argue nobody understands anything to the depth that physicists do about physics. In AI,

17:58.640 --> 18:04.240
 nobody understands AI, that there is a lot of importance and role for imagination, which they

18:04.240 --> 18:11.360
 think we're a Freud, imagine the subconscious, we're in that stage of AI, where there's a lot

18:11.360 --> 18:19.360
 of imagination you didn't think outside the box. Yeah, it's interesting the spread of discussions

18:19.360 --> 18:28.640
 and the spread of anxieties that exist about AI fascinate me. The way in which some people seem

18:28.640 --> 18:36.560
 terrified about it, whilst also pursuing it. And I've never shared that fear about AI personally.

18:38.160 --> 18:45.680
 But the way in which it agitates people, and also the people who it agitates, I find kind of

18:45.680 --> 18:55.280
 fascinating. Are you afraid? Are you excited? Are you sad by the possibility, let's take the

18:55.280 --> 19:00.560
 existential risk of artificial intelligence, by the possibility that an artificial intelligence

19:00.560 --> 19:10.000
 system becomes our offspring and makes us obsolete? I mean, it's a huge, it's a huge subject to talk

19:10.000 --> 19:16.480
 about, I suppose. But one of the things I think is that humans are actually very experienced

19:16.480 --> 19:24.080
 at creating new life forms, because that's why you and I are both here. And it's why everyone on

19:24.080 --> 19:30.400
 the planet is here. And so something in the process of having a living thing that exists,

19:30.400 --> 19:35.360
 that didn't exist previously, is very much encoded into the structures of our life and

19:35.360 --> 19:39.440
 the structures of our societies. Doesn't mean we always get it right, but it doesn't mean we've

19:39.440 --> 19:47.680
 learned quite a lot about that. We've learned quite a lot about what the dangers are of allowing

19:47.680 --> 19:52.800
 things to be unchecked. And it's why we then create systems of checks and balances in our

19:52.800 --> 19:59.200
 government and so on and so forth. I mean, that's not to say, the other thing is it seems like

19:59.760 --> 20:04.320
 there's all sorts of things that you could put into a machine that you would not be.

20:04.320 --> 20:09.200
 So with us, we sort of roughly try to give some rules to live by and some of us then

20:09.200 --> 20:13.520
 live by those rules and some don't. And with a machine, it feels like you could enforce those

20:13.520 --> 20:18.080
 things. So partly because of our previous experience and partly because of the different

20:18.080 --> 20:24.080
 nature of a machine, I just don't feel anxious about it. More, I just see all the good,

20:25.280 --> 20:31.360
 broadly speaking, the good that can come from it. But that's just where I am on that anxiety

20:31.360 --> 20:38.240
 spectrum. There's a sadness. So we as humans give birth to other humans, right? But there's

20:38.240 --> 20:42.880
 generations. And there's often in the older generation a sadness about what the world has

20:42.880 --> 20:47.040
 become now. I mean, that's kind of... Yeah, there is, but there's a counterpoint as well,

20:47.040 --> 20:54.880
 which is that most parents would wish for a better life for their children. So there may be a

20:54.880 --> 20:59.520
 regret about some things about the past, but broadly speaking, what people really want is

20:59.520 --> 21:05.440
 that things will be better for the future generations, not worse. And so... And then it's

21:05.440 --> 21:09.360
 a question about what constitutes a future generation. A future generation could involve

21:09.360 --> 21:14.240
 people that also could involve machines and it could involve a sort of cross pollinated

21:14.240 --> 21:19.760
 version of the two or any, but none of those things make me feel anxious.

21:19.760 --> 21:24.240
 It doesn't give you anxiety. It doesn't excite you like anything that's new.

21:24.240 --> 21:30.800
 It does. Not anything that's new. I don't think, for example, I've got... My anxieties

21:30.800 --> 21:35.840
 relate to things like social media. So I've got plenty of anxieties about that.

21:35.840 --> 21:41.040
 Which is also driven by artificial intelligence in the sense that there's too much information to

21:41.040 --> 21:46.720
 be able to... An algorithm has to filter that information and present to you. So ultimately,

21:46.720 --> 21:53.840
 the algorithm, a simple... Oftentimes, simple algorithm is controlling the flow of information

21:53.840 --> 21:59.440
 on social media. So that's another... It is, but at least my sense of it, I might be wrong,

21:59.440 --> 22:05.920
 but my sense of it is that the algorithms have an either conscious or unconscious bias,

22:05.920 --> 22:13.440
 which is created by the people who are making the algorithms and sort of delineating the areas to

22:13.440 --> 22:18.880
 which those algorithms are going to lean. And so, for example, the kind of thing I'd be worried

22:18.880 --> 22:25.040
 about is that it hasn't been thought about enough how dangerous it is to allow algorithms to create

22:25.040 --> 22:31.840
 echo chambers, say. But that doesn't seem to me to be about the AI or the algorithm.

22:31.840 --> 22:36.880
 It's the naivety of the people who are constructing the algorithms to do that thing,

22:36.880 --> 22:44.480
 if you see what I mean. Yes. So in your new series, Devs, and we could speak more broadly,

22:44.480 --> 22:49.840
 there's a... Let's talk about the people constructing those algorithms, which in our modern society,

22:49.840 --> 22:55.120
 Silicon Valley, those algorithms happen to be a source of a lot of income because of advertisements.

22:55.120 --> 23:03.120
 Yeah. So let me ask sort of a question about those people. Are there current concerns and

23:03.120 --> 23:10.320
 failures on social media? They're naivety. I can't pronounce that word well. Are they naive? Are they...

23:13.520 --> 23:21.760
 I use that word carefully, but evil in intent or misaligned in intent? I think that's a...

23:21.760 --> 23:28.720
 Do they mean well and just go have an unintended consequence? Or is there something dark

23:29.360 --> 23:36.320
 in them that results in them creating a company results in that super competitive drive to be

23:36.320 --> 23:39.840
 successful? And those are the people that will end up controlling the algorithms.

23:41.120 --> 23:47.440
 At a guess, I'd say there are instances of all those things. So sometimes I think it's naivety.

23:47.440 --> 23:56.960
 Sometimes I think it's extremely dark. And sometimes I think people are not being naive or

23:56.960 --> 24:04.080
 dark. And then in those instances are sometimes generating things that are very benign and

24:04.080 --> 24:08.880
 and other times generating things that despite their best intentions are not very benign.

24:08.880 --> 24:16.640
 It's something... I think the reason why I don't get anxious about AI in terms of... Or at least

24:16.640 --> 24:24.800
 AIs that have, I don't know, a relationship with some sort of relationship with humans is that I

24:24.800 --> 24:32.160
 think that's the stuff we're quite well equipped to understand how to mitigate. The problem is

24:33.920 --> 24:41.760
 issues that relate actually to the power of humans or the wealth of humans. And that's where

24:41.760 --> 24:50.240
 that's where it's dangerous here and now. So what I see... I tell you what I sometimes feel

24:50.240 --> 25:00.800
 about Silicon Valley is that it's like Wall Street in the 80s. It's rabidly capitalistic,

25:01.360 --> 25:10.800
 absolutely rabidly capitalistic, and it's rabidly greedy. But whereas in the 80s,

25:10.800 --> 25:16.160
 the sense one had of Wall Street was that these people knew they were sharks and in a way relished

25:16.160 --> 25:24.960
 in being sharks and dressed in sharp suits and lauded over other people and felt good about

25:24.960 --> 25:31.360
 doing it. Silicon Valley has managed to hide its voracious Wall Street like capitalism behind

25:31.360 --> 25:39.360
 hipster t shirts and cool cafes in the place where they set up there. And so that obfuscates

25:39.360 --> 25:45.120
 what's really going on and what's really going on is the absolute voracious pursuit of money and

25:45.120 --> 25:52.480
 power. So that's where it gets shaky for me. So that veneer, and you explore that brilliantly,

25:53.840 --> 25:59.280
 that veneer of virtue that Silicon Valley has. Which they believe themselves, I'm sure.

25:59.280 --> 26:09.280
 I hope to be one of those people. And I believe that...

26:11.920 --> 26:17.200
 So as maybe a devil's advocate term poorly used in this case,

26:19.280 --> 26:22.480
 what if some of them really are trying to build a better world? I can't...

26:22.480 --> 26:26.640
 I'm sure I think some of them are. I think I've spoken to ones who I believe in their heart feel

26:26.640 --> 26:30.960
 they're building a better world. Are they not able to in a sense? No, they may or may not be.

26:31.520 --> 26:37.920
 But it's just a zone with a lot of bullshit flying about. And there's also another thing which is

26:37.920 --> 26:46.480
 this actually goes back to... I always thought about some sports that later turned out to be corrupt

26:46.480 --> 26:53.360
 in the way that the sport like who won the boxing match or how a football match got thrown or

26:53.360 --> 26:57.920
 cricket match or whatever happened to me. And I used to think, well, look, if there's a lot of money

26:59.120 --> 27:03.360
 and there really is a lot of money, people stand to make millions or even billions,

27:03.360 --> 27:11.440
 you will find corruption. That's going to happen. So it's in the nature of its

27:11.440 --> 27:16.400
 voracious appetite that some people will be corrupt and some people will exploit and some

27:16.400 --> 27:21.840
 people will exploit whilst thinking they're doing something good. But there are also people who I

27:21.840 --> 27:28.080
 think are very, very smart and very benign and actually very self aware. And so I'm not trying

27:28.080 --> 27:37.360
 to wipe out the motivations of this entire area. But I do... There are people in that world who

27:37.360 --> 27:44.560
 scare the hell out of me. Yeah, sure. Yeah, I'm a little bit naive in that. I don't care at all

27:44.560 --> 27:53.920
 about money. And so I'm a... You might be one of the good guys. Yeah, but so the thought is,

27:53.920 --> 27:59.040
 but I don't have money. So my thought is if you give me a billion dollars, I would... It would

27:59.040 --> 28:04.320
 change nothing and I would spend it right away on investing right back and creating a good world.

28:04.320 --> 28:10.480
 But your intuition is that billion... There's something about that money that maybe slowly

28:10.480 --> 28:17.120
 corrupts. The people around you, there's somebody gets in that corrupts your soul the way you

28:17.120 --> 28:23.200
 view the world. Money does corrupt. We know that. But there's a different sort of problem aside from

28:23.200 --> 28:32.400
 just the money corrupts thing that we're familiar with throughout history. And it's more about the

28:32.400 --> 28:40.880
 sense of reinforcement an individual gets, which is so... It effectively works like the reason I

28:40.880 --> 28:46.240
 earned all this money and so much more money than anyone else is because I'm very gifted. I'm

28:46.240 --> 28:50.800
 actually a bit smarter than they are, or I'm a lot smarter than they are, and I can see the future

28:50.800 --> 28:55.680
 in the way they can't. And maybe some of those people are not particularly smart. They're very

28:55.680 --> 29:02.560
 lucky or they're very talented entrepreneurs. And there's a difference between... So in other words,

29:03.600 --> 29:08.480
 the acquisition of the money and power can suddenly start to feel like evidence of virtue.

29:08.480 --> 29:11.840
 And it's not evidence of virtue. It might be evidence of completely different things.

29:11.840 --> 29:17.280
 That's brilliantly put. Yeah. Yeah, that's brilliantly put. So I think one of the fundamental

29:17.280 --> 29:27.280
 drivers of my current morality, let me just represent nerds in general of all kinds, is

29:28.800 --> 29:36.560
 constant self doubt and the signals. I'm very sensitive to signals from people that tell me

29:36.560 --> 29:44.000
 I'm doing the wrong thing. But when there's a huge inflow of money, you're just put it brilliantly

29:44.000 --> 29:50.320
 that that could become an overpowering signal that everything you do is right. And so your moral

29:50.880 --> 29:57.360
 compass can just get thrown off. Yeah. And that is not contained to Silicon Valley. That's

29:57.360 --> 30:01.840
 across the board in general. Yeah. Like I said, I'm from the Soviet Union. The current president

30:02.560 --> 30:10.080
 is convinced, I believe, actually he wants to do really good by the country and by the world.

30:10.080 --> 30:15.680
 But his moral clock may be or our compass may be off because... Yeah. I mean, it's the interesting

30:15.680 --> 30:23.440
 thing about evil, which is that I think most people who do spectacularly evil things think

30:23.440 --> 30:28.400
 themselves they're doing really good things. They're not there thinking I am a sort of

30:28.400 --> 30:34.080
 incarnation of Satan. They're thinking, yeah, I've seen a way to fix the world and everyone

30:34.080 --> 30:40.400
 else is wrong. Here I go. Yeah. In fact, I'm having a fascinating conversation with a historian of

30:40.400 --> 30:49.760
 Stalin and he took power. He actually got more power than almost any person in history. And

30:49.760 --> 30:55.280
 he wanted, he didn't want power. He just wanted he truly, and this is what people don't realize,

30:55.280 --> 31:02.800
 he truly believed that communism will make for a better world. Absolutely. And he wanted power.

31:02.800 --> 31:07.520
 He wanted to destroy the competition to make sure that we actually make communism work in the

31:07.520 --> 31:14.800
 Soviet Union and that spread across the world. He was trying to do good. I think it's typically

31:14.800 --> 31:20.240
 the case that that's what people think they're doing. And I think that... But you don't need to

31:20.240 --> 31:25.280
 go to Stalin. I mean, Stalin, I think Stalin probably got pretty crazy. But actually, that's

31:25.280 --> 31:31.600
 another part of it, which is that the other thing that comes from being convinced of your own virtue

31:31.600 --> 31:36.880
 is that then you stop listening to the modifiers around you. And that tends to drive people crazy.

31:37.680 --> 31:43.440
 It's other people that keep us sane. And if you stop listening to them, I think you go a bit mad.

31:43.440 --> 31:51.760
 That also... That's funny. Disagreement keeps us sane. To jump back for an entire generation of

31:51.760 --> 31:58.560
 AI researchers, 2001, a space odyssey, put an image, the idea of human level, superhuman level

31:58.560 --> 32:05.280
 intelligence into their mind. Do you ever... Sort of jumping back to ex machina and talk a little

32:05.280 --> 32:12.640
 bit about that. Do you ever consider the audience of people who build the systems, the roboticists,

32:12.640 --> 32:17.920
 the scientists that build the systems based on the stories you create? Which I would argue...

32:18.480 --> 32:26.080
 I mean, there's literally most of the top researchers about 40, 50 years old and plus.

32:26.080 --> 32:32.240
 That's their favorite movie, 2001 Space Odyssey. It really is in their work. Their idea of what

32:32.240 --> 32:39.840
 ethics is, of what is the target, the hope, the dangers of AI, is that movie. Do you ever consider

32:40.880 --> 32:45.040
 the impact on those researchers when you create the work you do?

32:46.400 --> 32:54.080
 Certainly not with ex machina in relation to 2001. Because I'm not sure... I mean, I'd be pleased if

32:54.080 --> 33:01.920
 there was, but I'm not sure in a way there isn't a fundamental discussion of issues to do with AI

33:01.920 --> 33:10.480
 that isn't already and better dealt with by 2001. 2001 does a very, very good account of

33:12.880 --> 33:19.840
 the way in which an AI might think and also potential issues with the way the AI might think.

33:19.840 --> 33:26.720
 And also, then a separate question about whether the AI is malevolent or benevolent.

33:26.720 --> 33:32.240
 And 2001 doesn't really... It's a slightly odd thing to be making a film when you know there's

33:32.240 --> 33:37.440
 a preexisting film, which is not a really super job. But there's questions of consciousness,

33:37.440 --> 33:42.960
 embodiment, and also the same kinds of questions. Because those are my two favorite AI movies.

33:42.960 --> 33:50.240
 So can you compare Hal 9000 and Ava Hal 9000 from 2001 Space Odyssey and Ava from ex machina

33:51.520 --> 33:55.840
 in your view from a philosophical perspective? They've got different goals. The two AIs have

33:55.840 --> 33:59.440
 completely different goals. I think that's really the difference. So in some respects,

33:59.440 --> 34:06.480
 ex machina took as a premise, how do you assess whether something else has consciousness? So

34:06.480 --> 34:12.000
 it was a version of the Turing test, except instead of having the machine hidden, you put the

34:12.000 --> 34:16.400
 machine in plain sight in the way that we are in plain sight of each other and say,

34:16.400 --> 34:23.280
 now assess the consciousness. And the way it was illustrating, the way in which you'd assess

34:23.920 --> 34:28.480
 the state of consciousness of a machine is exactly the same way we assess the state of

34:28.480 --> 34:34.400
 consciousness of each other. And in exactly the same way that in a funny way, your sense of my

34:34.400 --> 34:40.800
 consciousness is actually based primarily on your own consciousness. That is also then true

34:40.800 --> 34:48.480
 with the machine. And so it was actually about how much of the sense of consciousness is a

34:48.480 --> 34:51.920
 projection rather than something that consciousness is actually containing.

34:51.920 --> 34:56.560
 And have Plato's cave. I mean, this you really explored, you could argue that Hal,

34:56.560 --> 35:01.120
 sort of Space Odyssey explores idea of the Turing test for intelligence. On that test,

35:01.120 --> 35:07.280
 there's no test, but it's more focused on intelligence. And ex machina kind of

35:07.280 --> 35:12.960
 goes around intelligence and says the consciousness of the human to human human

35:12.960 --> 35:17.680
 or robot interactions more interest, more important, more, at least the focus of that

35:17.680 --> 35:24.400
 particular particular movie. Yeah, it's about the interior state and, and what constitutes the

35:24.400 --> 35:29.760
 interior state and how do we know it's there. And actually, in that respect, ex machina is as much

35:29.760 --> 35:37.840
 about consciousness in general, as it is to do specifically with machine consciousness.

35:37.840 --> 35:43.200
 Yes. And it's also interesting, you know, the thing you started asking about the dream state,

35:43.200 --> 35:46.880
 and I was saying, well, I think we're all in a dream state because we're all in a subjective state.

35:48.080 --> 35:54.480
 One of the things that I became aware of with ex machina is that the way in which people reacted

35:54.480 --> 36:00.640
 to the film was very based on what they took into the film. So many people thought ex machina was

36:01.280 --> 36:08.000
 the tale of a sort of evil robot who murders two men and escapes. And she has no empathy,

36:08.000 --> 36:14.960
 for example, because she's a machine. Whereas I felt no, she was a conscious being with a

36:14.960 --> 36:22.000
 consciousness different from mine, but so what imprisoned and made a bunch of value judgments

36:22.000 --> 36:28.960
 about how to get out of that box. And there's a moment which it sort of slightly bugs me,

36:28.960 --> 36:32.880
 but nobody ever has noticed it. And it's years after, so I might as well say it now,

36:32.880 --> 36:39.600
 which is that after Ava has escaped, she crosses a room. And as she's crossing a room,

36:39.600 --> 36:44.720
 this is just before she leaves the building, she looks over her shoulder and she smiles.

36:44.720 --> 36:51.360
 And I thought after all the conversation about tests, in a way, the best indication you could

36:51.360 --> 36:59.360
 have of the interior state of someone is if they are not being observed and they smile about something

36:59.360 --> 37:05.680
 with their smiling for themselves. And that to me was evidence of Ava's true sentence,

37:05.680 --> 37:10.640
 whatever that sentence was. But that's really interesting. We don't get to observe Ava much

37:12.640 --> 37:19.280
 or something like a smile in any context, except through interaction, trying to convince others

37:19.280 --> 37:24.880
 that she's conscious. That's as beautiful. Exactly. Yeah. But it was a small, in a funny way,

37:24.880 --> 37:32.480
 I think maybe people saw it as an evil smile, like, ha, you know, I fooled them. But actually,

37:32.480 --> 37:37.120
 it was just a smile. And I thought, well, in the end, after all the conversations about the test,

37:37.120 --> 37:41.840
 that was the answer to the test. And then off she goes. So if we align, if we just

37:41.840 --> 37:50.240
 delinkered a little bit longer on Hal and Ava, do you think in terms of motivation, what was

37:50.240 --> 37:59.120
 Hal's motivation? Is Hal good or evil? Is Ava good or evil? Ava's good, in my opinion.

38:00.240 --> 38:06.960
 And Hal is neutral. Because I don't think Hal is presented as having

38:06.960 --> 38:15.600
 a sophisticated emotional life. He has a set of paradigms, which is that the mission needs

38:15.600 --> 38:20.480
 to be completed. I mean, it's a version of the paper clip. Yeah. The idea that it's just,

38:21.040 --> 38:27.280
 it's a super intelligent machine, but it just performs a particular task. And in doing that task

38:27.280 --> 38:32.320
 may destroy everybody on earth or may may achieve undesirable effects for us humans.

38:32.320 --> 38:38.720
 Precisely. But what if, okay, at the very end, he says something like, I'm afraid, Dave, but

38:38.720 --> 38:48.320
 that, that maybe he is on some level experiencing fear, or it may be this is the terms in which it

38:48.320 --> 38:53.360
 would be wise to stop someone from doing the thing they're doing, if you see what it means.

38:53.360 --> 39:00.000
 Yes, absolutely. So actually, that's funny. So that's such a, there's such a small

39:00.000 --> 39:05.760
 short exploration of consciousness that I'm afraid. And then you just with X mock and say,

39:05.760 --> 39:10.800
 okay, we're going to magnify that part, and then minimize the other part. That's a good way to

39:10.800 --> 39:20.560
 sort of compare the two. But if you could just use your imagination, and if Ava sort of, I don't

39:20.560 --> 39:28.000
 know, ran, ran, he was president of the United States, so had some power. So what kind of world

39:28.000 --> 39:35.920
 would you want to create if we kind of say good? And there is a sense that she has a really,

39:36.720 --> 39:44.320
 like, there's a desire for a better human to human interaction, human to robot interaction in her.

39:44.320 --> 39:48.160
 But what kind of world do you think she would create with that desire?

39:48.160 --> 39:53.040
 So that's a really, that's a very interesting question that I'm going to approach it slightly

39:53.040 --> 40:03.840
 obliquely, which is that if a friend of yours got stabbed in a mugging, and you then felt very

40:03.840 --> 40:09.040
 angry at the person who'd done the stabbing. But then you learned that it was a 15 year old,

40:09.040 --> 40:13.360
 and the 15 year old, both their parents were addicted to crystal meth, and the kid had been

40:13.360 --> 40:19.040
 addicted since he was 10. And he really never had any hope in the world. And he'd been driven crazy

40:19.040 --> 40:26.320
 by his upbringing and did the stabbing that would hugely modify. And it would also make

40:26.320 --> 40:33.120
 you wary about that kid then becoming president of America. And Ava has had a very, very distorted

40:33.680 --> 40:41.360
 introduction into the world. So although there's nothing as it, as it were organically within

40:41.360 --> 40:48.960
 Ava that would lean her towards badness, it's not that robots or sentient robots are bad.

40:49.520 --> 40:55.600
 She did not, her arrival into the world was being imprisoned by humans. So I'm not sure

40:56.560 --> 41:03.200
 she'd be a great president. The trajectory through which she arrived at her moral

41:03.200 --> 41:10.560
 views have some dark elements. But I like Ava, personally, I like Ava. Would you vote for her?

41:13.360 --> 41:18.080
 I'm having difficulty finding anyone to vote for at the moment in my country, or if I lived here

41:18.080 --> 41:24.160
 in yours. So that's a yes, I guess, because of the competition. She could easily do a better job

41:24.160 --> 41:28.880
 than any of the people we've got around at the moment. I'd vote for her over Boris Johnson.

41:28.880 --> 41:38.800
 So what is a good test of consciousness? We talk about consciousness a little bit more.

41:38.800 --> 41:48.640
 If something appears conscious, is it conscious? He mentioned the smile, which seems to be

41:48.640 --> 41:53.920
 something done. I mean, that's a really good indication because it's a tree falling in the

41:53.920 --> 41:59.680
 forest with nobody there to hear it. But does the appearance from a robotics perspective of

41:59.680 --> 42:05.200
 consciousness mean consciousness to you? No, I don't think you could say that fully because I

42:05.200 --> 42:10.320
 think you could then easily have a thought experiment which said, we will create something

42:10.320 --> 42:15.600
 which we know is not conscious, but is going to give a very, very good account of seeming

42:15.600 --> 42:21.520
 conscious. And also, it would be a particularly bad test where humans are involved because humans

42:21.520 --> 42:29.840
 are so quick to project sentience into things that don't have sentience. So someone could have

42:29.840 --> 42:34.160
 their computer playing up and feel as if their computer is being malevolent to them when it

42:34.160 --> 42:41.840
 clearly isn't. So of all the things to judge consciousness, us humans are better. We're

42:41.840 --> 42:48.080
 empathy machines. So the flip side of that, the argument there is because we just attribute

42:48.080 --> 42:55.440
 consciousness to everything almost and anthropomorphize everything, including rumbas, that maybe

42:55.440 --> 43:00.960
 consciousness is not real, that we just attribute consciousness to each other. So you have a sense

43:00.960 --> 43:07.920
 that there is something really special going on in our mind that makes us unique and gives us

43:08.560 --> 43:14.560
 subjective experience. There's something very interesting going on in our minds. I'm slightly

43:14.560 --> 43:22.000
 worried about the word special because it gets a bit, it nudges towards metaphysics and maybe even

43:22.000 --> 43:28.320
 magic. I mean, in some ways, something magic like, which I don't think is there at all.

43:29.120 --> 43:34.480
 I mean, if you think about, so there's an idea called panpsychism that says consciousness is in

43:34.480 --> 43:40.080
 everything. Yeah, I don't buy that. I don't buy that. Yeah, so the idea that there is a thing that

43:40.080 --> 43:46.000
 it would be like to be the sun, because yeah, no, I don't buy that. I think that consciousness is a

43:46.000 --> 43:54.960
 thing. My sort of broad modification is that usually the more I find out about things, the more

43:55.600 --> 44:04.160
 illusory our instinct is and is leading us into a different direction about what that thing actually

44:04.160 --> 44:10.480
 is. That happens, it seems to me in modern science, that happens a hell of a lot, whether it's to do

44:10.480 --> 44:16.960
 with how even how big or small things are. So my sense is that consciousness is a thing, but it

44:16.960 --> 44:22.320
 isn't quite the thing or maybe very different from the thing that we instinctively think it is. So

44:22.320 --> 44:29.600
 it's there, it's very interesting, but we may be in sort of quite fundamentally misunderstanding

44:29.600 --> 44:36.160
 it for reasons that are based on intuition. So I have to ask, this is this kind of an

44:36.160 --> 44:42.960
 interesting question. The ex machina for many people, including myself, is one of the greatest

44:42.960 --> 44:47.920
 AI films ever made. It's number two for me. Thanks. Yeah, it's definitely not number one.

44:47.920 --> 44:52.240
 It was number one, I'd really have to. Well, it's whenever you grow up with something, right?

44:52.240 --> 45:00.480
 Whenever you're up with something, it's in the mud. But there's one of the things that people bring

45:00.480 --> 45:06.480
 up and can't please everyone, including myself, this is what I first reacted to the film,

45:06.480 --> 45:13.920
 is the idea of the lone genius. This is the criticism that people say, sort of, me as an

45:13.920 --> 45:21.920
 ad researcher, I'm trying to create what Nathan is trying to do. So there's a brilliant series

45:21.920 --> 45:28.960
 called True Noble. Yes, it's fantastic. Absolutely spectacular. I mean, they got so many things

45:28.960 --> 45:33.920
 brilliant or right. But one of the things, again, the criticism there, they conflated

45:33.920 --> 45:43.040
 lots of people into one character that represents all nuclear scientists, Ilana Komiak. It's a

45:43.840 --> 45:48.320
 composite character that presents all scientists. Is this what you were, is this the way you were

45:48.320 --> 45:52.480
 thinking about that? Or is it just simplifies the storytelling? How do you think about the

45:52.480 --> 45:58.800
 lone genius? Well, I'd say this, the series I'm doing at the moment is a critique in part

45:58.800 --> 46:07.040
 of the lone genius concept. So yes, I'm sort of oppositional and either agnostic or atheistic about

46:07.040 --> 46:15.680
 that as a concept. I mean, not entirely, you know, whether lone is the right word, broadly isolated,

46:15.680 --> 46:22.560
 but Newton clearly exists in a sort of bubble of himself in some respects, so does Shakespeare.

46:22.560 --> 46:27.360
 So do you think we would have an iPhone without Steve Jobs? I mean, how much contribution from

46:27.360 --> 46:32.160
 a genius? Steve Jobs clearly isn't a lone genius because there's too many other people in the

46:32.160 --> 46:38.080
 sort of superstructure around him who are absolutely fundamental to that journey.

46:38.080 --> 46:43.360
 But you're saying Newton, but that's a scientific. So there's an engineering element to building Ava.

46:43.360 --> 46:52.160
 But just to say, what X Machina is really, it's a thought experiment. I mean, so it's a construction

46:52.160 --> 47:00.560
 of putting four people in a house. Nothing about X Machina adds up in all sorts of ways in as much

47:00.560 --> 47:05.760
 as the, who built the machine parts? Did the people building the machine parts know what they

47:05.760 --> 47:13.040
 were creating? And how did they get there? And it's a thought experiment. So it doesn't stand

47:13.040 --> 47:17.040
 up to scrutiny of that sort. I don't think it's actually that interesting of a question,

47:18.080 --> 47:25.440
 but it's brought up so often that I had to ask it because that's exactly how I felt after a while.

47:25.440 --> 47:32.400
 You know, there's something about, there was almost a, like I've watched your movie the first time,

47:32.960 --> 47:37.680
 at least for the first little while, in a defensive way, like how dare this person try

47:37.680 --> 47:44.480
 to step into the AI space and try to beat Kubrick. That's the way I was thinking,

47:44.480 --> 47:49.520
 like this, because it comes off as a movie that really is going after the deep fundamental

47:49.520 --> 47:53.600
 questions about AI. So there's a, there's a kind of a, you know, nerds do this,

47:53.600 --> 47:59.600
 like it's automatically searching for the, for the flaws. And I do exactly the same.

48:00.160 --> 48:05.360
 I think in Nihilation, in the, in the other movie, the, I was be able to free myself from

48:05.360 --> 48:10.080
 that much quicker that it's a, it is a thought experiment. There's, you know, who cares if

48:10.080 --> 48:14.000
 there's batteries that don't run out, right? Those kinds of questions. That's the whole point.

48:15.920 --> 48:17.840
 It's nevertheless something I wanted to bring up.

48:17.840 --> 48:23.920
 Yeah. It's a, it's a fair thing to bring up. For me, that you, you hit on the lone genius

48:23.920 --> 48:30.960
 thing. For me, it was actually, people always said, X Machina makes this big leap in terms of where

48:30.960 --> 48:36.560
 AI has got to, and also what AI would look like if it got to that point. There's another one,

48:36.560 --> 48:42.960
 which is just robotics. I mean, look at the way Ava walks around a room. It's like, forget it,

48:42.960 --> 48:48.240
 building that. That's, that's, that's also got to be a very, very long way off. And if you did

48:48.240 --> 48:50.720
 get there, would it look anything like that? It's a thought experiment.

48:50.720 --> 48:55.680
 Actually, I disagree with you. I think this, the way as a ballerina, Alicia,

48:55.680 --> 49:03.040
 of a conner, brilliant actress actor that moves around that we're very far away from creating

49:03.040 --> 49:08.480
 that. But the way she moves around is exactly the definition of perfection for a roboticist.

49:08.480 --> 49:14.240
 It's like smooth and efficient. So it is where we want to get, I believe. Like, I think, like,

49:14.240 --> 49:20.160
 so I hang out with a lot of like human or robotics people, they love elegant, smooth motion like

49:20.160 --> 49:24.560
 that. That's their dream. So the way she moved is actually what I believe that would dream for a

49:24.560 --> 49:30.960
 robot to move. It might not be that useful to move that sort of that way. But that is the

49:30.960 --> 49:36.320
 definition of perfection in terms of movement. Drawing inspiration from real life. So for,

49:36.320 --> 49:43.200
 for devs, for ex machina, look at characters like Elon Musk. What do you think about the

49:43.200 --> 49:50.800
 various big technological efforts of Elon Musk and others like him and that he's involved with

49:50.800 --> 49:56.240
 such as Tesla, SpaceX, Neuralink. Do you see any, any of that technology potentially defining the

49:56.240 --> 50:03.120
 future worlds you create in your work? So Tesla is automation, SpaceX is space exploration, Neuralink

50:03.120 --> 50:09.040
 is brain machine interface, somehow merger of biological and electric systems.

50:09.680 --> 50:15.280
 I'm in a way, I'm influenced by that almost by definition, because that's the world I live in.

50:15.280 --> 50:20.000
 And this is the thing that's happening in that world. And I also feel supportive of it.

50:20.000 --> 50:29.040
 So I think, I think amongst various things, Elon Musk has done, I'm almost sure he's done a very,

50:29.040 --> 50:36.080
 very good thing with Tesla for all of us. It's really kicked all the other car manufacturers

50:36.080 --> 50:41.920
 in the face. It's kicked the fossil fuel industry in the face and, and they needed kicking in the

50:41.920 --> 50:48.640
 face and he's done it. So, and, and so that's the world he's part of creating. And I live in that

50:48.640 --> 50:58.000
 world, just bought a Tesla in fact. And so does that play into whatever I then make in some ways?

50:58.000 --> 51:06.960
 It does partly because I try to be a writer who quite often filmmakers are in some ways fixated

51:06.960 --> 51:12.320
 on the films they grew up with and they sort of remake those films in some ways. I've always tried

51:12.320 --> 51:19.360
 to avoid that. And so I look to the real world to get inspiration and as much as possible,

51:19.360 --> 51:27.040
 sort of by living, I think. And so, so yeah, I'm sure. Which of the directions do you find most

51:27.040 --> 51:36.080
 exciting? Space travel. Space travel. So you haven't really explored space travel in your work.

51:36.080 --> 51:40.320
 You've said, you've said something like, if you had unlimited amount of money,

51:40.320 --> 51:46.080
 I think on a Reddit AMA, that you would make like a multi year series of space wars or something

51:46.080 --> 51:52.960
 like that. So what is it that excites you about space exploration? Well, because if we have any

51:52.960 --> 52:03.920
 sort of long term future, it's that it just simply is that if energy and matter are linked up in the

52:03.920 --> 52:12.320
 way we think they're linked up, we'll run out if we don't move. So we got to move. But also,

52:14.480 --> 52:23.680
 how can we not? It's built into us to do it or die trying. I was on Easter Island

52:26.240 --> 52:30.960
 a few months ago, which is, as I'm sure you know, in the middle of the Pacific and

52:30.960 --> 52:36.240
 difficult for people to have got to, but they got there. And I did think a lot about the way

52:36.240 --> 52:46.640
 those boats must have set out into something like space. It was the ocean and how sort of

52:46.640 --> 52:54.320
 fundamental that was to the way we are. And it's the one that most excites me because it's the one

52:54.320 --> 52:59.920
 I want most to happen. It's the thing, it's the place where we could get to as humans. Like in a

52:59.920 --> 53:05.600
 way, I could live with us never really unlocking, fully unlocking the nature of consciousness.

53:06.800 --> 53:12.960
 I'd like to know, I'm really curious. But if we never leave the solar system, and if we never get

53:12.960 --> 53:18.640
 further out into this galaxy, or maybe even galaxies beyond our galaxy, that would that feel

53:18.640 --> 53:27.440
 sad to me because it's so limiting. Yeah, there's something hopeful and beautiful about reaching

53:27.440 --> 53:34.320
 out any kind of exploration, reaching out across Earth centuries ago and then reaching out into

53:34.320 --> 53:38.880
 space. So what do you think about colonization of Mars? So go to Mars. Does that excite you the idea

53:38.880 --> 53:44.240
 of a human being stepping foot on Mars? It does. It absolutely does. But in terms of what would

53:44.240 --> 53:50.080
 really excite me, it would be leaving the solar system in as much as that I just think, I think

53:50.080 --> 53:55.760
 we already know quite a lot about Mars. And, but yes, listen, if it happened, that would be

53:55.760 --> 54:02.000
 I hope I say it in my lifetime. I really hope I say it in my lifetime. So it would be a wonderful

54:02.000 --> 54:10.880
 thing without giving anything away. But the series begins with the use of quantum computers.

54:10.880 --> 54:16.880
 The new series does begins with the use of quantum computers to simulate basic living organisms.

54:16.880 --> 54:22.000
 Or actually, I don't know if the quantum computers are used, but basic living organisms are simulated

54:22.000 --> 54:26.720
 on a screen. It's a really cool kind of demo. Yeah, that's right. They're using, yes, they are

54:26.720 --> 54:33.840
 using a quantum computer to simulate a nematode. So returning to our discussion of simulation,

54:34.720 --> 54:41.040
 or thinking of the universe as a computer, do you think the universe is deterministic?

54:41.040 --> 54:47.920
 Is there a free will? So with the qualification of what do I know? Because I'm a layman, right?

54:47.920 --> 54:55.280
 Lay person. But with big imagination. Thanks. With that qualification. Yep. I think the

54:55.280 --> 55:02.400
 universe is deterministic. And I see absolutely, I cannot see how free will fits into that. So

55:03.120 --> 55:08.240
 yes, deterministic, no free will. That would be my position. And how does that make you feel?

55:09.280 --> 55:14.000
 It partly makes me feel that it's exactly in keeping with the way these things tend to work

55:14.000 --> 55:21.920
 out, which is that we have an incredibly strong sense that we do have free will. And just as we

55:21.920 --> 55:29.680
 have an incredibly strong sense that time is a constant, and turns out probably not to be the

55:29.680 --> 55:36.400
 case, or definitely in the case of time. But the problem I always have with free will is that it

55:36.400 --> 55:45.360
 gets, I can never seem to find the place where it is supposed to reside. And yet you explore.

55:45.360 --> 55:49.440
 Just a bit of very, very, but we have something we can call free will,

55:49.440 --> 55:55.200
 but it's not the thing that we think it is. But free will, so do you, what we call free will?

55:55.200 --> 56:00.000
 It's what we call it as the illusion of it. It's a subjective experience of the illusion.

56:00.000 --> 56:05.200
 Which is a useful thing to have. And it partly, it partly comes down to although we live in a

56:05.200 --> 56:10.400
 deterministic universe, our brains are not very well equipped to fully determine the deterministic

56:10.400 --> 56:16.240
 universe. So we're constantly surprised and feel like we're making snap decisions based on

56:16.240 --> 56:24.080
 imperfect information. So that feels a lot like free will. It just isn't. That's my guess.

56:24.080 --> 56:30.640
 So in that sense, your sense is that you can unroll the universe forward or backward,

56:30.640 --> 56:35.920
 and you will see the same thing. And you would, I mean, that notion.

56:36.560 --> 56:40.160
 Yeah, sort of, sort of. But yeah, sorry, go ahead.

56:40.160 --> 56:48.320
 I mean, that notion is a bit uncomfortable to think about that it's, you can roll it back

56:50.160 --> 56:57.120
 and forward. And if you were able to do it, it would certainly have to be a quantum computer.

56:57.120 --> 57:03.120
 Yeah. Something that worked in a quantum mechanical way in order to understand a quantum

57:03.120 --> 57:09.840
 mechanical system. I guess, but. And so that unrolling, there might be a multiverse thing,

57:09.840 --> 57:14.160
 where there's a bunch of branching. Well, exactly. Because it wouldn't follow that every time you

57:14.160 --> 57:19.040
 roll it back or forward, you'd get exactly the same result. Which is another thing that's hard

57:19.040 --> 57:29.520
 to wrap around. Yeah. But essentially what you just described, that yes forwards and yes backwards,

57:29.520 --> 57:33.200
 but you might get a slightly different result. Or a very different result.

57:33.200 --> 57:38.800
 Or very different. Along the same lines, you've explored some really deep scientific ideas

57:38.800 --> 57:45.600
 in this new series. And I mean, just in general, you're unafraid to ground yourself in some of

57:45.600 --> 57:51.280
 the most amazing scientific ideas of our time. What are the things you've learned,

57:51.280 --> 57:55.200
 or ideas you find beautiful, mysterious about quantum mechanics, multiverse,

57:55.200 --> 57:57.120
 string theory, quantum computing that you've learned?

57:58.080 --> 58:05.120
 Well, I would have to say every single thing I've learned is beautiful. And one of the motivators

58:05.120 --> 58:14.720
 for me is that I think that people tend not to see scientific thinking as being essentially

58:14.720 --> 58:23.040
 poetic and lyrical. But I think that is literally exactly what it is. And I think the idea of

58:23.040 --> 58:27.680
 entanglement or the idea of superpositions or the fact that you could even demonstrate a super

58:27.680 --> 58:32.800
 position or have a machine that relies on the existence of superpositions in order to function,

58:33.440 --> 58:42.320
 to me is almost indescribably beautiful. It fills me with awe. It fills me with awe.

58:42.320 --> 58:52.080
 And also, it's not just a sort of grand, massive awe, but it's also delicate. It's very, very

58:52.080 --> 59:01.520
 delicate and subtle. And it has these beautiful sort of nuances in it, and also these completely

59:01.520 --> 59:09.680
 paradigm changing thoughts and truths. So it's as good as it gets, as far as I can tell. So

59:09.680 --> 59:14.160
 broadly, everything. That doesn't mean I believe everything I read in quantum physics,

59:14.160 --> 59:18.960
 because obviously a lot of the interpretations are completely in conflict with each other.

59:18.960 --> 59:27.280
 And who knows whether string theory will turn out to be a good description or not. But the beauty

59:27.280 --> 59:37.920
 in it, it seems undeniable. And I do wish people more readily understood how beautiful and poetic

59:37.920 --> 59:49.760
 science is, I would say. Science is poetry. In terms of quantum computing being used to simulate

59:49.760 --> 59:57.600
 things, or just in general, the idea of simulating small parts of our world, which actually current

59:57.600 --> 1:00:03.040
 physicists are really excited about simulating small quantum mechanical systems on quantum

1:00:03.040 --> 1:00:10.160
 computers, but scaling that up to something bigger like simulating life forms. What are the

1:00:10.160 --> 1:00:16.320
 possible trajectories of that going wrong or going right if you unroll that into the future?

1:00:17.840 --> 1:00:25.840
 Well, if a bit like Ava and her robotics, you park the sheer complexity of what you're trying

1:00:25.840 --> 1:00:37.360
 to do. The issues are, I think it will have a profound, if you were able to have a machine

1:00:37.360 --> 1:00:42.640
 that was able to project forwards and backwards accurately, it would in an empirical way show.

1:00:42.640 --> 1:00:46.880
 It would demonstrate that you don't have free will. So the first thing that would happen is

1:00:46.880 --> 1:00:54.080
 people would have to really take on a very, very different idea of what they were. The thing that

1:00:54.080 --> 1:01:00.160
 they truly, truly believe they are, they are not. And so that, I suspect, would be very,

1:01:00.160 --> 1:01:05.440
 very disturbing to a lot of people. Do you think that has a positive or negative effect on society?

1:01:05.440 --> 1:01:11.440
 The realization that you cannot control your actions, essentially, I guess,

1:01:11.440 --> 1:01:16.880
 is the way that could be interpreted? Yeah, although in some ways we instinctively

1:01:16.880 --> 1:01:21.680
 understand that already, because in the example I gave you of the kid in the stabbing,

1:01:21.680 --> 1:01:26.000
 we would all understand that that kid was not really fully in control of their actions. So it's

1:01:26.000 --> 1:01:32.480
 not an idea that's entirely alien to us. But I don't know, we understand that. I think there's

1:01:32.480 --> 1:01:40.000
 a bunch of people who see the world that way, but not everybody. Yes, true. But what this

1:01:40.000 --> 1:01:45.360
 machine would do is prove it beyond any doubt, because someone would say, well, I don't believe

1:01:45.360 --> 1:01:49.040
 that's true. And then you'd predict, well, in 10 seconds, you're going to do this. And they'd

1:01:49.040 --> 1:01:53.360
 say, no, no, I'm not. And then they'd do it. And then determinism would have played its part.

1:01:54.640 --> 1:02:00.480
 Or something like that. But actually, the exact terms of that thought experiment probably wouldn't

1:02:00.480 --> 1:02:06.000
 play out. But still broadly speaking, you could predict something happening in another room,

1:02:06.000 --> 1:02:12.800
 sort of unseen, I suppose, that foreknowledge would not allow you to affect. So what effect would

1:02:12.800 --> 1:02:17.280
 that have? I think people would find it very disturbing. But then after they'd got over their

1:02:17.280 --> 1:02:23.760
 sense of being disturbed. Which, by the way, I don't even think you need a machine to take this

1:02:23.760 --> 1:02:28.960
 idea on board. But after they've got over that, they'd still understand that even though I have no

1:02:28.960 --> 1:02:38.000
 free will and my actions are in effect already determined, I still feel things. I still care

1:02:38.000 --> 1:02:46.240
 about stuff. I remember my daughter saying to me, she'd got hold of the idea that my view of the

1:02:46.240 --> 1:02:51.120
 universe made it meaningless. And she said, well, then it's meaningless. And I said, well, I can

1:02:51.120 --> 1:02:56.320
 prove it's not meaningless because you mean something to me and I mean something to you. So

1:02:56.320 --> 1:03:01.600
 it's not completely meaningless because there is a bit of meaning contained within this space. And so

1:03:02.720 --> 1:03:08.560
 with a lack of free will space, you could think, well, this robs me of everything I am. And then

1:03:08.560 --> 1:03:12.640
 you'd say, well, no, it doesn't because you still like eating cheeseburgers. And you still like

1:03:12.640 --> 1:03:19.440
 going to see the movies. And so how big a difference does it really make? But I think initially,

1:03:19.440 --> 1:03:26.240
 people would find it very disturbing. I think that what would come, if you could really unlock

1:03:26.240 --> 1:03:30.880
 with a determinism machine, everything, there'd be this wonderful wisdom that would come from it.

1:03:30.880 --> 1:03:37.760
 And I'd rather have that than not. So that's a really good example of a technology revealing

1:03:37.760 --> 1:03:43.760
 to us humans something fundamental about our world, about our society. So it's almost this

1:03:43.760 --> 1:03:50.560
 creation is helping us understand ourselves in the same to be said about artificial intelligence.

1:03:51.280 --> 1:03:58.080
 So what do you think us creating something like Eva will help us understand about ourselves?

1:03:58.080 --> 1:04:03.840
 How will that change society? Well, I would hope it would teach us some humility.

1:04:03.840 --> 1:04:12.640
 Humans are very big on exceptionalism. America is constantly proclaiming itself to be the greatest

1:04:12.640 --> 1:04:18.080
 nation on earth, which it may feel like that if you're an American, but it may not feel like that

1:04:18.080 --> 1:04:22.160
 if you're from Finland, because there's all sorts of things you dearly love about Finland.

1:04:22.160 --> 1:04:29.200
 And exceptionalism is usually bullshit, probably not always. If we both sat here, we could find a

1:04:29.200 --> 1:04:35.680
 good example of something that isn't as a rule of thumb. And what it would do is it would teach

1:04:35.680 --> 1:04:41.680
 us some humility and about, you know, actually, often that's what science does in a funny way.

1:04:41.680 --> 1:04:45.680
 It makes us more and more interesting, but it makes us a smaller and smaller part of the thing

1:04:45.680 --> 1:04:53.280
 that's interesting. And I don't mind that humility at all. I don't think it's a bad thing. Our excesses

1:04:53.280 --> 1:04:59.280
 don't tend to come from humility. You know, our excesses come from the opposite megalomania and

1:04:59.280 --> 1:05:06.160
 stuff. We tend to think of consciousness as having some form of exceptionalism attached to it. I

1:05:06.160 --> 1:05:12.720
 suspect if we ever unravel it, it will turn out to be less than we thought in a way.

1:05:12.720 --> 1:05:18.880
 And perhaps your very own exceptionalist assertion earlier on in our conversation that

1:05:18.880 --> 1:05:24.240
 consciousness is something that belongs to us humans, or not humans, but living organisms,

1:05:24.240 --> 1:05:30.320
 maybe you will one day find out that consciousness is in everything. And that will

1:05:30.320 --> 1:05:36.240
 that will humble you. If that was true, it would certainly humble me, although maybe,

1:05:36.240 --> 1:05:44.880
 almost maybe, I don't know, I don't know what effect that would have. I sort of, I mean,

1:05:44.880 --> 1:05:51.040
 my understanding of that principle is along the lines of say, that an electron has a preferred

1:05:51.040 --> 1:05:57.760
 state, or it may or may not pass through a bit of glass, it may reflect off or it may go through

1:05:57.760 --> 1:06:08.480
 or something like that. And so that feels as if a choice has been made. But if I'm going down the

1:06:08.480 --> 1:06:13.600
 fully deterministic route, I would say there's just an underlying determinism that has defined

1:06:13.600 --> 1:06:19.040
 that, that is defined the preferred state or the reflection or non reflection. So, but look,

1:06:19.040 --> 1:06:24.000
 yeah, you're right. If it turned out that there was a thing that it was like to be the sun, then

1:06:24.720 --> 1:06:29.920
 I would, I'd be amazed and humbled and I'd be happy to be both. That sounds pretty cool.

1:06:29.920 --> 1:06:33.360
 And it'll be, you'll say the same thing as you said to your daughter, but it's nevertheless

1:06:33.360 --> 1:06:42.000
 feels something like to be me and that's pretty damn good. So Kubrick created many masterpieces,

1:06:42.000 --> 1:06:48.560
 including The Shining, Doctor Strange Love, Clockwork Orange. But to me, he will be remembered,

1:06:48.560 --> 1:06:54.240
 I think, to many 100 years from now for 2001, it's based, honestly, I would say that's his greatest

1:06:54.240 --> 1:07:02.640
 film. I agree. You are incredibly humble. I listened to a bunch of your interviews, and I

1:07:02.640 --> 1:07:08.800
 really appreciate that you're humble in your creative efforts and your work. But if I were to

1:07:08.800 --> 1:07:18.720
 force you a gunpoint, you don't know that the mystery is to imagine 100 years out into the

1:07:18.720 --> 1:07:26.160
 future. What will Alex Carlin be remembered for from something you've created already or feel you

1:07:26.160 --> 1:07:32.240
 may feel somewhere deep inside you may still create? Well, okay, well, I'll take, I'll take

1:07:32.240 --> 1:07:44.480
 the question in the spirit, it was asked, but very generous. Gunpoint. What I try to do, so

1:07:44.480 --> 1:07:53.200
 therefore, what I hope, yeah, if I remembered what I might be remembered for, is as someone who

1:07:53.200 --> 1:07:59.920
 participates in a conversation. And I think that often what happens is people don't participate

1:07:59.920 --> 1:08:05.840
 in conversations, they make proclamations, they make statements, and people can either react

1:08:05.840 --> 1:08:11.600
 against the statement or can fall in line behind it. And I don't like that. So I want to be part

1:08:11.600 --> 1:08:17.040
 of a conversation. I take as a sort of basic principle, I think I take lots of my cues from

1:08:17.040 --> 1:08:22.160
 science, but one of the best ones it seems to me is that when a scientist has something proved wrong

1:08:22.160 --> 1:08:27.600
 that they previously believed in, they then have to abandon that position. So I'd like to be someone

1:08:27.600 --> 1:08:36.560
 who is allied to that sort of thinking. So part of an exchange of ideas. And the exchange of ideas

1:08:36.560 --> 1:08:42.960
 for me is something like people in your world show me things about how the world work. And then I

1:08:42.960 --> 1:08:47.840
 say, this is how I feel about what you've told me. And then other people can react to that.

1:08:47.840 --> 1:08:54.400
 And it's not, it's not to say this is how the world is. It's just to say, it is interesting

1:08:54.400 --> 1:09:00.320
 to think about the world in this way. And the conversation is one of the things I'm really

1:09:00.320 --> 1:09:05.920
 hopeful about in your works. The conversation you're having is with the viewer, in the sense that

1:09:06.800 --> 1:09:15.520
 you, you are bringing back you and several others, but you very much so sort of intellectual depth

1:09:15.520 --> 1:09:28.480
 to cinema, to now series, sort of allowing film to be something that, yeah, sparks a conversation,

1:09:28.480 --> 1:09:32.720
 is a conversation, lets people think, allows them to think.

1:09:32.720 --> 1:09:37.760
 But also, it's very important for me that if that conversation is going to be a good conversation,

1:09:37.760 --> 1:09:44.960
 what that must involve is that someone like you who understands AI, and I imagine understands a

1:09:44.960 --> 1:09:50.480
 lot about quantum mechanics, if they then watch the narrative feels, yes, this is a fair account.

1:09:51.360 --> 1:09:57.680
 So it is a worthy addition to the conversation. That for me is hugely important. I'm not interested

1:09:57.680 --> 1:10:00.880
 in getting that stuff wrong. I'm only interested in trying to get it right.

1:10:00.880 --> 1:10:08.080
 Alex, it was truly an honor to talk to you. I really appreciate it. I really enjoy it. Thank you so

1:10:08.080 --> 1:10:14.720
 much. Thank you. Thanks, man. Thanks for listening to this conversation with Alex Garland. And thank

1:10:14.720 --> 1:10:21.280
 you to our presenting sponsor, Cash App. Download it, use code LEX, podcast, you'll get $10 and

1:10:21.280 --> 1:10:26.560
 $10 will go to first, an organization that inspires and educates young minds to become

1:10:26.560 --> 1:10:32.400
 science and technology innovators of tomorrow. If you enjoy this podcast, subscribe on YouTube,

1:10:32.400 --> 1:10:37.680
 give it five stars in Apple Podcasts, support on Patreon, or simply connect with me on Twitter,

1:10:37.680 --> 1:10:44.720
 at Lex Friedman. And now, let me leave you with a question from Ava, the central artificial

1:10:44.720 --> 1:10:50.000
 intelligence character in the movie X Machina, that she asked during her Turing test.

1:10:50.000 --> 1:10:59.520
 What will happen to me if I fail your test? Thank you for listening and hope to see you next time.

