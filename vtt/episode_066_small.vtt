WEBVTT

00:00.000 --> 00:03.400
 The following is a conversation with Ayanna Howard.

00:03.400 --> 00:06.200
 She's a roboticist, Professor Georgia Tech,

00:06.200 --> 00:09.840
 and Director of the Human Automation Systems Lab.

00:09.840 --> 00:12.800
 With research interests in human robot interaction,

00:12.800 --> 00:16.000
 assistive robots in the home, therapy gaming apps,

00:16.000 --> 00:20.280
 and remote robotic exploration of extreme environments.

00:20.280 --> 00:23.440
 Like me, in her work, she cares a lot

00:23.440 --> 00:26.360
 about both robots and human beings.

00:26.360 --> 00:29.560
 And so I really enjoyed this conversation.

00:29.560 --> 00:32.600
 This is the Artificial Intelligence Podcast.

00:32.600 --> 00:34.960
 If you enjoy it, subscribe on YouTube,

00:34.960 --> 00:36.960
 give it five stars on Apple Podcast,

00:36.960 --> 00:39.600
 follow on Spotify, support it on Patreon,

00:39.600 --> 00:41.720
 or simply connect with me on Twitter.

00:41.720 --> 00:45.640
 Alex Friedman, spelled F R I D M A N.

00:45.640 --> 00:48.680
 I recently started doing ads at the end of the introduction.

00:48.680 --> 00:51.640
 I'll do one or two minutes after introducing the episode

00:51.640 --> 00:53.200
 and never any ads in the middle

00:53.200 --> 00:55.480
 that can break the flow of the conversation.

00:55.480 --> 00:56.880
 I hope that works for you

00:56.880 --> 01:00.160
 and doesn't hurt the listening experience.

01:00.160 --> 01:02.280
 This show is presented by Cash App,

01:02.280 --> 01:04.760
 the number one finance app in the App Store.

01:04.760 --> 01:07.520
 I personally use Cash App to send money to friends,

01:07.520 --> 01:09.320
 but you can also use it to buy, sell,

01:09.320 --> 01:11.720
 and deposit Bitcoin in just seconds.

01:11.720 --> 01:14.600
 Cash App also has a new investing feature.

01:14.600 --> 01:17.520
 You can buy fractions of a stock, say $1 worth,

01:17.520 --> 01:19.640
 no matter what the stock price is.

01:19.640 --> 01:22.560
 Broker services are provided by Cash App Investing,

01:22.560 --> 01:25.840
 a subsidiary of Square and member SIPC.

01:25.840 --> 01:28.160
 I'm excited to be working with Cash App

01:28.160 --> 01:31.560
 to support one of my favorite organizations called First,

01:31.560 --> 01:35.080
 best known for their first robotics and Lego competitions.

01:35.080 --> 01:38.360
 They educate and inspire hundreds of thousands of students

01:38.360 --> 01:40.200
 in over 110 countries

01:40.200 --> 01:42.840
 and have a perfect rating and charity navigator,

01:42.840 --> 01:44.120
 which means the donated money

01:44.120 --> 01:46.880
 is used to maximum effectiveness.

01:46.880 --> 01:49.600
 When you get Cash App from the App Store, Google Play,

01:49.600 --> 01:53.520
 and use code LEX Podcast, you'll get $10,

01:53.520 --> 01:56.440
 and Cash App will also donate $10 the first,

01:56.440 --> 01:58.280
 which again, is an organization

01:58.280 --> 02:01.040
 that I've personally seen inspire girls and boys

02:01.040 --> 02:04.280
 to dream of engineering a better world.

02:04.280 --> 02:08.320
 And now, here's my conversation with Ayanna Howard.

02:09.440 --> 02:13.640
 What or who is the most amazing robot you've ever met,

02:13.640 --> 02:16.720
 or perhaps had the biggest impact on your career?

02:16.720 --> 02:21.080
 I haven't met her, but I grew up with her,

02:21.080 --> 02:22.720
 but of course, Rosie.

02:22.720 --> 02:25.200
 So, and I think it's because also...

02:25.200 --> 02:26.080
 Who's Rosie?

02:26.080 --> 02:27.760
 Rosie from the Jetsons.

02:27.760 --> 02:30.920
 She is all things to all people, right?

02:30.920 --> 02:32.840
 Think about it, like anything you wanted,

02:32.840 --> 02:35.040
 it was like magic, it happened.

02:35.040 --> 02:37.840
 So people not only anthropomorphize,

02:37.840 --> 02:41.880
 but project whatever they wish for the robot to be onto.

02:41.880 --> 02:42.880
 Onto Rosie.

02:42.880 --> 02:44.560
 But also, I mean, think about it,

02:44.560 --> 02:46.760
 she was socially engaging.

02:46.760 --> 02:50.000
 She, every so often had an attitude, right?

02:50.000 --> 02:51.920
 She kept us honest.

02:51.920 --> 02:53.720
 She would push back sometimes

02:53.720 --> 02:57.000
 when George was doing some weird stuff.

02:57.000 --> 02:59.800
 But she cared about people, especially the kids.

03:01.200 --> 03:03.960
 She was like the perfect robot.

03:03.960 --> 03:06.480
 And you've said that people don't want

03:06.480 --> 03:08.320
 the robots to be perfect.

03:09.760 --> 03:11.120
 Can you elaborate that?

03:11.120 --> 03:12.000
 What do you think that is?

03:12.000 --> 03:13.480
 Just like you said,

03:13.480 --> 03:15.720
 Rosie pushed back a little bit every once in a while.

03:15.720 --> 03:18.240
 Yeah, so I think it's that.

03:18.240 --> 03:19.840
 So if you think about robotics in general,

03:19.840 --> 03:23.880
 we want them because they enhance our quality of life.

03:23.880 --> 03:26.360
 And usually that's linked to something that's functional,

03:26.360 --> 03:27.200
 right?

03:27.200 --> 03:28.600
 Even if you think of self driving cars,

03:28.600 --> 03:29.960
 why is there a fascination?

03:29.960 --> 03:31.480
 Because people really do hate to drive.

03:31.480 --> 03:34.120
 Like there's the, like Saturday driving

03:34.120 --> 03:35.240
 where I can just speed,

03:35.240 --> 03:36.080
 but then there was the,

03:36.080 --> 03:37.480
 I have to go to work every day

03:37.480 --> 03:38.960
 and I'm in traffic for an hour.

03:38.960 --> 03:40.360
 I mean, people really hate that.

03:40.360 --> 03:45.360
 And so robots are designed to basically enhance

03:45.360 --> 03:49.280
 our ability to increase our quality of life.

03:49.280 --> 03:53.640
 And so the perfection comes from this aspect of interaction.

03:54.840 --> 03:57.680
 If I think about how we drive,

03:57.680 --> 04:01.560
 if we drove perfectly, we would never get anywhere, right?

04:01.560 --> 04:06.560
 So think about how many times you had to run past the light

04:06.560 --> 04:08.360
 because you see the car behind you

04:08.360 --> 04:09.800
 is about to crash into you.

04:09.800 --> 04:14.800
 Or that little kid kind of runs into the street

04:14.800 --> 04:16.720
 and so you have to cross on the other side

04:16.720 --> 04:17.920
 because there's no cars, right?

04:17.920 --> 04:19.040
 Like if you think about it,

04:19.040 --> 04:20.680
 we are not perfect drivers.

04:20.680 --> 04:23.080
 Some of it is because it's our world.

04:23.080 --> 04:26.240
 And so if you have a robot that is perfect

04:26.240 --> 04:28.240
 in that sense of the word,

04:28.240 --> 04:30.720
 they wouldn't really be able to function with us.

04:30.720 --> 04:34.000
 Can you linger a little bit on the word perfection?

04:34.000 --> 04:36.880
 So from the robotics perspective,

04:36.880 --> 04:38.920
 what does that word mean?

04:38.920 --> 04:42.480
 And how is sort of the optimal behaviors

04:42.480 --> 04:46.160
 you're describing different than what we think is perfection?

04:46.160 --> 04:49.000
 Yeah, so perfection, if you think about it

04:49.000 --> 04:51.480
 in the more theoretical point of view,

04:51.480 --> 04:53.560
 it's really tied to accuracy, right?

04:53.560 --> 04:55.120
 So if I have a function,

04:55.120 --> 04:59.040
 can I complete it at 100% accuracy with zero errors?

05:00.200 --> 05:03.680
 And so that's kind of if you think about perfection

05:03.680 --> 05:04.760
 in the sense of the word.

05:04.760 --> 05:07.080
 And in a self driving car realm,

05:07.080 --> 05:10.000
 do you think from a robotics perspective,

05:10.000 --> 05:13.440
 we kind of think that perfection means

05:13.440 --> 05:15.120
 following the rules perfectly,

05:15.120 --> 05:19.120
 sort of defining, staying in the lane, changing lanes.

05:19.120 --> 05:20.400
 When there's a green light, you go,

05:20.400 --> 05:21.840
 when there's a red light, you stop.

05:21.840 --> 05:24.320
 And that's the,

05:24.320 --> 05:28.680
 and be able to perfectly see all the entities in the scene.

05:28.680 --> 05:31.520
 That's the limit of what we think of as perfection.

05:31.520 --> 05:33.280
 And I think that's where the problem comes,

05:33.280 --> 05:37.880
 is that when people think about perfection for robotics,

05:37.880 --> 05:40.360
 the ones that are the most successful

05:40.360 --> 05:42.800
 are the ones that are, quote unquote, perfect.

05:42.800 --> 05:44.200
 Like I said, Rosie is perfect,

05:44.200 --> 05:46.880
 but she actually wasn't perfect in terms of accuracy,

05:46.880 --> 05:49.880
 but she was perfect in terms of how she interacted

05:49.880 --> 05:51.160
 and how she adapted.

05:51.160 --> 05:52.800
 And I think that's some of the disconnect,

05:52.800 --> 05:56.000
 is that we really want perfection

05:56.000 --> 05:59.520
 with respect to its ability to adapt to us.

05:59.520 --> 06:03.040
 We don't really want perfection with respect to 100% accuracy

06:03.040 --> 06:06.320
 with respect to the rules that we just made up anyway, right?

06:06.320 --> 06:09.160
 And so I think there's this disconnect sometimes

06:09.160 --> 06:12.920
 between what we really want and what happens.

06:12.920 --> 06:15.560
 And we see this all the time, like in my research, right?

06:15.560 --> 06:20.000
 Like the optimal, quote unquote, optimal interactions

06:20.000 --> 06:23.960
 are when the robot is adapting based on the person,

06:23.960 --> 06:28.960
 not 100% following what's optimal based on the rules.

06:29.200 --> 06:32.240
 Just a link around autonomous vehicles for a second.

06:32.240 --> 06:35.200
 Just your thoughts maybe off the top of the head

06:35.200 --> 06:37.480
 is how hard is that problem,

06:37.480 --> 06:39.840
 do you think based on what we just talked about?

06:39.840 --> 06:41.600
 You know, there's a lot of folks

06:41.600 --> 06:43.840
 in the automotive industry, they're very confident

06:43.840 --> 06:47.600
 from Elon Musk to Waymo to all these companies.

06:47.600 --> 06:50.440
 How hard is it to solve that last piece?

06:50.440 --> 06:51.280
 The last mile.

06:51.280 --> 06:54.600
 The gap between the perfection

06:54.600 --> 06:57.480
 and the human definition

06:57.480 --> 06:59.440
 of how you actually function in this world.

06:59.440 --> 07:00.560
 Yeah, so this is a moving target.

07:00.560 --> 07:04.440
 So I remember when all the big companies

07:04.440 --> 07:06.800
 started to heavily invest in this.

07:06.800 --> 07:09.880
 And there was a number of even roboticists

07:09.880 --> 07:13.200
 as well as folks who were putting in the VCs

07:13.200 --> 07:15.360
 and corporations, Elon Musk being one of them,

07:15.360 --> 07:19.480
 that said, self driving cars on the road with people

07:19.480 --> 07:24.480
 within five years, that was a little while ago.

07:24.560 --> 07:29.560
 And now people are saying five years, 10 years, 20 years,

07:29.800 --> 07:31.520
 some are saying never, right?

07:31.520 --> 07:33.760
 I think if you look at some of the things

07:33.760 --> 07:37.680
 that are being successful is these

07:39.480 --> 07:41.200
 basically fixed environments

07:41.200 --> 07:44.000
 where you still have some anomalies, right?

07:44.000 --> 07:46.520
 You still have people walking, you still have stores,

07:46.520 --> 07:50.120
 but you don't have other drivers, right?

07:50.120 --> 07:53.560
 Like other human drivers or is a dedicated space

07:53.560 --> 07:55.640
 for the cars.

07:55.640 --> 07:57.200
 Because if you think about robotics in general,

07:57.200 --> 07:59.040
 where has always been successful?

07:59.040 --> 08:00.600
 I mean, you can say manufacturing,

08:00.600 --> 08:02.320
 like way back in the day, right?

08:02.320 --> 08:04.320
 It was a fixed environment, humans were not part

08:04.320 --> 08:07.160
 of the equation, we're a lot better than that.

08:07.160 --> 08:10.920
 But like when we can carve out scenarios

08:10.920 --> 08:13.760
 that are closer to that space,

08:13.760 --> 08:16.640
 then I think that it's where we are.

08:16.640 --> 08:20.520
 So a closed campus where you don't have self driving cars

08:20.520 --> 08:23.760
 and maybe some protection so that the students

08:23.760 --> 08:27.200
 don't jet in front just because they wanna see what happens.

08:27.200 --> 08:29.920
 Like having a little bit, I think that's where

08:29.920 --> 08:32.240
 we're gonna see the most success in the near future.

08:32.240 --> 08:33.640
 And be slow moving.

08:33.640 --> 08:37.840
 Right, not 55, 60, 70 miles an hour,

08:37.840 --> 08:42.040
 but the speed of a golf cart, right?

08:42.040 --> 08:45.160
 So that said, the most successful

08:45.160 --> 08:47.840
 in the automotive industry robots operating today

08:47.840 --> 08:50.720
 in the hands of real people are ones

08:50.720 --> 08:53.880
 that are traveling over 55 miles an hour.

08:53.880 --> 08:55.520
 And in uncle's trains environment,

08:55.520 --> 08:58.840
 which is Tesla vehicles, so the Tesla autopilot.

08:58.840 --> 09:01.680
 So I would love to hear sort of your,

09:01.680 --> 09:04.240
 just thoughts of two things.

09:04.240 --> 09:07.000
 So one, I don't know if you've gotten to see

09:07.000 --> 09:10.240
 you've heard about something called smart summon.

09:10.240 --> 09:13.480
 It would Tesla system autopilot system

09:13.480 --> 09:17.120
 where the car drives zero occupancy, no driver

09:17.120 --> 09:19.960
 in the parking lot slowly sort of tries to navigate

09:19.960 --> 09:22.680
 the parking lot to find itself to you.

09:22.680 --> 09:25.840
 And there's some incredible amounts of videos

09:25.840 --> 09:27.640
 and just hilarity that happens.

09:27.640 --> 09:30.880
 Is it awkwardly tries to navigate this environment?

09:30.880 --> 09:33.520
 But it's a beautiful nonverbal communication

09:33.520 --> 09:37.360
 between machine and human that I think is a from,

09:37.360 --> 09:39.320
 it's like, it's some of the work that you do

09:39.320 --> 09:42.040
 in this kind of interesting human robot interaction space.

09:42.040 --> 09:43.760
 So what are your thoughts in general about it?

09:43.760 --> 09:46.120
 So I do have that feature.

09:46.960 --> 09:47.800
 Do you drive a Tesla?

09:47.800 --> 09:52.120
 I do, mainly because I'm a gadget freak, right?

09:52.120 --> 09:55.640
 So I say it's a gadget that happens to have some wheels.

09:55.640 --> 09:58.200
 And yeah, I've seen some of the videos.

09:58.200 --> 09:59.400
 But what's your experience like?

09:59.400 --> 10:02.680
 I mean, you're a human robot interaction roboticist,

10:02.680 --> 10:05.560
 you're a legit sort of expert in the field.

10:05.560 --> 10:08.040
 So what does it feel for a machine to come to you?

10:08.040 --> 10:11.880
 It's one of these very fascinating things,

10:11.880 --> 10:16.080
 but also I am hyper, hyper alert, right?

10:16.080 --> 10:20.520
 Like I'm hyper alert, like my, but my thumb is like,

10:20.520 --> 10:23.200
 oh, okay, I'm ready to take over.

10:23.200 --> 10:26.160
 Even when I'm in my car or I'm doing things

10:26.160 --> 10:28.720
 like automated backing into,

10:28.720 --> 10:30.560
 so there's like a feature where you can do this

10:30.560 --> 10:33.120
 automating backing into our parking space

10:33.120 --> 10:35.640
 or bring the car out of your garage

10:35.640 --> 10:40.240
 or even, you know, pseudo autopilot on the freeway, right?

10:40.240 --> 10:42.200
 I am hyper sensitive.

10:42.200 --> 10:44.840
 I can feel like as I'm navigating, I'm like,

10:44.840 --> 10:46.880
 yeah, that's an error right there.

10:46.880 --> 10:50.120
 Like I am very aware of it,

10:50.120 --> 10:54.280
 but I'm also fascinated by it and it does get better.

10:54.280 --> 10:57.400
 Like I look and see it's learning

10:57.400 --> 11:00.360
 from all of these people who are cutting it on.

11:00.360 --> 11:04.120
 Like every time I come on, it's getting better, right?

11:04.120 --> 11:07.120
 And so I think that's what's amazing about it is that.

11:07.120 --> 11:10.320
 This nice dance of you're still hyper vigilant.

11:10.320 --> 11:12.720
 So you're still not trusting it at all.

11:12.720 --> 11:13.560
 Yeah.

11:13.560 --> 11:16.400
 And yet you're using it on the highway if I were to,

11:16.400 --> 11:18.600
 like what, as a roboticist,

11:18.600 --> 11:20.520
 we'll talk about trust a little bit.

11:22.640 --> 11:23.640
 How do you explain that?

11:23.640 --> 11:25.040
 You still use it.

11:25.040 --> 11:26.480
 Is it the gadget freak part?

11:26.480 --> 11:30.720
 Like where you just enjoy exploring technology

11:30.720 --> 11:33.680
 or is that the right actually balance

11:33.680 --> 11:36.880
 between robotics and humans is where you use it,

11:36.880 --> 11:38.320
 but don't trust it.

11:38.320 --> 11:40.080
 And somehow there's this dance

11:40.080 --> 11:42.080
 that ultimately is a positive.

11:42.080 --> 11:44.600
 Yeah. So I think I'm,

11:44.600 --> 11:48.080
 I just don't necessarily trust technology,

11:48.080 --> 11:50.120
 but I'm an early adopter, right?

11:50.120 --> 11:54.280
 So when it first comes out, I will use everything,

11:54.280 --> 11:57.440
 but I will be very, very cautious of how I use it.

11:57.440 --> 12:01.040
 Do you read about it or do you explore it, but just try it?

12:01.040 --> 12:05.000
 Do you like crudely, to put it crudely,

12:05.000 --> 12:07.960
 do you read the manual or do you learn through exploration?

12:07.960 --> 12:08.800
 I'm an explorer.

12:08.800 --> 12:12.320
 If I have to read the manual, then I do design,

12:12.320 --> 12:15.040
 then it's a bad user interface, it's a failure.

12:16.480 --> 12:19.560
 Elon Musk is very confident that you kind of take it

12:19.560 --> 12:21.800
 from where it is now to full autonomy.

12:21.800 --> 12:24.520
 So from this human robot interaction

12:24.520 --> 12:26.720
 where you don't really trust and then you try

12:26.720 --> 12:29.200
 and then you catch it when it fails to,

12:29.200 --> 12:32.320
 it's going to incrementally improve itself

12:32.320 --> 12:36.560
 into full, full way you don't need to participate.

12:36.560 --> 12:39.880
 What's your sense of that trajectory?

12:39.880 --> 12:41.080
 Is it feasible?

12:41.080 --> 12:44.600
 So the promise there is by the end of next year,

12:44.600 --> 12:47.240
 by the end of 2020 is the current promise.

12:47.240 --> 12:52.240
 What's your sense about that journey that Tesla's on?

12:53.600 --> 12:56.600
 So there's kind of three things going on though.

12:56.600 --> 13:01.600
 I think in terms of will people go,

13:01.960 --> 13:04.800
 like as a user, as a adopter,

13:04.800 --> 13:08.440
 will you trust going to that point?

13:08.440 --> 13:10.080
 I think so, right?

13:10.080 --> 13:12.480
 Like there are some users and it's because what happens

13:12.480 --> 13:16.680
 is when you're hypersensitive at the beginning

13:16.680 --> 13:19.280
 and then the technology tends to work,

13:19.280 --> 13:23.800
 your apprehensions slowly goes away.

13:23.800 --> 13:28.240
 And as people, we tend to swing to the other extreme, right?

13:28.240 --> 13:30.880
 Because like, oh, I was like hyper, hyper fearful

13:30.880 --> 13:33.920
 or hypersensitive and it was awesome.

13:33.920 --> 13:35.560
 And we just tend to swing.

13:35.560 --> 13:37.320
 That's just human nature.

13:37.320 --> 13:38.840
 And so you will have, I mean,

13:38.840 --> 13:41.480
 That's a scary notion because most people

13:41.480 --> 13:44.960
 are now extremely untrusting of autopilot.

13:44.960 --> 13:46.440
 They use it, but they don't trust it.

13:46.440 --> 13:48.840
 And it's a scary notion that there's a certain point

13:48.840 --> 13:51.320
 where you allow yourself to look at the smartphone

13:51.320 --> 13:53.040
 for like 20 seconds.

13:53.040 --> 13:55.360
 And then there'll be this phase shift

13:55.360 --> 13:57.520
 where it'll be like 20 seconds, 30 seconds,

13:57.520 --> 13:58.880
 one minute, two minutes.

13:59.920 --> 14:01.960
 It's a scary proposition.

14:01.960 --> 14:03.440
 But that's people, right?

14:03.440 --> 14:05.520
 That's just humans.

14:05.520 --> 14:09.920
 I mean, I think of even our use of,

14:09.920 --> 14:12.320
 I mean, just everything on the internet, right?

14:12.320 --> 14:16.800
 Like think about how reliant we are on certain apps

14:16.800 --> 14:19.320
 and certain engines, right?

14:20.160 --> 14:21.640
 20 years ago, people have been like,

14:21.640 --> 14:22.600
 oh yeah, that's stupid.

14:22.600 --> 14:23.880
 Like that makes no sense.

14:23.880 --> 14:25.800
 Like of course that's false.

14:25.800 --> 14:29.000
 Like now it's just like, oh, of course I've been using it.

14:29.000 --> 14:30.640
 It's been correct all this time.

14:30.640 --> 14:34.280
 Of course, aliens, I didn't think they existed,

14:34.280 --> 14:37.440
 but now it says they do, obviously.

14:37.440 --> 14:39.400
 100% Earth is flat.

14:39.400 --> 14:43.800
 So, okay, but you said three things.

14:43.800 --> 14:44.640
 So one is the human.

14:44.640 --> 14:45.800
 Okay, so one is the human.

14:45.800 --> 14:47.840
 And I think there will be a group of individuals

14:47.840 --> 14:49.560
 that will swing, right?

14:49.560 --> 14:50.400
 I just...

14:50.400 --> 14:51.240
 Teenagers.

14:51.240 --> 14:54.400
 Teenage, I mean, it'll be adults.

14:54.400 --> 14:56.400
 There's actually an age demographic

14:56.400 --> 15:00.760
 that's optimal for technology adoption.

15:00.760 --> 15:02.280
 And you can actually find them.

15:02.280 --> 15:03.880
 And they're actually pretty easy to find.

15:03.880 --> 15:07.000
 Just based on their habits, based on...

15:07.000 --> 15:10.400
 So someone like me who wasn't a roboticist

15:10.400 --> 15:13.520
 would probably be the optimal kind of person, right?

15:13.520 --> 15:15.600
 Early adopter, okay with technology,

15:15.600 --> 15:20.000
 very comfortable and not hypersensitive, right?

15:20.000 --> 15:22.600
 I'm just hypersensitive because I designed this stuff.

15:23.520 --> 15:25.880
 So there is a target demographic that will swing.

15:25.880 --> 15:29.800
 The other one though is you still have these humans

15:29.800 --> 15:31.320
 that are on the road.

15:31.320 --> 15:35.560
 That one is a harder thing to do.

15:35.560 --> 15:40.320
 And as long as we have people that are on the same streets,

15:40.320 --> 15:42.480
 that's gonna be the big issue.

15:42.480 --> 15:45.240
 And it's just because you can't possibly,

15:45.240 --> 15:49.480
 you can't possibly map some of the silliness

15:49.480 --> 15:51.400
 of human drivers, right?

15:51.400 --> 15:56.240
 Like as an example, when you're next to that car

15:56.240 --> 15:59.760
 that has that big sticker called student driver, right?

15:59.760 --> 16:04.600
 Like you are like, oh, either I am going to like go around.

16:04.600 --> 16:06.760
 Like we are, we know that that person

16:06.760 --> 16:09.280
 is just gonna make mistakes that make no sense, right?

16:09.280 --> 16:11.000
 How do you map that information?

16:11.880 --> 16:14.320
 Or if I am in a car and I look over

16:14.320 --> 16:19.240
 and I see two fairly young looking individuals

16:19.240 --> 16:21.160
 and there's no student driver bumper

16:21.160 --> 16:22.880
 and I see them chatting to each other,

16:22.880 --> 16:26.160
 I'm like, oh, that's an issue, right?

16:26.160 --> 16:28.520
 So how do you get that kind of information

16:28.520 --> 16:35.280
 and that experience into basically an autopilot?

16:35.280 --> 16:37.280
 Yeah, and there's millions of cases like that

16:37.280 --> 16:41.240
 where we take little hints to establish context.

16:41.240 --> 16:44.400
 I mean, you said kind of beautifully poetic human things,

16:44.400 --> 16:47.160
 but there's probably subtle things about the environment,

16:47.160 --> 16:52.160
 about it being maybe time for commuters

16:52.920 --> 16:55.320
 to start going home from work.

16:55.320 --> 16:57.160
 And therefore you can make some kind of judgment

16:57.160 --> 16:59.400
 about the group behavior of pedestrians,

16:59.400 --> 17:01.200
 blah, blah, blah, so on and so on.

17:01.200 --> 17:02.680
 Are even cities, right?

17:02.680 --> 17:07.120
 Like if you're in Boston, how people cross the street,

17:07.120 --> 17:10.680
 like lights are not an issue versus other places

17:10.680 --> 17:15.600
 where people will actually wait for the crosswalk.

17:15.600 --> 17:18.680
 Seattle or somewhere peaceful.

17:18.680 --> 17:22.560
 And but what I've also seen, so just even in Boston

17:22.560 --> 17:25.520
 that intersection to intersection is different.

17:25.520 --> 17:28.920
 So every intersection has a personality of its own.

17:28.920 --> 17:30.840
 So certain neighborhoods of Boston are different.

17:30.840 --> 17:35.200
 So we kind of end based on different timing of day

17:35.200 --> 17:40.320
 at night, it's all, there's a dynamic to human behavior

17:40.320 --> 17:42.440
 that we kind of figure out ourselves.

17:42.440 --> 17:46.080
 We're not able to introspect and figure it out,

17:46.080 --> 17:49.320
 but somehow our brain learns it.

17:49.320 --> 17:50.360
 We do.

17:50.360 --> 17:54.800
 And so you're saying, is there a shortcut?

17:54.800 --> 17:56.400
 Is there a shortcut though for a robot?

17:56.400 --> 17:58.080
 Is there something that could be done?

17:58.080 --> 18:02.640
 You think that, you know, that's what we humans do.

18:02.640 --> 18:04.640
 It's just like bird flight, right?

18:04.640 --> 18:06.480
 This example they give for flight.

18:06.480 --> 18:09.280
 Do you necessarily need to build a bird that flies

18:09.280 --> 18:10.680
 or can you do an airplane?

18:11.560 --> 18:13.040
 So is there a shortcut to it?

18:13.040 --> 18:15.360
 So I think that the shortcut is,

18:15.360 --> 18:19.360
 and I kind of, I talk about it as a fixed space.

18:19.360 --> 18:23.280
 Where, so imagine that there's a neighborhood

18:23.280 --> 18:26.760
 that's a new smart city or a new neighborhood that says,

18:26.760 --> 18:31.440
 you know what, we are going to design this new city

18:31.440 --> 18:33.800
 based on supporting self driving cars.

18:33.800 --> 18:37.640
 And then doing things, knowing that there's anomalies,

18:37.640 --> 18:39.600
 knowing that people are like this, right?

18:39.600 --> 18:42.080
 And designing it based on that assumption

18:42.080 --> 18:44.000
 that like we're gonna have this,

18:44.000 --> 18:45.520
 that would be an example of a shortcut.

18:45.520 --> 18:49.240
 So you still have people, but you do very specific things

18:49.240 --> 18:51.840
 to try to minimize the noise a little bit.

18:51.840 --> 18:53.840
 As an example.

18:53.840 --> 18:55.520
 And the people themselves become accepting

18:55.520 --> 18:57.760
 of the notion that there's autonomous cars, right?

18:57.760 --> 18:59.720
 Right, like they move into,

18:59.720 --> 19:01.480
 so right now you have like a,

19:01.480 --> 19:03.600
 you will have a self selection bias, right?

19:03.600 --> 19:06.240
 Like individuals will move into this neighborhood

19:06.240 --> 19:10.640
 knowing like this is part of like the real estate pitch, right?

19:10.640 --> 19:14.160
 And so I think that's a way to do a shortcut.

19:14.160 --> 19:17.600
 When it allows you to deploy,

19:17.600 --> 19:21.960
 it allows you to collect then data with these variances

19:21.960 --> 19:24.040
 and anomalies, cause people are still people,

19:24.040 --> 19:28.840
 but it's a safer space and is more of an accepting space.

19:28.840 --> 19:31.960
 IE when something in that space might happen

19:31.960 --> 19:35.080
 because things do, because you already have

19:35.080 --> 19:37.200
 the self selection, like people would be,

19:37.200 --> 19:40.760
 I think a little more forgiving than other places.

19:40.760 --> 19:43.120
 And you said three things, did we cover all of them?

19:43.120 --> 19:46.360
 The third is legal law, liability,

19:46.360 --> 19:47.840
 which I don't really want to touch,

19:47.840 --> 19:50.920
 but it's still of concern.

19:50.920 --> 19:53.280
 And the mishmash with like, with policy as well,

19:53.280 --> 19:55.760
 sort of government, all that, that whole.

19:55.760 --> 19:57.760
 That big ball of stuff.

19:57.760 --> 19:59.120
 Yeah, got you.

19:59.120 --> 20:01.760
 So that's, so we're out of time now.

20:03.600 --> 20:06.040
 Do you think from a robotics perspective,

20:07.200 --> 20:09.800
 you know, if you're kind of honest with what cars do,

20:09.800 --> 20:14.800
 they kind of threaten each other's life all the time.

20:14.800 --> 20:19.240
 So cars are very, I mean, in order to navigate intersections,

20:19.240 --> 20:22.240
 there's an assertiveness, there's a risk taking,

20:22.240 --> 20:25.200
 and if you were to reduce it to an objective function,

20:25.200 --> 20:28.720
 there's a probability of murder in that function,

20:28.720 --> 20:31.840
 meaning you killing another human being,

20:31.840 --> 20:33.520
 and you're using that.

20:33.520 --> 20:35.640
 First of all, it has to be low enough

20:36.880 --> 20:39.640
 to be acceptable to you on an ethical level,

20:39.640 --> 20:41.240
 as an individual human being,

20:41.240 --> 20:45.240
 but it has to be high enough for people to respect you,

20:45.240 --> 20:47.480
 to not sort of take advantage of you completely,

20:47.480 --> 20:49.560
 and jaywalk in front of you, and so on.

20:49.560 --> 20:53.080
 So, I mean, I don't think there's a right answer here,

20:53.080 --> 20:56.040
 but how do we solve that?

20:56.040 --> 20:57.960
 How do we solve that from a robotics perspective

20:57.960 --> 21:00.160
 when danger and human life is at stake?

21:00.160 --> 21:01.960
 Yeah, as they say, cars don't kill people,

21:01.960 --> 21:02.960
 people kill people.

21:02.960 --> 21:05.080
 Kill people, kill people.

21:05.080 --> 21:08.600
 Right, so I think.

21:08.600 --> 21:10.760
 And now robotic algorithms would be killing people.

21:10.760 --> 21:14.400
 Right, so it will be robotics algorithms that are,

21:14.400 --> 21:17.000
 no, it will be robotic algorithms don't kill people,

21:17.000 --> 21:19.760
 developers of robotic algorithms kill people, right?

21:19.760 --> 21:22.960
 I mean, one of the things is people are still in the loop,

21:22.960 --> 21:26.560
 and at least in the near and midterm,

21:26.560 --> 21:28.800
 I think people will still be in the loop.

21:28.800 --> 21:30.320
 At some point, even if it's the developer,

21:30.320 --> 21:31.880
 like we're not necessarily at the stage

21:31.880 --> 21:36.760
 where robots are programming autonomous robots

21:36.760 --> 21:40.160
 with different behaviors quite yet.

21:40.160 --> 21:42.280
 That's a scary notion, sorry, to interrupt,

21:42.280 --> 21:47.400
 that a developer has some responsibility

21:47.400 --> 21:49.680
 in the death of a human being.

21:49.680 --> 21:50.560
 That's a heavy burden.

21:50.560 --> 21:55.440
 I mean, I think that's why the whole aspect of ethics

21:55.440 --> 21:58.480
 in our community is so, so important, right?

21:58.480 --> 22:03.080
 Like, because it's true, if you think about it,

22:03.080 --> 22:04.840
 you can basically say,

22:04.840 --> 22:07.440
 I'm not going to work on weaponized AI, right?

22:07.440 --> 22:09.840
 Like, people can say, that's not what I'm gonna do.

22:09.840 --> 22:12.720
 But yet, you are programming algorithms

22:12.720 --> 22:15.600
 that might be used in healthcare algorithms

22:15.600 --> 22:17.240
 that might decide whether this person

22:17.240 --> 22:18.960
 should get this medication or not,

22:18.960 --> 22:21.400
 and they don't, and they die.

22:21.400 --> 22:25.080
 Okay, so that is your responsibility, right?

22:25.080 --> 22:27.320
 And if you're not conscious and aware

22:27.320 --> 22:30.000
 that you do have that power when you're coding

22:30.000 --> 22:31.680
 and things like that,

22:31.680 --> 22:35.000
 I think that's just not a good thing.

22:35.000 --> 22:38.040
 Like, we need to think about this responsibility

22:38.040 --> 22:41.840
 as we program robots and computing devices

22:41.840 --> 22:44.320
 much more than we are.

22:44.320 --> 22:46.960
 Yeah, so it's not an option to not think about ethics.

22:46.960 --> 22:51.360
 I think it's a majority, I would say, of computer science.

22:51.360 --> 22:53.840
 Sort of, it's kind of a hot topic now,

22:53.840 --> 22:55.680
 I think about bias and so on,

22:55.680 --> 22:57.720
 but it's, and we'll talk about it,

22:57.720 --> 22:59.120
 but usually it's kind of,

23:00.400 --> 23:02.680
 it's like a very particular group of people

23:02.680 --> 23:04.280
 that work on that.

23:04.280 --> 23:06.920
 And then, people who do robotics are like,

23:06.920 --> 23:09.320
 well, I don't have to think about that.

23:09.320 --> 23:11.120
 There's other smart people thinking about it.

23:11.120 --> 23:14.560
 It seems that everybody has to think about it.

23:14.560 --> 23:17.000
 It's not, you can't escape the ethics,

23:17.000 --> 23:21.120
 whether it's bias or just every aspect of ethics

23:21.120 --> 23:22.680
 that has to do with human beings.

23:22.680 --> 23:23.520
 Everyone.

23:23.520 --> 23:25.680
 So think about, I'm gonna age myself,

23:25.680 --> 23:30.080
 but I remember when we didn't have like testers, right?

23:30.080 --> 23:31.040
 And so what did you do?

23:31.040 --> 23:33.560
 As a developer, you had to test your own code, right?

23:33.560 --> 23:35.200
 Like you had to go through all the cases

23:35.200 --> 23:36.600
 and figure it out and, you know,

23:36.600 --> 23:38.560
 and then they realized that, you know,

23:38.560 --> 23:40.560
 like we probably need to have testing

23:40.560 --> 23:42.360
 because we're not getting all the things.

23:42.360 --> 23:45.480
 And so from there, what happens is like most developers,

23:45.480 --> 23:47.240
 they do, you know, a little bit of testing,

23:47.240 --> 23:49.720
 but it's usually like, okay, did my compiler bug out?

23:49.720 --> 23:51.080
 Let me look at the warnings.

23:51.080 --> 23:52.840
 Okay, is that acceptable or not?

23:52.840 --> 23:53.680
 Right?

23:53.680 --> 23:55.760
 Like that's how you typically think about as a developer

23:55.760 --> 23:58.120
 and you're just assume that is going to go

23:58.120 --> 24:01.000
 to another process and they're gonna test it out.

24:01.000 --> 24:04.280
 But I think we need to go back to those early days

24:04.280 --> 24:07.520
 when, you know, you're a developer, you're developing.

24:07.520 --> 24:09.720
 There should be like this a, you know, okay,

24:09.720 --> 24:12.120
 let me look at the ethical outcomes of this

24:12.120 --> 24:16.000
 because there isn't a second like testing ethical testers,

24:16.000 --> 24:17.040
 right? It's you.

24:18.000 --> 24:21.120
 We did it back in the early coding days.

24:21.120 --> 24:23.240
 I think that's where we are with respect to ethics.

24:23.240 --> 24:26.240
 Like let's go back to what was good practices

24:26.240 --> 24:30.000
 and only because we were just developing the field.

24:30.000 --> 24:34.000
 Yeah, and it's a really heavy burden.

24:34.000 --> 24:37.520
 I've had to feel it recently in the last few months,

24:37.520 --> 24:39.880
 but I think it's a good one to feel like I've gotten

24:39.880 --> 24:43.720
 a message more than one from people, you know,

24:43.720 --> 24:47.440
 I've unfortunately gotten some attention recently

24:47.440 --> 24:51.040
 and I've gotten messages that say that I have blood

24:51.040 --> 24:56.040
 in my hands because of working on semi autonomous vehicles.

24:56.280 --> 24:59.560
 So the idea that you have semi autonomy means people

24:59.560 --> 25:01.960
 would become, would lose vigilance and so on.

25:01.960 --> 25:05.120
 That's actually be humans as we described.

25:05.120 --> 25:08.080
 And because of that, because of this idea

25:08.080 --> 25:10.000
 that we're creating automation,

25:10.000 --> 25:12.760
 there'll be people be hurt because of it.

25:12.760 --> 25:14.520
 And I think that's a beautiful thing.

25:14.520 --> 25:16.160
 I mean, it's, you know, there's many nights

25:16.160 --> 25:18.800
 where I wasn't able to sleep because of this notion.

25:18.800 --> 25:22.360
 You know, you really do think about people that might die

25:22.360 --> 25:23.800
 because of this technology.

25:23.800 --> 25:26.520
 Of course, you can then start rationalizing and saying,

25:26.520 --> 25:28.280
 well, you know what, 40,000 people die

25:28.280 --> 25:29.640
 in the United States every year

25:29.640 --> 25:32.400
 and we're trying to ultimately try to save lives.

25:32.400 --> 25:35.800
 But the reality is your code you've written

25:35.800 --> 25:37.920
 might kill somebody and that's an important burden

25:37.920 --> 25:40.160
 to carry with you as you design the code.

25:41.200 --> 25:43.800
 I don't even think of it as a burden

25:43.800 --> 25:47.560
 if we train this concept correctly from the beginning.

25:47.560 --> 25:50.320
 And I use, and not to say that coding is like

25:50.320 --> 25:52.400
 being a medical doctor, but think about it.

25:52.400 --> 25:56.080
 Medical doctors, if they've been in situations

25:56.080 --> 25:58.320
 where their patient didn't survive, right?

25:58.320 --> 26:00.800
 Do they give up and go away?

26:00.800 --> 26:02.480
 No, every time they come in,

26:02.480 --> 26:05.440
 they know that there might be a possibility

26:05.440 --> 26:07.240
 that this patient might not survive.

26:07.240 --> 26:10.080
 And so when they approach every decision,

26:10.080 --> 26:11.920
 like that's in the back of their head.

26:11.920 --> 26:15.840
 And so why isn't that we aren't teaching,

26:15.840 --> 26:17.200
 and those are tools though, right?

26:17.200 --> 26:19.680
 They are given some of the tools to address that

26:19.680 --> 26:21.440
 so that they don't go crazy.

26:21.440 --> 26:24.200
 But we don't give those tools

26:24.200 --> 26:26.160
 so that it does feel like a burden

26:26.160 --> 26:28.680
 versus something of I have a great gift

26:28.680 --> 26:31.080
 and I can do great, awesome good,

26:31.080 --> 26:33.320
 but with it comes great responsibility.

26:33.320 --> 26:35.840
 I mean, that's what we teach in terms of,

26:35.840 --> 26:37.400
 you think about the medical schools, right?

26:37.400 --> 26:39.520
 Great gift, great responsibility.

26:39.520 --> 26:42.120
 I think if we just change the messaging a little,

26:42.120 --> 26:45.560
 great gift being a developer, great responsibility.

26:45.560 --> 26:48.360
 And this is how you combine those.

26:48.360 --> 26:51.160
 But do you think, I mean, this is really interesting.

26:51.160 --> 26:54.320
 It's outside, I actually have no friends

26:54.320 --> 26:57.280
 who are sort of surgeons or doctors.

26:58.280 --> 27:00.000
 I mean, what does it feel like

27:00.000 --> 27:03.760
 to make a mistake in a surgery and somebody to die

27:03.760 --> 27:04.800
 because of that?

27:04.800 --> 27:07.000
 Like is that something you could be taught

27:07.000 --> 27:10.600
 in medical school sort of how to be accepting of that risk?

27:10.600 --> 27:14.960
 So because I do a lot of work with healthcare robotics,

27:14.960 --> 27:18.480
 I have not lost a patient, for example.

27:18.480 --> 27:20.880
 The first one's always the hardest, right?

27:20.880 --> 27:25.880
 But they really teach the value, right?

27:27.320 --> 27:28.760
 So they teach responsibility,

27:28.760 --> 27:30.800
 but they also teach the value.

27:30.800 --> 27:34.800
 Like you're saving 40,000,

27:34.800 --> 27:38.280
 but in order to really feel good about that,

27:38.280 --> 27:40.120
 when you come to a decision,

27:40.120 --> 27:42.280
 you have to be able to say at the end,

27:42.280 --> 27:45.320
 I did all that I could possibly do, right?

27:45.320 --> 27:49.160
 Versus a, well, I just picked the first widget, right?

27:49.160 --> 27:52.240
 Like, so every decision is actually thought through.

27:52.240 --> 27:53.800
 It's not a habit, it's not a,

27:53.800 --> 27:55.320
 let me just take the best algorithm

27:55.320 --> 27:57.080
 that my friend gave me, right?

27:57.080 --> 27:58.640
 It's a, is this it?

27:58.640 --> 27:59.520
 Is this the best?

27:59.520 --> 28:03.120
 Have I done my best to do good, right?

28:03.120 --> 28:03.960
 And so...

28:03.960 --> 28:06.400
 And I think burden is the wrong word.

28:06.400 --> 28:10.760
 It's a gift, but you have to treat it extremely seriously.

28:10.760 --> 28:11.600
 Correct.

28:13.280 --> 28:16.440
 So on a slightly related note, in a recent paper,

28:16.440 --> 28:20.160
 The Ugly Truth About Ourselves and Our Robot Creations,

28:20.160 --> 28:24.320
 you discuss, you highlight some biases

28:24.320 --> 28:27.120
 that may affect the function of various robotic systems.

28:27.120 --> 28:30.120
 Can you talk through, if you remember, examples of some?

28:30.120 --> 28:31.360
 There's a lot of examples.

28:31.360 --> 28:32.200
 I usually...

28:32.200 --> 28:33.040
 What is bias, first of all?

28:33.040 --> 28:37.080
 Yeah, so bias is this,

28:37.080 --> 28:38.840
 and so bias, which is different than prejudice.

28:38.840 --> 28:41.880
 So bias is that we all have these preconceived notions

28:41.880 --> 28:45.960
 about particular, everything from particular groups

28:45.960 --> 28:49.720
 to habits, to identity, right?

28:49.720 --> 28:51.400
 So we have these predispositions.

28:51.400 --> 28:54.080
 And so when we address a problem,

28:54.080 --> 28:56.040
 we look at a problem and make a decision,

28:56.040 --> 29:01.040
 those preconceived notions might affect our outputs,

29:01.320 --> 29:02.240
 our outcomes.

29:02.240 --> 29:04.680
 So there, the bias can be positive and negative,

29:04.680 --> 29:07.680
 and then it's prejudice, the negative kind of bias?

29:07.680 --> 29:09.160
 Prejudice is the negative, right?

29:09.160 --> 29:13.520
 So prejudice is that not only are you aware of your bias,

29:13.520 --> 29:18.520
 but you are then taken and have a negative outcome,

29:18.800 --> 29:20.640
 even though you are aware, like...

29:20.640 --> 29:22.920
 And there could be gray areas too.

29:22.920 --> 29:24.600
 There's always gray areas.

29:24.600 --> 29:27.520
 That's the challenging aspect of all ethical questions.

29:27.520 --> 29:29.960
 So I always like, so there's a funny one.

29:29.960 --> 29:31.720
 And in fact, I think it might be in the paper

29:31.720 --> 29:34.120
 because I think I talk about self driving cars.

29:34.120 --> 29:35.440
 But think about this.

29:35.440 --> 29:39.480
 We, for teenagers, right?

29:39.480 --> 29:44.480
 Typically, insurance companies charge quite a bit of money

29:44.520 --> 29:46.760
 if you have a teenage driver.

29:46.760 --> 29:50.840
 So you could say that's an age bias, right?

29:50.840 --> 29:54.040
 But no one will, I mean, parents will be grumpy,

29:54.040 --> 29:58.640
 but no one really says that that's not fair.

29:58.640 --> 29:59.480
 That's interesting.

29:59.480 --> 30:00.960
 We don't, that's right.

30:00.960 --> 30:01.800
 That's right.

30:01.800 --> 30:06.800
 It's everybody in human factors and safety research almost,

30:06.800 --> 30:10.480
 I mean, it's quite ruthlessly critical of teenagers.

30:11.480 --> 30:13.680
 And we don't question, is that okay?

30:13.680 --> 30:15.960
 Is that okay to be agist in this kind of way?

30:15.960 --> 30:17.280
 And it is age, right?

30:17.280 --> 30:19.560
 It's definitely age, there's no question about it.

30:19.560 --> 30:23.560
 And so this is the gray area, right?

30:23.560 --> 30:28.560
 Because you know that teenagers are more likely

30:28.560 --> 30:31.760
 to be in accidents and so there's actually some data to it.

30:31.760 --> 30:34.560
 But then if you take that same example and you say,

30:34.560 --> 30:38.160
 well, I'm going to make the insurance higher

30:38.160 --> 30:43.160
 for an area of Boston because there's a lot of accidents.

30:43.560 --> 30:47.000
 And then they find out that that's correlated

30:47.000 --> 30:51.000
 with socioeconomics, well, then it becomes a problem, right?

30:51.000 --> 30:53.640
 Like that is not acceptable,

30:53.640 --> 30:58.640
 but yet the teenager, which is age, it's against age is, right?

31:00.160 --> 31:02.600
 And the way we figure that out as society

31:02.600 --> 31:04.840
 by having conversations, by having discourse,

31:04.840 --> 31:07.040
 I mean, throughout history, the definition

31:07.040 --> 31:09.960
 of what is ethical or not has changed

31:09.960 --> 31:12.880
 and hopefully always for the better.

31:12.880 --> 31:14.120
 Correct, correct.

31:14.120 --> 31:19.120
 So in terms of bias or prejudice in robotic,

31:19.120 --> 31:24.120
 in algorithms, what examples do you sometimes think about?

31:24.120 --> 31:27.600
 So I think about quite a bit the medical domain

31:27.600 --> 31:29.960
 just because historically, right?

31:29.960 --> 31:33.360
 The health care domain has had these biases,

31:33.360 --> 31:38.360
 typically based on gender and ethnicity primarily,

31:38.760 --> 31:41.080
 a little on age, but not so much.

31:42.440 --> 31:47.440
 Historically, if you think about FDA and drug trials,

31:47.960 --> 31:52.960
 it's harder to find a woman that aren't childbearing

31:53.320 --> 31:55.680
 and so you may not test on drugs at the same level.

31:55.680 --> 31:57.760
 Right, so there's these things.

31:57.760 --> 32:01.680
 And so if you think about robotics, right?

32:01.680 --> 32:06.680
 Something as simple as, I like to design an exoskeleton, right?

32:06.680 --> 32:07.960
 What should the material be?

32:07.960 --> 32:08.880
 What should the weight be?

32:08.880 --> 32:11.480
 What should the form factor be?

32:12.480 --> 32:15.680
 Are you, who are you gonna design it around?

32:15.680 --> 32:17.920
 I will say that in the US,

32:17.920 --> 32:21.360
 women average height and weight is slightly different

32:21.360 --> 32:24.560
 than guys, so who are you gonna choose?

32:24.560 --> 32:28.240
 Like, if you're not thinking about it from the beginning

32:28.240 --> 32:32.840
 as, okay, when I design this and I look at the algorithms

32:32.840 --> 32:35.000
 and I design the control system and the forces

32:35.000 --> 32:37.400
 and the torques, if you're not thinking about,

32:37.400 --> 32:40.800
 well, you have different types of body structure,

32:40.800 --> 32:43.640
 you're gonna design to what you're used to.

32:43.640 --> 32:47.400
 Oh, this fits in all the folks in my lab, right?

32:47.400 --> 32:50.600
 So think about it from the very beginning as important.

32:50.600 --> 32:53.680
 What about sort of algorithms that train on data?

32:53.680 --> 32:56.080
 Data kind of thing.

32:56.080 --> 33:01.080
 Sadly, our society already has a lot of negative bias.

33:01.320 --> 33:03.280
 And so if we collect a lot of data,

33:04.760 --> 33:06.280
 even if it's a balanced way,

33:06.280 --> 33:07.800
 there's going to contain the same bias

33:07.800 --> 33:09.000
 that a society contains.

33:09.000 --> 33:13.720
 And so, yeah, is there things there that bother you?

33:13.720 --> 33:15.600
 Yeah, so you actually said something.

33:15.600 --> 33:19.920
 You had said how we have biases,

33:19.920 --> 33:23.120
 but hopefully we learn from them and we become better, right?

33:23.120 --> 33:25.120
 And so that's where we are now, right?

33:25.120 --> 33:28.200
 So the data that we're collecting is historic.

33:28.200 --> 33:30.120
 It's, so it's based on these things.

33:30.120 --> 33:32.600
 When we knew it was bad to discriminate,

33:32.600 --> 33:33.920
 but that's the data we have

33:33.920 --> 33:36.080
 and we're trying to fix it now,

33:36.080 --> 33:37.840
 but we're fixing it based on the data

33:37.840 --> 33:39.480
 that was used in the first place.

33:39.480 --> 33:40.640
 Fix it in post.

33:40.640 --> 33:43.720
 Right, and so the decisions,

33:43.720 --> 33:46.880
 and you can look at everything from the whole aspect

33:46.880 --> 33:51.400
 of predictive policing, criminal recidivism.

33:51.400 --> 33:54.320
 There was a recent paper that had the healthcare algorithms,

33:54.320 --> 33:58.280
 which had a kind of a sensational titles.

33:58.280 --> 34:01.200
 I'm not pro sensationalism in titles,

34:01.200 --> 34:03.720
 but again, you read it, right?

34:03.720 --> 34:05.760
 So it makes you read it,

34:05.760 --> 34:08.960
 but I'm like really like, ah, you could have...

34:08.960 --> 34:10.840
 What's the topic of the sensationalism?

34:10.840 --> 34:13.320
 I mean, what's underneath it?

34:13.320 --> 34:16.320
 What's, if you could sort of educate me

34:16.320 --> 34:19.160
 on what kind of bias creeps into the healthcare space.

34:19.160 --> 34:21.600
 Yeah, so... I mean, you already kind of mentioned...

34:21.600 --> 34:23.120
 Yeah, so this one was,

34:23.120 --> 34:27.520
 the headline was racist AI algorithms.

34:27.520 --> 34:30.880
 Okay, like, okay, that's totally a clickbait title.

34:30.880 --> 34:32.160
 And so you looked at it,

34:32.160 --> 34:36.680
 and so there was data that these researchers had collected.

34:36.680 --> 34:39.440
 I believe I want to say it was either science or nature.

34:39.440 --> 34:40.680
 It just was just published,

34:40.680 --> 34:42.640
 but they didn't have a sensational title.

34:42.640 --> 34:44.920
 It was like the media.

34:44.920 --> 34:47.520
 And so they had looked at demographics,

34:47.520 --> 34:52.200
 I believe, between black and white women, right?

34:52.200 --> 34:56.880
 And they showed that there was a discrepancy

34:56.880 --> 34:59.240
 in the outcomes, right?

34:59.240 --> 35:02.440
 And so, and it was tied to ethnicity, tied to race.

35:02.440 --> 35:04.840
 The piece that the researchers did

35:04.840 --> 35:08.840
 actually went through the whole analysis, but of course...

35:08.840 --> 35:12.120
 I mean, the journals with AI are problematic

35:12.120 --> 35:14.360
 across the board, let's say.

35:14.360 --> 35:16.200
 And so this is a problem, right?

35:16.200 --> 35:18.280
 And so there's this thing about,

35:18.280 --> 35:20.600
 oh, AI, it has all these problems,

35:20.600 --> 35:22.920
 we're doing it on historical data,

35:22.920 --> 35:26.080
 and the outcomes aren't even based on gender

35:26.080 --> 35:28.120
 or ethnicity or age.

35:28.120 --> 35:30.840
 But I'm always saying, it's like, yes,

35:30.840 --> 35:32.560
 we need to do better, right?

35:32.560 --> 35:33.680
 We need to do better.

35:33.680 --> 35:35.400
 It is our duty to do better,

35:36.840 --> 35:39.880
 but the worst AI is still better than us.

35:39.880 --> 35:42.000
 Like, you take the best of us,

35:42.000 --> 35:44.200
 and we're still worse than the worst AI,

35:44.200 --> 35:45.680
 at least in terms of these things.

35:45.680 --> 35:48.040
 And that's actually not discussed, right?

35:48.040 --> 35:52.000
 And so I think, and that's why the sensational title, right?

35:52.000 --> 35:54.360
 And so it's like, so then you can have individuals go like,

35:54.360 --> 35:55.600
 oh, we don't need to use this AI.

35:55.600 --> 35:56.840
 I'm like, oh, no, no, no, no.

35:56.840 --> 36:01.000
 I want the AI instead of the doctors

36:01.000 --> 36:02.080
 that provided that data,

36:02.080 --> 36:04.240
 because it's still better than that, right?

36:04.240 --> 36:06.840
 I think that's really important to linger on.

36:06.840 --> 36:09.640
 Is the idea that this AI is racist,

36:09.640 --> 36:14.920
 it's like, well, compared to what?

36:14.920 --> 36:19.920
 Sort of the, I think we set, unfortunately,

36:20.120 --> 36:23.240
 way too high of a bar for AI algorithms.

36:23.240 --> 36:24.760
 And in the ethical space,

36:24.760 --> 36:28.080
 where perfect is, I would argue, probably impossible.

36:28.920 --> 36:33.040
 Then if we set the bar of perfection, essentially,

36:33.040 --> 36:36.440
 of it has to be perfectly fair, whatever that means,

36:36.440 --> 36:39.640
 is it means we're setting it up for failure.

36:39.640 --> 36:42.000
 But that's really important to say what you just said,

36:42.000 --> 36:44.960
 which is, well, it's still better than it is.

36:44.960 --> 36:46.920
 And one of the things I think

36:46.920 --> 36:51.240
 that we don't get enough credit for just in terms of,

36:51.240 --> 36:55.880
 as developers, is that you can now poke at it, right?

36:55.880 --> 36:58.880
 So it's harder to say, is this hospital,

36:58.880 --> 37:01.080
 is this city doing something, right?

37:01.080 --> 37:04.400
 Until someone brings in a civil case, right?

37:04.400 --> 37:05.920
 Well, with AI, it can process

37:05.920 --> 37:09.720
 through all this data and say, hey, yes,

37:09.720 --> 37:14.520
 there's an issue here, but here it is, we've identified it.

37:14.520 --> 37:16.200
 And then the next step is to fix it.

37:16.200 --> 37:18.120
 I mean, that's a nice feedback loop,

37:18.120 --> 37:21.360
 versus like waiting for someone to sue someone else

37:21.360 --> 37:22.800
 before it's fixed, right?

37:22.800 --> 37:25.120
 And so I think that power,

37:25.120 --> 37:27.640
 we need to capitalize on a little bit more, right?

37:27.640 --> 37:31.520
 Instead of having the sensational titles, have the,

37:31.520 --> 37:34.600
 okay, this is a problem, and this is how we're fixing it.

37:34.600 --> 37:36.560
 And people are putting money to fix it

37:36.560 --> 37:38.640
 because we can make it better.

37:38.640 --> 37:43.000
 I look at like facial recognition, how Joy,

37:43.000 --> 37:45.840
 she basically called out a couple of companies and said,

37:45.840 --> 37:50.520
 hey, and most of them were like, oh, embarrassment.

37:50.520 --> 37:53.360
 And the next time it had been fixed, right?

37:53.360 --> 37:54.920
 It had been fixed better, right?

37:54.920 --> 37:56.840
 And then it was like, oh, here's some more issues.

37:56.840 --> 38:01.840
 And I think that conversation then moves that needle

38:01.840 --> 38:06.840
 to having much more of fear and unbiased and ethical aspects.

38:07.640 --> 38:10.640
 As long as both sides, the developers are willing to say,

38:10.640 --> 38:14.120
 okay, I hear you, yes, we are going to improve.

38:14.120 --> 38:16.120
 And you have other developers who are like,

38:16.120 --> 38:19.720
 hey, AI, it's wrong, but I love it, right?

38:19.720 --> 38:20.600
 Yes.

38:20.600 --> 38:25.480
 So speaking of this really nice notion that AI is maybe flawed,

38:25.480 --> 38:27.080
 but better than humans.

38:27.080 --> 38:29.200
 So just made me think of it,

38:29.200 --> 38:34.120
 one example of flawed humans is our political system.

38:34.120 --> 38:38.720
 Do you think, or you said judicial as well,

38:38.720 --> 38:43.720
 do you have a hope for AI sort of being elected

38:46.160 --> 38:49.800
 for president or running our Congress

38:49.800 --> 38:54.000
 or being able to be a powerful representative of the people?

38:54.000 --> 38:58.000
 So I mentioned, and I truly believe that

38:58.000 --> 39:01.440
 this whole world of AI is in partnerships with people.

39:01.440 --> 39:02.600
 And so what does that mean?

39:02.600 --> 39:07.600
 I don't believe or maybe I just don't,

39:07.800 --> 39:11.600
 I don't believe that we should have an AI for president,

39:11.600 --> 39:14.640
 but I do believe that a president should use AI

39:14.640 --> 39:16.080
 as an advisor, right?

39:16.080 --> 39:17.560
 Like if you think about it,

39:17.560 --> 39:22.080
 every president has a cabinet of individuals

39:22.080 --> 39:23.840
 that have different expertise

39:23.840 --> 39:26.200
 that they should listen to, right?

39:26.200 --> 39:28.160
 Like that's kind of what we do.

39:28.160 --> 39:31.280
 And you put smart people with smart expertise

39:31.280 --> 39:33.600
 around certain issues and you listen.

39:33.600 --> 39:35.840
 I don't see why AI can't function

39:35.840 --> 39:39.400
 as one of those smart individuals giving input.

39:39.400 --> 39:41.200
 So maybe there's an AI on healthcare,

39:41.200 --> 39:44.000
 maybe there's an AI on education and right?

39:44.000 --> 39:48.920
 Like all of these things that a human is processing, right?

39:48.920 --> 39:53.720
 Because at the end of the day there's people that are human

39:53.720 --> 39:55.680
 that are going to be at the end of the decision.

39:55.680 --> 39:59.480
 And I don't think as a world, as a culture, as a society

39:59.480 --> 40:03.200
 that we would totally, and this is us,

40:03.200 --> 40:05.480
 like this is some fallacy about us,

40:05.480 --> 40:10.480
 but we need to see that leader, that person as human.

40:12.000 --> 40:15.600
 And most people don't realize that like leaders

40:15.600 --> 40:17.160
 have a whole lot of advice, right?

40:17.160 --> 40:18.360
 Like when they say something,

40:18.360 --> 40:19.760
 it's not that they woke up,

40:19.760 --> 40:22.000
 well usually they don't wake up in the morning

40:22.000 --> 40:24.560
 and be like, I have a brilliant idea, right?

40:24.560 --> 40:26.840
 It's usually a, okay, let me listen.

40:26.840 --> 40:27.680
 I have a brilliant idea

40:27.680 --> 40:31.160
 but let me get a little bit of feedback on this, like, okay.

40:31.160 --> 40:33.240
 And then it's a, yeah, that was an awesome idea

40:33.240 --> 40:36.000
 or it's like, yeah, let me go back.

40:36.000 --> 40:37.520
 We already talked to a bunch of them,

40:37.520 --> 40:41.560
 but are there some possible solutions

40:41.560 --> 40:45.320
 to the biases present in our algorithms

40:45.320 --> 40:46.760
 beyond what we just talked about?

40:46.760 --> 40:49.400
 So I think there's two paths.

40:49.400 --> 40:53.840
 One is to figure out how to systematically

40:53.840 --> 40:56.600
 do the feedback and correction.

40:56.600 --> 40:58.240
 So right now it's ad hoc, right?

40:58.240 --> 41:02.520
 It's a researcher identify some outcomes

41:02.520 --> 41:05.480
 that are not, don't seem to be fair, right?

41:05.480 --> 41:08.000
 They publish it, they write about it

41:08.000 --> 41:10.600
 and the, either the developer

41:10.600 --> 41:13.200
 or the companies that have adopted the algorithms

41:13.200 --> 41:14.320
 may try to fix it, right?

41:14.320 --> 41:18.920
 And so it's really ad hoc and it's not systematic.

41:18.920 --> 41:22.520
 There's, it's just, it's kind of like, I'm a researcher,

41:22.520 --> 41:24.720
 that seems like an interesting problem,

41:24.720 --> 41:26.560
 which means that there's a whole lot out there

41:26.560 --> 41:29.160
 that's not being looked at, right?

41:29.160 --> 41:31.080
 Because it's kind of researcher driven.

41:32.960 --> 41:35.680
 And I don't necessarily have a solution,

41:35.680 --> 41:40.680
 but that process, I think could be done a little bit better.

41:41.240 --> 41:45.040
 One way is I'm going to poke a little bit

41:45.040 --> 41:48.280
 at some of the corporations, right?

41:48.280 --> 41:51.720
 Like maybe the corporations, when they think about a product,

41:51.720 --> 41:53.920
 they should, instead of,

41:53.920 --> 41:58.920
 in addition to hiring these, bug, they give these...

41:59.880 --> 42:01.600
 Oh yeah, yeah, yeah.

42:01.600 --> 42:02.960
 Like awards when you find a bug.

42:02.960 --> 42:07.200
 Yeah, security bug, you know, let's put it like,

42:07.200 --> 42:09.760
 we will give the, whatever the award is

42:09.760 --> 42:12.600
 that we give for the people who find these security holes,

42:12.600 --> 42:14.000
 find an ethics hole, right?

42:14.000 --> 42:15.400
 Like find an unfairness hole

42:15.400 --> 42:17.840
 and we will pay you X for each one you find.

42:17.840 --> 42:19.800
 I mean, why can't they do that?

42:19.800 --> 42:23.080
 One is a win win, they show that they're concerned about it,

42:23.080 --> 42:24.320
 that this is important,

42:24.320 --> 42:26.360
 and they don't have to necessarily dedicate

42:26.360 --> 42:28.920
 their own internal resources.

42:28.920 --> 42:32.800
 And it also means that everyone who has their own bias lens,

42:32.800 --> 42:34.600
 like I'm interested in age,

42:34.600 --> 42:36.560
 and so I'll find the ones based on age,

42:36.560 --> 42:38.400
 and I'm interested in gender, right?

42:38.400 --> 42:41.560
 Which means that you get all of these different perspectives.

42:41.560 --> 42:43.360
 But you think of it in a data driven way.

42:43.360 --> 42:48.360
 So like, sort of, if we look at a company like Twitter,

42:48.360 --> 42:53.040
 it's under a lot of fire for discriminating

42:53.040 --> 42:54.840
 against certain political beliefs.

42:54.840 --> 42:55.920
 Correct.

42:55.920 --> 42:58.120
 And sort of, there's a lot of people,

42:58.120 --> 42:59.280
 this is the sad thing,

42:59.280 --> 43:00.760
 because I know how hard the problem is,

43:00.760 --> 43:03.120
 and I know the Twitter folks are working really hard at it,

43:03.120 --> 43:05.000
 even Facebook, that everyone seems to hate

43:05.000 --> 43:06.920
 are working really hard at this.

43:06.920 --> 43:09.360
 You know, the kind of evidence that people bring

43:09.360 --> 43:11.280
 is basically anecdotal evidence.

43:11.280 --> 43:15.040
 Well, me or my friend, all we said is X,

43:15.040 --> 43:17.160
 and for that we got banned.

43:17.160 --> 43:20.920
 And that's kind of a discussion of saying,

43:20.920 --> 43:23.240
 well, look, that's usually, first of all,

43:23.240 --> 43:25.480
 the whole thing is taken out of context.

43:25.480 --> 43:28.640
 So they present sort of anecdotal evidence.

43:28.640 --> 43:31.120
 And how are you supposed to, as a company,

43:31.120 --> 43:33.040
 in a healthy way have a discourse

43:33.040 --> 43:35.720
 about what is and isn't ethical?

43:35.720 --> 43:38.040
 What, how do we make algorithms ethical

43:38.040 --> 43:40.760
 when people are just blowing everything?

43:40.760 --> 43:45.120
 Like, they're outraged about a particular

43:45.120 --> 43:48.200
 anecdotal piece of evidence that's very difficult

43:48.200 --> 43:51.640
 to sort of contextualize in a big data driven way.

43:52.640 --> 43:55.760
 Do you have a hope for companies like Twitter and Facebook?

43:55.760 --> 43:59.800
 Yeah, so I think there's a couple of things going on, right?

43:59.800 --> 44:04.800
 First off, the, remember this whole aspect

44:04.840 --> 44:09.400
 of we are becoming reliant on technology,

44:09.400 --> 44:14.360
 we're also becoming reliant on a lot of these,

44:14.360 --> 44:18.000
 the apps and the resources that are provided, right?

44:18.000 --> 44:21.640
 So some of it is kind of anger, like, I need you, right?

44:21.640 --> 44:23.120
 And you're not working for me, right?

44:23.120 --> 44:24.640
 Yeah, you're not working for me, they're right.

44:24.640 --> 44:27.280
 But I think, and so some of it,

44:27.280 --> 44:31.400
 and I wish that there was a little bit

44:31.400 --> 44:32.840
 of change of rethinking.

44:32.840 --> 44:35.560
 So some of it is like, oh, we'll fix it in house.

44:35.560 --> 44:39.000
 No, that's like, okay, I'm a fox

44:39.000 --> 44:40.960
 and I'm going to watch these hens

44:40.960 --> 44:44.080
 because I think it's a problem that foxes eat hens.

44:44.080 --> 44:45.160
 No, right?

44:45.160 --> 44:48.840
 Like use, like be good citizens and say, look,

44:48.840 --> 44:53.840
 we have a problem and we are willing to open ourselves up

44:54.800 --> 44:57.040
 for others to come in and look at it

44:57.040 --> 44:58.720
 and not try to fix it in house.

44:58.720 --> 45:01.960
 Because if you fix it in house, there's conflict of interest.

45:01.960 --> 45:04.440
 If I find something, I'm probably going to want to fix it

45:04.440 --> 45:07.320
 and hopefully the media won't pick it up, right?

45:07.320 --> 45:09.320
 And that then caused this distrust

45:09.320 --> 45:11.880
 because someone inside is going to be mad at you

45:11.880 --> 45:13.600
 and go out and talk about how,

45:13.600 --> 45:16.440
 yeah, they can the resume survey

45:16.440 --> 45:19.320
 because it rightly be best people.

45:19.320 --> 45:22.760
 Like just say, look, we had this issue.

45:22.760 --> 45:24.440
 Community, help us fix it.

45:24.440 --> 45:25.800
 And we will give you like, you know,

45:25.800 --> 45:28.120
 the bug finder fee if you do.

45:28.120 --> 45:31.280
 So do you ever hope that the community,

45:31.280 --> 45:35.360
 us as a human civilization on the whole is good

45:35.360 --> 45:38.520
 and can be trusted to guide the future

45:38.520 --> 45:40.960
 of our civilization into positive direction?

45:40.960 --> 45:41.880
 I think so.

45:41.880 --> 45:44.120
 So I'm an optimist, right?

45:44.120 --> 45:49.120
 And, you know, there were some dark times in history always.

45:50.000 --> 45:52.920
 I think now we're in one of those dark times.

45:52.920 --> 45:53.760
 I truly do.

45:53.760 --> 45:54.600
 In which aspect?

45:54.600 --> 45:56.240
 The polarization.

45:56.240 --> 45:57.560
 And it's not just US, right?

45:57.560 --> 46:00.040
 So if it was just US, I'd be like, yeah, it's a US thing.

46:00.040 --> 46:03.480
 But we're seeing it like worldwide this polarization.

46:04.360 --> 46:06.560
 And so I worry about that.

46:06.560 --> 46:11.560
 But I do fundamentally believe that at the end of the day,

46:12.000 --> 46:13.440
 people are good, right?

46:13.440 --> 46:14.760
 And why do I say that?

46:14.760 --> 46:17.680
 Because anytime there's a scenario

46:17.680 --> 46:20.800
 where people are in danger, and I will use,

46:20.800 --> 46:24.240
 so Atlanta, we had a snowmageddon

46:24.240 --> 46:26.600
 and people can laugh about that.

46:26.600 --> 46:30.440
 People at the time, so the city closed for, you know,

46:30.440 --> 46:33.440
 little snow, but it was ice and the city closed down.

46:33.440 --> 46:35.680
 But you had people opening up their homes and saying,

46:35.680 --> 46:37.760
 hey, you have nowhere to go.

46:37.760 --> 46:39.000
 Come to my house, right?

46:39.000 --> 46:41.760
 Hotels were just saying like, sleep on the floor.

46:41.760 --> 46:44.360
 Like places like, you know, the grocery stores were like,

46:44.360 --> 46:45.880
 hey, here's food.

46:45.880 --> 46:47.880
 There was no like, oh, how much are you gonna pay me?

46:47.880 --> 46:50.440
 It was like this, such a community.

46:50.440 --> 46:52.080
 And like people who didn't know each other,

46:52.080 --> 46:55.440
 strangers were just like, can I give you a ride home?

46:55.440 --> 46:57.760
 And that was a point I was like, you know what?

46:57.760 --> 46:59.600
 Like.

46:59.600 --> 47:02.000
 That reveals that the deeper thing

47:02.000 --> 47:06.840
 is there's a compassion or love that we all have within us.

47:06.840 --> 47:09.400
 It's just that when all of that is taken care of

47:09.400 --> 47:11.120
 and get bored, we love drama.

47:11.120 --> 47:12.440
 Yes.

47:12.440 --> 47:15.240
 And that's, I think almost like the division is a sign

47:15.240 --> 47:18.960
 of the time is being good, is that it's just entertaining

47:18.960 --> 47:23.960
 on some unpleasant mammalian level to watch,

47:24.120 --> 47:26.040
 to disagree with others.

47:26.040 --> 47:30.160
 And Twitter and Facebook are actually taking advantage

47:30.160 --> 47:33.120
 of that in a sense because it brings you back

47:33.120 --> 47:36.040
 to the platform and their advertises are driven

47:36.040 --> 47:37.520
 so they make a lot of money.

47:37.520 --> 47:39.160
 So you go back and you flick.

47:39.160 --> 47:43.560
 Love doesn't sell quite as well in terms of advertisement.

47:43.560 --> 47:44.800
 It doesn't.

47:44.800 --> 47:46.840
 So you've started your career

47:46.840 --> 47:49.000
 at NASA Jet Propulsion Laboratory.

47:49.000 --> 47:51.880
 But before I ask a few questions there,

47:51.880 --> 47:54.320
 have you happened to have ever seen Space Odyssey,

47:54.320 --> 47:55.800
 2001 Space Odyssey?

47:57.080 --> 47:57.840
 Yes.

47:57.840 --> 48:01.400
 Okay, do you think HAL 9000?

48:01.400 --> 48:03.360
 So we're talking about ethics.

48:03.360 --> 48:06.640
 Do you think HAL did the right thing

48:06.640 --> 48:08.520
 by taking the priority of the mission

48:08.520 --> 48:10.200
 over the lives of the astronauts?

48:10.200 --> 48:12.360
 Do you think HAL is good or evil?

48:15.880 --> 48:16.880
 Easy questions.

48:16.880 --> 48:17.720
 Yeah.

48:19.360 --> 48:21.360
 HAL was misguided.

48:21.360 --> 48:24.040
 You're one of the people that would be in charge

48:24.040 --> 48:26.280
 of an algorithm like HAL.

48:26.280 --> 48:28.240
 So how would you do better?

48:28.240 --> 48:32.200
 If you think about what happened was

48:32.200 --> 48:35.280
 there was no fail safe, right?

48:35.280 --> 48:37.680
 So we, perfection, right?

48:37.680 --> 48:38.520
 Like what is that?

48:38.520 --> 48:40.760
 I'm gonna make something that I think is perfect.

48:40.760 --> 48:44.520
 But if my assumptions are wrong,

48:44.520 --> 48:47.480
 it'll be perfect based on the wrong assumptions, right?

48:47.480 --> 48:50.400
 That's something that you don't know

48:50.400 --> 48:53.720
 until you deploy and then you're like, oh yeah, messed up.

48:53.720 --> 48:57.440
 But what that means is that when we design software

48:57.440 --> 48:59.640
 such as in Space Odyssey,

48:59.640 --> 49:03.160
 when we put things out that there has to be a fail safe.

49:03.160 --> 49:06.560
 There has to be the ability that once it's out there,

49:06.560 --> 49:10.280
 we can grade it as an F and it fails

49:10.280 --> 49:12.120
 and it doesn't continue, right?

49:12.120 --> 49:15.120
 There's some way that it can be brought in

49:15.120 --> 49:18.600
 and removed and that's aspect.

49:18.600 --> 49:20.120
 Because that's what happened with HAL.

49:20.120 --> 49:22.600
 It was like assumptions were wrong.

49:22.600 --> 49:27.040
 It was perfectly correct based on those assumptions

49:27.040 --> 49:30.280
 and there was no way to change it,

49:30.280 --> 49:33.240
 change the assumptions at all.

49:33.240 --> 49:36.240
 And the change, the fallback would be to humans.

49:36.240 --> 49:39.200
 You ultimately think like humans should be,

49:41.040 --> 49:44.840
 it's not turtles or AI all the way down.

49:44.840 --> 49:46.440
 It's at some point, there's a human

49:46.440 --> 49:47.280
 that actually makes this change.

49:47.280 --> 49:49.040
 I still think that, and again,

49:49.040 --> 49:50.680
 because I do human robot interaction,

49:50.680 --> 49:53.480
 I still think the human needs to be part

49:53.480 --> 49:55.880
 of the equation at some point.

49:55.880 --> 49:57.880
 So what, just looking back,

49:57.880 --> 50:01.280
 what are some fascinating things in robotic space

50:01.280 --> 50:02.880
 that NASA was working at the time?

50:02.880 --> 50:07.080
 Or just in general, what have you gotten to play with

50:07.080 --> 50:09.480
 and what are your memories from working at NASA?

50:09.480 --> 50:14.000
 Yeah, so one of my first memories was,

50:14.000 --> 50:18.280
 they were working on a surgical robot system

50:18.280 --> 50:21.840
 that could do eye surgery, right?

50:21.840 --> 50:23.920
 And this was back in, oh my gosh,

50:23.920 --> 50:28.920
 it must have been, oh, maybe 92, 93, 94.

50:30.560 --> 50:32.840
 So it's like almost like a remote operation.

50:32.840 --> 50:34.520
 Yeah, it was remote operation.

50:34.520 --> 50:38.360
 And in fact, you can even find some old tech reports on it.

50:38.360 --> 50:41.600
 So think of it, like now we have DaVinci, right?

50:41.600 --> 50:45.840
 Like think of it, but these were like the late 90s, right?

50:45.840 --> 50:48.200
 And I remember going into the lab one day

50:48.200 --> 50:50.960
 and I was like, what's that, right?

50:50.960 --> 50:53.880
 And of course it wasn't pretty, right?

50:53.880 --> 50:56.600
 Cause the technology, but it was like functional.

50:56.600 --> 50:59.160
 And you had this individual that could use

50:59.160 --> 51:01.880
 the version of haptics to actually do the surgery.

51:01.880 --> 51:05.520
 And they had this mockup of a human face and like the eyeballs

51:05.520 --> 51:08.360
 and you can see this little drill.

51:08.360 --> 51:11.640
 And I was like, oh, that is so cool.

51:11.640 --> 51:13.640
 That one I vividly remember

51:13.640 --> 51:18.560
 because it was so outside of my like possible thoughts

51:18.560 --> 51:19.960
 of what could be done.

51:19.960 --> 51:21.280
 It's the kind of precision.

51:21.280 --> 51:26.040
 And I mean, what's the most amazing of a thing like that?

51:26.040 --> 51:28.160
 I think it was the precision.

51:28.160 --> 51:31.880
 It was the kind of first time

51:31.880 --> 51:36.880
 that I had physically seen this robot machine,

51:37.440 --> 51:39.560
 human interface, right?

51:39.560 --> 51:42.320
 Versus, cause manufacturing had been,

51:42.320 --> 51:44.480
 you saw those kind of big robots, right?

51:44.480 --> 51:48.000
 But this was like, oh, this is in a person.

51:48.000 --> 51:51.360
 There's a person and a robot like in the same space.

51:51.360 --> 51:52.960
 The meeting them in person.

51:52.960 --> 51:55.960
 Like for me, it was a magical moment that I can't,

51:55.960 --> 51:58.880
 as life transforming that I recently met

51:58.880 --> 52:00.600
 Spotmini from Boston Dynamics.

52:00.600 --> 52:01.440
 Oh, see.

52:01.440 --> 52:04.640
 I don't know why, but on the human robot interaction,

52:04.640 --> 52:07.720
 for some reason I realized how easy it is

52:07.720 --> 52:09.760
 to anthropomorphize.

52:09.760 --> 52:13.440
 And it was, I don't know, it was almost like falling in love

52:13.440 --> 52:14.720
 with this feeling of meeting.

52:14.720 --> 52:18.200
 And I've obviously seen these robots a lot in video and so on,

52:18.200 --> 52:20.920
 but meeting in person, just having that one on one time

52:20.920 --> 52:21.760
 is different.

52:21.760 --> 52:22.600
 It's different.

52:22.600 --> 52:25.080
 So have you had a robot like that in your life

52:25.080 --> 52:28.320
 that made you maybe fall in love with robotics?

52:28.320 --> 52:30.520
 Sort of like meeting in person?

52:32.160 --> 52:35.040
 I mean, I loved robotics.

52:35.040 --> 52:35.880
 From the beginning.

52:35.880 --> 52:37.920
 Yeah, so that was a 12 year old,

52:37.920 --> 52:39.520
 like I'm gonna be a roboticist.

52:39.520 --> 52:41.240
 Actually was, I called it cybernetics,

52:41.240 --> 52:44.760
 but so my motivation was Bionic Woman.

52:44.760 --> 52:46.320
 I don't know if you know that.

52:46.320 --> 52:49.560
 And so, I mean, that was like a seminal moment,

52:49.560 --> 52:52.400
 but I didn't meet like that was TV, right?

52:52.400 --> 52:54.600
 Like it wasn't like I was in the same space and I met,

52:54.600 --> 52:56.600
 I was like, oh my gosh, you're like real.

52:56.600 --> 52:58.880
 Just looking at Bionic Woman, which by the way,

52:58.880 --> 53:03.280
 because I read that about you, I watched a bit of it

53:03.280 --> 53:05.640
 and it's just so, no offense, terrible.

53:05.640 --> 53:08.400
 It's cheesy, look at it now.

53:08.400 --> 53:09.240
 It's cheesy.

53:09.240 --> 53:11.560
 I've seen a couple of reruns lately.

53:11.560 --> 53:15.320
 But of course at the time, it's probably

53:15.320 --> 53:16.640
 a captured imagination.

53:16.640 --> 53:17.480
 But this is out of fix.

53:17.480 --> 53:18.320
 I shouldn't.

53:20.080 --> 53:23.120
 Especially when you're younger, just capture you.

53:23.120 --> 53:24.720
 But which aspect, did you think of it,

53:24.720 --> 53:27.720
 you mentioned cybernetics, did you think of it as robotics

53:27.720 --> 53:30.120
 or did you think of it as almost constructing

53:30.120 --> 53:31.640
 artificial beings?

53:31.640 --> 53:34.160
 Like is it the intelligent part

53:34.160 --> 53:37.000
 that captured your fascination

53:37.000 --> 53:38.040
 or was it the whole thing?

53:38.040 --> 53:39.800
 Like even just the limbs and just the.

53:39.800 --> 53:42.880
 So for me, it would have, in another world,

53:42.880 --> 53:46.800
 I probably would have been more of a biomedical engineer

53:46.800 --> 53:50.000
 because what fascinated me was the parts,

53:50.000 --> 53:55.000
 like the bionic parts, the limbs, those aspects of it.

53:55.040 --> 53:57.120
 Are you especially drawn to humanoid

53:57.120 --> 53:59.600
 or human like robots?

53:59.600 --> 54:03.040
 I would say human like, not humanoid, right?

54:03.040 --> 54:05.880
 And when I say human like, I think it's this aspect

54:05.880 --> 54:09.160
 of that interaction, whether it's social

54:09.160 --> 54:10.680
 and it's like a dog, right?

54:10.680 --> 54:14.120
 Like that's human like, because it understand us,

54:14.120 --> 54:17.600
 it interacts with us at that very social level

54:18.480 --> 54:21.880
 to, you know, humanoid is a part of that,

54:21.880 --> 54:26.880
 but only if they interact with us as if we are human.

54:27.880 --> 54:30.920
 But just to linger on NASA for a little bit,

54:30.920 --> 54:34.080
 what do you think maybe if you have other memories,

54:34.080 --> 54:35.920
 but also what do you think is the future

54:35.920 --> 54:38.560
 of robots in space?

54:38.560 --> 54:41.880
 We mentioned how, but there's incredible robots

54:41.880 --> 54:43.400
 that NASA is working on in general,

54:43.400 --> 54:48.160
 thinking about in our, as we venture out,

54:48.160 --> 54:50.440
 human civilization ventures out into space.

54:50.440 --> 54:52.240
 What do you think the future of robots is there?

54:52.240 --> 54:53.680
 Yeah, so I mean, there's the near term.

54:53.680 --> 54:57.280
 For example, they just announced the rover

54:57.280 --> 55:00.760
 that's going to the moon, which, you know,

55:00.760 --> 55:05.200
 that's kind of exciting, but that's like near term.

55:06.040 --> 55:11.040
 You know, my favorite, favorite, favorite series

55:11.120 --> 55:13.280
 is Star Trek, right?

55:13.280 --> 55:17.160
 You know, I really hope and even Star Trek,

55:17.160 --> 55:20.080
 like if I calculate the years, I wouldn't be alive,

55:20.080 --> 55:25.080
 but I would really, really love to be in that world.

55:26.680 --> 55:28.440
 Like even if it's just at the beginning,

55:28.440 --> 55:33.160
 like, you know, like voyage, like adventure one.

55:33.160 --> 55:35.720
 So basically living in space.

55:35.720 --> 55:36.560
 Yeah.

55:36.560 --> 55:39.720
 With what robots, what are robots?

55:39.720 --> 55:40.560
 The data.

55:40.560 --> 55:41.400
 What role?

55:41.400 --> 55:42.840
 The data would have to be, even though that wasn't,

55:42.840 --> 55:44.760
 you know, that was like later, but.

55:44.760 --> 55:49.160
 So data is a robot that has human like qualities.

55:49.160 --> 55:51.080
 Right, without the emotion ship, yeah.

55:51.080 --> 55:52.240
 You don't like emotion in your robots.

55:52.240 --> 55:58.560
 Well, so data with the emotion ship was kind of a mess, right?

55:58.560 --> 56:03.560
 It took a while for that, for him to adapt.

56:04.640 --> 56:08.600
 But, and so why was that an issue?

56:08.600 --> 56:13.600
 The issue is, is that emotions make us irrational agents.

56:14.240 --> 56:15.240
 That's the problem.

56:16.280 --> 56:20.040
 And yet he could think through things,

56:20.040 --> 56:23.440
 even if it was based on an emotional scenario, right?

56:23.440 --> 56:25.080
 Based on pros and cons.

56:25.080 --> 56:28.520
 But as soon as you made him emotional,

56:28.520 --> 56:31.160
 one of the metrics he used for evaluation

56:31.160 --> 56:33.280
 was his own emotions.

56:33.280 --> 56:35.480
 Not people around him, right?

56:35.480 --> 56:37.280
 Like, and so.

56:37.280 --> 56:39.040
 We do that as children, right?

56:39.040 --> 56:40.880
 So we're very egocentric when we're young.

56:40.880 --> 56:42.320
 We are very egocentric.

56:42.320 --> 56:44.920
 And so, isn't that just an early version

56:44.920 --> 56:46.400
 of the emotion ship then?

56:46.400 --> 56:48.280
 I haven't watched much Star Trek.

56:48.280 --> 56:52.480
 Except I have also met adults, right?

56:52.480 --> 56:54.640
 And so that is a developmental process.

56:54.640 --> 56:57.640
 And I'm sure there's a bunch of psychologists

56:57.640 --> 57:00.680
 that can go through, like you can have a 60 year old adult

57:00.680 --> 57:04.680
 who has the emotional maturity of a 10 year old, right?

57:04.680 --> 57:07.000
 And so there's various phases

57:07.000 --> 57:10.000
 that people should go through in order to evolve.

57:10.000 --> 57:11.480
 And sometimes you don't.

57:11.480 --> 57:14.880
 So how much psychology do you think

57:14.880 --> 57:17.640
 a topic that's rarely mentioned in robotics,

57:17.640 --> 57:19.720
 but how much does psychology come to play

57:19.720 --> 57:23.600
 when you're talking about HRI, human robot interaction?

57:23.600 --> 57:25.000
 When you have to have robots

57:25.000 --> 57:26.160
 that actually interact with you?

57:26.160 --> 57:27.000
 Tons.

57:27.000 --> 57:31.360
 So we, like my group, as well as I read a lot

57:31.360 --> 57:33.280
 in the cognitive science literature

57:33.280 --> 57:36.160
 as well as the psychology literature.

57:36.160 --> 57:41.160
 Because they understand a lot about human human relations

57:42.720 --> 57:45.960
 and developmental milestones and things like that.

57:45.960 --> 57:50.960
 And so we tend to look to see what's been done out there.

57:53.120 --> 57:56.520
 Sometimes what we'll do is we'll try to match that to see

57:56.520 --> 58:01.000
 is that human human relationship the same as human robot?

58:01.000 --> 58:03.080
 Sometimes it is and sometimes it's different.

58:03.080 --> 58:04.760
 And then when it's different, we have to,

58:04.760 --> 58:06.440
 we try to figure out, okay,

58:06.440 --> 58:09.040
 why is it different in this scenario?

58:09.040 --> 58:11.920
 But it's the same in the other scenario, right?

58:11.920 --> 58:15.320
 And so we try to do that quite a bit.

58:15.320 --> 58:16.360
 Would you say that's,

58:16.360 --> 58:19.120
 if we're looking at the future of human robot interaction,

58:19.120 --> 58:22.040
 would you say the psychology piece is the hardest?

58:22.040 --> 58:25.440
 Like if, I mean, it's a funny notion for you as,

58:25.440 --> 58:27.360
 I don't know if you consider, yeah.

58:27.360 --> 58:28.400
 I mean, one way to ask it,

58:28.400 --> 58:32.000
 do you consider yourself a roboticist or a psychologist?

58:32.000 --> 58:33.600
 Oh, I consider myself a roboticist

58:33.600 --> 58:36.200
 that plays the act of a psychologist.

58:36.200 --> 58:39.760
 But if you were to look at yourself sort of,

58:39.760 --> 58:42.360
 you know, 20, 30 years from now,

58:42.360 --> 58:45.360
 do you see yourself more and more wearing the psychology hat?

58:47.200 --> 58:49.000
 Sort of another way to put it is,

58:49.000 --> 58:51.600
 are the hard problems in human robot interactions,

58:51.600 --> 58:53.600
 fundamentally psychology,

58:53.600 --> 58:55.800
 or is it still robotics,

58:55.800 --> 58:57.320
 the perception manipulation,

58:57.320 --> 58:59.480
 planning all that kind of stuff?

58:59.480 --> 59:01.680
 It's actually neither.

59:01.680 --> 59:05.160
 The hardest part is the adaptation and the interaction.

59:06.120 --> 59:08.880
 So it's the interface, it's the learning.

59:08.880 --> 59:11.600
 And so if I think of,

59:11.600 --> 59:16.600
 like I've become much more of a roboticist slash AI person

59:17.200 --> 59:19.040
 than when I, like originally, again,

59:19.040 --> 59:20.160
 I was about the bionics.

59:20.160 --> 59:23.400
 I was electrical engineer, I was control theory, right?

59:23.400 --> 59:25.560
 Like, and then I started realizing

59:25.560 --> 59:30.560
 that my algorithms needed like human data, right?

59:30.600 --> 59:32.520
 And so then I was like, okay, what is this human thing?

59:32.520 --> 59:34.360
 Right, how do I incorporate human data?

59:34.360 --> 59:38.440
 And then I realized that human perception had,

59:38.440 --> 59:41.040
 like there was a lot in terms of how we perceive the world.

59:41.040 --> 59:41.960
 And so trying to figure out,

59:41.960 --> 59:44.440
 how do I model human perception from my,

59:44.440 --> 59:47.600
 and so I became a HRI person,

59:47.600 --> 59:49.360
 human robot interaction person,

59:49.360 --> 59:51.800
 from being a control theory and realizing

59:51.800 --> 59:54.360
 that humans actually offered quite a bit.

59:55.240 --> 59:56.080
 And then when you do that,

59:56.080 --> 59:59.200
 you become more of an artificial intelligence AI.

59:59.200 --> 1:00:04.200
 And so I see myself evolving more in this AI world

1:00:05.720 --> 1:00:09.560
 under the lens of robotics

1:00:09.560 --> 1:00:12.120
 having hardware interacting with people.

1:00:12.120 --> 1:00:17.120
 So you're a world class expert researcher in robotics

1:00:17.880 --> 1:00:21.880
 and yet others, there's a few, it's a small,

1:00:21.880 --> 1:00:24.160
 but fierce community of people,

1:00:24.160 --> 1:00:26.600
 but most of them don't take the journey

1:00:26.600 --> 1:00:29.440
 into the age of HRI, into the human.

1:00:29.440 --> 1:00:34.440
 So why did you brave into the interaction with humans?

1:00:34.440 --> 1:00:36.880
 It seems like a really hard problem.

1:00:36.880 --> 1:00:40.080
 It's a hard problem and it's very risky as an academic.

1:00:41.080 --> 1:00:45.200
 And I knew that when I started down that journey

1:00:46.200 --> 1:00:50.600
 that it was very risky as an academic in this world

1:00:50.600 --> 1:00:53.440
 that was nuanced, it was just developing.

1:00:53.440 --> 1:00:55.200
 We didn't even have a conference, right?

1:00:55.200 --> 1:00:56.720
 At the time.

1:00:56.720 --> 1:01:00.080
 Because it was the interesting problems.

1:01:00.080 --> 1:01:01.520
 That was what drove me.

1:01:01.520 --> 1:01:06.520
 It was the fact that I looked at what interests me

1:01:06.880 --> 1:01:10.360
 in terms of the application space and the problems.

1:01:10.360 --> 1:01:14.880
 And that pushed me into trying to figure out

1:01:14.880 --> 1:01:16.840
 what people were and what humans were

1:01:16.840 --> 1:01:19.000
 and how to adapt to them.

1:01:19.000 --> 1:01:21.240
 If those problems weren't so interesting,

1:01:22.160 --> 1:01:26.320
 I'd probably still be sending rovers to glaciers, right?

1:01:26.320 --> 1:01:28.040
 But the problems were interesting.

1:01:28.040 --> 1:01:30.600
 And the other thing was that they were hard, right?

1:01:30.600 --> 1:01:34.560
 So it's, I like having to go into a room

1:01:34.560 --> 1:01:37.000
 and being like, I don't know what to do.

1:01:37.000 --> 1:01:38.280
 And then going back and saying, okay,

1:01:38.280 --> 1:01:39.800
 I'm gonna figure this out.

1:01:39.800 --> 1:01:42.320
 I do not, I'm not driven when I go in like,

1:01:42.320 --> 1:01:44.040
 oh, there are no surprises.

1:01:44.040 --> 1:01:47.320
 Like I don't find that satisfying.

1:01:47.320 --> 1:01:48.160
 If that was the case,

1:01:48.160 --> 1:01:51.040
 I'd go someplace and make a lot more money, right?

1:01:51.040 --> 1:01:55.000
 I think I stay in academic and choose to do this

1:01:55.000 --> 1:01:58.280
 because I can go into a room and I'm like, that's hard.

1:01:58.280 --> 1:02:01.760
 Yeah, I think just from my perspective,

1:02:01.760 --> 1:02:03.240
 maybe you can correct me on it,

1:02:03.240 --> 1:02:06.720
 but if I just look at the field of AI broadly,

1:02:06.720 --> 1:02:11.720
 it seems that human robot interaction has the most,

1:02:12.040 --> 1:02:16.560
 one of the most number of open problems.

1:02:16.560 --> 1:02:18.920
 Like people, especially relative

1:02:18.920 --> 1:02:22.320
 to how many people are willing to acknowledge that there are.

1:02:23.440 --> 1:02:26.160
 This, because most people are just afraid of the humans

1:02:26.160 --> 1:02:28.200
 so they don't even acknowledge how many open problems there

1:02:28.200 --> 1:02:30.400
 but it's in terms of difficult problems

1:02:30.400 --> 1:02:32.360
 to solve exciting spaces,

1:02:32.360 --> 1:02:35.800
 it seems to be incredible for that.

1:02:35.800 --> 1:02:37.800
 It is, and it's exciting.

1:02:38.680 --> 1:02:40.000
 You've mentioned trust before.

1:02:40.000 --> 1:02:45.000
 What role does trust from interacting with autopilot

1:02:46.840 --> 1:02:48.440
 to in the medical context,

1:02:48.440 --> 1:02:51.320
 what role does trust play in the human robot interaction?

1:02:51.320 --> 1:02:53.920
 So some of the things I study in this domain

1:02:53.920 --> 1:02:56.920
 is not just trust, but it really is over trust.

1:02:56.920 --> 1:02:58.160
 How do you think about over trust?

1:02:58.160 --> 1:03:03.160
 Like first of all, what is trust and what is over trust?

1:03:03.360 --> 1:03:05.840
 Basically, the way I look at it is

1:03:05.840 --> 1:03:08.080
 trust is not what you click on a survey,

1:03:08.080 --> 1:03:09.600
 trust is about your behavior.

1:03:09.600 --> 1:03:11.920
 So if you interact with the technology

1:03:13.520 --> 1:03:17.320
 based on the decision or the actions of the technology

1:03:17.320 --> 1:03:21.520
 as if you trust that decision, then you're trusting, right?

1:03:22.400 --> 1:03:25.600
 And even in my group, we've done surveys

1:03:25.600 --> 1:03:28.280
 that on the thing do you trust robots?

1:03:28.280 --> 1:03:29.120
 Of course not.

1:03:29.120 --> 1:03:31.680
 Would you follow this robot in a burning building?

1:03:31.680 --> 1:03:32.960
 Of course not, right?

1:03:32.960 --> 1:03:34.480
 And then you look at their actions

1:03:34.480 --> 1:03:37.280
 and you're like, clearly your behavior

1:03:37.280 --> 1:03:39.680
 does not match what you think, right?

1:03:39.680 --> 1:03:42.040
 Or what you think you would like to think, right?

1:03:42.040 --> 1:03:44.080
 And so I'm really concerned about the behavior

1:03:44.080 --> 1:03:45.840
 because that's really at the end of the day

1:03:45.840 --> 1:03:47.360
 when you're in the world,

1:03:47.360 --> 1:03:50.520
 that's what will impact others around you.

1:03:50.520 --> 1:03:52.960
 It's not whether before you went onto the street,

1:03:52.960 --> 1:03:55.640
 you clicked on like, I don't trust self driving cars.

1:03:55.640 --> 1:03:58.680
 Yeah, that from an outsider perspective,

1:03:58.680 --> 1:04:00.600
 it's always frustrating to me.

1:04:00.600 --> 1:04:01.480
 Well, I read a lot.

1:04:01.480 --> 1:04:04.160
 So I'm insider in a certain philosophical sense.

1:04:06.080 --> 1:04:10.720
 It's frustrating to me how often trust is used in surveys

1:04:10.720 --> 1:04:14.440
 and how people say make claims

1:04:14.440 --> 1:04:16.240
 out of any kind of finding they make

1:04:16.240 --> 1:04:18.720
 while somebody clicking on answer.

1:04:18.720 --> 1:04:23.720
 You just trust is, yeah, behavior just,

1:04:23.760 --> 1:04:24.640
 you said it beautifully.

1:04:24.640 --> 1:04:28.120
 I mean, action, your own behavior is what trust is.

1:04:28.120 --> 1:04:30.800
 I mean, that everything else is not even close.

1:04:30.800 --> 1:04:35.800
 It's almost like absurd comedic poetry

1:04:36.120 --> 1:04:38.560
 that you weave around your actual behavior.

1:04:38.560 --> 1:04:41.880
 So some people can say they trust,

1:04:41.880 --> 1:04:46.080
 you know, I trust my wife, husband or not, whatever,

1:04:46.080 --> 1:04:48.040
 but the actions is what speaks volumes.

1:04:48.040 --> 1:04:52.240
 If you bug their car, you probably don't trust them.

1:04:52.240 --> 1:04:53.840
 I trust them, I'm just making sure.

1:04:53.840 --> 1:04:55.600
 No, no, that's, yeah.

1:04:55.600 --> 1:04:57.280
 Like even if you think about cars,

1:04:57.280 --> 1:04:58.600
 I think it's a beautiful case.

1:04:58.600 --> 1:05:00.840
 I came here at some point,

1:05:00.840 --> 1:05:03.600
 I'm sure on either Uber or Lyft, right?

1:05:03.600 --> 1:05:06.040
 I remember when it first came out, right?

1:05:06.040 --> 1:05:08.040
 I bet if they had had a survey,

1:05:08.040 --> 1:05:11.440
 would you get in the car with a stranger and pay them?

1:05:11.440 --> 1:05:12.800
 Yes.

1:05:12.800 --> 1:05:16.440
 How many people do you think would have said, like, really?

1:05:16.440 --> 1:05:17.760
 You know, wait, even worse,

1:05:17.760 --> 1:05:19.840
 would you get in the car with a stranger

1:05:19.840 --> 1:05:21.960
 at 1 a.m. in the morning

1:05:21.960 --> 1:05:24.800
 to have them drop you home as a single female?

1:05:24.800 --> 1:05:25.640
 Yeah.

1:05:25.640 --> 1:05:29.320
 Like how many people would say, that's stupid?

1:05:29.320 --> 1:05:30.160
 Yeah.

1:05:30.160 --> 1:05:31.600
 And now look at where we are.

1:05:31.600 --> 1:05:34.000
 I mean, people put kids, right?

1:05:34.000 --> 1:05:37.720
 Like, oh yeah, my child has to go to school

1:05:37.720 --> 1:05:40.600
 and I, yeah, I'm gonna put my kid in this car

1:05:40.600 --> 1:05:42.360
 with a stranger.

1:05:42.360 --> 1:05:45.280
 I mean, it's just fascinating how,

1:05:45.280 --> 1:05:48.360
 like, what we think we think is not necessarily

1:05:48.360 --> 1:05:49.720
 matching our behavior.

1:05:49.720 --> 1:05:52.360
 Yeah, and certainly with robots, with autonomous vehicles,

1:05:52.360 --> 1:05:54.720
 and all the kinds of robots you work with,

1:05:54.720 --> 1:05:59.720
 that's, it's, yeah, it's the way you answer it,

1:06:00.400 --> 1:06:01.880
 especially if you've never interacted

1:06:01.880 --> 1:06:03.320
 with that robot before.

1:06:04.400 --> 1:06:05.680
 If you haven't had the experience,

1:06:05.680 --> 1:06:07.440
 you being able to respond correctly

1:06:07.440 --> 1:06:09.640
 on a survey is impossible.

1:06:09.640 --> 1:06:13.400
 But what role does trust play in the interaction,

1:06:13.400 --> 1:06:14.280
 do you think?

1:06:14.280 --> 1:06:19.280
 Like, is it good to, is it good to trust a robot?

1:06:19.480 --> 1:06:21.680
 What does over trust mean?

1:06:21.680 --> 1:06:24.040
 Or is it, is it good to kind of how you feel

1:06:24.040 --> 1:06:26.560
 about autopilot currently, which is like,

1:06:26.560 --> 1:06:29.520
 from a robotics perspective, is like,

1:06:29.520 --> 1:06:31.520
 still very cautious?

1:06:31.520 --> 1:06:34.960
 Yeah, so this is still an open area of research.

1:06:34.960 --> 1:06:39.960
 But basically what I would like in a perfect world

1:06:40.720 --> 1:06:43.240
 is that people trust the technology

1:06:43.240 --> 1:06:44.920
 when it's working 100%,

1:06:44.920 --> 1:06:47.280
 and people will be hypersensitive

1:06:47.280 --> 1:06:49.080
 and identify when it's not.

1:06:49.080 --> 1:06:51.000
 But of course we're not there.

1:06:51.000 --> 1:06:52.720
 That's the ideal world.

1:06:53.640 --> 1:06:56.480
 And, but we find is that people swing, right?

1:06:56.480 --> 1:07:01.320
 They tend to swing, which means that if my first,

1:07:01.320 --> 1:07:02.920
 and like, we have some papers,

1:07:02.920 --> 1:07:05.280
 like first impressions is everything, right?

1:07:05.280 --> 1:07:08.520
 If my first instance with technology with robotics

1:07:08.520 --> 1:07:12.720
 is positive, it mitigates any risk,

1:07:12.720 --> 1:07:16.880
 it correlates with like best outcomes.

1:07:16.880 --> 1:07:21.480
 It means that I'm more likely to either not see it

1:07:21.480 --> 1:07:24.240
 when it makes some mistakes or faults,

1:07:24.240 --> 1:07:27.320
 or I'm more likely to forgive it.

1:07:28.680 --> 1:07:30.360
 And so this is a problem

1:07:30.360 --> 1:07:32.640
 because technology is not 100% accurate, right?

1:07:32.640 --> 1:07:35.080
 It's not 100% accurate, although it may be perfect.

1:07:35.080 --> 1:07:37.680
 How do you get that first moment right, do you think?

1:07:37.680 --> 1:07:40.720
 There's also an education about the capabilities

1:07:40.720 --> 1:07:42.480
 and limitations of the system.

1:07:42.480 --> 1:07:45.720
 Do you have a sense of how you educate people correctly

1:07:45.720 --> 1:07:47.120
 in that first interaction?

1:07:47.120 --> 1:07:50.240
 Again, this is an open ended problem.

1:07:50.240 --> 1:07:55.000
 So one of the study that actually has given me some hope

1:07:55.000 --> 1:07:57.640
 that I was trying to figure out how to put in robotics.

1:07:57.640 --> 1:08:01.280
 So there was a research study

1:08:01.280 --> 1:08:03.480
 that it showed for medical AI systems,

1:08:03.480 --> 1:08:07.840
 giving information to radiologists about, you know,

1:08:07.840 --> 1:08:12.840
 here you need to look at these areas on the X ray.

1:08:14.320 --> 1:08:19.320
 What they found was that when the system provided one choice,

1:08:20.560 --> 1:08:25.560
 there was this aspect of either no trust or over trust, right?

1:08:26.880 --> 1:08:29.840
 Like I'm not, I don't believe it at all,

1:08:29.840 --> 1:08:34.840
 or a yes, yes, yes, yes, and they would miss things, right?

1:08:34.840 --> 1:08:38.840
 Instead, when the system gave them multiple choices,

1:08:38.840 --> 1:08:41.640
 like here are the three, even if it knew, like, you know,

1:08:41.640 --> 1:08:44.280
 it had estimated that the top area you need to look at

1:08:44.280 --> 1:08:48.280
 was some place on the X ray.

1:08:48.280 --> 1:08:52.240
 If it gave like one plus others,

1:08:52.240 --> 1:08:55.840
 the trust was maintained

1:08:55.840 --> 1:09:00.840
 and the accuracy of the entire population increased.

1:09:00.840 --> 1:09:04.840
 Right? So basically it was a, you're still trusting the system,

1:09:04.840 --> 1:09:06.840
 but you're also putting in a little bit of like

1:09:06.840 --> 1:09:08.840
 your human expertise,

1:09:08.840 --> 1:09:12.840
 like your human decision processing into the equation.

1:09:12.840 --> 1:09:15.840
 So it helps to mitigate that over trust risk.

1:09:15.840 --> 1:09:18.840
 Yeah. So there's a fascinating balance at the strike.

1:09:18.840 --> 1:09:20.840
 Haven't figured out again.

1:09:20.840 --> 1:09:23.840
 It's exciting open area research. Exactly.

1:09:23.840 --> 1:09:25.840
 So what are some exciting applications

1:09:25.840 --> 1:09:27.840
 of human robot interaction?

1:09:27.840 --> 1:09:30.840
 You started a company, maybe you can talk about

1:09:30.840 --> 1:09:32.840
 the exciting efforts there,

1:09:32.840 --> 1:09:35.840
 but in general also what other space

1:09:35.840 --> 1:09:38.840
 can robots interact with humans and help?

1:09:38.840 --> 1:09:40.840
 Yeah. So besides healthcare, because, you know,

1:09:40.840 --> 1:09:41.840
 that's my bias lens.

1:09:41.840 --> 1:09:44.840
 My other bias lens is education.

1:09:44.840 --> 1:09:49.840
 I think that, well, one, we definitely, we,

1:09:49.840 --> 1:09:52.840
 in the US, you know, we're doing okay with teachers,

1:09:52.840 --> 1:09:54.840
 but there's a lot of school districts

1:09:54.840 --> 1:09:56.840
 that don't have enough teachers.

1:09:56.840 --> 1:10:00.840
 If you think about the teacher student ratio

1:10:00.840 --> 1:10:04.840
 for at least public education in some districts,

1:10:04.840 --> 1:10:05.840
 it's crazy.

1:10:05.840 --> 1:10:08.840
 It's like, how can you have learning in that classroom?

1:10:08.840 --> 1:10:09.840
 Right?

1:10:09.840 --> 1:10:11.840
 Because you just don't have the human capital.

1:10:11.840 --> 1:10:14.840
 And so if you think about robotics,

1:10:14.840 --> 1:10:17.840
 bringing that in to classrooms,

1:10:17.840 --> 1:10:19.840
 as well as the after school space,

1:10:19.840 --> 1:10:23.840
 where they offset some of this lack of resources

1:10:23.840 --> 1:10:27.840
 in certain communities, I think that's a good place.

1:10:27.840 --> 1:10:31.840
 And then turning on the other end is using these systems then

1:10:31.840 --> 1:10:37.840
 for workforce retraining and dealing with some of the things

1:10:37.840 --> 1:10:41.840
 that are going to come out later on of job loss,

1:10:41.840 --> 1:10:44.840
 like thinking about robots and NAI systems

1:10:44.840 --> 1:10:46.840
 for retraining and workforce development.

1:10:46.840 --> 1:10:51.840
 I think that's exciting areas that can be pushed even more,

1:10:51.840 --> 1:10:54.840
 and it would have a huge, huge impact.

1:10:54.840 --> 1:10:59.840
 What would you say are some of the open problems in education?

1:10:59.840 --> 1:11:01.840
 Sort of, it's exciting.

1:11:01.840 --> 1:11:08.840
 So young kids and the older folks or just folks of all ages

1:11:08.840 --> 1:11:10.840
 who need to be retrained,

1:11:10.840 --> 1:11:12.840
 who need to sort of open themselves up

1:11:12.840 --> 1:11:15.840
 to a whole other area of work.

1:11:15.840 --> 1:11:18.840
 What are the problems to be solved there?

1:11:18.840 --> 1:11:21.840
 How do you think robots can help?

1:11:21.840 --> 1:11:23.840
 We have the engagement aspect, right?

1:11:23.840 --> 1:11:25.840
 So we can figure out the engagement.

1:11:25.840 --> 1:11:27.840
 What do you mean by engagement?

1:11:27.840 --> 1:11:33.840
 So identifying whether a person is focused

1:11:33.840 --> 1:11:37.840
 is like that we can figure out.

1:11:37.840 --> 1:11:39.840
 What we can figure out,

1:11:39.840 --> 1:11:43.840
 and there's some positive results in this,

1:11:43.840 --> 1:11:48.840
 is that personalized adaptation based on any concepts, right?

1:11:48.840 --> 1:11:53.840
 So imagine I think about I have an agent

1:11:53.840 --> 1:12:00.840
 and I'm working with a kid learning, I don't know, Algebra 2.

1:12:00.840 --> 1:12:04.840
 Can that same agent then switch and teach

1:12:04.840 --> 1:12:10.840
 some type of new coding skill to a displaced mechanic?

1:12:10.840 --> 1:12:13.840
 What does that actually look like?

1:12:13.840 --> 1:12:16.840
 Hardware might be the same,

1:12:16.840 --> 1:12:18.840
 content is different,

1:12:18.840 --> 1:12:21.840
 two different target demographics of engagement.

1:12:21.840 --> 1:12:23.840
 How do you do that?

1:12:23.840 --> 1:12:25.840
 How important do you think personalization

1:12:25.840 --> 1:12:27.840
 is in human robot interaction?

1:12:27.840 --> 1:12:31.840
 Not just a mechanic or student,

1:12:31.840 --> 1:12:34.840
 but literally to the individual human being?

1:12:34.840 --> 1:12:36.840
 I think personalization is really important,

1:12:36.840 --> 1:12:41.840
 but a caveat is that I think we'd be okay

1:12:41.840 --> 1:12:43.840
 if we can personalize to the group, right?

1:12:43.840 --> 1:12:51.840
 And so if I can label you as along some certain dimensions,

1:12:51.840 --> 1:12:55.840
 then even though it may not be you specifically,

1:12:55.840 --> 1:12:57.840
 I can put you in this group.

1:12:57.840 --> 1:12:59.840
 So the sample size, this is how they best learn,

1:12:59.840 --> 1:13:01.840
 this is how they best engage.

1:13:01.840 --> 1:13:05.840
 Even at that level, it's really important.

1:13:05.840 --> 1:13:08.840
 And it's because, I mean, it's one of the reasons

1:13:08.840 --> 1:13:12.840
 why educating in large classrooms is so hard, right?

1:13:12.840 --> 1:13:14.840
 You teach to the median,

1:13:14.840 --> 1:13:18.840
 but there's these individuals that are struggling,

1:13:18.840 --> 1:13:21.840
 and then you have highly intelligent individuals,

1:13:21.840 --> 1:13:25.840
 and those are the ones that are usually kind of left out.

1:13:25.840 --> 1:13:27.840
 So highly intelligent individuals may be disruptive,

1:13:27.840 --> 1:13:29.840
 and those who are struggling might be disruptive

1:13:29.840 --> 1:13:31.840
 because they're both bored.

1:13:31.840 --> 1:13:34.840
 And if you narrow the definition of the group

1:13:34.840 --> 1:13:36.840
 or in the size of the group enough,

1:13:36.840 --> 1:13:40.840
 you'll be able to address their individual needs,

1:13:40.840 --> 1:13:44.840
 but really the most important group needs, right?

1:13:44.840 --> 1:13:46.840
 And that's kind of what a lot of successful

1:13:46.840 --> 1:13:49.840
 recommender systems do, Spotify and so on.

1:13:49.840 --> 1:13:52.840
 It's sad to believe, but as a music listener,

1:13:52.840 --> 1:13:54.840
 probably in some sort of large group,

1:13:54.840 --> 1:13:56.840
 it's very sadly predictable.

1:13:56.840 --> 1:13:58.840
 You have been labeled.

1:13:58.840 --> 1:14:01.840
 Yeah, I've been labeled and successfully so,

1:14:01.840 --> 1:14:04.840
 because they're able to recommend stuff.

1:14:04.840 --> 1:14:07.840
 Yeah, but applying that to education, right?

1:14:07.840 --> 1:14:09.840
 There's no reason why it can't be done.

1:14:09.840 --> 1:14:12.840
 Do you have a hope for our education system?

1:14:12.840 --> 1:14:15.840
 I have more hope for workforce development,

1:14:15.840 --> 1:14:19.840
 and that's because I'm seeing investments.

1:14:19.840 --> 1:14:22.840
 Even if you look at VC investments in education,

1:14:22.840 --> 1:14:27.840
 the majority of it has lately been going to workforce retraining,

1:14:27.840 --> 1:14:31.840
 right? And so I think that government investments

1:14:31.840 --> 1:14:33.840
 is increasing. There's like a claim.

1:14:33.840 --> 1:14:35.840
 And some of this based on fear, right?

1:14:35.840 --> 1:14:37.840
 Like AI is going to come and take over all these jobs.

1:14:37.840 --> 1:14:40.840
 What are we going to do with all these nonpaying taxes

1:14:40.840 --> 1:14:43.840
 that aren't coming to us by our citizens?

1:14:43.840 --> 1:14:46.840
 And so I think I'm more hopeful for that.

1:14:46.840 --> 1:14:50.840
 Not so hopeful for early education,

1:14:50.840 --> 1:14:55.840
 because it's this, it's still a who's going to pay for it,

1:14:55.840 --> 1:15:02.840
 and you won't see the results for like 16 to 18 years.

1:15:02.840 --> 1:15:06.840
 It's hard for people to wrap their heads around that.

1:15:06.840 --> 1:15:09.840
 But on the retraining part, what are your thoughts?

1:15:09.840 --> 1:15:13.840
 There's a candidate, Andrew Yang, running for president,

1:15:13.840 --> 1:15:18.840
 saying that sort of AI automation, robots,

1:15:18.840 --> 1:15:20.840
 universal basic income.

1:15:20.840 --> 1:15:23.840
 Universal basic income in order to support us

1:15:23.840 --> 1:15:26.840
 as we kind of automation takes people's jobs

1:15:26.840 --> 1:15:29.840
 and allows you to explore and find other means.

1:15:29.840 --> 1:15:36.840
 Like, do you have a concern of society transforming effects

1:15:36.840 --> 1:15:39.840
 of automation and robots and so on?

1:15:39.840 --> 1:15:45.840
 I do. I do know that AI robotics will displace workers.

1:15:45.840 --> 1:15:47.840
 Like, we do know that.

1:15:47.840 --> 1:15:54.840
 But there'll be other workers that will be defined new jobs.

1:15:54.840 --> 1:15:56.840
 What I worry about is, that's not what I worry about.

1:15:56.840 --> 1:15:58.840
 Like, will all the jobs go away?

1:15:58.840 --> 1:16:01.840
 What I worry about is a type of jobs that will come out, right?

1:16:01.840 --> 1:16:05.840
 Like, people who graduate from Georgia Tech will be okay, right?

1:16:05.840 --> 1:16:07.840
 We give them the skills, they will adapt,

1:16:07.840 --> 1:16:09.840
 even if their current job goes away.

1:16:09.840 --> 1:16:14.840
 I do worry about those that don't have that quality of an education, right?

1:16:14.840 --> 1:16:20.840
 Will they have the ability, the background to adapt to those new jobs?

1:16:20.840 --> 1:16:23.840
 That, I don't know. That I worry about.

1:16:23.840 --> 1:16:29.840
 Which will create even more polarization in our society, internationally,

1:16:29.840 --> 1:16:31.840
 and everywhere. I worry about that.

1:16:31.840 --> 1:16:37.840
 I also worry about not having equal access to all these wonderful things

1:16:37.840 --> 1:16:40.840
 that AI can do and robotics can do.

1:16:40.840 --> 1:16:42.840
 I worry about that.

1:16:42.840 --> 1:16:49.840
 People like me from Georgia Tech, from say MIT, will be okay, right?

1:16:49.840 --> 1:16:52.840
 But that's such a small part of the population

1:16:52.840 --> 1:16:57.840
 that we need to think much more globally of having access to the beautiful things,

1:16:57.840 --> 1:17:04.840
 whether it's AI in healthcare, AI in education, AI in politics, right?

1:17:04.840 --> 1:17:05.840
 I worry about that.

1:17:05.840 --> 1:17:07.840
 And that's part of the thing that you were talking about

1:17:07.840 --> 1:17:13.840
 is people that build the technology had to be thinking about, ethics had to be thinking about access

1:17:13.840 --> 1:17:17.840
 and all those things, and not just a small subset.

1:17:17.840 --> 1:17:21.840
 Let me ask some philosophical, slightly romantic questions.

1:17:21.840 --> 1:17:25.840
 People that listen to this will be like, here he goes again.

1:17:25.840 --> 1:17:31.840
 Okay. Do you think one day we'll build an AI system

1:17:31.840 --> 1:17:37.840
 that a person can fall in love with and it would love them back?

1:17:37.840 --> 1:17:39.840
 Like in a movie, Her, for example.

1:17:39.840 --> 1:17:40.840
 Oh, yeah.

1:17:40.840 --> 1:17:43.840
 Although she kind of didn't fall in love with him.

1:17:43.840 --> 1:17:46.840
 She fell in love with like a million other people, something like that.

1:17:46.840 --> 1:17:48.840
 You're the jealous type, I see.

1:17:48.840 --> 1:17:50.840
 We humans are the jealous type.

1:17:50.840 --> 1:17:51.840
 Yes.

1:17:51.840 --> 1:17:57.840
 So I do believe that we can design systems where people would fall in love

1:17:57.840 --> 1:18:02.840
 with their robot, with their AI partner.

1:18:02.840 --> 1:18:04.840
 That I do believe.

1:18:04.840 --> 1:18:08.840
 Because it's actually, and I don't like to use the word manipulate,

1:18:08.840 --> 1:18:12.840
 but as we see, there are certain individuals that can be manipulated

1:18:12.840 --> 1:18:15.840
 if you understand the cognitive science about it, right?

1:18:15.840 --> 1:18:16.840
 Right.

1:18:16.840 --> 1:18:20.840
 So I mean, if you could think of all close relationship and love in general

1:18:20.840 --> 1:18:26.840
 as a kind of mutual manipulation, that dance, the human dance.

1:18:26.840 --> 1:18:29.840
 I mean, manipulation is a negative connotation.

1:18:29.840 --> 1:18:32.840
 And that's why I don't like to use that word particularly.

1:18:32.840 --> 1:18:34.840
 I guess another way to phrase it is you're getting at it,

1:18:34.840 --> 1:18:37.840
 it could be algorithmatized or something.

1:18:37.840 --> 1:18:39.840
 The relationship building part can be.

1:18:39.840 --> 1:18:40.840
 Yeah.

1:18:40.840 --> 1:18:41.840
 I mean, just think about it.

1:18:41.840 --> 1:18:46.840
 We have, and I don't use dating sites, but from what I heard,

1:18:46.840 --> 1:18:52.840
 there are some individuals that have been dating that have never saw each other, right?

1:18:52.840 --> 1:18:57.840
 In fact, there's a show, I think, that tries to weed out fake people.

1:18:57.840 --> 1:18:59.840
 There's a show that comes out, right?

1:18:59.840 --> 1:19:01.840
 Because people start faking.

1:19:01.840 --> 1:19:07.840
 What's the difference of that person on the other end being an AI agent, right?

1:19:07.840 --> 1:19:11.840
 And having a communication and you building a relationship remotely,

1:19:11.840 --> 1:19:15.840
 there's no reason why that can't happen.

1:19:15.840 --> 1:19:17.840
 In terms of human robot interaction,

1:19:17.840 --> 1:19:22.840
 what role, you've kind of mentioned with data, emotion being,

1:19:22.840 --> 1:19:25.840
 can be problematic if not implemented well, I suppose.

1:19:25.840 --> 1:19:29.840
 What role does emotion and some other human like things,

1:19:29.840 --> 1:19:34.840
 the imperfect things come into play here for good human robot interaction

1:19:34.840 --> 1:19:36.840
 and something like love?

1:19:36.840 --> 1:19:37.840
 Yeah.

1:19:37.840 --> 1:19:42.840
 So in this case, and you had asked, can an AI agent love a human back?

1:19:42.840 --> 1:19:46.840
 I think they can emulate love back, right?

1:19:46.840 --> 1:19:48.840
 And so what does that actually mean?

1:19:48.840 --> 1:19:51.840
 It just means that if you think about their programming,

1:19:51.840 --> 1:19:57.840
 they might put the other person's needs in front of theirs in certain situations, right?

1:19:57.840 --> 1:19:59.840
 Think about it as return on investment.

1:19:59.840 --> 1:20:02.840
 Like, was my return on investment as part of that equation,

1:20:02.840 --> 1:20:07.840
 that person's happiness has some type of algorithm waiting to it.

1:20:07.840 --> 1:20:10.840
 And the reason why is because I care about them, right?

1:20:10.840 --> 1:20:12.840
 That's the only reason, right?

1:20:12.840 --> 1:20:17.840
 But if I care about them and I show that, then my final objective function

1:20:17.840 --> 1:20:19.840
 is length of time of the engagement, right?

1:20:19.840 --> 1:20:23.840
 So you can think of how to do this actually quite easily.

1:20:23.840 --> 1:20:26.840
 But that's not love?

1:20:26.840 --> 1:20:28.840
 Well, so that's the thing.

1:20:28.840 --> 1:20:37.840
 I think it emulates love because we don't have a classical definition of love.

1:20:37.840 --> 1:20:38.840
 Right.

1:20:38.840 --> 1:20:44.840
 And we don't have the ability to look into each other's minds to see the algorithm.

1:20:44.840 --> 1:20:49.840
 And I guess what I'm getting at is, is it possible that,

1:20:49.840 --> 1:20:50.840
 especially if that's learned,

1:20:50.840 --> 1:20:57.840
 especially if there's some mystery and black box nature to the system, how is that, you know,

1:20:57.840 --> 1:20:58.840
 How is it any different?

1:20:58.840 --> 1:20:59.840
 How is it any different?

1:20:59.840 --> 1:21:04.840
 And in terms of sort of, if the system says, I'm conscious, I'm afraid of death,

1:21:04.840 --> 1:21:10.840
 and it does indicate that it loves you.

1:21:10.840 --> 1:21:13.840
 Another way to sort of phrase it, I'd be curious to see what you think.

1:21:13.840 --> 1:21:19.840
 Do you think there'll be a time when robots should have rights?

1:21:19.840 --> 1:21:22.840
 You've kind of phrased the robot in a very roboticist way.

1:21:22.840 --> 1:21:24.840
 It's just a really good way.

1:21:24.840 --> 1:21:27.840
 But saying, okay, well, there's an objective function,

1:21:27.840 --> 1:21:32.840
 and I could see how you can create a compelling human robot interaction experience

1:21:32.840 --> 1:21:35.840
 that makes you believe that the robot cares for your needs

1:21:35.840 --> 1:21:38.840
 and even something like loves you.

1:21:38.840 --> 1:21:43.840
 But what if the robot says, please don't turn me off?

1:21:43.840 --> 1:21:48.840
 What if the robot starts making you feel like there's an entity of being a soul there?

1:21:48.840 --> 1:21:49.840
 Right.

1:21:49.840 --> 1:21:52.840
 Do you think there'll be a future?

1:21:52.840 --> 1:21:55.840
 Hopefully you won't laugh too much at this,

1:21:55.840 --> 1:21:59.840
 but where they do ask for rights.

1:21:59.840 --> 1:22:07.840
 So I can see a future if we don't address it in the near term,

1:22:07.840 --> 1:22:12.840
 where these agents, as they adapt and learn, could say, hey,

1:22:12.840 --> 1:22:15.840
 this should be something that's fundamental.

1:22:15.840 --> 1:22:19.840
 I hopefully think that we would address it before it gets to that point.

1:22:19.840 --> 1:22:21.840
 You think that's a bad future.

1:22:21.840 --> 1:22:26.840
 Is that a negative thing where they ask or being discriminated against?

1:22:26.840 --> 1:22:33.840
 I guess it depends on what role have they attained at that point.

1:22:33.840 --> 1:22:35.840
 And so if I think about now...

1:22:35.840 --> 1:22:39.840
 Careful what you say, because the robot's 50 years when I'll be listening to this,

1:22:39.840 --> 1:22:43.840
 and you'll be on TV saying, this is what roboticists used to believe.

1:22:43.840 --> 1:22:44.840
 Well, right?

1:22:44.840 --> 1:22:47.840
 And so this is my, and as I said, I have a biased lens,

1:22:47.840 --> 1:22:50.840
 and my robot friends will understand that.

1:22:50.840 --> 1:22:53.840
 But so if you think about it,

1:22:53.840 --> 1:22:57.840
 and I actually put this in kind of the...

1:22:57.840 --> 1:23:03.840
 As a roboticist, you don't necessarily think of robots as human with human rights,

1:23:03.840 --> 1:23:08.840
 but you could think of them either in the category of property,

1:23:08.840 --> 1:23:13.840
 or you could think of them in the category of animals, right?

1:23:13.840 --> 1:23:17.840
 And so both of those have different types of rights.

1:23:17.840 --> 1:23:22.840
 So animals have their own rights as a living being,

1:23:22.840 --> 1:23:27.840
 but they can't vote, they can't write, they can be euthanized.

1:23:27.840 --> 1:23:32.840
 But as humans, if we abuse them, we go to jail, right?

1:23:32.840 --> 1:23:35.840
 So they do have some rights that protect them,

1:23:35.840 --> 1:23:39.840
 but don't give them the rights of citizenship.

1:23:39.840 --> 1:23:41.840
 And then if you think about property,

1:23:41.840 --> 1:23:45.840
 property, the rights are associated with the person, right?

1:23:45.840 --> 1:23:49.840
 So if someone vandalizes your property,

1:23:49.840 --> 1:23:53.840
 or steals your property, like there are some rights,

1:23:53.840 --> 1:23:57.840
 but it's associated with the person who owns that.

1:23:57.840 --> 1:24:01.840
 If you think about it, back in the day,

1:24:01.840 --> 1:24:05.840
 and remember we talked about how society has changed,

1:24:05.840 --> 1:24:08.840
 women were property, right?

1:24:08.840 --> 1:24:11.840
 They were not thought of as having rights,

1:24:11.840 --> 1:24:15.840
 they were thought of as property of...

1:24:15.840 --> 1:24:19.840
 Assaulting a woman meant assaulting the property of somebody else.

1:24:19.840 --> 1:24:20.840
 Exactly.

1:24:20.840 --> 1:24:24.840
 And so what I envision is that we will establish

1:24:24.840 --> 1:24:29.840
 some type of norm at some point, but that it might evolve, right?

1:24:29.840 --> 1:24:31.840
 If you look at women's rights now,

1:24:31.840 --> 1:24:35.840
 there are still some countries that don't have,

1:24:35.840 --> 1:24:37.840
 and the rest of the world is like, why?

1:24:37.840 --> 1:24:39.840
 That makes no sense, right?

1:24:39.840 --> 1:24:44.840
 And so I do see a world where we do establish some type of grounding.

1:24:44.840 --> 1:24:46.840
 It might be based on property rights.

1:24:46.840 --> 1:24:48.840
 It might be based on animal rights.

1:24:48.840 --> 1:24:51.840
 And if it evolves that way,

1:24:51.840 --> 1:24:55.840
 I think we will have this conversation at that time,

1:24:55.840 --> 1:25:00.840
 because that's the way our society traditionally has evolved.

1:25:00.840 --> 1:25:02.840
 Beautifully put.

1:25:02.840 --> 1:25:04.840
 Just out of curiosity,

1:25:04.840 --> 1:25:07.840
 Anki, Gibo, Mayfield Robotics,

1:25:07.840 --> 1:25:09.840
 with the Robot Curie, SciFile Works,

1:25:09.840 --> 1:25:12.840
 we think Robotics were all these amazing robotics companies

1:25:12.840 --> 1:25:15.840
 created by incredible roboticists,

1:25:15.840 --> 1:25:20.840
 and they've all went out of business recently.

1:25:20.840 --> 1:25:23.840
 Why do you think they didn't last long?

1:25:23.840 --> 1:25:26.840
 Why is it so hard to run a robotics company,

1:25:26.840 --> 1:25:29.840
 especially one like these,

1:25:29.840 --> 1:25:32.840
 which are fundamentally HRI,

1:25:32.840 --> 1:25:35.840
 human robot interaction robots?

1:25:35.840 --> 1:25:38.840
 Yeah, each one has a story.

1:25:38.840 --> 1:25:40.840
 Only one of them I don't understand.

1:25:40.840 --> 1:25:42.840
 And that was Anki.

1:25:42.840 --> 1:25:44.840
 That's actually the only one I don't understand.

1:25:44.840 --> 1:25:46.840
 I don't understand it either.

1:25:46.840 --> 1:25:48.840
 I mean, from the outside,

1:25:48.840 --> 1:25:50.840
 I've looked at their sheets,

1:25:50.840 --> 1:25:52.840
 I've looked at the data that's...

1:25:52.840 --> 1:25:54.840
 Oh, you mean like business wise?

1:25:54.840 --> 1:25:58.840
 Yeah, and I look at that data,

1:25:58.840 --> 1:26:02.840
 and I'm like, they seem to have product market fit.

1:26:02.840 --> 1:26:05.840
 So that's the only one I don't understand.

1:26:05.840 --> 1:26:07.840
 The rest of it was product market fit.

1:26:07.840 --> 1:26:10.840
 What's product market fit?

1:26:10.840 --> 1:26:12.840
 How do you think about it?

1:26:12.840 --> 1:26:15.840
 Yeah, so although we think Robotics was getting there,

1:26:15.840 --> 1:26:17.840
 but I think it's just the timing,

1:26:17.840 --> 1:26:20.840
 their clock just timed out.

1:26:20.840 --> 1:26:22.840
 I think if they had been given a couple more years,

1:26:22.840 --> 1:26:24.840
 they would have been okay.

1:26:24.840 --> 1:26:28.840
 But the other ones were still fairly early

1:26:28.840 --> 1:26:30.840
 by the time they got into the market.

1:26:30.840 --> 1:26:32.840
 And so product market fit is,

1:26:32.840 --> 1:26:36.840
 I have a product that I want to sell at a certain price.

1:26:36.840 --> 1:26:39.840
 Are there enough people out there, the market,

1:26:39.840 --> 1:26:42.840
 that are willing to buy the product at that market price

1:26:42.840 --> 1:26:47.840
 for me to be a functional, viable, profit bearing company?

1:26:47.840 --> 1:26:48.840
 Right?

1:26:48.840 --> 1:26:50.840
 So product market fit.

1:26:50.840 --> 1:26:54.840
 If it costs you $1,000 and everyone wants it

1:26:54.840 --> 1:26:56.840
 and only is willing to pay a dollar,

1:26:56.840 --> 1:26:58.840
 you have no product market fit.

1:26:58.840 --> 1:27:01.840
 Even if you could sell it for, you know,

1:27:01.840 --> 1:27:03.840
 it's enough for a dollar because you can't...

1:27:03.840 --> 1:27:05.840
 So how hard is it for robots?

1:27:05.840 --> 1:27:07.840
 Maybe if you look at iRobot,

1:27:07.840 --> 1:27:10.840
 the company that makes Roombas vacuum cleaners,

1:27:10.840 --> 1:27:13.840
 can you comment on did they find the right product,

1:27:13.840 --> 1:27:15.840
 a market product fit?

1:27:15.840 --> 1:27:18.840
 Like are people willing to pay for robots?

1:27:18.840 --> 1:27:20.840
 It's also another kind of question.

1:27:20.840 --> 1:27:23.840
 So if you think about iRobot and their story, right?

1:27:23.840 --> 1:27:28.840
 Like when they first, they had enough of a runway, right?

1:27:28.840 --> 1:27:31.840
 When they first started, they weren't doing vacuum cleaners, right?

1:27:31.840 --> 1:27:36.840
 They were contracts, primarily government contracts,

1:27:36.840 --> 1:27:37.840
 designing robots.

1:27:37.840 --> 1:27:38.840
 Military robots.

1:27:38.840 --> 1:27:39.840
 Yeah, I mean, that's what they were.

1:27:39.840 --> 1:27:40.840
 That's how they started, right?

1:27:40.840 --> 1:27:42.840
 They still do a lot of incredible work there.

1:27:42.840 --> 1:27:44.840
 But yeah, that was the initial thing

1:27:44.840 --> 1:27:46.840
 that gave them enough funding to...

1:27:46.840 --> 1:27:47.840
 To then try to...

1:27:47.840 --> 1:27:50.840
 The vacuum cleaner is what I've been told

1:27:50.840 --> 1:27:53.840
 was not like their first rendezvous

1:27:53.840 --> 1:27:56.840
 in terms of designing a product, right?

1:27:56.840 --> 1:27:58.840
 And so they were able to survive

1:27:58.840 --> 1:28:02.840
 until they got to the point that they found a product,

1:28:02.840 --> 1:28:04.840
 price market, right?

1:28:04.840 --> 1:28:07.840
 And even with, if you look at the Roomba,

1:28:07.840 --> 1:28:09.840
 the price point now is different

1:28:09.840 --> 1:28:11.840
 than when it was first released, right?

1:28:11.840 --> 1:28:12.840
 It was an early adopter price,

1:28:12.840 --> 1:28:15.840
 but they found enough people who were willing to fund it.

1:28:15.840 --> 1:28:19.840
 And I mean, I forgot what their loss profile was

1:28:19.840 --> 1:28:21.840
 for the first couple of years,

1:28:21.840 --> 1:28:24.840
 but they became profitable in sufficient time

1:28:24.840 --> 1:28:27.840
 that they didn't have to close their doors.

1:28:27.840 --> 1:28:29.840
 So they found the right,

1:28:29.840 --> 1:28:31.840
 there's still people willing to pay

1:28:31.840 --> 1:28:32.840
 a large amount of money,

1:28:32.840 --> 1:28:35.840
 sort of over $1,000 for a vacuum cleaner.

1:28:35.840 --> 1:28:37.840
 Unfortunately for them,

1:28:37.840 --> 1:28:38.840
 now that they've proved everything out,

1:28:38.840 --> 1:28:40.840
 figured it all out, now there's competitors.

1:28:40.840 --> 1:28:43.840
 Yeah, and so that's the next thing, right?

1:28:43.840 --> 1:28:46.840
 The competition, and they have quite a number,

1:28:46.840 --> 1:28:47.840
 even internationally,

1:28:47.840 --> 1:28:49.840
 like there's some products out there,

1:28:49.840 --> 1:28:52.840
 you can go to Europe and be like,

1:28:52.840 --> 1:28:54.840
 oh, I didn't even know this one existed.

1:28:54.840 --> 1:28:56.840
 So this is the thing though,

1:28:56.840 --> 1:28:58.840
 like with any market,

1:28:58.840 --> 1:29:02.840
 I would, this is not a bad time,

1:29:02.840 --> 1:29:04.840
 although, you know, as a roboticist,

1:29:04.840 --> 1:29:05.840
 it's kind of depressing,

1:29:05.840 --> 1:29:10.840
 but I actually think about things like with,

1:29:10.840 --> 1:29:12.840
 I would say that all of the companies

1:29:12.840 --> 1:29:15.840
 that are now in the top five or six,

1:29:15.840 --> 1:29:19.840
 they weren't the first to the stage, right?

1:29:19.840 --> 1:29:22.840
 Like Google was not the first search engine,

1:29:22.840 --> 1:29:24.840
 sorry, Alta Vista, right?

1:29:24.840 --> 1:29:27.840
 Facebook was not the first, sorry, MySpace, right?

1:29:27.840 --> 1:29:28.840
 Like, think about it,

1:29:28.840 --> 1:29:30.840
 they were not the first players.

1:29:30.840 --> 1:29:32.840
 Those first players, like,

1:29:32.840 --> 1:29:38.840
 they're not in the top five, 10 of Fortune 500 companies, right?

1:29:38.840 --> 1:29:43.840
 They proved, they started to prove out the market,

1:29:43.840 --> 1:29:45.840
 they started to get people interested,

1:29:45.840 --> 1:29:47.840
 they started the buzz,

1:29:47.840 --> 1:29:49.840
 but they didn't make it to that next level.

1:29:49.840 --> 1:29:51.840
 But the second batch, right?

1:29:51.840 --> 1:29:56.840
 The second batch, I think, might make it to the next level.

1:29:56.840 --> 1:30:01.840
 When do you think the Facebook of...

1:30:01.840 --> 1:30:03.840
 The Facebook of robotics.

1:30:03.840 --> 1:30:06.840
 Sorry, I take that phrase back

1:30:06.840 --> 1:30:08.840
 because people deeply, for some reason,

1:30:08.840 --> 1:30:09.840
 well, I know why,

1:30:09.840 --> 1:30:12.840
 but it's, I think, exaggerated distrust Facebook

1:30:12.840 --> 1:30:14.840
 because of the privacy concerns and so on.

1:30:14.840 --> 1:30:15.840
 And with robotics,

1:30:15.840 --> 1:30:17.840
 one of the things you have to make sure

1:30:17.840 --> 1:30:19.840
 is all the things we talked about

1:30:19.840 --> 1:30:22.840
 is to be transparent and have people deeply trust you

1:30:22.840 --> 1:30:25.840
 to let a robot into their lives, into their home.

1:30:25.840 --> 1:30:28.840
 But when do you think the second batch of robots...

1:30:28.840 --> 1:30:31.840
 Is it five, 10 years, 20 years

1:30:31.840 --> 1:30:34.840
 that we'll have robots in our homes

1:30:34.840 --> 1:30:36.840
 and robots in our hearts?

1:30:36.840 --> 1:30:37.840
 So if I think about...

1:30:37.840 --> 1:30:40.840
 Because I try to follow the VC kind of space

1:30:40.840 --> 1:30:42.840
 in terms of robotic investments.

1:30:42.840 --> 1:30:43.840
 And right now,

1:30:43.840 --> 1:30:45.840
 and I don't know if they're going to be successful,

1:30:45.840 --> 1:30:48.840
 I don't know if this is the second batch,

1:30:48.840 --> 1:30:52.840
 but there's only one batch that's focused on the first batch, right?

1:30:52.840 --> 1:30:55.840
 And then there's all these self driving Xs, right?

1:30:55.840 --> 1:30:58.840
 And so I don't know if they're a first batch of something

1:30:58.840 --> 1:31:02.840
 or if, like, I don't know quite where they fit in,

1:31:02.840 --> 1:31:04.840
 but there's a number of companies,

1:31:04.840 --> 1:31:07.840
 the co robot, I would call them co robots,

1:31:07.840 --> 1:31:11.840
 that are still getting VC investments.

1:31:11.840 --> 1:31:13.840
 They, some of them have some of the flavor

1:31:13.840 --> 1:31:15.840
 of, like, rethink robotics.

1:31:15.840 --> 1:31:17.840
 Some of them have some of the flavor of, like, Curie.

1:31:17.840 --> 1:31:19.840
 What's a co robot?

1:31:19.840 --> 1:31:25.840
 So basically a robot and human working in the same space.

1:31:25.840 --> 1:31:29.840
 So some of the companies are focused on manufacturing.

1:31:29.840 --> 1:31:35.840
 So having a robot and human working together in a factory,

1:31:35.840 --> 1:31:38.840
 some of these co robots are robots

1:31:38.840 --> 1:31:41.840
 and humans working in the home, working in clinics.

1:31:41.840 --> 1:31:43.840
 Like there's different versions of these companies

1:31:43.840 --> 1:31:44.840
 in terms of their products,

1:31:44.840 --> 1:31:48.840
 but they're all, so rethink robotics would be, like,

1:31:48.840 --> 1:31:51.840
 one of the first, at least well known,

1:31:51.840 --> 1:31:53.840
 companies focused on this space.

1:31:53.840 --> 1:31:56.840
 So I don't know if this is a second batch

1:31:56.840 --> 1:32:00.840
 or if this is still part of the first batch,

1:32:00.840 --> 1:32:01.840
 that I don't know.

1:32:01.840 --> 1:32:03.840
 And then you have all these other companies

1:32:03.840 --> 1:32:06.840
 in this self driving, you know, space.

1:32:06.840 --> 1:32:08.840
 And I don't know if that's a first batch

1:32:08.840 --> 1:32:10.840
 or, again, a second batch.

1:32:10.840 --> 1:32:11.840
 Yeah.

1:32:11.840 --> 1:32:13.840
 So there's a lot of mystery about this now.

1:32:13.840 --> 1:32:15.840
 Of course, it's hard to say that this is the second batch

1:32:15.840 --> 1:32:17.840
 until it proves out, right?

1:32:17.840 --> 1:32:18.840
 Correct.

1:32:18.840 --> 1:32:19.840
 Yeah, exactly.

1:32:19.840 --> 1:32:20.840
 We need a unicorn.

1:32:20.840 --> 1:32:22.840
 Yeah, exactly.

1:32:22.840 --> 1:32:26.840
 Why do you think people are so afraid,

1:32:26.840 --> 1:32:29.840
 at least in popular culture of legged robots

1:32:29.840 --> 1:32:31.840
 like those worked in Boston Dynamics

1:32:31.840 --> 1:32:33.840
 or just robotics in general,

1:32:33.840 --> 1:32:35.840
 if you were to psychoanalyze that fear,

1:32:35.840 --> 1:32:37.840
 what do you make of it?

1:32:37.840 --> 1:32:38.840
 And should they be afraid?

1:32:38.840 --> 1:32:39.840
 Sorry.

1:32:39.840 --> 1:32:40.840
 So should people be afraid?

1:32:40.840 --> 1:32:42.840
 I don't think people should be afraid.

1:32:42.840 --> 1:32:44.840
 But with a caveat.

1:32:44.840 --> 1:32:46.840
 I don't think people should be afraid

1:32:46.840 --> 1:32:50.840
 given that most of us in this world

1:32:50.840 --> 1:32:54.840
 understand that we need to change something, right?

1:32:54.840 --> 1:32:56.840
 So given that.

1:32:56.840 --> 1:33:00.840
 Now, if things don't change, be very afraid.

1:33:00.840 --> 1:33:03.840
 Which is the dimension of change that's needed?

1:33:03.840 --> 1:33:06.840
 So changing, thinking about the ramifications,

1:33:06.840 --> 1:33:08.840
 thinking about like the ethics,

1:33:08.840 --> 1:33:11.840
 thinking about like the conversation is going on, right?

1:33:11.840 --> 1:33:13.840
 It's no longer a...

1:33:13.840 --> 1:33:16.840
 We're going to deploy it and forget that, you know,

1:33:16.840 --> 1:33:19.840
 this is a car that can kill pedestrians

1:33:19.840 --> 1:33:21.840
 that are walking across the street, right?

1:33:21.840 --> 1:33:22.840
 We're not in that stage.

1:33:22.840 --> 1:33:24.840
 We're putting these roads out.

1:33:24.840 --> 1:33:26.840
 There are people out there.

1:33:26.840 --> 1:33:28.840
 A car could be a weapon.

1:33:28.840 --> 1:33:31.840
 People are now, solutions aren't there yet.

1:33:31.840 --> 1:33:34.840
 But people are thinking about this

1:33:34.840 --> 1:33:37.840
 as we need to be ethically responsible

1:33:37.840 --> 1:33:39.840
 as we send these systems out,

1:33:39.840 --> 1:33:42.840
 robotics, medical, self driving.

1:33:42.840 --> 1:33:43.840
 And military too.

1:33:43.840 --> 1:33:44.840
 And military.

1:33:44.840 --> 1:33:46.840
 Which is not as often talked about,

1:33:46.840 --> 1:33:49.840
 but it's really where probably these robots

1:33:49.840 --> 1:33:51.840
 will have a significant impact as well.

1:33:51.840 --> 1:33:52.840
 Correct, correct.

1:33:52.840 --> 1:33:56.840
 Right, making sure that they can think rationally,

1:33:56.840 --> 1:33:58.840
 even having the conversations,

1:33:58.840 --> 1:34:00.840
 who should pull the trigger, right?

1:34:00.840 --> 1:34:02.840
 But overall, you're saying if we start to think

1:34:02.840 --> 1:34:04.840
 more and more as a community about these ethical issues,

1:34:04.840 --> 1:34:06.840
 people should not be afraid.

1:34:06.840 --> 1:34:08.840
 Yeah, I don't think people should be afraid.

1:34:08.840 --> 1:34:10.840
 I think that the return on investment,

1:34:10.840 --> 1:34:13.840
 the impact, positive impact will outweigh

1:34:13.840 --> 1:34:16.840
 any of the potentially negative impacts.

1:34:16.840 --> 1:34:19.840
 Do you have worries of existential threats

1:34:19.840 --> 1:34:23.840
 of robots or AI that some people kind of

1:34:23.840 --> 1:34:26.840
 talk about and romanticize about

1:34:26.840 --> 1:34:29.840
 in the next decade, in the next few decades?

1:34:29.840 --> 1:34:30.840
 No, I don't.

1:34:30.840 --> 1:34:32.840
 Singularity would be an example.

1:34:32.840 --> 1:34:35.840
 So my concept is that, so remember,

1:34:35.840 --> 1:34:38.840
 robots, AI is designed by people.

1:34:38.840 --> 1:34:39.840
 Yes.

1:34:39.840 --> 1:34:40.840
 It has our values.

1:34:40.840 --> 1:34:42.840
 And I always correlate this with

1:34:42.840 --> 1:34:44.840
 a parent and a child, right?

1:34:44.840 --> 1:34:45.840
 So think about it.

1:34:45.840 --> 1:34:46.840
 As a parent, what do we want?

1:34:46.840 --> 1:34:49.840
 We want our kids to have a better life than us.

1:34:49.840 --> 1:34:51.840
 We want them to expand.

1:34:51.840 --> 1:34:55.840
 We want them to experience the world.

1:34:55.840 --> 1:34:58.840
 And then as we grow older, our kids think

1:34:58.840 --> 1:35:00.840
 and know they're smarter and better

1:35:00.840 --> 1:35:04.840
 and more intelligent and have better opportunities

1:35:04.840 --> 1:35:07.840
 and they may even stop listening to us.

1:35:07.840 --> 1:35:10.840
 They don't go out and then kill us, right?

1:35:10.840 --> 1:35:11.840
 Think about it.

1:35:11.840 --> 1:35:14.840
 It's because it's instilled in them values.

1:35:14.840 --> 1:35:17.840
 We instilled in them this whole aspect of community.

1:35:17.840 --> 1:35:19.840
 And yes, even though you're maybe smarter

1:35:19.840 --> 1:35:22.840
 and have more money and da, da, da,

1:35:22.840 --> 1:35:26.840
 it's still about this love, caring relationship.

1:35:26.840 --> 1:35:27.840
 And so that's what I believe.

1:35:27.840 --> 1:35:30.840
 So even if we've created the Singularity

1:35:30.840 --> 1:35:33.840
 and some archaic system back in 1980

1:35:33.840 --> 1:35:36.840
 that suddenly evolves, the fact is it might say,

1:35:36.840 --> 1:35:39.840
 I am smarter, I am sentient.

1:35:39.840 --> 1:35:42.840
 These humans are really stupid.

1:35:42.840 --> 1:35:45.840
 But I think it'll be like, yeah,

1:35:45.840 --> 1:35:47.840
 but I just can't destroy them.

1:35:47.840 --> 1:35:49.840
 Yeah, percent of mental value.

1:35:49.840 --> 1:35:51.840
 Still just to come back

1:35:51.840 --> 1:35:53.840
 for Thanksgiving dinner every once in a while.

1:35:53.840 --> 1:35:54.840
 Exactly.

1:35:54.840 --> 1:35:56.840
 That's so beautifully put.

1:35:56.840 --> 1:35:59.840
 You've also said that The Matrix may be

1:35:59.840 --> 1:36:02.840
 one of your more favorite A.I. related movies.

1:36:02.840 --> 1:36:04.840
 Can you elaborate why?

1:36:04.840 --> 1:36:07.840
 Yeah, it is one of my favorite movies.

1:36:07.840 --> 1:36:10.840
 And it's because it represents

1:36:10.840 --> 1:36:13.840
 kind of all the things I think about.

1:36:13.840 --> 1:36:15.840
 So there's a symbiotic relationship

1:36:15.840 --> 1:36:19.840
 between robots and humans, right?

1:36:19.840 --> 1:36:21.840
 That symbiotic relationship is that they don't destroy us.

1:36:21.840 --> 1:36:23.840
 They enslave us, right?

1:36:23.840 --> 1:36:27.840
 But think about it.

1:36:27.840 --> 1:36:29.840
 Even though they enslaved us,

1:36:29.840 --> 1:36:31.840
 they needed us to be happy, right?

1:36:31.840 --> 1:36:33.840
 And in order to be happy,

1:36:33.840 --> 1:36:34.840
 they had to create this crudy world

1:36:34.840 --> 1:36:36.840
 that they then had to live in, right?

1:36:36.840 --> 1:36:38.840
 That's the whole premise.

1:36:38.840 --> 1:36:43.840
 But then there were humans that had a choice, right?

1:36:43.840 --> 1:36:46.840
 Like you had a choice to stay in this horrific,

1:36:46.840 --> 1:36:50.840
 horrific world where it was your fantasied life

1:36:50.840 --> 1:36:53.840
 with all of the anomalies, perfection, but not accurate.

1:36:53.840 --> 1:36:57.840
 Or you can choose to be on your own

1:36:57.840 --> 1:37:01.840
 and have maybe no food for a couple of days,

1:37:01.840 --> 1:37:04.840
 but you were totally autonomous.

1:37:04.840 --> 1:37:06.840
 And so I think of that as...

1:37:06.840 --> 1:37:07.840
 And that's why.

1:37:07.840 --> 1:37:09.840
 So it's not necessarily us being enslaved,

1:37:09.840 --> 1:37:12.840
 but I think about us having the symbiotic relationship.

1:37:12.840 --> 1:37:15.840
 Robots and A.I., even if they become sentient,

1:37:15.840 --> 1:37:16.840
 they're still part of our society,

1:37:16.840 --> 1:37:19.840
 and they will suffer just as much as we.

1:37:19.840 --> 1:37:20.840
 Just as us.

1:37:20.840 --> 1:37:23.840
 And there will be some kind of equilibrium

1:37:23.840 --> 1:37:26.840
 that we'll have to find, some symbiotic relationship.

1:37:26.840 --> 1:37:28.840
 And then you have the ethicists, the robotics folks,

1:37:28.840 --> 1:37:31.840
 that are like, no, this has got to stop.

1:37:31.840 --> 1:37:35.840
 I will take the other pill in order to make a difference.

1:37:35.840 --> 1:37:40.840
 So if you could hang out for a day with a robot,

1:37:40.840 --> 1:37:43.840
 real or from science fiction, movies, books,

1:37:43.840 --> 1:37:48.840
 safely and get to pick his or her, their brain,

1:37:48.840 --> 1:37:50.840
 who would you pick?

1:37:50.840 --> 1:37:54.840
 I got to say it's data.

1:37:54.840 --> 1:37:55.840
 Data.

1:37:55.840 --> 1:37:57.840
 I was going to say Rosie,

1:37:57.840 --> 1:38:00.840
 but I don't, I'm not really interested in her brain.

1:38:00.840 --> 1:38:02.840
 I'm interested in data's brain.

1:38:02.840 --> 1:38:05.840
 Data, pre or post emotion chip?

1:38:05.840 --> 1:38:06.840
 Pre.

1:38:06.840 --> 1:38:11.840
 But don't you think it'd be a more interesting conversation,

1:38:11.840 --> 1:38:12.840
 post emotion chip?

1:38:12.840 --> 1:38:14.840
 Yeah, it would be drama.

1:38:14.840 --> 1:38:16.840
 And I, you know, I'm human.

1:38:16.840 --> 1:38:20.840
 I deal with drama all the time.

1:38:20.840 --> 1:38:22.840
 But the reason why I want to pick data's brain

1:38:22.840 --> 1:38:26.840
 is because I could have a conversation with him

1:38:26.840 --> 1:38:32.840
 and ask, for example, how can we fix this ethics problem?

1:38:32.840 --> 1:38:35.840
 And he could go through like the rational thinking

1:38:35.840 --> 1:38:39.840
 and through that, he could also help me think through it as well.

1:38:39.840 --> 1:38:41.840
 And so that's, there's like these questions,

1:38:41.840 --> 1:38:43.840
 fundamental questions I think I could ask him.

1:38:43.840 --> 1:38:46.840
 That he would help me also learn from.

1:38:46.840 --> 1:38:49.840
 And that fascinates me.

1:38:49.840 --> 1:38:52.840
 I don't think there's a better place to end it.

1:38:52.840 --> 1:38:54.840
 Ayanna, thank you so much for talking to us in honor.

1:38:54.840 --> 1:38:55.840
 Thank you.

1:38:55.840 --> 1:38:56.840
 Thank you.

1:38:56.840 --> 1:38:57.840
 This was fun.

1:38:57.840 --> 1:38:59.840
 Thanks for listening to this conversation.

1:38:59.840 --> 1:39:02.840
 And thank you to our presenting sponsor, Cash App.

1:39:02.840 --> 1:39:03.840
 Download it.

1:39:03.840 --> 1:39:05.840
 Use code Lex podcast.

1:39:05.840 --> 1:39:09.840
 You'll get $10 and $10 will go to first a STEM education nonprofit

1:39:09.840 --> 1:39:13.840
 that inspires hundreds of thousands of young minds

1:39:13.840 --> 1:39:16.840
 to become future leaders and innovators.

1:39:16.840 --> 1:39:19.840
 If you enjoy this podcast, subscribe on YouTube,

1:39:19.840 --> 1:39:21.840
 give it five stars on Apple podcast,

1:39:21.840 --> 1:39:23.840
 follow on Spotify, support on Patreon,

1:39:23.840 --> 1:39:26.840
 or simply connect with me on Twitter.

1:39:26.840 --> 1:39:29.840
 And now let me leave you with some words of wisdom

1:39:29.840 --> 1:39:32.840
 from Arthur C. Clarke.

1:39:32.840 --> 1:39:35.840
 Whether we are based on carbon or on silicon

1:39:35.840 --> 1:39:37.840
 makes no fundamental difference.

1:39:37.840 --> 1:39:41.840
 We should each be treated with appropriate respect.

1:39:41.840 --> 1:40:10.840
 Thank you for listening and hope to see you next time.

