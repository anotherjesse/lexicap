WEBVTT

00:00.000 --> 00:03.240
 The following is a conversation with Eric Brinjalson.

00:03.240 --> 00:05.840
 He's an economics professor at Stanford

00:05.840 --> 00:09.400
 and the director of Stanford's Digital Economy Lab.

00:09.400 --> 00:13.480
 Previously, he was a long, long time professor at MIT

00:13.480 --> 00:15.200
 where he did groundbreaking work

00:15.200 --> 00:17.760
 on the economics of information.

00:17.760 --> 00:19.800
 He's the author of many books,

00:19.800 --> 00:22.000
 including The Second Machine Age

00:22.000 --> 00:27.000
 and Machine Platform Crowd, coauthored with Andrew McAfee.

00:27.560 --> 00:29.160
 Quick mention of each sponsor,

00:29.160 --> 00:31.680
 followed by some thoughts related to the episode.

00:31.680 --> 00:34.200
 Ventura Watches, the maker of classy,

00:34.200 --> 00:36.080
 well performing watches.

00:36.080 --> 00:39.840
 FourSigmatic, the maker of delicious mushroom coffee.

00:39.840 --> 00:42.920
 ExpressVPN, the VPN I've used for many years

00:42.920 --> 00:45.040
 to protect my privacy on the internet.

00:45.040 --> 00:49.040
 And Cash App, the app I use to send money to friends.

00:49.040 --> 00:51.000
 Please check out these sponsors in the description

00:51.000 --> 00:54.600
 to get a discount and to support this podcast.

00:54.600 --> 00:56.920
 As a side note, let me say that the impact

00:56.920 --> 00:59.240
 of artificial intelligence and automation

00:59.240 --> 01:01.800
 on our economy and our world

01:01.800 --> 01:04.480
 is something worth thinking deeply about.

01:04.480 --> 01:06.400
 Like with many topics that are linked

01:06.400 --> 01:09.160
 to predicting the future evolution of technology,

01:09.160 --> 01:12.720
 it is often too easy to fall into one of two camps,

01:12.720 --> 01:14.800
 the fear of mongering camp

01:14.800 --> 01:18.320
 or the technologically utopianism camp.

01:18.320 --> 01:21.640
 As always, the future will land us the where in between.

01:21.640 --> 01:24.400
 I prefer to wear two hats in these discussions

01:24.400 --> 01:26.560
 and alternate between them often.

01:26.560 --> 01:29.520
 The hat of a pragmatic engineer

01:29.520 --> 01:31.960
 and the hat of a futurist.

01:31.960 --> 01:35.120
 This is probably a good time to mention Andrew Yang,

01:35.120 --> 01:38.080
 the presidential candidate who has been

01:38.080 --> 01:41.160
 one of the high profile thinkers on this topic.

01:41.160 --> 01:42.840
 And I'm sure I will speak with him

01:42.840 --> 01:44.760
 on this podcast eventually.

01:44.760 --> 01:48.680
 A conversation with Andrew has been on the table many times.

01:48.680 --> 01:50.600
 Our schedule is just having aligned,

01:50.600 --> 01:54.440
 especially because I have a strongly held to preference

01:54.440 --> 01:59.440
 for long form, two, three, four hours or more, and in person.

02:00.160 --> 02:03.080
 I work hard to not compromise on this.

02:03.080 --> 02:04.960
 Trust me, it's not easy.

02:04.960 --> 02:07.280
 Even more so in the times of COVID,

02:07.280 --> 02:09.800
 which requires getting tested nonstop,

02:09.800 --> 02:12.640
 staying isolated and doing a lot of costly

02:12.640 --> 02:15.920
 and uncomfortable things that minimize risk for the guests.

02:15.920 --> 02:17.960
 The reason I do this is because to me,

02:17.960 --> 02:20.920
 something is lost in remote conversation.

02:20.920 --> 02:23.560
 That's something that magic,

02:23.560 --> 02:25.320
 I think is worth the effort,

02:25.320 --> 02:29.560
 even if it ultimately leads to a failed conversation.

02:29.560 --> 02:31.480
 This is how I approach life,

02:31.480 --> 02:36.000
 treasuring the possibility of a rare moment of magic.

02:36.000 --> 02:38.440
 I'm willing to go to the ends of the world

02:38.440 --> 02:39.800
 for just such a moment.

02:40.840 --> 02:43.240
 If you enjoy this thing, subscribe on YouTube,

02:43.240 --> 02:45.520
 review it with fast stars on Apple Podcasts,

02:45.520 --> 02:48.120
 follow on Spotify, support on Patreon,

02:48.120 --> 02:51.280
 connect with me on Twitter at Lex Freedman.

02:51.280 --> 02:55.360
 And now, here's my conversation with Eric Greenjawson.

02:56.280 --> 03:00.000
 You posted a quote on Twitter by Albert Bartlett

03:00.000 --> 03:03.440
 saying that the greatest shortcoming of the human race

03:03.440 --> 03:06.540
 is our inability to understand the exponential function.

03:07.960 --> 03:09.920
 Why would you say the exponential growth

03:09.920 --> 03:11.200
 is important to understand?

03:12.320 --> 03:15.200
 Yeah, that quote, I remember posting that.

03:15.200 --> 03:17.400
 It's actually a reprise of something Andy McAfee

03:17.400 --> 03:19.440
 and I said in the second machine age,

03:19.440 --> 03:21.440
 but I posted it in early March

03:21.440 --> 03:23.900
 when COVID was really just beginning to take off

03:23.900 --> 03:25.840
 and I was really scared.

03:25.840 --> 03:28.280
 There were actually only a couple dozen cases,

03:28.280 --> 03:30.000
 maybe less at that time,

03:30.000 --> 03:32.240
 but they were doubling every two or three days

03:32.240 --> 03:35.480
 and I could see, oh my God, this is gonna be a catastrophe

03:35.480 --> 03:37.040
 and it's gonna happen soon,

03:37.040 --> 03:38.960
 but nobody was taking it very seriously

03:38.960 --> 03:41.000
 or not a lot of people were taking it very seriously.

03:41.000 --> 03:45.240
 In fact, I remember I did my last in person conference

03:45.240 --> 03:47.880
 that week, I was flying back from Las Vegas

03:47.880 --> 03:50.840
 and I was the only person on the plane wearing a mask

03:50.840 --> 03:52.320
 and the flight attendant came over to me,

03:52.320 --> 03:54.080
 she looked very concerned and she kind of put her hands

03:54.080 --> 03:55.320
 on my shoulder, she was touching me all over,

03:55.320 --> 03:56.640
 which I wasn't thrilled about

03:56.640 --> 03:59.480
 and she goes, do you have some kind of anxiety disorder?

03:59.480 --> 04:00.560
 Are you okay?

04:00.560 --> 04:03.000
 And I was like, no, it's because of COVID.

04:03.000 --> 04:04.120
 This is early March.

04:04.120 --> 04:08.600
 Early March, but I was worried because I knew I could see,

04:09.720 --> 04:10.880
 or I suspected, I guess,

04:10.880 --> 04:13.400
 that that doubling would continue and it did

04:13.400 --> 04:17.240
 and pretty soon we had thousands of times more cases.

04:17.240 --> 04:19.760
 Most of the time when I use that quote, I try to,

04:19.760 --> 04:22.200
 it's motivated by more optimistic things like Moore's law

04:22.200 --> 04:25.640
 and the wonders of having more computer power,

04:25.640 --> 04:28.680
 but in either case, it can be very counterintuitive.

04:28.680 --> 04:31.640
 I mean, if you walk for 10 minutes,

04:31.640 --> 04:33.160
 you get about 10 times as far away

04:33.160 --> 04:34.920
 as if you walk for one minute.

04:34.920 --> 04:36.160
 That's the way our physical world works.

04:36.160 --> 04:38.240
 That's the way our brains are wired,

04:38.240 --> 04:41.720
 but if something doubles for 10 times as long,

04:41.720 --> 04:43.320
 you don't get 10 times as much,

04:43.320 --> 04:45.480
 you get 1,000 times as much

04:45.480 --> 04:49.640
 and after 20, it's a billion, after 30, it's a,

04:49.640 --> 04:53.640
 no, sorry, after 20, it's a million, after 30, it's a billion.

04:53.640 --> 04:54.720
 And pretty soon after that,

04:54.720 --> 04:57.840
 it just gets to these numbers that you can barely grasp.

04:57.840 --> 05:00.880
 Our world is becoming more and more exponential,

05:00.880 --> 05:03.680
 mainly because of digital technologies.

05:03.680 --> 05:06.440
 So more and more often our intuitions are out of whack

05:06.440 --> 05:11.000
 and that can be good in the case of things creating wonders,

05:11.000 --> 05:13.760
 but it can be dangerous in the case of viruses

05:13.760 --> 05:14.680
 and other things.

05:14.680 --> 05:16.520
 Do you think it generally applies?

05:16.520 --> 05:18.360
 Like, is there spaces where it does apply

05:18.360 --> 05:19.560
 and where it doesn't?

05:19.560 --> 05:21.880
 How are we supposed to build an intuition

05:21.880 --> 05:25.440
 about in which aspects of our society

05:25.440 --> 05:27.560
 does exponential growth apply?

05:27.560 --> 05:29.760
 Well, you can learn the math,

05:29.760 --> 05:31.240
 but the truth is our brains,

05:31.240 --> 05:35.560
 I think tend to be learned more from experiences.

05:35.560 --> 05:37.680
 So we just start seeing it more and more often.

05:37.680 --> 05:39.600
 So hanging around Silicon Valley,

05:39.600 --> 05:41.800
 hanging around AI and computer researchers,

05:41.800 --> 05:45.040
 I see this kind of exponential growth a lot more frequently.

05:45.040 --> 05:47.000
 And I'm getting used to it, but I still make mistakes.

05:47.000 --> 05:48.920
 I still underestimate some of the progress

05:48.920 --> 05:51.160
 in just talking to someone about GPT3

05:51.160 --> 05:54.280
 and how rapidly natural language has improved.

05:54.280 --> 05:58.520
 But I think that as the world becomes more exponential,

05:58.520 --> 06:02.000
 we'll all start experiencing it more frequently.

06:02.000 --> 06:05.600
 The danger is that we may make some mistakes in the meantime

06:05.600 --> 06:07.920
 using our old kind of caveman intuitions

06:07.920 --> 06:09.520
 about how the world works.

06:09.520 --> 06:12.000
 Well, the weird thing is it always kind of looks linear

06:12.000 --> 06:12.920
 in the moment.

06:12.920 --> 06:17.160
 Like the, you know, it's hard to feel,

06:17.160 --> 06:22.160
 it's hard to retrospect and really acknowledge

06:22.320 --> 06:24.240
 how much has changed in just a couple of years

06:24.240 --> 06:27.440
 or five years or 10 years with the internet.

06:27.440 --> 06:29.880
 If we just look at investments of AI

06:29.880 --> 06:31.920
 or even just social media,

06:31.920 --> 06:33.920
 all the various technologies

06:33.920 --> 06:35.840
 that go into the digital umbrella.

06:35.840 --> 06:36.680
 Yeah.

06:36.680 --> 06:39.680
 It feels pretty calm and normal and gradual.

06:39.680 --> 06:40.960
 Well, a lot of stuff, you know,

06:40.960 --> 06:42.200
 I think there are parts of the world,

06:42.200 --> 06:44.480
 most of the world is not exponential.

06:44.480 --> 06:47.200
 You know, the way humans learn,

06:47.200 --> 06:49.440
 the way organizations change,

06:49.440 --> 06:52.240
 the way our whole institutions adapt and evolve,

06:52.240 --> 06:54.760
 those don't improve at exponential paces.

06:54.760 --> 06:56.560
 And that leads to a mismatch oftentimes

06:56.560 --> 06:59.160
 between these exponentially improving technologies

06:59.160 --> 07:00.640
 or let's say changing technologies

07:00.640 --> 07:03.240
 because some of them are exponentially more dangerous

07:03.240 --> 07:06.920
 and our intuitions and our human skills

07:06.920 --> 07:11.920
 and our institutions that just don't change very fast at all.

07:11.920 --> 07:13.960
 And that mismatch I think is at the root

07:13.960 --> 07:15.680
 of a lot of the problems in our society,

07:15.680 --> 07:20.680
 the growing inequality and other dysfunctions

07:21.880 --> 07:24.440
 in our political and economic systems.

07:24.440 --> 07:28.440
 So one guy that talks about exponential functions

07:28.440 --> 07:29.640
 a lot is Elon Musk.

07:29.640 --> 07:33.000
 He seems to internalize this kind of way

07:33.000 --> 07:34.800
 of exponential thinking.

07:34.800 --> 07:37.400
 He calls it first principles thinking,

07:37.400 --> 07:40.600
 sort of the kind of going to the basics,

07:40.600 --> 07:44.280
 asking the question like what were the assumptions

07:44.280 --> 07:47.760
 of the past, how can we throw them out the window?

07:47.760 --> 07:50.200
 How can we do this 10x much more efficiently

07:50.200 --> 07:52.440
 and constantly practicing that process?

07:52.440 --> 07:57.440
 And also using that kind of thinking to estimate sort of

07:57.440 --> 08:02.440
 when create deadlines and estimate

08:02.960 --> 08:07.200
 when you'll be able to deliver on some of these technologies.

08:07.200 --> 08:10.240
 Now, it often gets him in trouble

08:10.240 --> 08:13.200
 because he overestimates,

08:13.200 --> 08:18.040
 like he doesn't meet the initial estimates of the deadlines

08:18.040 --> 08:23.040
 but he seems to deliver late but deliver.

08:23.440 --> 08:25.040
 And which is kind of an interesting,

08:25.040 --> 08:26.880
 like what are your thoughts about this whole thing?

08:26.880 --> 08:28.840
 I think we can all learn from Elon.

08:28.840 --> 08:30.120
 I think going to first principles,

08:30.120 --> 08:32.840
 I talked about two ways of getting more of a grip

08:32.840 --> 08:34.560
 on the exponential function.

08:34.560 --> 08:36.440
 And one of them just comes from first principles.

08:36.440 --> 08:37.800
 If you understand the math of it,

08:37.800 --> 08:39.040
 you can see what's going to happen.

08:39.040 --> 08:41.040
 And even if it seems counterintuitive

08:41.040 --> 08:42.960
 that a couple of dozen of COVID cases

08:42.960 --> 08:45.760
 could become thousands or tens or hundreds

08:45.760 --> 08:48.080
 of thousands of them in a month,

08:48.080 --> 08:51.200
 it makes sense once you just do the math.

08:51.200 --> 08:53.920
 And I think Elon tries to do that a lot.

08:53.920 --> 08:57.040
 I think he also benefits from hanging out in Silicon Valley

08:57.040 --> 09:00.640
 and he's experienced it in a lot of different applications.

09:00.640 --> 09:04.120
 So, it's not as much of a shock to him anymore

09:04.120 --> 09:06.320
 but that's something we can all learn from.

09:07.200 --> 09:10.440
 In my own life, I remember one of my first experiences

09:10.440 --> 09:13.000
 really seeing it was when I was a grad student

09:13.000 --> 09:17.600
 and my advisor asked me to plot the growth of computer power

09:17.600 --> 09:20.040
 in the US economy in different industries.

09:20.040 --> 09:23.040
 And there are all these exponentially growing curves

09:23.040 --> 09:24.560
 and I was like, holy shit, look at this.

09:24.560 --> 09:26.880
 In each industry, it was just taking off.

09:26.880 --> 09:29.200
 And you didn't have to be a rocket scientist

09:29.200 --> 09:31.880
 to extend that and say, wow, this means that

09:31.880 --> 09:33.600
 this was in the late 80s and early 90s

09:33.600 --> 09:35.880
 that if it goes anything like that,

09:35.880 --> 09:37.320
 we're gonna have orders of magnitude

09:37.320 --> 09:39.480
 more computer power than we did at that time.

09:39.480 --> 09:41.320
 And of course we do.

09:41.320 --> 09:43.320
 So, when people look at Moore's law,

09:45.000 --> 09:46.880
 they often talk about it as just,

09:46.880 --> 09:51.360
 so the exponential function is actually a stack of S curves.

09:51.360 --> 09:56.360
 So basically it's you milk or whatever,

09:57.240 --> 10:01.240
 take the most advantage of a particular little revolution

10:01.240 --> 10:03.000
 and then you search for another revolution.

10:03.000 --> 10:06.720
 And it's basically revolutions stack on top of revolutions.

10:06.720 --> 10:08.960
 Do you have any intuition about how the head humans

10:08.960 --> 10:12.280
 keep finding ways to revolutionize things?

10:12.280 --> 10:14.200
 Well, first let me just unpack that first point

10:14.200 --> 10:17.160
 that I talked about exponential curves

10:17.160 --> 10:21.520
 but no exponential curve continues forever.

10:21.520 --> 10:25.000
 It's been said that if anything can't go on forever,

10:25.000 --> 10:26.720
 eventually it will stop.

10:26.720 --> 10:27.560
 And...

10:27.560 --> 10:28.400
 That's very profound.

10:28.400 --> 10:31.960
 It's very profound, but it seems that a lot of people

10:31.960 --> 10:34.000
 don't appreciate that half of it as well either.

10:34.000 --> 10:35.840
 And that's why all exponential functions

10:35.840 --> 10:38.280
 eventually turn into some kind of S curve

10:38.280 --> 10:41.280
 or stop in some other way, maybe catastrophically.

10:41.280 --> 10:42.840
 And that's happened with COVID as well.

10:42.840 --> 10:44.600
 I mean, it was, it went up and then it sort of,

10:44.600 --> 10:47.200
 at some point it starts saturating the pool

10:47.200 --> 10:49.080
 of people to be infected.

10:49.080 --> 10:51.240
 There's a standard epidemiological model

10:51.240 --> 10:53.000
 that's based on that.

10:53.000 --> 10:55.080
 And it's beginning to happen with Moore's law

10:55.080 --> 10:56.960
 or different generations of computer power.

10:56.960 --> 10:59.320
 It happens with all exponential curves.

10:59.320 --> 11:01.080
 The remarkable thing is you alluded,

11:01.080 --> 11:03.600
 the second part of your question is that we've been able

11:03.600 --> 11:06.880
 to come up with a new S curve on top of the previous one

11:06.880 --> 11:09.120
 and do that generation after generation

11:09.120 --> 11:12.120
 with new materials, new processes

11:12.120 --> 11:14.560
 and just extend it further and further.

11:15.720 --> 11:17.440
 I don't think anyone has a really good theory

11:17.440 --> 11:21.440
 about why we've been so successful in doing that.

11:21.440 --> 11:23.200
 It's great that we have been

11:23.200 --> 11:26.440
 and I hope it continues for some time.

11:26.440 --> 11:31.440
 But it's, you know, one beginning of a theory

11:31.520 --> 11:34.480
 is that there's huge incentives when other parts

11:34.480 --> 11:36.920
 of the system are going on that clock speed

11:36.920 --> 11:39.320
 of doubling every two to three years.

11:39.320 --> 11:42.160
 If there's one component of it that's not keeping up,

11:42.160 --> 11:44.840
 then the economic incentives become really large

11:44.840 --> 11:46.240
 to improve that one part.

11:46.240 --> 11:49.240
 It becomes a bottleneck and anyone who can do

11:49.240 --> 11:51.680
 improvements in that part can reap huge returns

11:51.680 --> 11:54.040
 so that the resources automatically get focused

11:54.040 --> 11:56.600
 on whatever part of the system is in keeping up.

11:56.600 --> 11:59.720
 Do you think some version of the Moore's law will continue?

11:59.720 --> 12:01.400
 Some version, yes, it is.

12:01.400 --> 12:04.640
 I mean, one version that has become more important

12:04.640 --> 12:06.280
 is something I call Kumi's law,

12:06.280 --> 12:08.480
 which is named after John Kumi,

12:08.480 --> 12:10.280
 which I should mention was also my college roommate,

12:10.280 --> 12:14.360
 but he identified the fact that energy consumption

12:14.360 --> 12:17.240
 has been declining by a factor of two.

12:17.240 --> 12:18.960
 And for most of us, that's more important, you know,

12:18.960 --> 12:21.320
 the new iPhones came out today as we're recording this.

12:21.320 --> 12:23.080
 I'm not sure when you're gonna make it available.

12:23.080 --> 12:24.920
 Very soon after this, yeah.

12:24.920 --> 12:27.360
 And for most of us, you know, having the iPhone

12:27.360 --> 12:30.840
 be twice as fast, you know, it's nice,

12:30.840 --> 12:33.160
 but having it the battery life longer,

12:33.160 --> 12:35.440
 that would be much more valuable.

12:35.440 --> 12:38.200
 And the fact that a lot of the progress in chips now

12:38.200 --> 12:42.800
 is reducing energy consumption is probably more important

12:42.800 --> 12:46.040
 for many applications than just the raw speed.

12:46.040 --> 12:50.120
 Other dimensions of Moore's law are in AI and machine learning.

12:51.560 --> 12:55.160
 Those tend to be very parallelizable functions,

12:55.160 --> 12:58.440
 especially deep neural nets.

12:58.440 --> 13:01.280
 And so instead of having one chip,

13:01.280 --> 13:05.000
 you can have multiple chips or you can have a GPU,

13:05.000 --> 13:07.000
 a graphic processing unit that goes faster

13:07.000 --> 13:09.640
 and now special chips designed for machine learning

13:09.640 --> 13:11.240
 like tensor processing units.

13:11.240 --> 13:13.040
 Each time you switch, there's another 10X

13:13.040 --> 13:16.760
 or 100X improvement above and beyond Moore's law.

13:16.760 --> 13:18.880
 So I think that the raw silicon

13:18.880 --> 13:20.800
 isn't improving as much as it used to,

13:20.800 --> 13:23.960
 but these other dimensions are becoming important,

13:23.960 --> 13:26.320
 more important than we're seeing progress in them.

13:26.320 --> 13:28.240
 I don't know if you've seen the work by OpenAI

13:28.240 --> 13:31.960
 where they show the exponential improvement

13:31.960 --> 13:34.400
 of the training of neural networks,

13:34.400 --> 13:36.960
 just literally in the techniques used.

13:36.960 --> 13:40.520
 So that's almost like the algorithm.

13:40.520 --> 13:43.640
 It's fascinating to think like, can I actually continue?

13:43.640 --> 13:45.160
 Us figuring out more and more tricks

13:45.160 --> 13:47.000
 on how to train networks faster.

13:47.000 --> 13:49.520
 Well, the progress has been staggering.

13:49.520 --> 13:51.720
 If you look at image recognition, as you mentioned,

13:51.720 --> 13:53.440
 I think it's a function of at least three things

13:53.440 --> 13:54.520
 that are coming together.

13:54.520 --> 13:56.560
 One, we just talked about faster chips,

13:56.560 --> 14:00.400
 not just Moore's law, but GPUs, TPUs and other technologies.

14:00.400 --> 14:02.760
 The second is just a lot more data.

14:02.760 --> 14:05.600
 I mean, we are awash in digital data today

14:05.600 --> 14:08.080
 in a way we weren't 20 years ago.

14:08.080 --> 14:09.960
 Photography, I'm old enough to remember,

14:09.960 --> 14:12.800
 used to be chemical and now everything is digital.

14:12.800 --> 14:16.440
 I took probably 50 digital photos yesterday.

14:16.440 --> 14:17.880
 I wouldn't have done that if it was chemical.

14:17.880 --> 14:20.680
 And we have the internet of things

14:20.680 --> 14:22.920
 and all sorts of other types of data.

14:22.920 --> 14:24.120
 When we walk around with our phone,

14:24.120 --> 14:27.240
 it's just broadcasting a huge amount of digital data

14:27.240 --> 14:29.240
 that can be used as training sets.

14:29.240 --> 14:30.800
 And then last but not least,

14:30.800 --> 14:34.280
 as they mentioned at OpenAI,

14:34.280 --> 14:37.320
 there have been significant improvements in the techniques.

14:37.320 --> 14:39.240
 The core idea of deep neural nets

14:39.240 --> 14:41.200
 has been around for a few decades,

14:41.200 --> 14:44.200
 but the advances in making it work more efficiently

14:44.200 --> 14:48.200
 have also improved a couple of orders of magnitude or more.

14:48.200 --> 14:50.760
 So you multiply together 100 fold improvement

14:50.760 --> 14:55.360
 in computer power, 100 fold or more improvement in data,

14:55.360 --> 14:59.680
 100 fold improvement in techniques of software

14:59.680 --> 15:01.480
 and algorithms, and soon you're getting

15:01.480 --> 15:03.920
 into a million fold improvements.

15:03.920 --> 15:08.920
 Somebody brought this idea with GPT3 that it's,

15:09.920 --> 15:11.960
 so it's trained in a self supervised way

15:11.960 --> 15:13.960
 on basically internet data.

15:15.000 --> 15:18.920
 And that's one of the, I've seen arguments made

15:18.920 --> 15:21.120
 and they seem to be pretty convincing

15:21.120 --> 15:23.440
 that the bottleneck there is going to be

15:23.440 --> 15:25.640
 how much data there is on the internet,

15:25.640 --> 15:29.160
 which is a fascinating idea that it literally

15:29.160 --> 15:33.280
 will just run out of human generated data to train on.

15:33.280 --> 15:34.120
 It's a...

15:34.120 --> 15:35.760
 I know we make it the point where it's consumed

15:35.760 --> 15:37.520
 basically all of human knowledge

15:37.520 --> 15:39.160
 or all digitized human knowledge, yeah.

15:39.160 --> 15:40.880
 And that would be the bottleneck.

15:40.880 --> 15:44.560
 But the interesting thing with bottlenecks

15:44.560 --> 15:47.680
 is people often use bottlenecks as a way

15:47.680 --> 15:49.960
 to argue against exponential growth.

15:49.960 --> 15:52.480
 They say, well, there's no way you can

15:52.480 --> 15:53.880
 overcome this bottleneck,

15:53.880 --> 15:57.000
 but we seem to somehow keep coming up in new ways

15:57.000 --> 16:01.200
 to overcome whatever bottlenecks the critics come up with.

16:01.200 --> 16:02.040
 Which is fascinating.

16:02.040 --> 16:04.640
 I don't know how you overcome the data bottleneck,

16:04.640 --> 16:07.040
 but probably more efficient training algorithms.

16:07.040 --> 16:08.280
 Yeah, well, you already mentioned that,

16:08.280 --> 16:10.280
 that these training algorithms are getting much better

16:10.280 --> 16:12.480
 at using smaller amounts of data.

16:12.480 --> 16:15.320
 We also are just capturing a lot more data

16:15.320 --> 16:17.680
 than we used to, especially in China,

16:17.680 --> 16:18.920
 but all around us.

16:18.920 --> 16:20.560
 So those are both important.

16:20.560 --> 16:24.200
 In some applications, you can simulate the data,

16:24.200 --> 16:28.960
 video games, some of the self driving car systems

16:28.960 --> 16:30.720
 are, you know, simulating driving.

16:30.720 --> 16:34.320
 And of course that has some risks and weaknesses,

16:34.320 --> 16:37.560
 but you can also, and if you want to, you know,

16:37.560 --> 16:39.920
 exhaust all the different ways you could beat a video game,

16:39.920 --> 16:42.360
 you could just simulate all the options.

16:42.360 --> 16:45.000
 Can we take a step in that direction of autonomous vehicles?

16:45.000 --> 16:48.680
 Make sure you're talking to the CTO of Waymo tomorrow.

16:48.680 --> 16:52.440
 And obviously, I'm talking to Elon again in a couple of weeks.

16:52.440 --> 16:57.080
 What's your thoughts on autonomous vehicles?

16:57.080 --> 17:01.880
 Like, where do we stand as a problem

17:01.880 --> 17:04.560
 that has the potential of revolutionizing the world?

17:04.560 --> 17:06.840
 Well, you know, I'm really excited about that,

17:06.840 --> 17:10.000
 but it's become much clearer that the original way

17:10.000 --> 17:11.440
 that I thought about it, and most people thought about it,

17:11.440 --> 17:13.520
 like, you know, will we have a self driving car or not,

17:13.520 --> 17:15.720
 is way too simple.

17:15.720 --> 17:18.360
 The better way to think about it is that there's a whole

17:18.360 --> 17:21.880
 continuum of how much driving and assisting

17:21.880 --> 17:22.720
 a car can do.

17:22.720 --> 17:24.800
 I noticed that you're right next to your next door

17:24.800 --> 17:26.000
 to Toyota Research Institute.

17:26.000 --> 17:27.640
 That's a total accident.

17:27.640 --> 17:29.360
 I love the TRI folks, but yeah.

17:29.360 --> 17:30.840
 Have you talked to Gil Pratt?

17:30.840 --> 17:33.800
 Yeah, we're going to, we're supposed to talk.

17:33.800 --> 17:35.000
 It's kind of hilarious.

17:35.000 --> 17:37.000
 So there's kind of the, I think it's a good counterpart

17:37.000 --> 17:40.080
 to see what Elon is doing, and hopefully they can be frank

17:40.080 --> 17:41.480
 in how they think about each other,

17:41.480 --> 17:43.920
 because I've heard both of them talk about it.

17:43.920 --> 17:46.360
 But they're much more, you know, this is an assistive,

17:46.360 --> 17:48.600
 a guardian angel that watches over you,

17:48.600 --> 17:50.480
 as opposed to try to do everything.

17:50.480 --> 17:53.360
 I think there's some things like driving on a highway,

17:53.360 --> 17:55.360
 you know, from LA to Phoenix,

17:55.360 --> 17:58.160
 where it's mostly good weather, straight roads.

17:58.160 --> 18:01.280
 That's close to a solved problem, let's face it.

18:01.280 --> 18:03.680
 In other situations, you know, driving through the snow

18:03.680 --> 18:06.240
 and in Boston, where the roads are kind of crazy.

18:06.240 --> 18:08.240
 And most importantly, you have to make a lot of judgments

18:08.240 --> 18:09.480
 about what the other driver's going to do

18:09.480 --> 18:11.720
 at these intersections that aren't really right angles

18:11.720 --> 18:13.440
 and aren't very well described.

18:13.440 --> 18:15.360
 It's more like game theory.

18:15.360 --> 18:19.120
 That's a much harder problem and requires understanding

18:19.120 --> 18:23.400
 of human motivations, and so there's a continuum there

18:23.400 --> 18:27.600
 of some places where the cars will work very well

18:27.600 --> 18:30.960
 and others where it could probably take decades.

18:30.960 --> 18:32.560
 What do you think about the Waymo?

18:33.520 --> 18:36.040
 So as you mentioned, two companies

18:36.040 --> 18:38.080
 that actually have cars on the road,

18:38.080 --> 18:40.680
 there's the Waymo approach that it's more like,

18:40.680 --> 18:42.840
 we're not going to release anything until it's perfect

18:42.840 --> 18:46.720
 and we're going to be very strict about the streets

18:46.720 --> 18:49.200
 that we travel on, but it'd be perfect.

18:49.200 --> 18:50.240
 Yeah.

18:50.240 --> 18:53.960
 Well, I'm smart enough to be humble

18:53.960 --> 18:55.080
 and not try to get between.

18:55.080 --> 18:57.080
 I know there's very bright people on both sides

18:57.080 --> 18:58.080
 of the argument, I've talked to them

18:58.080 --> 19:00.040
 and they make convincing arguments to me

19:00.040 --> 19:01.720
 about how careful they need to be

19:01.720 --> 19:04.040
 and the social acceptance.

19:04.040 --> 19:07.440
 Some people thought that when the first few people died

19:07.440 --> 19:09.880
 from self driving cars, I would shut down the industry,

19:09.880 --> 19:11.840
 but it was more of a blip actually.

19:11.840 --> 19:14.480
 And you know, so that was interesting.

19:14.480 --> 19:16.080
 Of course, there's still a concern

19:16.080 --> 19:19.040
 that if there could be setbacks,

19:19.040 --> 19:21.480
 if we do this wrong, you know,

19:21.480 --> 19:22.640
 your listeners may be familiar

19:22.640 --> 19:24.160
 with the different levels of self driving,

19:24.160 --> 19:26.800
 you know, level one, two, three, four, five.

19:26.800 --> 19:28.760
 I think Andrew Ang has convinced me

19:28.760 --> 19:32.600
 that this idea of really focusing on level four,

19:32.600 --> 19:35.000
 where you only go in areas that are well mapped,

19:35.000 --> 19:37.480
 rather than just going out in the wild,

19:37.480 --> 19:39.720
 is the way things are going to evolve.

19:39.720 --> 19:42.600
 But you can just keep expanding those areas

19:42.600 --> 19:44.040
 where you've mapped things really well,

19:44.040 --> 19:45.000
 where you really understand them

19:45.000 --> 19:47.960
 and eventually they all become kind of interconnected.

19:47.960 --> 19:51.200
 And that could be a kind of another way of progressing

19:52.120 --> 19:55.280
 to make it more feasible over time.

19:55.280 --> 19:57.440
 I mean, that's kind of like the Waymo approach,

19:57.440 --> 19:59.560
 which is they just now released,

19:59.560 --> 20:01.520
 I think just like a day or two ago,

20:04.080 --> 20:09.080
 anyone from the public in the Phoenix, Arizona,

20:11.160 --> 20:14.400
 to, you know, you can get a ride in a Waymo car

20:14.400 --> 20:16.160
 with no person, no driver.

20:16.160 --> 20:17.680
 Oh, they've taken away the safety driver?

20:17.680 --> 20:21.080
 Oh yeah, for a while now there's been no safety driver.

20:21.080 --> 20:22.520
 Okay, because I mean, I've been following

20:22.520 --> 20:24.880
 that one in particular, but I thought it was kind of funny

20:24.880 --> 20:27.000
 about a year ago when they had the safety driver

20:27.000 --> 20:28.440
 and then they added a second safety driver

20:28.440 --> 20:30.920
 because the first safety driver would fall asleep.

20:30.920 --> 20:33.400
 I'm not sure they're going in the right direction with that.

20:33.400 --> 20:38.200
 No, they've Waymo in particular

20:38.200 --> 20:39.520
 done a really good job with that.

20:39.520 --> 20:44.360
 They actually have a very interesting infrastructure.

20:44.360 --> 20:47.480
 Of remote like observation.

20:47.480 --> 20:49.840
 So they're not controlling the vehicles remotely,

20:49.840 --> 20:52.440
 but they're able to, it's like a customer service.

20:52.440 --> 20:55.520
 They can anytime tune into the car.

20:55.520 --> 20:58.160
 I bet they can probably remotely control it as well,

20:58.160 --> 21:00.920
 but that's officially not the function that they.

21:00.920 --> 21:02.760
 Yeah, I can see that being really,

21:02.760 --> 21:06.280
 because I think the thing that's proven harder

21:06.280 --> 21:08.200
 than maybe some of the early people expected was

21:08.200 --> 21:10.840
 there's a long tail of weird exceptions.

21:10.840 --> 21:15.440
 So you can deal with 90, 99, 99.99% of the cases,

21:15.440 --> 21:17.440
 but then there's something that just never been seen

21:17.440 --> 21:18.840
 before in the training data.

21:18.840 --> 21:21.120
 And humans more or less can work around that,

21:21.120 --> 21:22.880
 although let me be clear and note,

21:22.880 --> 21:25.640
 there are about 30,000 human fatalities

21:25.640 --> 21:28.400
 just in the United States and maybe a million worldwide.

21:28.400 --> 21:30.040
 So they're far from perfect,

21:30.040 --> 21:33.480
 but I think people have higher expectations of machines.

21:33.480 --> 21:36.320
 They wouldn't tolerate that level of death

21:36.320 --> 21:40.080
 and damage from a machine.

21:40.080 --> 21:41.840
 And so we have to do a lot better

21:41.840 --> 21:43.520
 at dealing with those edge cases.

21:43.520 --> 21:45.920
 And also the tricky thing that,

21:45.920 --> 21:47.920
 if I have a criticism for the Waymo folks,

21:47.920 --> 21:50.640
 there's such a huge focus on safety,

21:51.840 --> 21:55.200
 where people don't talk enough about creating products

21:55.200 --> 22:00.200
 that customers love, that human beings love using.

22:00.360 --> 22:03.360
 You know, it's very easy to create a thing that's safe

22:03.360 --> 22:06.960
 at the extremes, but then nobody wants to get into it.

22:06.960 --> 22:08.760
 Yeah, well, back to Elon.

22:08.760 --> 22:10.560
 I think one of, part of his genius

22:10.560 --> 22:12.840
 was with the electric cars, before he came along,

22:12.840 --> 22:15.720
 electric cars were all kind of underpowered, really light,

22:15.720 --> 22:20.720
 and there were sort of wimpy cars that weren't fun.

22:20.720 --> 22:22.680
 And the first thing he did was,

22:22.680 --> 22:25.680
 he made a roadster that went zero to 60 faster

22:25.680 --> 22:28.520
 than just about any other car and went the other end.

22:28.520 --> 22:30.840
 And I think that was a really wise marketing move

22:30.840 --> 22:33.200
 as well as a wise technology move.

22:33.200 --> 22:34.880
 Yeah, it's difficult to figure out

22:34.880 --> 22:38.000
 what right marketing move is for AI systems.

22:38.000 --> 22:42.000
 That's always been, I think it requires guts

22:42.000 --> 22:46.440
 and risk taking, which is what Elon practices,

22:46.440 --> 22:50.600
 I mean, to the chagrin of perhaps investors or whatever,

22:50.600 --> 22:54.440
 but it also requires rethinking what you're doing.

22:54.440 --> 22:57.640
 I think way too many people are unimaginative,

22:57.640 --> 22:59.880
 intellectually lazy, and when they take AI,

22:59.880 --> 23:01.760
 they basically say, what are we doing now?

23:01.760 --> 23:04.120
 How can we make a machine do the same thing?

23:04.120 --> 23:06.840
 Maybe we'll save some costs, we'll have less labor.

23:06.840 --> 23:08.640
 And yeah, it's not necessarily the worst thing

23:08.640 --> 23:10.720
 in the world to do, but it's really not leading

23:10.720 --> 23:13.360
 to a quantum change in the way you do things.

23:13.360 --> 23:16.640
 When Jeff Bezos said, hey, we're gonna use the internet

23:16.640 --> 23:19.280
 to change how bookstores work, and we're gonna use technology,

23:19.280 --> 23:22.720
 he didn't go and say, okay, let's put a robot cashier

23:22.720 --> 23:25.640
 where the human cashier is and leave everything else alone.

23:25.640 --> 23:27.080
 That would have been a very lame way

23:27.080 --> 23:28.360
 to automate a bookstore.

23:28.360 --> 23:30.000
 He went from soup to nut to say,

23:30.000 --> 23:33.040
 let's just rethink it, we get rid of the physical bookstore,

23:33.040 --> 23:34.520
 we have a warehouse, we have delivery,

23:34.520 --> 23:36.360
 we have people order on a screen,

23:36.360 --> 23:38.440
 and everything was reinvented.

23:38.440 --> 23:41.640
 And that's been the story of these general purpose

23:41.640 --> 23:43.520
 technologies all through history.

23:43.520 --> 23:46.280
 In my books, I write about electricity

23:46.280 --> 23:50.320
 and how for 30 years, there was almost no productivity gain

23:50.320 --> 23:52.960
 from the electrification of factories a century ago.

23:52.960 --> 23:54.240
 And that's not because electricity

23:54.240 --> 23:55.760
 is a wimpy, useless technology,

23:55.760 --> 23:57.520
 we all know how awesome electricity is.

23:57.520 --> 24:00.480
 It's because at first, they really didn't rethink the factories.

24:00.480 --> 24:02.080
 It was only after they reinvented them,

24:02.080 --> 24:04.000
 and we describe how in the book,

24:04.000 --> 24:05.400
 then you suddenly got a doubling

24:05.400 --> 24:07.520
 and tripling of productivity growth.

24:07.520 --> 24:09.880
 But it's the combination of the technology

24:09.880 --> 24:12.920
 with the new business models, new business organization.

24:12.920 --> 24:13.960
 That just takes a long time

24:13.960 --> 24:16.880
 and it takes more creativity than most people have.

24:16.880 --> 24:18.960
 Can you maybe linger on electricity?

24:18.960 --> 24:19.920
 Because that's a fun one.

24:19.920 --> 24:22.440
 Yeah, I'll tell you what happened.

24:22.440 --> 24:25.720
 Before electricity, there were basically steam engines

24:25.720 --> 24:27.000
 or sometimes water wheels.

24:27.000 --> 24:28.320
 And to power the machinery,

24:28.320 --> 24:30.520
 you had to have pulleys and crankshafts.

24:30.520 --> 24:32.080
 And you really can't make them too long

24:32.080 --> 24:34.000
 because they'll break the torsion.

24:34.000 --> 24:36.440
 So all the equipment was kind of clustered around this,

24:36.440 --> 24:37.920
 one giant steam engine.

24:37.920 --> 24:39.440
 You can't make small steam engines either

24:39.440 --> 24:40.760
 because of thermodynamics.

24:40.760 --> 24:42.320
 So you have one giant steam engine,

24:42.320 --> 24:44.320
 all the equipment clustered around it, multi story,

24:44.320 --> 24:46.080
 they have it vertical to minimize the distance

24:46.080 --> 24:47.720
 as well as horizontal.

24:47.720 --> 24:48.920
 And then when they did electricity,

24:48.920 --> 24:50.040
 they took out the steam engine,

24:50.040 --> 24:51.320
 they got the biggest electric motor

24:51.320 --> 24:52.920
 they could buy from General Electric

24:52.920 --> 24:54.160
 or someone like that.

24:54.160 --> 24:56.360
 And nothing much else changed.

24:57.880 --> 25:01.960
 It took until a generation of managers retired or died

25:01.960 --> 25:04.400
 30 years later that people started thinking,

25:04.400 --> 25:05.800
 wait, we don't have to do it that way.

25:05.800 --> 25:07.680
 You can make electric motors,

25:07.680 --> 25:09.440
 big, small, medium.

25:09.440 --> 25:11.480
 You can put one with each piece of equipment.

25:11.480 --> 25:12.320
 There's this big debate,

25:12.320 --> 25:13.360
 if you read the management literature

25:13.360 --> 25:16.120
 between what they call group drive versus unit drive,

25:16.120 --> 25:18.920
 where every machine would have its own motor.

25:18.920 --> 25:19.800
 Well, once they did that,

25:19.800 --> 25:21.000
 once they went to unit drive,

25:21.000 --> 25:23.000
 those guys won the debate,

25:23.000 --> 25:25.040
 then you started having a new kind of factory,

25:25.040 --> 25:28.240
 which is sometimes spread out over acres,

25:28.240 --> 25:31.320
 single story and each piece of equipment had its own motor.

25:31.320 --> 25:32.320
 And most importantly,

25:32.320 --> 25:35.000
 they weren't laid out based on who needed the most power.

25:35.000 --> 25:37.600
 They were laid out based on

25:37.600 --> 25:40.560
 what is the workflow of materials, assembly line.

25:40.560 --> 25:41.720
 Let's have it go from this machine

25:41.720 --> 25:43.240
 to that machine to that machine.

25:43.240 --> 25:46.040
 Once they rethought the factory that way,

25:46.040 --> 25:48.360
 huge increases in productivity was just staggering.

25:48.360 --> 25:50.080
 People like Paul David have documented this

25:50.080 --> 25:51.760
 in their research papers.

25:51.760 --> 25:55.920
 And I think that there's a lesson you see over and over.

25:55.920 --> 25:57.080
 It happened when the steam engine

25:57.080 --> 25:58.560
 changed manual production.

25:58.560 --> 26:00.240
 It's happened with the computerization.

26:00.240 --> 26:01.520
 People like Michael Hammer said,

26:01.520 --> 26:03.600
 don't automate, obliterate.

26:03.600 --> 26:05.160
 In each case,

26:05.160 --> 26:09.320
 the big gains only came once smart entrepreneurs

26:09.320 --> 26:13.000
 and managers basically reinvented their industries.

26:13.000 --> 26:14.640
 I mean, one other interesting point about all that

26:14.640 --> 26:17.880
 is that during that reinvention period,

26:18.880 --> 26:20.640
 you often actually,

26:20.640 --> 26:22.200
 not only don't see productivity growth,

26:22.200 --> 26:24.320
 you can actually see a slipping back,

26:24.320 --> 26:26.520
 measured productivity actually falls.

26:26.520 --> 26:29.040
 I just wrote a paper with Chad Severson and Daniel Rock

26:29.040 --> 26:31.520
 called the Productivity J Curve,

26:31.520 --> 26:33.840
 which basically shows that in a lot of these cases,

26:33.840 --> 26:36.520
 you have a downward dip before it goes up.

26:36.520 --> 26:38.320
 And that downward dip is when everyone's trying

26:38.320 --> 26:40.400
 to reinvent things.

26:40.400 --> 26:43.160
 And you could say that they're creating knowledge

26:43.160 --> 26:44.640
 and intangible assets,

26:44.640 --> 26:46.800
 but that doesn't show up on anyone's balance sheet.

26:46.800 --> 26:48.320
 It doesn't show up in GDP.

26:48.320 --> 26:50.080
 So it's as if they're doing nothing.

26:50.080 --> 26:52.480
 Like take self driving cars, we're just talking about it.

26:52.480 --> 26:55.760
 There have been hundreds of billions of dollars spent

26:55.760 --> 26:57.880
 developing self driving cars.

26:57.880 --> 27:01.560
 And basically no chauffeur has lost his job,

27:01.560 --> 27:02.400
 no taxi driver.

27:02.400 --> 27:03.560
 I guess I gotta check on the one thing.

27:03.560 --> 27:04.480
 It's a big J curve.

27:04.480 --> 27:06.080
 Yeah, so there's a bunch of spending

27:06.080 --> 27:08.120
 and no real consumer benefit.

27:08.120 --> 27:11.440
 Now they're doing that in the belief,

27:11.440 --> 27:13.240
 I think the justified belief

27:13.240 --> 27:15.160
 that they will get the upward part of the J curve

27:15.160 --> 27:16.920
 and they will be some big returns,

27:16.920 --> 27:19.320
 but in the short run, you're not seeing it.

27:19.320 --> 27:21.880
 That's happening with a lot of other AI technologies,

27:21.880 --> 27:25.080
 just as it happened with earlier general purpose technologies.

27:25.080 --> 27:26.520
 And it's one of the reasons we're having

27:26.520 --> 27:29.280
 relatively low productivity growth lately.

27:29.280 --> 27:31.400
 As an economist, one of the things that disappoints me

27:31.400 --> 27:34.440
 is that as eye popping as these technologies are,

27:34.440 --> 27:36.880
 you and I are both excited about some things they can do.

27:36.880 --> 27:40.280
 The economic productivity statistics are kind of dismal.

27:40.280 --> 27:42.200
 We actually believe it or not,

27:42.200 --> 27:45.440
 have had lower productivity growth in the past,

27:45.440 --> 27:48.840
 about 15 years than we did in the previous 15 years,

27:48.840 --> 27:51.360
 in the 90s and early 2000s.

27:51.360 --> 27:53.200
 And so that's not what you would have expected

27:53.200 --> 27:55.520
 if these technologies were that much better.

27:55.520 --> 27:59.400
 But I think we're in kind of a long J curve there.

27:59.400 --> 28:00.560
 Personally, I'm optimistic.

28:00.560 --> 28:02.080
 We'll start seeing the upward tick,

28:02.080 --> 28:04.520
 maybe as soon as next year.

28:04.520 --> 28:08.520
 But the past decade has been a bit disappointing

28:08.520 --> 28:10.000
 if you thought there's a one to one relationship

28:10.000 --> 28:12.760
 between cool technology and higher productivity.

28:12.760 --> 28:15.240
 What would you place your biggest hope

28:15.240 --> 28:17.240
 for productivity increases on?

28:17.240 --> 28:19.840
 Like you kind of said at a high level AI,

28:19.840 --> 28:22.840
 but if I were to think about

28:22.840 --> 28:27.840
 what has been so revolutionary in the last 10 years,

28:28.200 --> 28:32.240
 I would have 15 years and thinking about the internet,

28:32.240 --> 28:34.280
 I would say things like,

28:35.800 --> 28:37.160
 hope I'm not saying anything ridiculous,

28:37.160 --> 28:41.040
 but everything from Wikipedia to Twitter.

28:41.040 --> 28:46.080
 So like these kind of websites, not so much AI,

28:46.080 --> 28:49.760
 but like I would expect to see some kind of big productivity

28:49.760 --> 28:52.680
 increases from just the connectivity

28:52.680 --> 28:57.680
 between people and the access to more information.

28:58.040 --> 29:00.040
 Yeah, well, that's another area

29:00.040 --> 29:01.840
 I've done quite a bit of research on actually

29:01.840 --> 29:06.040
 is these free goods like Wikipedia, Facebook, Twitter,

29:06.040 --> 29:08.120
 Zoom, we're actually doing this in person,

29:08.120 --> 29:11.080
 but almost everything else I do these days is online.

29:12.360 --> 29:14.680
 The interesting thing about all those is

29:14.680 --> 29:17.640
 most of them have a price of zero.

29:17.640 --> 29:19.920
 You know, what do you pay for Wikipedia?

29:19.920 --> 29:21.680
 Maybe like a little bit for the electrons

29:21.680 --> 29:22.960
 to come to your house.

29:22.960 --> 29:24.120
 Basically zero, right?

29:25.600 --> 29:26.880
 Take a small pause and say,

29:26.880 --> 29:28.880
 I donate to Wikipedia often, you should too.

29:28.880 --> 29:30.040
 It's good for you, yeah.

29:30.040 --> 29:32.440
 So, but what does that do mean for GDP?

29:32.440 --> 29:36.080
 GDP is based on the price and quantity

29:36.080 --> 29:37.720
 of all the goods things bought and sold.

29:37.720 --> 29:39.520
 If something has zero price,

29:39.520 --> 29:42.240
 you know how much it contributes to GDP?

29:42.240 --> 29:44.520
 To a first approximation, zero.

29:44.520 --> 29:47.440
 So these digital goods that we're getting more and more

29:47.440 --> 29:50.200
 of we're spending more and more hours a day

29:50.200 --> 29:52.600
 consuming stuff off the screens, little screens,

29:52.600 --> 29:56.120
 big screens, that doesn't get priced into GDP.

29:56.120 --> 29:58.600
 It's like they don't exist.

29:58.600 --> 29:59.920
 That doesn't mean they don't create value.

29:59.920 --> 30:03.320
 I get a lot of value from watching cat videos

30:03.320 --> 30:06.080
 and reading Wikipedia articles and listening to podcasts,

30:06.080 --> 30:08.400
 even if I don't pay for them.

30:08.400 --> 30:10.360
 So we've got a mismatch there.

30:10.360 --> 30:13.400
 Now, in fairness, economists since Simon Kuznets

30:13.400 --> 30:16.200
 invented GDP and productivity, all those statistics

30:16.200 --> 30:19.640
 back in the 1930s, he recognized, he in fact said,

30:19.640 --> 30:21.480
 this is not a measure of wellbeing.

30:21.480 --> 30:23.160
 This is not a measure of welfare.

30:23.160 --> 30:25.080
 It's a measure of production.

30:25.080 --> 30:28.960
 But almost everybody has kind of forgotten

30:28.960 --> 30:30.960
 that he said that and they just use it.

30:30.960 --> 30:32.120
 It's like, how well off are we?

30:32.120 --> 30:33.240
 What was GDP last year?

30:33.240 --> 30:35.760
 It was 2.3% growth or whatever.

30:35.760 --> 30:39.440
 That is how much physical production,

30:39.440 --> 30:42.320
 but it's not the value we're getting.

30:42.320 --> 30:43.800
 We need a new set of statistics.

30:43.800 --> 30:45.280
 And I'm working with some colleagues,

30:45.280 --> 30:48.360
 Avi Collis and others to develop something

30:48.360 --> 30:50.440
 we call GDP dash B.

30:50.440 --> 30:55.440
 GDP B measures the benefits you get, not the cost.

30:55.440 --> 31:00.400
 If you get benefit from Zoom or Wikipedia or Facebook,

31:00.400 --> 31:04.560
 then that gets counted in GDP B, even if you pay zero for it.

31:04.560 --> 31:07.360
 So, you know, back to the original point,

31:07.360 --> 31:10.480
 I think there is a lot of gain over the past decade

31:10.480 --> 31:15.280
 in these digital goods that doesn't show up in GDP,

31:15.280 --> 31:16.440
 doesn't show up in productivity.

31:16.440 --> 31:18.560
 By the way, productivity is just defined as GDP

31:18.560 --> 31:20.080
 divided by hours worked.

31:20.080 --> 31:23.200
 So, if you mismeasure GDP, you mismeasure productivity

31:23.200 --> 31:25.360
 by the exact same amount.

31:25.360 --> 31:26.480
 That's something we need to fix.

31:26.480 --> 31:28.440
 I'm working with the statistical agencies

31:28.440 --> 31:30.200
 to come up with a new set of metrics.

31:30.200 --> 31:33.240
 And over the coming years, I think we'll see,

31:33.240 --> 31:34.480
 we're not going to do away with GDP.

31:34.480 --> 31:37.240
 It's very useful, but we'll see a parallel set of accounts

31:37.240 --> 31:38.400
 that measure the benefits.

31:38.400 --> 31:41.080
 How difficult is it to get that B in the GDP B?

31:41.080 --> 31:41.800
 It's pretty hard.

31:41.800 --> 31:44.360
 I mean, one of the reasons it hasn't been done before

31:44.360 --> 31:46.680
 is that, you know, you can measure it,

31:46.680 --> 31:49.000
 the cash register, what people pay for stuff.

31:49.000 --> 31:51.000
 But how do you measure what they would have paid?

31:51.000 --> 31:53.360
 Like what the value is, that's a lot harder.

31:53.360 --> 31:56.000
 You know, how much is Wikipedia worth to you?

31:56.000 --> 31:57.480
 That's what we have to answer.

31:57.480 --> 31:59.280
 And to do that, what we do is,

31:59.280 --> 32:00.680
 we can use online experiments.

32:00.680 --> 32:03.080
 We do massive online choice experiments.

32:03.080 --> 32:05.680
 We ask hundreds of thousands, not millions of people,

32:05.680 --> 32:07.720
 to do lots of sort of AB tests.

32:07.720 --> 32:09.080
 How much would I have to pay you

32:09.080 --> 32:10.840
 to give up Wikipedia for a month?

32:10.840 --> 32:12.000
 How much would I have to pay you

32:12.000 --> 32:14.120
 to stop using your phone?

32:14.120 --> 32:15.960
 And in some cases, it's hypothetical.

32:15.960 --> 32:17.520
 In other cases, we actually enforce it,

32:17.520 --> 32:18.920
 which is kind of expensive.

32:18.920 --> 32:22.440
 Like we pay somebody $30 to stop using Facebook

32:22.440 --> 32:23.440
 and we see if they'll do it.

32:23.440 --> 32:26.280
 And some people will give it up for $10.

32:26.280 --> 32:27.120
 Some people won't give it up

32:27.120 --> 32:28.880
 even if you give them $100.

32:28.880 --> 32:31.080
 And then you get a whole demand curve.

32:31.080 --> 32:33.600
 You get to see what all the different prices are

32:33.600 --> 32:36.000
 and how much value different people get.

32:36.000 --> 32:36.920
 And not surprisingly,

32:36.920 --> 32:38.280
 different people have different values.

32:38.280 --> 32:41.560
 We find that women tend to value Facebook more than men.

32:41.560 --> 32:43.240
 Old people tend to value it a little bit more

32:43.240 --> 32:44.480
 than young people, I was interesting.

32:44.480 --> 32:46.640
 I think young people maybe know about other networks

32:46.640 --> 32:47.720
 that I don't know the name of,

32:47.720 --> 32:49.360
 that are better than Facebook.

32:50.320 --> 32:53.480
 And so you get to see these patterns,

32:53.480 --> 32:55.480
 but every person's individual.

32:55.480 --> 32:57.280
 And then if you add up all those numbers,

32:57.280 --> 33:00.080
 you start getting an estimate of the value.

33:00.080 --> 33:01.280
 Okay, first of all, that's brilliant.

33:01.280 --> 33:05.760
 Is this work that will soon eventually be published?

33:05.760 --> 33:07.080
 Yeah, well, there's a version of it

33:07.080 --> 33:09.560
 in the proceedings of the National Academy of Sciences

33:09.560 --> 33:11.880
 about, I think we call it massive online choice experiments.

33:11.880 --> 33:14.920
 I should remember the title, but it's on my website.

33:14.920 --> 33:17.520
 So yeah, we have some more papers coming out on it,

33:17.520 --> 33:20.160
 but the first one is already out.

33:20.160 --> 33:22.320
 You know, it's kind of a fascinating mystery

33:22.320 --> 33:24.320
 that Twitter, Facebook,

33:24.320 --> 33:26.800
 like all these social networks are free.

33:26.800 --> 33:29.800
 And it seems like almost none of them,

33:29.800 --> 33:31.440
 except for YouTube,

33:31.440 --> 33:35.200
 have experimented with removing ads for money.

33:35.200 --> 33:37.160
 Can you like, do you understand that

33:37.160 --> 33:39.760
 from both economics and the product perspective?

33:39.760 --> 33:41.920
 Yeah, it's something that, so I teach a course

33:41.920 --> 33:43.280
 on digital business models.

33:43.280 --> 33:45.040
 So I used to at MIT at Stanford.

33:45.040 --> 33:45.880
 I'm not quite sure.

33:45.880 --> 33:47.400
 I'm not teaching until next spring.

33:47.400 --> 33:50.080
 I'm still thinking what my course is gonna be.

33:50.080 --> 33:52.240
 But there are a lot of different business models.

33:52.240 --> 33:54.920
 And when you have something that's zero marginal cost,

33:54.920 --> 33:56.440
 there's a lot of forces,

33:56.440 --> 33:57.920
 especially if there's any kind of competition

33:57.920 --> 33:59.960
 that push prices down to zero.

33:59.960 --> 34:03.360
 But you can have ad supported systems.

34:03.360 --> 34:05.560
 You can bundle things together.

34:05.560 --> 34:07.360
 You can have volunteer, you mentioned Wikipedia.

34:07.360 --> 34:08.800
 There's donations.

34:08.800 --> 34:11.680
 And I think economists underestimate the power

34:11.680 --> 34:14.600
 of volunteerism and donations.

34:14.600 --> 34:16.080
 You know, national public radio.

34:16.080 --> 34:17.640
 Actually, how do you, this podcast,

34:17.640 --> 34:19.480
 how is this, what's the revenue model?

34:19.480 --> 34:22.280
 There's sponsors at the beginning.

34:22.280 --> 34:24.680
 And then, and people, the funny thing is,

34:24.680 --> 34:26.680
 I tell people they can, it's very,

34:26.680 --> 34:27.880
 I tell them the timestamp.

34:27.880 --> 34:31.000
 So if you wanna skip the sponsors, you're free.

34:31.000 --> 34:33.600
 But the, it's funny that a bunch of people,

34:33.600 --> 34:36.240
 so I read the advertisement.

34:36.240 --> 34:38.440
 And a bunch of people enjoy reading it.

34:38.440 --> 34:39.800
 Well, they may learn something from it.

34:39.800 --> 34:42.960
 And also, from the advertiser's perspective,

34:42.960 --> 34:45.080
 those are people who are actually interested, you know?

34:45.080 --> 34:46.960
 Like, I mean, the example I sometimes get is like,

34:46.960 --> 34:48.360
 I bought a car recently.

34:48.360 --> 34:50.760
 And all of a sudden, all the car ads were like,

34:50.760 --> 34:51.600
 interesting to me.

34:51.600 --> 34:54.360
 And then like, now that I have the car,

34:54.360 --> 34:56.280
 like I sort of zone out on, okay, but that's fine.

34:56.280 --> 34:58.720
 The car companies, they don't really wanna be advertising

34:58.720 --> 35:01.360
 to me if I'm not gonna buy their product.

35:01.360 --> 35:03.560
 So there are a lot of these different revenue models.

35:03.560 --> 35:06.880
 And, you know, it's a little complicated,

35:06.880 --> 35:08.680
 but the economic theory has to do with what the shape

35:08.680 --> 35:09.520
 of the demand curve is,

35:09.520 --> 35:13.200
 when it's better to monetize it with charging people

35:13.200 --> 35:15.640
 versus when you're better off doing advertising.

35:15.640 --> 35:18.320
 I mean, in short, when the demand curve

35:18.320 --> 35:20.640
 is relatively flat and wide,

35:20.640 --> 35:22.800
 like generic news and things like that,

35:22.800 --> 35:25.960
 then you tend to do better with advertising.

35:25.960 --> 35:28.880
 If it's a good that's only useful to a small number

35:28.880 --> 35:30.360
 of people, but they're willing to pay a lot.

35:30.360 --> 35:32.800
 They have a very high value for it.

35:32.800 --> 35:34.600
 Then you, advertising isn't gonna work as well.

35:34.600 --> 35:36.120
 You're better off charging for it.

35:36.120 --> 35:38.080
 Both of them have some inefficiencies.

35:38.080 --> 35:39.480
 And then when you get into targeting

35:39.480 --> 35:40.600
 and you get into these other revenue models,

35:40.600 --> 35:41.960
 it gets more complicated.

35:41.960 --> 35:45.320
 But there's some economic theory on it.

35:45.320 --> 35:47.560
 I also think, to be frank,

35:47.560 --> 35:49.560
 there's just a lot of experimentation that's needed

35:49.560 --> 35:53.200
 because sometimes things are a little counterintuitive,

35:53.200 --> 35:55.160
 especially when you get into what are called

35:55.160 --> 35:57.680
 two sided networks or platform effects,

35:57.680 --> 36:01.840
 where you may grow the market on one side

36:01.840 --> 36:03.920
 and harvest the revenue on the other side.

36:03.920 --> 36:06.080
 You know, Facebook tries to get more and more users

36:06.080 --> 36:08.960
 and then they harvest the revenue from advertising.

36:08.960 --> 36:12.040
 So that's another way of kind of thinking about it.

36:12.040 --> 36:14.400
 Is it strange to you that they haven't experimented?

36:14.400 --> 36:15.360
 Well, they are experimenting.

36:15.360 --> 36:17.600
 So, you know, they are doing some experiments

36:17.600 --> 36:21.000
 about what the willingness is for people to pay.

36:21.000 --> 36:23.560
 I think that when they do the math,

36:23.560 --> 36:26.360
 it's gonna work out that they still are better off

36:26.360 --> 36:29.400
 with an advertising driven model, but...

36:29.400 --> 36:30.360
 What about a mix?

36:30.360 --> 36:32.360
 Like this is what YouTube is, right?

36:32.360 --> 36:36.360
 Yeah, you allow the person to decide,

36:36.360 --> 36:39.040
 the customer to decide exactly which model they prefer.

36:39.040 --> 36:40.920
 Yeah, no, that can work really well, you know.

36:40.920 --> 36:41.760
 And newspapers, of course,

36:41.760 --> 36:42.760
 I've known this for a long time,

36:42.760 --> 36:44.560
 the Wall Street Journal, the New York Times,

36:44.560 --> 36:45.840
 they had subscription revenue,

36:45.840 --> 36:48.080
 they also have advertising revenue.

36:48.080 --> 36:51.960
 And that can definitely work.

36:51.960 --> 36:54.080
 The online is a lot easier to have a dial

36:54.080 --> 36:55.240
 that's much more personalized

36:55.240 --> 36:57.720
 and everybody can kind of roll their own mix.

36:57.720 --> 36:59.360
 And I could imagine, you know,

36:59.360 --> 37:03.080
 having a little slider about how much advertising

37:03.080 --> 37:05.040
 you want or are willing to take.

37:05.040 --> 37:07.400
 And if it's done right and it's incentive compatible,

37:07.400 --> 37:10.960
 it could be a win win where both the content provider

37:10.960 --> 37:12.560
 and the consumer are better off

37:12.560 --> 37:14.480
 than they would have been before.

37:14.480 --> 37:17.920
 Yeah, you know, the done right part is a really good point.

37:17.920 --> 37:21.080
 Like with Jeff Bezos and the single click purchase

37:21.080 --> 37:23.880
 on Amazon, the frictionless effort there,

37:23.880 --> 37:25.760
 if I could just rant for a second

37:25.760 --> 37:27.240
 about the Wall Street Journal,

37:27.240 --> 37:29.280
 all the newspapers you mentioned

37:29.280 --> 37:34.280
 is I have to click so many times to subscribe to them

37:34.800 --> 37:37.400
 that I literally don't subscribe

37:37.400 --> 37:39.520
 just because of the number of times I have to click.

37:39.520 --> 37:40.360
 I'm totally with you.

37:40.360 --> 37:44.560
 I don't understand why so many companies make it so hard.

37:44.560 --> 37:47.240
 I mean, another example is when you buy a new iPhone

37:47.240 --> 37:48.880
 or a new computer, whatever,

37:48.880 --> 37:51.440
 I feel like, okay, I'm gonna like lose an afternoon,

37:51.440 --> 37:53.840
 just like loading up and getting all my stuff back.

37:53.840 --> 37:57.360
 And for a lot of us, that's more of a deterrent

37:57.360 --> 37:58.640
 than the price.

37:58.640 --> 38:01.840
 And if they could make it painless,

38:01.840 --> 38:03.720
 we'd give them a lot more money.

38:03.720 --> 38:06.480
 So I'm hoping somebody listening is working

38:06.480 --> 38:10.040
 on making it more painless for us to buy your products.

38:10.040 --> 38:12.320
 If we could just like linger a little bit

38:12.320 --> 38:13.720
 on the social network thing,

38:13.720 --> 38:18.240
 because there's this Netflix social dilemma.

38:18.240 --> 38:19.320
 Yeah, I saw that.

38:19.320 --> 38:22.680
 And Tristan Harris and company, yeah.

38:22.680 --> 38:27.680
 And people's data, it's really sensitive

38:30.440 --> 38:32.440
 and social networks are at the core,

38:32.440 --> 38:37.440
 arguably of many of societal tension

38:37.480 --> 38:39.680
 and some of the most important things happening in society.

38:39.680 --> 38:42.080
 So it feels like it's important to get this right,

38:42.080 --> 38:44.000
 both from a business model perspective

38:44.000 --> 38:46.400
 and just like a trust perspective.

38:46.400 --> 38:49.880
 I still gotta, I mean, it just still feels like,

38:49.880 --> 38:52.160
 I know there's experimentation going on.

38:52.160 --> 38:54.720
 It still feels like everyone is afraid

38:54.720 --> 38:57.520
 to try different business models, like really try.

38:57.520 --> 38:59.600
 Well, I'm worried that people are afraid

38:59.600 --> 39:01.200
 to try different business models.

39:01.200 --> 39:03.480
 I'm also worried that some of the business models

39:03.480 --> 39:06.280
 may lead them to bad choices.

39:06.280 --> 39:10.280
 And Danny Kahneman talks about system one

39:10.280 --> 39:12.280
 and system two, sort of like our reptilian brain

39:12.280 --> 39:14.360
 that reacts quickly to what we see,

39:14.360 --> 39:16.160
 see something interesting, we click on it,

39:16.160 --> 39:20.800
 we retweet it versus our system two,

39:20.800 --> 39:24.120
 our frontal cortex that's supposed to be more careful

39:24.120 --> 39:27.000
 and rational that really doesn't make as many decisions

39:27.000 --> 39:27.840
 as it should.

39:28.880 --> 39:32.720
 I think there's a tendency for a lot of these social networks

39:32.720 --> 39:37.720
 to really exploit system one, our quick instant reaction,

39:37.760 --> 39:41.000
 make it, so we just click on stuff and pass it on

39:41.000 --> 39:42.360
 and not really think carefully about it.

39:42.360 --> 39:47.360
 And in that system, it tends to be driven by sex, violence,

39:47.360 --> 39:52.360
 disgust, anger, fear, these relatively primitive kinds

39:52.880 --> 39:56.000
 of emotions, maybe they're important for a lot of purposes,

39:56.000 --> 39:58.960
 but they're not a great way to organize a society.

39:58.960 --> 40:01.960
 And most importantly, when you think about this huge,

40:01.960 --> 40:04.360
 amazing information infrastructure we've had

40:04.360 --> 40:08.040
 that's connected billions of brains across the globe,

40:08.040 --> 40:09.680
 not just we can all access information,

40:09.680 --> 40:12.720
 but we can all contribute to it and share it.

40:12.720 --> 40:14.160
 Arguably the most important thing

40:14.160 --> 40:19.160
 that that network should do is favor truth over falsehoods.

40:19.400 --> 40:21.680
 And the way it's been designed,

40:21.680 --> 40:24.680
 not necessarily intentionally, is exactly the opposite.

40:24.680 --> 40:29.480
 My MIT colleagues, Aral and Deb Roy and others at MIT

40:29.480 --> 40:31.800
 did a terrific paper in the cover of science

40:31.800 --> 40:33.520
 and they document what we all feared,

40:33.520 --> 40:37.800
 which is that lies spread faster than truth

40:37.800 --> 40:39.800
 on social networks.

40:39.800 --> 40:42.800
 They looked at a bunch of tweets and weed tweets

40:42.800 --> 40:45.360
 and they found that false information was more likely

40:45.360 --> 40:49.040
 to spread further, faster to more people.

40:49.040 --> 40:50.000
 And why was that?

40:50.000 --> 40:53.440
 It's not because people like lies,

40:53.440 --> 40:56.640
 it's because people like things that are shocking, amazing.

40:56.640 --> 40:57.920
 Can you believe this?

40:57.920 --> 41:00.360
 Something that is not mundane,

41:00.360 --> 41:02.560
 not that it's something everybody else already knew.

41:02.560 --> 41:05.480
 And what are the most unbelievable things?

41:05.480 --> 41:09.920
 Well, lies, and so if you wanna find something unbelievable,

41:09.920 --> 41:10.760
 it's a lot easier to do that

41:10.760 --> 41:12.520
 if you're not constrained by the truth.

41:12.520 --> 41:15.680
 So they found that the emotional valence

41:15.680 --> 41:18.000
 of false information was just much higher,

41:18.000 --> 41:19.720
 it was more likely to be shocking

41:19.720 --> 41:21.600
 and therefore more likely to be spread.

41:22.920 --> 41:24.040
 Another interesting thing was that

41:24.040 --> 41:26.640
 that wasn't necessarily driven by the algorithms.

41:27.600 --> 41:29.720
 I know that there is some evidence,

41:29.720 --> 41:32.440
 Zennip Tafeki and others have pointed out in YouTube,

41:32.440 --> 41:34.720
 some of the algorithms unintentionally were tuned

41:34.720 --> 41:37.880
 to amplify more extremist content.

41:37.880 --> 41:42.480
 But in the study of Twitter that Sinan and Deb and others did,

41:42.480 --> 41:44.440
 they found that even if you took out all the bots

41:44.440 --> 41:47.840
 and all the automated tweets,

41:47.840 --> 41:50.720
 you still had lies spreading significantly faster.

41:50.720 --> 41:52.560
 It's just the problems with ourselves

41:52.560 --> 41:57.040
 that we just can't resist passing on the salacious content.

41:58.440 --> 41:59.920
 But I also blame the platforms

41:59.920 --> 42:03.120
 because there's different ways you can design a platform.

42:03.120 --> 42:05.360
 You can design a platform in a way

42:05.360 --> 42:07.240
 that makes it easy to spread lies

42:07.240 --> 42:09.480
 and to retweet and spread things on.

42:09.480 --> 42:11.480
 Or you can kind of put some friction on that

42:11.480 --> 42:13.920
 and try to favor truth.

42:13.920 --> 42:15.440
 I had dinner with Jimmy Wales once,

42:15.440 --> 42:17.880
 the guy who helped found Wikipedia.

42:19.680 --> 42:22.000
 And he convinced me that,

42:22.000 --> 42:24.440
 look, you can make some design choices,

42:24.440 --> 42:27.600
 whether it's at Facebook, at Twitter, at Wikipedia,

42:27.600 --> 42:29.200
 or Reddit, whatever.

42:29.200 --> 42:31.280
 And depending on how you make those choices,

42:32.280 --> 42:35.040
 you're more likely or less likely to have false news.

42:35.040 --> 42:37.080
 Create a little bit of friction, like you said.

42:37.080 --> 42:37.920
 Yeah.

42:37.920 --> 42:39.240
 You know, that's the...

42:39.240 --> 42:41.280
 It could be friction or it could be speeding the truth.

42:41.280 --> 42:42.240
 You know, either way.

42:42.240 --> 42:44.080
 But I don't totally understand...

42:44.080 --> 42:45.440
 Speeding the truth, I love it.

42:45.440 --> 42:46.960
 Yeah, yeah.

42:46.960 --> 42:48.800
 Amplifying it and giving it more credit.

42:48.800 --> 42:50.760
 And you know, like in academia,

42:50.760 --> 42:52.440
 which is far, far from perfect,

42:52.440 --> 42:55.560
 but you know, when someone has important discovery,

42:55.560 --> 42:56.800
 it tends to get more cited

42:56.800 --> 42:58.080
 and people kind of look to it more

42:58.080 --> 43:00.680
 and sort of it tends to get amplified a little bit.

43:00.680 --> 43:03.240
 So you could try to do that too.

43:03.240 --> 43:04.600
 I don't know what the silver bullet is,

43:04.600 --> 43:08.360
 but the meta point is that if we spend time thinking about it,

43:08.360 --> 43:10.720
 we can amplify truth over falsehoods.

43:10.720 --> 43:14.840
 And I'm disappointed in the heads of these social networks

43:14.840 --> 43:16.600
 that they haven't been as successful

43:16.600 --> 43:19.480
 or maybe haven't tried as hard to amplify truth.

43:19.480 --> 43:21.520
 And part of it, going back to what we said earlier,

43:21.520 --> 43:23.320
 is you know, these revenue models

43:23.320 --> 43:28.080
 may push them more towards growing fast,

43:28.080 --> 43:31.400
 spreading information rapidly, getting lots of users,

43:31.400 --> 43:34.520
 which isn't the same thing as finding truth.

43:35.400 --> 43:36.400
 Yeah.

43:36.400 --> 43:38.800
 I mean, implicit in what you're saying now

43:38.800 --> 43:42.200
 is a hopeful message that with platforms,

43:42.200 --> 43:47.200
 we can take a step towards greater and greater popularity

43:49.920 --> 43:53.120
 of truth, but the more cynical view

43:53.120 --> 43:56.800
 is that what the last few years have revealed

43:56.800 --> 43:59.000
 is that there's a lot of money to be made

43:59.000 --> 44:03.080
 in dismantling even the idea of truth,

44:03.080 --> 44:04.960
 that nothing is true.

44:04.960 --> 44:07.000
 And as a thought experiment,

44:07.000 --> 44:09.320
 I've been thinking about if it's possible

44:09.320 --> 44:11.200
 that our future will have,

44:11.200 --> 44:14.360
 like the idea of truth is something we won't even have.

44:14.360 --> 44:17.800
 Do you think it's possible like in the future

44:17.800 --> 44:20.960
 that everything is on the table in terms of truth

44:20.960 --> 44:24.880
 and we're just swimming in this kind of digital economy

44:24.880 --> 44:29.880
 where ideas are just little toys

44:29.880 --> 44:33.200
 that are not at all connected to reality?

44:33.200 --> 44:35.880
 Yeah, I think that's definitely possible.

44:35.880 --> 44:38.080
 I'm not a technological determinist.

44:38.080 --> 44:40.400
 So I don't think that's inevitable.

44:40.400 --> 44:42.440
 I don't think it's inevitable that it doesn't happen.

44:42.440 --> 44:44.080
 I mean, the thing that I've come away with

44:44.080 --> 44:45.400
 every time I do these studies

44:45.400 --> 44:47.320
 and I emphasize it in my books and elsewhere

44:47.320 --> 44:50.160
 is that technology doesn't shape our destiny,

44:50.160 --> 44:51.800
 we shape our destiny.

44:51.800 --> 44:54.760
 So just by us having this conversation,

44:54.760 --> 44:58.560
 I hope that your audience is gonna take it upon themselves

44:58.560 --> 44:59.960
 as they design their products

44:59.960 --> 45:01.360
 and they think about and they use products

45:01.360 --> 45:02.880
 as they manage companies.

45:02.880 --> 45:05.400
 How can they make conscious decisions

45:05.400 --> 45:08.520
 to favor truth over false?

45:08.520 --> 45:11.000
 So it's favor the better kinds of societies

45:11.000 --> 45:13.800
 and not abdicate and say, well, we just build the tools.

45:13.800 --> 45:18.400
 I think there was a saying that was it the German scientists

45:18.400 --> 45:23.120
 when they were working on the missiles in late World War II.

45:23.120 --> 45:25.760
 They said, well, our job is to make the missiles go up

45:25.760 --> 45:28.760
 where they come down, that's someone else's department.

45:28.760 --> 45:31.040
 And that's obviously not the,

45:31.040 --> 45:32.800
 I think it's obvious that's not the right attitude

45:32.800 --> 45:33.920
 that technologists should have,

45:33.920 --> 45:35.640
 that engineers should have,

45:35.640 --> 45:37.320
 they should be very conscious about

45:37.320 --> 45:38.760
 what the implications are.

45:38.760 --> 45:40.560
 And if we think carefully about it,

45:40.560 --> 45:42.880
 we can avoid the kind of world that you just described

45:42.880 --> 45:45.040
 where truth is all relative.

45:45.040 --> 45:47.800
 There are going to be people who benefit from a world

45:47.800 --> 45:51.280
 of where people don't check facts

45:51.280 --> 45:54.200
 and where truth is relative and popularity

45:54.200 --> 45:59.200
 or fame or money is orthogonal to truth.

45:59.840 --> 46:01.840
 But one of the reasons I suspect

46:01.840 --> 46:05.360
 that we've had so much progress over the past few hundred years

46:05.360 --> 46:07.560
 is the invention of the scientific method,

46:07.560 --> 46:10.160
 which is a really powerful tool or meta tool

46:10.160 --> 46:15.160
 for finding truth and favoring things that are true

46:15.360 --> 46:16.560
 versus things that are false.

46:16.560 --> 46:18.520
 If they don't pass the scientific method,

46:18.520 --> 46:20.600
 they're less likely to be true.

46:20.600 --> 46:25.520
 And that has the societies and the people

46:25.520 --> 46:27.720
 and the organizations that embrace that

46:27.720 --> 46:30.480
 have done a lot better than the ones who haven't.

46:30.480 --> 46:32.760
 And so I'm hoping that people keep that in mind

46:32.760 --> 46:35.400
 and continue to try to embrace not just the truth,

46:35.400 --> 46:37.600
 but methods that lead to the truth.

46:37.600 --> 46:40.480
 So maybe on a more personal question,

46:41.360 --> 46:45.440
 if one were to try to build a competitor to Twitter,

46:45.440 --> 46:47.320
 what would you advise?

46:48.600 --> 46:53.360
 Is there, I mean, the matter of question,

46:53.360 --> 46:55.680
 is that the right way to improve systems?

46:55.680 --> 46:59.360
 Yeah, no, I think that the underlying premise

46:59.360 --> 47:01.360
 behind Twitter and all these networks is amazing

47:01.360 --> 47:02.800
 that we can communicate with each other.

47:02.800 --> 47:04.000
 And I use it a lot.

47:04.000 --> 47:05.920
 There's a subpart of Twitter called econ Twitter,

47:05.920 --> 47:08.640
 where we economists tweet to each other

47:08.640 --> 47:10.560
 and talk about new papers.

47:10.560 --> 47:11.960
 Something came out in the NBER,

47:11.960 --> 47:13.320
 the National Bureau of Economic Research,

47:13.320 --> 47:14.160
 and we share about it.

47:14.160 --> 47:15.360
 People critique it.

47:15.360 --> 47:16.880
 I think it's been a godsend

47:16.880 --> 47:20.000
 because it's really sped up the scientific process,

47:20.000 --> 47:21.800
 if you can call it economic scientific.

47:21.800 --> 47:23.520
 Does it get divisive in that little?

47:23.520 --> 47:24.480
 Sometimes, yeah, sure.

47:24.480 --> 47:25.320
 Sometimes it does.

47:25.320 --> 47:27.000
 It can also be done in nasty ways.

47:27.000 --> 47:28.320
 And there's the bad parts.

47:28.320 --> 47:29.640
 But the good parts are great

47:29.640 --> 47:31.600
 because you just speed up that clock speed

47:31.600 --> 47:33.280
 of learning about things.

47:33.280 --> 47:35.440
 Instead of like in the old, old days,

47:35.440 --> 47:36.760
 waiting to read it in a journal,

47:36.760 --> 47:39.480
 or the not so old days when you'd see it posted

47:39.480 --> 47:41.560
 on a website and you'd read it.

47:41.560 --> 47:43.960
 Now on Twitter, people will distill it down

47:43.960 --> 47:47.160
 and there's a real art to getting to the essence of things.

47:47.160 --> 47:49.040
 So that's been great.

47:49.040 --> 47:52.320
 But it certainly, we all know that Twitter

47:52.320 --> 47:55.520
 can be a cesspool of misinformation.

47:55.520 --> 47:59.000
 And like I just said, unfortunately misinformation

47:59.000 --> 48:02.320
 tends to spread faster on Twitter than truth.

48:02.320 --> 48:04.200
 And there are a lot of people who are very vulnerable to it.

48:04.200 --> 48:06.000
 I'm sure I've been fooled at times.

48:06.000 --> 48:09.120
 There are agents, whether from Russia

48:09.120 --> 48:11.680
 or from political groups or others

48:11.680 --> 48:15.640
 that explicitly create efforts at misinformation

48:15.640 --> 48:17.920
 and efforts at getting people to hate each other.

48:17.920 --> 48:21.200
 Or even more importantly, I've discovered, is nut picking.

48:21.200 --> 48:22.320
 You know the idea of nut picking?

48:22.320 --> 48:23.160
 No, what's that?

48:23.160 --> 48:24.320
 It's a good term.

48:24.320 --> 48:27.800
 Nut picking is when you find like an extreme nut case

48:27.800 --> 48:30.680
 on the other side and then you amplify them

48:30.680 --> 48:33.960
 and make it seem like that's typical of the other side.

48:33.960 --> 48:35.440
 So you're not literally lying.

48:35.440 --> 48:37.720
 You're taking some idiot, you know,

48:37.720 --> 48:39.880
 ranting on the subway or just, you know,

48:39.880 --> 48:42.800
 whether they're in the KKK or Antifa or whatever,

48:42.800 --> 48:44.800
 they're just, and you normally,

48:44.800 --> 48:46.000
 nobody would pay attention to this guy.

48:46.000 --> 48:48.080
 Like 12 people would see him and it'd be the end.

48:48.080 --> 48:51.120
 Instead, with video or whatever,

48:51.120 --> 48:54.480
 you get tens of millions of people say it.

48:54.480 --> 48:57.120
 And I've seen this, you know, I look at him, I get angry.

48:57.120 --> 48:59.720
 I'm like, I can't believe that person did something so terrible.

48:59.720 --> 49:02.840
 Let me tell all my friends about this terrible person.

49:02.840 --> 49:06.640
 And it's a great way to generate division.

49:06.640 --> 49:09.400
 I talked to a friend who studied

49:09.400 --> 49:11.520
 Russian misinformation campaigns.

49:11.520 --> 49:13.960
 And they're very clever about literally being

49:13.960 --> 49:15.840
 on both sides of some of these debates.

49:15.840 --> 49:18.600
 They would have some people pretend to be part of BLM,

49:18.600 --> 49:21.000
 some people pretend to be white nationalists,

49:21.000 --> 49:22.920
 and they would be throwing epithets at each other,

49:22.920 --> 49:25.040
 saying crazy things at each other.

49:25.040 --> 49:26.560
 And they're literally playing both sides of it,

49:26.560 --> 49:28.560
 but their goal wasn't for one or the other to win.

49:28.560 --> 49:30.080
 It was for everybody to get behating

49:30.080 --> 49:31.960
 and distrusting everyone else.

49:31.960 --> 49:34.440
 So these tools can definitely be used for that,

49:34.440 --> 49:36.560
 and they are being used for that.

49:36.560 --> 49:39.640
 It's been super destructive for our democracy

49:39.640 --> 49:41.080
 and our society.

49:41.080 --> 49:43.520
 And the people who run these platforms,

49:43.520 --> 49:46.080
 I think have a social responsibility,

49:46.080 --> 49:48.640
 a moral and ethical personal responsibility,

49:48.640 --> 49:51.720
 to do a better job and to shut that stuff down.

49:51.720 --> 49:52.920
 Well, I don't know if you can shut it down,

49:52.920 --> 49:56.440
 but to design them in a way that, as I said earlier,

49:56.440 --> 49:59.720
 favors truth over falsehoods and favors

49:59.720 --> 50:04.720
 positive types of communication versus destructive ones.

50:06.000 --> 50:09.600
 And just like you said, it's also on us.

50:09.600 --> 50:12.800
 I try to be all about love and compassion and empathy

50:12.800 --> 50:13.640
 on Twitter.

50:13.640 --> 50:14.840
 I mean, one of the things,

50:14.840 --> 50:16.600
 not picking is a fascinating term,

50:16.600 --> 50:18.960
 one of the things that people do

50:18.960 --> 50:21.800
 that's I think even more dangerous

50:21.800 --> 50:26.800
 is not picking applied to individual statements

50:26.800 --> 50:28.440
 of good people.

50:28.440 --> 50:32.200
 So basically, worst case analysis in computer science

50:32.200 --> 50:35.360
 is taking sometimes out of context,

50:35.360 --> 50:37.040
 but sometimes in context,

50:38.480 --> 50:42.320
 a statement, one statement by a person.

50:42.320 --> 50:43.160
 Like I've been,

50:43.160 --> 50:45.360
 because I've been reading The Rise and Fall of the Third Reich,

50:45.360 --> 50:48.960
 I often talk about Hitler on this podcast with folks,

50:48.960 --> 50:50.600
 and it is so easy.

50:50.600 --> 50:52.040
 That's really dangerous.

50:52.040 --> 50:53.200
 But I'm all leaning in.

50:53.200 --> 50:56.960
 I'm 100% because, well, it's actually a safer place

50:56.960 --> 50:58.040
 than people realize,

50:58.040 --> 51:01.920
 because it's history and history in long form

51:01.920 --> 51:05.000
 is actually very fascinating to think about.

51:05.000 --> 51:09.560
 And it's, but I could see how that could be taken

51:09.560 --> 51:11.320
 totally out of context and it's very worrying.

51:11.320 --> 51:12.800
 I think about these digital infrastructures,

51:12.800 --> 51:14.040
 not just they disseminate things,

51:14.040 --> 51:14.880
 but they're sort of permanent.

51:14.880 --> 51:16.560
 Anything you say at some point,

51:16.560 --> 51:19.680
 someone can go back and find something you said three years ago,

51:19.680 --> 51:21.080
 perhaps jokingly, perhaps not.

51:21.080 --> 51:22.800
 Maybe you're just wrong and you made them,

51:22.800 --> 51:24.000
 and like that becomes,

51:24.000 --> 51:26.840
 they can use that to define you if they have an intent.

51:26.840 --> 51:29.080
 And we all need to be a little more forgiving.

51:29.080 --> 51:31.200
 I mean, somewhere in my 20s,

51:31.200 --> 51:33.840
 I told myself, I was going through all my different friends

51:33.840 --> 51:35.520
 and I was like, you know,

51:36.600 --> 51:39.440
 every one of them has at least like one nutty opinion.

51:39.440 --> 51:40.680
 Yeah, exactly.

51:40.680 --> 51:43.120
 And I was like, there's like nobody who's like completely,

51:43.120 --> 51:44.160
 except me, of course,

51:44.160 --> 51:45.720
 but I'm sure they thought that about me too.

51:45.720 --> 51:47.760
 And so you just kind of like learned

51:47.760 --> 51:51.120
 to be a little bit tolerant that like, okay, there's just,

51:51.120 --> 51:55.240
 Yeah, I wonder who the responsibility lays on there.

51:55.240 --> 51:59.680
 Like, I think ultimately it's about leadership,

51:59.680 --> 52:03.280
 like the previous president, Barack Obama has been,

52:04.240 --> 52:07.640
 I think quite eloquent at walking this very difficult line

52:07.640 --> 52:09.840
 of talking about cancel culture,

52:09.840 --> 52:11.520
 but it's a difficult, it takes skill.

52:11.520 --> 52:12.360
 Yeah.

52:12.360 --> 52:15.440
 You say the wrong thing and you piss off a lot of people.

52:15.440 --> 52:17.400
 And so you have to do it well,

52:17.400 --> 52:19.840
 but then also the platform of the technology is

52:21.160 --> 52:22.440
 should slow down,

52:22.440 --> 52:25.400
 create friction and spreading this kind of nut picking

52:25.400 --> 52:26.400
 in all its forms.

52:26.400 --> 52:27.240
 Absolutely.

52:27.240 --> 52:29.760
 And your point that we have to like learn over time

52:29.760 --> 52:30.600
 how to manage it.

52:30.600 --> 52:31.760
 I mean, we can't put it all on the platform

52:31.760 --> 52:32.960
 and say you guys design it.

52:32.960 --> 52:35.160
 And cause if we're idiots about using it, you know,

52:35.160 --> 52:37.840
 nobody can design a platform that withstands that.

52:37.840 --> 52:41.680
 And every new technology people learn it's dangerous.

52:41.680 --> 52:43.920
 You know, when someone invented fire,

52:43.920 --> 52:44.960
 it's great cooking and everything,

52:44.960 --> 52:46.160
 but then somebody burned himself.

52:46.160 --> 52:48.200
 And then you had to like learn how to like avoid,

52:48.200 --> 52:50.000
 maybe somebody invented a fire extinguisher later

52:50.000 --> 52:50.840
 and what's up.

52:50.840 --> 52:52.840
 So you kind of like figure out ways

52:52.840 --> 52:54.680
 of working around these technologies.

52:54.680 --> 52:57.440
 Someone invented seat belts, et cetera.

52:57.440 --> 52:58.640
 And that's certainly true

52:58.640 --> 53:00.640
 with all the new digital technologies

53:00.640 --> 53:02.360
 that we have to figure out,

53:02.360 --> 53:05.320
 not just technologies that protect us,

53:05.320 --> 53:08.680
 but ways of using them that emphasize

53:08.680 --> 53:11.560
 that are more likely to be successful than dangerous.

53:11.560 --> 53:13.240
 So you've written quite a bit about

53:13.240 --> 53:16.040
 how artificial intelligence might change our world.

53:19.040 --> 53:21.760
 How do you think, if we look forward again,

53:21.760 --> 53:23.240
 it's impossible to predict the future.

53:23.240 --> 53:26.480
 But if we look at trends from the past

53:26.480 --> 53:28.240
 and we try to predict what's gonna happen

53:28.240 --> 53:29.760
 in the rest of the 21st century,

53:29.760 --> 53:32.120
 how do you think AI will change our world?

53:32.120 --> 53:34.200
 That's a big question.

53:34.200 --> 53:37.480
 You know, I'm mostly a techno optimist.

53:37.480 --> 53:38.680
 I'm not at the extreme, you know,

53:38.680 --> 53:41.120
 the singularity is near end of the spectrum.

53:41.120 --> 53:44.600
 But I do think that we are likely in

53:44.600 --> 53:47.520
 for some significantly improved living standards,

53:47.520 --> 53:49.320
 some really important progress,

53:49.320 --> 53:51.280
 even just the technologies that are already kind of like

53:51.280 --> 53:53.120
 in the can that haven't diffused.

53:53.120 --> 53:54.920
 You know, when I talked earlier about the J curve,

53:54.920 --> 53:58.800
 it could take 10, 20, 30 years for an existing technology

53:58.800 --> 54:00.800
 to have the kind of profound effects.

54:00.800 --> 54:03.800
 And when I look at whether it's, you know,

54:03.800 --> 54:07.880
 vision systems, voice recognition, problem solving systems,

54:07.880 --> 54:09.440
 even if nothing new got invented,

54:09.440 --> 54:11.840
 we would have a few decades of progress.

54:11.840 --> 54:13.480
 So I'm excited about that.

54:13.480 --> 54:16.880
 And I think that's gonna lead to us being wealthier,

54:16.880 --> 54:18.840
 healthier, I mean, the healthcare is probably

54:18.840 --> 54:21.360
 one of the applications I'm most excited about.

54:22.560 --> 54:23.800
 So that's good news.

54:23.800 --> 54:27.640
 I don't think we're gonna have the end of work anytime soon.

54:27.640 --> 54:31.000
 There's just too many things that machines still can't do.

54:31.000 --> 54:32.040
 When I look around the world

54:32.040 --> 54:34.680
 and think of whether it's childcare or healthcare,

54:34.680 --> 54:37.800
 clean the environment, interacting with people,

54:37.800 --> 54:40.920
 scientific work, artistic creativity.

54:40.920 --> 54:42.600
 These are things that for now,

54:42.600 --> 54:45.680
 machines aren't able to do nearly as well as humans,

54:45.680 --> 54:47.200
 even just something as mundane as, you know,

54:47.200 --> 54:48.760
 folding laundry or whatever.

54:48.760 --> 54:52.960
 And many of these, I think are gonna be years or decades

54:52.960 --> 54:54.760
 before machines catch up.

54:54.760 --> 54:56.160
 You know, I may be surprised on some of them,

54:56.160 --> 54:59.760
 but overall, I think there's plenty of work for humans to do.

54:59.760 --> 55:01.360
 There's plenty of problems in society

55:01.360 --> 55:02.600
 that need the human touch.

55:02.600 --> 55:04.200
 So we'll have to repurpose.

55:04.200 --> 55:07.880
 We'll have to, as machines are able to do some tasks,

55:07.880 --> 55:11.080
 people are gonna have to reskill and move into other areas.

55:11.080 --> 55:12.760
 And that's probably what's gonna be going on

55:12.760 --> 55:16.280
 for the next, you know, 10, 20, 30 years or more,

55:16.280 --> 55:18.960
 kind of big restructuring of society.

55:18.960 --> 55:22.440
 We'll get wealthier and people will have to do new skills.

55:22.440 --> 55:23.920
 Now, if you turn the doubt further,

55:23.920 --> 55:26.960
 I don't know, 50 or 100 years into the future,

55:26.960 --> 55:29.640
 then, you know, maybe all bets are off.

55:29.640 --> 55:32.200
 Then it's possible that machines

55:32.200 --> 55:34.240
 will be able to do most of what people do.

55:34.240 --> 55:37.360
 You know, say one or 200 years, I think it's even likely.

55:37.360 --> 55:38.400
 And at that point,

55:38.400 --> 55:41.040
 then we're more in the sort of abundance economy.

55:41.040 --> 55:44.040
 Then we're in a world where there's really little

55:44.040 --> 55:48.000
 for the humans can do economically better than machines

55:48.000 --> 55:49.920
 other than be human.

55:49.920 --> 55:53.640
 And, you know, that will take a transition as well,

55:53.640 --> 55:56.520
 kind of more of a transition of how we get meaning in life

55:56.520 --> 55:58.240
 and what our values are.

55:58.240 --> 56:00.440
 But shame on us if we screw that up.

56:00.440 --> 56:02.760
 I mean, that should be like great, great news.

56:02.760 --> 56:03.720
 And it kind of saddens me

56:03.720 --> 56:05.560
 that some people see that as like a big problem.

56:05.560 --> 56:07.640
 You know, I think it should be wonderful

56:07.640 --> 56:10.400
 if people have all the health and material things

56:10.400 --> 56:14.200
 that they need and can focus on loving each other

56:14.200 --> 56:16.840
 and discussing philosophy and playing

56:16.840 --> 56:19.440
 and doing all the other things that don't require work.

56:19.440 --> 56:23.480
 Do you think you'll be surprised to see what the 20,

56:23.480 --> 56:25.400
 like if we were to travel in time,

56:25.400 --> 56:27.400
 100 years into the future,

56:27.400 --> 56:29.560
 do you think you'll be able to,

56:29.560 --> 56:32.320
 like if I gave you a month to like talk to people,

56:32.320 --> 56:34.160
 no, like let's say a week,

56:34.160 --> 56:37.800
 would you be able to understand what the house going on?

56:37.800 --> 56:39.200
 You mean if I was there for a week?

56:39.200 --> 56:40.880
 Yeah, if you were there for a week.

56:40.880 --> 56:42.120
 100 years in the future?

56:42.120 --> 56:43.040
 Yeah.

56:43.040 --> 56:45.200
 So like, so I'll give you one thought experiment.

56:45.200 --> 56:47.600
 It's like, isn't it possible

56:47.600 --> 56:50.400
 that we're all living in virtual reality by then?

56:50.400 --> 56:51.240
 Yeah.

56:51.240 --> 56:52.480
 No, I think that's very possible.

56:52.480 --> 56:54.680
 You know, I've played around with some of those VR headsets

56:54.680 --> 56:55.520
 and they're not great,

56:55.520 --> 56:59.000
 but I mean, the average person spends

56:59.000 --> 57:03.160
 many waking hours staring at screens right now.

57:03.160 --> 57:04.440
 You know, they're kind of low res

57:04.440 --> 57:07.720
 compared to what they could be in 30 or 50 years,

57:08.880 --> 57:13.880
 but certainly games and why not any other interactions

57:13.960 --> 57:15.360
 could be done with VR.

57:15.360 --> 57:16.360
 And that would be a pretty different world

57:16.360 --> 57:19.400
 that we'd all, you know, in some ways be as rich as we wanted.

57:19.400 --> 57:20.520
 You know, we could have castles

57:20.520 --> 57:22.600
 and it could be traveling anywhere we want.

57:23.680 --> 57:26.000
 And it could obviously be multi sensory.

57:26.000 --> 57:28.000
 So that would be possible.

57:28.000 --> 57:30.880
 You know, of course, there's people, you know,

57:30.880 --> 57:32.720
 you've had Elon Musk on and others, you know,

57:32.720 --> 57:34.120
 there are people, Nick Bostrom, you know,

57:34.120 --> 57:36.880
 makes the simulation argument that maybe we're already there.

57:36.880 --> 57:37.720
 We're already there.

57:37.720 --> 57:39.560
 So, but, but in general,

57:39.560 --> 57:42.480
 or do you not even think about it in this kind of way?

57:42.480 --> 57:45.080
 You're self critically thinking,

57:45.080 --> 57:47.880
 how good are you as an economist

57:47.880 --> 57:50.360
 at predicting what the future looks like?

57:50.360 --> 57:52.000
 Well, it starts getting, I mean,

57:52.000 --> 57:54.160
 I feel reasonably comfortable next, you know,

57:54.160 --> 57:58.720
 five, 10, 20 years in terms of that path.

57:58.720 --> 58:01.720
 When you start getting truly superhuman

58:01.720 --> 58:06.000
 artificial intelligence, kind of by definition,

58:06.000 --> 58:07.040
 be able to think of a lot of things

58:07.040 --> 58:08.280
 that I couldn't have thought of

58:08.280 --> 58:10.960
 and create a world that I couldn't even imagine.

58:10.960 --> 58:14.080
 And so I'm not sure I can,

58:14.080 --> 58:16.520
 I can predict what that world is going to be like.

58:16.520 --> 58:18.840
 One thing that AI researchers,

58:18.840 --> 58:20.360
 AI safety researchers worry about

58:20.360 --> 58:22.520
 is what's called the alignment problem.

58:22.520 --> 58:25.080
 When an AI is that powerful,

58:25.080 --> 58:27.960
 then they can do all sorts of things.

58:27.960 --> 58:30.560
 And you really hope that their values

58:30.560 --> 58:32.440
 are aligned with our values.

58:32.440 --> 58:34.480
 And it's even tricky defining what our values are.

58:34.480 --> 58:37.240
 I mean, first off, we all have different values.

58:37.240 --> 58:40.440
 And secondly, maybe if we were smarter,

58:40.440 --> 58:41.640
 we would have better values.

58:41.640 --> 58:43.240
 Like, you know, I like to think

58:43.240 --> 58:46.880
 that we have better values than he did in 1860.

58:46.880 --> 58:51.360
 And, or in, you know, the year 200 BC on a lot of dimensions,

58:51.360 --> 58:53.440
 things that we consider barbaric today.

58:53.440 --> 58:56.080
 And it may be that if I thought about it more deeply,

58:56.080 --> 58:57.400
 I would also be morally evolved.

58:57.400 --> 59:00.120
 Maybe I'd be a vegetarian or do other things

59:00.120 --> 59:03.000
 that right now, whether my future self

59:03.000 --> 59:05.240
 would consider kind of immoral.

59:05.240 --> 59:07.760
 So that's a tricky problem,

59:07.760 --> 59:11.120
 getting the AI to do what we want.

59:11.120 --> 59:12.960
 Assuming it's even a friendly AI.

59:12.960 --> 59:14.760
 I mean, I should probably mention,

59:14.760 --> 59:17.120
 there's a non trivial other branch

59:17.120 --> 59:18.720
 where we destroy ourselves, right?

59:18.720 --> 59:22.960
 I mean, there's a lot of exponentially improving technologies

59:22.960 --> 59:26.640
 that could be ferociously destructive,

59:26.640 --> 59:29.480
 whether it's in nanotechnology or biotech

59:29.480 --> 59:34.280
 and weaponized viruses, AI, and other things that...

59:34.280 --> 59:35.120
 Nuclear weapons.

59:35.120 --> 59:36.240
 Nuclear weapons, of course.

59:36.240 --> 59:37.320
 The old school technology.

59:37.320 --> 59:39.280
 Yeah, good old nuclear weapons

59:39.280 --> 59:43.520
 that could be devastating or even existential.

59:43.520 --> 59:45.200
 And new things yet to be invented.

59:45.200 --> 59:50.200
 So that's a branch that I think is pretty significant.

59:52.200 --> 59:54.240
 And there are those who think that one of the reasons

59:54.240 --> 59:57.480
 we haven't been contacted by other civilizations, right?

59:57.480 --> 1:00:00.600
 Is that once you get to a certain level

1:00:00.600 --> 1:00:03.040
 of complexity in technology,

1:00:03.040 --> 1:00:04.600
 there's just too many ways to go wrong.

1:00:04.600 --> 1:00:08.440
 There's a lot of ways to blow yourself up and people,

1:00:08.440 --> 1:00:10.560
 or I should say species end up falling into

1:00:10.560 --> 1:00:13.600
 one of those traps, the great filter.

1:00:13.600 --> 1:00:14.960
 The great filter.

1:00:14.960 --> 1:00:16.720
 I mean, there's an optimistic view of that.

1:00:16.720 --> 1:00:19.400
 If there is literally no intelligent life out there

1:00:19.400 --> 1:00:22.360
 in the universe, or at least in our galaxy,

1:00:22.360 --> 1:00:26.120
 that means that we've passed at least one of the great filters

1:00:26.120 --> 1:00:30.040
 or some of the great filters that we survived.

1:00:30.040 --> 1:00:31.680
 Yeah, no, I think, I think it's Robin Hansen

1:00:31.680 --> 1:00:32.800
 has a good way of, maybe others,

1:00:32.800 --> 1:00:33.920
 they have a good way of thinking about this,

1:00:33.920 --> 1:00:38.640
 that if there are no other intelligence creatures out there

1:00:38.640 --> 1:00:40.640
 and that we've been able to detect,

1:00:40.640 --> 1:00:43.440
 one possibility is that there's a filter ahead of us.

1:00:43.440 --> 1:00:44.760
 And when you get a little more advanced,

1:00:44.760 --> 1:00:47.600
 maybe in a hundred or a thousand or 10,000 years,

1:00:47.600 --> 1:00:50.560
 things just get destroyed for some reason.

1:00:50.560 --> 1:00:53.000
 The other one is the great filters behind us.

1:00:53.000 --> 1:00:57.680
 That'll be good is that most planets don't even evolve life

1:00:57.680 --> 1:00:58.920
 or if they don't evolve life,

1:00:58.920 --> 1:01:00.280
 they don't involve intelligent life.

1:01:00.280 --> 1:01:02.000
 Maybe we've gotten past that.

1:01:02.000 --> 1:01:05.680
 And so now maybe we're on the good side of the great filter.

1:01:05.680 --> 1:01:10.480
 So if we sort of rewind back and look at the thing

1:01:10.480 --> 1:01:12.760
 where we could say something a little bit more comfortably

1:01:12.760 --> 1:01:14.760
 at five years and 10 years out,

1:01:15.920 --> 1:01:20.240
 you've written about jobs

1:01:20.240 --> 1:01:24.760
 and the impact on sort of our economy and the jobs

1:01:24.760 --> 1:01:28.280
 in terms of artificial intelligence that it might have.

1:01:28.280 --> 1:01:30.600
 It's a fascinating question of what kind of jobs are safe,

1:01:30.600 --> 1:01:32.560
 what kind of jobs are not.

1:01:32.560 --> 1:01:34.640
 He maybe speak to your intuition

1:01:34.640 --> 1:01:37.720
 about how we should think about AI

1:01:37.720 --> 1:01:39.960
 changing the landscape of work.

1:01:39.960 --> 1:01:40.920
 Sure, absolutely.

1:01:40.920 --> 1:01:42.640
 Well, this is a really important question

1:01:42.640 --> 1:01:43.880
 because I think we're very far

1:01:43.880 --> 1:01:45.720
 from artificial general intelligence,

1:01:45.720 --> 1:01:48.120
 which is AI that can just do the full breadth

1:01:48.120 --> 1:01:49.520
 of what humans can do.

1:01:49.520 --> 1:01:53.000
 But we do have human level or super human level,

1:01:53.000 --> 1:01:56.800
 narrow intelligence, narrow artificial intelligence.

1:01:56.800 --> 1:01:59.840
 And obviously my calculator can do math a lot better

1:01:59.840 --> 1:02:00.680
 than I can.

1:02:00.680 --> 1:02:01.520
 And there's a lot of other things

1:02:01.520 --> 1:02:03.160
 that machines can do better than I can.

1:02:03.160 --> 1:02:04.440
 So which is which?

1:02:04.440 --> 1:02:06.840
 We actually set out to address that question.

1:02:06.840 --> 1:02:10.760
 With Tom Mitchell, I wrote a paper called

1:02:10.760 --> 1:02:13.440
 What Can Machine Learning Do That Was in Science?

1:02:13.440 --> 1:02:16.840
 And we went and interviewed a whole bunch of AI experts

1:02:16.840 --> 1:02:19.920
 and kind of synthesized what they thought

1:02:19.920 --> 1:02:22.240
 machine learning was good at and wasn't good at.

1:02:22.240 --> 1:02:25.560
 And we came up with what we called a rubric,

1:02:25.560 --> 1:02:28.160
 basically a set of questions you can ask about any task

1:02:28.160 --> 1:02:30.960
 that will tell you whether it's likely to score high or low

1:02:30.960 --> 1:02:33.760
 on suitability for machine learning.

1:02:33.760 --> 1:02:35.520
 And then we've applied that to a bunch of tasks

1:02:35.520 --> 1:02:36.960
 in the economy.

1:02:36.960 --> 1:02:39.120
 In fact, there's a data set of all the tasks

1:02:39.120 --> 1:02:40.160
 in the US economy, believe it or not.

1:02:40.160 --> 1:02:41.640
 It's called ONET.

1:02:41.640 --> 1:02:43.160
 The US government put it together,

1:02:43.160 --> 1:02:45.040
 part of Bureau of Labor Statistics.

1:02:45.040 --> 1:02:48.720
 They divide the economy into about 970 occupations

1:02:48.720 --> 1:02:52.200
 like bus driver, economist, primary school teacher,

1:02:52.200 --> 1:02:53.440
 radiologist.

1:02:53.440 --> 1:02:54.840
 And then for each one of them,

1:02:54.840 --> 1:02:57.640
 they describe which tasks need to be done.

1:02:57.640 --> 1:03:00.760
 Like for radiologists, there are 27 distinct tasks.

1:03:00.760 --> 1:03:02.200
 So we went through all those tasks

1:03:02.200 --> 1:03:05.000
 to see whether or not a machine could do them.

1:03:05.000 --> 1:03:06.760
 And what we found interestingly was

1:03:06.760 --> 1:03:07.680
 Brilliant study weather.

1:03:07.680 --> 1:03:08.880
 That's so awesome.

1:03:08.880 --> 1:03:10.240
 Yeah, thank you.

1:03:10.240 --> 1:03:13.800
 So what we found was that there was no occupation

1:03:13.800 --> 1:03:16.280
 in our data set where machine learning just ran the table

1:03:16.280 --> 1:03:17.560
 and did everything.

1:03:17.560 --> 1:03:19.560
 And there was almost no occupation where machine learning

1:03:19.560 --> 1:03:22.160
 didn't have like a significant ability to do things.

1:03:22.160 --> 1:03:23.760
 Like take radiology, a lot of people,

1:03:23.760 --> 1:03:26.720
 I hear it saying, you know, it's the end of radiology.

1:03:26.720 --> 1:03:29.920
 And one of the 27 tasks is read medical images.

1:03:29.920 --> 1:03:32.000
 Really important one, like it's kind of a core job.

1:03:32.000 --> 1:03:34.680
 And machines have basically gotten as good

1:03:34.680 --> 1:03:35.920
 or better than radiologists.

1:03:35.920 --> 1:03:38.400
 There was just an article in Nature last week,

1:03:38.400 --> 1:03:39.640
 but you know, they've been publishing them

1:03:39.640 --> 1:03:44.640
 for the past few years showing that machine learning

1:03:45.000 --> 1:03:46.520
 can do as well as humans

1:03:46.520 --> 1:03:49.640
 on many kinds of diagnostic imaging tasks.

1:03:49.640 --> 1:03:51.160
 But other things radiologists do, you know,

1:03:51.160 --> 1:03:54.480
 they sometimes administer conscious sedation.

1:03:54.480 --> 1:03:56.000
 They sometimes do physical exams.

1:03:56.000 --> 1:03:57.360
 They have to synthesize the results

1:03:57.360 --> 1:04:01.720
 and explain to the other doctors or to the patients.

1:04:01.720 --> 1:04:02.560
 In all those categories,

1:04:02.560 --> 1:04:05.600
 machine learning isn't really up to snuff yet.

1:04:05.600 --> 1:04:09.320
 So that job, we're gonna see a lot of restructuring.

1:04:09.320 --> 1:04:11.440
 Parts of the job, they'll hand over to machines,

1:04:11.440 --> 1:04:13.160
 others, humans will do more of.

1:04:13.160 --> 1:04:15.120
 That's been more or less the pattern in all of them.

1:04:15.120 --> 1:04:16.920
 So, you know, to oversimplify,

1:04:16.920 --> 1:04:20.440
 but we see a lot of restructuring, reorganization of work.

1:04:20.440 --> 1:04:22.320
 And it's real gonna be a great time.

1:04:22.320 --> 1:04:24.760
 It is a great time for smart entrepreneurs and managers

1:04:24.760 --> 1:04:27.320
 to do that reinvention of work.

1:04:27.320 --> 1:04:29.360
 I'm not gonna see mass unemployment

1:04:30.640 --> 1:04:33.160
 to get more specifically to your question.

1:04:33.160 --> 1:04:36.600
 The kinds of tasks that machines tend to be good at

1:04:36.600 --> 1:04:39.080
 are a lot of routine problem solving,

1:04:39.080 --> 1:04:42.560
 mapping inputs X into outputs Y.

1:04:42.560 --> 1:04:44.880
 If you have a lot of data on the X's and the Y's,

1:04:44.880 --> 1:04:45.720
 the inputs and the outputs,

1:04:45.720 --> 1:04:47.120
 you can do that kind of mapping

1:04:47.120 --> 1:04:48.560
 and find the relationships.

1:04:48.560 --> 1:04:50.680
 They tend to not be very good at,

1:04:50.680 --> 1:04:53.680
 even now, fine motor control and dexterity,

1:04:54.560 --> 1:04:58.960
 emotional intelligence and human interactions,

1:04:58.960 --> 1:05:01.720
 and thinking outside the box, creative work.

1:05:01.720 --> 1:05:03.200
 If you give it a well structured task,

1:05:03.200 --> 1:05:05.040
 machines can be very good at it,

1:05:05.040 --> 1:05:08.680
 but even asking the right questions, that's hard.

1:05:08.680 --> 1:05:10.680
 There's a quote that Andrew McAfee and I use

1:05:10.680 --> 1:05:13.000
 in our book, Second Machine Age.

1:05:13.000 --> 1:05:16.840
 Apparently Pablo Picasso was shown an early computer

1:05:16.840 --> 1:05:18.480
 and he came away kind of unimpressed.

1:05:18.480 --> 1:05:20.680
 He goes, well, I don't see all the fusses.

1:05:20.680 --> 1:05:22.720
 All that does is answer questions.

1:05:24.000 --> 1:05:24.840
 And, you know, to him,

1:05:24.840 --> 1:05:26.800
 the interesting thing was asking the questions.

1:05:26.800 --> 1:05:27.640
 Yeah.

1:05:27.640 --> 1:05:31.360
 Try to replace me GPT three, I dare you.

1:05:31.360 --> 1:05:33.200
 Although some people think I'm a robot.

1:05:33.200 --> 1:05:35.400
 You have this cool plot that shows,

1:05:37.080 --> 1:05:39.680
 I just remember where economists landed,

1:05:39.680 --> 1:05:42.880
 where I think the X axis is the income.

1:05:42.880 --> 1:05:43.720
 Yes.

1:05:43.720 --> 1:05:47.600
 And then the Y axis, I guess, aggregating the information

1:05:47.600 --> 1:05:49.440
 of how replaceable the job is,

1:05:49.440 --> 1:05:50.280
 or I think there's an index.

1:05:50.280 --> 1:05:52.480
 There's a suitability for machine learning index, exactly.

1:05:52.480 --> 1:05:55.520
 So we have all 970 occupations on that chart.

1:05:55.520 --> 1:05:56.560
 It's a cool plot.

1:05:56.560 --> 1:05:59.240
 And there's gatters in all four corners

1:05:59.240 --> 1:06:01.080
 have some occupations,

1:06:01.080 --> 1:06:02.760
 but there is a definite pattern,

1:06:02.760 --> 1:06:05.720
 which is the lower wage occupations tend to have more tasks

1:06:05.720 --> 1:06:08.000
 that are suitable for machine learning, like cashiers.

1:06:08.000 --> 1:06:10.760
 I mean, anyone who's gone to a supermarket or CVS knows

1:06:10.760 --> 1:06:12.440
 that, you know, they not only read barcodes,

1:06:12.440 --> 1:06:14.560
 but they can recognize, you know, an apple and an orange

1:06:14.560 --> 1:06:17.120
 and a lot of things that cashiers,

1:06:17.120 --> 1:06:18.600
 humans used to be needed for.

1:06:19.480 --> 1:06:21.040
 At the other end of the spectrum,

1:06:21.040 --> 1:06:23.560
 there are some jobs like airline pilot

1:06:23.560 --> 1:06:26.640
 that are among the highest paid in our economy,

1:06:26.640 --> 1:06:28.800
 but also a lot of them are suitable for machine learning.

1:06:28.800 --> 1:06:30.920
 A lot of those tasks are.

1:06:30.920 --> 1:06:32.520
 And then, yeah, you mentioned economists.

1:06:32.520 --> 1:06:33.840
 I couldn't help peaking at those.

1:06:33.840 --> 1:06:36.080
 And they're paid a fair amount,

1:06:36.080 --> 1:06:39.120
 maybe not as much as some of us think they should be.

1:06:39.120 --> 1:06:44.120
 But they have some tasks they're suitable for machine learning,

1:06:44.160 --> 1:06:45.520
 but for now, at least,

1:06:45.520 --> 1:06:46.920
 most of the tasks that economists do

1:06:46.920 --> 1:06:48.520
 didn't end up being in that category.

1:06:48.520 --> 1:06:50.600
 And I should say, I didn't like create that data.

1:06:50.600 --> 1:06:54.440
 We just took the analysis and that's what came out of it.

1:06:54.440 --> 1:06:57.280
 And over time, that scatter plot will be updated

1:06:57.280 --> 1:06:59.920
 as the technology improves.

1:06:59.920 --> 1:07:02.840
 But it was just interesting to see the pattern there.

1:07:02.840 --> 1:07:05.120
 And it is a little troubling insofar

1:07:05.120 --> 1:07:08.080
 as if you just take the technology as it is today,

1:07:08.080 --> 1:07:10.480
 it's likely to worsen income inequality

1:07:10.480 --> 1:07:12.200
 on a lot of dimensions.

1:07:12.200 --> 1:07:16.440
 So on this topic of the effect of AI

1:07:16.440 --> 1:07:21.000
 on our landscape of work,

1:07:21.000 --> 1:07:23.640
 one of the people that have been speaking about it

1:07:23.640 --> 1:07:25.760
 in the public domain, public discourse

1:07:25.760 --> 1:07:28.080
 is the presidential candidate, Andrew Yang.

1:07:28.080 --> 1:07:29.000
 Yeah.

1:07:29.000 --> 1:07:31.880
 What are your thoughts about Andrew?

1:07:31.880 --> 1:07:34.320
 What are your thoughts about UBI,

1:07:34.320 --> 1:07:36.680
 that Universal Basic Income,

1:07:36.680 --> 1:07:39.040
 that he made one of the core idea.

1:07:39.040 --> 1:07:40.760
 By the way, he has like hundreds of ideas

1:07:40.760 --> 1:07:43.960
 about like everything, it's kind of interesting.

1:07:43.960 --> 1:07:45.360
 But what are your thoughts about him

1:07:45.360 --> 1:07:46.720
 and what are your thoughts about UBI?

1:07:46.720 --> 1:07:51.720
 Let me answer the question about his broader approach first.

1:07:52.040 --> 1:07:52.880
 I mean, I just love that.

1:07:52.880 --> 1:07:56.400
 He's really thoughtful, analytical.

1:07:56.400 --> 1:07:58.200
 I agree with his values.

1:07:58.200 --> 1:07:59.360
 So that's awesome.

1:07:59.360 --> 1:08:02.160
 And he read my book and mentions it sometimes,

1:08:02.160 --> 1:08:03.760
 so it makes me even more exciting.

1:08:04.760 --> 1:08:07.040
 And the thing that he really made

1:08:07.040 --> 1:08:09.920
 the centerpiece of his campaign was UBI.

1:08:09.920 --> 1:08:13.240
 And I was originally kind of a fan of it.

1:08:13.240 --> 1:08:15.960
 And then as I studied it more, I became less of a fan,

1:08:15.960 --> 1:08:17.360
 although I'm beginning to come back a little bit.

1:08:17.360 --> 1:08:19.120
 So let me tell you a little bit of my evolution.

1:08:19.120 --> 1:08:20.160
 You know, as an economist,

1:08:20.160 --> 1:08:23.000
 we have, by looking at the problem

1:08:23.000 --> 1:08:24.320
 of people not having enough income

1:08:24.320 --> 1:08:25.880
 and the simplest thing is, well, why don't we write them

1:08:25.880 --> 1:08:28.000
 a check, problem solved.

1:08:28.000 --> 1:08:30.400
 But then I talked to my sociologist friends

1:08:30.400 --> 1:08:34.360
 and they really convinced me that just writing a check

1:08:34.360 --> 1:08:36.680
 doesn't really get at the core values.

1:08:36.680 --> 1:08:38.720
 You know, Voltaire once said that

1:08:38.720 --> 1:08:43.320
 work solves three great ills, boredom, vice and need.

1:08:43.320 --> 1:08:45.440
 And you know, you can deal with the need thing

1:08:45.440 --> 1:08:46.640
 by writing a check,

1:08:46.640 --> 1:08:49.240
 but people need a sense of meaning,

1:08:49.240 --> 1:08:50.760
 they need something to do.

1:08:50.760 --> 1:08:55.760
 And when, you know, say steelworkers or coal miners

1:08:55.760 --> 1:09:00.360
 lost their jobs and were just given checks,

1:09:00.360 --> 1:09:03.760
 alcoholism, depression, divorce,

1:09:03.760 --> 1:09:06.520
 all those social indicators, drug use all went way up.

1:09:06.520 --> 1:09:09.360
 People just weren't happy just sitting around

1:09:09.360 --> 1:09:10.400
 collecting a check.

1:09:11.400 --> 1:09:13.200
 Maybe it's part of the way they were raised.

1:09:13.200 --> 1:09:14.720
 Maybe it's something innate in people

1:09:14.720 --> 1:09:17.200
 that they need to feel wanted and needed.

1:09:17.200 --> 1:09:19.560
 So it's not as simple as just writing people a check.

1:09:19.560 --> 1:09:23.960
 You need to also give them a way to have a sense of purpose.

1:09:23.960 --> 1:09:25.360
 And that was important to me.

1:09:25.360 --> 1:09:28.560
 And the second thing is that as I mentioned earlier,

1:09:28.560 --> 1:09:30.800
 you know, we are far from the end of work.

1:09:30.800 --> 1:09:32.240
 You know, I don't buy the idea

1:09:32.240 --> 1:09:34.160
 that there's just like not enough work to be done.

1:09:34.160 --> 1:09:37.120
 I see like our cities need to be cleaned up.

1:09:37.120 --> 1:09:39.000
 And I mean, robots can't do most of that.

1:09:39.000 --> 1:09:40.760
 You know, we need to have better childcare,

1:09:40.760 --> 1:09:41.640
 we need better healthcare,

1:09:41.640 --> 1:09:44.000
 we need to take care of people who are mentally ill

1:09:44.000 --> 1:09:46.480
 or older, we need to repair our roads.

1:09:46.480 --> 1:09:49.960
 There's so much work that require at least partly,

1:09:49.960 --> 1:09:52.280
 maybe entirely a human component.

1:09:52.280 --> 1:09:54.680
 So rather than like write all these people off,

1:09:54.680 --> 1:09:56.880
 well, let's find a way to repurpose them

1:09:56.880 --> 1:09:58.280
 and keep them engaged.

1:09:59.600 --> 1:10:03.720
 Now that said, I would like to see more buying power

1:10:04.640 --> 1:10:06.360
 from people who are sort of at the bottom end

1:10:06.360 --> 1:10:07.320
 of the spectrum.

1:10:07.320 --> 1:10:12.320
 The economy has been designed and evolved in a way

1:10:12.520 --> 1:10:15.600
 that's I think very unfair to a lot of hardworking people.

1:10:15.600 --> 1:10:17.000
 I see super hardworking people

1:10:17.000 --> 1:10:19.040
 who aren't really seeing their wages grow

1:10:19.040 --> 1:10:20.720
 over the past 20, 30 years,

1:10:20.720 --> 1:10:24.080
 while some other people who have been super smart

1:10:24.080 --> 1:10:29.080
 and or super lucky have made billions

1:10:29.480 --> 1:10:30.920
 or hundreds of billions.

1:10:30.920 --> 1:10:33.800
 And I don't think they need those hundreds of billions

1:10:33.800 --> 1:10:35.760
 to have the right incentives to invent things.

1:10:35.760 --> 1:10:39.440
 I think if you talk to almost any of them, as I have,

1:10:39.440 --> 1:10:42.440
 they don't think that they need an extra $10 billion

1:10:42.440 --> 1:10:43.560
 to do what they're doing.

1:10:43.560 --> 1:10:46.520
 Most of them probably would love to do it

1:10:46.520 --> 1:10:48.960
 for only a billion or maybe for nothing.

1:10:48.960 --> 1:10:50.800
 For nothing, many of them, yeah.

1:10:50.800 --> 1:10:53.120
 I mean, you know, an interesting point to make

1:10:53.120 --> 1:10:55.320
 is like, do we think that Bill Gates

1:10:55.320 --> 1:10:58.720
 would have founded Microsoft if tax rates were 70%?

1:10:58.720 --> 1:10:59.640
 Well, we know he would have

1:10:59.640 --> 1:11:03.680
 because they were tax rates of 70% when he founded it.

1:11:03.680 --> 1:11:06.200
 So I don't think that's as big a deterrent

1:11:06.200 --> 1:11:09.100
 and we could provide more buying power to people.

1:11:09.100 --> 1:11:12.800
 My own favorite tool is the earned income tax credit,

1:11:12.800 --> 1:11:16.240
 which is basically a way of supplementing income

1:11:16.240 --> 1:11:18.160
 of people who have jobs and giving employers

1:11:18.160 --> 1:11:20.280
 an incentive to hire even more people.

1:11:20.280 --> 1:11:22.400
 The minimum wage can discourage employment,

1:11:22.400 --> 1:11:25.160
 but the earned income tax credit encourages employment

1:11:25.160 --> 1:11:27.720
 by supplementing people's wages.

1:11:27.720 --> 1:11:30.800
 You know, if the employer can only afford to pay him $10

1:11:30.800 --> 1:11:35.200
 for a task, the rest of us kick in another $5 or $10

1:11:35.200 --> 1:11:37.640
 and bring their wages up to 15 or 20 total.

1:11:37.640 --> 1:11:39.360
 And then they have more buying power

1:11:39.360 --> 1:11:42.320
 than entrepreneurs are thinking, how can we cater to them?

1:11:42.320 --> 1:11:44.080
 How can we make products for them?

1:11:44.080 --> 1:11:47.200
 And it becomes a self reinforcing system

1:11:47.200 --> 1:11:49.840
 where people are better off.

1:11:49.840 --> 1:11:53.040
 And I had a good discussion where he suggested

1:11:53.040 --> 1:11:55.960
 instead of a universal basic income,

1:11:55.960 --> 1:11:59.080
 he suggested or instead of an unconditional basic income,

1:11:59.080 --> 1:12:00.600
 how about a conditional basic income

1:12:00.600 --> 1:12:03.040
 where the condition is you learn some new skills,

1:12:03.040 --> 1:12:05.040
 we need to reskill our workforce.

1:12:05.040 --> 1:12:09.120
 So let's make it easier for people to find ways

1:12:09.120 --> 1:12:11.280
 to get those skills and get rewarded for doing them.

1:12:11.280 --> 1:12:13.080
 And that's kind of a neat idea as well.

1:12:13.080 --> 1:12:13.920
 That's really interesting.

1:12:13.920 --> 1:12:16.160
 So I mean, one of the questions,

1:12:16.160 --> 1:12:19.000
 one of the dreams of UBI is that

1:12:19.000 --> 1:12:24.000
 you provide some little safety net while you retrain

1:12:24.320 --> 1:12:26.080
 while you're learning new skill.

1:12:26.080 --> 1:12:29.120
 But I think, I guess you're speaking to the intuition

1:12:29.120 --> 1:12:31.320
 that that doesn't always,

1:12:31.320 --> 1:12:33.800
 like there needs to be some incentive to reskill,

1:12:33.800 --> 1:12:35.360
 to train, to learn new things.

1:12:35.360 --> 1:12:36.200
 I think it helps.

1:12:36.200 --> 1:12:38.000
 I mean, there are lots of self motivated people,

1:12:38.000 --> 1:12:40.640
 but they're also people that maybe need a little guidance

1:12:40.640 --> 1:12:45.000
 or help and I think it's a really hard question

1:12:45.000 --> 1:12:47.280
 for someone who is losing a job in one area

1:12:47.280 --> 1:12:50.600
 to know what is the new area I should be learning skills in

1:12:50.600 --> 1:12:52.600
 and we could provide a much better set of tools

1:12:52.600 --> 1:12:54.480
 and platforms that mapped it.

1:12:54.480 --> 1:12:56.360
 Okay, here's a set of skills you already have.

1:12:56.360 --> 1:12:58.120
 Here's something that's in demand.

1:12:58.120 --> 1:13:00.440
 Let's create a path for you to go from where you are

1:13:00.440 --> 1:13:02.240
 to where you need to be.

1:13:03.120 --> 1:13:07.080
 So I'm a total, how do I put it nicely about myself?

1:13:07.080 --> 1:13:09.640
 I'm totally clueless about the economy.

1:13:09.640 --> 1:13:12.760
 It's not totally true, but pretty good approximation.

1:13:12.760 --> 1:13:17.200
 If you were to try to fix our tax system

1:13:20.520 --> 1:13:23.280
 and or maybe from another side,

1:13:23.280 --> 1:13:26.720
 if there's fundamental problems in taxation

1:13:26.720 --> 1:13:29.760
 or some fundamental problems about our economy,

1:13:29.760 --> 1:13:31.360
 what would you try to fix?

1:13:31.360 --> 1:13:33.480
 What would you try to speak to?

1:13:33.480 --> 1:13:36.360
 You know, I definitely think our whole tax system,

1:13:36.360 --> 1:13:39.920
 our political and economic system has gotten

1:13:39.920 --> 1:13:43.520
 more and more screwed up over the past 20, 30 years.

1:13:43.520 --> 1:13:46.520
 I don't think it's that hard to make headway

1:13:46.520 --> 1:13:47.360
 and improving it.

1:13:47.360 --> 1:13:49.880
 I don't think we need to totally reinvent stuff.

1:13:49.880 --> 1:13:52.400
 A lot of it is what I've elsewhere with Andy

1:13:52.400 --> 1:13:54.680
 and others called economics 101.

1:13:54.680 --> 1:13:56.400
 You know, there's just some basic principles

1:13:56.400 --> 1:14:00.640
 that have worked really well in the 20th century

1:14:00.640 --> 1:14:01.880
 that we sort of forgot, you know,

1:14:01.880 --> 1:14:03.960
 in terms of investing in education,

1:14:03.960 --> 1:14:07.560
 investing in infrastructure, welcoming immigrants,

1:14:07.560 --> 1:14:12.560
 having a tax system that was more progressive and fair.

1:14:13.280 --> 1:14:16.560
 At one point, tax rates were on top incomes,

1:14:16.560 --> 1:14:19.080
 were significantly higher and they've come down a lot

1:14:19.080 --> 1:14:20.680
 to the point where in many cases,

1:14:20.680 --> 1:14:23.400
 they're lower now than they are for poorer people.

1:14:24.760 --> 1:14:27.960
 So, and we could do things like an earned income tax credit

1:14:27.960 --> 1:14:29.240
 to get a little more wonky.

1:14:29.240 --> 1:14:31.440
 I'd like to see more Pagoovian taxes.

1:14:31.440 --> 1:14:35.720
 What that means is you tax things that are bad

1:14:35.720 --> 1:14:37.000
 instead of things that are good.

1:14:37.000 --> 1:14:40.680
 So right now we tax labor, we tax capital,

1:14:40.680 --> 1:14:43.640
 and which is unfortunate because one of the basic principles

1:14:43.640 --> 1:14:44.800
 of economics, if you tax something,

1:14:44.800 --> 1:14:46.440
 you tend to get less of it.

1:14:46.440 --> 1:14:48.840
 So, you know, right now there's still work to be done

1:14:48.840 --> 1:14:51.240
 and still capital to be invested in,

1:14:51.240 --> 1:14:54.640
 but instead we should be taxing things like pollution

1:14:54.640 --> 1:14:56.000
 and congestion.

1:14:57.240 --> 1:15:00.040
 And if we did that, we would have less pollution.

1:15:00.040 --> 1:15:03.080
 So a carbon tax is, you know, almost every economist

1:15:03.080 --> 1:15:04.120
 would say it's a no brainer,

1:15:04.120 --> 1:15:07.560
 whether they're Republican or Democrat.

1:15:07.560 --> 1:15:09.680
 Greg Mankiw, who's head of George Bush's

1:15:09.680 --> 1:15:13.000
 Council of Economic Advisors, or Dick Schmollensy,

1:15:13.000 --> 1:15:16.080
 who is another Republican economist degree,

1:15:16.080 --> 1:15:20.800
 and of course a lot of a Democratic economist degree

1:15:20.800 --> 1:15:22.800
 as well, if we taxed carbon,

1:15:22.800 --> 1:15:26.040
 we could raise hundreds of billions of dollars.

1:15:26.040 --> 1:15:28.680
 We could take that money and redistribute it

1:15:28.680 --> 1:15:31.200
 through an earned income tax credit or other things

1:15:31.200 --> 1:15:35.280
 so that overall our tax system would become more progressive.

1:15:35.280 --> 1:15:37.000
 We could tax congestion.

1:15:37.000 --> 1:15:39.040
 One of the things that kills me as an economist

1:15:39.040 --> 1:15:41.120
 is every time I sit in a traffic jam,

1:15:41.120 --> 1:15:43.040
 I know that it's completely unnecessary.

1:15:43.040 --> 1:15:44.840
 It's this is complete waste of time.

1:15:44.840 --> 1:15:47.000
 You could just visualize the cost and productivity

1:15:47.000 --> 1:15:47.840
 that this is creating.

1:15:47.840 --> 1:15:51.280
 Exactly, because they are taking costs for me

1:15:51.280 --> 1:15:52.720
 and all the people around me.

1:15:52.720 --> 1:15:54.880
 And if they charged a congestion tax,

1:15:54.880 --> 1:15:57.120
 they would take that same amount of money

1:15:57.120 --> 1:15:59.760
 and people would, it would streamline the roads,

1:15:59.760 --> 1:16:01.680
 like when you're in Singapore, the traffic just flows

1:16:01.680 --> 1:16:02.680
 because they have a congestion tax.

1:16:02.680 --> 1:16:03.680
 They listen to economists.

1:16:03.680 --> 1:16:06.520
 They invite it be and others to go talk to them.

1:16:06.520 --> 1:16:09.280
 And then I'd still be paying,

1:16:09.280 --> 1:16:11.760
 I'd be paying a congestion tax instead of paying in my time,

1:16:11.760 --> 1:16:14.280
 but that money would now be available for healthcare,

1:16:14.280 --> 1:16:15.560
 be available for infrastructure,

1:16:15.560 --> 1:16:16.920
 or be available just to give to people

1:16:16.920 --> 1:16:18.680
 so they could buy food or whatever.

1:16:18.680 --> 1:16:23.360
 So it saddens me when you sit in a traffic jam,

1:16:23.360 --> 1:16:25.080
 it's like taxing me and then taking that money

1:16:25.080 --> 1:16:27.840
 and dumping it in the ocean, just like destroying it.

1:16:27.840 --> 1:16:29.520
 So there are a lot of things like that

1:16:29.520 --> 1:16:32.520
 that economists, and I'm not,

1:16:32.520 --> 1:16:33.920
 I'm not like doing anything radical here.

1:16:33.920 --> 1:16:36.680
 Most good economists would,

1:16:36.680 --> 1:16:39.440
 I probably agree with me point by point on these things.

1:16:39.440 --> 1:16:42.200
 And we could do those things in our whole economy,

1:16:42.200 --> 1:16:43.760
 become much more efficient,

1:16:43.760 --> 1:16:47.000
 it become fair, invest in R&D and research,

1:16:47.000 --> 1:16:50.080
 which is close to a free lunch is what we have.

1:16:50.080 --> 1:16:53.160
 My erstwhile MIT colleague, Bob Solo,

1:16:53.160 --> 1:16:57.360
 got the Nobel Prize, not yesterday, but 30 years ago,

1:16:57.360 --> 1:17:00.560
 for describing that most improvements

1:17:00.560 --> 1:17:02.840
 in living standards come from tech progress.

1:17:02.840 --> 1:17:04.560
 And Paul Romer later got a Nobel Prize

1:17:04.560 --> 1:17:08.040
 for noting that investments in R&D and human capital

1:17:08.040 --> 1:17:11.040
 can speed the rate of tech progress.

1:17:11.040 --> 1:17:14.680
 So if we do that, then we'll be healthier and wealthier.

1:17:14.680 --> 1:17:16.200
 Yeah, from an economics perspective,

1:17:16.200 --> 1:17:18.440
 I remember taking an undergrad econ,

1:17:18.440 --> 1:17:20.360
 you mentioned econ 101,

1:17:20.360 --> 1:17:24.880
 it seemed from all the plots I saw that R&Ds,

1:17:24.880 --> 1:17:29.040
 that's close to free lunches as we have.

1:17:29.040 --> 1:17:32.360
 It seemed like obvious that we should do more research.

1:17:32.360 --> 1:17:33.200
 It is.

1:17:33.200 --> 1:17:34.040
 Like what?

1:17:34.040 --> 1:17:36.640
 Like, there's no,

1:17:36.640 --> 1:17:38.040
 but we should do basic research.

1:17:38.040 --> 1:17:39.480
 I mean, so, well, let me just be clear,

1:17:39.480 --> 1:17:41.440
 it'd be great if everybody did more research.

1:17:41.440 --> 1:17:44.360
 And I would make these things be to apply development

1:17:44.360 --> 1:17:46.080
 versus basic research.

1:17:46.080 --> 1:17:48.120
 So apply development, like,

1:17:48.120 --> 1:17:52.600
 how do we get this self driving car feature

1:17:52.600 --> 1:17:53.960
 to work better in the Tesla?

1:17:53.960 --> 1:17:55.240
 That's great for private companies

1:17:55.240 --> 1:17:57.080
 because they can capture the value from that.

1:17:57.080 --> 1:17:59.680
 If they make a better self driving car system,

1:17:59.680 --> 1:18:02.240
 they can sell cars that are more valuable

1:18:02.240 --> 1:18:03.080
 and then make money.

1:18:03.080 --> 1:18:03.920
 So there's an incentive,

1:18:03.920 --> 1:18:05.720
 there's not a big problem there.

1:18:05.720 --> 1:18:08.480
 And smart companies, Amazon, Tesla and others

1:18:08.480 --> 1:18:09.440
 are investing in it.

1:18:09.440 --> 1:18:11.240
 The problem is with basic research,

1:18:11.240 --> 1:18:14.400
 like coming up with core basic ideas,

1:18:14.400 --> 1:18:16.120
 whether it's in nuclear fusion

1:18:16.120 --> 1:18:18.960
 or artificial intelligence or biotech,

1:18:18.960 --> 1:18:21.640
 there, if someone invents something,

1:18:21.640 --> 1:18:23.920
 it's very hard for them to capture the benefits from it.

1:18:23.920 --> 1:18:26.760
 It's shared by everybody, which is great in a way,

1:18:26.760 --> 1:18:28.680
 but it means that they're not gonna have the incentives

1:18:28.680 --> 1:18:30.720
 to put as much effort into it.

1:18:30.720 --> 1:18:33.000
 There you need, it's a classic public good,

1:18:33.000 --> 1:18:35.120
 there you need the government to be involved in it.

1:18:35.120 --> 1:18:39.400
 And the US government used to be investing much more in R&D,

1:18:39.400 --> 1:18:43.000
 but we have slashed that part of the government

1:18:43.000 --> 1:18:46.960
 really foolishly and we're all poorer,

1:18:46.960 --> 1:18:48.480
 significantly poorer as a result.

1:18:48.480 --> 1:18:50.040
 Growth rates are down,

1:18:50.040 --> 1:18:51.720
 we're not having the kind of scientific progress

1:18:51.720 --> 1:18:53.280
 we used to have.

1:18:53.280 --> 1:18:57.840
 It's been sort of a short term, eating the seed corn,

1:18:57.840 --> 1:19:00.280
 whatever metaphor you wanna use,

1:19:00.280 --> 1:19:01.840
 where people grab some money,

1:19:01.840 --> 1:19:03.360
 put it in their pockets today,

1:19:03.360 --> 1:19:07.160
 but five, 10, 20 years later,

1:19:07.160 --> 1:19:10.200
 they're a lot poorer than they otherwise would have been.

1:19:10.200 --> 1:19:12.360
 So we're living through a pandemic right now,

1:19:12.360 --> 1:19:14.800
 globally in the United States.

1:19:16.600 --> 1:19:18.880
 From an economics perspective,

1:19:18.880 --> 1:19:23.120
 how do you think this pandemic will change the world?

1:19:23.120 --> 1:19:24.720
 It's been remarkable.

1:19:24.720 --> 1:19:27.840
 And it's horrible how many people have suffered,

1:19:27.840 --> 1:19:31.320
 the amount of death, the economic destruction.

1:19:31.320 --> 1:19:34.360
 It's also striking just the amount of change in work

1:19:34.360 --> 1:19:35.920
 that I've seen.

1:19:35.920 --> 1:19:37.400
 In the last 20 weeks,

1:19:37.400 --> 1:19:41.280
 I've seen more change than there were in the previous 20 years.

1:19:41.280 --> 1:19:42.440
 There's been nothing like it

1:19:42.440 --> 1:19:44.760
 since probably the World War II mobilization

1:19:44.760 --> 1:19:47.200
 in terms of reorganizing our economy.

1:19:47.200 --> 1:19:50.240
 The most obvious one is the shift to remote work.

1:19:50.240 --> 1:19:54.280
 And I and many other people stopped going into the office

1:19:54.280 --> 1:19:56.200
 and teaching my students in person.

1:19:56.200 --> 1:19:57.800
 I did a study on this with a bunch of colleagues

1:19:57.800 --> 1:19:59.200
 at MIT and elsewhere.

1:19:59.200 --> 1:20:02.480
 And what we found was that before the pandemic,

1:20:02.480 --> 1:20:04.120
 in the beginning of 2020,

1:20:04.120 --> 1:20:07.240
 about one in six, a little over 15% of Americans

1:20:07.240 --> 1:20:08.680
 were working remotely.

1:20:09.960 --> 1:20:11.120
 When the pandemic hit,

1:20:11.120 --> 1:20:13.600
 that grew steadily and hit 50%,

1:20:13.600 --> 1:20:16.080
 roughly half of Americans working at home.

1:20:16.080 --> 1:20:17.760
 So a complete transformation.

1:20:17.760 --> 1:20:19.120
 And of course, it wasn't even,

1:20:19.120 --> 1:20:20.520
 it wasn't like everybody did it.

1:20:20.520 --> 1:20:22.760
 If you're an information worker, professional,

1:20:22.760 --> 1:20:24.400
 if you work mainly with data,

1:20:24.400 --> 1:20:26.880
 then you're much more likely to work at home.

1:20:26.880 --> 1:20:28.800
 If you're a manufacturing worker,

1:20:28.800 --> 1:20:32.320
 working with other people or physical things,

1:20:32.320 --> 1:20:34.520
 then it wasn't so easy to work at home.

1:20:34.520 --> 1:20:36.480
 And instead, those people were much more likely

1:20:36.480 --> 1:20:39.280
 to become laid off or unemployed.

1:20:39.280 --> 1:20:41.840
 So it's been something that has had very disparate effects

1:20:41.840 --> 1:20:44.080
 on different parts of the workforce.

1:20:44.080 --> 1:20:46.200
 Do you think it's gonna be sticky

1:20:46.200 --> 1:20:49.320
 in a sense that after vaccine comes out

1:20:49.320 --> 1:20:51.080
 and the economy reopens,

1:20:51.080 --> 1:20:54.320
 do you think remote work will continue?

1:20:55.200 --> 1:20:57.080
 That's a great question.

1:20:57.080 --> 1:20:59.360
 My hypothesis is yes, a lot of it will.

1:20:59.360 --> 1:21:00.800
 Of course, some of it will go back,

1:21:00.800 --> 1:21:03.520
 but a surprising amount of it will stay.

1:21:03.520 --> 1:21:04.840
 I personally, for instance,

1:21:04.840 --> 1:21:08.840
 I moved my seminars, my academic seminars to Zoom,

1:21:08.840 --> 1:21:10.840
 and I was surprised how well it worked.

1:21:10.840 --> 1:21:11.680
 So it works.

1:21:11.680 --> 1:21:12.960
 Yeah, I mean, obviously,

1:21:12.960 --> 1:21:14.800
 we were able to reach a much broader audience.

1:21:14.800 --> 1:21:16.600
 So we have people tuning in from Europe

1:21:16.600 --> 1:21:18.560
 and other countries,

1:21:18.560 --> 1:21:20.320
 just all over the United States for that matter.

1:21:20.320 --> 1:21:21.800
 I also actually found that it would,

1:21:21.800 --> 1:21:23.520
 in many ways, is more egalitarian.

1:21:23.520 --> 1:21:25.920
 We use the chat feature and other tools,

1:21:25.920 --> 1:21:27.200
 and grad students and others

1:21:27.200 --> 1:21:29.400
 who might've been a little shy about speaking up,

1:21:29.400 --> 1:21:32.680
 we now kind of have more of ability for lots of voices,

1:21:32.680 --> 1:21:34.400
 and they're answering each other's questions

1:21:34.400 --> 1:21:35.960
 so you kind of get parallel.

1:21:35.960 --> 1:21:39.080
 Like if someone had some question about some of the data

1:21:39.080 --> 1:21:40.640
 or a reference or whatever,

1:21:40.640 --> 1:21:42.600
 then someone else in the chat would answer it.

1:21:42.600 --> 1:21:43.680
 And the whole thing just became

1:21:43.680 --> 1:21:46.680
 like a higher bandwidth, higher quality thing.

1:21:46.680 --> 1:21:48.480
 So I thought that was kind of interesting.

1:21:48.480 --> 1:21:50.360
 I think a lot of people are discovering

1:21:50.360 --> 1:21:53.960
 that these tools that, thanks to technologies

1:21:53.960 --> 1:21:56.520
 have been developed over the past decade,

1:21:56.520 --> 1:21:58.000
 they're a lot more powerful than we thought.

1:21:58.000 --> 1:22:00.200
 I mean, all the terrible things we've seen with COVID

1:22:00.200 --> 1:22:03.460
 and the real failure of many of our institutions

1:22:03.460 --> 1:22:05.040
 that I thought would work better.

1:22:05.040 --> 1:22:09.480
 One area that's been a bright spot is our technologies.

1:22:09.480 --> 1:22:11.920
 Bandwidth has held up pretty well,

1:22:11.920 --> 1:22:14.240
 and all of our email and other tools

1:22:14.240 --> 1:22:18.040
 have just scaled up kind of gracefully.

1:22:18.040 --> 1:22:20.320
 So that's been a plus.

1:22:20.320 --> 1:22:21.720
 Economists call this question

1:22:21.720 --> 1:22:24.000
 of whether it'll go back a hysteresis.

1:22:24.000 --> 1:22:25.920
 The question is like when you boil an egg,

1:22:25.920 --> 1:22:29.080
 after it gets cold again, it stays hard.

1:22:29.080 --> 1:22:30.880
 And I think that we're gonna have a fair amount

1:22:30.880 --> 1:22:32.200
 of hysteresis in the economy.

1:22:32.200 --> 1:22:33.480
 We're gonna move to this new,

1:22:33.480 --> 1:22:35.640
 we have moved to a new remote work system,

1:22:35.640 --> 1:22:37.280
 and it's not gonna snap all the way back

1:22:37.280 --> 1:22:38.760
 to where it was before.

1:22:38.760 --> 1:22:43.760
 One of the things that worries me is that the people

1:22:44.200 --> 1:22:49.200
 with lots of followers on Twitter and people with voices,

1:22:51.400 --> 1:22:55.240
 people that can, voices that can be magnified by,

1:22:55.240 --> 1:22:57.400
 you know, reporters and all that kind of stuff

1:22:57.400 --> 1:22:59.280
 are the people that fall into this category

1:22:59.280 --> 1:23:01.640
 that we were referring to just now

1:23:01.640 --> 1:23:04.480
 where they can still function and be successful

1:23:04.480 --> 1:23:06.280
 with remote work.

1:23:06.280 --> 1:23:11.240
 And then there is a kind of quiet suffering

1:23:11.240 --> 1:23:14.800
 of what feels like millions of people

1:23:14.800 --> 1:23:19.800
 whose jobs are disturbed profoundly by this pandemic,

1:23:21.200 --> 1:23:23.400
 but they don't have many followers on Twitter.

1:23:26.320 --> 1:23:31.320
 What do we, and again, I apologize,

1:23:31.840 --> 1:23:35.840
 but I've been reading the rise and fall of the third Reich

1:23:35.840 --> 1:23:38.120
 and there's a connection to the depression

1:23:38.120 --> 1:23:39.600
 on the American side.

1:23:39.600 --> 1:23:42.360
 There's a deep, complicated connection

1:23:42.360 --> 1:23:46.440
 to how suffering can turn into forces

1:23:46.440 --> 1:23:51.440
 that potentially change the world in destructive ways.

1:23:52.000 --> 1:23:53.880
 So like it's something I worry about is like,

1:23:53.880 --> 1:23:56.640
 what is this suffering going to materialize itself

1:23:56.640 --> 1:23:58.120
 in five, 10 years?

1:23:58.120 --> 1:24:01.040
 Is that something you worry about, think about?

1:24:01.040 --> 1:24:03.360
 It's like the center of what I worry about.

1:24:03.360 --> 1:24:05.440
 And let me break it down to two parts.

1:24:05.440 --> 1:24:07.320
 There's a moral and ethical aspect to it.

1:24:07.320 --> 1:24:09.360
 We need to relieve this suffering.

1:24:09.360 --> 1:24:13.320
 I mean, I share the values of I think most Americans,

1:24:13.320 --> 1:24:15.040
 we like to see shared prosperity

1:24:15.040 --> 1:24:16.640
 or most people on the planet.

1:24:16.640 --> 1:24:20.240
 And we would like to see people not falling behind

1:24:20.240 --> 1:24:23.120
 and they have fallen behind, not just due to COVID,

1:24:23.120 --> 1:24:25.800
 but in the previous couple of decades,

1:24:25.800 --> 1:24:28.000
 median income has barely moved,

1:24:28.000 --> 1:24:29.920
 depending on how you measure it.

1:24:29.920 --> 1:24:33.400
 And the incomes of the top 1% have skyrocketed.

1:24:33.400 --> 1:24:36.480
 And part of that is due to the ways technology has been used.

1:24:36.480 --> 1:24:37.920
 Part of this has been due to, frankly,

1:24:37.920 --> 1:24:42.520
 our political system has continually shifted more wealth

1:24:42.520 --> 1:24:45.160
 into those people who have the powerful interest.

1:24:45.160 --> 1:24:48.760
 So there's just, I think, a moral imperative

1:24:48.760 --> 1:24:49.840
 to do a better job.

1:24:49.840 --> 1:24:51.920
 And ultimately, we're all going to be wealthier

1:24:51.920 --> 1:24:53.360
 if more people can contribute,

1:24:53.360 --> 1:24:55.080
 more people have the wherewithal.

1:24:55.080 --> 1:24:58.680
 But the second thing is that there's a real political risk.

1:24:58.680 --> 1:24:59.960
 I'm not a political scientist,

1:24:59.960 --> 1:25:03.320
 but you don't have to be one, I think, to see

1:25:03.320 --> 1:25:05.680
 how a lot of people are really upset

1:25:05.680 --> 1:25:07.400
 with their getting a raw deal.

1:25:07.400 --> 1:25:12.400
 And they want to smash the system in different ways

1:25:13.720 --> 1:25:16.000
 in 2016 and 2018.

1:25:16.000 --> 1:25:18.280
 And now, I think there are a lot of people

1:25:18.280 --> 1:25:19.600
 who are looking at the political system

1:25:19.600 --> 1:25:21.120
 and they feel like it's not working for them

1:25:21.120 --> 1:25:23.760
 and they just want to do something radical.

1:25:24.720 --> 1:25:28.120
 Unfortunately, demagogues have harnessed that

1:25:28.120 --> 1:25:32.040
 in a way that is pretty destructive to the country.

1:25:33.120 --> 1:25:36.320
 And an analogy I see is what happened with trade.

1:25:37.240 --> 1:25:39.440
 Almost every economist thinks that free trade

1:25:39.440 --> 1:25:42.440
 is a good thing, that when two people voluntarily exchange

1:25:42.440 --> 1:25:44.920
 almost by definition, they're both better off

1:25:44.920 --> 1:25:45.920
 if it's voluntary.

1:25:47.320 --> 1:25:49.800
 And so generally, trade is a good thing,

1:25:49.800 --> 1:25:52.480
 but they also recognize that trade can lead

1:25:52.480 --> 1:25:56.240
 to uneven effects, that there can be winners and losers

1:25:56.240 --> 1:25:59.280
 in some of the people who didn't have the skills

1:25:59.280 --> 1:26:02.880
 to compete with somebody else or didn't have other assets.

1:26:02.880 --> 1:26:05.240
 And so trade can shift prices in ways

1:26:05.240 --> 1:26:07.480
 that are averse to some people.

1:26:08.480 --> 1:26:11.360
 So there's a formula that economists have,

1:26:11.360 --> 1:26:13.440
 which is that you have free trade,

1:26:13.440 --> 1:26:15.920
 but then you compensate the people who are hurt.

1:26:15.920 --> 1:26:18.400
 And free trade makes the pie bigger.

1:26:18.400 --> 1:26:20.200
 And since the pie is bigger, it's possible

1:26:20.200 --> 1:26:21.920
 for everyone to be better off.

1:26:21.920 --> 1:26:23.200
 You can make the winners better off,

1:26:23.200 --> 1:26:25.440
 but you can also compensate those who don't win.

1:26:25.440 --> 1:26:28.480
 And so they end up being better off as well.

1:26:28.480 --> 1:26:33.160
 What happened was that we didn't fulfill that promise.

1:26:33.160 --> 1:26:36.040
 We did have some more increased free trade

1:26:36.040 --> 1:26:39.120
 in the 80s and 90s, but we didn't compensate

1:26:39.120 --> 1:26:40.640
 the people who were hurt.

1:26:40.640 --> 1:26:43.760
 And so they felt like the people in power

1:26:43.760 --> 1:26:45.920
 were negged on the bargain, and I think they did.

1:26:45.920 --> 1:26:48.760
 And so then there's a backlash against trade.

1:26:48.760 --> 1:26:50.840
 And now both political parties,

1:26:50.840 --> 1:26:53.640
 but especially Trump and company,

1:26:53.640 --> 1:26:58.200
 have really pushed back against free trade.

1:26:58.200 --> 1:27:00.680
 Ultimately, that's bad for the country.

1:27:00.680 --> 1:27:02.760
 Ultimately, that's bad for living standards,

1:27:02.760 --> 1:27:04.400
 but in a way I can understand

1:27:04.400 --> 1:27:07.120
 that people felt they were betrayed.

1:27:07.120 --> 1:27:10.680
 Technology has a lot of similar characteristics.

1:27:10.680 --> 1:27:14.920
 Technology can make us all better off.

1:27:14.920 --> 1:27:17.680
 It makes the pie bigger, it creates wealth and health,

1:27:17.680 --> 1:27:18.920
 but it can also be uneven.

1:27:18.920 --> 1:27:21.280
 Not everyone automatically benefits.

1:27:21.280 --> 1:27:22.760
 It's possible for some people,

1:27:22.760 --> 1:27:25.080
 even a majority of people to get left behind,

1:27:25.080 --> 1:27:27.200
 while a small group benefits.

1:27:28.200 --> 1:27:29.560
 What most economists would say,

1:27:29.560 --> 1:27:30.880
 well, let's make the pie bigger,

1:27:30.880 --> 1:27:33.000
 but let's make sure we adjust the system

1:27:33.000 --> 1:27:35.240
 so we compensate the people who are hurt.

1:27:35.240 --> 1:27:38.000
 And since the pie is bigger, we can make the rich richer,

1:27:38.000 --> 1:27:39.200
 we can make the middle class richer,

1:27:39.200 --> 1:27:41.000
 we can make the poor richer.

1:27:41.000 --> 1:27:43.640
 Mathematically, everyone could be better off.

1:27:43.640 --> 1:27:45.400
 But again, we're not doing that.

1:27:45.400 --> 1:27:48.960
 And again, people are saying, this isn't working for us.

1:27:48.960 --> 1:27:52.560
 And again, instead of fixing the distribution,

1:27:52.560 --> 1:27:54.280
 a lot of people are beginning to say,

1:27:54.280 --> 1:27:57.280
 hey, technology sucks, we've got to stop it.

1:27:57.280 --> 1:27:59.040
 Let's throw rocks at the Google bus.

1:27:59.040 --> 1:28:00.000
 Let's blow it up.

1:28:00.000 --> 1:28:01.240
 Let's blow it up.

1:28:01.240 --> 1:28:04.760
 And there were the Luddites almost exactly 200 years ago

1:28:04.760 --> 1:28:08.040
 who smashed the looms and the spinning machines

1:28:08.040 --> 1:28:11.320
 because they felt like those machines weren't helping them.

1:28:11.320 --> 1:28:12.720
 We have a real imperative,

1:28:12.720 --> 1:28:14.720
 not just to do the morally right thing,

1:28:14.720 --> 1:28:17.520
 but to do the thing that is gonna save the country,

1:28:17.520 --> 1:28:19.440
 which is make sure that we create,

1:28:19.440 --> 1:28:22.900
 not just prosperity, but shared prosperity.

1:28:22.900 --> 1:28:27.600
 So you've been at MIT for over 30 years, I think.

1:28:27.600 --> 1:28:28.520
 Don't tell everyone how old I am.

1:28:28.520 --> 1:28:30.280
 Yeah, no, that's true, that's true.

1:28:30.280 --> 1:28:34.000
 And you're now moved to Stanford.

1:28:34.000 --> 1:28:35.760
 I'm gonna try not to say anything

1:28:37.240 --> 1:28:38.880
 about how great MIT is.

1:28:39.760 --> 1:28:41.520
 What's that move been like?

1:28:41.520 --> 1:28:44.960
 What, it's East Coast, the West Coast?

1:28:44.960 --> 1:28:46.160
 Well, MIT is great.

1:28:46.160 --> 1:28:48.080
 MIT has been very good to me.

1:28:48.080 --> 1:28:49.560
 It continues to be very good to me.

1:28:49.560 --> 1:28:51.000
 It's an amazing place.

1:28:51.000 --> 1:28:53.200
 There's, I continue to have so many amazing friends

1:28:53.200 --> 1:28:54.600
 and colleagues there.

1:28:54.600 --> 1:28:56.120
 I'm very fortunate to have been able

1:28:56.120 --> 1:28:58.480
 to spend a lot of time at MIT.

1:28:58.480 --> 1:29:00.200
 Stanford's also amazing.

1:29:00.200 --> 1:29:02.000
 And part of what attracted me out here

1:29:02.000 --> 1:29:04.960
 was not just the weather, but also Silicon Valley,

1:29:04.960 --> 1:29:07.360
 let's face it, is really more of the epicenter

1:29:07.360 --> 1:29:09.000
 of the technological revolution.

1:29:09.000 --> 1:29:10.400
 And I wanna be close to the people

1:29:10.400 --> 1:29:12.360
 who are inventing AI and elsewhere.

1:29:12.360 --> 1:29:14.920
 A lot of it is being invested at MIT, for that matter,

1:29:14.920 --> 1:29:18.840
 in Europe and China and elsewhere in NIA.

1:29:20.280 --> 1:29:23.800
 But being a little closer to some of the key technologists

1:29:23.800 --> 1:29:25.920
 was something that was important to me.

1:29:25.920 --> 1:29:30.240
 And it may be shallow, but I also do enjoy the good weather.

1:29:30.240 --> 1:29:33.120
 And I felt a little ripped off

1:29:33.120 --> 1:29:35.040
 when I came here a couple of months ago.

1:29:35.040 --> 1:29:36.640
 And immediately there are the fires

1:29:36.640 --> 1:29:39.840
 and my eyes were burning, the sky was orange,

1:29:39.840 --> 1:29:41.320
 and there's the heat waves.

1:29:41.320 --> 1:29:44.440
 And so it wasn't exactly what I'd been promised,

1:29:44.440 --> 1:29:47.960
 but fingers crossed it'll get back to better.

1:29:47.960 --> 1:29:50.720
 But maybe on a brief aside,

1:29:50.720 --> 1:29:52.720
 there's been some criticism of academia

1:29:52.720 --> 1:29:55.760
 and universities and different avenues.

1:29:55.760 --> 1:30:00.760
 And I, as a person who's gotten to enjoy universities

1:30:00.760 --> 1:30:04.360
 from the pure playground of ideas that it can be,

1:30:06.360 --> 1:30:08.840
 always kind of try to find the words

1:30:08.840 --> 1:30:13.160
 to tell people that these are magical places.

1:30:13.160 --> 1:30:17.000
 Is there something that you can speak to

1:30:17.000 --> 1:30:22.000
 that is beautiful or powerful about universities?

1:30:22.440 --> 1:30:23.280
 Well, sure.

1:30:23.280 --> 1:30:24.480
 I mean, first off, I mean,

1:30:24.480 --> 1:30:26.680
 economists have this concept called revealed preference.

1:30:26.680 --> 1:30:28.320
 You can ask people what they say,

1:30:28.320 --> 1:30:29.960
 or you can watch what they do.

1:30:29.960 --> 1:30:32.200
 And so obviously by reveal preferences,

1:30:32.200 --> 1:30:34.000
 I love academia, I'm happy here.

1:30:34.000 --> 1:30:35.560
 I could be doing lots of other things,

1:30:35.560 --> 1:30:37.600
 but it's something I enjoy a lot.

1:30:37.600 --> 1:30:39.680
 And I think the word magical is exactly right.

1:30:39.680 --> 1:30:41.480
 At least it is for me.

1:30:41.480 --> 1:30:43.120
 I do what I love, you know,

1:30:43.120 --> 1:30:44.320
 hopefully my Dean won't be listening,

1:30:44.320 --> 1:30:45.440
 but I would do this for free.

1:30:45.440 --> 1:30:49.080
 You know, it's just what I like to do.

1:30:49.080 --> 1:30:50.160
 I like to do research.

1:30:50.160 --> 1:30:51.840
 I love to have conversations like this with you

1:30:51.840 --> 1:30:53.720
 and with my students, with my fellow colleagues.

1:30:53.720 --> 1:30:55.760
 I love being around the smartest people I can find

1:30:55.760 --> 1:30:57.200
 and learning something from them

1:30:57.200 --> 1:30:58.640
 and having them challenge me.

1:30:58.640 --> 1:31:02.480
 And that just gives me joy.

1:31:02.480 --> 1:31:05.520
 And every day I find something new and exciting to work on.

1:31:05.520 --> 1:31:08.040
 And a university environment is really filled

1:31:08.040 --> 1:31:09.840
 with other people who feel that way.

1:31:09.840 --> 1:31:13.000
 And so I feel very fortunate to be part of it.

1:31:13.000 --> 1:31:14.880
 And I'm lucky that I'm in a society

1:31:14.880 --> 1:31:16.240
 where I can actually get paid for it

1:31:16.240 --> 1:31:17.280
 and put food on the table

1:31:17.280 --> 1:31:19.320
 while doing the stuff that I really love.

1:31:19.320 --> 1:31:21.640
 And I hope someday everybody can have jobs

1:31:21.640 --> 1:31:22.840
 that are like that.

1:31:22.840 --> 1:31:25.400
 And I appreciate that it's not necessarily easy

1:31:25.400 --> 1:31:27.440
 for everybody to have a job that they both love

1:31:27.440 --> 1:31:29.440
 and also they get paid for.

1:31:30.720 --> 1:31:34.080
 So there are things that don't go well in academia,

1:31:34.080 --> 1:31:35.720
 but by and large, I think it's a kind of,

1:31:35.720 --> 1:31:38.160
 you know, kinder, gentler version of a lot of the world.

1:31:38.160 --> 1:31:41.240
 You know, we sort of cut each other a little slack

1:31:41.240 --> 1:31:45.760
 on things like, you know, on just a lot of things.

1:31:45.760 --> 1:31:48.280
 You know, of course there's harsh debates

1:31:48.280 --> 1:31:49.840
 and discussions about things

1:31:49.840 --> 1:31:52.400
 and some petty politics here and there.

1:31:52.400 --> 1:31:53.480
 Personally, I try to stay away

1:31:53.480 --> 1:31:55.560
 from most of that sort of politics.

1:31:55.560 --> 1:31:56.520
 It's not my thing.

1:31:56.520 --> 1:31:58.280
 And so it doesn't affect me most of the time,

1:31:58.280 --> 1:32:00.440
 sometimes a little bit maybe.

1:32:00.440 --> 1:32:02.960
 But, you know, being able to pull together

1:32:02.960 --> 1:32:04.840
 something we have the digital economy lab,

1:32:04.840 --> 1:32:07.440
 we get all these brilliant grad students

1:32:07.440 --> 1:32:09.280
 and undergraduates and postdocs

1:32:09.280 --> 1:32:12.280
 that are just doing stuff that I learn from.

1:32:12.280 --> 1:32:14.720
 And every one of them has some aspect

1:32:14.720 --> 1:32:16.640
 of what they're doing that's just,

1:32:16.640 --> 1:32:17.560
 I couldn't even understand.

1:32:17.560 --> 1:32:19.320
 It's like way, way more brilliant.

1:32:19.320 --> 1:32:23.000
 And that's really, to me, actually, I really enjoy that.

1:32:23.000 --> 1:32:25.080
 Being in a room with lots of other smart people.

1:32:25.080 --> 1:32:29.400
 And Stanford has made it very easy to attract,

1:32:29.400 --> 1:32:31.240
 you know, those people.

1:32:31.240 --> 1:32:33.640
 I just, you know, say I'm gonna do a seminar or whatever

1:32:33.640 --> 1:32:36.800
 and the people come, they come and wanna work with me.

1:32:36.800 --> 1:32:38.840
 We get funding, we get data sets

1:32:38.840 --> 1:32:41.400
 and it's come together real nicely.

1:32:41.400 --> 1:32:44.200
 And the rest is just fun.

1:32:44.200 --> 1:32:45.880
 It's fun, yeah.

1:32:45.880 --> 1:32:47.480
 And we feel like we're working on important problems,

1:32:47.480 --> 1:32:50.280
 you know, and we're doing things that, you know,

1:32:50.280 --> 1:32:52.800
 I think our first order in terms of

1:32:52.800 --> 1:32:54.080
 what's important in the world

1:32:54.080 --> 1:32:56.280
 and that's very satisfying to me.

1:32:56.280 --> 1:32:58.040
 Maybe a bit of a fun question.

1:32:58.040 --> 1:33:02.120
 What three books, technical, fiction, philosophical,

1:33:02.120 --> 1:33:07.120
 you've enjoyed, had a big impact in your life?

1:33:07.400 --> 1:33:10.000
 Well, I guess I go back to like my teen years

1:33:10.000 --> 1:33:12.360
 and, you know, I read Sid Artha,

1:33:12.360 --> 1:33:13.480
 which is a philosophical book

1:33:13.480 --> 1:33:15.320
 and kind of helps keep me centered.

1:33:15.320 --> 1:33:17.400
 Yeah, my Herman Hess, exactly.

1:33:17.400 --> 1:33:20.400
 Don't get too wrapped up in material things

1:33:20.400 --> 1:33:22.000
 or other things and just sort of, you know,

1:33:22.000 --> 1:33:24.840
 try to find peace on things.

1:33:24.840 --> 1:33:26.360
 A book that actually influenced me a lot

1:33:26.360 --> 1:33:27.680
 in terms of my career was called

1:33:27.680 --> 1:33:30.480
 The Worldly Philosophers by Robert Howe Brenner.

1:33:30.480 --> 1:33:31.720
 It's actually about economists.

1:33:31.720 --> 1:33:33.600
 It goes through a series of different companies,

1:33:33.600 --> 1:33:34.960
 written in a very lively form.

1:33:34.960 --> 1:33:36.240
 And it probably sounds boring,

1:33:36.240 --> 1:33:38.880
 but it did describe whether it's Adam Smith,

1:33:38.880 --> 1:33:40.800
 or Karl Marx, or John Maynard Keynes

1:33:40.800 --> 1:33:43.360
 and each of them sort of what their key insights were,

1:33:43.360 --> 1:33:45.360
 but also kind of their personalities.

1:33:45.360 --> 1:33:46.560
 And I think that's one of the reasons

1:33:46.560 --> 1:33:50.640
 I became an economist was just understanding

1:33:50.640 --> 1:33:53.160
 how they grappled with the big questions of the world.

1:33:53.160 --> 1:33:54.000
 So would you recommend it

1:33:54.000 --> 1:33:57.560
 as a good whirlwind overview of the history of economics?

1:33:57.560 --> 1:33:59.080
 Yeah, yeah, I think that's exactly right.

1:33:59.080 --> 1:34:00.680
 It kind of takes you through the different things

1:34:00.680 --> 1:34:04.040
 and, you know, so you can understand how they reach,

1:34:04.040 --> 1:34:06.400
 thinking some of the strengths and weaknesses.

1:34:06.400 --> 1:34:07.920
 I mean, probably there's a little out of date now.

1:34:07.920 --> 1:34:09.520
 It needs to be updated a bit, but, you know,

1:34:09.520 --> 1:34:10.400
 you could at least look through

1:34:10.400 --> 1:34:12.960
 the first couple hundred years of economics,

1:34:12.960 --> 1:34:15.040
 which is not a bad place to start.

1:34:15.040 --> 1:34:17.560
 More recently, I mean, a book I really enjoyed

1:34:17.560 --> 1:34:20.280
 is by my friend and colleague, Max Tagmark,

1:34:20.280 --> 1:34:21.320
 called Life 3.0.

1:34:21.320 --> 1:34:22.400
 You should have him on your podcast

1:34:22.400 --> 1:34:23.240
 if you haven't already.

1:34:23.240 --> 1:34:25.480
 He was episode number one.

1:34:25.480 --> 1:34:26.520
 Oh my God.

1:34:26.520 --> 1:34:30.240
 And he's back, he'll be back, he'll be back soon.

1:34:30.240 --> 1:34:31.560
 Yeah, no, he's terrific.

1:34:31.560 --> 1:34:33.440
 I love the way his brain works

1:34:33.440 --> 1:34:35.760
 and he makes you think about profound things.

1:34:35.760 --> 1:34:38.560
 He's got such a joyful approach to life.

1:34:38.560 --> 1:34:41.080
 And so that's been a great book.

1:34:41.080 --> 1:34:43.160
 And, you know, I learn a lot from it, I think everybody,

1:34:43.160 --> 1:34:44.360
 but he explains it in a way,

1:34:44.360 --> 1:34:45.600
 even though he's so brilliant,

1:34:45.600 --> 1:34:47.360
 that, you know, everyone can understand,

1:34:47.360 --> 1:34:48.560
 that I can understand.

1:34:49.640 --> 1:34:51.720
 You know, that's three, but let me mention

1:34:51.720 --> 1:34:52.920
 maybe one or two others.

1:34:52.920 --> 1:34:55.360
 I mean, I recently read More From Less

1:34:55.360 --> 1:34:58.640
 by my sometimes coauthor, Andrew McAfee.

1:34:58.640 --> 1:35:01.920
 It made me optimistic about how we can continue

1:35:01.920 --> 1:35:04.560
 to have rising living standards

1:35:04.560 --> 1:35:06.120
 while living more lightly on the planet.

1:35:06.120 --> 1:35:07.840
 In fact, because of higher living standards,

1:35:07.840 --> 1:35:09.120
 because of technology,

1:35:09.120 --> 1:35:11.480
 because of digitization that I mentioned,

1:35:11.480 --> 1:35:13.600
 we don't have to have as big an impact on the planet.

1:35:13.600 --> 1:35:15.720
 And that's a great story to tell

1:35:15.720 --> 1:35:17.480
 and he documents it very carefully.

1:35:19.760 --> 1:35:21.400
 You know, a personal kind of self help book

1:35:21.400 --> 1:35:23.840
 that I found kind of useful, People is Atomic Habits.

1:35:23.840 --> 1:35:25.200
 I think it's, what's his name?

1:35:25.200 --> 1:35:26.200
 James Clear.

1:35:26.200 --> 1:35:27.520
 Yeah, James Clear.

1:35:27.520 --> 1:35:29.080
 He's just, yeah, it's a good name

1:35:29.080 --> 1:35:30.440
 because he writes very clearly.

1:35:30.440 --> 1:35:33.600
 And, you know, most of the sentences I read in that book,

1:35:33.600 --> 1:35:34.480
 I was like, yeah, I know that,

1:35:34.480 --> 1:35:36.120
 but it just really helps to have somebody

1:35:36.120 --> 1:35:37.720
 like remind you and tell you

1:35:37.720 --> 1:35:40.840
 and kind of just reinforce it and...

1:35:40.840 --> 1:35:42.480
 So build habits in your life

1:35:42.480 --> 1:35:46.120
 that you hope to have a positive impact

1:35:46.120 --> 1:35:48.040
 and don't have to make it big things.

1:35:48.040 --> 1:35:49.200
 It could be just tiny little...

1:35:49.200 --> 1:35:50.680
 Exactly, I mean, the word atomic,

1:35:50.680 --> 1:35:52.520
 it's a little bit of a pun, I think he says.

1:35:52.520 --> 1:35:54.080
 You know, atomic means a really small thing

1:35:54.080 --> 1:35:55.480
 to take these little things,

1:35:55.480 --> 1:35:56.800
 but also like atomic power,

1:35:56.800 --> 1:35:59.440
 it can have like, you know, big impact.

1:35:59.440 --> 1:36:00.480
 That's funny, yeah.

1:36:01.440 --> 1:36:04.160
 The biggest ridiculous question,

1:36:04.160 --> 1:36:05.760
 especially to ask an economist,

1:36:05.760 --> 1:36:08.360
 but also a human being, what's the meaning of life?

1:36:08.360 --> 1:36:11.480
 I hope you've gotten the answer to that from somebody else.

1:36:11.480 --> 1:36:13.480
 I think we're all still working on that one,

1:36:13.480 --> 1:36:14.720
 but what is it?

1:36:14.720 --> 1:36:18.080
 You know, I actually learned a lot from my son, Luke,

1:36:18.080 --> 1:36:22.080
 and he's 19 now, but he's always loved philosophy

1:36:22.080 --> 1:36:24.880
 and he reads way more sophisticated philosophy than I do.

1:36:24.880 --> 1:36:25.880
 I once took him to Oxford

1:36:25.880 --> 1:36:26.880
 and he spent the whole time like

1:36:26.880 --> 1:36:29.040
 pulling all these obscure books down and reading them.

1:36:29.040 --> 1:36:32.600
 And a couple of years ago, we had this argument

1:36:32.600 --> 1:36:34.520
 and he was trying to convince me that hedonism

1:36:34.520 --> 1:36:37.480
 was the ultimate, you know, meaning of life,

1:36:37.480 --> 1:36:40.400
 just pleasure, yeah, seeking and...

1:36:40.400 --> 1:36:41.600
 Well, how old was he at the time?

1:36:41.600 --> 1:36:42.440
 17.

1:36:44.440 --> 1:36:46.680
 But he made a really good like intellectual argument

1:36:46.680 --> 1:36:47.880
 for it too, and you know...

1:36:47.880 --> 1:36:48.720
 Of course.

1:36:48.720 --> 1:36:50.200
 But it just didn't strike me as right.

1:36:50.200 --> 1:36:52.840
 And I think that, you know,

1:36:52.840 --> 1:36:54.520
 while I am kind of a utilitarian,

1:36:54.520 --> 1:36:55.920
 like, you know, I do think we should do the grace,

1:36:55.920 --> 1:36:58.720
 good for the grace number, that's just too shallow.

1:36:58.720 --> 1:37:00.520
 And I think I've convinced myself

1:37:00.520 --> 1:37:04.240
 that real happiness doesn't come from seeking pleasure.

1:37:04.240 --> 1:37:05.680
 It's kind of a little, it's ironic.

1:37:05.680 --> 1:37:07.680
 Like if you really focus on being happy,

1:37:07.680 --> 1:37:09.720
 I think it doesn't work.

1:37:09.720 --> 1:37:11.600
 You gotta like be doing something bigger.

1:37:11.600 --> 1:37:14.880
 It's, I think the analogy I sometimes use is, you know,

1:37:14.880 --> 1:37:17.600
 when you look at a dim star in the sky,

1:37:17.600 --> 1:37:19.440
 if you look right at it, it kind of disappears,

1:37:19.440 --> 1:37:20.720
 but you have to look a little to the side

1:37:20.720 --> 1:37:23.160
 and then the parts of your retina

1:37:23.160 --> 1:37:24.960
 that are better at absorbing light,

1:37:24.960 --> 1:37:26.360
 you know, can pick it up better.

1:37:26.360 --> 1:37:27.440
 It's the same thing with happiness.

1:37:27.440 --> 1:37:32.440
 I think you need to sort of find something other goal,

1:37:32.520 --> 1:37:34.000
 something, some meaning in life.

1:37:34.000 --> 1:37:36.200
 And that ultimately makes you happier

1:37:36.200 --> 1:37:39.080
 than if you go squarely at just pleasure.

1:37:39.080 --> 1:37:42.320
 And so for me, you know, the kind of research I do

1:37:42.320 --> 1:37:44.280
 that I think is trying to change the world,

1:37:44.280 --> 1:37:46.160
 make the world a better place.

1:37:46.160 --> 1:37:48.000
 And I'm not like an evolutionary psychologist,

1:37:48.000 --> 1:37:50.880
 but my guess is that our brains are wired,

1:37:50.880 --> 1:37:53.880
 not just for pleasure, but we're social animals

1:37:53.880 --> 1:37:57.200
 and we're wired to like help others.

1:37:57.200 --> 1:37:59.440
 And ultimately, you know, that's something

1:37:59.440 --> 1:38:02.040
 that's really deeply rooted in our psyche.

1:38:02.040 --> 1:38:04.520
 And if we do help others, if we do,

1:38:04.520 --> 1:38:06.640
 or at least feel like we're helping others,

1:38:06.640 --> 1:38:08.240
 you know, our reward systems kick in

1:38:08.240 --> 1:38:10.440
 and we end up being more deeply satisfied

1:38:10.440 --> 1:38:13.600
 than if we just do something selfish and shallow.

1:38:13.600 --> 1:38:15.680
 Beautifully put, I don't think there's a better way

1:38:15.680 --> 1:38:17.000
 to end it, Eric.

1:38:17.000 --> 1:38:20.520
 You're one of the people when I first showed up at MIT

1:38:20.520 --> 1:38:22.440
 that made me proud to be at MIT.

1:38:22.440 --> 1:38:24.560
 So it's so sad that you're now at Stanford,

1:38:24.560 --> 1:38:28.720
 but I'm sure you'll do wonderful things at Stanford as well.

1:38:28.720 --> 1:38:30.920
 I can't wait till future books

1:38:30.920 --> 1:38:32.280
 and people should definitely read the other books.

1:38:32.280 --> 1:38:33.120
 Well, thank you so much.

1:38:33.120 --> 1:38:34.120
 And I think we're all,

1:38:34.120 --> 1:38:36.200
 we're all part of the invisible college as we call it.

1:38:36.200 --> 1:38:38.720
 You know, we're all part of this intellectual

1:38:38.720 --> 1:38:41.680
 and human community where we all can learn from each other.

1:38:41.680 --> 1:38:43.120
 It doesn't really matter physically

1:38:43.120 --> 1:38:44.880
 where we are so much anymore.

1:38:44.880 --> 1:38:46.560
 Beautiful. Thanks for talking today.

1:38:46.560 --> 1:38:48.040
 My pleasure.

1:38:48.040 --> 1:38:49.440
 Thanks for listening to this conversation

1:38:49.440 --> 1:38:52.640
 with Eric Brynjalsson and thank you to our sponsors.

1:38:52.640 --> 1:38:55.080
 Bencero Watches, the maker of classy,

1:38:55.080 --> 1:38:57.840
 well performing watches, FourSigmatic,

1:38:57.840 --> 1:39:01.360
 the maker of delicious mushroom coffee, ExpressVPN,

1:39:01.360 --> 1:39:03.120
 the VPN I've used for many years

1:39:03.120 --> 1:39:06.640
 to protect my privacy on the internet, and Cash App,

1:39:06.640 --> 1:39:09.160
 the app I use to send money to friends.

1:39:09.160 --> 1:39:11.200
 Please check out these sponsors in the description

1:39:11.200 --> 1:39:14.920
 to get a discount and to support this podcast.

1:39:14.920 --> 1:39:17.280
 If you enjoy this thing, subscribe on YouTube,

1:39:17.280 --> 1:39:19.480
 review it with five stars on Apple Podcast,

1:39:19.480 --> 1:39:22.080
 follow on Spotify, support on Patreon,

1:39:22.080 --> 1:39:25.400
 or connect with me on Twitter at Lex Freedman.

1:39:25.400 --> 1:39:27.680
 And now let me leave you with some words

1:39:27.680 --> 1:39:30.000
 from Albert Einstein.

1:39:30.000 --> 1:39:32.880
 It has become appallingly obvious

1:39:32.880 --> 1:39:36.600
 that our technology has exceeded our humanity.

1:39:36.600 --> 1:39:48.600
 Thank you for listening and hope to see you next time.

