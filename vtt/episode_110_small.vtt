WEBVTT

00:00.000 --> 00:05.360
 The following is a conversation with Jitendra Malik, a professor at Berkeley and one of the

00:05.360 --> 00:10.960
 seminal figures in the field of computer vision, the kind before the deep learning revolution

00:10.960 --> 00:19.280
 and the kind after. He has been cited over 180,000 times and has mentored many world

00:19.280 --> 00:25.520
 class researchers in computer science. Quick summary of the ads. Two sponsors,

00:25.520 --> 00:32.400
 one new one, which is BetterHelp and an old, goodie, ExpressVPN. Please consider supporting

00:32.400 --> 00:39.680
 this podcast by going to betterhelp.com slash lex and signing up at expressvpn.com slash lex pod.

00:40.640 --> 00:46.000
 Click the links, buy the stuff. It really is the best way to support this podcast and the journey

00:46.000 --> 00:51.280
 I'm on. If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple

00:51.280 --> 00:56.880
 Podcasts supported on Patreon are connected with me on Twitter at Lex Friedman. However,

00:56.880 --> 01:02.320
 the heck you spell that. As usual, I'll do a few minutes of ads now and never any ads in the middle

01:02.320 --> 01:10.160
 that can break the flow of the conversation. This show is sponsored by BetterHelp spelled H E L P

01:10.160 --> 01:16.960
 help. Check it out at betterhelp.com slash lex. They figure out what you need and match you with

01:16.960 --> 01:24.000
 a licensed professional therapist in under 48 hours. It's not a crisis line. It's not self help.

01:24.000 --> 01:29.760
 It's professional counseling done securely online. I'm a bit from the David Goggins line

01:29.760 --> 01:36.800
 of creatures as you may know. And so have some demons to contend with usually on long runs

01:36.800 --> 01:43.120
 or all nights working forever and possibly full of self doubt. It may be because I'm Russian,

01:43.120 --> 01:49.760
 but I think suffering is essential for creation. But I also think you can suffer beautifully in a

01:49.760 --> 01:55.280
 way that doesn't destroy you. For most people, I think a good therapist can help in this. So it's

01:55.280 --> 02:01.600
 at least worth a try. Check out their reviews. They're good. It's easy, private, affordable,

02:01.600 --> 02:07.680
 available worldwide. You can communicate by text and your time and schedule weekly audio and video

02:07.680 --> 02:16.160
 sessions. I highly recommend that you check them out at betterhelp.com slash lex. This show is

02:16.160 --> 02:24.480
 also sponsored by ExpressVPN. Get it at expressvpn.com slash lex pod to support this podcast and

02:24.480 --> 02:31.200
 to get an extra three months free on a one year package. I've been using ExpressVPN for many years.

02:31.200 --> 02:37.920
 I love it. I think ExpressVPN is the best VPN out there. They told me to say it, but it happens to

02:37.920 --> 02:45.760
 be true. It doesn't log your data. It's crazy fast and is easy to use literally just one big sexy

02:45.760 --> 02:51.280
 power on button. Again, for obvious reasons, it's really important that they don't log your data.

02:51.280 --> 02:57.760
 It works on Linux and everywhere else too. But really, why use anything else? Shout out to my

02:57.760 --> 03:05.920
 favorite flavor of Linux Ubuntu Mate 2004. Once again, get it at expressvpn.com slash lex pod

03:05.920 --> 03:11.760
 to support this podcast and to get an extra three months free on a one year package.

03:12.960 --> 03:22.640
 And now here's my conversation with Jitendra Malik. In 1966, Seymour Papert at MIT wrote up a

03:22.640 --> 03:29.760
 proposal called the summer vision project to be given as far as we know to 10 students to work on

03:29.760 --> 03:35.280
 and solve that summer. So that proposal outlined many of the computer vision tasks we still work on

03:35.280 --> 03:41.840
 today. Why do you think we underestimate and perhaps we did underestimate and perhaps still

03:41.840 --> 03:49.600
 underestimate how hard computer vision is? Because most of what we do in vision, we do unconsciously

03:49.600 --> 03:57.760
 or subconsciously in human vision. So that effortlessness gives us the sense that, oh,

03:57.760 --> 04:07.760
 this must be very easy to implement on a computer. Now, this is why the early researchers in AI got

04:07.760 --> 04:17.120
 it so wrong. However, if you go into neuroscience or psychology of human vision, then the complexity

04:17.120 --> 04:24.240
 becomes very clear. The fact is that a very large part of the the cerebral cortex is devoted to

04:24.240 --> 04:31.280
 visual processing. I mean, and this is true in other primates as well. So once we looked at it

04:31.280 --> 04:37.120
 from a neuroscience or psychology perspective, it becomes quite clear that the problem is very

04:37.120 --> 04:42.880
 challenging and it will take some time. You said the high level parts are the harder parts?

04:42.880 --> 04:53.200
 I think vision appears to to be easy because most of what visual processing is subconscious or

04:53.200 --> 05:03.440
 unconscious. So we underestimate the difficulty. Whereas when you are like proving a mathematical

05:03.440 --> 05:10.160
 theorem or playing chess, the difficulty is much more evident. So because it is your conscious

05:10.160 --> 05:17.920
 brain, which is processing various aspects of the problem solving behavior. Whereas in vision,

05:18.560 --> 05:24.880
 all this is happening, but it's not in your awareness. It's in your it's operating below that.

05:25.520 --> 05:31.360
 But it still seems strange. Yes, that's true. But it seems strange that as computer vision

05:31.360 --> 05:39.360
 researchers, for example, the community broadly is time and time again makes the mistake of

05:40.160 --> 05:44.480
 thinking the problem is easier than it is. Or maybe it's not a mistake. We'll talk a little bit

05:44.480 --> 05:52.080
 about autonomous driving, for example, how hard of a vision task that is. Do you think, I mean,

05:53.760 --> 05:58.160
 is it just human nature or is there something fundamental to the vision problem that we

05:58.160 --> 06:04.800
 underestimate? We're still not able to be cognizant of how hard the problem is.

06:05.680 --> 06:11.600
 Yeah, I think in the early days, it could have been excused because in the early days,

06:11.600 --> 06:18.560
 all aspects of AI were regarded as too easy. But I think today it is much less excusable.

06:19.440 --> 06:27.760
 And I think why people fall for this is because of what I call the fallacy of the successful

06:27.760 --> 06:36.720
 first step. There are many problems in vision where getting 50% of the solution you can get in one

06:36.720 --> 06:44.480
 minute, getting to 90% can take you a day, getting to 99% may take you five years and

06:45.440 --> 06:51.280
 99.99% may be not in your lifetime. I wonder if that's a unique division.

06:51.280 --> 06:57.680
 It seems that language people are not so confident about, so natural language processing,

06:57.680 --> 07:04.640
 people are a little bit more cautious about our ability to solve that problem. I think for

07:04.640 --> 07:11.040
 language people intuit that we have to be able to do natural language understanding. For vision,

07:13.760 --> 07:18.880
 it seems that we're not cognizant or we don't think about how much understanding is required.

07:18.880 --> 07:26.160
 It's probably still an open problem. But in your sense, how much understanding is required to solve

07:26.160 --> 07:35.120
 vision? Put another way, how much something called common sense reasoning is required to

07:35.120 --> 07:43.840
 really be able to interpret even static scenes? Yeah, so vision operates at all levels. And there

07:43.840 --> 07:52.640
 are parts which can be solved with what we could call maybe peripheral processing. So in the human

07:52.640 --> 07:59.120
 vision literature, there used to be these terms sensation, perception, and cognition, which

07:59.120 --> 08:05.680
 roughly speaking referred to the front end of processing, middle stages of processing, and

08:05.680 --> 08:11.920
 higher level of processing. And I think they made a big deal out of this and they wanted

08:11.920 --> 08:18.400
 to study only perception and then dismiss certain problems as being, quote, cognitive.

08:18.960 --> 08:25.440
 But really, I think these are artificial divides. The problem is continuous at all levels,

08:26.080 --> 08:31.920
 and there are challenges at all levels. The techniques that we have today, they work better

08:31.920 --> 08:37.360
 at the lower and mid levels of the problem. I think the higher levels of the problem, quote,

08:37.360 --> 08:45.040
 the cognitive levels of the problem are there. And we, in many real applications, we have to

08:45.040 --> 08:52.080
 confront them. Now, how much that is necessary will depend on the application. For some problems,

08:52.080 --> 09:00.240
 it doesn't matter. For some problems, it matters a lot. So I am, for example, a pessimist on

09:00.240 --> 09:07.200
 fully autonomous driving in the near future. And the reason is because I think there will be

09:07.760 --> 09:16.800
 that 0.01% of the cases where quite sophisticated cognitive reasoning is called for. However,

09:16.800 --> 09:24.240
 there are tasks where you can, first of all, they are much more, they are robust. So in the sense

09:24.240 --> 09:32.160
 that error rates, error is not so much of a problem. For example, let's say you're doing

09:32.160 --> 09:40.080
 image search. You're trying to get images based on some description, some visual description.

09:41.680 --> 09:46.240
 We are very tolerant of errors there, right? I mean, when Google image search gives you some

09:46.240 --> 09:52.320
 images back and a few of them are wrong, it's okay. It doesn't hurt anybody. There's no,

09:52.320 --> 10:01.440
 there's not a matter of life and death. But making mistakes when you are driving at 60 miles per hour

10:01.440 --> 10:06.720
 and you could potentially kill somebody is much more important. So just for the,

10:07.840 --> 10:12.640
 for the fun of it, since you mentioned, let's go there briefly about autonomous vehicles.

10:12.640 --> 10:19.440
 So one of the companies in the space, Tesla, is with Andre Capati and Elon Musk are working on

10:19.440 --> 10:25.680
 a system called autopilot, which is primarily a vision based system with eight cameras and

10:26.320 --> 10:33.280
 basically a single neural network, a multitask neural network. They call it hydranet multiple heads.

10:33.280 --> 10:37.360
 So it does multiple tasks, but is forming the same representation at the core.

10:38.560 --> 10:45.920
 Do you think driving can be converted in this way to a purely a vision problem and then solved

10:45.920 --> 10:53.760
 with learning? Or even more specifically in the current approach, what do you think about

10:53.760 --> 11:00.240
 what Tesla autopilot team is doing? So the way I think about it is that there are certainly

11:01.280 --> 11:06.160
 subsets of the visual based driving problem, which are quite solvable. So for example,

11:06.160 --> 11:14.640
 driving in freeway conditions is quite a solvable problem. I think there were demonstrations of that

11:14.640 --> 11:23.120
 going back to the 1980s by someone called Ernst Tickman's in Munich. In the 90s, there were

11:23.120 --> 11:29.520
 approaches from Carnegie Mellon. There were approaches from our team at Berkeley. In the 2000s,

11:29.520 --> 11:36.880
 there were approaches from Stanford and so on. So autonomous driving in certain settings is

11:36.880 --> 11:45.360
 very doable. The challenge is to have an autopilot work under all kinds of driving conditions. At

11:45.360 --> 11:51.440
 that point, it's not just a question of vision or perception, but really also of control and

11:51.440 --> 11:58.000
 dealing with all the edge cases. So where do you think most of the difficult cases, to me,

11:58.000 --> 12:06.240
 even the highway driving is an open problem because it applies the same 50, 90, 95, 99 rule

12:06.240 --> 12:12.000
 or the first step, the fallacy of the first step. I forget how you put it. We fall victim too.

12:12.000 --> 12:16.320
 I think even highway driving has a lot of elements because to solve autonomous driving,

12:16.880 --> 12:24.000
 you have to completely relinquish the fat help of a human being. You're always in control. So

12:24.000 --> 12:29.200
 you're really going to feel the edge cases. So I think even highway driving is really difficult.

12:29.200 --> 12:34.800
 But in terms of the general driving task, do you think vision is the fundamental problem?

12:34.800 --> 12:41.520
 Or is it also your action, the interaction with the environment,

12:42.720 --> 12:47.520
 the ability to... And then the middle ground, I don't know if you put that under vision,

12:47.520 --> 12:53.520
 which is trying to predict the behavior of others, which is a little bit in the world of

12:53.520 --> 12:59.920
 understanding the scene, but it's also trying to form a model of the actors in the scene

12:59.920 --> 13:05.920
 and predict their behavior. Yeah, I include that in vision because to me, perception blends into

13:05.920 --> 13:12.480
 cognition and building predictive models of other agents in the world, which could be other agents,

13:12.480 --> 13:17.200
 could be people, other agents, could be other cars. That is part of the task of perception

13:18.240 --> 13:25.440
 because perception always has to not tell us what is now, but what will happen because what's

13:25.440 --> 13:33.200
 now is boring. It's done. It's over with. We care about the future because we act in the future.

13:33.200 --> 13:37.680
 And we care about the past in as much as it informs what's going to happen in the future.

13:38.800 --> 13:45.920
 So I think we have to build predictive models of behaviors of people and those can get quite

13:45.920 --> 13:58.080
 complicated. So I mean, I've seen examples of this in actually, I mean, I own a Tesla and

13:58.080 --> 14:05.520
 it has various safety features built in. And what I see are these examples where

14:05.520 --> 14:12.000
 let's say there is some skateboarder. I mean, and I don't want to be too critical because

14:12.000 --> 14:18.720
 obviously these systems are always being improved and any specific criticism I have,

14:19.280 --> 14:24.720
 maybe the system six months from now will not have that particular failure mode.

14:25.600 --> 14:37.040
 So it had the wrong response and it's because it couldn't predict what this skateboarder was going

14:37.040 --> 14:44.400
 to do. And because it really required that higher level cognitive understanding of what

14:44.400 --> 14:49.360
 skateboarders typically do as opposed to a normal pedestrian. So what might have been

14:49.360 --> 14:54.160
 the correct behavior for a pedestrian, a typical behavior for pedestrian was not the

14:54.880 --> 15:04.240
 typical behavior for a skateboarder. And so therefore to do a good job there,

15:04.240 --> 15:09.360
 you need to have enough data where you have pedestrians, you also have skateboarders,

15:09.360 --> 15:15.760
 you've seen enough skateboarders to see what kinds of patterns or behavior they have.

15:16.320 --> 15:24.160
 So it is in principle with enough data that problem could be solved. But I think our current

15:25.200 --> 15:31.920
 systems, computer vision systems, they need far, far more data than humans do for learning

15:31.920 --> 15:36.720
 those same capabilities. So say that there is going to be a system that solves autonomous

15:36.720 --> 15:42.960
 driving. Do you think it will look similar to what we have today, but have a lot more data,

15:42.960 --> 15:48.720
 perhaps more compute, but the fundamental architecture is involved? Well, in the case

15:48.720 --> 15:55.120
 of Tesla autopilot is neural networks. Do you think it will look similar? In that regard,

15:55.120 --> 16:02.320
 and we'll just have more data. That's a scientific hypothesis as to which way is it going to go. I

16:02.320 --> 16:10.160
 will tell you what I would bet on. And this is my general philosophical position on how these

16:10.880 --> 16:18.000
 learning systems have been. What we have found currently very effective in computer vision

16:18.000 --> 16:25.840
 in the deep learning paradigm is sort of tabular ASA learning and tabular ASA learning in a

16:25.840 --> 16:30.400
 supervised way with lots and lots of... What's tabular ASA learning? Tabular ASA in the sense

16:30.400 --> 16:37.600
 that blank slate. We just have the system which is given a series of experiences in this setting

16:37.600 --> 16:44.480
 and then it learns there. Now, if let's think about human driving, it is not tabular ASA learning.

16:44.480 --> 16:56.320
 So at the age of 16 in high school, a teenager goes into driver head class. And now at that point,

16:56.320 --> 17:03.760
 they learn, but at the age of 16, they are already visual geniuses because from 0 to 16,

17:04.560 --> 17:10.480
 they have built a certain repertoire of vision. In fact, most of it has probably been achieved by

17:10.480 --> 17:18.000
 age two. In this period of age up to age two, they know that the world is three dimensional.

17:18.000 --> 17:23.680
 They know how objects look like from different perspectives. They know about occlusion.

17:24.480 --> 17:31.120
 They know about common dynamics of humans and other bodies. They have some notion of intuitive

17:31.120 --> 17:38.480
 physics. So they built that up from their observations and interactions in early childhood

17:38.480 --> 17:46.800
 and of course, reinforced through their growing up to age 16. So then at age 16, when they go into

17:46.800 --> 17:53.200
 driver head, what are they learning? They're not learning afresh the visual world. They have a mastery

17:53.200 --> 18:00.080
 of the visual world. What they are learning is control. They are learning how to be smooth

18:00.080 --> 18:05.920
 about control, about steering and brakes and so forth. They're learning a sense of typical

18:05.920 --> 18:15.600
 traffic situations. Now, that education process can be quite short because they are coming in as

18:15.600 --> 18:22.240
 visual geniuses. And of course, in their future, they're going to encounter situations which are

18:22.240 --> 18:29.680
 very novel. So during my driver head class, I may not have had to deal with a skateboarder.

18:29.680 --> 18:37.760
 I may not have had to deal with a truck driving in front of me where the back opens up and some

18:37.760 --> 18:45.040
 junk gets dropped from the truck and I have to deal with it. But I can deal with this as a driver,

18:45.040 --> 18:50.000
 even though I did not encounter this in my driver head class. And the reason I can deal with it is

18:50.000 --> 18:57.680
 because I have all this general visual knowledge and expertise. And do you think the learning

18:57.680 --> 19:04.640
 mechanisms we have today can do that kind of long term accumulation of knowledge? Or do we have to

19:06.320 --> 19:13.120
 do some kind of... The work that led up to expert systems with knowledge representation,

19:13.120 --> 19:19.360
 the broader field of artificial intelligence worked on this kind of accumulation of knowledge.

19:19.920 --> 19:27.440
 Do you think neural networks can do the same? I think I don't see any in principle problem with

19:27.440 --> 19:32.880
 neural networks doing it. But I think the learning techniques would need to evolve significantly.

19:33.520 --> 19:42.240
 So the current learning techniques that we have are supervised learning. You're giving lots of

19:42.240 --> 19:49.520
 examples, X, Y, Y pairs, and you learn the functional mapping between them. I think that

19:49.520 --> 19:55.760
 human learning is far richer than that. It includes many different components. There is

19:55.760 --> 20:06.240
 a child who explores the world and sees... For example, a child takes an object and manipulates it

20:06.240 --> 20:13.120
 in his or her hand and therefore gets to see the object from different points of view. And the child

20:13.120 --> 20:17.840
 has commanded the movement. So that's a kind of learning data. But the learning data has been

20:17.840 --> 20:26.560
 arranged by the child. And this is a very rich kind of data. The child can do various experiments

20:26.560 --> 20:34.880
 with the world. So there are many aspects of human learning. And these have been studied in

20:36.320 --> 20:43.920
 child development by psychologists. And what they tell us is that supervised learning is a very

20:43.920 --> 20:51.120
 small part of it. There are many different aspects of learning. And what we would need to do is to

20:51.120 --> 21:02.160
 develop models of all of these and then train our systems with that kind of protocol.

21:02.160 --> 21:07.760
 So new methods of learning, some of which might imitate the human brain. But you also,

21:09.040 --> 21:13.200
 in your talks, I've mentioned sort of the compute side of things. In terms of the

21:13.200 --> 21:22.400
 difference in the human brain or referencing Hans Maravak. So do you think there's something

21:22.400 --> 21:28.560
 interesting, valuable to consider about the difference in the computational power of the human

21:28.560 --> 21:36.160
 brain versus the computers of today in terms of instructions per second? Yes. So if we go back...

21:36.160 --> 21:44.480
 So this is a point I've been making for 20 years now. And I think once upon a time, the way I used

21:44.480 --> 21:49.760
 to argue this was that we just didn't have the computing power of the human brain. Our computers

21:49.760 --> 21:59.280
 were not quite there. And I mean, there is a well known tradeoff, which we know that

21:59.280 --> 22:08.080
 the neurons are slow compared to transistors. But we have a lot of them and they have a very high

22:08.080 --> 22:14.880
 connectivity. Whereas in silicon, you have much faster devices, transistors switch at...

22:14.880 --> 22:22.400
 On the order of nanoseconds, but the connectivity is usually smaller. At this point in time,

22:22.400 --> 22:28.800
 I mean, we are now talking about 2020, we do have, if you consider the latest GPUs and so on,

22:28.800 --> 22:36.960
 amazing computing power. And if we look back at Hans Maravak's type of calculations, which he

22:36.960 --> 22:43.840
 did in the 1990s, we may be there today in terms of computing power comparable to the brain. But

22:43.840 --> 22:52.080
 it's not in the same style. It's of a very different style. So I mean, for example, the style of

22:52.080 --> 22:59.120
 computing that we have in our GPUs is far, far more power hungry than the style of computing that

22:59.120 --> 23:08.320
 is there in the human brain or other biological entities. Yeah. And that the efficiency part

23:08.320 --> 23:13.680
 is we're going to have to solve that in order to build actual real world systems of large scale.

23:14.880 --> 23:20.640
 Let me ask sort of the high level question. Taking a step back, how would you articulate

23:20.640 --> 23:26.560
 the general problem of computer vision? Does such a thing exist? So if you look at the computer vision

23:26.560 --> 23:32.320
 conferences and the work that's been going on, it's often separated into different little segments,

23:33.360 --> 23:39.440
 breaking the problem of vision apart into whether segmentation, 3D reconstruction,

23:40.000 --> 23:46.400
 object detection, I don't know, image capturing, whatever, there's benchmarks for each. But if

23:46.400 --> 23:52.160
 you were to sort of philosophically say, what is the big problem of computer vision? Does such a

23:52.160 --> 24:04.880
 thing exist? Yes, but it's not in isolation. So for all intelligence tasks, I always go back to

24:04.880 --> 24:12.640
 sort of biology or humans. And if you think about vision or perception in that setting,

24:12.640 --> 24:19.440
 we realize that perception is always to guide action. Perception for a biological system

24:19.440 --> 24:25.760
 does not give any benefits unless it is coupled with action. So we can go back and think about

24:26.240 --> 24:32.640
 the first multicellular animals which arose in the Cambrian era 500 million years ago.

24:33.360 --> 24:41.440
 And these animals could move and they could see in some way. And the two activities helped each

24:41.440 --> 24:50.400
 other because how does movement help? Movement helps that because you can get food in different

24:50.400 --> 24:58.320
 places. But you need to know where to go. And that's really about perception or seeing. I mean,

24:58.320 --> 25:04.800
 vision is perhaps the single most perception sense. But all the others are equally are also

25:04.800 --> 25:11.600
 important. So perception and action kind of go together. So earlier it was in these very

25:11.600 --> 25:19.040
 simple feedback loops which were about finding food or avoiding becoming food if there's a

25:19.040 --> 25:27.600
 predator running, trying to eat you up and so forth. So we must at the fundamental level

25:27.600 --> 25:35.760
 connect perception to action. Then as we evolved, perception became more and more sophisticated

25:36.400 --> 25:44.000
 because it served many more purposes. And so today we have what seems like a fairly general

25:44.000 --> 25:50.800
 purpose capability which can look at the external world and build a model of the external world

25:50.800 --> 25:57.680
 inside the head. We do have that capability. That model is not perfect. And psychologists

25:57.680 --> 26:03.440
 have great fun in pointing out the ways in which the model in your head is not a perfect model

26:03.440 --> 26:10.080
 of the external world. They create various illusions to show the ways in which it is

26:10.080 --> 26:17.840
 imperfect. But it's amazing how far it has come from a very simple perception action loop that

26:17.840 --> 26:26.640
 you exist in, you know, an animal 500 million years ago. Once we have these very sophisticated

26:26.640 --> 26:32.880
 visual systems, we can then impose a structure on them. It's we as scientists who are imposing

26:32.880 --> 26:39.280
 that structure where we have chosen to characterize this part of the system as this

26:39.280 --> 26:46.000
 quote module of object detection or quote this module of 3D reconstruction. What's going on

26:46.000 --> 26:56.240
 is really all of these processes are running simultaneously and they are running simultaneously

26:56.240 --> 27:02.640
 because originally their purpose was in fact to help guide action. So as a guiding general

27:02.640 --> 27:09.120
 statement of a problem, do you think we can say that the general problem of computer vision,

27:09.120 --> 27:16.080
 you said in humans, it was tied to action. Do you think we should also say that ultimately

27:16.080 --> 27:23.120
 that the goal, the problem of computer vision is to sense the world in the way that helps you

27:24.400 --> 27:30.880
 act in the world? Yes, I think that's the most fundamental, that's the most fundamental purpose.

27:30.880 --> 27:39.040
 We have by now hyper evolved. So we have this visual system which can be used for other things,

27:39.040 --> 27:46.480
 for example, judging the aesthetic value of a painting. And this is not guiding action,

27:46.480 --> 27:51.600
 maybe it's guiding action in terms of how much money you will put in your auction bid, but

27:51.600 --> 27:58.880
 that's a bit stretched. But the basics are in fact in terms of action, but we are not

27:58.880 --> 28:07.360
 talking about action, but we have evolved really this hyper, we have hyper evolved our visual

28:07.360 --> 28:13.440
 system. Actually, just to, sorry to interrupt, but perhaps it is fundamentally about action.

28:13.440 --> 28:20.880
 You're kind of jokingly said about spending, but perhaps the capitalistic drive that drives

28:20.880 --> 28:26.000
 a lot of the development in this world is about the exchange of money and the fundamental action

28:26.000 --> 28:30.400
 is money. If you watch Netflix, if you enjoy watching movies, you're using your perception

28:30.400 --> 28:35.200
 system to interpret the movie. Ultimately, your enjoyment of that movie means you'll

28:35.200 --> 28:42.800
 subscribe to Netflix. So the action is this extra layer that we've developed in modern society,

28:42.800 --> 28:49.040
 perhaps is fundamentally tied to the action of spending money. Well, certainly with respect to

28:49.040 --> 28:58.320
 interactions with firms. So in this homo economic role, when you're interacting with firms,

29:00.240 --> 29:02.560
 it does become that. That's it. What else is there?

29:05.440 --> 29:12.960
 And that was a rhetorical question. Okay. So to link on the division between the static and the

29:12.960 --> 29:18.320
 dynamic, so much of the work in computer vision, so many of the breakthroughs that you've been a

29:18.320 --> 29:26.080
 part of have been in the static world in looking at static images. And then you've also worked on

29:26.080 --> 29:30.640
 starting, but it's a much smaller degree. The community is looking at dynamic at video

29:31.360 --> 29:38.080
 at dynamic scenes. And then there is robotic vision, which is dynamic, but also where you're

29:38.080 --> 29:41.440
 actually have a robot in the physical world interacting based on that vision.

29:41.440 --> 29:51.680
 Which problem is harder? The sort of the trivial first answers of, well, of course,

29:51.680 --> 30:01.680
 one image is harder. But if you look at a deeper question there, are we, what's the term, cutting

30:01.680 --> 30:07.120
 ourselves at the knees or like making the problem harder by focusing on the images?

30:07.120 --> 30:15.360
 That's a fair question. I think sometimes we, we can simplify a problem so much

30:17.040 --> 30:23.040
 that we essentially lose part of the juice that could enable us to solve the problem.

30:24.160 --> 30:29.680
 And one could reasonably argue that to some extent this happens when we go from video to

30:29.680 --> 30:37.200
 single images. Now, historically, you have to consider the limits of imposed by the

30:37.840 --> 30:45.200
 computation capabilities we had. So if we, many of the choices made in the computer vision community

30:47.040 --> 30:56.960
 through the 70s, 80s, 90s can be understood as choices which were forced upon us by the

30:56.960 --> 31:01.600
 fact that we just didn't have access to compute enough compute.

31:01.600 --> 31:04.080
 Not enough memory, not enough hard drive.

31:04.080 --> 31:09.280
 Exactly. Not enough, not enough compute, not enough storage. So, so think of these choices.

31:09.280 --> 31:14.320
 So one of the choices is focusing on single images rather than video. Okay,

31:15.360 --> 31:23.680
 clear questions, storage and compute. We had to focus on, we did, we used to detect edges and

31:23.680 --> 31:29.600
 throw away the image, right? So you have an image which I say 256 by 256 pixels.

31:29.600 --> 31:35.360
 And instead of keeping around the grayscale value, what we did was we detected edges,

31:35.360 --> 31:41.120
 find the places where the brightness changes a lot. So now that's, and now, and then throw away

31:41.120 --> 31:47.040
 the rest. So this was a major compression device. And the hope was that this makes it,

31:47.040 --> 31:51.520
 that you can still work with it. And the logic was humans can interpret a line drawing.

31:51.520 --> 32:00.320
 And, and yes, and this will save us a computation. So many of the choices were dictated by that.

32:00.960 --> 32:09.440
 I think today we are no longer detecting edges, right? We process images with conlets

32:09.440 --> 32:13.280
 because we don't need to, we don't have that those compute restrictions anymore.

32:13.840 --> 32:18.880
 Now video is still understudied because video compute is still quite challenging

32:18.880 --> 32:25.200
 if you are a university researcher. I think video computing is not so challenging if you are at

32:25.200 --> 32:31.280
 Google or Facebook or Amazon. Still super challenging. I just spoke with the

32:31.280 --> 32:35.920
 VP of engineer and Google head of the YouTube search and discovery, and they still struggle

32:35.920 --> 32:42.240
 doing stuff on video. It's very difficult except doing, except using techniques that are essentially

32:42.240 --> 32:47.120
 the techniques used in the 90s, some very basic computer vision techniques.

32:47.120 --> 32:53.600
 No, that's when you want to do things at scale. So if you want to operate at the scale of all the

32:53.600 --> 32:57.680
 content of YouTube, it's very challenging. And there are similar issues in Facebook.

32:57.680 --> 33:04.240
 But as a researcher, you, you have, you have more, you know, opportunities.

33:04.240 --> 33:09.040
 You can train large networks with relatively large video data sets. Yeah.

33:09.040 --> 33:15.360
 Yes. So I think that this is part of the reason why we have so emphasized static images.

33:15.360 --> 33:21.840
 I think that this is changing. And over the next few years, I see a lot more progress happening

33:21.840 --> 33:30.080
 in video. So I, I have this generic statement that to me, video recognition feels like 10 years

33:30.080 --> 33:36.080
 behind object recognition. And you can quantify that because you can take some of the challenging

33:36.080 --> 33:43.520
 video data sets and their performance on action classification is like say 30%, which is kind

33:43.520 --> 33:51.440
 of what we used to have around 2009 in object detection, you know, it's like about 10 years

33:51.440 --> 33:57.440
 behind. And whether it'll take 10 years to catch up is a different question. Hopefully,

33:57.440 --> 34:03.600
 it will take less than that. Let me ask a similar question I've already asked. But once again,

34:03.600 --> 34:10.880
 so for dynamic scenes, do you think, do you think some kind of injection of knowledge

34:10.880 --> 34:19.040
 basis and reasoning is required to help improve like action recognition? Like if, if, if, if we

34:19.040 --> 34:24.000
 solve the general action recognition problem, what do you think the solution would look like?

34:24.000 --> 34:31.920
 There's another way. Yeah. So I, I completely agree that knowledge is called for. And that

34:31.920 --> 34:38.000
 knowledge can be quite sophisticated. So the way I would say it is, is that you have to

34:38.000 --> 34:43.200
 be quite sophisticated. So the way I would say it is that perception blends into cognition.

34:43.760 --> 34:52.400
 And cognition brings in issues of memory and this notion of a schema from psychology, which is,

34:53.440 --> 34:59.600
 let me use the classic example, which is you go to a restaurant, right? Now there are things

34:59.600 --> 35:06.240
 happen in a certain order, you walk in, somebody takes you to a table, waiter comes,

35:06.240 --> 35:13.680
 gives you a menu, takes the order, food arrives, eventually bill arrives, etc., etc.

35:14.800 --> 35:22.720
 This is a classic example of AI from the 1970s. It was called, there was the term frames and

35:23.440 --> 35:30.640
 scripts and schemas. These are all quite similar ideas. Okay. And then in the 70s, the way the AI

35:30.640 --> 35:36.960
 of the time dealt with it was by hand coding this. So they hand coded in this notion of a script and

35:36.960 --> 35:44.480
 the various stages and the actors and so on and so forth and use that to interpret, for example,

35:44.480 --> 35:51.600
 language. I mean, if there's a description of a, of a story involving some people eating at a

35:51.600 --> 35:58.320
 restaurant, there are all these inferences you can make because you know what happens typically

35:58.320 --> 36:05.120
 at a restaurant. So I think this kind of, this kind of knowledge is absolutely essential.

36:05.920 --> 36:10.640
 So I think that when we are going to do long form video understanding,

36:11.520 --> 36:16.720
 we are going to need to do this. I think the kinds of technology that we have right now with

36:16.720 --> 36:23.920
 3D convolutions over a couple of second of clip or video, it's very much tailored towards short

36:23.920 --> 36:30.800
 term video understanding, not that long term understanding, long term understanding requires

36:30.800 --> 36:37.520
 a notion of this notion of schemas that I talked about, perhaps some notions of goals,

36:37.520 --> 36:46.160
 intentionality, functionality and so on and so forth. Now, how will we bring that in? So we

36:46.160 --> 36:53.040
 could either revert back to the 70s and say, okay, I'm going to hand code in a script or

36:53.040 --> 37:01.920
 we might try to learn it. So I tend to believe that we have to find learning ways of doing this

37:02.800 --> 37:08.560
 because I think learning ways land up being more robust. And there must be a learning version of

37:08.560 --> 37:17.600
 the story because children acquire a lot of this knowledge by sort of just observation. So at no

37:17.600 --> 37:25.920
 moment in a child's life, it's possible, but I think it's not so typical that somebody that a

37:25.920 --> 37:31.280
 mother coaches a child through all the stages of what happens in a restaurant. They just go as a

37:31.280 --> 37:37.040
 family, they go to the restaurant, they eat, come back and the child goes through 10 such

37:37.040 --> 37:42.480
 experiences and the child has got a schema of what happens when you go to a restaurant.

37:42.480 --> 37:47.840
 So we somehow need to, we need to provide that capability to our systems.

37:47.840 --> 37:52.000
 You mentioned the following line from the end of the Alan Turing paper,

37:53.040 --> 37:58.720
 Computing Machinery and Intelligence that many people, like you said, many people know and

37:58.720 --> 38:05.120
 very few have read where he proposes the Turing test. This is how you know because it's towards

38:05.120 --> 38:09.920
 the end of the paper. Instead of trying to produce a program to simulate the adult mind,

38:09.920 --> 38:16.240
 why not rather try to produce one which simulates the child's? So that's a really interesting point.

38:16.960 --> 38:24.320
 If I think about the benchmarks we have before us, the tests of our computer vision systems,

38:24.320 --> 38:30.880
 they're often kind of trying to get to the adult. So what kind of benchmarks should we have?

38:30.880 --> 38:35.840
 What kind of tests for computer vision do you think we should have that mimic the child's

38:35.840 --> 38:41.920
 in computer vision? Yeah, I think we should have those and we don't have those today.

38:42.560 --> 38:49.200
 And I think the part of the challenge is that we should really be collecting data

38:49.840 --> 38:58.400
 of the type that a child experiences. So that gets into issues of privacy and so on and so

38:58.400 --> 39:05.040
 forth. But there are attempts in this direction to sort of try to collect the kind of data that

39:05.040 --> 39:11.680
 a child encounters growing up. So what's the child's linguistic environment? What's the child's

39:11.680 --> 39:20.000
 visual environment? So if we could collect that kind of data and then develop learning schemes

39:20.000 --> 39:27.600
 based on that data, that would be one way to do it. I think that's a very promising direction

39:27.600 --> 39:33.280
 myself. There might be people who would argue that we could just short circuit this in some way

39:33.280 --> 39:44.160
 and sometimes we have imitated, we have had success by not imitating nature in detail.

39:44.160 --> 39:55.520
 So the usual example is airplanes. We don't build flapping wings. So yes, that's one of the points

39:55.520 --> 40:04.960
 of debate. In my mind, I would bet on this learning like a child approach.

40:04.960 --> 40:11.760
 So one of the fundamental aspects of learning like a child is the interactivity. So the child

40:11.760 --> 40:16.240
 gets to play with the data set it's learning from. Yes. It's against the select. I mean,

40:16.240 --> 40:21.760
 you can call that active learning in the machine learning world. You can call it a lot of terms.

40:21.760 --> 40:28.080
 What are your thoughts about this whole space of being able to play with the data set or select

40:28.080 --> 40:36.480
 what you're learning? Yeah. So I think that I believe in that and I think that we could achieve

40:36.480 --> 40:48.320
 it in two ways and I think we should use both. So one is actually real robotics. So real physical

40:48.320 --> 40:53.920
 embodiments of agents who are interacting with the world and they have a physical body with

40:53.920 --> 41:01.040
 a dynamics and mass and moment of inertia and friction and all the rest and you learn your

41:01.040 --> 41:10.160
 body, the robot learns its body by doing a series of actions. The second is that simulation

41:10.160 --> 41:19.280
 environments. So I think simulation environments are getting much, much better. In my life,

41:19.280 --> 41:26.160
 in Facebook, AI research, our group has worked on something called Habitat, which is a simulation

41:26.160 --> 41:36.160
 environment, which is a visually photo realistic environment of places like houses or interiors

41:36.160 --> 41:42.800
 of various urban spaces and so forth. And as you move, you get a picture, which is a pretty

41:42.800 --> 41:52.720
 accurate picture. So now you can imagine that subsequent generations of these simulators

41:52.720 --> 42:01.200
 will be accurate, not just visually, but with respect to forces and masses and haptic interactions

42:01.200 --> 42:11.200
 and so on. And then we have that environment to play with. I think that, let me state one reason

42:11.200 --> 42:18.000
 why I think this being able to act in the world is important. I think that this is one way to break

42:18.800 --> 42:25.840
 the correlation versus causation barrier. So this is something which is of a great

42:25.840 --> 42:33.520
 deal of interest these days. People like Judea Pearl have talked a lot about that we are

42:33.520 --> 42:39.840
 neglecting causality and he describes the entire set of successes of deep learning as just curve

42:39.840 --> 42:49.440
 fitting. But I don't quite agree. He's a troublemaker, he is. But causality is important. But

42:49.440 --> 42:56.400
 causality is not like a single silver bullet. It's not like one single principle. There are many

42:56.400 --> 43:04.240
 different aspects here. And one of the ways in which one of our most reliable ways of establishing

43:04.240 --> 43:11.200
 causal links and this is the way, for example, the medical community does this is randomized

43:11.200 --> 43:17.680
 control trials. So you pick some situation and now in some situation you perform an action

43:17.680 --> 43:25.520
 and for certain others you don't. So you have a controlled experiment. Well, the child is in fact

43:25.520 --> 43:34.800
 performing controlled experiments all the time. In a small scale. But that is a way

43:35.520 --> 43:42.640
 that the child gets to build and refine its causal models of the world. And my colleague,

43:42.640 --> 43:47.440
 Alison Gopnik, has together with a couple of authors, co authors has this book called The

43:47.440 --> 43:53.520
 Scientist in the Crib, referring to his children. So I like the part that I like about that is

43:54.080 --> 44:00.400
 the scientist wants to do, wants to build causal models and the scientist does control

44:00.400 --> 44:05.760
 experiments. And I think the child is doing that. So to enable that, we will need to

44:05.760 --> 44:13.680
 have these, these active experiments. And I think this could be done some in the real world

44:13.680 --> 44:18.080
 and some in simulation. So you have hope for simulation. I have hope for simulation. That's

44:18.080 --> 44:23.200
 an exciting possibility if we can get to not just photo realistic, but what's that called

44:23.920 --> 44:33.120
 life realistic simulation. So you don't see any fundamental blocks to why we can't eventually

44:33.120 --> 44:37.600
 simulate the principles of what it means to exist in the world.

44:39.280 --> 44:44.240
 I don't see any fundamental problems that I mean, and look, the computer graphics community has come

44:44.240 --> 44:48.800
 a long way. So the in the early days, back going back to the 80s and 90s, they were,

44:49.760 --> 44:54.560
 they were focusing on visual realism, right? And then they could do the easy stuff, but they

44:54.560 --> 45:01.520
 couldn't do stuff like hair or fur and so on. Okay, well, they managed to do that. Then they

45:01.520 --> 45:08.160
 couldn't do physical actions, right? Like there's a bowl of glass and it falls down and it shatters,

45:08.160 --> 45:13.920
 but then they could start to do pretty realistic models of that and so on and so forth. So the

45:13.920 --> 45:20.400
 graphics people have shown that they can do this forward direction, not just for optical

45:20.400 --> 45:26.880
 interactions, but also for physical interactions. So I think, of course, some of that is very

45:26.880 --> 45:33.760
 computer intensive, but I think by and by, we will find ways of making our models ever more

45:33.760 --> 45:39.920
 realistic. You break vision apart into, in one of your presentations, early vision,

45:39.920 --> 45:43.680
 static scene understanding, dynamic scene understanding, and raise a few interesting

45:43.680 --> 45:50.160
 questions. I thought I could just throw some, some at you to see if you want to talk about them.

45:50.160 --> 45:58.320
 So early vision, so it's, what is it that you said? Sensation, perception, and cognition. So

45:58.320 --> 46:04.560
 is this a sensation? Yes. What can we learn from image statistics that we don't already know?

46:05.440 --> 46:12.960
 So at the lowest level, what, what can we make from just the, the, the, the

46:12.960 --> 46:17.760
 statistics, the basics or the, the variations in the rock pixels, the textures and so on?

46:17.760 --> 46:27.040
 Yeah. So what we seem to have learned is, is that there's a lot of redundancy in these images.

46:27.920 --> 46:34.000
 And as a result, we are able to do a lot of compression and, and this compression is very

46:34.000 --> 46:40.080
 important in biological settings, right? So you might have 10 to the eight photo receptors and

46:40.080 --> 46:44.800
 only 10 to the six fibers in the optic nerve. So you have to do this compression by a factor of

46:44.800 --> 46:53.600
 100 is to one. And, and so there are analogs of that which are happening in, in our neural

46:53.600 --> 46:57.680
 net, artificial neural network. That's the early layers. So you think there's, there's a lot of

46:57.680 --> 47:02.640
 compression that can be done in the beginning. Yeah. Just, just the statistics. Yeah.

47:05.440 --> 47:12.880
 How much, how much? Well, I saw, I mean, the, the way to think about it is just how successful is

47:12.880 --> 47:19.440
 image compression, right? And we, we, and there are, and that's been done with older technologies,

47:19.440 --> 47:27.040
 but it can be done with, there are several companies which are trying to use sort of these

47:27.040 --> 47:32.480
 more advanced neural network type techniques for compression, both for static images as well as for,

47:33.280 --> 47:39.760
 for video. One of my former students has a company which is trying to do stuff like this.

47:39.760 --> 47:47.920
 And I think, I think that they are showing quite interesting results. And I think that

47:47.920 --> 47:53.520
 that's all the success of that's really about image statistics and video statistics.

47:53.520 --> 47:58.080
 But that's still not doing compression of the kind. When I see a picture of a cat,

47:58.720 --> 48:02.560
 all I have to say is it's a cat. That's another semantic kind of compression.

48:02.560 --> 48:07.280
 Yeah. So this is, this is at the lower level, right? So we are, we are, as I said, yeah,

48:07.280 --> 48:11.760
 that's focusing on low level statistics. So to linger on that for a little bit,

48:12.960 --> 48:21.200
 you mentioned how far can bottom up image segmentation go? And in general, what you mentioned

48:21.200 --> 48:26.000
 that the central question for scene understanding is the interplay of bottom up and top down

48:26.000 --> 48:30.960
 information. Maybe this is a good time to elaborate on that. Maybe define what is,

48:31.920 --> 48:36.640
 what is bottom up, what is top down in the context of computer vision?

48:36.640 --> 48:44.800
 Right. That's, so today what we have are very interesting systems because they work completely

48:44.800 --> 48:49.360
 bottom up. How are they? What does bottom up mean? Sorry. So bottom up means, in this case,

48:49.360 --> 48:53.600
 means a feed forward neural network. So starting from the raw pixels.

48:53.600 --> 48:58.960
 Yeah. They start from the raw pixels and they, they end up with some, something like cat or

48:58.960 --> 49:05.520
 not a cat, right? So our, our systems are running totally feed forward. They're trained in a very

49:05.520 --> 49:11.280
 top down way. So they're trained by saying, okay, this is a cat. There's a cat. There's a dog.

49:11.280 --> 49:19.680
 There's a zebra, et cetera. And I'm not happy with either of these choices fully. We have gone into,

49:20.480 --> 49:26.640
 because we have completely separated these processes, right? So there's a, so I would

49:26.640 --> 49:35.440
 like the, the process, the, the, so what do we know compared to biology? So in biology, what we

49:35.440 --> 49:43.600
 know is that the processes in at test time at runtime, those processes are not purely feed

49:43.600 --> 49:49.840
 forward, but they involve feedback. So, and they involve much shallower neural networks.

49:49.840 --> 49:55.680
 So the kinds of neural networks we are using in computer vision, say a ResNet 50 has 50 layers.

49:55.680 --> 50:02.960
 Well, in, in the brain, in the visual cortex, going from the retina to IT, maybe we have like seven,

50:03.760 --> 50:08.880
 right? So they're far shallower, but we have the possibility of feedback. So there are backward

50:08.880 --> 50:17.440
 connections. And this might enable us to, to deal with the more ambiguous stimuli, for example.

50:17.440 --> 50:26.480
 So the, the biological solution seems to involve feedback. The solution in, in artificial vision

50:26.480 --> 50:32.560
 seems to be just feed forward, but with a much deeper network. And the two are functionally

50:32.560 --> 50:37.360
 equivalent, because if you have a feedback network, which just has like three rounds of feedback,

50:37.360 --> 50:43.600
 you can just unroll it and make it three times the depth and create it in a totally feed forward

50:43.600 --> 50:49.840
 way. So this is something which, I mean, we have written some papers on this theme, but I really

50:49.840 --> 50:54.640
 feel that this should, this theme should be pursued further.

50:54.640 --> 50:57.120
 Oh, some kind of recurrence mechanism.

50:57.120 --> 51:04.960
 Yeah. Okay. The other, so that's, so I, so I want to have a little bit more top down in the

51:04.960 --> 51:12.800
 at test time. Okay. Then at training time, we make use of a lot of top down knowledge right now.

51:13.600 --> 51:19.520
 So basically to learn to segment an object, we have to have all these examples of this is the

51:19.520 --> 51:23.360
 boundary of a cat, and this is the boundary of a chair, and this is the boundary of a horse,

51:23.360 --> 51:30.960
 and so on. And this is too much top down knowledge. How do humans do this? We manage to,

51:30.960 --> 51:37.360
 we manage with far less supervision, and we do it in a sort of bottom up way, because for example,

51:37.920 --> 51:45.360
 we're looking at a video stream and the horse moves, and that enables me to say that all these

51:45.360 --> 51:51.600
 pixels are together. So the Gestalt psychologists used to call this the principle of common fate.

51:52.880 --> 51:58.160
 So there was a bottom up process by which we were able to segment out these objects,

51:58.160 --> 52:06.960
 and we have totally focused on this top down training signal. So in my view, we have currently

52:06.960 --> 52:13.600
 solved it in machine vision, this top down bottom up interaction, but I don't find the

52:13.600 --> 52:20.000
 solution fully satisfactory. And I would rather have a bit of both in at both stages.

52:20.000 --> 52:23.600
 For all computer vision problems, not just segmentation.

52:23.600 --> 52:30.160
 And the question that you can ask is, so for me, I'm inspired a lot by human vision,

52:30.160 --> 52:34.800
 and I care about that. You could be just a hard boiled engineer, not give a damn.

52:35.360 --> 52:42.240
 So to you, I would then argue that you would need far less training data if you could make my

52:43.360 --> 52:46.480
 research and, you know, fruitful.

52:46.480 --> 52:52.800
 Okay, so then maybe taking a step into segmentation, static scene understanding,

52:53.840 --> 53:00.000
 what is the interaction between segmentation and recognition? You mentioned the movement of objects.

53:00.560 --> 53:07.600
 So for people who don't know computer vision, segmentation is this weird activity that computer

53:07.600 --> 53:14.480
 vision folks have all agreed is very important of drawing outlines around objects versus

53:14.480 --> 53:23.920
 a bounding box, and then classifying that object. What's the value of segmentation? What is it

53:24.560 --> 53:29.280
 as a problem in computer vision? How is it fundamentally different from

53:29.280 --> 53:37.280
 detection recognition and the other problems? Yeah, so I think, so segmentation enables us to say

53:37.280 --> 53:45.680
 that some set of pixels are an object without necessarily even being able to name that object

53:45.680 --> 53:53.600
 or knowing properties of that object. Oh, so you mean segmentation purely as the act of separating

53:53.600 --> 54:01.200
 an object from its background, a blob of that's united in some way from its background. Yeah,

54:01.200 --> 54:09.680
 so entityfication, if you will, making an entity out of it. Entityfication, beautifully. So I think

54:09.680 --> 54:20.640
 that we have that capability, and that enables us to, as we are growing up, to acquire names of

54:20.640 --> 54:26.800
 objects with very little supervision. So suppose the child, let's posit that the child has this

54:26.800 --> 54:34.560
 ability to separate out objects in the world. Then when the mother says, pick up your bottle or

54:36.320 --> 54:44.960
 the cat's behaving funny today, the word cat suggests some object, and then the child sort

54:44.960 --> 54:53.120
 of does the mapping. The mother doesn't have to teach specific object labels by pointing to them.

54:53.120 --> 55:02.400
 Weak supervision works in the context that you have the ability to create objects. So I think

55:02.400 --> 55:09.520
 that, so to me, that's a very fundamental capability. There are applications where this is very

55:09.520 --> 55:17.600
 important. For example, medical diagnosis. So in medical diagnosis, you have some brain scan,

55:17.600 --> 55:24.080
 I mean, this is some work that we did in my group where you have CT scans of people who have had

55:24.080 --> 55:30.800
 traumatic brain injury, and what the radiologist needs to do is to precisely delineate various

55:31.520 --> 55:39.920
 places where there might be bleeds, for example. And there are clear needs like that. So there's

55:39.920 --> 55:46.320
 certainly very practical applications of computer vision where segmentation is necessary. But

55:46.320 --> 55:54.960
 philosophically, segmentation enables the task of recognition to proceed with much weaker

55:54.960 --> 56:00.080
 supervision than we require today. And you think of segmentation as this kind of task

56:00.720 --> 56:09.600
 that takes on a visual scene and breaks it apart into interesting entities that might be useful

56:09.600 --> 56:17.520
 for whatever the task is. Yeah. And it is not semantics free. So I think, I mean, it blends

56:17.520 --> 56:26.080
 into, it involves perception and cognition. It is not, I think the mistake that we used

56:26.080 --> 56:32.320
 to make in the early days of computer vision was to treat it as a purely bottom up perceptual task.

56:32.320 --> 56:40.560
 It is not just that because we do revise our notion of segmentation with more experience,

56:41.280 --> 56:46.080
 right? Because, for example, there are objects which are non rigid, like animals or humans.

56:46.880 --> 56:53.360
 And I think understanding that all the pixels of a human are one entity is actually quite a

56:53.360 --> 57:00.560
 challenge because the parts of the human, they can move independently. The human wears clothes,

57:00.560 --> 57:04.560
 so they might be differently colored. So it's all sort of a challenge.

57:05.360 --> 57:09.440
 You mentioned the three hours of computer vision, our recognition reconstruction,

57:10.240 --> 57:15.200
 reorganization. Can you describe these three hours and how they interact?

57:15.200 --> 57:24.400
 Yeah. So recognition is the easiest one because that's what I think people generally think of

57:24.400 --> 57:32.400
 as computer vision achieving these days, which is labels. So is this a cat? Is this a dog?

57:32.400 --> 57:40.080
 Is this a chihuahua? I mean, it could be very fine grained, like specific breed of a dog

57:40.800 --> 57:46.800
 or a specific species of bird, or it could be very abstract like animal.

57:46.800 --> 57:51.360
 But given a part of an image or a whole image, say put a label on it.

57:51.360 --> 58:02.640
 Yeah. So that's recognition. Reconstruction is essentially, you can think of it as inverse

58:02.640 --> 58:10.320
 graphics. I mean, that's one way to think about it. So graphics is you have some internal computer

58:10.320 --> 58:17.200
 representation and you have a computer representation of some objects arranged in a scene.

58:17.200 --> 58:22.560
 And what you do is you produce a picture. You produce the pixels corresponding to a rendering

58:22.560 --> 58:35.360
 of that scene. So let's do the inverse of this. We are given an image and we say, oh, this image

58:35.360 --> 58:42.240
 arises from some objects in a scene looked at with a camera from this viewpoint. And we might

58:42.240 --> 58:49.920
 have more information about the objects like their shape, maybe the textures, maybe color,

58:49.920 --> 58:57.680
 et cetera, et cetera. So that's the reconstruction problem. In a way, you are in your head creating

58:57.680 --> 59:06.800
 a model of the external world. Okay. Reorganization is to do with essentially finding these entities.

59:06.800 --> 59:19.760
 So it's organization. The word organization implies structure. So in perception, in psychology,

59:19.760 --> 59:27.840
 we use the term perceptual organization, that the world is not just an image is not just seen as

59:28.560 --> 59:35.280
 is not internally represented as just a collection of pixels. But we make these entities, we create

59:35.280 --> 59:39.600
 these entities, objects, whatever you want to call it. And the relationship between the entities

59:39.600 --> 59:44.720
 as well? Or is it purely about the entities? It could be about the relationships, but mainly

59:44.720 --> 59:51.520
 we focus on the fact that there are entities. Okay. So I'm trying to pinpoint what the organization

59:51.520 --> 1:00:00.400
 means. So organization is that instead of like a uniform grid, we have this structure of objects.

1:00:00.400 --> 1:00:08.160
 So segmentation is the small part of that. So segmentation gets us going towards that.

1:00:08.160 --> 1:00:13.440
 Yeah. And you kind of have this triangle where they all interact together.

1:00:13.440 --> 1:00:22.560
 Yes. So how do you see that interaction in sort of reorganization is yes,

1:00:22.560 --> 1:00:31.760
 defining the entities in the world. The recognition is labeling those entities. And then reconstruction

1:00:31.760 --> 1:00:40.880
 is what filling in the gaps? Well, to, for example, see impute some 3D objects corresponding to each

1:00:40.880 --> 1:00:47.680
 of these entities, that would be part of adding more information that's not there in the raw data.

1:00:47.680 --> 1:00:57.280
 Correct. I mean, I started pushing this kind of a view in the around 2010 or something like that,

1:00:57.920 --> 1:01:06.480
 because at that time in computer vision, the distinction that people were just working on

1:01:06.480 --> 1:01:11.360
 many different problems, but they treated each of them as a separate isolated problem

1:01:11.920 --> 1:01:16.800
 with each with its own data set. And then you try to solve that and get good numbers on it.

1:01:16.800 --> 1:01:22.720
 So I wasn't, I didn't like that approach because I wanted to see the connection between these.

1:01:23.440 --> 1:01:31.280
 And if people divided up vision into various modules, the way they would do it is as low

1:01:31.280 --> 1:01:36.720
 level, mid level and high level vision corresponding roughly to the psychologist's

1:01:36.720 --> 1:01:43.840
 notion of sensation, perception and cognition. And I didn't, that didn't map to tasks that people

1:01:43.840 --> 1:01:50.800
 cared about. Okay. So therefore, I tried to promote this particular framework as a way

1:01:50.800 --> 1:01:54.800
 of considering the problems that people in computer vision were actually working on

1:01:55.360 --> 1:02:01.040
 and trying to be more explicit about the fact that they actually are connected to each other.

1:02:02.080 --> 1:02:08.640
 And I was at that time just doing this on the basis of information flow. Now it turns out

1:02:08.640 --> 1:02:18.880
 in the last five years or so, in the post the deep learning revolution that this architecture

1:02:19.600 --> 1:02:27.920
 has turned out to be very conducive to that because basically in these neural networks,

1:02:27.920 --> 1:02:35.520
 we are trying to build multiple representations. There can be multiple output heads sharing

1:02:35.520 --> 1:02:42.240
 common representations. So in a certain sense, today, given the reality of what solutions people

1:02:42.240 --> 1:02:52.400
 have to this, I do not need to preach this anymore. It is just there. It's part of the solution space.

1:02:52.400 --> 1:02:59.520
 So speaking of neural networks, how much of this problem of computer vision,

1:02:59.520 --> 1:03:11.200
 of reorganization, recognition can be reconstruction, how much of it can be learned end to end,

1:03:11.200 --> 1:03:20.000
 do you think? Sort of set it and forget it, just plug and play, have a giant dataset multiple,

1:03:20.000 --> 1:03:28.480
 perhaps multimodal, and then just learn the entirety of it. Well, so I think that currently

1:03:28.480 --> 1:03:35.440
 what that end to end learning means nowadays is end to end supervised learning. And that I would

1:03:35.440 --> 1:03:43.280
 argue is too narrow a view of the problem. I like this child development view, this lifelong

1:03:43.280 --> 1:03:48.560
 learning view, one where there are certain capabilities that are built up and then there

1:03:48.560 --> 1:03:59.120
 are certain capabilities which are built up on top of that. So that's what I believe in. So I think

1:04:02.240 --> 1:04:09.600
 end to end learning in this supervised setting for a very precise task to me is

1:04:09.600 --> 1:04:17.280
 kind of a sort of a limited view of the learning process.

1:04:18.080 --> 1:04:25.200
 Got it. So if we think about beyond purely supervised, look back to children. You mentioned

1:04:25.200 --> 1:04:33.360
 six lessons that we can learn from children of be multimodal, be incremental, be physical,

1:04:33.360 --> 1:04:40.320
 explore, be social, use language. Can you speak to these, perhaps picking one that you find most

1:04:40.320 --> 1:04:46.960
 fundamental to our time today? Yeah. So I mean, I should say to give a due credit, this is from a

1:04:46.960 --> 1:04:55.920
 paper by Smith and Gasser. And it reflects essentially, I would say, common wisdom among

1:04:55.920 --> 1:05:05.120
 child development people. It's just that these are, this is not common wisdom among people in

1:05:05.120 --> 1:05:14.640
 computer vision and AI and machine learning. So I view my role as trying to bridge the two worlds.

1:05:15.680 --> 1:05:22.880
 So let's take an example of a multimodal. I like that. So multimodal, a canonical example is

1:05:22.880 --> 1:05:32.320
 a child interacting with an object. So then the child holds a ball and plays with it.

1:05:32.320 --> 1:05:41.200
 So at that point, it's getting a touch signal. So the touch signal is getting a notion of 3D

1:05:41.200 --> 1:05:48.800
 shape, but it is sparse. And then the child is also seeing a visual signal. And these two,

1:05:48.800 --> 1:05:55.600
 so imagine these are two in totally different spaces. So one is the space of receptors on the

1:05:55.600 --> 1:06:02.880
 skin of the fingers and the thumb and the palm. And then these map onto these neuronal fibers

1:06:02.880 --> 1:06:10.320
 are getting activated somewhere. These lead to some activation in somatosensory cortex.

1:06:10.320 --> 1:06:17.600
 I mean, a similar thing will happen if we have a robot hand. And then we have the pixels corresponding

1:06:17.600 --> 1:06:25.440
 to the visual view, but we know that they correspond to the same object. So that's a very,

1:06:25.440 --> 1:06:31.360
 very strong cross calibration signal. And it is self supervisory, which is beautiful.

1:06:32.240 --> 1:06:36.800
 There's nobody assigning a label. The mother doesn't have to come and assign a label.

1:06:37.680 --> 1:06:40.960
 The child doesn't even have to know that this object is called a ball.

1:06:40.960 --> 1:06:47.120
 Okay, but the child is learning something about the three dimensional world from this

1:06:47.840 --> 1:06:54.880
 signal. I think tactile and visual, there is some work on, there is a lot of work currently

1:06:54.880 --> 1:07:01.680
 on audio and visual. Okay, and audio visual. So there is some event that happens in the world.

1:07:01.680 --> 1:07:07.680
 And that event has a visual signature, and it has an auditory signature. So there is this

1:07:07.680 --> 1:07:13.440
 glass bowl on the table, and it falls and breaks. And I hear the smashing sound and I see the pieces

1:07:13.440 --> 1:07:21.680
 of glass. Okay, I've built that connection between the two, right? We have people, I mean, this

1:07:21.680 --> 1:07:27.120
 become a hot topic in computer vision in the last couple of years. There are problems like

1:07:28.960 --> 1:07:35.280
 separating out multiple speakers, right? Which was a classic problem in, in audition,

1:07:35.280 --> 1:07:39.840
 they call this the problem of source separation or the cocktail party effect and so on.

1:07:40.400 --> 1:07:47.600
 But just try to do it visually when you also have, it becomes so much easier and so much

1:07:48.720 --> 1:07:56.640
 more useful. So the multimodal, I mean, there's so much more signal with multimodal and you can use

1:07:56.640 --> 1:08:02.240
 that for some kind of weak supervision as well. Yes, because they are occurring at the same time

1:08:02.240 --> 1:08:08.960
 in time. So you have time, which links the two, right? So at a certain moment, T1, you got a certain

1:08:08.960 --> 1:08:13.760
 signal in the auditory domain and a certain signal in the visual domain, but they must be causally

1:08:13.760 --> 1:08:19.600
 related. Yeah, that's an exciting area. Not well studied yet. Yeah, I mean, we have a little bit

1:08:19.600 --> 1:08:29.040
 of work at this, but so much more needs to be done. So this is a good example. Be physical,

1:08:29.040 --> 1:08:35.200
 that's to do with like something we talked about earlier that there's an embodied world.

1:08:36.320 --> 1:08:43.040
 To mention language, use language. So No Chomsky believes that language may be at the core of

1:08:43.040 --> 1:08:48.080
 cognition, at the core of everything in the human mind. What is the connection between language

1:08:48.080 --> 1:08:55.920
 and vision to you? Like what's more fundamental? Are they neighbors, is one the parent and the child,

1:08:55.920 --> 1:09:01.680
 the chicken and the egg? Oh, it's very clear. It is vision that is the parent. The parent is

1:09:01.680 --> 1:09:10.080
 the fundamental ability. Okay. Well, it comes before you think vision is more fundamental

1:09:10.080 --> 1:09:18.800
 in language. Correct. And you can think of it either in phylogeny or in ontogeny. So phylogeny

1:09:18.800 --> 1:09:25.360
 means if you look at evolutionary time, right? So we have vision that developed 500 million years

1:09:25.360 --> 1:09:32.400
 ago. Okay. Then something like when we get to maybe like 5 million years ago, you have the first

1:09:33.040 --> 1:09:40.400
 bipedal primate. So when we started to walk, then the hand became free. And so then manipulation,

1:09:40.400 --> 1:09:47.040
 the ability to manipulate objects and build tools and so on and so forth. So you said 500,000 years

1:09:47.040 --> 1:09:55.760
 ago? No, sorry. The first multicellular animals, which you can say had some intelligence, arose

1:09:55.760 --> 1:10:02.960
 500 million years ago. Okay. And now let's fast forward to say the last 7 million years,

1:10:03.680 --> 1:10:09.600
 which is the development of the hominid line, right? Where from the other primates, we have the

1:10:09.600 --> 1:10:18.320
 branch which leads on to modern humans. Now, there are many of these hominids, but the ones which

1:10:20.800 --> 1:10:25.360
 people talk about Lucy because that's like a skeleton from 3 million years ago. And we know

1:10:25.360 --> 1:10:33.520
 that Lucy walked. Okay. So at this stage, you have that the hand is free for manipulating objects.

1:10:33.520 --> 1:10:42.400
 And then the ability to manipulate objects, build tools and the brain size grew in this era.

1:10:43.280 --> 1:10:48.480
 So, okay. So now you have manipulation. Now, we don't know exactly when language arose.

1:10:49.360 --> 1:10:56.560
 But after that. But after that, because no apes have, I mean, so, I mean Chomsky is correct in

1:10:56.560 --> 1:11:04.160
 that that it is a uniquely human capability. And we primates, other primates don't have that.

1:11:04.160 --> 1:11:11.360
 But so it developed somewhere in this era. But it developed, I would, I mean,

1:11:11.360 --> 1:11:19.200
 argue that it probably developed after we had this stage of humans. I mean, the human species

1:11:19.200 --> 1:11:28.000
 already able to manipulate and hands free, much bigger brain size. And for that, there's a lot of

1:11:28.000 --> 1:11:34.960
 vision has already had to have developed. So the sensation and the perception may be some of the

1:11:34.960 --> 1:11:46.320
 cognition. Yeah. So those, so that vision, so the world, so these ancestors of us,

1:11:46.320 --> 1:11:52.480
 you know, three, four million years ago, they had, they had spatial intelligence.

1:11:53.120 --> 1:11:57.440
 So they knew that the world consists of objects. They knew that the objects were in

1:11:57.440 --> 1:12:05.040
 certain relationships to each other. They had observed causal interactions among objects.

1:12:05.040 --> 1:12:12.160
 They could move in space. So they had space and time and all of that. So language builds on that

1:12:12.160 --> 1:12:19.680
 substrate. So language has a lot of, I mean, I mean, the, all human languages have constructs

1:12:19.680 --> 1:12:25.520
 which depend on a notion of space and time. Where did that notion of space and time come from?

1:12:26.640 --> 1:12:30.880
 It had to come from perception and action in the world we live in.

1:12:30.880 --> 1:12:36.160
 Yeah. Well, you've referred to the spatial intelligence. Yeah. Yeah. So to linger a little

1:12:36.160 --> 1:12:45.440
 bit, we mentioned Turing and his mention of we should learn from children. Nevertheless, language

1:12:45.440 --> 1:12:51.520
 is the fundamental piece of the test of intelligence that Turing proposed. Yes. What do you think is

1:12:51.520 --> 1:12:56.560
 a good test of intelligence? Are you, what would impress the heck out of you? Is it

1:12:57.360 --> 1:13:01.200
 fundamentally in natural language or is there something in vision?

1:13:01.200 --> 1:13:08.640
 I think I wouldn't, I don't think we should have created a single test of intelligence.

1:13:09.920 --> 1:13:17.200
 So just like I don't believe in IQ as a single number, I think generally there can be many

1:13:17.200 --> 1:13:25.920
 capabilities which are correlated perhaps. So I think that there will be, there will be

1:13:25.920 --> 1:13:32.000
 accomplishments which are visual accomplishments, accomplishments which are accomplishments in

1:13:32.000 --> 1:13:38.240
 manipulation or robotics, and then accomplishments in language. I do believe that language will

1:13:38.240 --> 1:13:45.040
 be the hardest nut to crack. Really? Yeah. So what's harder to pass the spirit of the Turing

1:13:45.040 --> 1:13:50.960
 test or like whatever formulation will make it natural language, convincingly a natural language,

1:13:50.960 --> 1:13:54.480
 like somebody you would want to have a beer with, hang out and have a chat with,

1:13:54.480 --> 1:14:01.360
 or the general natural scene understanding, you think language is the problem?

1:14:01.360 --> 1:14:10.400
 I think I'm not a fan of the, I think Turing test, that Turing as he proposed the test in 1950

1:14:11.360 --> 1:14:17.280
 was trying to solve a certain problem. Yeah, imitation. Yeah. And I think it made a lot of

1:14:17.280 --> 1:14:26.800
 sense then, where we are today, 70 years later, I think we should not worry about that. I think

1:14:26.800 --> 1:14:34.800
 the Turing test is no longer the right way to channel research in AI, because that it takes

1:14:34.800 --> 1:14:40.960
 us down this path of this chatbot which can fool us for five minutes or whatever. I think

1:14:40.960 --> 1:14:48.480
 I would rather have a list of 10 different tasks. I mean, I think the tasks which they're

1:14:48.480 --> 1:14:53.680
 tasked in the manipulation domain, tasks in navigation, tasks in visual scene understanding,

1:14:53.680 --> 1:15:01.200
 tasks in reading a story and answering questions based on that. I mean, so my favorite language

1:15:02.160 --> 1:15:07.760
 understanding tasks would be reading a novel and being able to answer arbitrary questions from it.

1:15:07.760 --> 1:15:15.600
 Okay. Right. I think that to me, and this is not an exhaustive list by any means,

1:15:15.600 --> 1:15:21.760
 so I would, I think that that's what we, where we need to be going to and each of these,

1:15:22.480 --> 1:15:25.920
 on each of these axes, there's a fair amount of work to be done.

1:15:25.920 --> 1:15:30.960
 So on the visual understanding side, in this intelligence Olympics that we've set up,

1:15:30.960 --> 1:15:40.880
 what's a good test for one of many of visual scene understanding? Do you think such benchmarks

1:15:40.880 --> 1:15:46.000
 exist? Sorry to interrupt. No, there aren't any. I think, I think essentially to me,

1:15:46.640 --> 1:15:55.120
 a really good aid to the blind. So suppose there was a blind person and I needed to assist the

1:15:55.120 --> 1:16:03.040
 blind person. So ultimately, like we said, vision that aids in the action in the survival in this

1:16:03.040 --> 1:16:12.640
 world. Yeah. Maybe in the simulated world. Maybe easier to, to measure performance in a simulated

1:16:12.640 --> 1:16:19.920
 world. What we are ultimately after is performance in the real world. So David Hilbert in 1900 proposed

1:16:19.920 --> 1:16:26.320
 23 open problems of mathematics, some of which are still unsolved, most important, famous of

1:16:26.320 --> 1:16:31.280
 which is probably the Riemann hypothesis you've thought about and presented about the Hilbert

1:16:31.280 --> 1:16:37.680
 problems of computer vision. So let me ask, what do you today? I don't know when the last year you

1:16:37.680 --> 1:16:44.000
 presented that 2015, but versions of it, you're kind of the face and the spokesperson for computer

1:16:44.000 --> 1:16:52.000
 vision. It's your job to state what the problem, the open problems are for the field. So what

1:16:52.000 --> 1:16:58.240
 today are the Hilbert problems of computer vision? Do you think? Let me pick one to,

1:16:58.880 --> 1:17:06.480
 which I regard as clearly, clearly unsolved, which is what I would call long form video

1:17:06.480 --> 1:17:16.400
 understanding. So, so we have a video clip and we want to understand the behavior in there

1:17:17.120 --> 1:17:27.920
 in terms of agents, their goals, intentionality and make predictions about what might happen.

1:17:27.920 --> 1:17:37.760
 So that kind of understanding which goes away from atomic visual action. So in the short

1:17:37.760 --> 1:17:42.160
 range, the question is, are you sitting? Are you standing? Are you catching a ball?

1:17:43.760 --> 1:17:49.280
 That we can do now. Or even if we can't do it fully accurately, if we can do it at 50%,

1:17:50.160 --> 1:17:57.440
 maybe next year we'll do it at 65 and so forth. But I think the long range video understanding,

1:17:57.440 --> 1:18:04.640
 I don't think we can do today. And that means so long. And it blends into cognition. That's

1:18:04.640 --> 1:18:10.080
 the reason why it's challenging. So you have to track, you have to understand the entities,

1:18:10.080 --> 1:18:15.040
 you have to understand the entities, you have to track them, and you have to have some kind of model

1:18:15.040 --> 1:18:22.240
 of their behavior. Correct. And their behavior might be, these are agents. So they're not just

1:18:22.240 --> 1:18:28.160
 like passive objects, but they're agents. So therefore, they might, they would exhibit goal

1:18:28.160 --> 1:18:35.120
 directed behavior. Okay. So this is, this is one area. Then I will talk about, say, understanding

1:18:35.120 --> 1:18:42.080
 the world in 3D. Now, this may seem paradoxical because in a way, we have been able to do 3D

1:18:42.080 --> 1:18:48.640
 understanding even like 30 years ago, right? But I don't think we currently have the richness of

1:18:48.640 --> 1:18:56.560
 3D understanding in our computer vision system that we would like. Because, so let me elaborate on

1:18:56.560 --> 1:19:03.280
 that a bit. So currently, we have two kinds of techniques which are not fully unified. So

1:19:03.280 --> 1:19:08.560
 there are the kinds of techniques from multi view geometry that you have multiple pictures of a scene

1:19:08.560 --> 1:19:15.520
 and you do a reconstruction using stereoscopic vision or structure for motion. But these techniques

1:19:15.520 --> 1:19:22.400
 do not, they totally fail if you just have a single view because they are relying on this,

1:19:23.440 --> 1:19:29.040
 this multiple view geometry. Okay. Then we have some techniques that we have developed in the

1:19:29.040 --> 1:19:35.920
 computer vision community, which try to guess 3D from single views. And these techniques are based

1:19:35.920 --> 1:19:44.160
 on, on a supervised learning and they are based on having a training time, 3D models of objects

1:19:44.160 --> 1:19:51.520
 available. And this is completely unnatural supervision, right? That's not, CAD models are

1:19:51.520 --> 1:19:57.920
 not injected into your brain. Okay. So what would I like? What I would like would be a kind of

1:19:59.360 --> 1:20:11.680
 learning as you move around the world notion of 3D. So we, we have our succession of visual

1:20:11.680 --> 1:20:20.640
 experiences. And from those, we, so in, as part of that, I might see a chair from different view

1:20:20.640 --> 1:20:27.680
 points or a table from view point, different view points and so on. Now as part, that enables me

1:20:27.680 --> 1:20:35.120
 to build some internal representation. And then next time I just see a single photograph.

1:20:35.120 --> 1:20:40.240
 And it may not even be of that chair, it's of some other chair. And I have a guess of what its

1:20:40.240 --> 1:20:47.040
 3D shape is like. So you're almost learning the CAD model kind of? Yeah, implicitly. I mean,

1:20:47.040 --> 1:20:51.680
 implicitly. I mean, the CAD model need not be in the same form as used by computer graphics

1:20:51.680 --> 1:20:55.920
 programs. It's hidden in the representation. It's hidden in the representation, the ability

1:20:55.920 --> 1:21:03.040
 to predict new views and what I would see if I went to such and such position.

1:21:03.040 --> 1:21:12.800
 By the way, on a small tangent on that, are you uncomfortable, are you okay or comfortable with

1:21:13.440 --> 1:21:18.320
 neural networks that do achieve visual understanding that do, for example, achieve this kind of 3D

1:21:18.320 --> 1:21:25.040
 understanding? And you don't know how they, you don't know the, you're not able to interest,

1:21:25.040 --> 1:21:31.920
 but you're not able to visualize or understand or interact with the representation. So the fact

1:21:31.920 --> 1:21:39.840
 that they're not or may not be explainable. Yeah, I think that's fine. To me, that is, so

1:21:41.760 --> 1:21:47.520
 let me put some caveats on that. So it depends on the setting. So first of all, I think

1:21:53.760 --> 1:22:00.640
 humans are not explainable. Yeah, that's a really good point. One human to another human is not

1:22:00.640 --> 1:22:07.920
 fully explainable. I think there are settings where explainability matters. And these might,

1:22:07.920 --> 1:22:15.200
 these are, these might be, for example, questions on medical diagnosis. So I'm in a setting where

1:22:16.400 --> 1:22:20.080
 maybe the doctor, maybe a computer program has made a certain diagnosis.

1:22:21.120 --> 1:22:25.840
 And then depending on the diagnosis, perhaps I should have treatment A or treatment B,

1:22:25.840 --> 1:22:37.520
 right? So now is the computer programs diagnosis based on data, which was data collected off

1:22:38.400 --> 1:22:44.560
 for American males who are in their 30s and 40s, and maybe not so relevant to me,

1:22:45.120 --> 1:22:50.160
 maybe it is relevant, you know, et cetera, et cetera. And we, I mean, in medical diagnosis,

1:22:50.160 --> 1:22:55.440
 we have major issues to do with the reference class. So we may have acquired statistics from

1:22:55.440 --> 1:23:01.120
 one group of people and applying it to a different group of people who may not share all the same

1:23:01.120 --> 1:23:08.240
 characteristics. The data might have, there might be error bars in the prediction. So that prediction

1:23:08.880 --> 1:23:15.680
 should really be taken with a huge grain of salt. And, but this has an impact on what treatments

1:23:16.960 --> 1:23:23.840
 should be picked, right? So, so there are settings where I want to know more than just

1:23:23.840 --> 1:23:32.000
 this is the answer. But what I acknowledge is that the, so, so, so I, in that sense,

1:23:32.000 --> 1:23:38.320
 explainability and interpretability may matter. It's about giving error bounds and a better sense

1:23:38.320 --> 1:23:46.480
 of the quality of the decision. Where, what I, where I'm willing to sacrifice interpretability

1:23:46.480 --> 1:23:52.960
 is that I believe that there can be systems which can be highly performant, but which are internally

1:23:53.840 --> 1:23:59.440
 black boxes. And, and that seems to be words headed. Some of the best performing systems

1:23:59.440 --> 1:24:05.600
 are essentially black boxes, fundamentally by their construction. You and I are black boxes

1:24:05.600 --> 1:24:12.800
 to each other. Yeah. So the nice thing about the black boxes we are is, so we ourselves are black

1:24:12.800 --> 1:24:20.480
 boxes, but we're also the, those of us who are charming are able to convince others, like explain

1:24:20.480 --> 1:24:26.080
 the black, what's going on inside the black box with narratives of stories. So in some sense,

1:24:26.960 --> 1:24:32.320
 neural networks don't have to actually explain what's going on inside. They just have to come

1:24:32.320 --> 1:24:39.120
 up with stories real or fake that convince you that they know what's going on. And I'm sure we

1:24:39.120 --> 1:24:44.320
 can do that. We can create those neural, those stories, neural networks can create those stories.

1:24:44.320 --> 1:24:53.840
 Yeah. And the transformer will be involved. Do you think we will ever build a system of

1:24:53.840 --> 1:24:59.120
 human level or super human level intelligence? We've kind of defined what it takes to try to

1:24:59.120 --> 1:25:03.200
 approach that. But do you think we'll, do you think that's within our reach? The thing that we

1:25:03.200 --> 1:25:09.600
 thought we could do, what Turing thought actually we could do by year 2000, right? Do you think

1:25:09.600 --> 1:25:14.480
 we'll ever be able to do? Yeah. So I think there are two answers here. One question, one answer is

1:25:14.480 --> 1:25:23.040
 in principle, can we do this at some time? And my answer is yes. The second answer is a pragmatic

1:25:23.040 --> 1:25:28.240
 one. Do you think we will be able to do it in the next 20 years or whatever? And to that my

1:25:28.240 --> 1:25:38.640
 answer is no. So, and of course that's a wild guess. I think that Donald Rumsfeld is not a

1:25:38.640 --> 1:25:44.320
 favorite person of mine, but one of his lines was very good, which is about known unknowns,

1:25:45.200 --> 1:25:52.960
 known unknowns and unknown unknowns. So in the business we are in, there are known unknowns

1:25:52.960 --> 1:26:00.480
 and we have unknown unknowns. So I think with respect to a lot of what the case in

1:26:01.600 --> 1:26:09.040
 vision and robotics, I feel like we have known unknowns. So I have a sense of where we need

1:26:09.040 --> 1:26:16.080
 to go and what the problems that need to be solved are. I feel with respect to natural language,

1:26:16.080 --> 1:26:23.440
 understanding and high level cognition, it's not just known unknowns, but also unknown unknowns.

1:26:24.000 --> 1:26:28.560
 So it is very difficult to put any kind of a time frame to that.

1:26:30.720 --> 1:26:36.960
 Do you think some of the unknown unknowns might be positive in that they'll surprise us and make

1:26:36.960 --> 1:26:42.800
 the job much easier? So fundamental breakthroughs? I think that is possible because certainly I have

1:26:42.800 --> 1:26:50.800
 been very positively surprised by how effective these deep learning systems have been because I

1:26:50.800 --> 1:27:02.720
 certainly would not have believed that in 2010. I think what we knew from the mathematical theory

1:27:03.760 --> 1:27:07.920
 was that convex optimization works when there's a single global optima than

1:27:07.920 --> 1:27:15.360
 this gradient descent techniques would work. Now these are nonlinear systems with nonconvex

1:27:15.360 --> 1:27:22.640
 systems. Huge number of variables. So overparameterized. Overparameterized. And the people who used to

1:27:22.640 --> 1:27:29.680
 play with them a lot, the ones who were totally immersed in the lore and the black magic, they

1:27:29.680 --> 1:27:36.160
 knew that they worked well even though they were. Really? I thought like everybody was.

1:27:36.160 --> 1:27:43.520
 No, the claim that I hear from my friends like Jan Lekoon and so forth is that they feel that

1:27:43.520 --> 1:27:47.680
 they were comfortable with them. Well, he says that now. But the community as a whole

1:27:48.800 --> 1:27:56.080
 was certainly not. And I think we were, to me, that was the surprise that they actually

1:27:56.720 --> 1:28:03.760
 worked robustly for a wide range of problems from a wide range of initializations and so on.

1:28:03.760 --> 1:28:13.920
 And so that was certainly more rapid progress than we expected. But then there are certainly

1:28:13.920 --> 1:28:21.120
 lots of times. In fact, most of the history in AI is when we have made less progress at a slower

1:28:21.120 --> 1:28:33.120
 rate than we expected. So we just keep going. I think what I regard as really unwarranted

1:28:33.120 --> 1:28:42.400
 are these fears of AGI in 10 years and 20 years and that kind of stuff. Because that's based on

1:28:43.040 --> 1:28:47.440
 completely unrealistic models of how rapidly we will make progress in this field.

1:28:48.560 --> 1:28:54.800
 So I agree with you. But I've also gotten a chance to interact with very smart people who

1:28:54.800 --> 1:29:00.480
 really worry about the existential threats of AI. And as an open minded person, I'm sort of

1:29:00.480 --> 1:29:12.080
 taking it in. Do you think if AI systems, in some way, the unknown unknowns, not super

1:29:12.080 --> 1:29:17.280
 intelligent AI, but in ways we don't quite understand the nature of super intelligence,

1:29:17.280 --> 1:29:21.760
 will have a detrimental effect on society? Do you think this is something we should be

1:29:22.880 --> 1:29:28.800
 worried about? Or we need to first allow the unknown unknowns to become known unknowns?

1:29:28.800 --> 1:29:35.600
 I think we need to be worried about AI today. I think that it is not just a worry we need to

1:29:35.600 --> 1:29:43.760
 have when we get that AGI. I think that AI is being used in many systems today. And there might

1:29:43.760 --> 1:29:50.960
 be settings, for example, when it causes biases or decisions which could be harmful, I mean,

1:29:50.960 --> 1:29:56.240
 decisions which could be unfair to some people, or it could be a self driving cars which kills

1:29:56.240 --> 1:30:02.720
 a pedestrian. So AI systems are being deployed today, right? And they are being deployed in

1:30:02.720 --> 1:30:06.800
 many different settings, maybe in medical diagnosis, maybe in a self driving car, maybe

1:30:07.360 --> 1:30:14.080
 in selecting applicants for an interview. So I would argue that when these systems

1:30:14.080 --> 1:30:20.880
 make mistakes, there are consequences. And we are in a certain sense responsible for those

1:30:20.880 --> 1:30:29.760
 consequences. So I would argue that this is a continuous effort. And this is something that

1:30:30.320 --> 1:30:36.000
 in a way is not so surprising. It's about all engineering and scientific progress which

1:30:37.200 --> 1:30:42.320
 great power comes, great responsibility. So as these systems are deployed, we have to worry

1:30:42.320 --> 1:30:47.360
 about them. And it's a continuous problem. I don't think of it as something which will

1:30:47.360 --> 1:30:54.080
 suddenly happen on some day in 2079, for which I need to design some clever trick.

1:30:54.800 --> 1:31:01.040
 I'm saying that these problems exist today. And we need to be continuously on the lookout for

1:31:02.240 --> 1:31:09.680
 worrying about safety, biases, risks, right? I mean, the self driving car kills a pedestrian

1:31:09.680 --> 1:31:17.440
 and they have, right? I mean, there's Uber incident in Arizona, right? It has happened, right?

1:31:17.440 --> 1:31:23.680
 This is not about AGI. In fact, it's about a very dumb intelligence which is killing people.

1:31:23.680 --> 1:31:30.320
 The worry people have with AGI is the scale. And I, but I think you're 100% right is

1:31:31.280 --> 1:31:37.280
 like the thing that worries me about AI today. And it's happening in a huge scale is recommender

1:31:37.280 --> 1:31:44.160
 system, recommendation systems. So if you look at Twitter or Facebook or YouTube, they're controlling

1:31:45.600 --> 1:31:52.000
 the ideas that we have access to, the news and so on. And that's a fundamentally machine learning

1:31:52.000 --> 1:31:58.320
 algorithm behind each of these recommendations. And they, I mean, my life would not be the same

1:31:58.320 --> 1:32:04.000
 without these sources of information. I'm a totally new human being. And the ideas that I know

1:32:04.000 --> 1:32:08.880
 are very much because of the internet, because of the algorithm that recommend those ideas.

1:32:08.880 --> 1:32:16.400
 And so as they get smarter and smarter, I mean, that is the AGI is that's the algorithm that's

1:32:16.400 --> 1:32:24.080
 recommending the next YouTube video you should watch has control of millions of billions of people

1:32:25.040 --> 1:32:31.040
 that that algorithm is already super intelligent and has complete control of the population.

1:32:31.040 --> 1:32:38.000
 Not a complete, but very strong control. For now, we can turn off YouTube, we can just go have a

1:32:38.000 --> 1:32:45.440
 normal life outside of that. But the more and more that gets into our life, it's that algorithm will

1:32:45.440 --> 1:32:49.200
 start depending on it and the different companies that are working on the algorithm. So I think

1:32:49.200 --> 1:32:55.600
 it's, you're right, it's already, it's already there. And YouTube in particular is using computer

1:32:55.600 --> 1:33:03.200
 vision, doing their hardest to try to understand the content of videos so they could be able to

1:33:03.200 --> 1:33:09.120
 connect videos with the people who would benefit from those videos the most. And so that development

1:33:09.840 --> 1:33:15.680
 could go in a bunch of different directions, some of which might be harmful. So yeah, you're

1:33:15.680 --> 1:33:22.080
 right. The, the, the threats of AI here already, we should be thinking about them. On a philosophical

1:33:22.080 --> 1:33:30.960
 notion. If you could personal, perhaps, if you could relive a moment in your life outside of family,

1:33:31.760 --> 1:33:38.080
 because it made you truly happy or it was a profound moment that impacted the direction of your life,

1:33:38.720 --> 1:33:40.000
 what moment would you go to?

1:33:43.760 --> 1:33:51.040
 I don't think of single moments, but I look over the long haul. I feel that I've been very lucky

1:33:51.040 --> 1:34:01.040
 because I feel that I think that in scientific research, a lot of it is about being at the

1:34:01.040 --> 1:34:08.960
 right place at the right time. And you can, you can work on problems at a time when they're just

1:34:08.960 --> 1:34:14.800
 too premature, you know, you beat but your head against them and, and nothing happens because

1:34:14.800 --> 1:34:21.680
 it's the prerequisites for success are not there. And then there are times when you are in a field

1:34:21.680 --> 1:34:30.800
 which is all pretty mature and you can only solve curricules upon curricules. I've been lucky to

1:34:30.800 --> 1:34:37.840
 have been in this field, which for 34 years, 34, well, actually 34 years is a professor at Berkeley,

1:34:37.840 --> 1:34:47.680
 so longer than that, which when I started in it was just like some little crazy, absolutely

1:34:48.800 --> 1:34:55.280
 useless field, which couldn't really do anything to a time when it's really, really

1:34:56.720 --> 1:35:02.960
 solving a lot of practical problems has a lot has offered a lot of tools for scientific research,

1:35:02.960 --> 1:35:10.640
 right, because computer vision is impactful for images in biology or astronomy and so on and

1:35:10.640 --> 1:35:17.760
 so forth. And we have, so we have made great scientific progress, which has had real practical

1:35:17.760 --> 1:35:24.800
 impact in the world. And I feel lucky that I, I got in at a time when the field was

1:35:24.800 --> 1:35:33.680
 very young and at a time when it is, it's now mature, but not fully mature. It's mature, but not

1:35:33.680 --> 1:35:40.560
 done. I mean, it's really in still in a, in a productive phase. Yeah, I think people 500 years

1:35:40.560 --> 1:35:47.040
 from now would laugh at you calling this field mature. That is very possible. Yeah. So, but you're

1:35:47.040 --> 1:35:53.280
 also, lest I forget to mention, you've also mentored some of the biggest names of computer

1:35:53.280 --> 1:36:00.640
 vision, computer science and AI today. There's so many questions I could ask, but really is

1:36:00.640 --> 1:36:06.320
 what, what is it? How did you do it? What does it take to be a good mentor? What does it take to be

1:36:06.320 --> 1:36:16.640
 a good guide? Yeah, I think what I feel I've been lucky to have had very, very smart and hardworking

1:36:16.640 --> 1:36:24.960
 and creative students. I think some part of the credit just belongs to being at Berkeley. I think

1:36:24.960 --> 1:36:32.880
 those of us who are at top universities are blessed because we have very, very smart and capable

1:36:32.880 --> 1:36:39.120
 students coming on, knocking on our door. So, so I have to be humble enough to acknowledge that.

1:36:39.120 --> 1:36:47.760
 But what have I added? I think I have added something. What I have added is, I think what

1:36:47.760 --> 1:36:57.840
 I've always tried to teach them is a sense of picking the right problems. So, I think that in

1:36:57.840 --> 1:37:05.680
 science, in the short run, success is always based on technical competence. You're, you know,

1:37:05.680 --> 1:37:11.840
 you're quick with math or you are whatever. I mean, there's certain technical capabilities

1:37:11.840 --> 1:37:19.360
 which make for short range progress. Long range progress is really determined by asking the right

1:37:19.360 --> 1:37:28.160
 questions and focusing on the right problems. And I feel that what I've been able to bring to the

1:37:28.160 --> 1:37:36.000
 table in terms of advising these students is some sense of taste of what are good problems.

1:37:36.640 --> 1:37:41.360
 What are problems that are worth attacking now as opposed to waiting 10 years?

1:37:41.360 --> 1:37:46.720
 What's a good problem if you could summarize? If is that possible to even summarize? Like what's

1:37:46.720 --> 1:37:52.560
 your sense of a good problem? I think I think I have a sense of what is a good problem, which is

1:37:52.560 --> 1:38:01.680
 there is a British scientist. In fact, he won a Nobel Prize, Peter Medover, who has a book on this.

1:38:02.480 --> 1:38:09.840
 And basically he calls it the research is the art of the soluble. So, we need to sort of find

1:38:10.960 --> 1:38:19.760
 problems which are not yet solved, but which are approachable. And he sort of refers to this

1:38:19.760 --> 1:38:26.560
 sense that there is this problem which isn't quite solved yet, but it has a soft underbelly.

1:38:27.120 --> 1:38:35.360
 There is some place where you can spear the beast. And having that intuition that this

1:38:35.360 --> 1:38:41.120
 problem is ripe is a good thing, because otherwise you can just beat your head and not make progress.

1:38:42.160 --> 1:38:49.520
 So, I think that is important. So, if I have that and if I can convey that to students,

1:38:49.520 --> 1:38:55.120
 it's not just that they do great research while they're working with me, but that they continue

1:38:55.120 --> 1:39:00.480
 to do great research. So, in a sense, I'm proud of my students and their achievements and their

1:39:00.480 --> 1:39:06.800
 great research, even 20 years after they've seized being my student. So, some part developing,

1:39:06.800 --> 1:39:12.640
 helping them develop that sense that a problem is not yet solved, but is solvable. Correct.

1:39:12.640 --> 1:39:21.600
 The other thing which I have, which I think I bring to the table, is a certain intellectual

1:39:21.600 --> 1:39:28.800
 breadth. I've spent a fair amount of time studying psychology, neuroscience, relevant

1:39:28.800 --> 1:39:34.960
 areas of applied math and so forth. So, I can probably help them see some connections

1:39:34.960 --> 1:39:44.480
 to disparate things, which they might not have otherwise. So, the smart students coming into

1:39:44.480 --> 1:39:52.400
 Berkeley can be very deep in the sense, they can think very deeply, meaning very hard down one

1:39:52.400 --> 1:40:02.560
 particular path. But where I could help them is the shallow breadth, but whereas they would have

1:40:02.560 --> 1:40:11.280
 the narrow depth, but that's of some value. Well, it was beautifully refreshing just to hear you

1:40:12.720 --> 1:40:17.280
 naturally jump to psychology back to computer science and this conversation back and forth.

1:40:17.280 --> 1:40:23.440
 I mean, that's actually a rare quality and I think it's certainly for students empowering

1:40:23.440 --> 1:40:28.400
 to think about problems in a new way. So, for that and for many other reasons, I really enjoyed

1:40:28.400 --> 1:40:31.840
 this conversation. Thank you so much. It was a huge honor. Thanks for talking to me.

1:40:31.840 --> 1:40:37.920
 It's been my pleasure. Thanks for listening to this conversation with Jitendra Malik and thank

1:40:37.920 --> 1:40:45.920
 you to our sponsors, BetterHelp and ExpressVPN. Please consider supporting this podcast by going

1:40:45.920 --> 1:40:53.840
 to betterhelp.com slash Lex and signing up at expressvpn.com slash Lex pod. Click the links,

1:40:53.840 --> 1:41:00.000
 buy the stuff. It's how they know I sent you and it really is the best way to support this podcast

1:41:00.000 --> 1:41:05.760
 and the journey I'm on. If you enjoy this thing, subscribe on YouTube, review it with five stars

1:41:05.760 --> 1:41:11.920
 on an app or podcast, support it on Patreon or connect with me on Twitter at Lex Friedman.

1:41:11.920 --> 1:41:17.520
 Don't ask me how to spell that. I don't remember myself. And now let me leave you with some words

1:41:17.520 --> 1:41:25.520
 from Prince Mishkin in The Idiot by Dostoevsky. Beauty will save the world. Thank you for listening

1:41:25.520 --> 1:41:31.120
 and hope to see you next time.

