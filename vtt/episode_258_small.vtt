WEBVTT

00:00.000 --> 00:02.720
 The following is a conversation with Yann LeCun,

00:02.720 --> 00:04.560
 his second time in the podcast.

00:04.560 --> 00:09.160
 He is the chief AI scientist at Meta, formerly Facebook,

00:09.160 --> 00:13.040
 professor at NYU, touring award winner,

00:13.040 --> 00:15.600
 one of the seminal figures in the history

00:15.600 --> 00:18.480
 of machine learning and artificial intelligence,

00:18.480 --> 00:21.960
 and someone who is brilliant and opinionated

00:21.960 --> 00:23.400
 in the best kind of way,

00:23.400 --> 00:26.000
 and so it was always fun to talk to.

00:26.000 --> 00:28.840
 This is the Lex Friedman podcast, to support it,

00:28.840 --> 00:31.240
 please check out our sponsors in the description,

00:31.240 --> 00:35.040
 and now here's my conversation with Yann LeCun.

00:36.160 --> 00:37.560
 You cowrote the article,

00:37.560 --> 00:40.920
 self supervised learning, the dark matter of intelligence.

00:40.920 --> 00:43.720
 Great title, by the way, with Yann Misra.

00:43.720 --> 00:46.640
 So let me ask, what is self supervised learning

00:46.640 --> 00:49.920
 and why is it the dark matter of intelligence?

00:49.920 --> 00:51.760
 I'll start by the dark matter part.

00:53.320 --> 00:55.680
 There is obviously a kind of learning

00:55.680 --> 00:59.880
 that humans and animals are doing

00:59.880 --> 01:02.800
 that we currently are not reproducing properly

01:02.800 --> 01:04.680
 with machines or with AI, right?

01:04.680 --> 01:07.440
 So the most popular approaches to machine learning today

01:07.440 --> 01:09.680
 are, or pydimes I should say,

01:09.680 --> 01:12.720
 are supervised learning and reinforcement learning,

01:12.720 --> 01:15.120
 and they are extremely efficient.

01:15.120 --> 01:17.640
 Supervised learning requires many samples

01:17.640 --> 01:19.760
 for learning anything,

01:19.760 --> 01:22.760
 and reinforcement learning requires a ridiculously large

01:22.760 --> 01:27.320
 number of trial and errors for a system to run anything.

01:29.320 --> 01:31.560
 And that's why we don't have self driving cars.

01:32.960 --> 01:35.320
 That's a big leap from one to the other, okay?

01:35.320 --> 01:38.760
 So to solve difficult problems,

01:38.760 --> 01:42.320
 you have to have a lot of human annotation

01:42.320 --> 01:44.080
 for supervised learning to work,

01:44.080 --> 01:45.520
 and to solve those difficult problems

01:45.520 --> 01:46.680
 with reinforcement learning,

01:46.680 --> 01:50.240
 you have to have some way to maybe simulate that problem

01:50.240 --> 01:52.720
 such that you can do that large scale kind of learning

01:52.720 --> 01:54.440
 that reinforcement learning requires.

01:54.440 --> 01:58.040
 Right, so how is it that most teenagers

01:58.040 --> 02:02.280
 can learn to drive a car in about 20 hours of practice,

02:02.280 --> 02:07.280
 whereas even with millions of hours of simulated practice,

02:07.480 --> 02:09.200
 a self driving car can't actually learn

02:09.200 --> 02:10.720
 to drive itself properly?

02:12.120 --> 02:13.920
 And so obviously we're missing something, right?

02:13.920 --> 02:15.600
 And it's quite obvious for a lot of people

02:15.600 --> 02:19.520
 that the immediate response you get from many people

02:19.520 --> 02:22.880
 is, well, humans use their background knowledge

02:22.880 --> 02:25.840
 to learn faster, and they're right.

02:25.840 --> 02:28.320
 Now, how was that background knowledge acquired?

02:28.320 --> 02:30.120
 And that's the big question.

02:30.120 --> 02:32.440
 So now you have to ask,

02:32.440 --> 02:35.160
 how do babies in the first few months of life

02:35.160 --> 02:37.160
 learn how the world works?

02:37.160 --> 02:38.280
 Mostly by observation,

02:38.280 --> 02:40.360
 because they can hardly act in the world.

02:41.400 --> 02:43.200
 And they learn an enormous amount of background knowledge

02:43.200 --> 02:46.040
 about the world that may be the basis

02:46.040 --> 02:48.160
 of what we call common sense.

02:48.160 --> 02:51.280
 This type of learning, it's not learning a task,

02:51.280 --> 02:53.720
 it's not being reinforced for anything,

02:53.720 --> 02:57.320
 it's just observing the world and figuring out how it works.

02:58.400 --> 03:01.240
 Building world models, learning world models.

03:01.240 --> 03:02.120
 How do we do this?

03:02.120 --> 03:04.560
 And how do we reproduce this in machines?

03:04.560 --> 03:09.520
 So self supervision learning is one instance

03:09.520 --> 03:13.120
 or one attempt at trying to reproduce this kind of learning.

03:13.120 --> 03:16.400
 Okay, so you're looking at just observation,

03:16.400 --> 03:18.720
 so not even the interacting part of a child.

03:18.720 --> 03:21.600
 It's just sitting there watching mom and dad walk around,

03:21.600 --> 03:23.480
 pick up stuff, all of that.

03:23.480 --> 03:25.520
 That's what we mean by background knowledge.

03:25.520 --> 03:27.520
 Perhaps not even watching mom and dad,

03:27.520 --> 03:30.000
 just watching the world go by.

03:30.000 --> 03:31.960
 Just having eyes open or having eyes closed

03:31.960 --> 03:34.480
 or the very active opening and closing eyes

03:34.480 --> 03:36.280
 that the world appears and disappears,

03:36.280 --> 03:37.840
 all that basic information.

03:39.120 --> 03:43.160
 And you're saying in order to learn to drive,

03:43.160 --> 03:45.840
 like the reason humans are able to learn to drive quickly,

03:45.840 --> 03:47.360
 some faster than others,

03:47.360 --> 03:48.680
 is because of the background knowledge

03:48.680 --> 03:51.760
 they were able to watch cars operate in the world

03:51.760 --> 03:53.640
 in the many years leading up to it,

03:53.640 --> 03:55.760
 the physics of basics objects and all that kind of stuff.

03:55.760 --> 03:56.600
 That's right.

03:56.600 --> 03:57.440
 I mean, the basic physics of objects,

03:57.440 --> 04:00.880
 you don't even need to know how a car works, right?

04:00.880 --> 04:02.480
 Because that you can learn fairly quickly.

04:02.480 --> 04:03.840
 I mean, the example I use very often

04:03.840 --> 04:05.760
 is you're driving next to a cliff.

04:06.680 --> 04:10.560
 And you know in advance because of your understanding

04:10.560 --> 04:13.760
 of intuitive physics that if you turn the wheel to the right,

04:13.760 --> 04:16.760
 the car will veer to the right, we'll run off the cliff,

04:16.760 --> 04:17.600
 fall off the cliff,

04:17.600 --> 04:20.440
 and nothing good will come out of this, right?

04:20.440 --> 04:22.760
 But if you are a sort of, you know,

04:22.760 --> 04:25.120
 tabular rice reinforcement learning system

04:25.120 --> 04:27.080
 that doesn't have a model of the world,

04:28.200 --> 04:31.320
 you have to repeat folding off this cliff thousands of times

04:31.320 --> 04:32.800
 before you figure out it's a bad idea.

04:32.800 --> 04:34.600
 And then a few more thousand times

04:34.600 --> 04:37.040
 before you figure out how to not do it.

04:37.040 --> 04:39.240
 And then a few more million times before you figure out

04:39.240 --> 04:42.560
 how to not do it in every situation you ever encounter.

04:42.560 --> 04:45.840
 So self supervised learning still has to have

04:45.840 --> 04:50.640
 some source of truth being told to it by somebody.

04:50.640 --> 04:54.560
 So you have to figure out a way without human assistance

04:54.560 --> 04:56.640
 or without significant amount of human assistance

04:56.640 --> 04:59.160
 to get that truth from the world.

04:59.160 --> 05:04.000
 So the mystery there is how much signal is there,

05:04.000 --> 05:06.320
 how much truth is there that the world gives you,

05:06.320 --> 05:08.200
 whether it's the human world,

05:08.200 --> 05:10.080
 like you watch YouTube or something like that,

05:10.080 --> 05:12.960
 or it's the more natural world.

05:12.960 --> 05:14.920
 So how much signal is there?

05:14.920 --> 05:16.280
 So here's the trick.

05:16.280 --> 05:20.600
 There is way more signal in sort of a self supervised setting

05:20.600 --> 05:22.480
 than there is in either a supervised

05:22.480 --> 05:24.520
 or reinforcement setting.

05:24.520 --> 05:28.360
 And this is going to my analogy of the cake.

05:30.240 --> 05:32.320
 The cake has someone that's called it,

05:32.320 --> 05:36.000
 where when you try to figure out how much information

05:36.000 --> 05:37.840
 you ask the machine to predict

05:37.840 --> 05:41.000
 and how much feedback you give the machine at every trial,

05:41.000 --> 05:41.880
 in reinforcement learning,

05:41.880 --> 05:43.320
 you give the machine a single scalar,

05:43.320 --> 05:45.400
 you tell the machine you did good, you did bad,

05:45.400 --> 05:49.640
 and you only tell this to the machine once in a while.

05:49.640 --> 05:51.440
 When I say you, it could be the universe

05:51.440 --> 05:52.640
 telling the machine, right?

05:54.120 --> 05:55.840
 But it's just one scalar.

05:55.840 --> 05:58.480
 So as a consequence, you could not possibly learn

05:58.480 --> 06:01.120
 something very complicated without many, many, many trials

06:01.120 --> 06:04.760
 where you get many, many feedbacks of this type.

06:04.760 --> 06:08.880
 In supervised learning, you give a few bits to the machine

06:08.880 --> 06:10.240
 at every sample.

06:11.280 --> 06:14.160
 Let's say you're training a system on,

06:14.160 --> 06:16.320
 you know, recognizing images on ImageNet.

06:16.320 --> 06:17.680
 There is 1,000 categories,

06:17.680 --> 06:20.920
 that's a little less than 10 bits of information per sample.

06:22.200 --> 06:23.840
 But self supervised learning here is the setting.

06:23.840 --> 06:26.360
 You, ideally, we don't know how to do this yet,

06:26.360 --> 06:31.360
 but ideally you would show a machine a segment of video

06:31.680 --> 06:33.760
 and then stop the video and ask the machine

06:33.760 --> 06:35.560
 to predict what's going to happen next.

06:37.640 --> 06:38.720
 So you let the machine predict,

06:38.720 --> 06:42.840
 and then you let time go by and show the machine

06:42.840 --> 06:46.000
 what actually happened, and hope the machine will,

06:46.000 --> 06:47.920
 you know, learn to do a better job

06:47.920 --> 06:49.400
 at predicting next time around.

06:49.400 --> 06:51.600
 There's a huge amount of information you give the machine,

06:51.600 --> 06:56.080
 because it's an entire video clip of, you know,

06:56.080 --> 06:59.280
 of the future after the video clip you fed it

06:59.280 --> 07:00.280
 in the first place.

07:00.280 --> 07:02.880
 So both for language and for vision,

07:02.880 --> 07:06.880
 there's a subtle, seemingly trivial construction,

07:06.880 --> 07:08.480
 but maybe that's representative

07:08.480 --> 07:10.600
 of what is required to create intelligence,

07:10.600 --> 07:12.880
 which is filling the gap.

07:13.720 --> 07:14.560
 So...

07:14.560 --> 07:15.400
 Filling the gaps.

07:15.400 --> 07:18.320
 It sounds dumb, but can you?

07:18.320 --> 07:21.120
 It is possible that you could solve

07:21.120 --> 07:23.000
 all of intelligence in this way.

07:23.000 --> 07:27.840
 Just for both language, just give a sentence

07:27.840 --> 07:29.800
 and continue it, or give a sentence,

07:29.800 --> 07:33.480
 and there's a gap in it, some words blanked out,

07:33.480 --> 07:35.680
 and you fill in what words go there.

07:35.680 --> 07:39.160
 For vision, you give a sequence of images

07:39.160 --> 07:40.920
 and predict what's going to happen next,

07:40.920 --> 07:43.840
 or you fill in what happened in between.

07:43.840 --> 07:46.920
 Do you think it's possible that formulation alone

07:48.600 --> 07:50.960
 as a signal for self supervised learning

07:50.960 --> 07:53.600
 can solve intelligence for vision and language?

07:53.600 --> 07:56.280
 I think that's our best shot at the moment.

07:56.280 --> 07:59.120
 So whether this will take us all the way

07:59.120 --> 08:01.760
 to human level intelligence or something,

08:01.760 --> 08:04.840
 or just cat level intelligence is not clear,

08:04.840 --> 08:07.320
 but among all the possible approaches

08:07.320 --> 08:09.480
 that people have proposed, I think is our best shot.

08:09.480 --> 08:14.480
 So I think this idea of an intelligent system

08:14.640 --> 08:18.840
 filling in the blanks, either predicting the future,

08:18.840 --> 08:22.160
 inferring the past, filling in missing information.

08:23.760 --> 08:26.640
 I'm currently filling the blank of what is behind your head

08:26.640 --> 08:30.600
 and what your head looks like from the back,

08:30.600 --> 08:33.760
 because I have a basic knowledge about how humans are made.

08:33.760 --> 08:35.600
 And I don't know if you're gonna,

08:35.600 --> 08:37.280
 what word you're gonna say, at which point you're gonna speak,

08:37.280 --> 08:38.960
 whether you're gonna move your head this way or that way,

08:38.960 --> 08:40.280
 which way you're gonna look,

08:40.280 --> 08:42.080
 but I know you're not gonna just dematerialize

08:42.080 --> 08:44.920
 and reappear three meters down the hall,

08:46.280 --> 08:49.480
 because I know what's possible and what's impossible

08:49.480 --> 08:51.160
 according to the physics.

08:51.160 --> 08:53.280
 You have a model of what's possible, what's impossible,

08:53.280 --> 08:55.120
 and then you'd be very surprised if it happens,

08:55.120 --> 08:57.880
 and then you'll have to reconstruct your model.

08:57.880 --> 08:59.640
 Right, so that's the model of the world.

08:59.640 --> 09:02.280
 It's what tells you, what fills in the blanks.

09:02.280 --> 09:04.520
 So given your partial information

09:04.520 --> 09:07.240
 about the state of the world, given by your perception,

09:08.120 --> 09:11.400
 your model of the world fills in the missing information,

09:11.400 --> 09:13.800
 and that includes predicting the future,

09:13.800 --> 09:16.920
 retradicting the past, filling in things

09:16.920 --> 09:18.440
 you don't immediately perceive.

09:18.440 --> 09:22.320
 And that doesn't have to be purely generic vision

09:22.320 --> 09:24.360
 or visual information or generic language.

09:24.360 --> 09:28.960
 You can go to specifics like predicting

09:28.960 --> 09:30.320
 what control decision you make

09:30.320 --> 09:31.640
 when you're driving in a lane.

09:31.640 --> 09:35.680
 You have a sequence of images from a vehicle,

09:35.680 --> 09:39.680
 and then you have information, if you recorded on video,

09:39.680 --> 09:41.880
 where the car ended up going,

09:41.880 --> 09:44.280
 so you can go back in time and predict

09:44.280 --> 09:46.720
 where the car went based on the visual information.

09:46.720 --> 09:49.440
 That's very specific, domain specific.

09:49.440 --> 09:51.520
 Right, but the question is whether we can come up

09:51.520 --> 09:56.400
 with sort of a generic method for, you know,

09:56.400 --> 09:58.560
 training machines to do this kind of prediction

09:58.560 --> 09:59.880
 or filling in the blanks.

09:59.880 --> 10:03.280
 So right now, this type of approach

10:03.280 --> 10:05.640
 has been unbelievably successful

10:05.640 --> 10:08.240
 in the context of natural language processing.

10:08.240 --> 10:09.760
 Every modern natural language processing

10:09.760 --> 10:12.320
 is pre trained in self supervised manner

10:12.320 --> 10:13.240
 to fill in the blanks.

10:13.240 --> 10:15.000
 So you show it a sequence of words,

10:15.000 --> 10:16.440
 you remove 10% of them,

10:16.440 --> 10:17.960
 and then you train some gigantic neural net

10:17.960 --> 10:19.880
 to predict the words that are missing.

10:19.880 --> 10:22.680
 And once you've pre trained that network,

10:22.680 --> 10:26.560
 you can use the internal representation learned by it

10:26.560 --> 10:28.600
 as input to, you know,

10:28.600 --> 10:32.160
 something that you train supervised or whatever.

10:32.160 --> 10:33.320
 That's been incredibly successful,

10:33.320 --> 10:35.000
 not so successful in images,

10:35.000 --> 10:37.520
 although it's making progress.

10:37.520 --> 10:42.520
 And it's based on sort of manual data augmentation.

10:42.520 --> 10:43.520
 We can go into this later,

10:43.520 --> 10:47.160
 but what has not been successful yet is training from video.

10:47.160 --> 10:48.480
 So getting a machine to learn,

10:48.480 --> 10:51.520
 to represent the visual world, for example,

10:51.520 --> 10:52.800
 by just watching video,

10:52.800 --> 10:54.800
 nobody has really succeeded in doing this.

10:54.800 --> 10:57.520
 Okay, well, let's kind of give a high level overview.

10:57.520 --> 11:02.360
 What's the difference in kind and in difficulty

11:02.360 --> 11:03.960
 between vision and language?

11:03.960 --> 11:08.840
 So you said people haven't been able to really kind of crack

11:08.840 --> 11:10.480
 the problem of vision open

11:10.480 --> 11:11.960
 in terms of self supervised learning,

11:11.960 --> 11:13.800
 but that may not be necessary

11:13.800 --> 11:15.840
 because it's fundamentally more difficult.

11:15.840 --> 11:18.680
 Maybe like when we're talking about achieving,

11:18.680 --> 11:22.280
 like passing the touring test in the full spirit

11:22.280 --> 11:23.800
 of the touring test in language

11:23.800 --> 11:24.880
 might be harder than vision.

11:24.880 --> 11:26.360
 That's not obvious.

11:26.360 --> 11:29.400
 So in your view, which is harder

11:29.400 --> 11:31.920
 or perhaps are they just the same problem?

11:31.920 --> 11:34.800
 When the farther we get to solving each,

11:34.800 --> 11:36.680
 the more we realize it's all the same thing.

11:36.680 --> 11:37.640
 It's all the same cake.

11:37.640 --> 11:40.160
 I think what I'm looking for are methods

11:40.160 --> 11:43.560
 that make them look essentially like the same cake,

11:43.560 --> 11:44.760
 but currently they're not.

11:44.760 --> 11:48.480
 And the main issue with learning world models

11:48.480 --> 11:50.160
 or learning predictive models

11:50.160 --> 11:55.160
 is that the prediction is never a single thing

11:55.880 --> 11:59.200
 because the world is not entirely predictable.

11:59.200 --> 12:00.680
 It may be deterministic or stochastic.

12:00.680 --> 12:02.960
 We can get into the philosophical discussion about it,

12:02.960 --> 12:05.280
 but even if it's deterministic,

12:05.280 --> 12:07.440
 it's not entirely predictable.

12:07.440 --> 12:11.760
 And so if I play a short video clip

12:11.760 --> 12:14.160
 and then I ask you to predict what's going to happen next,

12:14.160 --> 12:16.360
 there's many, many plausible continuations

12:16.360 --> 12:18.320
 for that video clip.

12:18.320 --> 12:20.520
 And the number of continuation grows

12:20.520 --> 12:22.840
 with the interval of time

12:22.840 --> 12:26.480
 that you're asking the system to make a prediction for.

12:26.480 --> 12:29.800
 And so one big question we start supervising

12:29.800 --> 12:32.320
 is how you represent this uncertainty,

12:32.320 --> 12:35.200
 how you represent multiple discrete outcomes,

12:35.200 --> 12:40.200
 how you represent a continuum of possible outcomes, et cetera.

12:40.200 --> 12:45.000
 And if you are sort of a classical machine learning person,

12:45.000 --> 12:47.640
 you say, oh, you just represent a distribution, right?

12:48.920 --> 12:51.000
 And that we know how to do

12:51.000 --> 12:53.480
 when we're predicting words, missing words in the text

12:53.480 --> 12:56.040
 because you can have a neural net,

12:56.040 --> 12:58.400
 give a score for every word in the dictionary.

12:58.400 --> 13:02.240
 It's a big list of numbers, maybe 100,000 or so.

13:02.240 --> 13:04.600
 And you can turn them into a parallel distribution

13:04.600 --> 13:07.400
 that tells you when I say a sentence,

13:07.400 --> 13:12.360
 the cat is chasing the blank in the kitchen.

13:12.360 --> 13:15.320
 There are only a few words that make sense there.

13:15.320 --> 13:18.440
 It could be a mouse or it could be a lizard spot

13:18.440 --> 13:19.960
 or something like that, right?

13:21.640 --> 13:25.880
 And if I say the blank is chasing the blank in the savannah,

13:25.880 --> 13:27.880
 you also have a bunch of plausible options

13:27.880 --> 13:29.240
 for those two words, right?

13:31.000 --> 13:33.720
 Because you have kind of a underlying reality

13:33.720 --> 13:38.120
 you can refer to to sort of fill in those blanks.

13:38.120 --> 13:42.040
 So you cannot say for sure in the savannah

13:42.040 --> 13:44.480
 if it's a lion or a cheetah or whatever,

13:44.480 --> 13:49.560
 you cannot know if it's a zebra or a do or whatever,

13:49.560 --> 13:51.000
 wildebeest, the same thing.

13:55.360 --> 13:56.840
 But you can represent the uncertainty

13:56.840 --> 13:58.520
 by just a long list of numbers.

13:58.520 --> 14:01.800
 Now, if I do the same thing with video

14:01.800 --> 14:04.360
 when I ask you to predict a video clip,

14:04.360 --> 14:07.400
 it's not a discrete set of potential frames.

14:07.400 --> 14:10.000
 You have to have somewhere representing

14:10.000 --> 14:13.560
 a sort of infinite number of plausible continuations

14:13.560 --> 14:16.240
 of multiple frames in a high dimensional,

14:16.240 --> 14:17.520
 continuous space.

14:17.520 --> 14:20.560
 And we just have no idea how to do this properly.

14:20.560 --> 14:22.920
 Finite, high dimensional.

14:22.920 --> 14:23.760
 So like you...

14:23.760 --> 14:25.360
 It's finite, high dimensional, yes.

14:25.360 --> 14:26.280
 Just like the words,

14:26.280 --> 14:31.280
 they try to get it to down to a small finite set

14:32.240 --> 14:34.240
 of like under a million, something like that.

14:34.240 --> 14:35.080
 Something like that.

14:35.080 --> 14:36.040
 I mean, it's kind of ridiculous

14:36.040 --> 14:39.040
 that we're doing a distribution

14:39.040 --> 14:42.920
 of every single possible word for language, and it works.

14:42.920 --> 14:45.360
 It feels like that's a really dumb way to do it.

14:46.520 --> 14:49.760
 Like there seems to be like there should be

14:49.760 --> 14:52.960
 some more compressed representation

14:52.960 --> 14:55.040
 of the distribution of the words.

14:55.040 --> 14:56.160
 You're right about that.

14:56.160 --> 14:58.920
 And so do you have any interesting ideas

14:58.920 --> 15:01.880
 about how to represent all the reality in a compressed way

15:01.880 --> 15:03.800
 such that you can form a distribution over?

15:03.800 --> 15:05.040
 That's one of the big questions, you know,

15:05.040 --> 15:06.200
 how do you do that?

15:06.200 --> 15:07.960
 But I mean, what's kind of, you know,

15:07.960 --> 15:12.120
 another thing that really is stupid about,

15:12.120 --> 15:13.080
 I shouldn't say stupid,

15:13.080 --> 15:15.560
 but like simplistic about current approaches

15:15.560 --> 15:19.360
 to self supervision in NLP in text

15:19.360 --> 15:21.920
 is that not only do you represent

15:21.920 --> 15:23.800
 a giant distribution over words,

15:23.800 --> 15:25.640
 but for multiple words that are missing,

15:25.640 --> 15:27.680
 those distributions are essentially independent

15:27.680 --> 15:28.520
 of each other.

15:30.160 --> 15:33.040
 And, you know, you don't pay too much of a price for this.

15:33.040 --> 15:34.560
 So you can't.

15:34.560 --> 15:36.720
 So, you know, the system, you know,

15:36.720 --> 15:39.640
 in the sentence that I gave earlier,

15:39.640 --> 15:43.600
 if it gives a certain probability for lion and cheetah,

15:43.600 --> 15:46.240
 and then a certain probability for, you know,

15:46.240 --> 15:50.240
 gazelle, wildebeest and zebra,

15:50.240 --> 15:54.840
 those two probabilities are independent of each other.

15:55.960 --> 15:58.080
 And it's not the case that those things are independent.

15:58.080 --> 16:01.480
 Lions actually attack like bigger animals than cheetahs.

16:01.480 --> 16:04.560
 So, you know, there's a huge independence hypothesis

16:04.560 --> 16:07.800
 in this process, which is not actually true.

16:07.800 --> 16:09.920
 The reason for this is that we don't know

16:09.920 --> 16:13.000
 how to represent properly distributions

16:13.000 --> 16:16.680
 over combinatorial sequences of symbols, essentially,

16:16.680 --> 16:19.000
 when they're, because the number grows exponentially

16:19.000 --> 16:21.320
 with the length of the symbols.

16:21.320 --> 16:22.800
 And so we have to use tricks for this,

16:22.800 --> 16:26.400
 but those techniques can, you know, get around,

16:26.400 --> 16:27.800
 like don't even deal with it.

16:27.800 --> 16:31.360
 So the big question is, like, would there be

16:31.360 --> 16:35.680
 some sort of abstract latent representation of text

16:35.680 --> 16:40.680
 that would say that, you know, when I switch lion for gazelle,

16:40.680 --> 16:45.520
 lion for cheetah, I also have to switch zebra for gazelle.

16:45.520 --> 16:48.760
 Yeah, so this independence assumption,

16:48.760 --> 16:50.280
 let me throw some criticism at you

16:50.280 --> 16:52.960
 that I often hear and see how you respond.

16:52.960 --> 16:56.040
 So this kind of feeling in the blanks is just statistics.

16:56.040 --> 16:58.920
 You're not learning anything,

16:58.920 --> 17:01.640
 like the deep underlying concepts.

17:01.640 --> 17:05.680
 You're just mimicking stuff from the past.

17:05.680 --> 17:07.560
 You're not learning anything new

17:07.560 --> 17:10.840
 such that you can use it to generalize about the world.

17:12.000 --> 17:14.160
 Or, okay, let me just say the crude version,

17:14.160 --> 17:16.240
 which is just statistics.

17:16.240 --> 17:17.880
 It's not intelligence.

17:17.880 --> 17:19.640
 Oh, what do you have to say to that?

17:19.640 --> 17:20.880
 What do you usually say to that

17:20.880 --> 17:22.640
 if you kind of hear this kind of thing?

17:22.640 --> 17:23.960
 I don't get into those discussions

17:23.960 --> 17:26.720
 because they are kind of pointless.

17:26.720 --> 17:28.720
 So first of all, it's quite possible

17:28.720 --> 17:30.440
 that intelligence is just statistics.

17:30.440 --> 17:32.720
 It's just statistics of a particular kind.

17:32.720 --> 17:33.640
 Yes.

17:33.640 --> 17:35.280
 Where this is the philosophical question.

17:35.280 --> 17:40.280
 Is it possible that intelligence is just statistics?

17:40.280 --> 17:41.560
 Yeah.

17:41.560 --> 17:43.520
 But what kind of statistics?

17:43.520 --> 17:46.200
 So if you are asking the question,

17:46.200 --> 17:50.640
 are the models of the world that we learn,

17:50.640 --> 17:52.320
 do they have some notion of causality?

17:52.320 --> 17:53.400
 Yes.

17:53.400 --> 17:56.360
 So if the criticism comes from people who say,

17:56.360 --> 17:59.440
 a current machine learning system don't care about causality,

17:59.440 --> 18:03.600
 which by the way is wrong, I agree with that.

18:03.600 --> 18:06.560
 Your model of the world should have your actions

18:06.560 --> 18:09.080
 as one of the inputs,

18:09.080 --> 18:11.400
 and that will drive you to learn causal models of the world

18:11.400 --> 18:15.080
 where you know what intervention in the world

18:15.080 --> 18:16.720
 will cause what result,

18:16.720 --> 18:18.000
 or you can do this by observation

18:18.000 --> 18:20.160
 of other agents acting in the world

18:20.160 --> 18:24.200
 and observing the effect of other humans, for example.

18:24.200 --> 18:28.400
 So I think at some level of description,

18:28.400 --> 18:30.200
 intelligence is just statistics.

18:31.640 --> 18:35.160
 But that doesn't mean you won't have models

18:35.160 --> 18:40.040
 that have deep mechanistic explanation for what goes on.

18:40.040 --> 18:41.760
 The question is how do you learn them?

18:41.760 --> 18:44.400
 That's the question I'm interested in.

18:44.400 --> 18:49.320
 Because a lot of people who actually voice their criticism

18:49.320 --> 18:51.000
 say that those mechanistic model

18:51.000 --> 18:52.640
 have to come from someplace else.

18:52.640 --> 18:54.040
 They have to come from human designers.

18:54.040 --> 18:56.200
 They have to come from, I don't know what.

18:56.200 --> 18:57.880
 And obviously we learn them.

18:59.280 --> 19:01.800
 Or if we don't learn them as an individual,

19:01.800 --> 19:04.920
 nature learned them for us using evolution.

19:04.920 --> 19:07.160
 So regardless of what you think,

19:07.160 --> 19:10.240
 those processes have been learned somehow.

19:10.240 --> 19:12.960
 So if you look at the human brain,

19:12.960 --> 19:14.680
 just like when we humans introspect

19:14.680 --> 19:16.360
 about how the brain works,

19:16.360 --> 19:20.280
 it seems like when we think about what is intelligence,

19:20.280 --> 19:22.480
 we think about the high level stuff,

19:22.480 --> 19:24.000
 like the models we've constructed,

19:24.000 --> 19:25.600
 concepts like cognitive science,

19:25.600 --> 19:28.760
 like concepts of memory and reasoning module,

19:28.760 --> 19:31.680
 almost like these high level modules.

19:31.680 --> 19:34.440
 Is this serve as a good analogy?

19:35.440 --> 19:40.440
 Like are we ignoring the dark matter,

19:40.440 --> 19:43.560
 the basic low level mechanisms?

19:43.560 --> 19:45.800
 Just like we ignore the way the operating system works,

19:45.800 --> 19:49.680
 we're just using the high level software.

19:49.680 --> 19:52.760
 We're ignoring that at the low level,

19:52.760 --> 19:56.480
 the neural network might be doing something like statistics.

19:56.480 --> 19:59.080
 Like me, sorry to use this word,

19:59.080 --> 20:00.600
 probably incorrectly and crudely,

20:00.600 --> 20:03.320
 but doing this kind of fill in the gap kind of learning

20:03.320 --> 20:05.720
 and just kind of updating the model constantly

20:05.720 --> 20:09.280
 in order to be able to support the raw sensory information,

20:09.280 --> 20:12.480
 to predict it and adjust to the prediction when it's wrong.

20:12.480 --> 20:15.880
 But like when we look at our brain at the high level,

20:15.880 --> 20:18.400
 it feels like we're playing chess,

20:18.400 --> 20:22.280
 like we're playing with high level concepts

20:22.280 --> 20:23.760
 and we're stitching them together

20:23.760 --> 20:26.080
 and we're putting them into long term memory,

20:26.080 --> 20:28.320
 but really what's going underneath

20:28.320 --> 20:30.240
 is something we're not able to introspect,

20:30.240 --> 20:34.480
 which is this kind of simple, large neural network

20:34.480 --> 20:36.080
 that's just filling in the gaps.

20:36.080 --> 20:38.280
 Right, well, okay, so there's a lot of questions

20:38.280 --> 20:39.800
 and a lot of answers there.

20:39.800 --> 20:40.640
 Okay, so first of all,

20:40.640 --> 20:42.680
 there's a whole school of thought in neuroscience,

20:42.680 --> 20:45.240
 competition on neuroscience in particular,

20:45.240 --> 20:47.800
 that likes the idea of predictive coding,

20:47.800 --> 20:50.120
 which is really related to the idea

20:50.120 --> 20:52.080
 I was talking about in self supervised learning.

20:52.080 --> 20:53.560
 So everything is about prediction.

20:53.560 --> 20:56.360
 The essence of intelligence is the ability to predict

20:56.360 --> 21:00.800
 and everything the brain does is trying to predict everything

21:00.800 --> 21:02.160
 from everything else.

21:02.160 --> 21:04.800
 Okay, and that's really sort of the underlying principle

21:04.800 --> 21:07.880
 if you want that self supervised learning

21:07.880 --> 21:10.720
 is trying to kind of reproduce this idea of prediction

21:10.720 --> 21:13.160
 as kind of an essential mechanism

21:13.160 --> 21:16.400
 of task independent learning if you want.

21:16.400 --> 21:19.400
 The next step is what kind of intelligence

21:19.400 --> 21:21.200
 are you interested in reproducing?

21:21.200 --> 21:25.360
 And of course, we all think about trying to reproduce

21:25.360 --> 21:28.400
 high level cognitive processes in humans,

21:28.400 --> 21:30.480
 but like with machines, we're not even at the level

21:30.480 --> 21:35.480
 of even reproducing the learning processes in a cat brain.

21:35.480 --> 21:38.240
 You know, the most intelligent of our intelligence systems

21:38.240 --> 21:41.160
 don't have as much common sense as a house cat.

21:42.160 --> 21:44.160
 So how is it that cats learn?

21:44.160 --> 21:46.760
 And cats don't do a whole lot of reasoning.

21:46.760 --> 21:48.480
 They certainly have causal models.

21:48.480 --> 21:52.480
 They certainly have, because many cats can figure out

21:52.480 --> 21:55.240
 like how they can act on the world to get what they want.

21:55.240 --> 22:00.240
 They certainly have a fantastic model of intuitive physics,

22:00.640 --> 22:03.560
 certainly of the dynamics of their own bodies,

22:03.560 --> 22:05.800
 but also of praise and things like that, right?

22:05.800 --> 22:08.760
 So they're pretty smart.

22:08.760 --> 22:11.240
 They only do this with about 800 million neurons.

22:12.560 --> 22:16.560
 We are not anywhere close to reproducing this kind of thing.

22:16.560 --> 22:20.040
 So to some extent, I could say,

22:20.040 --> 22:23.800
 let's not even worry about like the high level cognition

22:25.040 --> 22:27.160
 and kind of long term planning and reasoning that humans

22:27.160 --> 22:29.000
 can do until we figure out like,

22:29.000 --> 22:31.280
 can we even reproduce what cats are doing?

22:31.280 --> 22:35.880
 Now that said, this ability to learn world models,

22:35.880 --> 22:39.080
 I think is the key to the possibility

22:39.080 --> 22:41.920
 of running machines that can also reason.

22:41.920 --> 22:44.680
 So whenever I give a talk, I say there are three challenges

22:44.680 --> 22:46.320
 in the three main challenges in machine learning.

22:46.320 --> 22:48.160
 The first one is, you know,

22:48.160 --> 22:50.760
 getting machines to learn to represent the world.

22:50.760 --> 22:52.760
 And I'm proposing self supervised learning.

22:53.760 --> 22:56.920
 The second is getting machines to reason

22:56.920 --> 23:00.480
 in ways that are compatible with essentially gradient based

23:00.480 --> 23:02.480
 learning, because this is what deep learning is all about,

23:02.480 --> 23:03.480
 really.

23:04.480 --> 23:06.480
 And the third one is something we have no idea how to solve.

23:06.480 --> 23:09.480
 At least I have no idea how to solve is,

23:10.480 --> 23:13.480
 can we get machines to learn hierarchical representations

23:13.480 --> 23:17.480
 of action plans, you know, like, you know,

23:17.480 --> 23:19.480
 we know how to trend them to learn hierarchical representations

23:19.480 --> 23:22.480
 of perception, you know, with convolutional nets

23:22.480 --> 23:24.480
 and things like that and transformers.

23:24.480 --> 23:25.480
 But what about action plans?

23:25.480 --> 23:28.480
 Can we get them to spontaneously learn good hierarchical

23:28.480 --> 23:30.480
 representations of actions?

23:30.480 --> 23:31.480
 Also gradient based.

23:32.480 --> 23:35.480
 Yeah, all of that, you know, needs to be somewhat differentiable

23:35.480 --> 23:38.480
 so that you can apply sort of gradient based learning,

23:38.480 --> 23:40.480
 which is really what deep learning is about.

23:42.480 --> 23:47.480
 So it's background knowledge, ability to reason in a way

23:49.480 --> 23:52.480
 that's differentiable, that is somehow connected deeply

23:52.480 --> 23:55.480
 integrated with that background knowledge or builds

23:55.480 --> 23:57.480
 on top of that background knowledge.

23:57.480 --> 23:59.480
 And then giving that background knowledge be able to make

23:59.480 --> 24:01.480
 hierarchical plans in the world.

24:01.480 --> 24:02.480
 Right.

24:02.480 --> 24:05.480
 So if you take classical optimal control,

24:05.480 --> 24:07.480
 there's something classical optimal control called

24:08.480 --> 24:10.480
 model predictive control.

24:10.480 --> 24:13.480
 And it's, you know, it's been around since the early 60s.

24:13.480 --> 24:16.480
 NASA uses that to compute trajectories of rockets.

24:16.480 --> 24:20.480
 And the basic idea is that you have a predictive model

24:20.480 --> 24:23.480
 of the rocket, let's say, or whatever system you intend

24:23.480 --> 24:27.480
 to control, which given the state of the system at time t

24:27.480 --> 24:31.480
 and given an action that you're taking the system.

24:31.480 --> 24:33.480
 So for a rocket to be thrust and, you know,

24:33.480 --> 24:35.480
 all the controls you can have.

24:35.480 --> 24:38.480
 It gives you the state of the system at time t plus delta t.

24:38.480 --> 24:39.480
 Right.

24:39.480 --> 24:41.480
 So basically differential equation, something like that.

24:43.480 --> 24:47.480
 And if you have this model and you have this model in the form

24:47.480 --> 24:51.480
 of some sort of neural net or some sort of set of formula that

24:51.480 --> 24:53.480
 you can back propagate gradient through,

24:53.480 --> 24:55.480
 you can do what's called model predictive control

24:55.480 --> 24:58.480
 or gradient based model predictive control.

24:58.480 --> 25:04.480
 So you have, you can enroll that model in time.

25:04.480 --> 25:10.480
 You feel it a hypothesized sequence of actions.

25:10.480 --> 25:13.480
 And then you have some objective function that measures

25:13.480 --> 25:16.480
 how well at the end of the trajectory of the system

25:16.480 --> 25:19.480
 has succeeded or matched what you wanted to do.

25:19.480 --> 25:21.480
 You know, is it a robot harm?

25:21.480 --> 25:23.480
 Have you grasped the object you want to grasp?

25:23.480 --> 25:26.480
 If it's a rocket, you know, are you at the right place

25:26.480 --> 25:27.480
 near the space station?

25:27.480 --> 25:28.480
 Things like that.

25:28.480 --> 25:31.480
 And by back propagation through time, and again,

25:31.480 --> 25:34.480
 this was invented in the 1960s by optimal control theorists,

25:34.480 --> 25:38.480
 you can figure out what is the optimal sequence of actions

25:38.480 --> 25:44.480
 that will, you know, get my system to the best final state.

25:44.480 --> 25:47.480
 So that's a form of reasoning.

25:47.480 --> 25:50.480
 It's basically planning and a lot of planning systems

25:50.480 --> 25:52.480
 in robotics are actually based on this.

25:52.480 --> 25:56.480
 And you can think of this as a form of reasoning.

25:56.480 --> 25:59.480
 So, you know, to take the example of the teenager driving

25:59.480 --> 26:02.480
 a car again, you have a pretty good dynamical model of the car.

26:02.480 --> 26:04.480
 It doesn't need to be very accurate.

26:04.480 --> 26:07.480
 But you know, again, that if you turn the wheel to the right

26:07.480 --> 26:09.480
 and there is a cliff, you're going to run off the cliff, right?

26:09.480 --> 26:11.480
 You don't need to have a very accurate model to predict that.

26:11.480 --> 26:14.480
 And you can run this in your mind and decide not to do it

26:14.480 --> 26:17.480
 for that reason because you can predict in advance

26:17.480 --> 26:18.480
 that the result is going to be bad.

26:18.480 --> 26:20.480
 So you can sort of imagine different scenarios

26:20.480 --> 26:24.480
 and then, you know, employ or take the first step

26:24.480 --> 26:26.480
 in the scenario that is most favorable

26:26.480 --> 26:27.480
 and then repeat the process of planning.

26:27.480 --> 26:30.480
 That's called receding horizon model predictive control.

26:30.480 --> 26:32.480
 So, you know, all those things have names, you know,

26:32.480 --> 26:35.480
 going back, you know, decades.

26:35.480 --> 26:39.480
 And so if you're not, you know, in classical optimal control,

26:39.480 --> 26:42.480
 the model of the world is not generally learned.

26:42.480 --> 26:44.480
 There's, you know, sometimes a few parameters

26:44.480 --> 26:46.480
 you have to identify that's called systems identification.

26:46.480 --> 26:51.480
 But generally, the model is mostly deterministic

26:51.480 --> 26:52.480
 and mostly built by hand.

26:52.480 --> 26:56.480
 So the big question of AI, I think the big challenge of AI

26:56.480 --> 26:59.480
 for the next decade is how do we get machines

26:59.480 --> 27:01.480
 to learn predictive models of the world

27:01.480 --> 27:04.480
 that deal with uncertainty and deal with the real world

27:04.480 --> 27:05.480
 in all this complexity.

27:05.480 --> 27:07.480
 So it's not just trajectory of a rocket,

27:07.480 --> 27:09.480
 which you can reduce to first principles.

27:09.480 --> 27:12.480
 It's not even just a trajectory of a robot arm,

27:12.480 --> 27:15.480
 which again, you can model by, you know, careful mathematics.

27:15.480 --> 27:17.480
 But it's everything else, everything we observe in the world,

27:17.480 --> 27:22.480
 you know, people, behavior, you know, physical systems

27:22.480 --> 27:27.480
 that involve collective phenomena like water or, you know,

27:27.480 --> 27:31.480
 trees and, you know, branches in a tree or something

27:31.480 --> 27:35.480
 or like complex things that, you know, humans have no trouble

27:35.480 --> 27:37.480
 developing abstract representations

27:37.480 --> 27:39.480
 and predictive model for, but we still don't know

27:39.480 --> 27:40.480
 how to deal with machines.

27:40.480 --> 27:45.480
 Where do you put in these three maybe in the planning stages

27:45.480 --> 27:49.480
 the game theoretic nature of this world,

27:49.480 --> 27:53.480
 where your actions not only respond to the dynamic nature

27:53.480 --> 27:56.480
 of the world, the environment, but also affect it.

27:56.480 --> 28:01.480
 So if there's other humans involved, is this point number four

28:01.480 --> 28:04.480
 or is it somehow integrated into the hierarchical representation

28:04.480 --> 28:05.480
 of action in your view?

28:05.480 --> 28:07.480
 I think it's integrated.

28:07.480 --> 28:10.480
 It's just that now your model of the world has to deal with,

28:10.480 --> 28:12.480
 you know, it just makes it more complicated, right?

28:12.480 --> 28:16.480
 The fact that humans are complicated and not easily predictable,

28:16.480 --> 28:19.480
 that makes your model of the world much more complicated,

28:19.480 --> 28:20.480
 that much more complicated.

28:20.480 --> 28:24.480
 Well, there's a chess, I mean, I suppose chess is an analogy.

28:24.480 --> 28:27.480
 So Monte Carlo tree search.

28:27.480 --> 28:31.480
 I mean, there is a, I go, you go, I go, you go.

28:31.480 --> 28:37.480
 Like Andre Carpathia recently gave a talk at MIT about car doors.

28:37.480 --> 28:40.480
 I think there's some machine learning too, but mostly car doors.

28:40.480 --> 28:44.480
 And there's a dynamic nature to the car, like the person opening

28:44.480 --> 28:45.480
 the door checking.

28:45.480 --> 28:46.480
 I mean, he wasn't talking about that.

28:46.480 --> 28:48.480
 He was talking about the perception problem of what the,

28:48.480 --> 28:50.480
 the ontology of what defines a car door,

28:50.480 --> 28:52.480
 this big philosophical question.

28:52.480 --> 28:55.480
 But to me, it was interesting because like it's obvious that

28:55.480 --> 28:58.480
 the person opening the car doors, they're trying to get out like here

28:58.480 --> 29:00.480
 in New York, trying to get out of the car.

29:00.480 --> 29:03.480
 You're slowing down is going to signal something.

29:03.480 --> 29:05.480
 You're speeding up is going to signal something.

29:05.480 --> 29:06.480
 And that's a dance.

29:06.480 --> 29:09.480
 It's a asynchronous chess game.

29:09.480 --> 29:10.480
 I don't know.

29:10.480 --> 29:17.480
 So it feels like it's not just, I mean, I guess you can integrate

29:17.480 --> 29:19.480
 all of them to one giant model.

29:19.480 --> 29:23.480
 Like the entirety of these little interactions,

29:23.480 --> 29:25.480
 because it's not as complicated as chess.

29:25.480 --> 29:26.480
 It's just like a little dance.

29:26.480 --> 29:28.480
 We do like a little dance together.

29:28.480 --> 29:29.480
 And then we figure it out.

29:29.480 --> 29:33.480
 Well, in some ways it's way more complicated than chess because,

29:33.480 --> 29:34.480
 because it's continuous.

29:34.480 --> 29:37.480
 It's uncertain in a continuous manner.

29:37.480 --> 29:40.480
 It doesn't feel more complicated, but it doesn't feel more complicated

29:40.480 --> 29:43.480
 because that's what we're, we've evolved to solve.

29:43.480 --> 29:45.480
 This is the kind of problem we've evolved to solve.

29:45.480 --> 29:49.480
 And so we're good at it because, you know, nature has made us good at it.

29:49.480 --> 29:52.480
 Nature has not made us good at chess.

29:52.480 --> 29:54.480
 We completely suck at chess.

29:54.480 --> 29:58.480
 In fact, that's why we designed it as a, as a game is to be challenging.

29:58.480 --> 30:01.480
 And if there is something that, you know,

30:01.480 --> 30:06.480
 recent progress in chess and Go has made us realize is that humans

30:06.480 --> 30:07.480
 are really terrible at those things.

30:07.480 --> 30:08.480
 Like really bad.

30:08.480 --> 30:10.480
 You know, there was a story, right?

30:10.480 --> 30:15.480
 Before AlphaGo that, you know, the best Go player thought there were

30:15.480 --> 30:17.480
 maybe two or three stones behind, you know,

30:17.480 --> 30:19.480
 an ideal player that they would call God.

30:19.480 --> 30:23.480
 In fact, no, there are like nine or 10 stones behind.

30:23.480 --> 30:24.480
 I mean, we're just bad.

30:24.480 --> 30:29.480
 So we're not good at, and it's because we have limited working memory.

30:29.480 --> 30:32.480
 We, you know, we're not very good at like doing this tree exploration

30:32.480 --> 30:36.480
 that, you know, computers are much better at doing than we are.

30:36.480 --> 30:39.480
 But we are much better at learning differentiable models to the world.

30:39.480 --> 30:43.480
 I mean, I said differentiable in the kind of, you know,

30:43.480 --> 30:46.480
 I should say not differentiable in the sense that, you know,

30:46.480 --> 30:49.480
 we went back far through it, but in the sense that our brain has some

30:49.480 --> 30:53.480
 mechanism for estimating gradients of some kind.

30:53.480 --> 30:56.480
 And that's what, you know, makes us efficient.

30:56.480 --> 31:02.480
 So if you have an agent that consists of a model of the world,

31:02.480 --> 31:05.480
 which, you know, in the human brain is basically the entire front half

31:05.480 --> 31:13.480
 of your brain, an objective function, which in humans is a combination

31:13.480 --> 31:14.480
 of two things.

31:14.480 --> 31:17.480
 There is your sort of intrinsic motivation module,

31:17.480 --> 31:19.480
 which is in the basal ganglia, you know, the base of your brain.

31:19.480 --> 31:22.480
 That's the thing that measures pain and hunger and things like that.

31:22.480 --> 31:26.480
 Like immediate feelings and emotions.

31:26.480 --> 31:30.480
 And then there is, you know, the equivalent of what people

31:30.480 --> 31:34.480
 in Reference Metronomy call a critic, which is a sort of module

31:34.480 --> 31:41.480
 that predicts ahead what the outcome of a situation will be.

31:41.480 --> 31:44.480
 And so it's not a cost function, but it's sort of not an objective

31:44.480 --> 31:48.480
 function, but it's sort of a, you know, trained predictor

31:48.480 --> 31:50.480
 of the ultimate objective function.

31:50.480 --> 31:52.480
 And that also is differentiable.

31:52.480 --> 31:55.480
 And so if all of this is differentiable, your cost function,

31:55.480 --> 32:01.480
 your critic, your, you know, your world model, then you can use

32:01.480 --> 32:04.480
 gradient based type methods to do planning, to do reasoning,

32:04.480 --> 32:07.480
 to do learning, you know, to do all the things that would like

32:07.480 --> 32:11.480
 an intelligent agent to do.

32:11.480 --> 32:15.480
 And gradient based learning, like what's your intuition?

32:15.480 --> 32:18.480
 That's probably at the core of what can solve intelligence.

32:18.480 --> 32:25.480
 So you don't need like a logic based reasoning in your view.

32:25.480 --> 32:27.480
 I don't know how to make logic based reasoning compatible

32:27.480 --> 32:29.480
 with efficient learning.

32:29.480 --> 32:30.480
 Yeah.

32:30.480 --> 32:32.480
 And okay, I mean, there is a big question, perhaps a

32:32.480 --> 32:33.480
 philosophical question.

32:33.480 --> 32:37.480
 I mean, it's not that philosophical, but that we can ask is,

32:37.480 --> 32:40.480
 is that, you know, all the learning algorithms we know from

32:40.480 --> 32:44.480
 engineering and computer science proceed by optimizing

32:44.480 --> 32:46.480
 some objective function.

32:46.480 --> 32:47.480
 Yeah.

32:47.480 --> 32:52.480
 So one question we may ask is, is does learning in the brain

32:52.480 --> 32:54.480
 minimize an objective function?

32:54.480 --> 32:57.480
 I mean, it could be, you know, a composite of multiple

32:57.480 --> 33:00.480
 objective functions, but it's still an objective function.

33:00.480 --> 33:05.480
 Second, if it does optimize an objective function, does it do,

33:05.480 --> 33:09.480
 does it do it by some sort of gradient estimation?

33:09.480 --> 33:11.480
 You know, it doesn't need to be backprop, but, you know, some

33:11.480 --> 33:14.480
 way of estimating the gradient in efficient manner, whose

33:14.480 --> 33:17.480
 complexity is on the same order of magnitude as, you know,

33:17.480 --> 33:22.480
 actually running the inference.

33:22.480 --> 33:24.480
 Because you can't afford to do things like, you know,

33:24.480 --> 33:26.480
 perturbing a weight in your brain to figure out what the

33:26.480 --> 33:30.480
 effect is, and then sort of, you know, you can do sort of

33:30.480 --> 33:32.480
 estimating gradient by perturbation.

33:32.480 --> 33:36.480
 It's, to me, it seems very implausible that the brain uses

33:36.480 --> 33:40.480
 some sort of, you know, zeroth order, black box, gradient

33:40.480 --> 33:44.480
 free optimization, because it's so much less efficient than

33:44.480 --> 33:46.480
 gradient optimization.

33:46.480 --> 33:48.480
 So it has to have a way of estimating gradient.

33:48.480 --> 33:52.480
 Is it possible that some kind of logic based reasoning

33:52.480 --> 33:56.480
 emerges in pockets as a useful, like you said, if the brain

33:56.480 --> 33:58.480
 is an objective function?

33:58.480 --> 34:00.480
 Maybe it's a mechanism for creating objective functions.

34:00.480 --> 34:05.480
 It's a mechanism for creating knowledge bases, for example,

34:05.480 --> 34:07.480
 that can then be quarried.

34:07.480 --> 34:10.480
 Like maybe it's like an efficient representation of knowledge

34:10.480 --> 34:13.480
 that's learned in a gradient based way or something like that.

34:13.480 --> 34:15.480
 Well, so I think there is a lot of different types of

34:15.480 --> 34:17.480
 intelligence.

34:17.480 --> 34:19.480
 So first of all, I think the type of logical reasoning that

34:19.480 --> 34:23.480
 we think about, that we are, you know, maybe stemming from,

34:23.480 --> 34:28.480
 you know, sort of classical AI of the 1970s and 80s.

34:28.480 --> 34:32.480
 I think humans use that relatively rarely and are not

34:32.480 --> 34:34.480
 particularly good at it.

34:34.480 --> 34:38.480
 But we judge each other based on our ability to solve those

34:38.480 --> 34:40.480
 rare problems.

34:40.480 --> 34:41.480
 It's called IQ test.

34:41.480 --> 34:42.480
 I don't think so.

34:42.480 --> 34:44.480
 Like, I'm not very good at chess.

34:44.480 --> 34:48.480
 Yes, I'm judging you this whole time, because, well, we

34:48.480 --> 34:49.480
 actually...

34:49.480 --> 34:52.480
 With your, you know, heritage, I'm sure you're good at chess.

34:52.480 --> 34:54.480
 No, stereotypes.

34:54.480 --> 34:57.480
 Not all stereotypes are true.

34:57.480 --> 34:59.480
 Well, I'm terrible at chess.

34:59.480 --> 35:04.480
 You know, but I think perhaps another type of intelligence

35:04.480 --> 35:08.480
 that I have is this, you know, ability of sort of building

35:08.480 --> 35:13.480
 models to the world from, you know, reasoning, obviously,

35:13.480 --> 35:15.480
 but also data.

35:15.480 --> 35:18.480
 And those models generally are more kind of analogical, right?

35:18.480 --> 35:24.480
 So it's reasoning by simulation and by analogy, where you use

35:24.480 --> 35:27.480
 one model to apply to a new situation, even though you've

35:27.480 --> 35:31.480
 seen that situation, you can sort of connect it to a situation

35:31.480 --> 35:33.480
 you've encountered before.

35:33.480 --> 35:37.480
 And your reasoning is more, you know, akin to some sort of

35:37.480 --> 35:38.480
 internal simulation.

35:38.480 --> 35:41.480
 So you're kind of simulating what's happening when you're

35:41.480 --> 35:43.480
 building, I don't know, a box out of wood or something, right?

35:43.480 --> 35:47.480
 You can imagine in advance, like, will we be the result of,

35:47.480 --> 35:49.480
 you know, cutting the wood in this particular way?

35:49.480 --> 35:52.480
 Are you going to use, you know, screws on nails or whatever?

35:52.480 --> 35:55.480
 When you're interacting with someone, you also have a model

35:55.480 --> 36:00.480
 in mind to kind of tell the person what you think is useful

36:00.480 --> 36:01.480
 to them.

36:01.480 --> 36:06.480
 So I think this ability to construct models to the world is

36:06.480 --> 36:10.480
 basically the essence of intelligence, and the ability

36:10.480 --> 36:17.480
 to use it then to plan actions that will fulfill a particular

36:17.480 --> 36:21.480
 criterion, of course, is necessary as well.

36:21.480 --> 36:26.480
 So I'm going to ask you a series of impossible questions as we

36:26.480 --> 36:29.480
 keep asking, has that been doing?

36:29.480 --> 36:32.480
 So if that's the fundamental sort of dark matter of

36:32.480 --> 36:36.480
 intelligence, this ability to form a background model, what's

36:36.480 --> 36:40.480
 your intuition about how much knowledge is required?

36:40.480 --> 36:45.480
 You know, I think dark matter, you can put a percentage on it

36:45.480 --> 36:50.480
 of the composition of the universe and how much of it is dark

36:50.480 --> 36:55.480
 matter, how much of it is dark energy, how much information

36:55.480 --> 36:59.480
 do you think is required to be a house cat?

36:59.480 --> 37:02.480
 So you have to be able to, when you see a box going, when you

37:02.480 --> 37:06.480
 see a human compute the most evil action, if there's a thing

37:06.480 --> 37:10.480
 that's near an edge, you knock it off, all of that.

37:10.480 --> 37:13.480
 Plus the extra stuff you mentioned, which is a great

37:13.480 --> 37:18.480
 self awareness of the physics of your own body and the world.

37:18.480 --> 37:21.480
 How much knowledge is required, do you think, to solve it?

37:21.480 --> 37:25.480
 I don't even know how to measure an answer to that question.

37:25.480 --> 37:27.480
 I'm not sure how to measure it, but whatever it is, it fits

37:27.480 --> 37:33.480
 in about 800,000 neurons, 800 million neurons.

37:33.480 --> 37:35.480
 The representation does.

37:35.480 --> 37:39.480
 Everything, all knowledge, everything, right?

37:39.480 --> 37:41.480
 There's less than a billion.

37:41.480 --> 37:45.480
 A dog is 2 billion, but a cat is less than 1 billion.

37:45.480 --> 37:49.480
 And so multiply that by 1000 and you get the number of synapses.

37:49.480 --> 37:54.480
 And I think almost all of it is learned through a sort of

37:54.480 --> 37:56.480
 self supervised running.

37:56.480 --> 37:59.480
 Although I think a tiny sliver is learned through reinforcement

37:59.480 --> 38:02.480
 running and certainly very little through classical

38:02.480 --> 38:04.480
 supervised running, although it's not even clear how

38:04.480 --> 38:08.480
 supervised running actually works in the biological world.

38:08.480 --> 38:12.480
 So I think almost all of it is self supervised running.

38:12.480 --> 38:17.480
 But it's driven by the sort of ingrained objective functions

38:17.480 --> 38:21.480
 that a cat or human have at the base of their brain,

38:21.480 --> 38:24.480
 which kind of drives their behavior.

38:24.480 --> 38:29.480
 So, you know, nature tells us, you're hungry.

38:29.480 --> 38:31.480
 It doesn't tell us how to feed ourselves.

38:31.480 --> 38:35.480
 That's something that the rest of our brain has to figure out, right?

38:35.480 --> 38:38.480
 What's interesting is there might be more like

38:38.480 --> 38:41.480
 deeper objective functions than allowing the whole thing.

38:41.480 --> 38:44.480
 So hunger may be some kind of...

38:44.480 --> 38:46.480
 Now you go to like neurobiology.

38:46.480 --> 38:52.480
 It might be just the brain trying to maintain homeostasis.

38:52.480 --> 38:58.480
 So hunger is just one of the human perceivable symptoms

38:58.480 --> 39:01.480
 of the brain being unhappy with the way things are currently.

39:01.480 --> 39:05.480
 It could be just like one really dumb objective function at the core.

39:05.480 --> 39:08.480
 But that's how behavior is driven.

39:08.480 --> 39:12.480
 The fact that, you know, the Orbezo Ganglia

39:12.480 --> 39:16.480
 drive us to do things that are different from, say, an orangutan

39:16.480 --> 39:20.480
 or certainly a cat is what makes, you know, human nature

39:20.480 --> 39:23.480
 versus orangutan nature versus cat nature.

39:23.480 --> 39:27.480
 So for example, you know, our Bezo Ganglia

39:27.480 --> 39:32.480
 drives us to seek the company of other humans.

39:32.480 --> 39:35.480
 And that's because nature has figured out that we need to be

39:35.480 --> 39:38.480
 a social animal for our species to survive.

39:38.480 --> 39:41.480
 And it's true of many primates.

39:41.480 --> 39:43.480
 It's not true of orangutans.

39:43.480 --> 39:45.480
 Orangutans are solitary animals.

39:45.480 --> 39:47.480
 They don't seek the company of others.

39:47.480 --> 39:49.480
 In fact, they avoid them.

39:49.480 --> 39:51.480
 In fact, they scream at them when they come too close

39:51.480 --> 39:53.480
 because they're territorial.

39:53.480 --> 39:57.480
 Because for their survival, you know, evolution has figured out

39:57.480 --> 39:59.480
 that's the best thing.

39:59.480 --> 40:02.480
 I mean, they're occasionally social, of course, for, you know,

40:02.480 --> 40:06.480
 reproduction and stuff like that, but they're mostly solitary.

40:06.480 --> 40:09.480
 So all of those behaviors are not part of intelligence.

40:09.480 --> 40:11.480
 You know, people say, oh, you're never going to have intelligent

40:11.480 --> 40:13.480
 machines because, you know, human intelligence is social.

40:13.480 --> 40:16.480
 But then you look at orangutans, you look at octopus.

40:16.480 --> 40:18.480
 Octopus never know their parents.

40:18.480 --> 40:20.480
 They barely interact with any other.

40:20.480 --> 40:23.480
 And they get to be really smart in less than a year

40:23.480 --> 40:25.480
 in like half a year.

40:25.480 --> 40:28.480
 You know, in a year of their adults, in two years they're dead.

40:28.480 --> 40:34.480
 So there are things that we think as humans are intimately linked

40:34.480 --> 40:39.480
 with intelligence, like social interaction, like language.

40:39.480 --> 40:43.480
 We think, I think we give way too much importance to language

40:43.480 --> 40:46.480
 as a substrate of intelligence as humans.

40:46.480 --> 40:49.480
 Because we think our reasoning is so linked with language.

40:49.480 --> 40:53.480
 So for, to solve the house cat intelligence problem,

40:53.480 --> 40:55.480
 you think you could do it on a desert island.

40:55.480 --> 41:01.480
 You could have a cat sitting there looking at the waves,

41:01.480 --> 41:05.480
 at the ocean waves and figure a lot of it out.

41:05.480 --> 41:09.480
 It needs to have sort of, you know, the right set of drives

41:09.480 --> 41:12.480
 to kind of, you know, get it to do the thing

41:12.480 --> 41:14.480
 and learn the appropriate things, right?

41:14.480 --> 41:19.480
 But like, for example, you know, baby humans are driven

41:19.480 --> 41:22.480
 to learn to stand up and walk.

41:22.480 --> 41:25.480
 Okay, that's kind of, this desire is hardwired.

41:25.480 --> 41:28.480
 How to do it precisely is not, that's learned.

41:28.480 --> 41:32.480
 But the desire to walk, move around and stand up,

41:32.480 --> 41:35.480
 that's sort of hardwired.

41:35.480 --> 41:38.480
 It's very simple to hardwire this kind of stuff.

41:38.480 --> 41:42.480
 Oh, like the desire to, well, that's interesting.

41:42.480 --> 41:45.480
 You're hardwired to want to walk.

41:45.480 --> 41:50.480
 That's not a, there's got to be a deeper need for walking.

41:50.480 --> 41:53.480
 I think it was probably socially imposed by society

41:53.480 --> 41:55.480
 that you need to walk all the other bipedal.

41:55.480 --> 41:58.480
 No, like a lot of simple animals that, you know,

41:58.480 --> 42:02.480
 would probably walk without ever watching any other members

42:02.480 --> 42:03.480
 of the species.

42:03.480 --> 42:06.480
 It seems like a scary thing to have to do

42:06.480 --> 42:09.480
 because you suck at bipedal walking at first.

42:09.480 --> 42:13.480
 It seems crawling is much safer, much more like,

42:13.480 --> 42:15.480
 why are you in a hurry?

42:15.480 --> 42:18.480
 Well, because you have this thing that drives you to do it,

42:18.480 --> 42:24.480
 you know, which is sort of part of the sort of human development.

42:24.480 --> 42:26.480
 Is that understood actually what?

42:26.480 --> 42:27.480
 Not entirely.

42:27.480 --> 42:28.480
 No.

42:28.480 --> 42:29.480
 What's the reason you get on two feet?

42:29.480 --> 42:30.480
 It's really hard.

42:30.480 --> 42:32.480
 Like most animals don't get on two feet.

42:32.480 --> 42:33.480
 Well, they get on four feet.

42:33.480 --> 42:35.480
 You know, many mammals get on four feet.

42:35.480 --> 42:36.480
 Yeah, they do.

42:36.480 --> 42:37.480
 Very quickly.

42:37.480 --> 42:38.480
 Some of them extremely quickly.

42:38.480 --> 42:41.480
 But I don't, you know, like from the last time I've interacted

42:41.480 --> 42:43.480
 with a table, that's much more stable than a thing

42:43.480 --> 42:44.480
 than two legs.

42:44.480 --> 42:46.480
 It's just a really hard problem.

42:46.480 --> 42:49.480
 Yeah, I mean birds have figured it out with two feet.

42:49.480 --> 42:52.480
 Well, technically, we can go into ontology.

42:52.480 --> 42:53.480
 They have four.

42:53.480 --> 42:54.480
 I guess they have two feet.

42:54.480 --> 42:55.480
 They have two feet.

42:55.480 --> 42:56.480
 Chickens.

42:56.480 --> 42:58.480
 You know, dinosaurs have two feet.

42:58.480 --> 42:59.480
 Many of them.

42:59.480 --> 43:01.480
 Allegedly.

43:01.480 --> 43:04.480
 I'm just now learning that T. rex was eating grass,

43:04.480 --> 43:05.480
 not other animals.

43:05.480 --> 43:08.480
 T. rex might have been a friendly pet.

43:08.480 --> 43:13.480
 What do you think about, I don't know if you looked at the test

43:13.480 --> 43:15.480
 for general intelligence that Francois Chollet put together?

43:15.480 --> 43:18.480
 I don't know if you got a chance to look at that kind of thing.

43:18.480 --> 43:23.480
 What's your intuition about how to solve like an IQ type of test?

43:23.480 --> 43:24.480
 I don't know.

43:24.480 --> 43:28.480
 I think it's so outside of my radar screen that it's not really relevant,

43:28.480 --> 43:30.480
 I think, in the short term.

43:30.480 --> 43:36.480
 Well, I guess one way to ask another way, perhaps more closer to what do

43:36.480 --> 43:42.480
 your work is like, how do you solve MNIST with very little example data?

43:42.480 --> 43:43.480
 That's right.

43:43.480 --> 43:45.480
 The answer to this probably is self supervised running.

43:45.480 --> 43:49.480
 Just learn to represent images and then learning to recognize

43:49.480 --> 43:53.480
 handwritten digits on top of this will only require a few samples.

43:53.480 --> 43:55.480
 We observe this in humans, right?

43:55.480 --> 44:00.480
 You show a young child a picture book with a couple of pictures of an elephant

44:00.480 --> 44:01.480
 and that's it.

44:01.480 --> 44:03.480
 The child knows what an elephant is.

44:03.480 --> 44:08.480
 We see this today with practical systems that we train image recognition

44:08.480 --> 44:14.480
 systems with enormous amounts of images, either completely self

44:14.480 --> 44:16.480
 supervised or very weakly supervised.

44:16.480 --> 44:21.480
 For example, you can train a neural net to predict whatever hashtag

44:21.480 --> 44:23.480
 people type on Instagram, right?

44:23.480 --> 44:25.480
 Then you can do this with billions of images because there's billions

44:25.480 --> 44:27.480
 per day that are showing up.

44:27.480 --> 44:31.480
 So the amount of training data there is essentially unlimited.

44:31.480 --> 44:35.480
 And then you take the output representation, a couple of layers

44:35.480 --> 44:40.480
 down from the output of what the system learned and feed this as input

44:40.480 --> 44:43.480
 to a classifier for any object in the world that you want.

44:43.480 --> 44:44.480
 And it works pretty well.

44:44.480 --> 44:50.480
 So that's transfer learning or weakly supervised transfer learning.

44:50.480 --> 44:54.480
 People are making very, very fast progress using self supervised

44:54.480 --> 44:57.480
 running with this kind of scenario as well.

44:57.480 --> 45:02.480
 And my guess is that that's going to be the future.

45:02.480 --> 45:06.480
 For self supervised learning, how much cleaning do you think is needed

45:06.480 --> 45:12.480
 for filtering malicious signal or what's the better term?

45:12.480 --> 45:19.480
 But like a lot of people use hashtags on Instagram to get like good SEO

45:19.480 --> 45:22.480
 that doesn't fully represent the contents of the image.

45:22.480 --> 45:26.480
 Like they'll put a picture of a cat and hashtag it with like science,

45:26.480 --> 45:27.480
 awesome, fun.

45:27.480 --> 45:28.480
 I don't know.

45:28.480 --> 45:30.480
 Why would you put science?

45:30.480 --> 45:32.480
 That's not very good SEO.

45:32.480 --> 45:36.480
 The way my colleagues who worked on this project at Facebook now,

45:36.480 --> 45:41.480
 META, META AI, a few years ago, dealt with this is that they only

45:41.480 --> 45:44.480
 selected something like 17,000 tags that correspond to kind of

45:44.480 --> 45:47.480
 physical things or situations.

45:47.480 --> 45:51.480
 Like, you know, that has some visual content.

45:51.480 --> 45:56.480
 So, you know, you wouldn't have like hash TBT or anything like that.

45:56.480 --> 46:00.480
 Also, they keep a very select set of hashtags.

46:00.480 --> 46:01.480
 Is that what you're saying?

46:01.480 --> 46:02.480
 Yeah.

46:02.480 --> 46:03.480
 Okay.

46:03.480 --> 46:06.480
 But it's still on the order of, you know, 10 to 20,000.

46:06.480 --> 46:08.480
 So it's fairly large.

46:08.480 --> 46:09.480
 Okay.

46:09.480 --> 46:11.480
 Can you tell me about data augmentation?

46:11.480 --> 46:14.480
 What the heck is data augmentation and how is it used?

46:14.480 --> 46:18.480
 Maybe contrast of learning for video?

46:18.480 --> 46:20.480
 What are some cool ideas here?

46:20.480 --> 46:21.480
 Right.

46:21.480 --> 46:23.480
 So data augmentation, I mean, first data augmentation, you know,

46:23.480 --> 46:26.480
 is the idea of artificially increasing the size of your training set

46:26.480 --> 46:30.480
 by distorting the images that you have in ways that don't change

46:30.480 --> 46:31.480
 the nature of the image.

46:31.480 --> 46:32.480
 Right.

46:32.480 --> 46:35.480
 So you take, you do MNIST, you can do data augmentation on MNIST.

46:35.480 --> 46:37.480
 And people have done this since the 1990s, right?

46:37.480 --> 46:41.480
 You take a MNIST digit and you shift it a little bit or you change

46:41.480 --> 46:46.480
 the size or rotate it, skew it, you know, et cetera.

46:46.480 --> 46:47.480
 Add noise.

46:47.480 --> 46:49.480
 Add noise, et cetera.

46:49.480 --> 46:52.480
 And it works better if you train a supervised classifier with

46:52.480 --> 46:53.480
 augmented data.

46:53.480 --> 46:55.480
 You're going to get better results.

46:55.480 --> 47:00.480
 Now it's become really interesting over the last couple of years

47:00.480 --> 47:05.480
 because a lot of self supervised learning techniques to pre train

47:05.480 --> 47:07.480
 vision systems are based on data augmentation.

47:07.480 --> 47:13.480
 And the basic techniques is originally inspired by techniques

47:13.480 --> 47:16.480
 that I worked on in the early 90s and Jeff Newton worked on

47:16.480 --> 47:17.480
 also in the early 90s.

47:17.480 --> 47:19.480
 There was sort of parallel work.

47:19.480 --> 47:21.480
 I used to call this same is network.

47:21.480 --> 47:25.480
 So basically you take two identical copies of the same network.

47:25.480 --> 47:30.480
 They share the same weights and you show two different views of

47:30.480 --> 47:31.480
 the same object.

47:31.480 --> 47:33.480
 Either those two different views may have been obtained by

47:33.480 --> 47:36.480
 data augmentation or maybe it's two different views of the same

47:36.480 --> 47:39.480
 scene from a camera that you moved or at different times or

47:39.480 --> 47:41.480
 something like that, right?

47:41.480 --> 47:43.480
 Or two pictures of the same person, things like that.

47:43.480 --> 47:47.480
 And then you train this neural net, those two identical copies

47:47.480 --> 47:51.480
 of this neural net to produce an output representation, a vector

47:51.480 --> 47:55.480
 in such a way that the representation for those two images

47:55.480 --> 47:59.480
 are as close to each other as possible, as identical to each

47:59.480 --> 48:00.480
 other as possible, right?

48:00.480 --> 48:04.480
 Because you want the system to basically learn a function that

48:04.480 --> 48:07.480
 will be invariant that will not change, whose output will not

48:07.480 --> 48:11.480
 change when you transform those inputs in those particular

48:11.480 --> 48:13.480
 ways, right?

48:13.480 --> 48:15.480
 So that's easy to do.

48:15.480 --> 48:18.480
 What's complicated is how do you make sure that when you show

48:18.480 --> 48:20.480
 two images that are different, the system will produce different

48:20.480 --> 48:21.480
 things?

48:21.480 --> 48:26.480
 Because if you don't have a specific provision for this, the

48:26.480 --> 48:28.480
 system will just ignore the inputs.

48:28.480 --> 48:30.480
 When you train it, it will end up ignoring the input and just

48:30.480 --> 48:33.480
 produce a constant vector that is the same for every input,

48:33.480 --> 48:34.480
 right?

48:34.480 --> 48:35.480
 That's called a collapse.

48:35.480 --> 48:36.480
 Now, how do you avoid collapse?

48:36.480 --> 48:38.480
 So there's two ideas.

48:38.480 --> 48:42.480
 One idea that I proposed in the early 90s with my colleagues at

48:42.480 --> 48:47.480
 the lab, Jane Bromley and a couple other people, which we now

48:47.480 --> 48:50.480
 call contrastive learning, which is to have negative examples,

48:50.480 --> 48:51.480
 right?

48:51.480 --> 48:54.480
 So you have pairs of images that you know are different.

48:54.480 --> 48:58.480
 And you show them to the network and those two copies, and then

48:58.480 --> 49:01.480
 you push the two output vectors away from each other.

49:01.480 --> 49:04.480
 And it will eventually guarantee that things that are

49:04.480 --> 49:07.480
 symmetrically similar produce similar representations and

49:07.480 --> 49:10.480
 things that are different produce different representations.

49:10.480 --> 49:13.480
 So we actually came up with this idea for a project of doing

49:13.480 --> 49:14.480
 signature verification.

49:14.480 --> 49:19.480
 So we would collect signatures from like multiple signatures on

49:19.480 --> 49:22.480
 the same person and then train a neural net to produce the same

49:22.480 --> 49:23.480
 representation.

49:23.480 --> 49:27.480
 And then, you know, force the system to produce different

49:27.480 --> 49:30.480
 representation from different signatures.

49:30.480 --> 49:34.480
 This was actually the problem was proposed by people from what

49:34.480 --> 49:38.480
 was a subsidiary of AT&T at the time called NCR.

49:38.480 --> 49:41.480
 They were interested in storing representation of the signature

49:41.480 --> 49:46.480
 on the 80 bytes of the magnetic strip of a credit card.

49:46.480 --> 49:49.480
 So we came up with this idea of having a neural net with 80

49:49.480 --> 49:52.480
 outputs, you know, that we would quantize on bytes so that we

49:52.480 --> 49:54.480
 could encode the signature.

49:54.480 --> 49:56.480
 And that encoding was then used to compare whether the

49:56.480 --> 49:57.480
 signature matches or not.

49:57.480 --> 49:58.480
 That's right.

49:58.480 --> 50:00.480
 So then you would, you know, sign, it would run through the

50:00.480 --> 50:02.480
 neural net and then you would compare the output vector to

50:02.480 --> 50:03.480
 whatever is stored on your card.

50:03.480 --> 50:04.480
 Did it actually work?

50:04.480 --> 50:06.480
 It worked, but they ended up not using it.

50:06.480 --> 50:09.480
 Because nobody cares actually.

50:09.480 --> 50:13.480
 I mean, the American, you know, financial payment system is

50:13.480 --> 50:17.480
 incredibly lax in that respect compared to Europe.

50:17.480 --> 50:18.480
 Oh, the signatures.

50:18.480 --> 50:20.480
 What's the purpose of signatures anyway?

50:20.480 --> 50:21.480
 Nobody looks at them.

50:21.480 --> 50:22.480
 Nobody cares.

50:22.480 --> 50:23.480
 Yeah.

50:23.480 --> 50:27.480
 So that's contrastive learning, right?

50:27.480 --> 50:29.480
 So you need positive and negative pairs.

50:29.480 --> 50:32.480
 And the problem with that is that, you know, even though I had

50:32.480 --> 50:35.480
 the original paper on this, I actually not very positive about

50:35.480 --> 50:38.480
 it because it doesn't work in high dimension.

50:38.480 --> 50:41.480
 If your representation is high dimensional, there's just too

50:41.480 --> 50:43.480
 many ways for two things to be different.

50:43.480 --> 50:47.480
 And so you would need lots and lots and lots of negative pairs.

50:47.480 --> 50:50.480
 So there is a particular implementation of this, which is

50:50.480 --> 50:54.480
 relatively recent from actually the Google Toronto group,

50:54.480 --> 50:58.480
 where, you know, Jeff Hinton is the senior member there.

50:58.480 --> 51:01.480
 It's called SimClear, SIN, CLR.

51:01.480 --> 51:04.480
 And, you know, basically a particular way of implementing this

51:04.480 --> 51:08.480
 idea of contrastive learning is a particular objective function.

51:08.480 --> 51:12.480
 Now, what I'm much more enthusiastic about these days is

51:12.480 --> 51:14.480
 non contrastive methods.

51:14.480 --> 51:19.480
 So other ways to guarantee that the representations would be

51:19.480 --> 51:23.480
 different for different inputs.

51:23.480 --> 51:28.480
 And it's actually based on an idea that Jeff Hinton proposed

51:28.480 --> 51:31.480
 in the early 90s with his student at the time, Sue Becker.

51:31.480 --> 51:33.480
 And it's based on the idea of maximizing the mutual

51:33.480 --> 51:35.480
 information between the outputs of the two systems.

51:35.480 --> 51:37.480
 You only show positive pairs.

51:37.480 --> 51:41.480
 You only show pairs of images that you know are somewhat similar.

51:41.480 --> 51:44.480
 And you train the two networks to be informative,

51:44.480 --> 51:49.480
 but also to be as informative of each other as possible.

51:49.480 --> 51:51.480
 So basically one representation has to be predictable

51:51.480 --> 51:54.480
 from the other, essentially.

51:54.480 --> 51:58.480
 And, you know, he proposed that idea had, you know,

51:58.480 --> 52:00.480
 a couple of papers in the early 90s, and then nothing was

52:00.480 --> 52:03.480
 done about it for decades.

52:03.480 --> 52:05.480
 And I kind of revived this idea together with my postdocs

52:05.480 --> 52:09.480
 at FAIR, particularly a postdoc called Steph Anthony,

52:09.480 --> 52:12.480
 who is now a junior professor in Finland at University of

52:12.480 --> 52:14.480
 Alto.

52:14.480 --> 52:18.480
 We came up with something called, that we call Balu twins.

52:18.480 --> 52:22.480
 And it's a particular way of maximizing the information content

52:22.480 --> 52:28.480
 of a vector, you know, using some hypothesis.

52:28.480 --> 52:32.480
 And we have kind of another version of it that's more recent

52:32.480 --> 52:34.480
 now called VICRAG, V I C A R E G.

52:34.480 --> 52:37.480
 That means variance invariance covariance regularization.

52:37.480 --> 52:40.480
 And it's the thing I'm the most excited about in machine

52:40.480 --> 52:41.480
 learning in the last 15 years.

52:41.480 --> 52:44.480
 I mean, I'm not, I'm really, really excited about this.

52:44.480 --> 52:47.480
 What kind of data augmentation is useful for that

52:47.480 --> 52:49.480
 noncontrasting learning method?

52:49.480 --> 52:52.480
 Are we talking about, does that not matter that much?

52:52.480 --> 52:55.480
 Or it seems like a very important part of the step.

52:55.480 --> 52:56.480
 Yeah.

52:56.480 --> 52:58.480
 Do you generate the images that are similar but sufficiently

52:58.480 --> 52:59.480
 different?

52:59.480 --> 53:00.480
 Yeah, that's right.

53:00.480 --> 53:01.480
 It's an important step.

53:01.480 --> 53:03.480
 And it's also an annoying step because you need to have that

53:03.480 --> 53:07.480
 knowledge of what the documentation you can do that do not

53:07.480 --> 53:10.480
 change the nature of the, of the object.

53:10.480 --> 53:14.480
 And so the standard scenario, which, you know, a lot of people

53:14.480 --> 53:19.480
 working in this area are using is you use the type of distortion.

53:19.480 --> 53:22.480
 So, so basically you do a geometric distortion.

53:22.480 --> 53:24.480
 So one basically just shifts the image a little bit.

53:24.480 --> 53:25.480
 It's called cropping.

53:25.480 --> 53:27.480
 Another one kind of changes the scale a little bit.

53:27.480 --> 53:29.480
 Another one kind of rotates it.

53:29.480 --> 53:30.480
 Another one changes the colors.

53:30.480 --> 53:33.480
 You know, you can do a shift in color balance or something like

53:33.480 --> 53:34.480
 that.

53:34.480 --> 53:35.480
 Saturation.

53:35.480 --> 53:37.480
 Another one sort of blurs it.

53:37.480 --> 53:38.480
 Another one has noise.

53:38.480 --> 53:41.480
 So you have like a catalog of kind of standard things and people

53:41.480 --> 53:44.480
 try to use the same ones for different algorithms so that they

53:44.480 --> 53:45.480
 can compare.

53:45.480 --> 53:49.480
 But some algorithms, some surface algorithm actually can deal

53:49.480 --> 53:52.480
 with much bigger, like more aggressive data augmentation and

53:52.480 --> 53:53.480
 some don't.

53:53.480 --> 53:56.480
 So that kind of makes the whole thing difficult.

53:56.480 --> 53:58.480
 But that's the kind of distortions we're talking about.

53:58.480 --> 54:05.480
 And so you train with those distortions and then you chop

54:05.480 --> 54:11.480
 off the last layer, a couple layers of the network and you use

54:11.480 --> 54:13.480
 the representation as input to a classifier.

54:13.480 --> 54:18.480
 You train the classifier on ImageNet, let's say, or whatever

54:18.480 --> 54:20.480
 and measure the performance.

54:20.480 --> 54:24.480
 And interestingly enough, the methods that are really good at

54:24.480 --> 54:27.480
 eliminating the information that is irrelevant, which is the

54:27.480 --> 54:31.480
 distortions between those images, do a good job at eliminating

54:31.480 --> 54:32.480
 it.

54:32.480 --> 54:36.480
 And as a consequence, you cannot use the representations in

54:36.480 --> 54:39.480
 those systems for things like object detection and localization

54:39.480 --> 54:42.480
 because that information is gone.

54:42.480 --> 54:45.480
 So the type of data augmentation you need to do depends on the

54:45.480 --> 54:48.480
 tasks you want eventually the system to solve.

54:48.480 --> 54:51.480
 And the type of data augmentation, standard data augmentation

54:51.480 --> 54:54.480
 that we use today are only appropriate for object recognition

54:54.480 --> 54:55.480
 or image classification.

54:55.480 --> 54:57.480
 They're not appropriate for things like.

54:57.480 --> 55:00.480
 Can you help me out understand why the localization?

55:00.480 --> 55:04.480
 So you're saying it's just not good at the negative, like at

55:04.480 --> 55:05.480
 classifying the negative.

55:05.480 --> 55:07.480
 So that's why it can't be used for the localization.

55:07.480 --> 55:11.480
 No, it's just that you train the system, you know, you give

55:11.480 --> 55:14.480
 it an image and then you give it the same image shifted and

55:14.480 --> 55:16.480
 scaled and you tell it that's the same image.

55:16.480 --> 55:19.480
 So the system basically is trained to eliminate the

55:19.480 --> 55:21.480
 information about position and size.

55:21.480 --> 55:24.480
 So now, and now you want to use that.

55:24.480 --> 55:27.480
 Oh, like where an object is and what size.

55:27.480 --> 55:28.480
 It's like a bounding box.

55:28.480 --> 55:29.480
 They'd be able to actually.

55:29.480 --> 55:30.480
 Okay.

55:30.480 --> 55:33.480
 It can still find the object in the image.

55:33.480 --> 55:36.480
 It's just not very good at finding the exact boundaries of

55:36.480 --> 55:37.480
 that object.

55:37.480 --> 55:38.480
 Interesting.

55:38.480 --> 55:39.480
 Interesting.

55:39.480 --> 55:42.480
 Which, you know, that's an interesting sort of philosophical

55:42.480 --> 55:43.480
 question.

55:43.480 --> 55:46.480
 How important is object localization anyway?

55:46.480 --> 55:50.480
 We're like obsessed by measuring like image segmentation.

55:50.480 --> 55:53.480
 Obsessed by measuring perfectly knowing the boundaries of

55:53.480 --> 56:01.480
 objects when arguably that's not that essential to understanding

56:01.480 --> 56:03.480
 what are the contents of the scene.

56:03.480 --> 56:06.480
 On the other hand, I think evolutionarily, the first vision

56:06.480 --> 56:09.480
 systems in animals were basically all about localization,

56:09.480 --> 56:11.480
 very little about recognition.

56:11.480 --> 56:15.480
 And in the human brain, you have two separate pathways for

56:15.480 --> 56:21.480
 recognizing the nature of a scene or an object and localizing

56:21.480 --> 56:22.480
 objects.

56:22.480 --> 56:25.480
 So you use the first pathway called a ventral pathway for,

56:25.480 --> 56:28.480
 you know, telling what you're looking at.

56:28.480 --> 56:31.480
 The other path with the dorsal pathway is used for navigation,

56:31.480 --> 56:33.480
 for grasping, for everything else.

56:33.480 --> 56:35.480
 And, you know, basically a lot of the things you need for

56:35.480 --> 56:40.480
 survival are localization and detection.

56:40.480 --> 56:45.480
 Is similarity learning or contrastive learning or these

56:45.480 --> 56:48.480
 noncontrastive methods the same as understanding something?

56:48.480 --> 56:51.480
 Just because, you know, a distorted cat is the same as a

56:51.480 --> 56:52.480
 non distorted cat.

56:52.480 --> 56:56.480
 Does that mean you understand what it means to be a cat?

56:56.480 --> 56:57.480
 To some extent.

56:57.480 --> 56:59.480
 I mean, it's a superficial understanding, obviously.

56:59.480 --> 57:02.480
 But like what is the ceiling of this method, do you think?

57:02.480 --> 57:06.480
 Is this just one trick on the path to doing self

57:06.480 --> 57:07.480
 supervised learning?

57:07.480 --> 57:09.480
 Can we go really, really far?

57:09.480 --> 57:11.480
 I think we can go really far.

57:11.480 --> 57:16.480
 So if we figure out how to use techniques of that type, perhaps

57:16.480 --> 57:20.480
 very different, but, you know, of the same nature to train a

57:20.480 --> 57:25.480
 system from video to do video prediction, essentially, I think

57:25.480 --> 57:30.480
 we'll have a path, you know, towards, you know, I wouldn't

57:30.480 --> 57:35.480
 say unlimited, but a path towards some level of, you know,

57:35.480 --> 57:37.480
 physical common sense in machines.

57:37.480 --> 57:44.480
 And I also think that that ability to learn how the world

57:44.480 --> 57:48.480
 works from a sort of high throughput channel like vision

57:48.480 --> 57:55.480
 is a necessary step towards sort of real artificial intelligence.

57:55.480 --> 57:57.480
 In other words, I believe in grounded intelligence.

57:57.480 --> 58:00.480
 I don't think we can train a machine to be intelligent purely

58:00.480 --> 58:04.480
 from text, because I think the amount of information about the

58:04.480 --> 58:09.480
 world that's contained in text is tiny compared to what we need

58:09.480 --> 58:10.480
 to know.

58:10.480 --> 58:14.480
 So for example, let's, and I know people have attempted to do

58:14.480 --> 58:16.480
 this for 30 years, right?

58:16.480 --> 58:18.480
 The site project and things like that, right?

58:18.480 --> 58:21.480
 Of basically kind of writing down all the facts that are known

58:21.480 --> 58:24.480
 and hoping that some sort of common sense will emerge.

58:24.480 --> 58:26.480
 I think it's basically hopeless.

58:26.480 --> 58:28.480
 But let me take an example.

58:28.480 --> 58:29.480
 You take an object.

58:29.480 --> 58:31.480
 I describe a situation to you.

58:31.480 --> 58:32.480
 I take an object.

58:32.480 --> 58:34.480
 I put it on the table and I push the table.

58:34.480 --> 58:37.480
 It's completely obvious to you that the object will be pushed

58:37.480 --> 58:39.480
 with the table, right?

58:39.480 --> 58:41.480
 Because it's sitting on it.

58:41.480 --> 58:44.480
 There's no text in the world, I believe, that explains this.

58:44.480 --> 58:48.480
 And so if you train a machine as powerful as it could be,

58:48.480 --> 58:53.480
 you know, your GPT 5000 or whatever it is,

58:53.480 --> 58:56.480
 it's never going to learn about this.

58:56.480 --> 59:00.480
 That information is just not present in any text.

59:00.480 --> 59:03.480
 Well, the question, like with the site project,

59:03.480 --> 59:10.480
 the dream I think is to have like 10 million, say, facts like that

59:10.480 --> 59:15.480
 that give you a head start, like a parent guiding you.

59:15.480 --> 59:19.480
 Now we humans don't need a parent to tell us that the table will move.

59:19.480 --> 59:21.480
 Sorry, the smartphone will move with the table.

59:21.480 --> 59:25.480
 But we get a lot of guidance in other ways.

59:25.480 --> 59:28.480
 So it's possible that we can give it a quick shortcut.

59:28.480 --> 59:30.480
 What about cat? The cat knows that.

59:30.480 --> 59:32.480
 No, but they evolved.

59:32.480 --> 59:35.480
 No, they learned like us.

59:35.480 --> 59:38.480
 Sorry, the physics of stuff.

59:38.480 --> 59:42.480
 Well, yeah, so you're saying it's,

59:42.480 --> 59:47.480
 you're putting a lot of intelligence onto the nurture side, not the nature.

59:47.480 --> 59:51.480
 We seem to have, you know, there's a very inefficient,

59:51.480 --> 59:57.480
 arguably, process of evolution that got us from bacteria to who we are today.

59:57.480 --> 59:59.480
 Started at the bottom. Now we're here.

59:59.480 --> 1:00:03.480
 So the question is how, okay.

1:00:03.480 --> 1:00:08.480
 So the question is how fundamental is that the nature of the whole hardware?

1:00:08.480 --> 1:00:12.480
 And then is there any way to shortcut it if it's fundamental?

1:00:12.480 --> 1:00:15.480
 If it's not, if it's most of intelligence, most of the cool stuff we've been talking about

1:00:15.480 --> 1:00:18.480
 is mostly nurture, mostly trained.

1:00:18.480 --> 1:00:20.480
 We figured out by observing the world.

1:00:20.480 --> 1:00:24.480
 We can form that big, beautiful, sexy background model

1:00:24.480 --> 1:00:28.480
 that you're talking about just by sitting there.

1:00:28.480 --> 1:00:34.480
 Then, okay, then you need to, then like maybe,

1:00:34.480 --> 1:00:37.480
 it is all supervised learning all the way down.

1:00:37.480 --> 1:00:39.480
 Self supervised learning, say.

1:00:39.480 --> 1:00:43.480
 Whatever it is that makes, you know, human intelligence different from other animals,

1:00:43.480 --> 1:00:48.480
 which, you know, a lot of people think is language and logical reasoning and this kind of stuff.

1:00:48.480 --> 1:00:52.480
 It cannot be that complicated because it only popped up in the last million years.

1:00:52.480 --> 1:00:59.480
 And, you know, it only involves, you know, less than 1% of a genome, right?

1:00:59.480 --> 1:01:03.480
 Which is the difference between human genome and chimps or whatever.

1:01:03.480 --> 1:01:07.480
 So it can be that complicated, you know, it can be that fundamental.

1:01:07.480 --> 1:01:12.480
 I mean, the most of the so complicated stuff already existing cats and dogs

1:01:12.480 --> 1:01:16.480
 and, you know, certainly primates, non human primates.

1:01:16.480 --> 1:01:22.480
 Yeah, that little thing with humans might be just something about social interaction

1:01:22.480 --> 1:01:27.480
 and ability to maintain ideas across like a collective of people.

1:01:27.480 --> 1:01:33.480
 It sounds very dramatic and very impressive, but it probably isn't mechanistically speaking.

1:01:33.480 --> 1:01:34.480
 It is, but we're not there yet.

1:01:34.480 --> 1:01:42.480
 Like, you know, we have, I mean, this is number 634, you know, in the list of problems we have to solve.

1:01:42.480 --> 1:01:46.480
 So basic physics of the world is number one.

1:01:46.480 --> 1:01:51.480
 What do you, just a quick tangent on data augmentation.

1:01:51.480 --> 1:01:57.480
 So a lot of it is hard coded versus learned.

1:01:57.480 --> 1:02:03.480
 Do you have any intuition that maybe there could be some weird data augmentation,

1:02:03.480 --> 1:02:07.480
 like generative type of data augmentation, like doing something weird to images,

1:02:07.480 --> 1:02:12.480
 which then improves the similarity learning process.

1:02:12.480 --> 1:02:17.480
 So not just kind of dumb, simple distortions, but by you shaking your head,

1:02:17.480 --> 1:02:20.480
 just saying that even simple distortions are enough.

1:02:20.480 --> 1:02:25.480
 I think no, I think data augmentation is a temporary necessary evil.

1:02:25.480 --> 1:02:28.480
 So what people are working on now is two things.

1:02:28.480 --> 1:02:35.480
 One is the type of self supervised learning, like trying to translate the type of self supervised learning people

1:02:35.480 --> 1:02:41.480
 using language, translating these two images, which is basically a denosing auto encoder method.

1:02:41.480 --> 1:02:46.480
 So you take an image, you block, you mask some parts of it,

1:02:46.480 --> 1:02:52.480
 and then you train some giant neural net to reconstruct the parts that are missing.

1:02:52.480 --> 1:02:58.480
 And until very recently, there was no working methods for that.

1:02:58.480 --> 1:03:03.480
 All the auto encoder type methods for images weren't producing very good representation.

1:03:03.480 --> 1:03:08.480
 But there's a paper now coming out of the Fair Group in Menlo Park that actually works very well.

1:03:08.480 --> 1:03:14.480
 So that doesn't require the documentation, that requires only masking.

1:03:14.480 --> 1:03:18.480
 Only masking for images, okay.

1:03:18.480 --> 1:03:21.480
 Right, so you mask part of the image and you train a system,

1:03:21.480 --> 1:03:30.480
 which in this case is a transformer because the transformer represents the image as non overlapping patches,

1:03:30.480 --> 1:03:33.480
 so it's easy to mask patches and things like that.

1:03:33.480 --> 1:03:36.480
 Okay, then my question transfers to that problem, then masking.

1:03:36.480 --> 1:03:39.480
 Why should the mask be a square rectangle?

1:03:39.480 --> 1:03:41.480
 So it doesn't matter.

1:03:41.480 --> 1:03:50.480
 I think we're going to come up probably in the future with ways to mask that are kind of random, essentially.

1:03:50.480 --> 1:03:52.480
 I mean, they are random already.

1:03:52.480 --> 1:03:59.480
 No, but something that's challenging, optimally challenging.

1:03:59.480 --> 1:04:07.480
 So maybe it's a metaphor that doesn't apply, but it seems like there's an data augmentation or masking.

1:04:07.480 --> 1:04:09.480
 There's an interactive element with it.

1:04:09.480 --> 1:04:15.480
 You're almost playing with an image and it's like the way we play with an image in our minds.

1:04:15.480 --> 1:04:16.480
 No, but it's like dropout.

1:04:16.480 --> 1:04:18.480
 It's like machine training.

1:04:18.480 --> 1:04:26.480
 Every time you see a percept, you can perturb it in some way.

1:04:26.480 --> 1:04:34.480
 And then the principle of the training procedure is to minimize the difference of the output or the representation

1:04:34.480 --> 1:04:39.480
 between the clean version and the corrupted version, essentially, right?

1:04:39.480 --> 1:04:41.480
 And you can do this in real time, right?

1:04:41.480 --> 1:04:44.480
 So it was a machine work like this, right?

1:04:44.480 --> 1:04:51.480
 You show a percept, you tell the machine that's a good combination of activities or your input neurons.

1:04:51.480 --> 1:05:01.480
 And then you either let them go their merry way without clamping them to values, or you only do this with a subset.

1:05:01.480 --> 1:05:08.480
 And what you're doing is you're training the system so that the stable state of the entire network is the same

1:05:08.480 --> 1:05:13.480
 regardless of whether it sees the entire input or whether it sees only part of it.

1:05:13.480 --> 1:05:16.480
 You know, denoting autoencoder method is basically the same thing, right?

1:05:16.480 --> 1:05:23.480
 You're training a system to reproduce the input, the complete input and filling the blanks regardless of which parts are missing.

1:05:23.480 --> 1:05:25.480
 And that's really the underlying principle.

1:05:25.480 --> 1:05:32.480
 And you could imagine sort of even in the brain some sort of neural principle where, you know, neurons can oscillate, right?

1:05:32.480 --> 1:05:37.480
 So they take their activity and then temporarily they kind of shut off to, you know,

1:05:37.480 --> 1:05:44.480
 force the rest of the system to basically reconstruct the input without their help, you know?

1:05:44.480 --> 1:05:50.480
 And I mean, you could imagine, you know, more or less biologically possible processes.

1:05:50.480 --> 1:05:51.480
 Something like that.

1:05:51.480 --> 1:06:00.480
 And I guess with this denoising autoencoder and masking and data augmentation, you don't have to worry about being super efficient.

1:06:00.480 --> 1:06:05.480
 You can just do as much as you want and get better over time.

1:06:05.480 --> 1:06:11.480
 Because I was thinking like you might want to be clever about the way you do all these procedures, you know?

1:06:11.480 --> 1:06:17.480
 But that's only, it's somehow costly to do every iteration, but it's not really.

1:06:17.480 --> 1:06:19.480
 Not really.

1:06:19.480 --> 1:06:23.480
 And then there is, you know, data augmentation without explicit data augmentation.

1:06:23.480 --> 1:06:28.480
 It's data augmentation by weighting, which is, you know, the sort of video prediction.

1:06:28.480 --> 1:06:35.480
 You're observing a video clip, observing the, you know, the continuation of that video clip.

1:06:35.480 --> 1:06:47.480
 You try to learn a representation using the joint embedding architectures in such a way that the representation of the future clip is easily predictable from the representation of the observed clip.

1:06:47.480 --> 1:06:55.480
 Do you think YouTube has enough raw data from which to learn how to be a cat?

1:06:55.480 --> 1:06:57.480
 I think so.

1:06:57.480 --> 1:07:00.480
 So the amount of data is not the constraint?

1:07:00.480 --> 1:07:03.480
 No, it would require some selection, I think.

1:07:03.480 --> 1:07:08.480
 Some selection of, you know, maybe the right type of data.

1:07:08.480 --> 1:07:09.480
 You need some...

1:07:09.480 --> 1:07:11.480
 Don't go down the rabbit hole of just cat videos.

1:07:11.480 --> 1:07:14.480
 You might need to watch some lectures or something.

1:07:14.480 --> 1:07:15.480
 No, you...

1:07:15.480 --> 1:07:26.480
 How meta would that be if it like watches lectures about intelligence and then learns, watches your lectures at NYU and learns from that how to be intelligent?

1:07:26.480 --> 1:07:29.480
 I don't think that would be enough.

1:07:29.480 --> 1:07:33.480
 What's your... do you find multimodal learning interesting?

1:07:33.480 --> 1:07:38.480
 We've been talking about visual language, like combining those together, maybe audio, all those kinds of things.

1:07:38.480 --> 1:07:46.480
 There's a lot of things that I find interesting in the short term, but are not addressing the important problem that I think are really kind of the big challenges.

1:07:46.480 --> 1:07:54.480
 So I think, you know, things like multitask learning, continual learning, you know, adversarial issues.

1:07:54.480 --> 1:08:00.480
 I mean, those have, you know, great practical interests in the relatively short term, possibly.

1:08:00.480 --> 1:08:04.480
 But I don't think they're fundamental, you know, active learning, even to some extent reinforcement learning.

1:08:04.480 --> 1:08:19.480
 I think those things will become either obsolete or useless or easy once we figure out how to do self supervised representation learning or learning predictive world models.

1:08:19.480 --> 1:08:24.480
 So I think that's what, you know, the entire community should be focusing on.

1:08:24.480 --> 1:08:31.480
 At least people are interested in sort of fundamental questions or, you know, really kind of pushing the envelope of AI towards the next stage.

1:08:31.480 --> 1:08:38.480
 But of course, there's like a huge amount of, you know, very interesting work to do in sort of practical questions that have, you know, short term impact.

1:08:38.480 --> 1:08:48.480
 Well, you know, it's difficult to talk about the temporal scale because all of human civilization will eventually be destroyed because the sun will die out.

1:08:48.480 --> 1:08:58.480
 And even if Elon Musk is successful, multi planetary colonization across the galaxy, eventually the entirety of it will just become giant black holes.

1:08:58.480 --> 1:09:01.480
 And that's going to take a while though.

1:09:01.480 --> 1:09:06.480
 So, but what I'm saying is then that logic can be used to say it's all meaningless.

1:09:06.480 --> 1:09:16.480
 I'm saying all that to say that multitask learning might be your song, you're calling it practical or pragmatic or whatever.

1:09:16.480 --> 1:09:28.480
 That might be the thing that achieves something very kinder intelligence while we're trying to solve the more general problem of self supervised learning of background knowledge.

1:09:28.480 --> 1:09:32.480
 So the reason I bring that up may be one way to ask that question.

1:09:32.480 --> 1:09:35.480
 I've been very impressed by what Tesla autopilot team is doing.

1:09:35.480 --> 1:09:44.480
 I don't know if you've gotten a chance to glance at this particular one example of multitask learning where they're literally taking the problem.

1:09:44.480 --> 1:09:48.480
 Like, I don't know, Charles Darwin start studying animals.

1:09:48.480 --> 1:09:54.480
 They're studying the problem of driving and asking, okay, what are all the things you have to perceive?

1:09:54.480 --> 1:10:00.480
 And the way they're solving it is one, there's an ontology where you're bringing that to the table.

1:10:00.480 --> 1:10:02.480
 So you formulate a bunch of different tasks.

1:10:02.480 --> 1:10:05.480
 It's like over a hundred tasks or something like that that they're involved in driving.

1:10:05.480 --> 1:10:10.480
 And then they're deploying it and then getting data back from people that run into trouble.

1:10:10.480 --> 1:10:15.480
 And then trying to figure out, do we add tasks? Do we, like we focus on each individual tasks separately?

1:10:15.480 --> 1:10:17.480
 In fact, half.

1:10:17.480 --> 1:10:20.480
 So the, I would say I'll classify Andre Capati's talking two ways.

1:10:20.480 --> 1:10:24.480
 So one was about doors and the other one about how much ImageNet sucks.

1:10:24.480 --> 1:10:32.480
 He kept going back and forth on those two topics, which ImageNet sucks, meaning you can't just use a single benchmark.

1:10:32.480 --> 1:10:39.480
 There's so like, you have to have like a giant suite of benchmarks to understand how well your system actually works.

1:10:39.480 --> 1:10:43.480
 Oh, I agree with him. I mean, he's a very sensible guy.

1:10:43.480 --> 1:10:51.480
 Now, okay, it's very clear that if you're faced with an engineering problem that you need to solve in a relatively short time,

1:10:51.480 --> 1:10:57.480
 particularly if you have it almost breathing down your neck, you're going to have to take shortcuts, right?

1:10:57.480 --> 1:11:04.480
 You might think about the fact that the right thing to do and the long term solution involves, you know,

1:11:04.480 --> 1:11:09.480
 some fancy software provisioning, but you have, you know, almost breathing down your neck.

1:11:09.480 --> 1:11:13.480
 And, you know, this involves, you know, human lives.

1:11:13.480 --> 1:11:25.480
 And so you have to basically just do the systematic engineering and, you know, fine tuning and refinements and try an error and all that stuff.

1:11:25.480 --> 1:11:28.480
 There's nothing wrong with that. That's, that's called engineering.

1:11:28.480 --> 1:11:35.480
 That's called, you know, putting technology out in the, in the world.

1:11:35.480 --> 1:11:47.480
 And you have to kind of ironclad it before, before you do this, you know, so much for, you know, grand ideas and principles.

1:11:47.480 --> 1:11:55.480
 But, you know, I'm placing myself sort of, you know, some, you know, upstream of this, you know, quite a bit upstream of this.

1:11:55.480 --> 1:12:02.480
 Your play, I don't think about platonic forms. Your platonic because eventually I want the stuff to get used.

1:12:02.480 --> 1:12:08.480
 But it's okay if it takes five or 10 years for the community to realize this is the right thing to do.

1:12:08.480 --> 1:12:13.480
 I've done this before. It's been the case before that, you know, I've made that case.

1:12:13.480 --> 1:12:21.480
 I mean, if you look back in the mid 2000, for example, and you ask yourself the question, okay, I want to recognize cars or faces or whatever.

1:12:21.480 --> 1:12:32.480
 You know, I can use convolutional net, so I can use a more conventional kind of computer vision techniques, you know, using interest point detectors or a sift,

1:12:32.480 --> 1:12:35.480
 density features and, you know, sticking an SVM on top.

1:12:35.480 --> 1:12:43.480
 At that time, the data sets were so small that those methods that use more hand engineering work better than com nets.

1:12:43.480 --> 1:12:50.480
 There was just not enough data for com nets, and com nets were, were a little, a little slow with the kind of hardware that was available at the time.

1:12:50.480 --> 1:12:58.480
 And there was a sea change when basically when, you know, data sets became bigger and GPUs became available.

1:12:58.480 --> 1:13:07.480
 That's what, you know, the two of the main factors that basically made people change their, change their mind.

1:13:07.480 --> 1:13:16.480
 And you can, you can look at the history of like all sub branches of AI or pattern recognition.

1:13:16.480 --> 1:13:24.480
 And there's a similar trajectory followed by techniques where people start by, you know, engineering the hell out of it.

1:13:24.480 --> 1:13:35.480
 You know, be it optical character recognition, speech recognition, computer vision, like image recognition in general, natural language understanding,

1:13:35.480 --> 1:13:40.480
 like, you know, translation, things like that, right, you start to engineer the hell out of it.

1:13:40.480 --> 1:13:46.480
 You start to acquire all the knowledge, the prior knowledge, you know, about image formation, about, you know, the shape of characters,

1:13:46.480 --> 1:13:54.480
 about, you know, morphological operations, about like feature extraction, Fourier transforms, you know, Wernicke moments, you know, whatever, right.

1:13:54.480 --> 1:14:01.480
 People have come up with thousands of ways of representing images so that they could be easily classified afterwards.

1:14:01.480 --> 1:14:09.480
 Same for speech recognition, right. There is, you know, two decades for people to figure out a good front end to pre process a speech signal

1:14:09.480 --> 1:14:17.480
 so that, you know, the information about what is being said is preserved, but most of the information about the identity of the speaker is gone.

1:14:17.480 --> 1:14:21.480
 You know, casserole coefficient, so whatever, right.

1:14:21.480 --> 1:14:33.480
 And same for text, right. You do name identity recognition and then you parse and you do tagging of the parts of speech and, you know,

1:14:33.480 --> 1:14:41.480
 you do this sort of tree representation of clauses and all that stuff right before you can do anything.

1:14:41.480 --> 1:14:46.480
 So that's how it starts, right. Just engineer the hell out of it.

1:14:46.480 --> 1:14:53.480
 And then you start having data and maybe you have more powerful computers, maybe you know something about statistical learning.

1:14:53.480 --> 1:15:00.480
 So you start using machine learning and it's usually a small sliver on top of your kind of handcrafted system where, you know, you extract features by hand.

1:15:00.480 --> 1:15:06.480
 Okay, and now, you know, nowadays the standard way of doing this is that you train the entire thing end to end with a deep learning system

1:15:06.480 --> 1:15:14.480
 and it learns its own features and, you know, speech recognition systems nowadays, OCR systems are completely end to end.

1:15:14.480 --> 1:15:21.480
 It's, you know, it's some giant neural net that takes raw waveforms and produces a sequence of characters coming out.

1:15:21.480 --> 1:15:28.480
 And it's just a huge neural net, right. There's no Markov model, there's no language model that is explicit other than, you know,

1:15:28.480 --> 1:15:31.480
 something that's ingrained in the, in the sort of neural language model if you want.

1:15:31.480 --> 1:15:34.480
 Same for translation, same for all kinds of stuff.

1:15:34.480 --> 1:15:44.480
 So you see this continuous evolution from, you know, less and less hand crafting and more and more learning.

1:15:44.480 --> 1:15:50.480
 And I think it's true in biology as well.

1:15:50.480 --> 1:15:59.480
 So, I mean, we might disagree about this, maybe not in this one little piece at the end, you mentioned active learning.

1:15:59.480 --> 1:16:06.480
 It feels like active learning, which is the selection of data and also the interactivity needs to be part of this giant neural network.

1:16:06.480 --> 1:16:09.480
 You cannot just be an observer to do self supervised learning.

1:16:09.480 --> 1:16:19.480
 You have to, well, self supervised learning is just a word, but I would, whatever this giant stack of a neural network that's automatically learning,

1:16:19.480 --> 1:16:31.480
 it feels my intuition is that you have to have a system, whether it's a physical robot or a digital robot that's interacting with the world

1:16:31.480 --> 1:16:40.480
 and doing so in a flawed way and improving over time in order to do form the self supervised learning.

1:16:40.480 --> 1:16:44.480
 Well, you can't just give it a giant sea of data.

1:16:44.480 --> 1:16:46.480
 Okay, I agree and I disagree.

1:16:46.480 --> 1:16:51.480
 I agree in the sense that I agree in two ways.

1:16:51.480 --> 1:16:59.480
 The first way I agree is that if you want and you certainly need a causal model of the world that allows you to predict the consequences of your actions,

1:16:59.480 --> 1:17:02.480
 to train that model, you need to take actions, right?

1:17:02.480 --> 1:17:08.480
 You need to be able to act in a world and see the effect for you to learn causal models of the world.

1:17:08.480 --> 1:17:11.480
 That's not obvious because you can observe others.

1:17:11.480 --> 1:17:12.480
 You can observe others.

1:17:12.480 --> 1:17:15.480
 And you can infer that they're similar to you and then you can learn from that.

1:17:15.480 --> 1:17:20.480
 Yeah, but then you have to kind of hardware that part and mirror neurons and all that stuff.

1:17:20.480 --> 1:17:24.480
 And it's not clear to me how you would do this in a machine.

1:17:24.480 --> 1:17:32.480
 So I think the action part would be necessary for having causal models of the world.

1:17:32.480 --> 1:17:44.480
 The second reason it may be necessary or at least more efficient is that active learning basically goes for the juggler of what you don't know, right?

1:17:44.480 --> 1:17:52.480
 There's obvious areas of uncertainty about your world and about how the world behaves.

1:17:52.480 --> 1:18:00.480
 And you can resolve this uncertainty by systematic exploration of that part that you don't know.

1:18:00.480 --> 1:18:03.480
 And if you know that you don't know, then it makes you curious.

1:18:03.480 --> 1:18:05.480
 You kind of look into situations that...

1:18:05.480 --> 1:18:13.480
 And across the animal world, different species at different levels of curiosity, right?

1:18:13.480 --> 1:18:15.480
 Depending on how they're built, right?

1:18:15.480 --> 1:18:18.480
 So, you know, cats and rats are incredibly curious.

1:18:18.480 --> 1:18:19.480
 Dogs know so much.

1:18:19.480 --> 1:18:20.480
 I mean, less.

1:18:20.480 --> 1:18:21.480
 Yeah.

1:18:21.480 --> 1:18:23.480
 So it could be useful to have that kind of curiosity.

1:18:23.480 --> 1:18:24.480
 So it'd be useful.

1:18:24.480 --> 1:18:26.480
 But curiosity just makes the process faster.

1:18:26.480 --> 1:18:29.480
 It doesn't make the process exist.

1:18:29.480 --> 1:18:38.480
 So what process, what learning process is it that active learning makes more efficient?

1:18:38.480 --> 1:18:40.480
 And I'm asking that first question.

1:18:40.480 --> 1:18:44.480
 You know, we haven't answered that question yet.

1:18:44.480 --> 1:18:48.480
 So, you know, I worry about active learning once this question is...

1:18:48.480 --> 1:18:50.480
 So it's the more fundamental question to ask.

1:18:50.480 --> 1:18:56.480
 And if active learning or interaction increases the efficiency of the learning...

1:18:56.480 --> 1:19:04.480
 See, sometimes it becomes very different if the increase is several orders of magnitude, right?

1:19:04.480 --> 1:19:05.480
 That's true.

1:19:05.480 --> 1:19:10.480
 But fundamentally, it's still the same thing in building up the intuition about how to...

1:19:10.480 --> 1:19:18.480
 In a self supervised way to construct background models, efficient or inefficient is the core problem.

1:19:18.480 --> 1:19:24.480
 What do you think about Yosha Benjos talking about consciousness and all of these kinds of concepts?

1:19:24.480 --> 1:19:25.480
 Okay.

1:19:25.480 --> 1:19:29.480
 I don't know what consciousness is, but...

1:19:29.480 --> 1:19:31.480
 It's a good opener.

1:19:31.480 --> 1:19:38.480
 And to some extent, a lot of the things that are said about consciousness remind me of the questions people were asking themselves

1:19:38.480 --> 1:19:44.480
 in the 18th century or 17th century when they discovered that, you know, how the eye works

1:19:44.480 --> 1:19:49.480
 and the fact that the image at the back of the eye was upside down, right?

1:19:49.480 --> 1:19:50.480
 Because you have a lens.

1:19:50.480 --> 1:19:55.480
 And so on your retina, the image that forms is an image of the world, but it's upside down.

1:19:55.480 --> 1:19:57.480
 How is it that you see right side up?

1:19:57.480 --> 1:20:03.480
 And, you know, with what we know today in science, you know, we realize this question doesn't make any sense

1:20:03.480 --> 1:20:06.480
 or is kind of ridiculous in some way, right?

1:20:06.480 --> 1:20:09.480
 So I think a lot of what is said about consciousness is of that nature.

1:20:09.480 --> 1:20:15.480
 Now that said, there's a lot of really smart people that for whom I have a lot of respect who are talking about this topic,

1:20:15.480 --> 1:20:19.480
 people like David Chalmers, who is a colleague of mine at NYU.

1:20:19.480 --> 1:20:28.480
 I have kind of an orthodox folk speculative hypothesis about consciousness.

1:20:28.480 --> 1:20:31.480
 So we're talking about the study of a world model.

1:20:31.480 --> 1:20:40.480
 And I think, you know, our entire prefrontal context basically is the engine for a world model.

1:20:40.480 --> 1:20:45.480
 But when we are attending at a particular situation, we're focused on that situation.

1:20:45.480 --> 1:20:48.480
 We basically cannot attend to anything else.

1:20:48.480 --> 1:20:59.480
 And that seems to suggest that we basically have only one world model engine in our prefrontal context.

1:20:59.480 --> 1:21:02.480
 That engine is configurable to the situation at hand.

1:21:02.480 --> 1:21:09.480
 So we are burning a box out of wood or we are, you know, driving down the highway playing chess.

1:21:09.480 --> 1:21:15.480
 We basically have a single model of the world that we configure into the situation at hand,

1:21:15.480 --> 1:21:18.480
 which is why we can only attend to one task at a time.

1:21:18.480 --> 1:21:27.480
 Now, if there is a task that we do repeatedly, it goes from the sort of deliberate reasoning using model of the world and prediction

1:21:27.480 --> 1:21:31.480
 and perhaps something like model predictive control, which I was talking about earlier,

1:21:31.480 --> 1:21:34.480
 to something that is more subconscious that becomes automatic.

1:21:34.480 --> 1:21:38.480
 So I don't know if you've ever played against a chess grandmaster.

1:21:38.480 --> 1:21:43.480
 You know, I get wiped out in, you know, 10, 10 plays, right?

1:21:43.480 --> 1:21:49.480
 And, you know, I have to think about my move for, you know, like 15 minutes.

1:21:49.480 --> 1:21:55.480
 And the person in front of me, the grandmaster, you know, would just like react within seconds, right?

1:21:55.480 --> 1:21:58.480
 You know, he doesn't need to think about it.

1:21:58.480 --> 1:22:04.480
 That's become part of subconscious because, you know, it's basically just pattern recognition at this point.

1:22:04.480 --> 1:22:09.480
 Same, you know, the first few hours you drive a car, you're really attentive, you can't do anything else.

1:22:09.480 --> 1:22:13.480
 And then after 20, 30 hours of practice, 50 hours, you know, it's subconscious.

1:22:13.480 --> 1:22:16.480
 You can talk to the person next to you, you know, things like that, right?

1:22:16.480 --> 1:22:20.480
 Unless the situation becomes unpredictable and then you have to stop talking.

1:22:20.480 --> 1:22:24.480
 So that suggests you only have one model in your head.

1:22:24.480 --> 1:22:31.480
 And it might suggest the idea that consciousness basically is the module that configures this world model of yours.

1:22:31.480 --> 1:22:40.480
 You know, you need to have some sort of executive kind of overseer that configures your world model for the situation at hand.

1:22:40.480 --> 1:22:47.480
 And that needs to kind of the really curious concept that consciousness is not a consequence of the power of our mind,

1:22:47.480 --> 1:22:49.480
 but of the limitation of our brains.

1:22:49.480 --> 1:22:53.480
 But because we have only one world model, we have to be conscious.

1:22:53.480 --> 1:22:58.480
 If we had as many world models as there are situations we encounter,

1:22:58.480 --> 1:23:04.480
 then we could do all of them simultaneously and we wouldn't need this sort of executive control that we call consciousness.

1:23:04.480 --> 1:23:10.480
 Yeah, interesting. And somehow maybe that executive controller, I mean, the hard problem of consciousness,

1:23:10.480 --> 1:23:18.480
 there's some kind of chemicals in biology that's creating a feeling like it feels to experience some of these things.

1:23:18.480 --> 1:23:24.480
 That's kind of like the hard question is, what the heck is that? And why is that useful?

1:23:24.480 --> 1:23:35.480
 Maybe the more pragmatic question, why is it useful to feel like this is really you experiencing this versus just like information being processed?

1:23:35.480 --> 1:23:41.480
 It could be just a very nice side effect of the way we evolved.

1:23:41.480 --> 1:23:52.480
 That's just very useful to feel a sense of ownership to the decisions you make, to the perceptions you make, to the model you're trying to maintain.

1:23:52.480 --> 1:23:57.480
 Like you own this thing and it's the only one you got and if you lose it, it's going to really suck.

1:23:57.480 --> 1:24:03.480
 And so you should really send the brain some signals about it.

1:24:03.480 --> 1:24:10.480
 What ideas do you believe might be true that most or at least many people disagree with you with?

1:24:10.480 --> 1:24:13.480
 Let's say in the space of machine learning.

1:24:13.480 --> 1:24:25.480
 Well, it depends who you talk about, but I think, so certainly there is a bunch of people who are nativists who think that a lot of the basic things about the world are kind of hardwired in our minds.

1:24:25.480 --> 1:24:30.480
 Things like the world is three dimensional, for example, is that hardwired?

1:24:30.480 --> 1:24:37.480
 Things like object permanence, is it something that we learn before the age of three months or so?

1:24:37.480 --> 1:24:39.480
 Or are we born with it?

1:24:39.480 --> 1:24:46.480
 And there are very wide disagreements among the cognitive scientists for this.

1:24:46.480 --> 1:24:50.480
 I think those things are actually very simple to learn.

1:24:50.480 --> 1:24:56.480
 Is it the case that the oriented edge detectors in V1 are learned or are they hardwired?

1:24:56.480 --> 1:24:57.480
 I think they are learned.

1:24:57.480 --> 1:25:04.480
 They might be learned before both because it's really easy to generate signals from the retina that actually will train edge detectors.

1:25:04.480 --> 1:25:09.480
 And again, those are things that can be learned within minutes of opening your eyes.

1:25:09.480 --> 1:25:19.480
 I mean, since the 1990s, we have algorithms that can learn oriented edge detectors completely unsupervised with the equivalent of a few minutes of real time.

1:25:19.480 --> 1:25:22.480
 So those things have to be learned.

1:25:22.480 --> 1:25:30.480
 And there's also those MIT experiments where you kind of plug the optical nerve on the auditory cortex of a baby ferret, right?

1:25:30.480 --> 1:25:33.480
 And that auditory cortex becomes a visual cortex essentially.

1:25:33.480 --> 1:25:37.480
 So clearly, there's running taking place there.

1:25:37.480 --> 1:25:45.480
 So I think a lot of what people think are so basic that they need to be hardwired, I think a lot of those things are learned because they are easy to learn.

1:25:45.480 --> 1:25:49.480
 So you put a lot of value in the power of learning.

1:25:49.480 --> 1:25:52.480
 What kind of things do you suspect might not be learned?

1:25:52.480 --> 1:25:55.480
 Is there something that could not be learned?

1:25:55.480 --> 1:25:59.480
 So your intrinsic drives are not learned.

1:25:59.480 --> 1:26:07.480
 There are the things that make humans human or make cats different from dogs, right?

1:26:07.480 --> 1:26:12.480
 It's the basic drives that are kind of hardwired in our bezoganglia.

1:26:12.480 --> 1:26:17.480
 I mean, there are people who are working on this kind of stuff that's called intrinsic motivation in the context of reinforcement learning.

1:26:17.480 --> 1:26:19.480
 So these are objective functions.

1:26:19.480 --> 1:26:24.480
 Whether reward doesn't come from the external world, it's computed by your own brain.

1:26:24.480 --> 1:26:29.480
 Your own brain computes whether you're happy or not, right?

1:26:29.480 --> 1:26:33.480
 It measures your degree of comfort or in comfort.

1:26:33.480 --> 1:26:40.480
 And because it's your brain computing this, presumably it knows also how to estimate gradients of this, right?

1:26:40.480 --> 1:26:46.480
 So it's easier to learn when your objective is intrinsic.

1:26:46.480 --> 1:26:49.480
 So that has to be hardwired.

1:26:49.480 --> 1:26:57.480
 The critic that makes long term prediction of the outcome, which is the eventual result of this, that's learned.

1:26:57.480 --> 1:27:01.480
 And perception is learned and your model of the world is learned.

1:27:01.480 --> 1:27:07.480
 But let me take an example of why the critic, I mean, an example of how the critic might be learned, right?

1:27:07.480 --> 1:27:13.480
 If I come to you, I reach across the table and I pinch your arm, right?

1:27:13.480 --> 1:27:15.480
 Complete surprise for you.

1:27:15.480 --> 1:27:17.480
 You would not have expected this from me.

1:27:17.480 --> 1:27:21.480
 Yes, right, let's say for the sake of the story.

1:27:21.480 --> 1:27:28.480
 Okay, your bezelganglia is going to light up because it's going to hurt, right?

1:27:28.480 --> 1:27:34.480
 And now your model of the world includes the fact that I may pinch you if I approach my...

1:27:34.480 --> 1:27:36.480
 Don't trust humans.

1:27:36.480 --> 1:27:38.480
 Right, my hand to your arm.

1:27:38.480 --> 1:27:53.480
 If I try again, you're going to recoil and that's your critic, your predictor of your ultimate pain system that predicts that something bad is going to happen and you recoil to avoid it.

1:27:53.480 --> 1:27:54.480
 So even that can be learned.

1:27:54.480 --> 1:27:56.480
 That is learned, definitely.

1:27:56.480 --> 1:28:00.480
 This is what allows you also to define some goals, right?

1:28:00.480 --> 1:28:15.480
 So the fact that you're a school child who wake up in the morning and you go to school and it's not because you necessarily like waking up early and going to school, but you know that there is a long term objective you're trying to optimize.

1:28:15.480 --> 1:28:26.480
 So Ernest Becker, I'm not sure if you're familiar with the philosopher who wrote the book Denial of Death and his idea is that one of the core motivations of human beings is our terror of death, our fear of death.

1:28:26.480 --> 1:28:28.480
 That's what makes us unique from cats.

1:28:28.480 --> 1:28:30.480
 Cats are just surviving.

1:28:30.480 --> 1:28:41.480
 They do not have a deep, like cognizance introspection that over the horizon is the end.

1:28:41.480 --> 1:28:55.480
 And he says that, I mean, there's a terror management theory that just all these psychological experiments that show basically this idea that all of human civilization, everything we create is kind of trying to forget.

1:28:55.480 --> 1:28:59.480
 Even for a brief moment that we're going to die.

1:28:59.480 --> 1:29:04.480
 When do you think humans understand that they're going to die?

1:29:04.480 --> 1:29:07.480
 Is it learned early on also?

1:29:07.480 --> 1:29:10.480
 I don't know at what point.

1:29:10.480 --> 1:29:15.480
 I mean, it's a question like, at what point do you realize that what death really is?

1:29:15.480 --> 1:29:18.480
 And I think most people don't actually realize what death is, right?

1:29:18.480 --> 1:29:21.480
 I mean, most people believe that you go to heaven or something, right?

1:29:21.480 --> 1:29:33.480
 So to push back on that, what Ernest Becker says and Sheldon Solomon, all of those folks, and I find those ideas a little bit compelling is that there is moments in life, early in life.

1:29:33.480 --> 1:29:50.480
 A lot of this fun happens early in life when you are, when you do deeply experience the terror of this realization and all the things you think about about religion, all those kinds of things that would kind of think about more like teenage years and later.

1:29:50.480 --> 1:29:52.480
 We're talking about way earlier.

1:29:52.480 --> 1:29:54.480
 No, it's like seven or eight years or something like that.

1:29:54.480 --> 1:30:00.480
 You realize, holy crap, this is like the mystery, the terror.

1:30:00.480 --> 1:30:08.480
 It's almost like you're a little prey, a little baby deer sitting in the darkness of the jungle of the woods, looking all around you.

1:30:08.480 --> 1:30:10.480
 There's darkness full of terror.

1:30:10.480 --> 1:30:26.480
 And that realization says, okay, I'm going to go back in the comfort of my mind where there is a deep meaning, where there is maybe like pretend I'm immortal in however way, however kind of idea I can construct to help me understand that I'm immortal.

1:30:26.480 --> 1:30:28.480
 Religion helps with that.

1:30:28.480 --> 1:30:38.480
 You can delude yourself in all kinds of ways, like lose yourself in the busyness of each day, have little goals in mind, all those kinds of things to think that it's going to go on forever.

1:30:38.480 --> 1:30:44.480
 You kind of know you're going to die and it's going to be sad, but you don't really understand that you're going to die.

1:30:44.480 --> 1:30:46.480
 And so that's their idea.

1:30:46.480 --> 1:30:58.480
 And I find that compelling because it does seem to be a core unique aspect of human nature that we were able to really understand that this life is finite.

1:30:58.480 --> 1:31:00.480
 That seems important.

1:31:00.480 --> 1:31:02.480
 There's a bunch of different things there.

1:31:02.480 --> 1:31:06.480
 So first of all, I don't think there is a qualitative difference between us and cats in the term.

1:31:06.480 --> 1:31:14.480
 I think the difference is that we just have a better long term ability to predict in the long term.

1:31:14.480 --> 1:31:16.480
 And so we have a better understanding of other world works.

1:31:16.480 --> 1:31:20.480
 So we have better understanding of finiteness of life and things like that.

1:31:20.480 --> 1:31:22.480
 So we have a better planning engine than cats?

1:31:22.480 --> 1:31:24.480
 Yeah.

1:31:24.480 --> 1:31:28.480
 But what's the motivation for planning that far?

1:31:28.480 --> 1:31:37.480
 Well, I think it's just a side effect of the fact that we have just a better planning engine because it makes us, as I said, the essence of intelligence is the ability to predict.

1:31:37.480 --> 1:31:47.480
 And so because we're smarter, as a side effect, we also have this ability to kind of make predictions about our own future existence or lack thereof.

1:31:47.480 --> 1:31:50.480
 You say religion helps with that.

1:31:50.480 --> 1:31:52.480
 I think religion hurts, actually.

1:31:52.480 --> 1:31:57.480
 It makes people worry about what's going to happen after their death, et cetera.

1:31:57.480 --> 1:32:02.480
 If you believe that you just don't exist after death, it solves completely the problem at least.

1:32:02.480 --> 1:32:07.480
 You're saying if you don't believe in God, you don't worry about what happens after death?

1:32:07.480 --> 1:32:09.480
 I don't know.

1:32:09.480 --> 1:32:14.480
 You only worry about this life because that's the only one you have.

1:32:14.480 --> 1:32:26.480
 Well, if I were to say what Ernest Becker says, and I actually agree with him more than not, is you do deeply worry.

1:32:26.480 --> 1:32:31.480
 If you believe there's no God, there's still a deep worry of the mystery of it all.

1:32:31.480 --> 1:32:33.480
 How does that make any sense?

1:32:33.480 --> 1:32:35.480
 That it just ends.

1:32:35.480 --> 1:32:39.480
 I don't think we can truly understand that this...

1:32:39.480 --> 1:32:46.480
 I mean, so much of our life, the consciousness, the ego is invested in this being.

1:32:46.480 --> 1:32:51.480
 Science keeps bringing humanity down from its pedestal.

1:32:51.480 --> 1:32:54.480
 That's another example of it.

1:32:54.480 --> 1:32:59.480
 That's wonderful, but for us individual humans, we don't like to be brought down from a pedestal.

1:32:59.480 --> 1:33:01.480
 I'm fine with it.

1:33:01.480 --> 1:33:03.480
 But see, you're fine with it because...

1:33:03.480 --> 1:33:08.480
 Well, so what Ernest Becker would say is you're fine with it because that's just a more peaceful existence for you,

1:33:08.480 --> 1:33:10.480
 but you're not really fine.

1:33:10.480 --> 1:33:16.480
 In fact, some of the people that experienced the deepest trauma earlier in life,

1:33:16.480 --> 1:33:20.480
 they often, before they seek extensive therapy, will say that I'm fine.

1:33:20.480 --> 1:33:23.480
 It's like when you talk to people who are truly angry,

1:33:23.480 --> 1:33:25.480
 they're like, how are you doing? I'm fine.

1:33:25.480 --> 1:33:27.480
 The question is, what's going on?

1:33:27.480 --> 1:33:29.480
 I had a near death experience.

1:33:29.480 --> 1:33:34.480
 I had a very bad motorbike accident when I was 17.

1:33:34.480 --> 1:33:40.480
 But that didn't have any impact on my reflection on that topic.

1:33:40.480 --> 1:33:45.480
 So I'm basically just playing a bit of devil's advocate, pushing back on wondering,

1:33:45.480 --> 1:33:47.480
 is it truly possible to accept death?

1:33:47.480 --> 1:33:57.480
 And the flip side that's more interesting, I think, for AI and robotics, is how important is it to have this as one of the suite of motivations,

1:33:57.480 --> 1:34:04.480
 is to not just avoid falling off the roof or something like that,

1:34:04.480 --> 1:34:09.480
 but ponder the end of the ride.

1:34:09.480 --> 1:34:14.480
 If you listen to the Stoics, it's a great motivator.

1:34:14.480 --> 1:34:16.480
 It adds a sense of urgency.

1:34:16.480 --> 1:34:28.480
 So maybe to truly fear death or be cognizant of it might give a deeper meaning and urgency to the moment to live fully.

1:34:28.480 --> 1:34:31.480
 Maybe I don't disagree with that.

1:34:31.480 --> 1:34:38.480
 I think what motivates me here is knowing more about human nature.

1:34:38.480 --> 1:34:42.480
 I think human nature and human intelligence is a big mystery.

1:34:42.480 --> 1:34:48.480
 It's a scientific mystery in addition to philosophical, et cetera.

1:34:48.480 --> 1:34:53.480
 But I'm a true believer in science.

1:34:53.480 --> 1:34:59.480
 And I do have a belief that for complex systems like the brain and the mind,

1:34:59.480 --> 1:35:06.480
 the way to understand it is to try to reproduce it with artifacts that you build,

1:35:06.480 --> 1:35:09.480
 because you know what's essential to it when you try to build it.

1:35:09.480 --> 1:35:13.480
 You know, the same way I've used this analogy before with you, I believe,

1:35:13.480 --> 1:35:18.480
 the same way we only started to understand aerodynamics when we started building airplanes,

1:35:18.480 --> 1:35:21.480
 and that helped us understand how birds fly.

1:35:21.480 --> 1:35:27.480
 So I think there's kind of a similar process here where we don't have a theory,

1:35:27.480 --> 1:35:35.480
 a full theory of intelligence, but building intelligent artifacts will help us perhaps develop some underlying theory

1:35:35.480 --> 1:35:43.480
 that encompasses not just artificial implements, but also human and biological intelligence in general.

1:35:43.480 --> 1:35:52.480
 So you're an interesting person to ask this question about sort of all kinds of different other intelligent entities or intelligences.

1:35:52.480 --> 1:35:58.480
 What are your thoughts about kind of like the touring or the Chinese room question?

1:35:58.480 --> 1:36:07.480
 If we create an AI system that exhibits a lot of properties of intelligence and consciousness,

1:36:07.480 --> 1:36:12.480
 how comfortable are you thinking of that entity as intelligent or conscious?

1:36:12.480 --> 1:36:17.480
 So you're trying to build now systems that have intelligence and there's metrics about their performance,

1:36:17.480 --> 1:36:22.480
 but that metric is external.

1:36:22.480 --> 1:36:28.480
 So are you okay calling a thing intelligent or are you going to be like most humans

1:36:28.480 --> 1:36:34.480
 and be once again unhappy to be brought down from a pedestal of consciousness or intelligence?

1:36:34.480 --> 1:36:43.480
 No, I'll be very happy to understand more about human nature, human mind,

1:36:43.480 --> 1:36:50.480
 and human intelligence through the construction of machines that have similar abilities.

1:36:50.480 --> 1:36:57.480
 And if a consequence of this is to bring down humanity one notch down from its already low pedestal,

1:36:57.480 --> 1:37:02.480
 I'm just fine with it. That's just a reality of life. So I'm fine with that.

1:37:02.480 --> 1:37:07.480
 Now you were asking me about things that opinions I have that a lot of people may disagree with.

1:37:07.480 --> 1:37:14.480
 I think if we think about the design of autonomous intelligence systems,

1:37:14.480 --> 1:37:21.480
 assuming that we are somewhat successful at some level of getting machines to learn models of the world,

1:37:21.480 --> 1:37:27.480
 we build intrinsic motivation objective functions to drive the behavior of that system.

1:37:27.480 --> 1:37:32.480
 The system also has perception modules that allows it to estimate the state of the world

1:37:32.480 --> 1:37:38.480
 and then have some way of figuring out a sequence of actions to optimize a particular objective.

1:37:38.480 --> 1:37:44.480
 If it has a critic of the type that was describing before, the thing that makes you recoil your arm

1:37:44.480 --> 1:37:51.480
 the second time I try to pinch you, intelligent autonomous machine will have emotions.

1:37:51.480 --> 1:37:55.480
 I think emotions are an integral part of autonomous intelligence.

1:37:55.480 --> 1:38:03.480
 If you have an intelligent system that is driven by intrinsic motivation, by objectives,

1:38:03.480 --> 1:38:10.480
 if it has a critic that allows you to predict in advance whether the outcome of a situation is going to be good or bad,

1:38:10.480 --> 1:38:17.480
 it's going to have emotions. It's going to have fear when it predicts that the outcome is going to be bad

1:38:17.480 --> 1:38:23.480
 and something to avoid is going to have elation when it predicts it's going to be good.

1:38:23.480 --> 1:38:34.480
 If it has drives to relate with humans in some ways, the way humans have, it's going to be social.

1:38:34.480 --> 1:38:39.480
 It's going to have emotions about attachment and things of that type.

1:38:39.480 --> 1:38:49.480
 I think the sci fi thing where you see commander data having an emotion chip that you can turn off,

1:38:49.480 --> 1:38:51.480
 I think that's ridiculous.

1:38:51.480 --> 1:38:57.480
 Here's the difficult philosophical, social question.

1:38:57.480 --> 1:39:03.480
 Do you think there will be a time, like a civil rights movement for robots where,

1:39:03.480 --> 1:39:09.480
 okay, if we get the movement, but a discussion like the Supreme Court,

1:39:09.480 --> 1:39:18.480
 that particular kinds of robots, particular kinds of systems deserve the same rights as humans

1:39:18.480 --> 1:39:24.480
 because they can suffer just as humans can, all those kinds of things?

1:39:24.480 --> 1:39:27.480
 Well, perhaps not.

1:39:27.480 --> 1:39:33.480
 Imagine that humans were, that you could die and be restored.

1:39:33.480 --> 1:39:40.480
 You could be 3D reprinted and your brain could be reconstructed in its finest details.

1:39:40.480 --> 1:39:43.480
 Our ideas of rights will change in that case.

1:39:43.480 --> 1:39:48.480
 You can always just, there's always a backup, you could always restore.

1:39:48.480 --> 1:39:52.480
 Maybe the importance of murder will go down one notch.

1:39:52.480 --> 1:39:53.480
 That's right.

1:39:53.480 --> 1:40:06.480
 But also your desire to do dangerous things like skydiving or race car driving,

1:40:06.480 --> 1:40:12.480
 car racing, all that kind of stuff would probably increase or airplane aerobatics or that kind of stuff.

1:40:12.480 --> 1:40:17.480
 It would be fine to do a lot of those things or explore dangerous areas and things like that

1:40:17.480 --> 1:40:19.480
 that would change your relationship.

1:40:19.480 --> 1:40:27.480
 Now, it's very likely that robots would be like that because they'll be based on perhaps technology

1:40:27.480 --> 1:40:32.480
 that is somewhat similar to this technology and you can always have a backup.

1:40:32.480 --> 1:40:34.480
 It's possible.

1:40:34.480 --> 1:40:39.480
 I don't know if you like video games, but there's a game called Diablo.

1:40:39.480 --> 1:40:42.480
 My sons are huge fans of this.

1:40:42.480 --> 1:40:43.480
 Yes.

1:40:43.480 --> 1:40:46.480
 In fact, they made a game that's inspired by it.

1:40:46.480 --> 1:40:47.480
 Awesome.

1:40:47.480 --> 1:40:49.480
 They built a game.

1:40:49.480 --> 1:40:52.480
 My three sons have a game design studio between them.

1:40:52.480 --> 1:40:53.480
 That's awesome.

1:40:53.480 --> 1:40:55.480
 They came out with a game last year.

1:40:55.480 --> 1:40:58.480
 No, this was last year, about a year ago.

1:40:58.480 --> 1:40:59.480
 That's awesome.

1:40:59.480 --> 1:41:05.480
 But in Diablo, there's something called hardcore mode, which if you die, there's no, you're gone.

1:41:05.480 --> 1:41:06.480
 Right.

1:41:06.480 --> 1:41:07.480
 That's it.

1:41:07.480 --> 1:41:15.480
 It's possible with AI systems for them to be able to operate successfully and for us to treat them in a certain way

1:41:15.480 --> 1:41:20.480
 because they have to be integrated in human society, they have to be able to die.

1:41:20.480 --> 1:41:21.480
 No copies allowed.

1:41:21.480 --> 1:41:23.480
 In fact, copying is illegal.

1:41:23.480 --> 1:41:28.480
 It's possible with humans as well, like cloning will be illegal, even when it's possible.

1:41:28.480 --> 1:41:29.480
 But cloning is not copying, right?

1:41:29.480 --> 1:41:33.480
 I mean, you don't reproduce the mind of the person and experience.

1:41:33.480 --> 1:41:34.480
 Right.

1:41:34.480 --> 1:41:36.480
 It's just a delayed twin.

1:41:36.480 --> 1:41:40.480
 But then it's, but we were talking about with computers that you will be able to copy.

1:41:40.480 --> 1:41:41.480
 Right.

1:41:41.480 --> 1:41:46.480
 You'll be able to perfectly save, pickle the mind state.

1:41:46.480 --> 1:41:55.480
 And it's possible that that will be illegal because that goes against, that will destroy the motivation of the system.

1:41:55.480 --> 1:41:56.480
 Okay.

1:41:56.480 --> 1:42:01.480
 So let's say you have a domestic robot sometime in the future.

1:42:01.480 --> 1:42:02.480
 Yes.

1:42:02.480 --> 1:42:08.480
 And the domestic robot comes to you kind of somewhat pre trained, it can do a bunch of things.

1:42:08.480 --> 1:42:09.480
 Yes.

1:42:09.480 --> 1:42:14.480
 But it has a particular personality that makes it slightly different from the other robots because that makes them more interesting.

1:42:14.480 --> 1:42:22.480
 And then because it's lived with you for five years, you've grown some attachment to it and vice versa.

1:42:22.480 --> 1:42:24.480
 And it's learned a lot about you.

1:42:24.480 --> 1:42:26.480
 Or maybe it's not a household robot.

1:42:26.480 --> 1:42:32.480
 Maybe it's a virtual assistant that lives in your augmented reality glasses or whatever, right?

1:42:32.480 --> 1:42:36.480
 You know, the her movie type thing, right?

1:42:36.480 --> 1:42:49.480
 And that system to some extent that the intelligence in that system is a bit like your child or maybe your PhD student in the sense that there's a lot of you in that machine now, right?

1:42:49.480 --> 1:42:50.480
 Yeah.

1:42:50.480 --> 1:42:56.480
 So if it were a living thing, you would do this for free if you want, right?

1:42:56.480 --> 1:43:01.480
 If it's your child, your child can then live his or her own life.

1:43:01.480 --> 1:43:06.480
 And you know, the fact that they learn stuff from you doesn't mean that you have any ownership of it, right?

1:43:06.480 --> 1:43:13.480
 But if it's a robot that you've trained, perhaps you have some intellectual property claim.

1:43:13.480 --> 1:43:20.480
 Intellectual property? Oh, I thought you meant like permanent value in the sense that's part of you in that way.

1:43:20.480 --> 1:43:21.480
 Well, there is permanent value, right?

1:43:21.480 --> 1:43:25.480
 So you would lose a lot if that robot were to be destroyed and you had no backup.

1:43:25.480 --> 1:43:26.480
 You would lose a lot.

1:43:26.480 --> 1:43:27.480
 You would lose a lot of investment.

1:43:27.480 --> 1:43:37.480
 You know, kind of like a person dying, you know, that a friend of yours dying or a coworker or something like that.

1:43:37.480 --> 1:43:46.480
 But also you have like intellectual property rights in the sense that that system is fine tuned to your particular existence.

1:43:46.480 --> 1:43:53.480
 So that's now a very unique instantiation of that original background model, whatever it was that arrived.

1:43:53.480 --> 1:43:55.480
 And then there are issues of privacy, right?

1:43:55.480 --> 1:44:01.480
 Because now imagine that that robot has its own kind of volition and decides to work with someone else.

1:44:01.480 --> 1:44:02.480
 Yes.

1:44:02.480 --> 1:44:09.480
 Or kind of thinks life with you is sort of untenable or whatever.

1:44:09.480 --> 1:44:20.480
 Now, all the things that that system learned from you, can you delete all the personal information that that system knows about you?

1:44:20.480 --> 1:44:22.480
 I mean, that would be kind of an ethical question.

1:44:22.480 --> 1:44:29.480
 Can you erase the mind of an intelligent robot to protect your privacy?

1:44:29.480 --> 1:44:31.480
 You can't do this with humans.

1:44:31.480 --> 1:44:35.480
 You can ask them to shut up, but that you don't have complete power over them.

1:44:35.480 --> 1:44:36.480
 Can't erase humans.

1:44:36.480 --> 1:44:43.480
 Yeah, it's the problem with the relationships, you know, that you break up, you can't you can't erase the other human with robots.

1:44:43.480 --> 1:44:55.480
 I think it will have to be the same thing with robots that that risk that there has to be some risk to our interactions to truly experience them deeply.

1:44:55.480 --> 1:44:59.480
 It feels like so you have to be able to lose your robot friend.

1:44:59.480 --> 1:45:03.480
 And that robot friend to go tweeting about how much of an asshole you are.

1:45:03.480 --> 1:45:08.480
 But then are you allowed to, you know, murder the robot to protect your private information?

1:45:08.480 --> 1:45:09.480
 Yeah, probably not.

1:45:09.480 --> 1:45:10.480
 If the robot decides to leave.

1:45:10.480 --> 1:45:16.480
 I have this intuition that for robots with with certain, like it's almost like regulation.

1:45:16.480 --> 1:45:23.480
 If you declare your robot to be, let's call it sentient or something like that, like this, this robot is designed for human interaction.

1:45:23.480 --> 1:45:25.480
 Then you're not allowed to murder these robots.

1:45:25.480 --> 1:45:27.480
 It's the same as murdering other humans.

1:45:27.480 --> 1:45:33.480
 Well, but what about you do a backup of the robot that you preserve on the on a high drive or the equivalent in the future.

1:45:33.480 --> 1:45:34.480
 That might be illegal.

1:45:34.480 --> 1:45:37.480
 It's like it's a piracy, piracy is illegal.

1:45:37.480 --> 1:45:39.480
 But it's your own, it's your own robot, right?

1:45:39.480 --> 1:45:41.480
 But you can't, you don't.

1:45:41.480 --> 1:45:44.480
 But then, but then you can wipe out his brain.

1:45:44.480 --> 1:45:51.480
 So the this robot doesn't know anything about you anymore, but you still have technically is still in existence because you backed it up.

1:45:51.480 --> 1:45:59.480
 And then there'll be these great speeches at the Supreme Court by saying, Oh, sure, you can erase the mind of the robot, just like you can erase the mind of a human.

1:45:59.480 --> 1:46:00.480
 We both can suffer.

1:46:00.480 --> 1:46:08.480
 There'll be some epic like Obama type character with a speech that we we like the robots and the humans are the same.

1:46:08.480 --> 1:46:09.480
 We can both suffer.

1:46:09.480 --> 1:46:10.480
 We can both hope.

1:46:10.480 --> 1:46:16.480
 We can both all of those, all those kinds of things, raise families, all that kind of stuff.

1:46:16.480 --> 1:46:26.480
 It's it's interesting for these, just like you said, emotion seems to be a fascinatingly powerful aspect of human human interaction, human robot interaction.

1:46:26.480 --> 1:46:39.480
 And if they're able to exhibit emotions at the end of the day, that's probably going to have us deeply consider human rights, like what we value in humans, what we value in other animals.

1:46:39.480 --> 1:46:41.480
 That's why robots and AI is great.

1:46:41.480 --> 1:46:43.480
 It makes us ask really good questions.

1:46:43.480 --> 1:46:44.480
 The hard questions.

1:46:44.480 --> 1:46:45.480
 Yeah.

1:46:45.480 --> 1:46:49.480
 I mean, you asked about you asked about the Chinese room type argument, you know, is it real?

1:46:49.480 --> 1:46:50.480
 If it looks real?

1:46:50.480 --> 1:46:51.480
 Yeah.

1:46:51.480 --> 1:46:54.480
 I think the Chinese room argument is the ridiculous one.

1:46:54.480 --> 1:47:12.480
 So, so for people that don't know Chinese room is you can, I don't even know how to formulate it well, but basically, you can mimic the behavior of an intelligent system by just following a giant algorithm code book that tells you exactly how to respond in exactly each case.

1:47:12.480 --> 1:47:14.480
 But is that really intelligent?

1:47:14.480 --> 1:47:16.480
 It's like a giant lookup table.

1:47:16.480 --> 1:47:18.480
 When this person says this, you answer this.

1:47:18.480 --> 1:47:20.480
 When this person says this, you answer this.

1:47:20.480 --> 1:47:26.480
 And if you understand how that works, you have this giant nearly infinite lookup table.

1:47:26.480 --> 1:47:28.480
 Is that really intelligence?

1:47:28.480 --> 1:47:34.480
 Because intelligence seems to be a mechanism that's much more interesting and complex than this lookup table.

1:47:34.480 --> 1:47:35.480
 I don't think so.

1:47:35.480 --> 1:47:47.480
 I mean, the real question comes down to, do you think, you know, you can, you can mechanize intelligence in some way, even if that involves learning?

1:47:47.480 --> 1:47:50.480
 And the answer is, of course, yes, there's no question.

1:47:50.480 --> 1:48:02.480
 There's a second question then, which is, assuming you can reproduce intelligence in sort of different hardware than biological hardware, you know, like computers.

1:48:02.480 --> 1:48:12.480
 Can you, you know, match human intelligence in all the domains in which humans are intelligent?

1:48:12.480 --> 1:48:14.480
 Is it possible, right?

1:48:14.480 --> 1:48:16.480
 So the hypothesis of strong AI.

1:48:16.480 --> 1:48:20.480
 The answer to this, in my opinion, is an unqualified yes.

1:48:20.480 --> 1:48:22.480
 This will as well happen at some point.

1:48:22.480 --> 1:48:28.480
 There's no question that machines at some point will become more intelligent than humans in all domains where humans are intelligent.

1:48:28.480 --> 1:48:37.480
 This is not for tomorrow, it's going to take a long time, regardless of what, you know, Elon and others have claimed or believed.

1:48:37.480 --> 1:48:42.480
 This is a lot, a lot harder than many of, many of those guys think it is.

1:48:42.480 --> 1:48:53.480
 And many of those guys who thought it was simpler than that years, you know, five years ago now think it's hard because it's been five years and they realize it's going to take a lot longer.

1:48:53.480 --> 1:48:55.480
 That includes a bunch of people at DeepMind, for example.

1:48:55.480 --> 1:49:12.480
 Oh, interesting. I haven't actually touched base with the DeepMind folks, but some of it, Elon or the MSSIs, I mean, sometimes in your role, you have to kind of create deadlines that are nearer than farther away to kind of create an urgency.

1:49:12.480 --> 1:49:16.480
 Because, you know, you have to believe the impossible is possible in order to accomplish it.

1:49:16.480 --> 1:49:22.480
 And there's, of course, a flip side to that coin, but it's a weird, you can't be too cynical if you want to get something done.

1:49:22.480 --> 1:49:24.480
 Absolutely. I agree with that.

1:49:24.480 --> 1:49:31.480
 I mean, you have to inspire people to work on sort of ambitious things.

1:49:31.480 --> 1:49:38.480
 So, you know, it's certainly a lot harder than we believe, but there's no question in my mind that this will happen.

1:49:38.480 --> 1:49:42.480
 And now, you know, people are kind of worried about what does that mean for humans.

1:49:42.480 --> 1:49:51.480
 They are going to be brought down from their pedestal, you know, a bunch of notches with that and, you know, is that going to be good or bad?

1:49:51.480 --> 1:49:53.480
 I mean, it's just going to give more power, right?

1:49:53.480 --> 1:49:56.480
 And so, amplify your four human intelligence, really.

1:49:56.480 --> 1:50:05.480
 So, speaking of doing cool, ambitious things, FAIR, the Facebook AI Research Group, has recently celebrated its eighth birthday.

1:50:05.480 --> 1:50:08.480
 Or maybe you can correct me on that.

1:50:08.480 --> 1:50:14.480
 Looking back, what has been the successes, the failures, the lessons learned from the eight years of FAIR?

1:50:14.480 --> 1:50:22.480
 And maybe you can also give context of where does the newly minted meta AI fit into how does it relate to FAIR?

1:50:22.480 --> 1:50:26.480
 Right, so let me tell you a little bit about the organization of all this.

1:50:26.480 --> 1:50:29.480
 Yeah, FAIR was created almost exactly eight years ago.

1:50:29.480 --> 1:50:31.480
 It wasn't called FAIR yet.

1:50:31.480 --> 1:50:34.480
 It took that name a few months later.

1:50:34.480 --> 1:50:37.480
 And at the time, I joined Facebook.

1:50:37.480 --> 1:50:46.480
 There was a group called the AI Group that had about 12 engineers and a few scientists, like, you know, 10 engineers and two scientists and something like that.

1:50:46.480 --> 1:50:50.480
 I run it for three and a half years as a director.

1:50:50.480 --> 1:51:06.480
 I hired the first few scientists and kind of set up the culture and organized it, you know, explained to the Facebook leadership what fundamental research was about and how it can work within industry and how it needs to be open and everything.

1:51:06.480 --> 1:51:25.480
 And I think it's been an unqualified success in the sense that FAIR has simultaneously produced, you know, top level research and advanced the science and the technology, provided tools, open source tools like PyTorch and many others.

1:51:25.480 --> 1:51:47.480
 But at the same time as had a direct or mostly indirect impact on Facebook at the time, now Meta, in the sense that a lot of systems that are, that Meta is built around now are based on research projects that started at FAIR.

1:51:47.480 --> 1:51:57.480
 And so if you were to take out, you know, deep running out of Facebook services now and Meta more generally, I mean, the company would literally crumble.

1:51:57.480 --> 1:52:00.480
 I mean, it's completely built around AI these days.

1:52:00.480 --> 1:52:03.480
 And it's really essential to the operations.

1:52:03.480 --> 1:52:08.480
 So what happened after three and a half years is that I changed role.

1:52:08.480 --> 1:52:09.480
 I became chief scientist.

1:52:09.480 --> 1:52:14.480
 So I'm not doing day to day management of FAIR anymore.

1:52:14.480 --> 1:52:18.480
 I'm more of a kind of, you know, think about strategy and things like that.

1:52:18.480 --> 1:52:21.480
 And I carry my, I conduct my own research.

1:52:21.480 --> 1:52:27.480
 I've, you know, my own kind of research group working on self supervised learning and things like this, which I didn't have time to do when I was director.

1:52:27.480 --> 1:52:45.480
 So now FAIR is run by Joel Pinot and Antoine Bard together because FAIR is kind of split into now there's something called FAIR Labs, which is sort of bottom up census driven research and FAIR Excel, which is slightly more organized for bigger projects that require a little

1:52:45.480 --> 1:52:49.480
 more kind of focus and more engineering support and things like that.

1:52:49.480 --> 1:52:52.480
 So Joel needs FAIR Lab and Antoine Bard needs FAIR Excel.

1:52:52.480 --> 1:52:53.480
 Where are they located?

1:52:53.480 --> 1:52:54.480
 All over.

1:52:54.480 --> 1:52:56.480
 It's delocalized all over.

1:52:56.480 --> 1:52:57.480
 Okay.

1:52:57.480 --> 1:53:06.480
 So there's no question that the leadership of the company believes that this was a very worthwhile investment.

1:53:06.480 --> 1:53:12.480
 And what that means is that it's there for the long run.

1:53:12.480 --> 1:53:13.480
 Right.

1:53:13.480 --> 1:53:25.480
 So there is, if you want to talk in these terms, which I don't like, there's a business model, if you want, where FAIR, despite being a very fundamental research lab, brings a lot of value to the company.

1:53:25.480 --> 1:53:29.480
 Mostly indirectly through other groups.

1:53:29.480 --> 1:53:40.480
 Now what happened three and a half years ago when I stepped down was also the creation of Facebook AI, which was basically a larger organization that covers FAIR.

1:53:40.480 --> 1:53:54.480
 So FAIR is included in it, but also has other organizations that are focused on applied research or advanced development of AI technology that is more focused on the products of the company.

1:53:54.480 --> 1:53:56.480
 So less emphasis on fundamental research.

1:53:56.480 --> 1:53:57.480
 Less fundamental.

1:53:57.480 --> 1:53:58.480
 But it's still a research.

1:53:58.480 --> 1:54:05.480
 I mean, there's a lot of papers coming out of those organizations and people are awesome and wonderful to interact with.

1:54:05.480 --> 1:54:20.480
 But it serves as kind of a way to kind of scale up if you want AI technology, which may be very experimental and sort of lab prototypes into things that are usable.

1:54:20.480 --> 1:54:22.480
 So FAIR is a subset of meta AI.

1:54:22.480 --> 1:54:24.480
 Is FAIR become like KFC?

1:54:24.480 --> 1:54:26.480
 It'll just keep the F.

1:54:26.480 --> 1:54:28.480
 Nobody cares what the F stands for.

1:54:28.480 --> 1:54:35.480
 We'll know soon enough by probably by the end of 2021.

1:54:35.480 --> 1:54:38.480
 This is not a giant change, FAIR.

1:54:38.480 --> 1:54:39.480
 Well, FAIR doesn't sound too good.

1:54:39.480 --> 1:54:50.480
 But the brand people are kind of deciding on this and they've been hesitating for a while now and they tell us they're going to come up with an answer as to whether FAIR is going to change name.

1:54:50.480 --> 1:54:53.480
 Or whether we're going to change just the meaning of the F.

1:54:53.480 --> 1:54:54.480
 That's a good call.

1:54:54.480 --> 1:54:56.480
 I'll keep FAIR and change the meaning of the F.

1:54:56.480 --> 1:54:57.480
 That would be my preference.

1:54:57.480 --> 1:55:00.480
 I would turn the F into fundamental.

1:55:00.480 --> 1:55:01.480
 Oh, that's good.

1:55:01.480 --> 1:55:02.480
 Fundamental AI research.

1:55:02.480 --> 1:55:03.480
 Oh, that's really good.

1:55:03.480 --> 1:55:04.480
 Within meta AI.

1:55:04.480 --> 1:55:06.480
 So this would be meta FAIR.

1:55:06.480 --> 1:55:07.480
 Yeah.

1:55:07.480 --> 1:55:08.480
 But people will call it FAIR, right?

1:55:08.480 --> 1:55:09.480
 Yeah, exactly.

1:55:09.480 --> 1:55:10.480
 I like it.

1:55:10.480 --> 1:55:20.480
 Meta AI is part of the reality lab.

1:55:20.480 --> 1:55:21.480
 Meta now, the new Facebook right?

1:55:21.480 --> 1:55:32.480
 It's called Meta and it's kind of divided into Facebook, Instagram, WhatsApp, and reality lab.

1:55:32.480 --> 1:55:40.480
 Reality lab is about AR, VR, telepresence, communication technology and stuff like that.

1:55:40.480 --> 1:55:51.480
 It's kind of the, you can think of it as the sort of a combination of sort of new products and technology part of meta.

1:55:51.480 --> 1:55:53.480
 Is that where the touch sensing for robots?

1:55:53.480 --> 1:55:55.480
 I saw that you were posting about that.

1:55:55.480 --> 1:55:57.480
 Touch sensing for robots is part of FAIR, actually.

1:55:57.480 --> 1:55:58.480
 That's a fair product.

1:55:58.480 --> 1:55:59.480
 Oh, it is.

1:55:59.480 --> 1:56:00.480
 Okay, cool.

1:56:00.480 --> 1:56:05.480
 This is also the, no, but there is the other way, the haptic glove, right?

1:56:05.480 --> 1:56:06.480
 Yes.

1:56:06.480 --> 1:56:08.480
 That's more reality lab.

1:56:08.480 --> 1:56:10.480
 That's reality lab research.

1:56:10.480 --> 1:56:11.480
 Reality lab research.

1:56:11.480 --> 1:56:14.480
 By the way, the touch sensors are super interesting.

1:56:14.480 --> 1:56:21.480
 Like integrating that modality into the whole sensing suite is very interesting.

1:56:21.480 --> 1:56:23.480
 So what do you think about the metaverse?

1:56:23.480 --> 1:56:30.480
 What do you think about this whole kind of expansion of the view of the role of Facebook and meta in the world?

1:56:30.480 --> 1:56:35.480
 Well, metaverse really should be thought of as the next step in the internet, right?

1:56:35.480 --> 1:56:48.480
 Sort of trying to kind of make the experience more compelling of being connected either with other people or with content.

1:56:48.480 --> 1:56:58.480
 And, you know, we are evolved and trained to evolve in, you know, 3D environments where, you know, we can see other people.

1:56:58.480 --> 1:57:05.480
 We can talk to them when you're near them or, you know, and other people are far away can hear us, you know, things like that, right?

1:57:05.480 --> 1:57:10.480
 So it's, there's a lot of social conventions that exist in the real world that we can try to transpose.

1:57:10.480 --> 1:57:16.480
 Now, what is going to be eventually the, how compelling is it going to be?

1:57:16.480 --> 1:57:24.480
 Is it going to be the case that people are going to be willing to do this if they have to wear, you know, a huge pair of goggles all day?

1:57:24.480 --> 1:57:25.480
 Maybe not.

1:57:25.480 --> 1:57:30.480
 But then again, if the experience is sufficiently compelling, maybe so.

1:57:30.480 --> 1:57:36.480
 Or if the device that you have to wear is just basically a pair of glasses, you know, technology makes sufficient progress for that.

1:57:36.480 --> 1:57:49.480
 You know, AR is a much easier concept to grasp that you're going to have, you know, augmented reality glasses that basically contain some sort of, you know, virtual assistant that can help you in your daily lives.

1:57:49.480 --> 1:57:53.480
 But at the same time with the AR, you have to contend with reality.

1:57:53.480 --> 1:57:55.480
 With VR, you can completely detach yourself from reality.

1:57:55.480 --> 1:57:56.480
 So it gives you freedom.

1:57:56.480 --> 1:57:59.480
 It might be easier to design worlds in VR.

1:57:59.480 --> 1:58:00.480
 Yeah.

1:58:00.480 --> 1:58:05.480
 But you can imagine, you know, the metaverse being a mix, right?

1:58:05.480 --> 1:58:13.480
 Or like you can have objects that exist in the metaverse that, you know, pop up on top of the real world or only exist in virtual reality.

1:58:13.480 --> 1:58:14.480
 Okay.

1:58:14.480 --> 1:58:16.480
 Let me ask the hard question.

1:58:16.480 --> 1:58:18.480
 Because all of this was easy.

1:58:18.480 --> 1:58:19.480
 This was easy.

1:58:19.480 --> 1:58:30.480
 The Facebook now meta, the social network has been painted by the media as net negative for society, even destructive and evil at times.

1:58:30.480 --> 1:58:33.480
 You've pushed back against this defending Facebook.

1:58:33.480 --> 1:58:36.480
 Can you explain your defense?

1:58:36.480 --> 1:58:37.480
 Yeah.

1:58:37.480 --> 1:58:46.480
 So the description, the company that is being described in the, in some media is not the company we know when we work inside.

1:58:46.480 --> 1:58:54.480
 And, you know, it could be claimed that a lot of employees are uninformed about what really goes on in the company.

1:58:54.480 --> 1:58:56.480
 But, you know, I'm a vice president.

1:58:56.480 --> 1:58:58.480
 I mean, I have a pretty good vision of what goes on.

1:58:58.480 --> 1:58:59.480
 You know, I don't know everything.

1:58:59.480 --> 1:59:06.480
 Obviously, I'm not involved in everything, but certainly not in decision about like, you know, content moderation or anything like this.

1:59:06.480 --> 1:59:09.480
 But I have some decent vision of what goes on.

1:59:09.480 --> 1:59:13.480
 And this evil that is being described, I just don't see it.

1:59:13.480 --> 1:59:26.480
 And then, you know, I think there is an easy story to buy, which is that, you know, all the bad things in the world and, you know, the reason your friend believes crazy stuff.

1:59:26.480 --> 1:59:33.480
 You know, there's an easy scapegoat, right, in social media in general, Facebook in particular.

1:59:33.480 --> 1:59:35.480
 But you have to look at the data.

1:59:35.480 --> 1:59:41.480
 Like, is it the case that Facebook, for example, polarizes people politically?

1:59:41.480 --> 1:59:44.480
 Are there academic studies that show this?

1:59:44.480 --> 1:59:51.480
 Is it the case that, you know, teenagers think of themselves less if they use Instagram more?

1:59:51.480 --> 2:00:05.480
 Is it the case that, you know, people get more riled up against, you know, opposite sides in a debate or political opinion if they are more on Facebook or if they are less?

2:00:05.480 --> 2:00:10.480
 And study after study show that none of this is true.

2:00:10.480 --> 2:00:12.480
 This is independent studies by academics.

2:00:12.480 --> 2:00:14.480
 They're not funded by Facebook or Meta.

2:00:14.480 --> 2:00:20.480
 You know, studied by Stanford, by some of my colleagues at NYU actually, with whom I have no connection.

2:00:20.480 --> 2:00:22.480
 You know, there's a study recently.

2:00:22.480 --> 2:00:31.480
 They paid people, I think it was in former Yugoslavia, I'm not exactly sure in what part.

2:00:31.480 --> 2:00:43.480
 But they paid people to not use Facebook for a while in the period before the anniversary of the cybernature massacres, right?

2:00:43.480 --> 2:00:47.480
 So, you know, people get riled up, like, you know, should we have a celebration?

2:00:47.480 --> 2:00:50.480
 I mean, a memorial kind of celebration for it or not.

2:00:50.480 --> 2:00:55.480
 So they paid a bunch of people to not use Facebook for a few weeks.

2:00:55.480 --> 2:01:05.480
 And it turns out that those people ended up being more polarized than they were at the beginning and the people who were more on Facebook were less polarized.

2:01:05.480 --> 2:01:15.480
 There's a study, you know, from Stanford of economists at Stanford that tried to identify the causes of increasing polarization in the U.S.

2:01:15.480 --> 2:01:22.480
 And it's been going on for 40 years before, you know, Mark Zuckerberg was born continuously.

2:01:22.480 --> 2:01:27.480
 And so if there is a cause, it's not Facebook or social media.

2:01:27.480 --> 2:01:29.480
 So you could say if social media just accelerated.

2:01:29.480 --> 2:01:35.480
 But no, I mean, it's basically a continuous evolution by some measure of polarization in the U.S.

2:01:35.480 --> 2:01:46.480
 And then you compare this with other countries like the West half of Germany, because you can go 40 years on the East side or Denmark or other countries.

2:01:46.480 --> 2:01:48.480
 And they use Facebook just as much.

2:01:48.480 --> 2:01:50.480
 And they're not getting more polarized.

2:01:50.480 --> 2:01:51.480
 They're getting less polarized.

2:01:51.480 --> 2:01:59.480
 So if you want to look for, you know, a causal relationship there, you can find a scapegoat, but you can't find a cause.

2:01:59.480 --> 2:02:02.480
 Now, if you want to fix the problem, you have to find the right cause.

2:02:02.480 --> 2:02:08.480
 And what rise me up is that people now are accusing Facebook of bad deeds that are done by others.

2:02:08.480 --> 2:02:11.480
 And those others are we're not doing anything about them.

2:02:11.480 --> 2:02:17.480
 And by the way, those others include the owner of the Wall Street Journal in which all of those papers were published.

2:02:17.480 --> 2:02:25.480
 So I should mention that I'm talking to Shreve, Mike Shreve for on this podcast and also Mark Zuckerberg and probably these are conversations you can have with them.

2:02:25.480 --> 2:02:31.480
 Because it's very interesting to me, even if Facebook has some measurable negative effect,

2:02:31.480 --> 2:02:33.480
 you can't just consider that an isolation.

2:02:33.480 --> 2:02:36.480
 You have to consider about all the positive ways it connects us.

2:02:36.480 --> 2:02:39.480
 So like every technology is a question.

2:02:39.480 --> 2:02:43.480
 You can't just say like there's an increase in division.

2:02:43.480 --> 2:02:47.480
 Yes, probably Google search engine has created increase in division.

2:02:47.480 --> 2:02:50.480
 We have to consider about how much information are brought to the world.

2:02:50.480 --> 2:02:53.480
 Like I'm sure Wikipedia created more division.

2:02:53.480 --> 2:02:59.480
 If you just look at the division, we have to look at the full context of the world and they didn't make a better world.

2:02:59.480 --> 2:03:01.480
 But the printing press has created more division.

2:03:01.480 --> 2:03:02.480
 Exactly.

2:03:02.480 --> 2:03:10.480
 So when the printing press was invented, the first books that were printed were things like the Bible.

2:03:10.480 --> 2:03:17.480
 And that allowed people to read the Bible by themselves, not get the message uniquely from priests in Europe.

2:03:17.480 --> 2:03:23.480
 And that created the Protestant movement and 200 years of religious persecution and wars.

2:03:23.480 --> 2:03:25.480
 So that's a bad side effect of the printing press.

2:03:25.480 --> 2:03:32.480
 Social networks aren't being nearly as bad as the printing press, but nobody would say the printing press was a bad idea.

2:03:32.480 --> 2:03:37.480
 Yeah, a lot of it is perception and there's a lot of different incentives operating here.

2:03:37.480 --> 2:03:39.480
 Maybe a quick comment.

2:03:39.480 --> 2:03:46.480
 Since you're one of the top leaders at Facebook and at Meta, sorry, that's in the tech space.

2:03:46.480 --> 2:03:52.480
 I'm sure Facebook involves a lot of incredible technological challenges that need to be solved.

2:03:52.480 --> 2:03:55.480
 A lot of it probably is in the computer infrastructure, the hardware.

2:03:55.480 --> 2:03:58.480
 I mean, it's just a huge amount.

2:03:58.480 --> 2:04:05.480
 Maybe can you give me context about how much of Shrap's life is AI and how much of it is low level compute?

2:04:05.480 --> 2:04:11.480
 How much of it is flying all around doing business stuff in the same way as Mark Zuckerberg?

2:04:11.480 --> 2:04:13.480
 They really focus on AI.

2:04:13.480 --> 2:04:23.480
 I mean, certainly in the run up of the creation of FAIR and for at least a year after that, if not more,

2:04:23.480 --> 2:04:29.480
 Mark was very, very much focused on AI and was spending quite a lot of effort on it.

2:04:29.480 --> 2:04:30.480
 And that's his style.

2:04:30.480 --> 2:04:33.480
 When he gets interested in something, he reads everything about it.

2:04:33.480 --> 2:04:39.480
 He read some of my papers, for example, before I joined.

2:04:39.480 --> 2:04:44.480
 And so he learns a lot about it.

2:04:44.480 --> 2:04:50.480
 And Shrap was really to it also.

2:04:50.480 --> 2:04:59.480
 I mean, Shrap is really kind of has something I've tried to preserve also despite my not so young age,

2:04:59.480 --> 2:05:02.480
 which is a sense of wonder about science and technology.

2:05:02.480 --> 2:05:05.480
 And he certainly has that.

2:05:05.480 --> 2:05:11.480
 He's also a wonderful person in terms of as a manager, dealing with people and everything.

2:05:11.480 --> 2:05:13.480
 Mark also actually.

2:05:13.480 --> 2:05:17.480
 I mean, they're very human people.

2:05:17.480 --> 2:05:24.480
 In the case of Mark, it's shockingly human, given his trajectory.

2:05:24.480 --> 2:05:29.480
 The personality of him that he spent in the press is just completely wrong.

2:05:29.480 --> 2:05:30.480
 Yeah.

2:05:30.480 --> 2:05:32.480
 But you have to know how to play the press.

2:05:32.480 --> 2:05:36.480
 I put some of that responsibility on him, too.

2:05:36.480 --> 2:05:44.480
 It's like the director, the conductor of an orchestra.

2:05:44.480 --> 2:05:49.480
 You have to play the press and the public in a certain kind of way where you convey your true self to them.

2:05:49.480 --> 2:05:51.480
 If there's a depth and kind of state.

2:05:51.480 --> 2:05:54.480
 And it's probably not the best at it.

2:05:54.480 --> 2:05:57.480
 You have to learn.

2:05:57.480 --> 2:06:03.480
 And it's sad to see, I'll talk to him about it, but Shrap is slowly stepping down.

2:06:03.480 --> 2:06:09.480
 It's always sad to see folks sort of be there for a long time and slowly.

2:06:09.480 --> 2:06:11.480
 I guess time is sad.

2:06:11.480 --> 2:06:21.480
 I think he's done the thing he set out to do and he's got family priorities and stuff like that.

2:06:21.480 --> 2:06:27.480
 And I understand after 13 years or something.

2:06:27.480 --> 2:06:28.480
 It's been a good run.

2:06:28.480 --> 2:06:34.480
 Which in Silicon Valley is basically a lifetime, because it's dog years.

2:06:34.480 --> 2:06:38.480
 So New Europe's the conference just wrapped up.

2:06:38.480 --> 2:06:40.480
 Let me just go back to something else.

2:06:40.480 --> 2:06:44.480
 You posted the paper you coauthored was rejected from New Europe's.

2:06:44.480 --> 2:06:47.480
 As you said, proudly in quotes rejected.

2:06:47.480 --> 2:06:48.480
 Good joke.

2:06:48.480 --> 2:06:50.480
 Yeah, I know.

2:06:50.480 --> 2:06:55.480
 Can you describe this paper and like what was the idea in it?

2:06:55.480 --> 2:07:00.480
 And also maybe this is a good opportunity to ask what are the pros and cons?

2:07:00.480 --> 2:07:03.480
 What works and what doesn't about the review process?

2:07:03.480 --> 2:07:04.480
 Yeah, let me talk about the paper first.

2:07:04.480 --> 2:07:08.480
 I'll talk about the review process afterwards.

2:07:08.480 --> 2:07:10.480
 The paper is called Vicreg.

2:07:10.480 --> 2:07:14.480
 So this is I mentioned that before variance in variance covariance regularization.

2:07:14.480 --> 2:07:21.480
 And it's a technique in non contrastive learning technique for what I call joint embedding architecture.

2:07:21.480 --> 2:07:24.480
 So Samy's nets are an example of joint embedding architecture.

2:07:24.480 --> 2:07:30.480
 So joint embedding architecture is let me back up a little bit, right?

2:07:30.480 --> 2:07:35.480
 So if you want to do self supervised learning, you can do it by prediction.

2:07:35.480 --> 2:07:38.480
 So let's say you want to train a system to predict video, right?

2:07:38.480 --> 2:07:44.480
 You show it a video clip and you train the system to predict the next, the continuation of that video clip.

2:07:44.480 --> 2:07:50.480
 Now, because you need to handle uncertainty because there are many, you know, many continuations that are plausible.

2:07:50.480 --> 2:07:53.480
 You need to have, you need to handle this in some way.

2:07:53.480 --> 2:07:59.480
 You need to have a way for the system to be able to produce multiple predictions.

2:07:59.480 --> 2:08:04.480
 And the way, the only way I know to do this is through what's called a latent variable.

2:08:04.480 --> 2:08:11.480
 So you have some sort of hidden vector of a variable that you can vary over a set or draw from a distribution.

2:08:11.480 --> 2:08:17.480
 And as you vary this vector over a set, the output, the prediction varies over a set of plausible predictions.

2:08:17.480 --> 2:08:23.480
 Okay, so that's called, I call this a generative latent variable model.

2:08:23.480 --> 2:08:28.480
 Okay, now there's an alternative to this to handle uncertainty.

2:08:28.480 --> 2:08:40.480
 And instead of directly predicting the next frames of the clip, you also run those through another neural net.

2:08:40.480 --> 2:08:48.480
 So you now have two neural nets, one that looks at the, you know, the initial segment of the video clip.

2:08:48.480 --> 2:08:53.480
 And another one that looks at the continuation during training, right?

2:08:53.480 --> 2:09:03.480
 And what you're trying to do is learn a representation of those two video clips that is maximally informative about the video clips themselves.

2:09:03.480 --> 2:09:11.480
 But it's such that you can predict the representation of the second video clip from the representation of the first one easily.

2:09:11.480 --> 2:09:17.480
 And you can sort of formalize this in terms of maximizing mutual information and some stuff like that, but it doesn't matter.

2:09:17.480 --> 2:09:27.480
 What you want is informative, representative, you know, informative representations of the two video clips that are mutually predictable.

2:09:27.480 --> 2:09:33.480
 What that means is that there's a lot of details in the second video clips that are irrelevant.

2:09:33.480 --> 2:09:41.480
 You know, let's say a video clip consists in, you know, a camera panning the scene.

2:09:41.480 --> 2:09:47.480
 There's going to be a piece of that room that is going to be revealed and I can somewhat predict what that room is going to look like.

2:09:47.480 --> 2:09:54.480
 But I may not be able to predict the details of the texture of the ground and where the tiles are ending and stuff like that, right?

2:09:54.480 --> 2:09:59.480
 So those are irrelevant details that perhaps my representation will eliminate.

2:09:59.480 --> 2:10:09.480
 And so what I need is to train this second neural net in such a way that whenever the continuation video clip varies

2:10:09.480 --> 2:10:15.480
 over all the plausible continuations, the representation doesn't change.

2:10:15.480 --> 2:10:17.480
 Got it.

2:10:17.480 --> 2:10:25.480
 Over the space of representations, doing the same kind of thing as you do with similarity learning.

2:10:25.480 --> 2:10:29.480
 So these are two ways to handle multimodality in a prediction, right?

2:10:29.480 --> 2:10:35.480
 In the first way, you parameterize the prediction with a latent variable, but you predict pixels essentially, right?

2:10:35.480 --> 2:10:40.480
 In the second one, you don't predict pixels, you predict an abstract representation of pixels,

2:10:40.480 --> 2:10:45.480
 and you guarantee that this abstract representation has as much information as possible about the input,

2:10:45.480 --> 2:10:51.480
 but sort of, you know, drops all the stuff that you really can't predict essentially.

2:10:51.480 --> 2:10:53.480
 I used to be a big fan of the first approach.

2:10:53.480 --> 2:10:59.480
 And in fact, in this paper with the Chen Mishra, this blog post, the Dark Matter Intelligence, I was kind of advocating for this.

2:10:59.480 --> 2:11:02.480
 And in the last year and a half, I've completely changed my mind.

2:11:02.480 --> 2:11:05.480
 I'm now a big fan of the second one.

2:11:05.480 --> 2:11:17.480
 And it's because of a small collection of algorithms that have been proposed over the last year and a half or so, two years to do this, including V Craig.

2:11:17.480 --> 2:11:21.480
 It's predecessor called Barlow Twins, which I mentioned.

2:11:21.480 --> 2:11:25.480
 A method from our friends at DeepMind could be YOL.

2:11:25.480 --> 2:11:29.480
 And there's a bunch of others now that kind of work similarly.

2:11:29.480 --> 2:11:32.480
 So they're all based on this idea of joint embedding.

2:11:32.480 --> 2:11:34.480
 Some of them have an explicit criterion.

2:11:34.480 --> 2:11:36.480
 There is an approximation of mutual information.

2:11:36.480 --> 2:11:39.480
 Some others are BOL work, but we don't really know why.

2:11:39.480 --> 2:11:42.480
 And there's been like lots of theoretical papers about why BOL works.

2:11:42.480 --> 2:11:45.480
 No, it's not that because we take it out and it still works.

2:11:45.480 --> 2:11:53.480
 I mean, so there's like a big debate, but the important point is that we now have a collection of noncontrastive joint embedding methods,

2:11:53.480 --> 2:11:56.480
 which I think is the best thing since sliced bread.

2:11:56.480 --> 2:12:06.480
 So I'm super excited about this because I think it's our best shot for techniques that would allow us to kind of build predictive world models.

2:12:06.480 --> 2:12:14.480
 And at the same time, learn hierarchical representations of the world where what matters about the world is preserved and what is irrelevant is eliminated.

2:12:14.480 --> 2:12:22.480
 By the way, the representations that before and after is in the space in a sequence of images or is it for single images?

2:12:22.480 --> 2:12:24.480
 It would be either for a single image for a sequence.

2:12:24.480 --> 2:12:26.480
 It doesn't have to be images. This could be applied to text.

2:12:26.480 --> 2:12:28.480
 This could be applied to just about any signal.

2:12:28.480 --> 2:12:35.480
 I'm looking for methods that are generally applicable that are not specific to one particular modality.

2:12:35.480 --> 2:12:37.480
 It could be audio or whatever.

2:12:37.480 --> 2:12:39.480
 Got it. So what's the story behind this paper?

2:12:39.480 --> 2:12:43.480
 This paper is describing one such method?

2:12:43.480 --> 2:12:44.480
 It's this Vicreg method.

2:12:44.480 --> 2:12:54.480
 So the first author is a student called Adrien Bard, who is a resident PhD student at Fer Paris.

2:12:54.480 --> 2:13:01.480
 He's coadvised by me and Jean Ponce, who is a professor at Economa Supérieure and also research director at INRIA.

2:13:01.480 --> 2:13:06.480
 So this is a wonderful program in France where PhD students can basically do their PhD in industry.

2:13:06.480 --> 2:13:10.480
 And that's kind of what's happening here.

2:13:10.480 --> 2:13:18.480
 And this paper is a follow up on this Balotuin paper by my former postdoc now, Stéphane Denis,

2:13:18.480 --> 2:13:24.480
 with Lijing and Yorich Bontar and a bunch of other people from Fer.

2:13:24.480 --> 2:13:31.480
 And one of the main criticism from reviewers is that Vicreg is not different enough from Balotuin's.

2:13:31.480 --> 2:13:40.480
 But my impression is that it's Balotuin's with a few bugs fixed essentially.

2:13:40.480 --> 2:13:43.480
 And in the end, this is what people reuse.

2:13:43.480 --> 2:13:44.480
 Right.

2:13:44.480 --> 2:13:49.480
 But I'm used to stuff that I submit being rejected for once.

2:13:49.480 --> 2:13:52.480
 So it might be rejected and actually exceptionally well cited because people use it.

2:13:52.480 --> 2:13:54.480
 Well, it's already cited a bunch of times.

2:13:54.480 --> 2:14:01.480
 So I mean, the question is then to the deeper question about peer review and conferences.

2:14:01.480 --> 2:14:04.480
 I mean, computer science as a field is kind of unique that the conference is highly prized.

2:14:04.480 --> 2:14:05.480
 That's one.

2:14:05.480 --> 2:14:06.480
 Right.

2:14:06.480 --> 2:14:10.480
 And it's interesting because the peer review process there is similar, I suppose, to journals,

2:14:10.480 --> 2:14:13.480
 but it's accelerated significantly.

2:14:13.480 --> 2:14:16.480
 Well, not significantly, but it goes fast.

2:14:16.480 --> 2:14:22.480
 And it's a nice way to get stuff out quickly, to peer review quickly, go to present it quickly to the community.

2:14:22.480 --> 2:14:25.480
 So not quickly, but quicker.

2:14:25.480 --> 2:14:31.480
 But nevertheless, it has many of the same flaws of peer review because it's a limited number of people look at it.

2:14:31.480 --> 2:14:33.480
 There's bias and following.

2:14:33.480 --> 2:14:37.480
 If you want to do new ideas, you're going to get pushed back.

2:14:37.480 --> 2:14:47.480
 There's self interested people that kind of can infer who submitted it and kind of be cranky about it, all that kind of stuff.

2:14:47.480 --> 2:14:48.480
 Yeah.

2:14:48.480 --> 2:14:50.480
 I mean, there's a lot of social phenomena there.

2:14:50.480 --> 2:14:59.480
 There's one social phenomenon, which is that because the field has been growing exponentially, the vast majority of people in the field are extremely junior.

2:14:59.480 --> 2:15:00.480
 Yeah.

2:15:00.480 --> 2:15:04.480
 So as a consequence, and that's just a consequence of the field growing, right?

2:15:04.480 --> 2:15:14.480
 So as the number of, as the size of the field kind of starts saturating, you will have less of that problem of reviewers being very inexperienced.

2:15:14.480 --> 2:15:24.480
 A consequence of this is that, you know, young reviewers, I mean, there's a phenomenon which is that reviewers try to make their life easy.

2:15:24.480 --> 2:15:27.480
 And to make their life easy when reviewing a paper is very simple.

2:15:27.480 --> 2:15:29.480
 You just have to find a flaw in the paper, right?

2:15:29.480 --> 2:15:34.480
 So basically, they see that task as finding flaws in papers.

2:15:34.480 --> 2:15:36.480
 And most papers have flaws, even the good ones.

2:15:36.480 --> 2:15:37.480
 Yeah.

2:15:37.480 --> 2:15:46.480
 So it's easy to, you know, to do that, your job is easier as a reviewer if you just focus on this.

2:15:46.480 --> 2:15:53.480
 But what's important is, like, is there a new idea in that paper that is likely to influence?

2:15:53.480 --> 2:16:01.480
 It doesn't matter if the experiments are not that great, if the protocol is, you know, so, so, you know, things like that.

2:16:01.480 --> 2:16:08.480
 As long as there is a worthy idea in it that will influence the way people think about the problem.

2:16:08.480 --> 2:16:15.480
 Even if they make it better, you know, eventually, I think that's, that's really what makes a paper useful.

2:16:15.480 --> 2:16:26.480
 And so this combination of social phenomena creates a disease that has plagued, you know, other fields in the past like speech recognition,

2:16:26.480 --> 2:16:31.480
 where basically, you know, people chase numbers on benchmarks.

2:16:31.480 --> 2:16:43.480
 And it's much easier to get a paper accepted if it brings an incremental improvement on a sort of mainstream, well accepted method or problem.

2:16:43.480 --> 2:16:45.480
 And those are, to me, boring papers.

2:16:45.480 --> 2:16:47.480
 I mean, they're not useless, right?

2:16:47.480 --> 2:16:51.480
 Because industry, you know, strives on those kind of progress.

2:16:51.480 --> 2:16:55.480
 But they're not the one that I'm interested in, in terms of like new concepts and new ideas.

2:16:55.480 --> 2:17:02.480
 So papers that are really trying to strike new advances generally don't make it.

2:17:02.480 --> 2:17:04.480
 Now, thankfully, we have archive.

2:17:04.480 --> 2:17:05.480
 Archive, exactly.

2:17:05.480 --> 2:17:08.480
 And then there's open review type of situations.

2:17:08.480 --> 2:17:11.480
 And then, I mean, Twitter is a kind of open review.

2:17:11.480 --> 2:17:15.480
 I'm a huge believer that review should be done by thousands of people, not two people.

2:17:15.480 --> 2:17:16.480
 I agree.

2:17:16.480 --> 2:17:22.480
 And so archive, like, do you see a future where a lot of really strong papers, it's already the present,

2:17:22.480 --> 2:17:25.480
 a growing future where it'll just be archive.

2:17:25.480 --> 2:17:35.480
 And you're presenting an ongoing continuous conference called Twitter and slash the internet slash archive sanity.

2:17:35.480 --> 2:17:37.480
 Andre just released a new version.

2:17:37.480 --> 2:17:43.480
 So just not, you know, not being so elitist about this particular gating.

2:17:43.480 --> 2:17:45.480
 It's not a question of being elitist or not.

2:17:45.480 --> 2:17:55.480
 It's a question of being basically recommendation and zero approvals for people who don't see themselves as having the ability to do so by themselves.

2:17:55.480 --> 2:17:56.480
 Right.

2:17:56.480 --> 2:17:57.480
 So it saves time.

2:17:57.480 --> 2:17:58.480
 Right.

2:17:58.480 --> 2:18:09.480
 If you rely on other people's opinion, and you trust those people or those groups to evaluate a paper for you, that saves you time.

2:18:09.480 --> 2:18:14.480
 Because, you know, you don't have to like scrutinize the paper as much, you know, is brought to your attention.

2:18:14.480 --> 2:18:18.480
 I mean, it's the whole idea of sort of, you know, collective recommender system.

2:18:18.480 --> 2:18:30.480
 So I actually thought about this a lot, you know, about 10, 15 years ago, because there were discussions at NIPPS and, you know, and we're about to create iClear with Yosha Benjo.

2:18:30.480 --> 2:18:42.480
 And so I wrote a document kind of describing a reviewing system, which basically was, you know, you post your paper on some repository, let's say archive, or now it could be open review.

2:18:42.480 --> 2:18:53.480
 And then you can form a reviewing entity, which is equivalent to a reviewing board, you know, of a journal or program committee of a conference.

2:18:53.480 --> 2:18:55.480
 You have to list the members.

2:18:55.480 --> 2:19:03.480
 And then that group reviewing entity can choose to review a particular paper spontaneously or not.

2:19:03.480 --> 2:19:08.480
 There is no exclusive relationship anymore between a paper and a venue or reviewing entity.

2:19:08.480 --> 2:19:14.480
 Any reviewing entity can review any paper or may choose not to.

2:19:14.480 --> 2:19:16.480
 And then, you know, give an evaluation.

2:19:16.480 --> 2:19:17.480
 It's not published, not published.

2:19:17.480 --> 2:19:23.480
 It's just an evaluation and a comment, which would be public signed by the reviewing entity.

2:19:23.480 --> 2:19:27.480
 And if it's signed by the reviewing entity, you know, it's one of the members of reviewing entity.

2:19:27.480 --> 2:19:35.480
 So if the reviewing entity is, you know, Lex Friedman's, you know, preferred papers, right, you know, it's Lex Friedman writing a review.

2:19:35.480 --> 2:19:36.480
 Yes.

2:19:36.480 --> 2:19:40.480
 But so for me, that's a beautiful system, I think.

2:19:40.480 --> 2:19:46.480
 But what's in addition to that, it feels like there should be a reputation system for the reviewers.

2:19:46.480 --> 2:19:47.480
 Absolutely.

2:19:47.480 --> 2:19:48.480
 For the reviewing entities.

2:19:48.480 --> 2:19:49.480
 Not the reviewers individually.

2:19:49.480 --> 2:19:51.480
 The reviewing entities, sure.

2:19:51.480 --> 2:19:56.480
 But even within that, the reviewers too, because there's another thing here.

2:19:56.480 --> 2:19:58.480
 It's not just the reputation.

2:19:58.480 --> 2:20:02.480
 It's an incentive for an individual person to do great.

2:20:02.480 --> 2:20:09.480
 Right now, in the academic setting, the incentive is kind of internal, just wanting to do a good job.

2:20:09.480 --> 2:20:17.480
 But honestly, that's not a strong enough incentive to do a really good job at reading a paper and finding the beautiful amidst the mistakes and the flaws and all that kind of stuff.

2:20:17.480 --> 2:20:18.480
 Right.

2:20:18.480 --> 2:20:27.480
 Like if you're the person that first discovered a powerful paper and you get to be proud of that discovery, then that gives a huge incentive to you.

2:20:27.480 --> 2:20:29.480
 That's a big part of my proposal, actually.

2:20:29.480 --> 2:20:42.480
 I describe that as if your evaluation of papers is predictive of future success, then your reputation should go up as a reviewing entity.

2:20:42.480 --> 2:20:44.480
 So yeah, exactly.

2:20:44.480 --> 2:20:50.480
 I even had a master's student who was a master's student in library science and computer science.

2:20:50.480 --> 2:20:55.480
 Actually, it kind of worked out exactly how that should work with formulas and everything.

2:20:55.480 --> 2:20:58.480
 So in terms of implementation, do you think that's something that's doable?

2:20:58.480 --> 2:21:05.480
 I mean, I've been sort of talking about this to various people like Andrew McCallum who started OpenReview.

2:21:05.480 --> 2:21:18.480
 And the reason why we picked OpenReview for iClear initially, even though it was very early for them, is because my hope was that iClear was eventually going to kind of inaugurate this type of system.

2:21:18.480 --> 2:21:21.480
 So iClear kept the idea of OpenReviews.

2:21:21.480 --> 2:21:26.480
 So whether reviews are published with a paper, which I think is very useful.

2:21:26.480 --> 2:21:34.480
 But in many ways, that's kind of reverted to kind of more of a conventional type conferences for everything else and that.

2:21:34.480 --> 2:21:37.480
 I mean, I don't run iClear.

2:21:37.480 --> 2:21:45.480
 I'm just the president of the foundation, but people who run it should make decisions about how to run it.

2:21:45.480 --> 2:21:50.480
 And I'm not going to tell them because there are volunteers and I'm really thankful that they do that.

2:21:50.480 --> 2:21:56.480
 But I'm saddened by the fact that we're not being innovative enough.

2:21:56.480 --> 2:21:57.480
 Yeah, me too.

2:21:57.480 --> 2:21:58.480
 I hope that changes.

2:21:58.480 --> 2:22:08.480
 Yeah, because the communication science broadly, but communication, computer science ideas is how you make those ideas have impact, I think.

2:22:08.480 --> 2:22:09.480
 Yeah.

2:22:09.480 --> 2:22:18.480
 And I think a lot of this is because people have in their mind kind of an objective, which is fairness for authors.

2:22:18.480 --> 2:22:24.480
 And the ability to count points basically and give credits accurately.

2:22:24.480 --> 2:22:28.480
 But that comes at the expense of the progress of science.

2:22:28.480 --> 2:22:31.480
 So to some extent, we're slowing down the progress of science.

2:22:31.480 --> 2:22:33.480
 And are we actually achieving fairness?

2:22:33.480 --> 2:22:35.480
 And we're not achieving fairness.

2:22:35.480 --> 2:22:37.480
 We still have biases.

2:22:37.480 --> 2:22:44.480
 We're doing a double blind review, but the biases are still there.

2:22:44.480 --> 2:22:46.480
 There are different kinds of biases.

2:22:46.480 --> 2:22:57.480
 You write that the phenomenon of emergence, collective behavior exhibited by a large collection of simple elements in interaction is one of the things that got you into neural nets in the first place.

2:22:57.480 --> 2:22:58.480
 I love cellular automata.

2:22:58.480 --> 2:23:03.480
 I love simple interacting elements and the things that emerge from them.

2:23:03.480 --> 2:23:10.480
 Do you think we understand how complex systems can emerge from such simple components that interact simply?

2:23:10.480 --> 2:23:11.480
 No, we don't.

2:23:11.480 --> 2:23:12.480
 It's a big mystery.

2:23:12.480 --> 2:23:17.480
 Also, it's a mystery for physicists, it's a mystery for biologists.

2:23:17.480 --> 2:23:25.480
 How is it that the universe around us seems to be increasing in complexity and not decreasing?

2:23:25.480 --> 2:23:36.480
 I mean, that is a kind of curious property of physics that despite the second law of thermodynamics, we seem to be evolution and learning.

2:23:36.480 --> 2:23:43.480
 It seems to be at least locally to increase complexity and not decrease it.

2:23:43.480 --> 2:23:48.480
 So perhaps the ultimate purpose of the universe is to just get more complex.

2:23:48.480 --> 2:23:54.480
 I mean, small pockets of beautiful complexity.

2:23:54.480 --> 2:24:06.480
 Does that, to cellular automata, these kinds of emergence and complex systems give you some intuition or guide your understanding of machine learning systems and neural networks and so on?

2:24:06.480 --> 2:24:09.480
 Are these for you right now disparate concepts?

2:24:09.480 --> 2:24:11.480
 Well, it got me into it.

2:24:11.480 --> 2:24:27.480
 I discovered the existence of the perceptron when I was a college student by reading a book and it was a debate between Chomsky and Piaget and Seymour Pepperd from MIT who was kind of singing the praise of the perceptron in that book.

2:24:27.480 --> 2:24:42.480
 And the first time I heard about the running machine, so I started digging the literature and I found those books which were basically transcription of workshops or conferences from the 50s and 60s about self organizing systems.

2:24:42.480 --> 2:24:48.480
 So there was a series of conferences on self organizing systems and these books.

2:24:48.480 --> 2:24:55.480
 Some of them are, you can actually get them at the Internet Archive, the digital version.

2:24:55.480 --> 2:25:02.480
 There are like fascinating articles in there by, there's a guy whose name has been largely forgotten, Heinz von Förster.

2:25:02.480 --> 2:25:11.480
 He's a German physicist who immigrated to the US and worked on self organizing systems in the 50s.

2:25:11.480 --> 2:25:21.480
 And in the 60s, he created, at University of Illinois, Japan at Champagne, he created the biological computer laboratory, BCL, which was all about neural nets.

2:25:21.480 --> 2:25:27.480
 Unfortunately, that was kind of towards the end of the popularity of neural nets, so that lab never kind of strived very much.

2:25:27.480 --> 2:25:33.480
 But he wrote a bunch of papers about self organization and about the mystery of self organization.

2:25:33.480 --> 2:25:37.480
 An example he has is, you take, imagine you are in space, there's no gravity.

2:25:37.480 --> 2:25:41.480
 You have a big box with magnets in it, okay?

2:25:41.480 --> 2:25:46.480
 You know, kind of rectangular magnets with north pole on one end, south pole on the other end.

2:25:46.480 --> 2:25:54.480
 You shake the box gently and the magnets will kind of stick to themselves and probably form like complex structure, you know, spontaneously.

2:25:54.480 --> 2:26:02.480
 You know, that could be an example of self organization, but you know, you have lots of example, neural nets are an example of self organization to, you know, in many respects.

2:26:02.480 --> 2:26:13.480
 And it's a bit of a mystery, you know, how, like what, what is possible with this, you know, pattern formation in physical systems, in chaotic system and things like that.

2:26:13.480 --> 2:26:16.480
 You know, the emergence of life, you know, things like that.

2:26:16.480 --> 2:26:19.480
 So, you know, how does that happen?

2:26:19.480 --> 2:26:22.480
 So it's a big puzzle for physicists as well.

2:26:22.480 --> 2:26:31.480
 It feels like understanding this, the mathematics of emergence in some constrained situations might help us create intelligence.

2:26:31.480 --> 2:26:41.480
 Like help us add a little spice to the systems because you seem to be able to, in complex systems with emergence,

2:26:41.480 --> 2:26:44.480
 to be able to get a lot from little.

2:26:44.480 --> 2:26:49.480
 And so that seems like a shortcut to get big leaps in performance.

2:26:49.480 --> 2:26:55.480
 But there's a missing concept that we don't have.

2:26:55.480 --> 2:27:00.480
 And it's something also I've been fascinated by since my undergrad days.

2:27:00.480 --> 2:27:03.480
 And it's how you measure complexity.

2:27:03.480 --> 2:27:11.480
 Right. So we don't actually have good ways of measuring, or at least we don't have good ways of interpreting the measures that we have at our disposal.

2:27:11.480 --> 2:27:14.480
 Like how do we measure the complexity of something, right?

2:27:14.480 --> 2:27:25.480
 So there's all those things, you know, like, you know, Karmogorov, Chaitin, Solomonov complexity of, you know, the length of the shortest program that would generate a bit string can be thought of as the complexity of that bit string.

2:27:25.480 --> 2:27:28.480
 I've been fascinated by that concept.

2:27:28.480 --> 2:27:36.480
 And the thing with that is that that complexity is defined up to a constant, which can be very large.

2:27:36.480 --> 2:27:49.480
 There are similar concepts that are derived from, you know, Bayesian probability theory, where, you know, the complexity of something is the negative log of its probability, essentially, right?

2:27:49.480 --> 2:27:52.480
 And you have a complete equivalence between the two things.

2:27:52.480 --> 2:27:58.480
 And there you would think, you know, the probability is something that's well defined mathematically, which means complexity is well defined.

2:27:58.480 --> 2:27:59.480
 But it's not true.

2:27:59.480 --> 2:28:02.480
 You need to have a model of the distribution.

2:28:02.480 --> 2:28:05.480
 You may need to have a prior if you're doing Bayesian inference.

2:28:05.480 --> 2:28:10.480
 And the prior plays the same role as the choice of the computer with which you measure Karmogorov complexity.

2:28:10.480 --> 2:28:20.480
 And so every measure of complexity we have has some arbitrary density, you know, an additive constant, which is, can be arbitrarily large.

2:28:20.480 --> 2:28:26.480
 And so, you know, how can we come up with a good theory of how things become more complex if we don't have a good measure of complexity?

2:28:26.480 --> 2:28:37.480
 Yeah, which we need for is one way that people study this in the space of biology, the people that study the origin of life or try to recreate life in the laboratory.

2:28:37.480 --> 2:28:44.480
 And the more interesting one is the alien one is when we go to other planets, how do we recognize this life?

2:28:44.480 --> 2:29:02.480
 Because, you know, complexity, we associate complexity, maybe some level of mobility with life, you know, we have to be able to like have concrete algorithms for like measuring the level of complexity we see in order to know the difference between life and nonlife.

2:29:02.480 --> 2:29:05.480
 And the problem is that complexity is in the eye of the beholder.

2:29:05.480 --> 2:29:25.480
 So, let me give you an example. If I give you an image of the MNIST digits, right, and I flip through MNIST digits, there is some obviously some structure to it because local structure, you know, neighboring pixels are correlated across the entire dataset.

2:29:25.480 --> 2:29:40.480
 Now, imagine that I apply a random permutation to all the pixels, a fixed random permutation. Now, I show you those images, they will look, you know, really disorganized to you, more complex.

2:29:40.480 --> 2:29:49.480
 In fact, they're not more complex in absolute terms, they're exactly the same as originally, right? And if you knew what the permutation was, you know, you could undo the permutation.

2:29:49.480 --> 2:29:57.480
 Now, imagine I give you special glasses that undo their permutation. Now, all of a sudden what looked complicated becomes simple.

2:29:57.480 --> 2:29:58.480
 Right.

2:29:58.480 --> 2:30:05.480
 So, if you have two, if you have, you know, humans on one end, and then another race of aliens that sees the universe with permutation glasses.

2:30:05.480 --> 2:30:07.480
 Yeah, with the permutation glasses.

2:30:07.480 --> 2:30:12.480
 What we perceive as simple to them is hardly complicated, it's probably heat.

2:30:12.480 --> 2:30:13.480
 Yeah, heat, yeah.

2:30:13.480 --> 2:30:19.480
 Okay, and what they perceive as simple to us is random fluctuation, it's heat.

2:30:19.480 --> 2:30:20.480
 Yeah.

2:30:20.480 --> 2:30:21.480
 So,

2:30:21.480 --> 2:30:28.480
 Truly in the eye of the beholder, depends what kind of glasses you're wearing, depends what kind of algorithm you're running in your perception system.

2:30:28.480 --> 2:30:42.480
 So, I don't think we'll have a theory of intelligence, self organization, evolution, things like that, until we have a good handle on a notion of complexity, which we know is in the eye of the beholder.

2:30:42.480 --> 2:30:50.480
 Yeah, it's sad to think that we might not be able to detect or interact with alien species because we're wearing different glasses.

2:30:50.480 --> 2:30:52.480
 Because the notion of locality might be different from ours.

2:30:52.480 --> 2:30:53.480
 Yeah.

2:30:53.480 --> 2:31:05.480
 This actually connects with fascinating questions in physics at the moment, like modern physics, quantum physics, like, you know, questions about, like, you know, can we recover the information that's lost in a black hole and things like this, right?

2:31:05.480 --> 2:31:11.480
 And that relies on notions of complexity, which, you know, I find is fascinating.

2:31:11.480 --> 2:31:19.480
 Can you describe your personal quest to build an expressive electronic wind instrument, EWI?

2:31:19.480 --> 2:31:20.480
 What is it?

2:31:20.480 --> 2:31:23.480
 What does it take to build it?

2:31:23.480 --> 2:31:24.480
 Well, I'm a thinker.

2:31:24.480 --> 2:31:26.480
 I like building things.

2:31:26.480 --> 2:31:31.480
 I like building things with combinations of electronics and, you know, mechanical stuff.

2:31:31.480 --> 2:31:41.480
 You know, I have a bunch of different hobbies, but, you know, probably my first one was little was building model airplanes and stuff like that, and I still do that to some extent.

2:31:41.480 --> 2:31:45.480
 But also electronics, I taught myself electronics before I studied it.

2:31:45.480 --> 2:31:49.480
 And the reason I taught myself electronics is because of music.

2:31:49.480 --> 2:31:54.480
 My cousin was an aspiring electronic musician, and then he had an analog synthesizer.

2:31:54.480 --> 2:32:00.480
 And I was, you know, basically modifying it for him and building sequencers and stuff like that, right, for him.

2:32:00.480 --> 2:32:02.480
 I was in high school when I was doing this.

2:32:02.480 --> 2:32:05.480
 How's the interest in, like, progressive rock, like, 80s?

2:32:05.480 --> 2:32:09.480
 Like, what's the greatest band of all time, according to Yamakun?

2:32:09.480 --> 2:32:11.480
 There's too many of them.

2:32:11.480 --> 2:32:26.480
 But, you know, it's a combination of, you know, Mayavishnu Orchestra, Weather Report, Yes, Genesis, you know, Pre Peter Gabriel,

2:32:26.480 --> 2:32:29.480
 Gentle Giant, you know, things like that.

2:32:29.480 --> 2:32:33.480
 Okay, so this love of electronics and this love of music combined together.

2:32:33.480 --> 2:32:39.480
 Right, so I was actually trained to play Baroque and Renaissance music,

2:32:39.480 --> 2:32:45.480
 and I played in an orchestra when I was in high school and first years of college.

2:32:45.480 --> 2:32:49.480
 And I played the recorder, crumb horn, a little bit of oboe, you know, things like that.

2:32:49.480 --> 2:32:53.480
 So I'm a wind instrument player, but I always wanted to play improvised music,

2:32:53.480 --> 2:32:55.480
 even though I don't know anything about it.

2:32:55.480 --> 2:33:00.480
 And the only way I figured, you know, short of, like, learning to play saxophone

2:33:00.480 --> 2:33:03.480
 was to play electronic wind instruments.

2:33:03.480 --> 2:33:06.480
 So they behave, you know, the fingering is similar to a saxophone,

2:33:06.480 --> 2:33:10.480
 but, you know, you have a wide variety of sound because you control the synthesizer with it.

2:33:10.480 --> 2:33:14.480
 So I had a bunch of those, you know, going back to the late 80s,

2:33:14.480 --> 2:33:22.480
 from either Yamaha or Akai, they're both kind of the main manufacturers of those.

2:33:22.480 --> 2:33:25.480
 So they were, classically, you know, going back several decades.

2:33:25.480 --> 2:33:30.480
 But I've never been completely satisfied with them because of lack of expressivity.

2:33:30.480 --> 2:33:33.480
 And, you know, those things, you know, are somewhat expressive.

2:33:33.480 --> 2:33:36.480
 I mean, they measure the breath pressure, they measure the lip pressure,

2:33:36.480 --> 2:33:41.480
 and, you know, you have various parameters you can vary with fingers,

2:33:41.480 --> 2:33:46.480
 but they're not really as expressive as an acoustic instrument, right?

2:33:46.480 --> 2:33:50.480
 You hear John Coltrane play two notes, and you know it's John Coltrane,

2:33:50.480 --> 2:33:54.480
 you know, it's got a unique sound, or Miles Davis, right?

2:33:54.480 --> 2:33:57.480
 You can hear it's Miles Davis playing the trumpet,

2:33:57.480 --> 2:34:04.480
 because the sound reflects their, you know, physiognomy,

2:34:04.480 --> 2:34:09.480
 basically the shape of the vocal track kind of shapes the sound.

2:34:09.480 --> 2:34:12.480
 So how do you do this with an electronic instrument?

2:34:12.480 --> 2:34:16.480
 And I was, many years ago, I met a guy called David Wessel.

2:34:16.480 --> 2:34:23.480
 He was a professor at Berkeley and created the center for, like, you know, music technology there.

2:34:23.480 --> 2:34:25.480
 And he was interested in that question.

2:34:25.480 --> 2:34:28.480
 And so I kept kind of thinking about this for many years.

2:34:28.480 --> 2:34:31.480
 And finally, because of COVID, you know, I was at home.

2:34:31.480 --> 2:34:32.480
 I was in my workshop.

2:34:32.480 --> 2:34:37.480
 My workshop serves also as my kind of Zoom room and home office.

2:34:37.480 --> 2:34:38.480
 This is in New Jersey?

2:34:38.480 --> 2:34:39.480
 In New Jersey.

2:34:39.480 --> 2:34:45.480
 And I started really being serious about, you know, building my own EU instrument.

2:34:45.480 --> 2:34:48.480
 What else is going on in the New Jersey workshop?

2:34:48.480 --> 2:34:55.480
 Is there some crazy stuff you built that just, or like left on the workshop floor left behind?

2:34:55.480 --> 2:35:01.480
 A lot of crazy stuff is, you know, electronics built with microcontrollers of various kinds,

2:35:01.480 --> 2:35:06.480
 and, you know, weird flying contraptions.

2:35:06.480 --> 2:35:08.480
 So you still love flying?

2:35:08.480 --> 2:35:09.480
 It's a family disease.

2:35:09.480 --> 2:35:13.480
 My dad got me into it when I was a kid.

2:35:13.480 --> 2:35:17.480
 And he was building model airplanes when he was a kid.

2:35:17.480 --> 2:35:19.480
 And he was a mechanical engineer.

2:35:19.480 --> 2:35:21.480
 He taught himself electronics also.

2:35:21.480 --> 2:35:27.480
 So he built his early radio control systems in the late 60s, early 70s.

2:35:27.480 --> 2:35:33.480
 And so that's what got me into, I mean, he got me into kind of, you know, engineering and science and technology.

2:35:33.480 --> 2:35:38.480
 Do you also have an interest and appreciation of flight in other forms, like with drones, quadroptors?

2:35:38.480 --> 2:35:41.480
 Or do you, is it model airplane?

2:35:41.480 --> 2:35:48.480
 You know, I, you know, before drones were, you know, kind of a consumer product.

2:35:48.480 --> 2:35:56.480
 You know, I built my own, you know, with also building a microcontroller with gyroscopes and accelerometers for stabilization,

2:35:56.480 --> 2:35:59.480
 writing the firmware for it, you know, and then when it became kind of a standard thing you could buy,

2:35:59.480 --> 2:36:01.480
 it was boring, you know, I stopped doing it.

2:36:01.480 --> 2:36:03.480
 It was in front anymore.

2:36:03.480 --> 2:36:06.480
 Yeah, you were doing it before it was cool.

2:36:06.480 --> 2:36:14.480
 What advice would you give to a young person today in high school and college that dreams of doing something big,

2:36:14.480 --> 2:36:18.480
 like Yanlacoon, like let's talk in the space of intelligence,

2:36:18.480 --> 2:36:23.480
 dreams of having a chance to solve some fundamental problem in space of intelligence,

2:36:23.480 --> 2:36:30.480
 both for their career and just in life, being somebody who was a part of creating something special.

2:36:30.480 --> 2:36:38.480
 So try to get interested by big questions, things like, you know, what is intelligence,

2:36:38.480 --> 2:36:44.480
 what is the universe made of, what's life all about, things like that.

2:36:44.480 --> 2:36:52.480
 Like even like crazy big questions like what's time, like nobody knows what time is.

2:36:52.480 --> 2:37:02.480
 And then learn basic things like basic methods, either from math, from physics or from engineering,

2:37:02.480 --> 2:37:04.480
 things that have a long shelf life.

2:37:04.480 --> 2:37:13.480
 Like if you have a choice between like, you know, learning, you know, mobile programming on iPhone or quantum mechanics,

2:37:13.480 --> 2:37:19.480
 take quantum mechanics, because you're going to learn things that you have no idea exist.

2:37:19.480 --> 2:37:26.480
 And you may not, you may never be a quantum physicist, but you will learn about path integrals.

2:37:26.480 --> 2:37:28.480
 And path integrals are used everywhere.

2:37:28.480 --> 2:37:32.480
 It's the same formula that you use for, you know, Bayesian integration and stuff like that.

2:37:32.480 --> 2:37:42.480
 So the ideas, the little ideas within quantum mechanics or within some of these kind of more solidified fields will have a longer shelf life.

2:37:42.480 --> 2:37:46.480
 They will use somehow use indirectly in your work.

2:37:46.480 --> 2:37:54.480
 Learn classical mechanics, like you learn about Lagrangians, for example, which is like a huge, hugely useful concept,

2:37:54.480 --> 2:37:56.480
 you know, for all kinds of different things.

2:37:56.480 --> 2:38:04.480
 Learn statistical physics, because all the math that comes out of, you know, for machine learning,

2:38:04.480 --> 2:38:10.480
 basically comes out of what we got out by statistical physicists in the, you know, late 19, early 20th century.

2:38:10.480 --> 2:38:18.480
 And for some of them, actually, more recently, by people like Giorgio Parisi, who just got the Nobel Prize for the replica method,

2:38:18.480 --> 2:38:22.480
 among other things, it's used for a lot of different things.

2:38:22.480 --> 2:38:27.480
 You know, variational inference, that math comes from statistical physics.

2:38:27.480 --> 2:38:35.480
 So a lot of those kind of, you know, basic courses, you know, if you do electrical engineering,

2:38:35.480 --> 2:38:39.480
 you take signal processing, you'll learn about Fourier transforms.

2:38:39.480 --> 2:38:44.480
 Again, something super useful is at the basis of things like graph neural nets,

2:38:44.480 --> 2:38:50.480
 which is an entirely new subarea of, you know, AI machine learning, deep learning,

2:38:50.480 --> 2:38:53.480
 which I think is super promising for all kinds of applications.

2:38:53.480 --> 2:38:56.480
 Something very promising, if you're more interested in applications,

2:38:56.480 --> 2:39:00.480
 is the applications of AI machine learning and deep learning to science,

2:39:00.480 --> 2:39:05.480
 or to science that can help solve big problems in the world.

2:39:05.480 --> 2:39:11.480
 I have colleagues at Meta, at Fair, who started this project called Open Catalyst.

2:39:11.480 --> 2:39:14.480
 And it's an open project collaborative.

2:39:14.480 --> 2:39:21.480
 And the idea is to use deep learning to help design new chemical compounds or materials

2:39:21.480 --> 2:39:25.480
 that would facilitate the separation of hydrogen from oxygen.

2:39:25.480 --> 2:39:33.480
 If you can efficiently separate oxygen from hydrogen with electricity, you solve climate change.

2:39:33.480 --> 2:39:40.480
 It's as simple as that, because you cover, you know, some random desert with solar panels,

2:39:40.480 --> 2:39:45.480
 and you have them work all day, produce hydrogen, and then you shoot the hydrogen wherever it's needed.

2:39:45.480 --> 2:39:48.480
 You don't need anything else.

2:39:48.480 --> 2:39:55.480
 You know, you have controllable power that can be transported anywhere.

2:39:55.480 --> 2:40:04.480
 So if we have a large scale, efficient energy storage technology like producing hydrogen,

2:40:04.480 --> 2:40:06.480
 we solve climate change.

2:40:06.480 --> 2:40:10.480
 Here's another way to solve climate change, is figuring out how to make fusion work.

2:40:10.480 --> 2:40:13.480
 Now, the problem with fusion is that you make a super hot plasma,

2:40:13.480 --> 2:40:15.480
 and the plasma is unstable, and you can't control it.

2:40:15.480 --> 2:40:18.480
 Maybe with deep learning, you can find controllers that would stabilize plasma

2:40:18.480 --> 2:40:21.480
 and make, you know, practical fusion reactors.

2:40:21.480 --> 2:40:26.480
 I mean, that's very speculative, but, you know, it's worth trying because, you know,

2:40:26.480 --> 2:40:28.480
 the payoff is huge.

2:40:28.480 --> 2:40:31.480
 There's a group at Google working on this led by John Platt.

2:40:31.480 --> 2:40:36.480
 So control, convert as many problems in science and physics and biology and chemistry

2:40:36.480 --> 2:40:41.480
 into a learnable problem and see if a machine can learn it?

2:40:41.480 --> 2:40:42.480
 Right.

2:40:42.480 --> 2:40:46.480
 I mean, there's properties of, you know, complex materials that we don't understand

2:40:46.480 --> 2:40:48.480
 from first principle, for example.

2:40:48.480 --> 2:40:54.480
 And so, you know, if we could design new, you know, new materials,

2:40:54.480 --> 2:40:56.480
 we could make more efficient batteries.

2:40:56.480 --> 2:40:59.480
 You know, we could make maybe faster electronics.

2:40:59.480 --> 2:41:04.480
 There's a lot of things we can imagine doing or, you know, lighter materials

2:41:04.480 --> 2:41:07.480
 for cars or airplanes and things like that, maybe better fuel cells.

2:41:07.480 --> 2:41:09.480
 I mean, there's all kinds of stuff we can imagine.

2:41:09.480 --> 2:41:13.480
 If we had good fuel cells, hydrogen fuel cells, we could use them to power airplanes

2:41:13.480 --> 2:41:17.480
 and, you know, transportation wouldn't be, or cars.

2:41:17.480 --> 2:41:24.480
 We wouldn't have emission problem, CO2 emission problems for air transportation anymore.

2:41:24.480 --> 2:41:29.480
 So there's a lot of those things, I think, where AI, you know, can be used.

2:41:29.480 --> 2:41:35.480
 And this is not even talking about all the sort of medicine biology and everything like that, right?

2:41:35.480 --> 2:41:40.480
 You know, like protein folding, you know, figuring out, like, how can you design your proteins

2:41:40.480 --> 2:41:45.480
 that it sticks to another protein at a particular site because that's how you design drugs in the end.

2:41:45.480 --> 2:41:47.480
 So, you know, deep learning would be useful, all of this.

2:41:47.480 --> 2:41:52.480
 And those are kind of, you know, would be sort of enormous progress if we could use it for that.

2:41:52.480 --> 2:41:53.480
 Here's an example.

2:41:53.480 --> 2:42:01.480
 If you take, this is like from recent material physics, you take a monoatomic layer of graphene, right?

2:42:01.480 --> 2:42:08.480
 So it's just carbon on an hexagonal mesh and you make this single atom thick.

2:42:08.480 --> 2:42:09.480
 You put another one on top.

2:42:09.480 --> 2:42:14.480
 You twist them by some magic number of degrees, three degrees or something.

2:42:14.480 --> 2:42:16.480
 It becomes superconductor.

2:42:16.480 --> 2:42:20.480
 Nobody has any idea why.

2:42:20.480 --> 2:42:22.480
 I want to know how that was discovered.

2:42:22.480 --> 2:42:25.480
 But that's the kind of thing that machine learning can actually discover, these kinds of things.

2:42:25.480 --> 2:42:26.480
 Well, maybe not.

2:42:26.480 --> 2:42:34.480
 But there is a hint, perhaps, that with machine learning, we could train a system to basically be a phenomenological model

2:42:34.480 --> 2:42:41.480
 of some complex emergent phenomenon, which, you know, superconductivity is one of those.

2:42:41.480 --> 2:42:46.480
 Where, you know, this collective phenomenon is too difficult to describe from first principles

2:42:46.480 --> 2:42:51.480
 with the current, you know, the usual sort of reductionist type method.

2:42:51.480 --> 2:42:58.480
 But we could have deep learning systems that predict the properties of a system from a description of it

2:42:58.480 --> 2:43:02.480
 after being trained with sufficiently many samples.

2:43:02.480 --> 2:43:09.480
 This guy, Pascal Foua, at DPFL, he has a startup company that,

2:43:09.480 --> 2:43:17.480
 where he basically trained a convolutional net, essentially, to predict the aerodynamic properties of solids.

2:43:17.480 --> 2:43:21.480
 And you can generate as much data as you want by just running computational free dynamics, right?

2:43:21.480 --> 2:43:29.480
 So you give, like, a wing, airfoil or something shape of some kind.

2:43:29.480 --> 2:43:37.480
 And you run computational free dynamics, you get, as a result, the drag and, you know, lift and all that stuff, right?

2:43:37.480 --> 2:43:41.480
 And you can generate lots of data, train a neural net to make those predictions.

2:43:41.480 --> 2:43:48.480
 And now what you have is a differentiable model of, let's say, drag and lift as a function of the shape of that solid.

2:43:48.480 --> 2:43:54.480
 And so you can do background and descent. You can optimize the shape so you get the properties you want.

2:43:54.480 --> 2:43:56.480
 Yeah, that's incredible. That's incredible.

2:43:56.480 --> 2:44:06.480
 And on top of all that, probably you should read a little bit of literature and a little bit of history for inspiration and for wisdom.

2:44:06.480 --> 2:44:10.480
 Because after all, all of these technologies will have to work in the human world.

2:44:10.480 --> 2:44:10.480
 Yes.

2:44:10.480 --> 2:44:12.480
 And the human world is complicated.

2:44:12.480 --> 2:44:14.480
 It is, certainly.

2:44:14.480 --> 2:44:20.480
 Yeah, and this is an amazing conversation. I'm really honored that you would talk with me today.

2:44:20.480 --> 2:44:23.480
 Thank you for all the amazing work you're doing at FAIR at Metta.

2:44:23.480 --> 2:44:28.480
 And thank you for being so passionate after all these years about everything that's going on.

2:44:28.480 --> 2:44:31.480
 You're a beacon of hope for the machine learning community.

2:44:31.480 --> 2:44:35.480
 And thank you so much for spending your valuable time with me today. That was awesome.

2:44:35.480 --> 2:44:38.480
 Thank you for having me on. That was a pleasure.

2:45:05.480 --> 2:45:12.480
 Thank you.

