WEBVTT

00:00.000 --> 00:08.120
 The following is a conversation with Anka Joghan, a professor of Berkeley working on human robot interaction.

00:08.120 --> 00:18.080
 Algorithms that look beyond the robot's function and isolation and generate robot behavior that accounts for interaction and coordination with human beings.

00:18.080 --> 00:27.120
 She also consults at Waymo, the autonomous vehicle company, but in this conversation, she is 100% wearing her Berkeley hat.

00:27.120 --> 00:32.480
 She is one of the most brilliant and fun roboticists in the world to talk with.

00:32.480 --> 00:41.400
 I had a tough and crazy day leading up to this conversation, so I was a bit tired, even more so than usual.

00:41.400 --> 00:48.880
 But almost immediately as she walked in, her energy, passion, and excitement for human robot interaction was contagious.

00:48.880 --> 00:52.840
 So I had a lot of fun and really enjoyed this conversation.

00:52.840 --> 00:55.520
 This is the Artificial Intelligence Podcast.

00:55.520 --> 01:03.680
 If you enjoy it, subscribe on YouTube, review it with 5 stars on Apple Podcasts, support it on Patreon, or simply connect with me on Twitter.

01:03.680 --> 01:08.120
 Alex Friedman, spelled F R I D M A N.

01:08.120 --> 01:14.800
 As usual, I'll do one or two minutes of ads now and never any ads in the middle that can break the flow of the conversation.

01:14.800 --> 01:20.400
 I hope that works for you and doesn't hurt the listening experience.

01:20.400 --> 01:25.480
 This show is presented by Cash App, the number one finance app in the App Store.

01:25.480 --> 01:29.280
 When you get it, use code LEX Podcast.

01:29.280 --> 01:36.800
 Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with as little as $1.

01:36.800 --> 01:48.160
 Since Cash App does fractional share trading, let me mention that the order execution algorithm that works behind the scenes to create the abstraction of fractional orders is an algorithmic marvel.

01:48.160 --> 02:05.840
 So big props to the Cash App engineers for solving a hard problem that in the end provides an easy interface that takes a step up to the next layer of abstraction over the stock market, making trading more accessible for new investors and diversification much easier.

02:05.840 --> 02:22.280
 So again, if you get Cash App from the App Store or Google Play and use the code LEX Podcast, you get $10 and Cash App will also donate $10 the first, an organization that is helping to advance robotics and STEM education for young people around the world.

02:22.280 --> 02:26.800
 And now, here's my conversation with Enka Droghan.

02:26.800 --> 02:29.880
 When did you first fall in love with robotics?

02:29.880 --> 02:47.880
 I think it was a very gradual process and it was somewhat accidental, actually, because I first started getting into programming when I was a kid and then into math and then into computer science was the thing I was going to do.

02:47.880 --> 02:59.000
 And then in college, I got into AI and then I applied to the robotics institute at Carnegie Mellon and I was coming from this little school in Germany that nobody had heard of.

02:59.000 --> 03:04.040
 But I had spent an exchange semester at Carnegie Mellon, so I had letters from Carnegie Mellon.

03:04.040 --> 03:09.200
 So that was the only place, MIT said no, Berkeley said no, Stanford said no.

03:09.200 --> 03:11.120
 That was the only place I got into.

03:11.120 --> 03:21.640
 So I went there to the robotics institute and I thought that robotics is a really cool way to actually apply the stuff that I knew and love like optimization.

03:21.640 --> 03:23.160
 So that's how I got into robotics.

03:23.160 --> 03:36.240
 I have a better story how I got into cars, which is I used to do mostly manipulation in my PhD, but now I do kind of a bit of everything application wise, including cars.

03:36.240 --> 03:48.200
 And I got into cars because I was here in Berkeley while I was a PhD student still for RSS 2014, Peter Bill organized it.

03:48.200 --> 03:54.240
 And he arranged for it was Google at the time to give us rides and self driving cars.

03:54.240 --> 04:01.640
 And I was in a robot and it was just making decision after decision, the right call.

04:01.640 --> 04:03.360
 And it was so amazing.

04:03.360 --> 04:05.520
 So it was a whole different experience, right?

04:05.520 --> 04:07.000
 Just I mean manipulation is so hard.

04:07.000 --> 04:07.840
 You can't do anything.

04:07.840 --> 04:08.640
 And there it was.

04:08.640 --> 04:11.160
 Was it the most magical robot you've ever met?

04:11.160 --> 04:18.400
 So like for me to meet Google self driving car for the first time was like a transformative moment.

04:18.400 --> 04:21.280
 Like I had two moments like that, that and spot mini.

04:21.280 --> 04:24.080
 I don't know if you met spot many from Boston Dynamics.

04:24.080 --> 04:30.800
 I felt like I felt like I fell in love or something like it because I thought I know how a spot mini works, right?

04:30.800 --> 04:33.960
 It's just I mean, there's nothing truly special.

04:33.960 --> 04:41.400
 It's great engineering work, but the anthropomorphism that went on into my brain that came to life.

04:41.400 --> 04:45.840
 Like I had a little arm and it like and looked at me.

04:45.840 --> 04:47.080
 He she looked at me.

04:47.080 --> 04:47.640
 You know, I don't know.

04:47.640 --> 04:50.560
 There's a magical connection there and it made me realize.

04:50.560 --> 04:54.200
 Wow, robots can be so much more than things that manipulate objects.

04:54.200 --> 04:56.880
 They can be things that have a human connection.

04:56.880 --> 05:00.400
 Jeff was the self driving car the moment.

05:00.400 --> 05:04.720
 Like was there a robot that truly sort of inspired you?

05:04.720 --> 05:11.040
 That was I remember that experience very viscerally riding in that car and being just wowed.

05:12.240 --> 05:17.480
 I had the they gave us a sticker that said I rode in a self driving car

05:17.480 --> 05:21.600
 and it had this cute little firefly on and or logo or something.

05:21.600 --> 05:23.600
 Oh, that was like the smaller one, like the firefly.

05:23.600 --> 05:24.840
 Yeah, the really cute one.

05:24.840 --> 05:31.960
 Yeah, and and I put it on my laptop and I had that for years until I finally changed my laptop out.

05:31.960 --> 05:35.640
 And, you know, what about if we walk back, you mentioned optimization.

05:36.240 --> 05:42.560
 Like what beautiful ideas inspired you in math, computer science early on?

05:42.560 --> 05:47.360
 Like why get into this field seems like a cold and boring field of math.

05:47.360 --> 05:48.960
 Like what was exciting to you about it?

05:48.960 --> 05:57.280
 The thing is, I liked math from very early on, from fifth grade is when I got into the Math Olympiad and all of that.

05:57.280 --> 05:58.480
 Oh, you competed too.

05:58.480 --> 06:01.080
 Yeah, this is Romania is like our national sport.

06:01.080 --> 06:02.760
 You got to understand.

06:02.760 --> 06:11.920
 So I got into that fairly early and and it was a little maybe too just theory with no kind of I didn't

06:11.920 --> 06:17.520
 kind of had a didn't really have a goal and other than understanding, which was cool.

06:17.520 --> 06:22.120
 I always liked learning and understanding, but there was no, OK, what am I applying this understanding to?

06:22.120 --> 06:31.200
 And so I think that's how I got into more heavily into computer science, because it was it was kind of math meets something you can do tangibly in the world.

06:31.200 --> 06:33.720
 Do you remember like the first program you've written?

06:34.360 --> 06:41.280
 OK, the first program I've written with I kind of do it was in Q basic and fourth grade.

06:42.480 --> 06:43.160
 Wow.

06:43.160 --> 06:54.160
 And it was drawing like a circle. Yeah, I don't know how to do that anymore, but in fourth grade, that's the first thing that they taught me.

06:54.160 --> 06:58.960
 I was like, you could take a special, I wouldn't say it was an extracurricular, isn't the sense an extracurricular?

06:58.960 --> 07:03.200
 So you could sign up for, you know, dance or music or programming.

07:03.200 --> 07:12.960
 And I did the programming thing and my mom was like, what, why did you compete in program like these days, Romania, probably that's like a big thing.

07:12.960 --> 07:14.840
 There's a program of competition.

07:15.360 --> 07:17.000
 Was that did that touch you at all?

07:17.000 --> 07:24.440
 I did a little bit of the computer science Olympian, but not not as serious as I did the math Olympian.

07:24.680 --> 07:25.720
 So it's programming.

07:25.720 --> 07:27.680
 Yeah, it's basically here's a hard math problem.

07:27.680 --> 07:29.400
 Solve it with a computer is kind of the deal.

07:29.400 --> 07:30.640
 Yeah, it's more like algorithm.

07:30.640 --> 07:32.320
 Exactly. It's always algorithmic.

07:32.560 --> 07:42.920
 So again, you kind of mentioned the Google self driving car, but outside of that, oh, what's like who or what is your favorite

07:42.920 --> 07:47.960
 robot, real or fictional that like captivated your imagination throughout?

07:48.240 --> 07:51.320
 I mean, I guess you kind of alluded to the Google self drive.

07:51.320 --> 07:54.640
 The firefly was a magical moment, but is there something else?

07:54.760 --> 07:55.840
 It was in the firefly there.

07:55.840 --> 07:57.560
 It was I think there was the Lexus, by the way.

07:57.840 --> 07:59.040
 This was back back then.

07:59.520 --> 08:01.400
 But yeah, so good question.

08:02.480 --> 08:03.360
 I might.

08:03.360 --> 08:03.680
 Okay.

08:04.040 --> 08:07.560
 My favorite fictional robot is Wally.

08:07.560 --> 08:13.560
 And I love how amazingly expressive it is.

08:13.560 --> 08:16.840
 I'm personally thinks a little bit about expressive motion kinds of things you're saying with.

08:16.840 --> 08:19.280
 You can do this and it's a head and it's the manipulator.

08:19.280 --> 08:20.280
 And what does it all mean?

08:21.280 --> 08:22.480
 I like to think about that stuff.

08:22.480 --> 08:23.480
 I love Pixar.

08:23.480 --> 08:24.480
 I love animation.

08:24.480 --> 08:26.520
 I love Wally has two big eyes, I think.

08:26.520 --> 08:32.640
 Or no, yeah, it has these, these cameras and they move.

08:33.160 --> 08:36.680
 So yeah, that's it's a, you know, it goes and then it goes.

08:36.680 --> 08:38.520
 And then it's super cute.

08:38.520 --> 08:41.400
 It's yeah, it's, you know, the way it moves is just so expressive.

08:41.400 --> 08:48.240
 The timing of that motion that what is doing with its arms and what it's doing with these lenses is amazing.

08:48.240 --> 08:53.320
 And so I've, I've really liked that from the start.

08:53.320 --> 08:56.400
 And then on top of that, sometimes I shared this.

08:56.400 --> 09:00.240
 It's a personal story I share with people or when I teach about AI or whatnot.

09:00.240 --> 09:09.600
 My husband proposed to me by building a Wally and he actuated it.

09:09.600 --> 09:12.840
 So it seven degrees of freedom, including the lens thing.

09:12.840 --> 09:21.600
 And it kind of came in and it had the, he made it have like a, you know, the belly box opening thing.

09:21.600 --> 09:23.120
 So it just did that.

09:23.120 --> 09:29.280
 And then it's viewed out this box made out of Legos that opens slowly and then bam.

09:29.280 --> 09:34.240
 Yeah, yeah, it was, it was quite, quite, it set a bar.

09:34.240 --> 09:36.880
 It could be like the most impressive thing I've ever heard.

09:37.520 --> 09:40.160
 Okay, that was special connection to Wally.

09:40.160 --> 09:43.600
 Long story short, I like Wally because I like animation and I like robots.

09:43.600 --> 09:48.720
 And I like, you know, the fact that this was, we still have this robot to this day.

09:49.760 --> 09:50.800
 How hard is that problem?

09:50.800 --> 09:54.160
 Do you think of the expressivity of robots?

09:54.160 --> 10:00.240
 Like the, with the Boston Dynamics, I never talked to those folks about this particular element.

10:00.240 --> 10:05.680
 I've talked to them a lot, but it seems to be like almost an accidental side effect for them

10:06.320 --> 10:08.560
 that they weren't, I don't know if they're faking it.

10:08.560 --> 10:11.600
 They weren't trying to, okay.

10:11.600 --> 10:16.240
 They do say that the gripper on it was not intended to be a face.

10:17.680 --> 10:21.600
 I don't know if that's a honest statement, but I think they're legitimate.

10:21.600 --> 10:29.120
 Probably yes. So do we automatically just anthropomorphize anything we can see about a robot?

10:29.120 --> 10:34.240
 So like the question is, how hard is it to create a Wally type robot that connects so

10:34.240 --> 10:35.680
 deeply with us humans? What do you think?

10:36.800 --> 10:39.840
 It's really hard, right? So it depends on what setting.

10:39.840 --> 10:47.040
 So if you want to do it in this very particular narrow setting where it does only one thing

10:47.040 --> 10:51.120
 and it's expressive, then you can get an animator, you know, can have Pixar on call,

10:51.120 --> 10:53.280
 come in, design some trajectories.

10:53.280 --> 10:57.600
 There was an Anki had a robot called Cosmo where they put in some of these animations.

10:58.320 --> 11:05.280
 That part is easy, right? The hard part is doing it not via these kind of handcrafted

11:05.280 --> 11:09.600
 behaviors, but doing it generally autonomously.

11:09.600 --> 11:14.480
 Like I want robots, I don't work on, just to clarify, I don't, I used to work a lot on this.

11:14.480 --> 11:20.880
 I don't work on that quite as much these days, but the notion of having robots

11:20.880 --> 11:25.680
 that, you know, when they pick something up and put it in a place, they can do that with

11:25.680 --> 11:31.200
 various forms of style or you can say, well, this robot is, you know, succeeding at this task

11:31.200 --> 11:35.280
 and it's confident versus it's hesitant versus, you know, maybe it's happy or it's,

11:35.280 --> 11:38.320
 you know, disappointed about something, some failure that it had.

11:38.320 --> 11:46.640
 Or I think that when robots move, they can communicate so much about internal states

11:46.640 --> 11:53.840
 or perceived internal states that they have. And I think that's really useful in an element

11:53.840 --> 12:01.600
 that we'll want in the future because I was reading this article about how kids are,

12:04.160 --> 12:11.120
 kids are being rude to Alexa because they can be rude to it and it doesn't really get angry,

12:11.120 --> 12:14.080
 right? It doesn't reply in any way. It just says the same thing.

12:14.080 --> 12:19.840
 So I think there's, at least for that, for the correct development of children,

12:19.840 --> 12:23.520
 for that these things, you kind of react differently. I also think, you know,

12:23.520 --> 12:27.520
 you walk in your home and you have a personal robot and if you're really pissed, presumably

12:27.520 --> 12:32.400
 the robot should kind of behave slightly differently than when you're super happy and excited.

12:32.400 --> 12:38.720
 But it's really hard because it's, I don't know, you know, the way I would think about it and the

12:38.720 --> 12:45.200
 way I thought about it when it came to expressing goals or intentions for robots, it's, well,

12:45.200 --> 12:51.760
 what's really happening is that instead of doing robotics where you have your state and you have

12:51.760 --> 12:57.760
 your action space and you have your space, the reward function that you're trying to optimize,

12:57.760 --> 13:02.640
 now you kind of have to expand the notion of state to include this human internal state.

13:02.640 --> 13:09.280
 What is the person actually perceiving? What do they think about the robot, something or

13:09.280 --> 13:13.920
 rather, and then you have to optimize in that system. And so that means you have to understand

13:13.920 --> 13:19.760
 how your motion, your actions end up sort of influencing the observer's kind of perception

13:19.760 --> 13:26.960
 of you. And it's very hard to write math about that. Right. So when you start to think about

13:26.960 --> 13:34.080
 incorporating the human into the state model, apologize for the philosophical question, but

13:34.080 --> 13:42.160
 how complicated are human beings, do you think? Like, can they be reduced to a kind of almost

13:42.160 --> 13:48.320
 like an object that moves and maybe has some basic intents? Or is there something, do we have to model

13:48.320 --> 13:54.800
 things like mood and general aggressiveness and time, I mean, all these kinds of human qualities

13:54.800 --> 14:00.080
 or like game theoretic qualities? Like, what's your sense? How complicated is,

14:00.080 --> 14:05.920
 how hard is the problem of human robot interaction? Yeah. Should we talk about what the problem of

14:05.920 --> 14:11.440
 human robot interaction is? Yeah, this is, what is human robot interaction? And then talk about

14:11.440 --> 14:17.200
 how that, yeah. So, and by the way, I'm going to talk about this very particular view of human

14:17.200 --> 14:23.360
 robot interaction, right, which is not so much on the social side or on the side of how you have

14:23.360 --> 14:27.360
 a good conversation with the robot, what should the robot's appearance be? It turns out that

14:27.360 --> 14:32.240
 if you make robots taller versus shorter, this has an effect on how people act with them. So,

14:32.240 --> 14:36.080
 I'm not, I'm not talking about that. But I'm talking about this very kind of narrow thing,

14:36.080 --> 14:44.320
 which is you take, if you want to take a task that a robot can do in isolation in a lab out

14:44.320 --> 14:49.600
 there in the world, but in isolation. And now you're asking, what does it mean for the robot

14:49.600 --> 14:55.120
 to be able to do this task for, presumably, what its actually end goal is, which is to help some

14:55.120 --> 15:04.560
 person? That ends up changing the problem in two ways. The first way it changes the problem is that

15:04.560 --> 15:10.880
 the robot is no longer the single agent acting. That you have humans who also take actions in

15:10.880 --> 15:15.840
 that same space, you know, cars navigating around people, robots around an office, navigating around

15:15.840 --> 15:22.240
 the people in that office. If I send the robot to over there in the cafeteria to get me a coffee,

15:22.240 --> 15:26.640
 then there's probably other people reaching for stuff in the same space. And so now you have

15:26.640 --> 15:31.520
 your robot and you're in charge of the actions that the robot is taking. Then you have these people

15:31.520 --> 15:37.200
 who are also making decisions and taking actions in that same space. And even if, you know,

15:37.200 --> 15:42.320
 the robot knows what it's, what it should do and all of that, just coexisting with these people,

15:42.320 --> 15:48.160
 right, kind of getting the actions, the gel well to mesh well together. That's sort of the kind of

15:48.160 --> 15:55.280
 problem number one. And then there's problem number two, which is, goes back to this notion of,

15:56.480 --> 16:02.560
 if I'm a programmer, I can specify some objective for the robot to go off and optimize and specify

16:02.560 --> 16:10.800
 the task. But if I put the robot in your home, presumably, you might have your own opinions

16:10.800 --> 16:14.880
 about, well, okay, I want my house clean, but how do I want it cleaned? And how should robot,

16:14.880 --> 16:19.520
 how close to me it should come and all of that. And so I think those are the two differences

16:19.520 --> 16:26.080
 that you have. You're acting around people and you, what you should be optimizing for should

16:26.080 --> 16:30.720
 satisfy the preferences of that end user, not of your programmer who programmed you.

16:30.720 --> 16:36.000
 Yeah. And the preferences thing is tricky. So figuring out those preferences, be able to

16:36.000 --> 16:41.120
 interactively adjust, to understand what the human is doing. So it really boils down to be

16:41.120 --> 16:45.840
 understanding humans in order to interact with them and in order to please them.

16:45.840 --> 16:53.360
 Right. So why is this hard? Yeah. Why is understanding humans hard? So I think

16:55.040 --> 17:00.080
 there's two tasks about understanding humans that in my mind are very, very similar, but not

17:00.080 --> 17:05.520
 everyone agrees. So there's the task of being able to just anticipate what people will do.

17:05.520 --> 17:09.520
 We all know that cars need to do this, right? We all know that, well, if I navigate around some

17:09.520 --> 17:14.320
 people, the robot has to get some notion of, okay, where, where is this person going to be?

17:15.280 --> 17:19.040
 So that's kind of the prediction side. And then there's what you are saying,

17:19.040 --> 17:23.280
 satisfying the preferences, right? So adapting to the person's preferences, knowing what to

17:23.280 --> 17:28.000
 optimize for, which is more this inference side, this, what is, what does this person want?

17:28.000 --> 17:35.040
 What is their intent? What are their preferences? And to me, those kind of go together because I

17:35.040 --> 17:42.160
 think that in, if you, at the very least, if you can understand, if you can look at human behavior

17:42.160 --> 17:47.280
 and understand what it is that they want, then that's sort of the key enabler to being able

17:47.280 --> 17:52.800
 to anticipate what they'll do in the future. Because I think that, you know, we're not arbitrary,

17:52.800 --> 17:57.200
 we make these decisions that we make, we act in the way we do, because we're trying to achieve

17:57.200 --> 18:03.200
 certain things. And so I think that's the relationship between them. Now, how complicated do these

18:03.200 --> 18:13.440
 models need to be in order to be able to understand what people want? So we've gotten a long way in

18:13.440 --> 18:17.920
 robotics with something called the inverse reinforcement learning, which is the notion of

18:17.920 --> 18:20.960
 someone acts demonstrates what, how they want the thing done.

18:20.960 --> 18:24.400
 What is an inverse reinforcement learning? You briefly said it.

18:24.400 --> 18:33.120
 Right. So it's, it's the problem of take human behavior and infer reward function from this,

18:33.120 --> 18:36.560
 figure out what it is that that behavior is optimal or respect to.

18:37.280 --> 18:41.520
 And it's a great way to think about learning human preferences in the sense of, you know,

18:41.520 --> 18:47.520
 you have a car and the person can drive it. And then you can say, well, okay, I can actually

18:47.520 --> 18:54.560
 learn what the person is optimizing for. I can learn their driving style, or you can, you can

18:54.560 --> 18:59.280
 have people demonstrate how they want the house clean. And then you can say, okay, this is,

18:59.280 --> 19:04.000
 this is, I'm getting the tradeoffs that they're, that they're making, I'm getting the preferences

19:04.000 --> 19:10.240
 that they want out of this. And so we've been successful in robotics somewhat with this.

19:10.240 --> 19:16.560
 And it's, it's based on a very simple model of human behavior is remarkably simple, which is

19:16.560 --> 19:21.840
 that human behavior is optimal with respect to whatever it is that people want, right?

19:21.840 --> 19:25.760
 So you make that assumption and now you can kind of inverse through that's why it's called inverse,

19:25.760 --> 19:29.680
 well, really optimal control, but, but also inverse reinforcement learning.

19:30.480 --> 19:37.520
 So this is based on a utility maximization in economics, right? So back in the 40s,

19:38.240 --> 19:44.800
 von Neumann Morgenstein, we're like, okay, people are making choices by maximizing utility, go.

19:44.800 --> 19:54.480
 And then in the late 50s, we had loose and shepherd come in and say, people are a little

19:54.480 --> 20:00.080
 bit noisy and approximate in that process. So they might choose something kind of

20:00.720 --> 20:07.680
 stochastically with probability proportional to how much utility something has. There's a bit

20:07.680 --> 20:13.600
 of noise in there. This has translated into robotics and something that we call Boltzmann

20:13.600 --> 20:17.760
 rationality. So it's a kind of an evolution of the inverse reinforcement learning that

20:17.760 --> 20:24.080
 accounts for, for human noise. And we've had some success with that too, for these tasks where it

20:24.080 --> 20:30.800
 turns out people act noisily enough that you can't just do vanilla, the vanilla version. Ah, you

20:30.800 --> 20:37.760
 can account for noise and still infer what, what they seem to want based on this. Then now we're

20:37.760 --> 20:44.320
 hitting tasks where that's no not enough. And what's, what, what are examples, what are examples?

20:44.320 --> 20:48.800
 So imagine you're trying to control some robot that's, that's fairly complicated. You're trying

20:48.800 --> 20:53.520
 to control a robot arm, because maybe you're a patient with a motor impairment, and you have

20:53.520 --> 20:58.880
 this wheelchair mounted arm, and you're trying to control it around. Or one test that we've looked

20:58.880 --> 21:04.880
 at with Sergey is, and our students did is a lunar lander. So just, I don't know if you know

21:04.880 --> 21:10.320
 this Atari game, it's called lunar lander. It's really hard. People really suck at landing the

21:10.320 --> 21:15.360
 thing. Mostly they just crash it left and right. Okay, so this is the kind of task. Imagine you're

21:15.360 --> 21:21.360
 trying to provide some assistance to a person operating such, such a robot, where you want the

21:21.360 --> 21:25.280
 kind of the autonomy that they can figure out what it is that you're trying to do and help you do it.

21:26.560 --> 21:33.520
 It's really hard to do that for, say, lunar lander, because people are all over the place.

21:33.520 --> 21:38.640
 And so they seem much more noisy than really irrational. That's an example of a task where

21:38.640 --> 21:44.880
 these models are kind of failing us. And it's not surprising because, so we, you know, we talked

21:44.880 --> 21:52.080
 about the forties utility late fifties, sort of noisy, then the seventies came and behavioral

21:52.080 --> 21:57.440
 economics started being a thing where people are like, no, no, no, no, no, people are not

21:57.440 --> 22:05.760
 rational. People are messy and emotional and irrational and have all sorts of heuristics

22:05.760 --> 22:11.520
 that might be domain specific. And they're just, they're just a mess. So, so what do, so what does

22:11.520 --> 22:17.920
 my robot do to understand what you want? And it's a very, it's very, that's why it's complicated.

22:17.920 --> 22:23.440
 It's, you know, for the most part, we get away with pretty simple models until we don't. And then

22:23.440 --> 22:30.880
 the question is, what do you do then? And I have days when I wanted to, you know, pack my bags and

22:30.880 --> 22:36.800
 go home and switch jobs because it's just, it feels really daunting to make sense of human behavior

22:36.800 --> 22:41.600
 enough that you can reliably understand what people want, especially as, you know, robot

22:41.600 --> 22:46.640
 capabilities will continue to get developed. You'll get these systems that are more and more

22:46.640 --> 22:50.080
 capable of all sorts of things. And then you really want to make sure that you're telling them the

22:50.080 --> 22:54.800
 right thing to do. What is that thing? Well, read it in human behavior.

22:55.920 --> 23:01.280
 So if I just sat here quietly and tried to understand something about you by listening to you

23:01.280 --> 23:08.240
 talk, it would be harder than if I got to say something and ask you and interact and control.

23:09.680 --> 23:13.600
 Can you, can the robot help its understanding of the human by

23:13.600 --> 23:21.920
 influencing the behavior by actually acting? Yeah, absolutely. So one of the things that's

23:21.920 --> 23:31.840
 been exciting to me lately is this notion that when you try to think of the robotics problem as,

23:31.840 --> 23:36.400
 okay, I have a robot and it needs to optimize for whatever it is that a person wants it to

23:36.400 --> 23:44.560
 optimize as opposed to maybe what a programmer said. That problem we think of as a human robot

23:44.560 --> 23:52.160
 collaboration problem in which both agents get to act in which the robot knows less than the human

23:52.160 --> 23:57.040
 because the human actually has access to, you know, at least implicitly to what it is that they want.

23:57.040 --> 24:02.160
 They can't write it down, but they can, they can talk about it. They can give all sorts of signals,

24:02.160 --> 24:08.080
 they can demonstrate. But the robot doesn't need to sit there and passively observe human

24:08.080 --> 24:13.600
 behavior and try to make sense of it. The robot can act too. And so there's these information

24:13.600 --> 24:20.960
 gathering actions that the robot can take to solicit responses that are actually informative.

24:20.960 --> 24:24.960
 So for instance, this is not for the purpose of assisting people, but with kind of back to

24:24.960 --> 24:31.760
 coordinating with people in cars and all of that. One thing that Dorsa did was,

24:31.760 --> 24:39.520
 so we were looking at cars being able to navigate around people and you might not know exactly the

24:39.520 --> 24:44.640
 driving style of a particular individual that's next to you, but you want to change lanes in front

24:44.640 --> 24:51.040
 of them. Navigating around other humans inside cars? Yeah, good clarification question. So

24:52.800 --> 24:58.160
 you have an autonomous car and it's trying to navigate the road around human driven vehicles.

24:58.160 --> 25:03.040
 Similar things, ideas apply to pedestrians as well, but let's just take human driven vehicles.

25:03.040 --> 25:09.760
 So now you're trying to change a lane. Well, you could be trying to infer this driving style of

25:09.760 --> 25:14.240
 this person next to you. You'd like to know if they're in particular, if they're sort of aggressive

25:14.240 --> 25:21.280
 or defensive, if they're going to let you kind of go in or if they're going to not. And it's very

25:21.280 --> 25:28.160
 and it's very difficult to just, if you think that if you want to hedge your bets and say,

25:28.160 --> 25:32.320
 maybe they're actually pretty aggressive, I shouldn't try this. You kind of end up driving

25:32.320 --> 25:38.160
 next to them and driving next to them, right? And then you don't know because you're not actually

25:38.160 --> 25:42.000
 getting the observations that you get away. Someone drives when they're next to you

25:42.880 --> 25:47.280
 and they just need to go straight. It's kind of the same regardless of their aggressive or defensive.

25:47.280 --> 25:55.040
 And so you need to enable the robot to reason about how it might actually be able to gather

25:55.040 --> 25:59.280
 information by changing the actions that it's taking. And then the robot comes up with these

25:59.280 --> 26:05.040
 cool things where it kind of nudges towards you and then sees if you're going to slow down or not.

26:05.040 --> 26:08.640
 Then if you slow down, it sort of updates its model of you and says, oh, okay,

26:09.440 --> 26:14.160
 you're more on the defensive side. So now I can actually like that's a fascinating dance.

26:14.160 --> 26:21.280
 That's so cool that you could use your own actions to gather information. That feels like a

26:21.280 --> 26:26.560
 totally open, exciting new world of robotics. I mean, how many people are even thinking about

26:26.560 --> 26:33.440
 that kind of thing? A handful of us? It's rare because it's actually leveraging human. I mean,

26:33.440 --> 26:40.800
 most roboticists, I've talked to a lot of colleagues and so on, are kind of being honest, kind of

26:40.800 --> 26:48.320
 afraid of humans. Because they're messy and complicated, right? I understand. Going back

26:48.320 --> 26:52.320
 to what we were talking about earlier, right now, we're kind of in this dilemma of, okay,

26:52.320 --> 26:56.160
 there are tasks that we can just assume people are approximately rational for and we can figure

26:56.160 --> 26:59.040
 out what they want. We can figure out their goals. We can figure out their driving styles,

26:59.040 --> 27:04.720
 whatever. Cool. There are these tasks that we can't. So what do we do, right? Do we pack our bags

27:04.720 --> 27:13.520
 and go home? I've had a little bit of hope recently. And I'm kind of doubting myself,

27:13.520 --> 27:18.640
 because what do I know that 50 years of behavioral economics hasn't figured out?

27:19.360 --> 27:24.400
 But maybe it's not really in contradiction with the way that field is headed. But basically,

27:24.400 --> 27:30.320
 one thing that we've been thinking about is instead of kind of giving up and saying people

27:30.320 --> 27:38.000
 are too crazy and irrational for us to make sense of them, maybe we can give them a bit the benefit

27:38.000 --> 27:44.640
 of the doubt. And maybe we can think of them as actually being relatively rational, but just under

27:44.640 --> 27:52.000
 different assumptions about the world, about how the world works, about, you know, they don't have,

27:52.000 --> 27:56.880
 we, when we think about rationality, implicit assumption is, or they're rational under all

27:56.880 --> 28:01.600
 the same assumptions and constraints as the robot, right? This is the state of the world,

28:01.600 --> 28:05.360
 that's what they know. This is the transition function, that's what they know. This is the

28:05.360 --> 28:12.160
 horizon, that's what they know. But maybe the kind of this difference, the way, the reason they can

28:12.160 --> 28:19.280
 seem a little messy and hectic, especially to robots, is that perhaps they just make different

28:19.280 --> 28:24.880
 assumptions or have different beliefs. I mean, that's another fascinating idea that

28:24.880 --> 28:32.240
 this, our kind of anecdotal desire to say that humans are irrational, perhaps grounded in behavioral

28:32.240 --> 28:37.680
 economics, is that we just don't understand the constraints and the rewards under which they

28:37.680 --> 28:42.800
 operate. And so our goal shouldn't be to throw our hands up and say they're irrational, is to say,

28:43.520 --> 28:48.880
 let's try to understand what are the constraints. What it is that they must be assuming that makes

28:48.880 --> 28:54.800
 this behavior make sense. Good life lesson, right? Good life lesson. That's true. It's just outside

28:54.800 --> 28:59.360
 of robotics. That's just good to, that's communicating with humans. That's just a good,

28:59.360 --> 29:05.440
 assume that you just don't sort of empathy, right? It's a... This is maybe there's something you're

29:05.440 --> 29:09.200
 missing and you, and it's, you know, it especially happens to robots because they're kind of dumb

29:09.200 --> 29:13.360
 and they don't know things. And oftentimes people are sort of supra rational and that they actually

29:13.360 --> 29:19.040
 know a lot of things that robots don't. Sometimes like with the lunar lander, the robot, you know,

29:19.040 --> 29:26.800
 knows much more. So it turns out that if you try to say, look, maybe people are operating this thing,

29:26.800 --> 29:32.880
 but assuming a much more simplified physics model, because they don't get the complexity of this kind

29:32.880 --> 29:37.760
 of craft or the robot arm with seven degrees of freedom with these inertias and whatever.

29:37.760 --> 29:43.120
 So maybe they have this intuitive physics model, which is not, you know, this notion of intuitive

29:43.120 --> 29:47.280
 physics is something that you studied actually in cognitive science, was like Josh Denenbaum,

29:47.280 --> 29:55.120
 Tom Griffith's work on this stuff. And what we found is that you can actually try to figure out what

29:56.560 --> 30:05.200
 physics model kind of best explains human actions. And then you can use that to sort of correct

30:05.200 --> 30:11.200
 what it is that they're commanding the craft to do. So they might be sending the craft somewhere,

30:11.200 --> 30:15.200
 but instead of executing that action, you can sort of take a step back and say,

30:15.200 --> 30:20.400
 according to their intuitive, if the world worked according to their intuitive physics model,

30:21.520 --> 30:25.200
 where do they think that the craft is going? Where are they trying to send it to?

30:25.840 --> 30:30.240
 And then you can use the real physics, right, the inverse of that to actually figure out what

30:30.240 --> 30:34.640
 you should do so that you do that instead of where they were actually sending you in the real world.

30:34.640 --> 30:41.760
 And I kid you not, it worked. People land the damn thing in between the two flags and all that.

30:42.400 --> 30:48.160
 So it's not conclusive in any way, but I'd say it's evidence that, yeah, maybe we're kind of

30:48.160 --> 30:52.400
 underestimating humans in some ways when we're giving up and saying, yeah, they're just crazy

30:52.400 --> 30:57.920
 noisy. So then you try to explicitly try to model the kind of worldview that they have?

30:57.920 --> 31:02.960
 That they have. That's right. That's right. There's not too, I mean, there's things in behavioral

31:02.960 --> 31:07.920
 economics, too, that, for instance, have touched upon the planning horizon. So there's this idea

31:07.920 --> 31:12.160
 that there's bounded rationality, essentially, and the idea that, well, maybe we work on their

31:12.160 --> 31:18.320
 computational constraints. And I think kind of our view recently has been, take the Bellman update

31:18.960 --> 31:23.680
 in AI and just break it in all sorts of ways by saying, state? No, no, no, the person doesn't

31:23.680 --> 31:28.800
 get to see the real state. Maybe they're estimating somehow. Transition function? No, no, no, no, no.

31:28.800 --> 31:34.160
 Even the actual reward evaluation, maybe they're still learning about what it is that they want.

31:35.200 --> 31:40.720
 Like, when you watch Netflix and you have all the things and then you have to pick something,

31:41.680 --> 31:48.800
 imagine that the AI system interpreted that choice as this is the thing you prefer to see.

31:48.800 --> 31:52.000
 How are you going to know? You're still trying to figure out what you like, what you don't like,

31:52.000 --> 31:56.640
 et cetera. So I think it's important to also account for that. So it's not irrationality,

31:56.640 --> 31:59.920
 because they're doing the right thing under the things that they know.

31:59.920 --> 32:05.440
 Yeah, that's brilliant. You mentioned recommender systems. What kind of, and we were talking about

32:05.440 --> 32:13.760
 human robot interaction, what kind of problem spaces are you thinking about? So is it robots,

32:13.760 --> 32:19.280
 like wheeled robots with autonomous vehicles? Is it object manipulation? Like, when you think

32:19.280 --> 32:24.800
 about human robot interaction in your mind, and maybe, I'm sure you can speak for the entire

32:24.800 --> 32:29.680
 community of human robot interaction. But like, what are the problems of interest here?

32:33.120 --> 32:40.800
 You know, I kind of think of open domain dialogue as human robot interaction,

32:40.800 --> 32:45.360
 and that happens not in the physical space, but it could just happen in the virtual space.

32:46.320 --> 32:50.800
 So where's the boundaries of this field for you when you're thinking about the things we've

32:50.800 --> 33:02.000
 been talking about? Yeah, so I tried to find kind of underlying, I don't know what to even call

33:02.000 --> 33:07.520
 them. I get tried to work on, you know, I might call what I do, the kind of working on the foundations

33:07.520 --> 33:13.920
 of algorithmic human robot interaction and trying to make contributions there. And it's important

33:13.920 --> 33:20.000
 to me that whatever we do is actually somewhat domain agnostic when it comes to is it about

33:20.000 --> 33:29.840
 you know, autonomous cars? Or is it about quadrotors? Or is it about the same underlying

33:29.840 --> 33:32.800
 principles apply? Of course, when you're trying to get a particular domain to work,

33:32.800 --> 33:37.040
 you usually have to do some extra work to adapt that to that particular domain. But these things

33:37.040 --> 33:43.360
 that we were talking about around, well, you know, how do you model humans? It turns out that a lot

33:43.360 --> 33:49.440
 of systems need to core benefit from a better understanding of how human behavior relates

33:49.440 --> 33:56.080
 to what people want and need to predict human behavior, physical robots of all sorts and beyond

33:56.080 --> 34:00.880
 that. And so I used to do manipulation, I used to be, you know, picking up stuff and then I was

34:00.880 --> 34:07.280
 picking up stuff with people around. And now it's sort of very broad when it comes to the application

34:07.280 --> 34:14.400
 level. But in a sense, very focused on, okay, how does the problem need to change? How do the

34:14.400 --> 34:21.280
 algorithms need to change when we're not doing a robot by itself, you know, emptying the dishwasher,

34:21.280 --> 34:27.120
 but we're stepping outside of that. I thought that popped into my head just now. On the game

34:27.120 --> 34:31.600
 theoretic side of things, you said this really interesting idea of using actions to gain more

34:31.600 --> 34:42.560
 information. But if we think a sort of game theory, the humans that are interacting with you,

34:42.560 --> 34:53.680
 with you, the robot, I'm taking the identity of the robot. Yeah, they also have a world model of you.

34:55.520 --> 35:00.800
 And you can manipulate that. I mean, if we look at autonomous vehicles, people have a certain

35:00.800 --> 35:08.480
 viewpoint. You said with the kids, people see Alexa as in a certain way. Is there some value

35:08.480 --> 35:16.240
 in trying to also optimize how people see you as a robot? Or is that a little too far

35:18.000 --> 35:21.280
 away from the specifics of what we can solve right now?

35:23.520 --> 35:30.880
 Both, right? So it's really interesting. And we've seen a little bit of progress on this problem,

35:30.880 --> 35:36.880
 on pieces of this problem. So you can, again, it kind of comes down to how complicated

35:36.880 --> 35:44.000
 is the human model need to be. But in one piece of work that we were looking at, we just said,

35:44.000 --> 35:51.520
 okay, there's these parameters that are internal to the robot and what the robot is about to do,

35:51.520 --> 35:56.560
 or maybe what objective, what driving style the robot has or something like that. And what we're

35:56.560 --> 36:00.320
 going to do is we're going to set up a system where part of the state is the person's belief

36:00.320 --> 36:08.240
 over those parameters. And now when the robot acts, that the person gets new evidence about

36:08.240 --> 36:14.160
 this robot internal state. And so they're updating their mental model of the robot, right? So if they

36:14.160 --> 36:19.040
 see a card that sort of cuts someone off, they're like, oh, that's an aggressive card. They know more,

36:19.040 --> 36:25.040
 right? If they see sort of a robot head towards a particular door, they're like, oh, the robot's

36:25.040 --> 36:29.440
 trying to get to that door. So this thing that we have to do with humans to try to understand their

36:29.440 --> 36:35.760
 goals and intentions, humans are inevitably going to do that to robots. And then that raises this

36:35.760 --> 36:39.200
 interesting question that you asked, which is, can we do something about that? This is going to

36:39.200 --> 36:44.320
 happen inevitably, but we can sort of be more confusing or less confusing to people. And it

36:44.320 --> 36:50.240
 turns out you can optimize for being more informative and less confusing. If you have an

36:50.240 --> 36:54.320
 understanding of how your actions are being interpreted by the human, how they're using

36:54.320 --> 37:00.640
 these actions to update their belief. And honestly, all we did is just base rule. Basically, okay,

37:01.440 --> 37:05.280
 the person has a belief, they see an action, they make some assumptions about how the robot

37:05.280 --> 37:08.880
 generates its actions, presumably as being rational, because robots are rational,

37:08.880 --> 37:17.040
 it's reasonable to assume that about them. And then they incorporate that new piece of evidence,

37:17.040 --> 37:21.840
 the Bayesian sense in their belief, and they obtain a posterior. And now the robot

37:21.840 --> 37:26.320
 is trying to figure out what actions to take, such that it steers the person's belief to put

37:26.320 --> 37:31.040
 as much probability mass as possible on the correct, on the correct parameters.

37:31.040 --> 37:37.600
 So that's kind of a mathematical formalization of that. But my worry, and I don't know if you

37:37.600 --> 37:46.160
 want to go there with me, but I talk about this quite a bit. The kids talking to Alexa

37:46.160 --> 37:53.120
 disrespectfully worries me. I worry in general about human nature. Like I said, I grew up in

37:53.120 --> 37:58.880
 Soviet Union, World War II, I'm a Jew too, so with the Holocaust and everything. I just worry

37:58.880 --> 38:04.880
 about how we humans sometimes treat the other, the group that we call the other, whatever it is,

38:04.880 --> 38:11.200
 the human history, the group that's the other has been changed faces. But it seems like the robot

38:11.200 --> 38:20.160
 will be the other, the other, the next the other. And one thing is, it feels to me that robots don't

38:20.160 --> 38:27.040
 get no respect. They get shoved around shoved around. And is there one at the shallow level,

38:27.040 --> 38:33.280
 for a better experience, it seems that robots need to talk back a little bit. Like my intuition

38:33.280 --> 38:39.600
 says, I mean, most companies from sort of Roomba autonomous vehicle companies might not be so happy

38:39.600 --> 38:45.520
 with the idea that a robot has a little bit of an attitude. But I feel, it feels to me that

38:45.520 --> 38:50.640
 that's necessary to create a compelling experience. Like we humans don't seem to respect anything that

38:50.640 --> 39:01.280
 doesn't give us some attitude. Or like a mix of mystery and attitude and anger and that threatens

39:01.280 --> 39:07.040
 us subtly, maybe passive aggressively. I don't know. It seems like we humans yet need that.

39:07.040 --> 39:11.040
 Do you, what are your, is there something you have thoughts on this?

39:11.040 --> 39:20.320
 I'll give you two thoughts on it. One is, it's, we respond to, you know, someone being assertive,

39:21.120 --> 39:28.160
 but we also respond to someone being vulnerable. So I think robots, my first thought is that

39:28.160 --> 39:32.960
 robots get shoved around and bullied a lot, because they're sort of, you know, tempting and

39:32.960 --> 39:38.480
 they're sort of showing off or they appear to be showing off. And so I think going back to these

39:38.480 --> 39:43.200
 things we were talking about in the beginning of making robots a little more, a little more

39:43.200 --> 39:49.200
 expressive, a little bit more like, eh, that wasn't cool to do. And now I'm bummed, right?

39:49.920 --> 39:53.520
 I think that that can actually help because people can't help but anthropomorphize and

39:53.520 --> 39:58.720
 respond to that. Even that though the emotion being communicated is not in any way a real thing.

39:58.720 --> 40:01.920
 And people know that it's not a real thing because they know it's just a machine.

40:01.920 --> 40:08.880
 We're still, you know, we watch, there's this famous psychology experiment with little triangles

40:08.880 --> 40:14.560
 and kind of dots on a screen and a triangle is chasing the square and you get really angry

40:14.560 --> 40:19.920
 at the darn triangle because why is it not leaving the square alone? So that's, yeah, we can't help.

40:19.920 --> 40:27.360
 So that was the first thought. The vulnerability, that's really interesting. I think of like being

40:27.360 --> 40:34.880
 a, pushing back, being assertive as the only mechanism of getting, of forming a connection,

40:34.880 --> 40:39.440
 of getting respect, but perhaps vulnerability. Perhaps there's other mechanism that are less

40:39.440 --> 40:45.840
 threatening. Yeah. Well, I see, well, a little bit, yes. But then this other thing that we can

40:45.840 --> 40:50.240
 think about is, it goes back to what you were saying, that interaction is really game theoretic.

40:50.240 --> 40:54.080
 Right? So the moment you're taking actions in a space, the humans are taking actions in that

40:54.080 --> 40:59.040
 same space, but you have your own objective, which is, you know, you're a car, you need to get your

40:59.040 --> 41:04.160
 passenger to the destination. And then the human nearby has their own objective, which someone

41:04.160 --> 41:09.200
 overlaps with you, but not entirely. You're not interested in getting into an accident with

41:09.200 --> 41:13.280
 each other, but you have different destinations and you want to get home faster and they want to

41:13.280 --> 41:20.080
 get home faster. And that's a general sum game at that point. And so that's, I think that's what,

41:20.080 --> 41:30.240
 what, treating it as such is kind of a way we can step outside of this kind of mode that where you

41:30.240 --> 41:35.600
 try to anticipate what people do and you don't realize you have any influence over it, while

41:35.600 --> 41:40.960
 still protecting yourself because you're understanding that people also understand that they can

41:40.960 --> 41:46.640
 influence you. And it's just kind of back and forth is this negotiation, which is really,

41:46.640 --> 41:53.120
 really talking about different equilibria of a game. The very basic way to solve coordination

41:53.120 --> 41:56.960
 is to just make predictions about what people will do and then stay out of their way.

41:57.680 --> 42:02.000
 And that's hard for the reasons we talked about, which is how you have to understand people's

42:02.000 --> 42:07.040
 intentions implicitly, explicitly, who knows, but somehow you have to get enough of an understanding

42:07.040 --> 42:12.480
 of that to be able to anticipate what happens next. And so that's challenging. But then it's

42:12.480 --> 42:17.440
 further challenged by the fact that people change what they're do based on what you do,

42:17.440 --> 42:23.200
 because they don't, they don't plan an isolation either, right? So when you see cars trying to merge

42:23.200 --> 42:30.400
 on a highway and not succeeding, one of the reasons this can be is because you, you, they,

42:30.400 --> 42:35.760
 they look at traffic that keeps coming, they predict what these people are planning on doing,

42:35.760 --> 42:39.680
 which is to just keep going. And then they stay out of the way because there's not,

42:39.680 --> 42:45.680
 there's no feasible plan, right? Any plan would actually intersect with one of these

42:45.680 --> 42:52.640
 other people. So that's bad. So you get stuck there. So now kind of, if, if you start thinking

42:52.640 --> 42:59.360
 about it as no, no, no, actually, these people change what they do, depending on what the car

42:59.360 --> 43:05.680
 does, like if the car actually tries to kind of inch itself forward, they might actually slow down

43:05.680 --> 43:12.000
 and let the car in. And now take an advantage of that. Well, that, you know, that's kind of the

43:12.000 --> 43:17.680
 next level. We call this like this underactuated system idea where it's like an underactive system

43:17.680 --> 43:23.120
 robotics, but it's kind of, it's, you don't, you're influenced these other degrees of freedom,

43:23.120 --> 43:28.400
 but you don't get to decide what they do. I've, I've, I've somewhere seen you mention it, this,

43:28.400 --> 43:34.560
 the human element in this picture as underactuated. So, you know, you understand underactured

43:34.560 --> 43:42.640
 robotics is, you know, that you can't fully control the system. You can't go in arbitrary

43:42.640 --> 43:47.920
 directions in the configuration space under your control. Yeah, it's a very simple way of

43:47.920 --> 43:51.840
 underactuation where basically there's literally these degrees of freedom that you can control

43:51.840 --> 43:55.120
 and these degrees of freedom that you can't, but you influence them. And I think that's the

43:55.120 --> 44:00.560
 important part is that they don't do whatever, regardless of what you do, that what you do

44:00.560 --> 44:05.360
 influences what they end up doing. I just also like the, the poetry of calling human and robot

44:05.360 --> 44:12.160
 interaction an underactuated robotics problem. And you also mentioned sort of nudging. It seems

44:12.160 --> 44:17.040
 that they're, I don't know, I think about this a lot in the case of pedestrians have

44:17.040 --> 44:21.840
 collected hundreds of hours of videos. I like to just watch pedestrians and it seems that

44:22.720 --> 44:28.480
 it's a funny hobby. Yeah, it's weird because I learn a lot. I learn a lot about myself,

44:28.480 --> 44:35.680
 about our human behavior from watching pedestrians, watching people in their environment. Basically,

44:36.400 --> 44:42.320
 crossing the street is like you're putting your life on the line. You know, I don't know, tens of

44:42.320 --> 44:48.720
 millions of time in America every day is people are just like playing this weird game of chicken

44:48.720 --> 44:53.360
 when they cross the street, especially when there's some ambiguity about the right of way.

44:53.360 --> 44:59.760
 That has to do either with the rules of the road or with the general personality of the intersection

44:59.760 --> 45:06.160
 based on the time of day and so on. And this nudging idea, I don't, you know, it seems that

45:06.160 --> 45:11.600
 people don't even nudge. They just aggressively take, make a decision. Somebody, there's a runner

45:11.600 --> 45:18.080
 that gave me this advice. I sometimes run in the street and, you know, not in the street,

45:18.080 --> 45:23.120
 on the sidewalk. And he said that if you don't make eye contact with people when you're running,

45:23.120 --> 45:28.560
 they will all move out of your way. It's called civil inattention. Civil inattention. That's

45:28.560 --> 45:34.640
 the thing. Oh, wow. I need to look this up, but it works. What is that? My sense was if you communicate

45:35.440 --> 45:42.240
 like confidence in your actions that you're unlikely to deviate from the action that you're

45:42.240 --> 45:47.040
 following, that's a really powerful signal to others that they need to plan around your actions,

45:47.040 --> 45:53.200
 as opposed to nudging where you're sort of hesitantly, then the hesitation might communicate

45:53.200 --> 45:57.920
 that you're now, you're still in the dance and the game that they can influence with their own

45:57.920 --> 46:04.080
 actions. I've recently had a conversation with Jim Keller, who's a sort of this

46:05.920 --> 46:14.160
 legendary chip architect, but he also led the autopilot team for a while. And his intuition

46:14.160 --> 46:20.240
 that driving is fundamentally still like a ballistics problem. Like you can ignore the human

46:20.800 --> 46:27.040
 element that is just not hitting things. And you can kind of learn the right dynamics required

46:27.040 --> 46:32.080
 to do the merger and all those kinds of things. And then my sense is, and I don't know if I can

46:32.080 --> 46:38.400
 provide sort of definitive proof of this, but my sense is like an order of magnitude or more

46:38.400 --> 46:46.720
 difficult when humans are involved. Like it's not simply a object, a collision avoidance problem.

46:46.720 --> 46:51.360
 What's, where does your intuition, of course, nobody knows the right answer here, but where does

46:51.360 --> 46:57.040
 your intuition fall on the difficulty, fundamental difficulty of the driving problem when humans

46:57.040 --> 47:06.320
 are involved? Yeah. Good question. I have many opinions on this. Imagine downtown San Francisco.

47:06.320 --> 47:13.840
 Yeah. Yeah. It's crazy, busy, everything. Okay, now take all the humans out. No pedestrians,

47:13.840 --> 47:19.680
 no human driven vehicles, no cyclists, no people on little electric scooters zipping around, nothing.

47:20.960 --> 47:26.080
 I think we're done. I think driving at that point is done. We're done. There's nothing really that's

47:26.800 --> 47:32.800
 needs still needs to be solved about that. Well, let's pause there. I think I agree with you.

47:32.800 --> 47:39.920
 Like, and I think a lot of people that will hear will agree with that. But we need to sort of

47:39.920 --> 47:45.120
 internalize that idea. So what's the problem there? Because we might not quite yet be done with that,

47:45.120 --> 47:50.720
 because a lot of people kind of focus on the perception problem. A lot of people kind of

47:51.440 --> 47:57.840
 map autonomous driving into how close are we to solving being able to detect all the, you know,

47:57.840 --> 48:05.840
 the drivable area, the objects in the scene. Do you see that as a, how hard is that problem?

48:07.280 --> 48:11.840
 So your intuition there behind your statement was we might have not solved it yet, but we're

48:11.840 --> 48:18.000
 close to solving basically the perception problem. I think the perception problem, I mean, and by the

48:18.000 --> 48:24.000
 way, a bunch of years ago, this would not have been true. And a lot of issues in the space came

48:24.000 --> 48:28.560
 were coming from the fact that, oh, we don't really, you know, we don't know what's, what's where.

48:29.360 --> 48:35.760
 But I think it's fairly safe to say that at this point, although you could always improve on things

48:35.760 --> 48:40.240
 and all of that, you can drive through downtown San Francisco if there are no people around.

48:40.240 --> 48:46.240
 There's no really perception issues standing in your way there. I think perception is hard. But

48:46.240 --> 48:50.480
 yeah, it's we've made a lot of progress on the perceptions and I to undermine the difficulty

48:50.480 --> 48:54.720
 of the problem. I think everything about robotics is really difficult, of course. I think that,

48:54.720 --> 48:59.760
 you know, the, the, the planning problem, the control problem, all very difficult. But I think

48:59.760 --> 49:06.000
 what's, what makes it really, kind of, yeah, it might be, I mean, you know, I, and I picked Anton

49:06.000 --> 49:12.960
 San Francisco, I, adapting to, well, now it's snowing, now it's no longer snowing, now it's

49:12.960 --> 49:21.920
 slippery in this way, now it's the dynamic sport. Could, I could imagine being, being still somewhat

49:21.920 --> 49:28.160
 challenging. But no, the thing that I think worries us in our tuition is not good there is

49:28.160 --> 49:34.400
 the perception problem at the edge cases. Sort of downtown San Francisco, the nice thing,

49:35.200 --> 49:40.880
 it's not actually, it may not be a good example because, because you know what to, what you're

49:40.880 --> 49:44.320
 getting from, well, there's like crazy construction zones and all that. Yeah, but the thing is,

49:44.320 --> 49:49.200
 you're traveling at slow speeds, so like it doesn't feel dangerous. To me, what feels dangerous is

49:49.200 --> 49:56.240
 highway speeds, when everything is, to us humans, super clear. Yeah, I'm assuming LiDAR here, by

49:56.240 --> 50:00.720
 the way. I think it's kind of irresponsible to not use LiDAR. That's just my personal opinion.

50:03.440 --> 50:07.280
 I mean, depending on your use case, but I think like, you know, if you, if you have the opportunity

50:07.280 --> 50:13.360
 to use LiDAR, then a lot, in a lot of cases, you might not. Good, your intuition makes more sense

50:13.360 --> 50:19.200
 now. So you don't take vision. I just really just don't know enough to say, well, vision alone,

50:20.000 --> 50:24.800
 what, you know, what's like, there's a lot of, how many cameras do they have? Is it how are you

50:24.800 --> 50:29.200
 using them? I don't know. There's all, there's a sort of sorts of details. I imagine there's stuff

50:29.200 --> 50:34.320
 that's really hard to actually see, you know, how do you deal with, with exactly what you were

50:34.320 --> 50:39.760
 saying, stuff that people would see that, that, that you don't. I think I have more, my intuition

50:39.760 --> 50:46.000
 comes from systems that can actually use LiDAR as well. Yeah. And until we know for sure, it's

50:46.000 --> 50:51.120
 makes sense to be using LiDAR. That's kind of the safety focus. But then the sort of the,

50:52.000 --> 50:58.720
 I also sympathize with the Elon Musk statement of LiDAR is a crutch. It's, it's, it's a,

50:58.720 --> 51:06.240
 it's a fun notion to think that the things that work today is a crutch for the invention of the

51:06.240 --> 51:13.760
 things that will work tomorrow, right? Like it, it's, it's kind of true in the sense that if,

51:13.760 --> 51:18.160
 you know, we want to stick to the comfort that you see this in academic and research settings all

51:18.160 --> 51:24.080
 the time, the things that work, uh, force you to not explore outside, think outside the box. I

51:24.080 --> 51:29.440
 think that happens all the time. The problem is in the safety critical systems. You kind of want

51:29.440 --> 51:34.640
 to stick with the things that work. Uh, so it's, it's a, it's an interesting and difficult trade

51:34.640 --> 51:40.160
 off in the, in the, in the case of real world sort of safety critical robotic systems. But

51:41.760 --> 51:49.760
 so your intuition is just to clarify how, I mean, how hard is this human element?

51:49.760 --> 51:55.360
 Uh, for, like how hard is driving when this human element is involved? Are we

51:57.440 --> 52:03.680
 years decades away from solving it? But perhaps actually the year isn't the, the thing I'm asking,

52:03.680 --> 52:09.040
 it doesn't matter what the timeline is, but do you think we're, uh, how many breakthroughs

52:09.040 --> 52:14.720
 are we away from in solving the human robot interaction problem to get this, to get this

52:14.720 --> 52:24.080
 right? I think it, in a sense, it really depends. I think that in, we were talking about how well,

52:24.080 --> 52:28.560
 look, it's really hard because, and this is what people do is hard. And on top of that,

52:28.560 --> 52:36.880
 playing the game is hard. But I think we sort of have the fundamental, some of the fundamental

52:36.880 --> 52:43.360
 understanding for that. And then you already see that these systems are being deployed in the real

52:43.360 --> 52:53.600
 world, you know, even, even driverless, because I think now a few companies that don't have a

52:53.600 --> 53:01.200
 driver in the car in some small areas. I got a chance to, I went to Phoenix and I, I shot a video

53:01.200 --> 53:07.760
 with Waymo. I need to get that video out. People have been giving me slack, but there's incredible

53:07.760 --> 53:11.760
 engineering work being done there. And it's one of those other seminal moments for me in my life

53:11.760 --> 53:17.920
 to be able to, it sounds silly, but to be able to drive without a, without a ride, sorry, without

53:17.920 --> 53:26.720
 a driver in the seat. I mean, it was an incredible robotics. I was driven by a robot without being

53:26.720 --> 53:32.560
 able to take over, without being able to take the steering wheel. That's a magical, that's a

53:32.560 --> 53:37.440
 magical moment. So in that regard, in those domains, at least for like Waymo, they're, they're,

53:37.440 --> 53:43.440
 they're solving that human, there's, I mean, they were, they're going, I mean, it felt fast,

53:43.440 --> 53:47.760
 because you're like freaking out at first. That was, this is my first experience, but it's going

53:47.760 --> 53:53.280
 like the speed limit, right? 30, 40, whatever it is. And there's humans and it deals with them

53:53.280 --> 53:58.080
 quite well. It detects them and, and it negotiates the intersections, the left turns and all that.

53:58.080 --> 54:02.560
 So at least in those domains, it's solving them. The open question for me is like,

54:02.560 --> 54:10.000
 like, how quickly can we expand? You know, that's the, you know, outside of the weather conditions,

54:10.000 --> 54:14.480
 all of those kinds of things, how quickly can we expand to like cities like San Francisco?

54:14.480 --> 54:19.520
 Yeah. And I wouldn't say that it's just, you know, now it's just pure engineering and it's

54:19.520 --> 54:26.240
 probably the, I mean, and by the way, I'm speaking kind of very generally here as hypothesizing,

54:26.240 --> 54:35.600
 but I think that, that there are successes and yet no one is everywhere out there. So that seems to

54:35.600 --> 54:41.520
 suggest that things can be expanded and can be scaled. And we know how to do a lot of things,

54:41.520 --> 54:47.680
 but there's still probably, you know, new algorithms or modified algorithms that, that

54:47.680 --> 54:53.760
 you still need to put in there as you, as you learn more and more about new challenges that

54:53.760 --> 54:58.800
 get, you get faced when. How much of this problem do you think can be learned through end to end?

54:58.800 --> 55:03.680
 There's the success of machine learning and reinforcement learning. How much of it can be

55:03.680 --> 55:09.040
 learned from sort of data from scratch and how much, which most of the success of autonomous

55:09.040 --> 55:15.600
 vehicle systems have a lot of heuristics and rule based stuff on top, like human expertise in

55:16.880 --> 55:22.160
 injected, forced into the system to make it work. What's your, what's your sense? How much,

55:22.160 --> 55:32.160
 what's the, what will be the role of learning in the near term? I think, I, I think on the one hand

55:32.160 --> 55:39.600
 that learning is inevitable here, right? I think on the other hand that when people characterize

55:39.600 --> 55:46.640
 the problem as it's a bunch of rules that some people wrote down versus it's an end to end

55:46.640 --> 55:54.640
 Darrell system or imitation learning, then maybe there's kind of something missing from, maybe

55:54.640 --> 56:04.240
 that's, that's more. So for instance, I think a very, very useful tool in this sort of problem,

56:04.240 --> 56:11.680
 both in how to generate the car's behavior and robots in general, and how to model human beings

56:11.680 --> 56:16.720
 is actually planning, search optimization, right? So robotics is a sequential decision

56:16.720 --> 56:28.160
 making problem. And when, when a robot can figure out on its own how to achieve its goal without

56:28.160 --> 56:32.080
 hitting stuff and all that stuff, right? All the good stuff for motion planning one on one,

56:32.960 --> 56:39.360
 I think of that as very much AI, not this is some rule or some, there's nothing rule based

56:39.360 --> 56:43.040
 on that, right? It's just you're, you're searching through a space and figuring out, are you optimizing

56:43.040 --> 56:48.960
 through a space and figure out what seems to be the right thing to do. And I think it's hard to

56:48.960 --> 56:54.880
 just do that because you need to learn models of the world. And I think it's hard to just do the

56:54.880 --> 56:59.840
 learning part where you don't, you know, you don't bother with any of that, because then you're

56:59.840 --> 57:04.480
 saying, well, I could do imitation, but then when I go off distribution, I'm really screwed,

57:04.480 --> 57:09.680
 or you can say, I can do reinforcement learning, which adds a lot of robustness,

57:09.680 --> 57:13.840
 but then you have to do either reinforcement learning in the real world, which sounds a

57:13.840 --> 57:19.520
 little challenging, or that trial and error, you know, or you have to do reinforcement learning

57:19.520 --> 57:25.200
 in simulation. And then that means, well, guess what, you need to model things, at least to

57:26.240 --> 57:31.520
 model people, model the world enough that you, you know, whatever policy you get of that is

57:31.520 --> 57:36.560
 like actually fine to roll out in the world and do some additional learning there. So

57:37.360 --> 57:43.280
 Do you think simulation, by the way, just a quick tangent has a role in the human robot

57:43.280 --> 57:48.320
 interaction space? Like, is it useful? It seems like humans, everything we've been talking about

57:48.320 --> 57:53.520
 are difficult to model and simulate. Do you think simulation has a role in this space?

57:53.520 --> 58:03.360
 I do. I think so, because you can take models and train with them ahead of time, for instance,

58:04.080 --> 58:10.400
 you can. But the models, sorry to interrupt, the models are sort of human constructed or learned?

58:10.400 --> 58:20.400
 I think they have to be a combination, because if you get some human data, and then you say,

58:20.400 --> 58:24.880
 this is going to be my model of the person, what are for simulation and training or for

58:24.880 --> 58:29.040
 just deployment time, and that's what I'm planning with as my model of how people work.

58:29.040 --> 58:35.520
 Regardless, if you take some data, and you don't assume anything else, and you just say, okay,

58:37.040 --> 58:42.400
 this is some data that I've collected, let me fit a policy to help people work based on that.

58:42.400 --> 58:46.000
 What does to happen is, you collected some data and some distribution,

58:46.000 --> 58:54.320
 and then now your robot sort of computes a best response to that. It's like, what should I do

58:54.320 --> 59:00.240
 if this is how people work, and easily goes off of distribution, where that model that you've built

59:00.240 --> 59:05.520
 of the human completely sucks, because out of distribution, you have no idea. If you think of

59:06.080 --> 59:11.680
 all the possible policies, and then you take only the ones that are consistent with the human data

59:11.680 --> 59:17.440
 that you've observed, that still leads a lot of things could happen outside of that distribution

59:17.440 --> 59:23.600
 where you're confident and you know what's going on. By the way, I've gotten used to this terminology

59:23.600 --> 59:30.720
 of out of distribution, but it's such a machine learning terminology, because it kind of assumes,

59:30.720 --> 59:37.840
 so distribution is referring to the data that you've seen. The set of states that you encountered.

59:37.840 --> 59:42.960
 They've encountered so far at training time, but it kind of also implies that there's a nice

59:44.480 --> 59:53.840
 statistical model that represents that data. Out of distribution, it raises to me philosophical

59:53.840 --> 1:00:00.800
 questions of how we humans reason out of distribution, reason about things that are completely

1:00:00.800 --> 1:00:08.560
 we haven't seen before. What we're talking about here is how do we reason about what other people

1:00:08.560 --> 1:00:15.120
 do in situations where we haven't seen them? Somehow we just magically navigate that. I can

1:00:15.760 --> 1:00:22.320
 anticipate what will happen in situations that are even novel in many ways. I have a pretty good

1:00:22.320 --> 1:00:26.720
 intuition for I don't always get it right, but I might be a little uncertain and so on. I think

1:00:26.720 --> 1:00:36.720
 it's this that if you just rely on data, there's just too many possibilities, too many policies

1:00:36.720 --> 1:00:40.480
 out there that fit the data. By the way, it's not just state, it's really history of state,

1:00:40.480 --> 1:00:44.240
 because to really be able to anticipate what the person will do, it depends on what they've

1:00:44.240 --> 1:00:50.000
 been doing so far, because that's the information you need to at least implicitly say, this is the

1:00:50.000 --> 1:00:54.080
 kind of person that this is, this is probably what they're trying to do. You're trying to map

1:00:54.080 --> 1:00:59.760
 history of states to actions. There's many mappings. History meaning the last few seconds

1:00:59.760 --> 1:01:04.960
 or the last few minutes or the last few months? Who knows? Who knows how much you need? In terms

1:01:04.960 --> 1:01:09.520
 of if your state is really like the positions of everything or whatnot and velocities,

1:01:09.520 --> 1:01:16.720
 who knows how much you need? Then there's so many mappings. Now you're talking about how do

1:01:16.720 --> 1:01:22.640
 you regularize that space? What priors do you impose or what's the inductive bias? There's all

1:01:22.640 --> 1:01:28.560
 very related things to think about it. Basically, what are assumptions that we should be making

1:01:29.840 --> 1:01:34.240
 such that these models actually generalize outside of the data that we've seen?

1:01:35.600 --> 1:01:39.520
 Now you're talking about, well, I don't know, what can you assume? Maybe you can assume that

1:01:39.520 --> 1:01:45.440
 people actually have intentions and that's what drives their actions. Maybe that's the right

1:01:45.440 --> 1:01:51.520
 thing to do when you haven't seen data very nearby that tells you otherwise. I don't know,

1:01:51.520 --> 1:01:57.040
 it gets a very open question. Do you think one of the dreams of artificial intelligence was to

1:01:57.760 --> 1:02:04.240
 solve common sense reasoning? Whatever the heck that means. Do you think something like common

1:02:04.240 --> 1:02:10.560
 sense reasoning has to be solved in part to be able to solve this dance of human robot interaction,

1:02:10.560 --> 1:02:16.240
 the driving space, or human robot interaction in general? Do you have to be able to reason about

1:02:16.240 --> 1:02:27.200
 these kinds of common sense concepts of physics, of, you know, all the things we've been talking

1:02:27.200 --> 1:02:33.360
 about humans, I don't even know how to express them with words, but the basics of human behavior,

1:02:33.360 --> 1:02:39.040
 of fear of death. So like, to me, it's really important to encode in some kind of sense,

1:02:40.080 --> 1:02:45.120
 maybe not, maybe it's implicit, but it feels that it's important to explicitly encode the fear of

1:02:45.120 --> 1:02:55.840
 death, that people don't want to die. Because it seems silly, but like that, the game of chicken

1:02:56.720 --> 1:03:02.000
 that involves with pedestrian crossing the street is playing with the idea of mortality.

1:03:02.800 --> 1:03:08.640
 Like, we really don't want to die. It's not just like a negative reward. I don't know. It just feels

1:03:08.640 --> 1:03:14.160
 like all these human concepts have to be encoded. Do you share that sense, or is it a lot simpler

1:03:14.160 --> 1:03:18.080
 than I'm making out to be? I think it might be simpler. And I'm the person who likes the

1:03:18.080 --> 1:03:24.800
 complicated. I think it might be simpler than that. Because it turns out, for instance, if you

1:03:26.240 --> 1:03:32.000
 say model people in the very, I'll call it traditional, I don't know if it's fair to look

1:03:32.000 --> 1:03:37.680
 at it as a traditional way, but you know, calling people as, okay, they're rational somehow,

1:03:37.680 --> 1:03:47.840
 the utilitarian perspective. Well, in that, once you say that, you automatically capture that they

1:03:47.840 --> 1:03:54.960
 have an incentive to keep on being. You know, Stuart likes to say, you can't fetch the coffee

1:03:54.960 --> 1:04:04.320
 if you're dead. Stuart Russell, by the way. That's a good line. So when you're sort of

1:04:04.320 --> 1:04:11.840
 treating agents as having these objectives, these incentives, humans or artificial,

1:04:12.640 --> 1:04:18.480
 you're kind of implicitly modeling that they'd like to stick around so that they can accomplish

1:04:18.480 --> 1:04:24.240
 those goals. So I think, I think in a sense, maybe that's what draws me so much to the

1:04:24.240 --> 1:04:29.840
 rationality framework, even though it's so broken, we've been able to, it's been such a useful

1:04:29.840 --> 1:04:33.760
 perspective. And like we were talking about earlier, what's the alternative I give up and go home,

1:04:33.760 --> 1:04:37.280
 or you know, I just use complete black boxes, but then I don't know what to assume out of

1:04:37.280 --> 1:04:43.360
 distribution that come back to this. It's just, it's been a very fruitful way to think about the

1:04:43.360 --> 1:04:49.600
 problem in a very more positive way, right? It's just people aren't just crazy, maybe they make

1:04:49.600 --> 1:04:57.120
 more sense than we think. But I think we also have to somehow be ready for it to be wrong,

1:04:57.120 --> 1:05:01.760
 be able to detect when these assumptions are unholding, be all of that stuff.

1:05:01.760 --> 1:05:08.560
 Let me ask sort of another small side of this, that we've been talking about the pure autonomous

1:05:08.560 --> 1:05:15.280
 driving problem. But there's also relatively successful systems already deployed out there

1:05:15.280 --> 1:05:21.200
 in what you may call like level two autonomy or semi autonomous vehicles, whether that's

1:05:21.200 --> 1:05:29.920
 Tesla autopilot, work quite a bit with Cadillac super guru system, which has a driver facing

1:05:29.920 --> 1:05:34.400
 camera that detects your state, there's a bunch of basically lane centering systems.

1:05:35.440 --> 1:05:43.040
 What's your sense about this kind of way of dealing with the human robot interaction problem

1:05:43.040 --> 1:05:51.040
 by having a really dumb robot and relying on the human to help the robot out to keep them both

1:05:51.040 --> 1:06:00.240
 alive? Is that from the research perspective, how difficult is that problem? And from a practical

1:06:00.240 --> 1:06:07.360
 deployment perspective, is that a fruitful way to approach this human robot interaction problem?

1:06:07.920 --> 1:06:16.080
 I think what we have to be careful about there is to not, it seems like some of these systems,

1:06:16.080 --> 1:06:24.800
 not all are making this underlying assumption that if, so I'm a driver and I'm now really

1:06:24.800 --> 1:06:30.240
 not driving but supervising and my job is to intervene, right? And so we have to be careful

1:06:30.240 --> 1:06:40.880
 with this assumption that when I'm, if I'm supervising, I will be just as safe as when I'm

1:06:40.880 --> 1:06:47.280
 driving, like that I will, you know, if I, if I wouldn't get into some kind of accident, if I'm

1:06:47.280 --> 1:06:53.840
 driving, I will be able to avoid that accident when I'm supervising too. And I think I'm concerned

1:06:53.840 --> 1:06:59.280
 about this assumption from a few perspectives. So from a technical perspective, it's that when

1:06:59.280 --> 1:07:03.680
 you let something kind of take control and do its thing, and it depends on what that thing is,

1:07:03.680 --> 1:07:07.760
 obviously, and how much is taking control and how, what things are you trusting it to do.

1:07:07.760 --> 1:07:14.000
 But if you let it do its thing and take control, it will go to what we might call

1:07:14.000 --> 1:07:18.640
 off policy from the person's perspective state. So states that the person wouldn't actually find

1:07:18.640 --> 1:07:23.920
 themselves in if they were the ones driving. And the assumption that the person functions

1:07:23.920 --> 1:07:27.920
 just as well there as they function in the states that they would normally encounter

1:07:27.920 --> 1:07:35.120
 is a little questionable. Now, another part is the kind of the human factor side of this,

1:07:35.120 --> 1:07:42.000
 which is that I don't know about you, but I think I definitely feel like I'm experiencing things

1:07:42.000 --> 1:07:46.960
 very differently when I'm actively engaged in the task versus when I'm a passive observer.

1:07:46.960 --> 1:07:51.040
 Even if I try to stay engaged, right, it's very different than when I'm actually

1:07:51.040 --> 1:07:57.200
 actively making decisions. And you see this in life in general, like you see students who are

1:07:57.200 --> 1:08:02.000
 actively trying to come up with the answer, learn to think better than when they're passively told

1:08:02.000 --> 1:08:06.160
 the answer. I think that's somewhat related. And I think people have studied this in human

1:08:06.160 --> 1:08:11.040
 factors for airplanes. And I think it's actually fairly established that these two are not the

1:08:11.040 --> 1:08:17.040
 same. So I, on that point, because I've gotten a huge amount of heat on this and I stand by it.

1:08:17.040 --> 1:08:24.000
 Okay. Because I know the human factors can be well. And the work here is really strong. And

1:08:24.000 --> 1:08:29.440
 there's many decades of work showing exactly what you're saying. Nevertheless, I've been

1:08:29.440 --> 1:08:34.320
 continuously surprised that much of the predictions of that work has been wrong and what I've seen.

1:08:35.280 --> 1:08:42.160
 So what we have to do, I still agree with everything you said, but we have to be a little bit more

1:08:44.080 --> 1:08:49.520
 open minded. So the, I'll tell you, there's a few surprising things that

1:08:50.240 --> 1:08:54.160
 supervise, like everything you said to the word is actually exactly correct.

1:08:54.160 --> 1:09:00.480
 But it doesn't say, what you didn't say is that these systems are, you said you can't assume a

1:09:00.480 --> 1:09:06.560
 bunch of things, but we don't know if these systems are fundamentally unsafe. That's still

1:09:06.560 --> 1:09:15.120
 unknown. There's a lot of interesting things. Like I'm surprised by the fact, not the fact,

1:09:15.120 --> 1:09:20.400
 that what seems to be anecdotally from, well, from large data collection that we've done,

1:09:20.400 --> 1:09:26.640
 but also from just talking to a lot of people, when in the supervisory role of semi autonomous

1:09:26.640 --> 1:09:33.440
 systems that are sufficiently dumb, at least, which is the, that might be an key element,

1:09:33.440 --> 1:09:38.960
 is the systems have to be dumb. The people are actually more energized as observers. So they

1:09:38.960 --> 1:09:46.480
 actually better, they're better at observing the situation. So there might be cases in systems,

1:09:46.480 --> 1:09:53.040
 if you get the interaction right, where you as a supervisor will do a better job with the system

1:09:53.040 --> 1:09:58.160
 together. I agree. I think that is actually really possible. I guess mainly I'm pointing out that if

1:09:58.160 --> 1:10:03.600
 you do it naively, you're an implicitly assuming something that assumption might actually really

1:10:03.600 --> 1:10:10.960
 be wrong. But I do think that if you explicitly think about what the agent should do such that

1:10:10.960 --> 1:10:16.880
 the person still stays engaged, what the, so that you essentially empower the person to want,

1:10:16.880 --> 1:10:21.760
 and they could, that's really the goal, right? Is you still have a driver. So you want to empower

1:10:21.760 --> 1:10:28.720
 them to be so much better than they would be by themselves. And that's different. It's a very

1:10:28.720 --> 1:10:38.960
 different mindset than I want them to basically not drive. And, but be ready to sort of take over.

1:10:38.960 --> 1:10:43.680
 So one of the interesting things we've been talking about is the rewards

1:10:44.720 --> 1:10:51.440
 that they seem to be fundamental to the way robots behaves. So broadly speaking,

1:10:52.240 --> 1:10:58.320
 we've been talking about utility functions, but comment on how do we approach the design

1:10:58.320 --> 1:11:01.600
 of reward functions? Like how do we come up with good reward functions?

1:11:01.600 --> 1:11:12.320
 Mm hmm. Well, really good question because the answer is we don't. This was, you know,

1:11:12.320 --> 1:11:18.320
 I used to think, I used to think about how, well, it's actually really hard to specify

1:11:18.320 --> 1:11:24.240
 rewards for interaction because it's really supposed to be what the people want. And then

1:11:24.240 --> 1:11:29.680
 you really, you know, we talked about how you have to customize what you want to do to the end

1:11:29.680 --> 1:11:37.440
 user. But I kind of realized that even if you take the interactive component away,

1:11:39.040 --> 1:11:44.000
 it's still really hard to design reward functions. So what do I mean by that? I mean,

1:11:44.720 --> 1:11:51.200
 if we assume this sort of AI paradigm in which there's an agent and his job is to optimize some

1:11:51.200 --> 1:11:59.920
 objectives, some reward, utility, loss, whatever cost. If you write it out, maybe it's a sad,

1:11:59.920 --> 1:12:06.080
 depending on the situation or whatever it is. If you write it out, and then you deploy the agent,

1:12:06.640 --> 1:12:12.800
 you'd want to make sure that whatever you specified incentivizes the behavior you want

1:12:13.440 --> 1:12:20.320
 from the agent in any situation that the agent will be faced with, right? So I do motion planning

1:12:20.320 --> 1:12:27.280
 on my robot arm. I specify some cost function, like, you know, this is how far away you should

1:12:27.280 --> 1:12:30.720
 try to stay, so much a matter to stay away from people and this is how much it matters to be

1:12:30.720 --> 1:12:36.560
 able to be efficient and blah, blah, blah, right? I need to make sure that whatever I specify those

1:12:36.560 --> 1:12:43.120
 constraints or tradeoffs or whatever they are, that when the robot goes and solves that problem

1:12:43.120 --> 1:12:48.480
 in every new situation, that behavior is the behavior that I want to see. And what I've been

1:12:48.480 --> 1:12:56.640
 finding is that we have no idea how to do that. Basically, what I can do is I can sample, I can

1:12:56.640 --> 1:13:00.720
 think of some situations that I think are representative of what the robot will face.

1:13:02.080 --> 1:13:10.960
 And I can tune and add and tune some reward function until the optimal behavior is what I

1:13:10.960 --> 1:13:17.760
 want on those situations, which, first of all, is super frustrating because, you know, through the

1:13:17.760 --> 1:13:23.040
 miracle of AI, we've taken, we don't have to specify rules for behavior anymore, right? The,

1:13:23.040 --> 1:13:28.320
 who were saying before, the robot comes up with the right thing to do, you plug in the situation,

1:13:28.320 --> 1:13:34.800
 it optimizes, bring that situation, it optimizes, but you have to spend still a lot of time on

1:13:34.800 --> 1:13:40.320
 actually defining what it is that that criterion should be. Make sure you didn't forget about 50

1:13:40.320 --> 1:13:45.280
 bazillion things that are important and how they all should be combining together to tell the robot

1:13:45.280 --> 1:13:53.200
 what's good and what's bad and how good and how bad. And so I think this is a lesson that,

1:13:54.720 --> 1:14:00.080
 I don't know, kind of, I guess I close my eyes to it for a while because I've been, you know,

1:14:00.080 --> 1:14:08.240
 tuning cost functions for 10 years now. But it really strikes me that, yeah, we've moved the

1:14:08.240 --> 1:14:19.200
 tuning and the, like, designing of features or whatever from the behavior side into the reward

1:14:19.200 --> 1:14:24.800
 side. And yes, I agree that there's way less of it, but it still seems really hard to anticipate

1:14:24.800 --> 1:14:31.920
 any possible situation and make sure you specify a reward function that when optimized will work

1:14:31.920 --> 1:14:39.040
 well in every possible situation. So you're kind of referring to unintended consequences or just,

1:14:39.040 --> 1:14:44.640
 in general, any kind of suboptimal behavior that emerges outside of the things you said,

1:14:44.640 --> 1:14:49.920
 out of distribution. Suboptimal behavior that is, you know, actually optimal. I mean, this,

1:14:49.920 --> 1:14:52.880
 I guess, the idea of unintended consequences, you know, it's optimal in respect to what you

1:14:52.880 --> 1:14:57.360
 specified, but it's not what you want. And there's a difference between those.

1:14:57.360 --> 1:15:01.120
 But that's not fundamentally a robotics problem, right? That's a human problem.

1:15:01.120 --> 1:15:05.120
 So like, that's the thing, right? So there's this thing called Good Hearts Law,

1:15:05.120 --> 1:15:11.360
 which is you set a metric for an organization. And the moment it becomes a target that people

1:15:11.360 --> 1:15:15.600
 actually optimize for, it's no longer a good metric. Well, what's it called? That's a quote.

1:15:15.600 --> 1:15:21.520
 Good Hearts Law. Good Hearts Law. So the moment you specify a metric, it stops doing his job.

1:15:21.520 --> 1:15:26.160
 Yeah, it stops doing his job. So there's, yeah, there's such a thing as optimizing for

1:15:26.160 --> 1:15:33.200
 things and, and, you know, failing to, to think ahead of time of all the possible

1:15:33.200 --> 1:15:38.320
 things that might be important. And so that's, so that's interesting because

1:15:39.760 --> 1:15:43.840
 historically I work a lot on reward learning from the perspective of customizing to the end user,

1:15:43.840 --> 1:15:49.920
 but it really seems like it's not just the interaction with the end user that's a problem

1:15:49.920 --> 1:15:54.720
 of the human and the robot collaborating so that the robot can do what the human wants,

1:15:54.720 --> 1:15:58.800
 right? This kind of back and forth, the robot probing, the person being informative, all of

1:15:58.800 --> 1:16:06.800
 that stuff might be actually just as applicable to this kind of maybe new form of human robot

1:16:06.800 --> 1:16:13.280
 interaction, which is the interaction between the robot and the expert programmer, roboticist,

1:16:13.280 --> 1:16:19.280
 designer in charge of actually specifying what the heck one should do and specifying the task

1:16:19.280 --> 1:16:24.560
 for the robot. Fascinating. That's so cool, like collaborating on the reward. Right, collaborating

1:16:24.560 --> 1:16:29.200
 on the reward design. And so what, what does it mean, right? What does it, when we think about

1:16:29.200 --> 1:16:35.600
 the problem, not as someone specifies all of your job is to optimize and we start thinking about

1:16:36.160 --> 1:16:42.720
 you're in this interaction and this collaboration. And the first thing that comes up is when the

1:16:42.720 --> 1:16:48.560
 person specifies a reward, it's not, you know, gospel, it's not like the letter of the law.

1:16:48.560 --> 1:16:54.080
 It's not the definition of the reward function you should be optimizing because they're doing

1:16:54.080 --> 1:16:58.320
 their best, but they're not some magic perfect oracle. And the sooner we start understanding

1:16:58.320 --> 1:17:05.280
 that, I think the sooner we'll get to more robots that function better in different situations.

1:17:06.320 --> 1:17:10.080
 And then, then you have kind of say, okay, well, it's, it's almost like robots are

1:17:10.080 --> 1:17:17.600
 over learning over, they're putting too much weight on the reward specified by definition.

1:17:18.320 --> 1:17:23.120
 And maybe leaving a lot of other information on the table, like what are other things we could do

1:17:23.120 --> 1:17:28.800
 to actually communicate to the robot about what we want them to do besides attempting to specify

1:17:28.800 --> 1:17:34.480
 a reward function. Yeah, you have this awesome, again, I love the poetry of leaked information.

1:17:34.480 --> 1:17:42.560
 So you mentioned humans leak information about what they want, you know, leak reward signal

1:17:42.560 --> 1:17:48.960
 for the, for the robot. So how do we detect these leaks? What is that? Yeah, what are these leaks?

1:17:49.680 --> 1:17:54.240
 But I just, I don't know, I did that, those were, there's recently saw it, read it, I don't know

1:17:54.240 --> 1:17:59.520
 where from you. And that's gonna stick with me for a while, for some reason, because it's not

1:17:59.520 --> 1:18:08.000
 explicitly expressed, it kind of leaks indirectly from our behavior. Yeah, absolutely. So I think

1:18:09.200 --> 1:18:15.040
 maybe some surprising bits, right? So we were talking before about I'm a robot arm and needs to

1:18:15.040 --> 1:18:25.040
 move around people, carry stuff, put stuff away, all of that. And now imagine that, you know, the

1:18:25.040 --> 1:18:30.560
 robot has some initial objective that the programmer gave it, so they can do all these things functionally,

1:18:30.560 --> 1:18:36.640
 it's capable of doing that. And now I noticed that it's doing something and maybe it's coming

1:18:37.520 --> 1:18:42.240
 too close to me, right? And maybe I'm the designer, maybe I'm the end user and this robot is now in

1:18:42.240 --> 1:18:51.040
 my home. And I push it away. So I push away because, you know, it's a reaction to what the robot is

1:18:51.040 --> 1:18:56.480
 currently doing. And this is what we call physical human robot interaction. And now there's a lot

1:18:56.480 --> 1:19:00.720
 of, there's a lot of interesting work on how they have to respond to physical human robot

1:19:00.720 --> 1:19:04.000
 interaction, what should the robot do if such an event occurs? And there's sort of different

1:19:04.000 --> 1:19:08.080
 schools of thought, it's well, you know, you can sort of treat it the controls erratic way and say

1:19:08.080 --> 1:19:15.680
 this is a disturbance that you must reject. You can sort of treat it more kind of heuristically

1:19:15.680 --> 1:19:19.280
 and say I'm going to go into some like gravity compensation mode so that I'm easily maneuverable

1:19:19.280 --> 1:19:23.680
 around I'm going to go into direction that the person pushed me. And, and to us,

1:19:25.680 --> 1:19:30.880
 part of realization has been that that is signal that communicates about the reward because if

1:19:30.880 --> 1:19:37.840
 my robot was moving in an optimal way, and I intervened, that means that I disagree with

1:19:37.840 --> 1:19:44.000
 his notion of optimality, whatever it thinks is optimal is not actually optimal. And sort of

1:19:44.000 --> 1:19:50.720
 optimization problems aside, that means that the cost function, the reward function is, is

1:19:50.720 --> 1:19:58.240
 incorrect, at least is not what I wanted to be. How difficult is that signal to, to, to interpret

1:19:58.240 --> 1:20:02.000
 and make actionable so like I because this connects to our autonomous vehicle discussion

1:20:02.000 --> 1:20:07.440
 whether in the semi autonomous vehicle or autonomous vehicle, when a safety driver disengages

1:20:07.440 --> 1:20:14.960
 the car, like they could have disengaged it for a million reasons. Yeah. Yeah. So that's true.

1:20:14.960 --> 1:20:20.640
 Again, it comes back to a, can you, can you structure a little bit your assumptions about

1:20:20.640 --> 1:20:25.760
 how human behavior relates to what they want? And you know, you can't one thing that we've

1:20:25.760 --> 1:20:32.320
 done is literally just treated this external torque that they applied as, you know, when you

1:20:32.320 --> 1:20:38.000
 take that and you add it with what the torque the robot was already applying, that overall action

1:20:38.000 --> 1:20:41.920
 is probably relatively optimal in respect to whatever it is that the person wants. And then

1:20:41.920 --> 1:20:45.600
 that gives you information about what it is that they want. So you can learn that people want you

1:20:45.600 --> 1:20:50.400
 to stay further away from them. Now, you're right that there might be many things that explain just

1:20:50.400 --> 1:20:54.880
 that one signal and that you might need much more data than that for, for, for the person to be able

1:20:54.880 --> 1:21:01.120
 to shape your reward function over time. You can also do this info gathering stuff that we were

1:21:01.120 --> 1:21:04.480
 talking about. Not that we've done that in that context just to clarify, but it's definitely

1:21:04.480 --> 1:21:11.120
 something we thought about where you can have the robot start acting in a way, like if there are a

1:21:11.120 --> 1:21:16.480
 bunch of different explanations, right? It moves in a way where it sees if you correct it in some

1:21:16.480 --> 1:21:22.160
 other way or not, and then kind of actually plans its motion so that it can disambiguate and collect

1:21:22.160 --> 1:21:27.280
 information about what you want. Anyway, so that's one way that's kind of sort of leaked information,

1:21:27.280 --> 1:21:33.280
 maybe even more subtle leaked information is if I just press the E stop, right? I just, I'm doing

1:21:33.280 --> 1:21:38.080
 it out of panic because the robot is about to do something bad. There's again information there,

1:21:38.080 --> 1:21:43.120
 right? Okay, the robot should definitely stop, but it should also figure out that whatever it was

1:21:43.120 --> 1:21:48.400
 about to do was not good. And in fact, it was so not good that stopping and remaining stop for a

1:21:48.400 --> 1:21:52.960
 while was better, a better trajectory for it than whatever it is that it was about to do. And that

1:21:52.960 --> 1:22:00.240
 again is information about what are my preferences? What do I want? Speaking of E stops, what are your

1:22:01.920 --> 1:22:09.680
 expert opinions on the three laws of robotics from Isaac Asimov that don't harm humans, obey

1:22:09.680 --> 1:22:14.880
 orders, protect yourself? I mean, it's such a silly notion, but I speak to so many people these

1:22:14.880 --> 1:22:20.080
 days, just regular folks, just, I don't know, my parents and so on about robotics, and they kind

1:22:20.080 --> 1:22:26.960
 of operate in that space of, you know, imagining our future with robots and thinking what are the

1:22:26.960 --> 1:22:34.640
 ethical, how do we get that dance, right? I know the three laws might be a silly notion, but do you

1:22:34.640 --> 1:22:42.960
 think about like what universal reward functions that might be that we should enforce on the robots

1:22:42.960 --> 1:22:50.400
 of the future? Or is that a little too far out? Or is the mechanism that you just described,

1:22:51.440 --> 1:22:54.480
 there shouldn't be three laws that should be constantly adjusting kind of thing?

1:22:55.040 --> 1:22:59.600
 I think it should constantly be adjusting kind of thing. You know, the issue with the laws is,

1:23:00.800 --> 1:23:05.680
 I don't even, you know, there are words and I have to write math and have to translate them into

1:23:05.680 --> 1:23:13.520
 math. What does it mean to? What does harm mean? What is, obey what, right? Because we just talked

1:23:13.520 --> 1:23:21.120
 about how you try to say what you want, but you don't always get it right and you want these machines

1:23:21.120 --> 1:23:25.840
 to do what you want, not necessarily exactly what you're literally, so you don't want them to take

1:23:25.840 --> 1:23:32.080
 you literally, you want to take what you say and interpret it in context. And that's what we do

1:23:32.080 --> 1:23:37.440
 with the specified rewards. We don't take them literally anymore from the designer. We, not we

1:23:37.440 --> 1:23:45.760
 as a community, we as, you know, some members of my group, we, and some of our collaborators like

1:23:45.760 --> 1:23:51.920
 Peter Beall and Stuart Russell, we should have said, okay, the designer specified this thing,

1:23:53.200 --> 1:23:57.760
 but I'm going to interpret it not as this is the universal reward function that I shall always

1:23:57.760 --> 1:24:04.560
 optimize always and forever, but as this is good evidence about what the person wants.

1:24:05.280 --> 1:24:10.800
 And I should interpret that evidence in the context of these situations that it was specified for,

1:24:10.800 --> 1:24:14.160
 because ultimately, that's what the designer thought about. That's what they had in mind.

1:24:14.160 --> 1:24:19.440
 And really, them specifying reward function that works for me in all these situations is really

1:24:20.160 --> 1:24:24.560
 kind of telling me that whatever behavior that incentivizes must be good behavior respect to

1:24:24.560 --> 1:24:30.240
 the thing that I should actually be optimizing for. And so now the robot kind of has uncertainty

1:24:30.240 --> 1:24:35.280
 about what it is that it should be, what its reward function is. And then there's all these

1:24:35.280 --> 1:24:40.000
 additional signals we've been finding that it can kind of continually learn from and adapt

1:24:40.000 --> 1:24:44.640
 its understanding of what people want. Every time the person corrects it, maybe they demonstrate,

1:24:44.640 --> 1:24:54.720
 maybe they stop, hopefully not. One really, really crazy one is the environment itself,

1:24:54.720 --> 1:25:02.320
 like our world. It's not, you know, you observe our world and the state of it. And it's not that

1:25:02.320 --> 1:25:05.760
 you're seeing behavior and you're saying, oh, people are making decisions that are rational,

1:25:05.760 --> 1:25:13.040
 blah, blah, blah. But our world is something that we've been acting when, according to our

1:25:13.040 --> 1:25:17.920
 preferences. So I have this example where like the robot walks into my home and my shoes are laid

1:25:17.920 --> 1:25:24.480
 down on the floor kind of in a line, right? It took effort to do that. So even though the robot

1:25:24.480 --> 1:25:30.640
 doesn't see me doing this, you know, actually aligning the shoes, it should still be able

1:25:30.640 --> 1:25:35.680
 to figure out that I want the shoes aligned. Because there's no way for them to have magically,

1:25:35.680 --> 1:25:42.960
 you know, been instantiated themselves in that way. Someone must have actually taken the time

1:25:42.960 --> 1:25:46.880
 to do that. So it must be important. So the environment actually tells the environment

1:25:46.880 --> 1:25:51.520
 leaks information, leaks information. I mean, the environment is the way it is because humans

1:25:51.520 --> 1:25:56.320
 somehow manipulated it. So you have to kind of reverse engineer the narrative that happened

1:25:56.320 --> 1:26:00.960
 to create the environments it is. And that leaks the preference information. Yeah.

1:26:00.960 --> 1:26:06.720
 You have to be careful, right? Because people don't have the bandwidth to do everything. So

1:26:06.720 --> 1:26:10.960
 just because, you know, my house is messy doesn't mean that I want it to be messy, right? But that

1:26:10.960 --> 1:26:16.080
 just, you know, I didn't put the effort into that. I put the effort into something else.

1:26:16.080 --> 1:26:19.760
 So the robot should figure out, well, that's me else was more important, but it doesn't mean that,

1:26:19.760 --> 1:26:24.160
 you know, the house being messy is not so it's a little subtle. But yeah, we really think of it.

1:26:24.160 --> 1:26:30.560
 The state itself is kind of like a choice that people implicitly made about how they want their

1:26:30.560 --> 1:26:37.280
 world. What book or books, technical or fiction or philosophical had, when you like look back

1:26:37.280 --> 1:26:42.560
 your life had a big impact, maybe it was a turning point was inspiring in some way.

1:26:42.560 --> 1:26:48.960
 Maybe we're talking about some silly book that nobody in their right mind want to read. Or maybe

1:26:48.960 --> 1:26:53.840
 it's a book that you would recommend to others to read. Or maybe those could be two different

1:26:53.840 --> 1:27:00.320
 recommendations that of books that could be useful for people on their journey.

1:27:01.600 --> 1:27:09.200
 When I was in, it's kind of a personal story, when I was in 12th grade, I got my hands on a

1:27:09.200 --> 1:27:16.960
 PDF copy in Romania of Russell Norvig AI modern approach. I didn't know anything about AI at that

1:27:16.960 --> 1:27:27.840
 point. I was, you know, I had watched the movie, The Matrix was my exposure. And so I started going

1:27:27.840 --> 1:27:35.200
 through this thing. And you know, you're asking in the beginning, what are, you know, it's math

1:27:35.200 --> 1:27:40.400
 and it's algorithms, what's interesting, it was so captivating, this notion that you could just

1:27:40.400 --> 1:27:46.880
 have a goal and figure out your way through a kind of a messy, complicated situation.

1:27:47.600 --> 1:27:52.960
 So what sequence of decisions you should make to autonomously to achieve that goal.

1:27:52.960 --> 1:28:00.000
 That was so cool. I'm, you know, I'm biased, but that's a cool book to look at.

1:28:00.000 --> 1:28:05.520
 Yeah, you can convert, you know, the goal, the goal of the process of intelligence and

1:28:05.520 --> 1:28:09.920
 mechanize it. I had the same experience. I was really interested in psychiatry and trying to

1:28:09.920 --> 1:28:15.760
 understand human behavior. And then AI modern approach is like, wait, you can just reduce it

1:28:15.760 --> 1:28:21.360
 all to write math about human behavior, right? Yeah. So that's, and I think that stuck with me

1:28:21.360 --> 1:28:28.240
 because, you know, a lot of what I do, a lot of what we do in my lab is write math about human

1:28:28.240 --> 1:28:32.800
 behavior, combine it with data and learning, put it all together, give it to robots to plan with

1:28:32.800 --> 1:28:38.160
 and, you know, hope that instead of writing rules for the robots, writing heuristics,

1:28:38.160 --> 1:28:43.840
 designing behavior, they can actually autonomously come up with the right thing to do around people.

1:28:43.840 --> 1:28:47.840
 That's kind of our, you know, that's our signature move. We wrote some math and then

1:28:47.840 --> 1:28:52.240
 instead of kind of hand crafting this and that and that and the robot figure and stuff out and

1:28:52.240 --> 1:28:57.280
 isn't that cool. And I think that is the same enthusiasm that I got from the robot figured

1:28:57.280 --> 1:29:05.200
 out how to reach that goal in that graph. Isn't that cool? So apologize for the romanticized

1:29:05.200 --> 1:29:13.600
 questions and the silly ones. If a doctor gave you five years to live, sort of emphasizing

1:29:13.600 --> 1:29:21.840
 the finiteness of our existence, what would you try to accomplish? It's like my biggest nightmare,

1:29:21.840 --> 1:29:29.440
 by the way. I really like living. So I'm actually, I really don't like the idea of being told that

1:29:29.440 --> 1:29:35.280
 I'm going to die. Sorry to link on that for a second. I mean, do you meditate or ponder on

1:29:35.280 --> 1:29:40.640
 your mortality or are human? The fact that this thing ends, it seems to be a fundamental

1:29:40.640 --> 1:29:47.440
 feature. Do you think of it as a feature or a bug too? You said you don't like the idea of dying,

1:29:47.440 --> 1:29:52.320
 but if I were to give you a choice of living forever, like you're not allowed to die.

1:29:52.320 --> 1:29:56.720
 Now I'll say that I want to live forever, but I watch this show. It's very silly. It's called

1:29:56.720 --> 1:30:02.480
 A Good Place. And they reflect a lot on this. And the moral of the story is that you have to make

1:30:02.480 --> 1:30:08.240
 the afterlife be a finite too, because otherwise people just kind of, it's like walley. It's like

1:30:08.240 --> 1:30:18.320
 whatever. So I think the finiteness helps. But yeah, it's just, I'm not a religious person.

1:30:19.040 --> 1:30:25.440
 I don't think that there's something after. And so I think it just ends and you stop existing.

1:30:25.440 --> 1:30:33.840
 And I really like existing. It's such a great privilege to exist that, yeah, it's just,

1:30:33.840 --> 1:30:39.680
 I think that's the scary part. I still think that we like existing so much because it ends.

1:30:40.320 --> 1:30:46.480
 And that's so sad. It's so sad to me every time. I find almost everything about this life beautiful.

1:30:46.480 --> 1:30:51.680
 Like the silliest, most mundane things are just beautiful. And I think I'm cognizant of the fact

1:30:51.680 --> 1:30:58.640
 that I find it beautiful because it ends. And it's so, I don't know. I don't know how to feel

1:30:58.640 --> 1:31:07.600
 about that. I also feel like there's a lesson in there for robotics and AI that is not like,

1:31:08.560 --> 1:31:14.080
 the finiteness of things seems to be a fundamental nature of human existence. I think

1:31:14.080 --> 1:31:19.920
 some people sort of accuse me of just being Russian and melancholic and romantic or something.

1:31:19.920 --> 1:31:26.640
 But that seems to be a fundamental nature of our existence that should be incorporated

1:31:27.360 --> 1:31:34.560
 in our reward functions. But anyway, if you were speaking of reward functions,

1:31:34.560 --> 1:31:38.320
 if you only had five years, what would you try to accomplish?

1:31:38.320 --> 1:31:45.120
 This is the thing. I'm thinking about this question and have a pretty joyous moment

1:31:45.120 --> 1:31:53.600
 because I don't know that I would change much. I'm trying to make some contribution

1:31:53.600 --> 1:31:58.480
 stuff, how we understand human AI interaction. I don't think I would change that.

1:32:00.400 --> 1:32:07.280
 Maybe I'll take more trips to the Caribbean or something. But I tried to solve that already

1:32:07.280 --> 1:32:14.640
 from time to time. So yeah, I mean, I try to do the things that bring me joy and thinking about

1:32:14.640 --> 1:32:19.920
 these things bring me joy is the Mary condo thing. Don't do stuff that doesn't spark joy.

1:32:19.920 --> 1:32:24.480
 For the most part, I do things that spark joy. Maybe I'll do less service in the department

1:32:24.480 --> 1:32:35.680
 or something. I'm not dealing with admissions anymore. But no, I think I have amazing colleagues

1:32:35.680 --> 1:32:41.280
 and amazing students and amazing family and friends and spending time and some balance

1:32:41.280 --> 1:32:46.000
 with all of them is what I do and that's what I'm doing already. So I don't know that I would

1:32:46.000 --> 1:32:52.640
 really change anything. So on the spirit of positiveness, what's small act of kindness

1:32:52.640 --> 1:32:56.560
 if one pops the mind where you once shown that you will never forget?

1:32:58.480 --> 1:33:09.840
 When I was in high school, my friends, my classmates did some tutoring. We were gearing up

1:33:09.840 --> 1:33:15.840
 for our baccalaureate exam, and they did some tutoring on, well, some on math, some on whatever.

1:33:15.840 --> 1:33:21.280
 I was comfortable enough with some of those subjects, but physics was something that I

1:33:21.280 --> 1:33:30.320
 hadn't focused on in a while. And so they were all working with this one teacher. And I started

1:33:30.320 --> 1:33:38.080
 working with that teacher. Her name is Nicole Bicanu. And she was the one who kind of opened

1:33:38.080 --> 1:33:45.600
 up this whole world for me because she sort of told me that I should take the SATs and apply to

1:33:45.600 --> 1:33:54.240
 go to college abroad and, you know, do better on my English and all of that. And when it came to,

1:33:54.240 --> 1:33:59.760
 well, financially, I couldn't, my parents couldn't really afford to do all these things. She started

1:33:59.760 --> 1:34:05.360
 tutoring me on physics for free. And on top of that, sitting down with me to kind of train me for

1:34:05.360 --> 1:34:14.640
 SATs and all that jazz that she had experience with. Wow. And obviously, that has taken you to

1:34:14.640 --> 1:34:21.040
 be here today, also to one of the world experts in robotics. It's funny, those little... Yeah,

1:34:21.040 --> 1:34:28.800
 people do it the small or large... For no reason, really, just out of karma... Wanting to support

1:34:28.800 --> 1:34:36.240
 someone, yeah. Yeah. So we talked a ton about reward functions. Let me talk about the most

1:34:36.240 --> 1:34:41.920
 ridiculous big question. What is the meaning of life? What's the reward function under which we

1:34:41.920 --> 1:34:48.320
 humans operate? Like, what maybe to your life, maybe broader to human life in general, what do you

1:34:48.320 --> 1:34:55.120
 think? What gives life fulfillment, purpose, happiness, meaning?

1:34:55.120 --> 1:35:01.760
 You can't even ask that question with a straight face. That's so ridiculous. I can't, I can't.

1:35:01.760 --> 1:35:07.360
 Okay. So, you know... You're going to try to answer it anyway, are you sure?

1:35:09.440 --> 1:35:17.440
 So, I was in a planetarium once. Yes. And, you know, they show you the thing and then they zoom out

1:35:17.440 --> 1:35:21.920
 and zoom out and this whole, like, you respect of dust kind of thing. I think I was conceptualizing

1:35:21.920 --> 1:35:26.800
 that we're kind of, you know, what are humans? We're just on this little planet, whatever. We

1:35:26.800 --> 1:35:32.640
 don't matter much in the grand scheme of things. And then my mind got really blown because they

1:35:32.640 --> 1:35:37.680
 talked about this multiverse theory where they kind of zoomed out and were like, this is our

1:35:37.680 --> 1:35:41.920
 universe. And then, like, there's a bazillion other ones and it just stays popped in and out of

1:35:41.920 --> 1:35:47.200
 existence. So, like, our whole thing that's that we can't even fathom how big it is was like a blimp

1:35:47.200 --> 1:35:52.720
 that went in and out. And at that point I was like, okay, like, I'm done. This is not, there is no

1:35:52.720 --> 1:35:59.760
 meaning. And clearly what we should be doing is try to impact whatever local thing we can impact.

1:35:59.760 --> 1:36:05.200
 Our communities leave a little bit behind there. Our friends, our family, our local communities,

1:36:05.200 --> 1:36:12.480
 and just try to be there for other humans. Because just everything beyond that seems

1:36:12.480 --> 1:36:18.240
 ridiculous. I mean, are you like, how do you make sense of these multiverses? Like, are you inspired

1:36:19.120 --> 1:36:24.880
 by the immensity of it? That do you, I mean, you, is there,

1:36:27.040 --> 1:36:32.960
 like, is it amazing to you? Or is it almost paralyzing in the mystery of it?

1:36:32.960 --> 1:36:43.840
 It's frustrating. I'm frustrated by my inability to comprehend. It just feels very frustrating.

1:36:43.840 --> 1:36:47.440
 It's like, there's, there's some stuff that, you know, we should time, blah, blah, blah,

1:36:47.440 --> 1:36:51.040
 that we should really be understanding. And I definitely don't understand it. But,

1:36:51.040 --> 1:36:56.560
 you know, the, the, the amazing physicists of the world have a much better understanding than me,

1:36:56.560 --> 1:37:01.040
 but it's just an epsilon and the grand scheme of things. So it's very frustrating. It's just,

1:37:01.040 --> 1:37:06.400
 it sort of feels like our brains don't have some fundamental capacity. Yeah. Well,

1:37:06.400 --> 1:37:11.040
 yet or ever, I don't know, but. Well, this, one of the dreams of artificial intelligence is to

1:37:11.040 --> 1:37:17.360
 create systems that will aid, expand our cognitive capacity in order to understand the, build the,

1:37:17.360 --> 1:37:23.520
 the theory of everything with the physics and understand what the heck these multiverses are.

1:37:25.360 --> 1:37:30.560
 So I think there's no better way to end it than talking about the meaning of life and the fundamental

1:37:30.560 --> 1:37:37.840
 nature of the universe and multiverse. So Anka is a huge honor. One of the, my favorite conversations

1:37:37.840 --> 1:37:43.200
 I've had. I really, really appreciate your time. Thank you for talking to them. Thank you for coming.

1:37:43.200 --> 1:37:48.800
 Come back again. Thanks for listening to this conversation with Anka Drugan. And thank you

1:37:48.800 --> 1:37:53.440
 to our presenting sponsor, Cash App. Please consider supporting the podcast by downloading

1:37:53.440 --> 1:37:59.360
 Cash App and using code Lex podcast. If you enjoy this podcast, subscribe on YouTube,

1:37:59.360 --> 1:38:04.320
 review it with five stars on Apple podcast, support on Patreon or simply connect with me

1:38:04.320 --> 1:38:11.120
 on Twitter and Lex Friedman. And now let me leave you with some words from Isaac Asimov.

1:38:12.400 --> 1:38:17.840
 Your assumptions are your windows in the world. Scrub them off every once in a while,

1:38:17.840 --> 1:38:33.120
 or the light won't come in. Thank you for listening and hope to see you next time.

