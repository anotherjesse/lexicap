WEBVTT

00:00.000 --> 00:04.360
 The following is a conversation with Max Tagmark, his second time in the podcast.

00:04.760 --> 00:10.440
 In fact, the previous conversation was episode number one of this very podcast.

00:10.960 --> 00:16.800
 He is a physicist and artificial intelligence researcher at MIT, co founder

00:16.800 --> 00:22.080
 of the Future of Life Institute and author of Life 3.0, being human

00:22.080 --> 00:23.840
 in the age of artificial intelligence.

00:24.560 --> 00:28.760
 He's also the head of a bunch of other huge fascinating projects and has

00:28.760 --> 00:31.760
 written a lot of different things that you should definitely check out.

00:32.200 --> 00:36.400
 He has been one of the key humans who has been outspoken about long term

00:36.400 --> 00:41.240
 existential risks of AI and also its exciting possibilities and solutions

00:41.240 --> 00:46.000
 to real world problems, most recently at the intersection of AI and physics.

00:46.400 --> 00:51.720
 And also in reengineering the algorithms that divide us by controlling

00:51.720 --> 00:56.120
 the information we see and thereby creating bubbles and all other kinds

00:56.120 --> 00:58.960
 of complex social phenomena that we see today.

00:59.600 --> 01:02.600
 In general, he's one of the most passionate and brilliant people I

01:02.600 --> 01:03.880
 have the fortune of knowing.

01:04.320 --> 01:07.720
 I hope to talk to him many more times on this podcast in the future.

01:08.240 --> 01:12.360
 Quick mention of our sponsors, the Jordan Harbinger Show, Four

01:12.360 --> 01:17.720
 Sigmatic Mushroom Coffee, BetterHelp Online Therapy and ExpressVPN.

01:18.440 --> 01:23.000
 So the choice is wisdom, caffeine, sanity or privacy.

01:23.000 --> 01:26.440
 Choose wisely, my friends, and if you wish, click the sponsor links

01:26.440 --> 01:29.160
 below to get a discount at the support this podcast.

01:29.920 --> 01:34.280
 As a side note, let me say that much of the researchers in the machine

01:34.280 --> 01:38.520
 learning and artificial intelligence communities do not spend much time

01:38.520 --> 01:41.680
 thinking deeply about existential risks of AI.

01:42.120 --> 01:46.160
 Because our current algorithms are seen as useful but dumb, it's difficult

01:46.160 --> 01:49.720
 to imagine how they may become destructive to the fabric of human

01:49.720 --> 01:51.920
 civilization in the foreseeable future.

01:51.920 --> 01:55.320
 I understand this mindset, but it's very troublesome to me.

01:55.320 --> 02:00.320
 This is both a dangerous and uninspiring perspective, reminiscent of

02:00.320 --> 02:04.920
 the lobster sitting in a pot of lukewarm water that a minute ago was cold.

02:05.320 --> 02:07.240
 I feel a kinship with this lobster.

02:07.720 --> 02:11.400
 I believe that already the algorithms that drive our interaction on social

02:11.400 --> 02:15.840
 media have an intelligence and power that far outstrip the intelligence

02:15.840 --> 02:17.560
 and power of any one human being.

02:17.560 --> 02:21.440
 Now really is the time to think about this, to define the trajectory

02:21.640 --> 02:25.040
 of the interplay of technology and human beings in our society.

02:25.480 --> 02:29.280
 I think that the future of human civilization very well may be at

02:29.280 --> 02:33.120
 stake over this very question of the role of artificial intelligence

02:33.120 --> 02:33.920
 in our society.

02:34.640 --> 02:37.560
 If you enjoy this thing, subscribe on YouTube, review it on Apple

02:37.560 --> 02:41.600
 Podcasts, follow on Spotify, support on Patreon, or connect with me on

02:41.600 --> 02:43.200
 Twitter, Alex Friedman.

02:43.200 --> 02:47.120
 And now, here's my conversation with Max Tagmark.

02:47.800 --> 02:51.120
 So people might not know this, but you were actually episode number

02:51.120 --> 02:56.880
 one of this podcast just a couple of years ago, and now we're back.

02:57.320 --> 03:01.760
 And it so happens that a lot of exciting things happened in both physics

03:01.760 --> 03:06.040
 and artificial intelligence, both fields that you're super passionate about.

03:06.560 --> 03:11.600
 Can we try to catch up to some of the exciting things happening in artificial

03:11.600 --> 03:16.040
 intelligence, especially in the context of the way it's cracking, open the

03:16.040 --> 03:18.040
 different problems of the sciences?

03:19.400 --> 03:24.920
 Yeah, I'd love to, especially now as we start 2021 here, it's a really fun

03:24.920 --> 03:28.880
 time to think about what were the biggest breakthroughs in AI.

03:29.120 --> 03:32.840
 Not the ones necessarily the media wrote about, but they really matter.

03:32.840 --> 03:36.440
 And what does that mean for our ability to do better science?

03:36.840 --> 03:41.080
 What does it mean for our ability to do better science?

03:41.080 --> 03:44.000
 To help people around the world?

03:44.200 --> 03:49.040
 And what does it mean for new problems that they could cause if we're not

03:49.040 --> 03:50.200
 smart enough to avoid them?

03:50.200 --> 03:52.720
 So, you know, what do we learn basically from this?

03:53.160 --> 03:53.880
 Yes, absolutely.

03:53.880 --> 03:57.920
 So one of the amazing things you're part of is the AI Institute for

03:57.920 --> 04:00.920
 Artificial Intelligence and Fundamental Interactions.

04:01.920 --> 04:03.240
 What's up with this institute?

04:03.480 --> 04:04.680
 What are you working on?

04:04.920 --> 04:05.840
 What are you thinking about?

04:05.840 --> 04:11.640
 Well, the idea is something I'm very on fire with, which is basically AI

04:11.640 --> 04:12.640
 meets physics.

04:13.240 --> 04:18.640
 And, you know, it's been almost five years now since I shifted my own MIT

04:18.640 --> 04:22.040
 research from physics to machine learning.

04:22.040 --> 04:24.520
 And in the beginning, I noticed a lot of my colleagues, even though they

04:24.520 --> 04:29.040
 were polite about it, well, I kind of, what is Max doing?

04:29.040 --> 04:30.280
 What is this weird stuff?

04:30.280 --> 04:31.280
 He's lost his mind.

04:31.280 --> 04:36.680
 But then, but then gradually, I, together with some colleagues, were

04:36.680 --> 04:42.120
 able to persuade more and more of the other professors in our physics

04:42.120 --> 04:43.720
 department to get interested in this.

04:43.720 --> 04:50.000
 And now we got this amazing NSF center, so 20 million bucks for the next

04:50.000 --> 04:53.600
 five years, MIT and a bunch of neighboring universities here also.

04:54.400 --> 04:57.720
 And I noticed now those colleagues who were looking at me, funny, have

04:57.720 --> 05:03.120
 stopped asking what the point is of this, because it's becoming more clear.

05:03.720 --> 05:08.840
 And I really believe that, of course, AI can help physics a lot to do

05:08.840 --> 05:16.440
 better physics, but physics can also help AI a lot, both by building better

05:16.440 --> 05:17.120
 hardware.

05:17.560 --> 05:23.120
 My colleague, Martin Solzacic, for example, is working on an optical chip for

05:23.120 --> 05:27.160
 much faster machine learning, where the computation is done, not by moving

05:27.160 --> 05:32.400
 electrons around, but by moving photons around, dramatically less energy

05:32.400 --> 05:33.600
 use, faster, better.

05:35.240 --> 05:43.840
 We can also help AI a lot, I think, by having a different set of tools and a

05:43.840 --> 05:46.480
 different, maybe more audacious attitude.

05:47.000 --> 05:52.440
 You know, AI has, to a significant extent, been an engineering discipline,

05:52.440 --> 05:54.000
 where you're just trying to make things that work.

05:54.000 --> 05:57.600
 And being less, more interested in maybe selling them than in figuring out

05:57.600 --> 06:02.880
 exactly how they work and proving theorems about that they will always work.

06:02.880 --> 06:07.840
 Contrast that with physics, you know, when Elon Musk sends a rocket to the

06:07.840 --> 06:11.840
 International Space Station, they didn't just train with machine learning,

06:11.840 --> 06:14.480
 oh, let's fire it a little bit left, more to the left, a bit more to the right,

06:14.480 --> 06:19.200
 so that also missed, let's try here, no, you know, we figured out Newton's

06:19.200 --> 06:24.480
 laws of gravitation and other things and got a really deep fundamental

06:24.480 --> 06:30.680
 understanding, and that's what gives us such confidence in rockets.

06:30.680 --> 06:37.480
 And my vision is that in the future, all machine learning systems that

06:37.480 --> 06:41.440
 actually have impact on people's lives will be understood at a really,

06:41.440 --> 06:45.440
 really deep level, right, so we trust them, not because some sales rep told

06:45.440 --> 06:51.440
 us to, but because they've earned our trust, and really safety critical

06:51.440 --> 06:55.440
 things even prove that they will always do what we expect them to do.

06:55.440 --> 06:59.440
 That's very much the physics mindset, so it's interesting if you look at

06:59.440 --> 07:03.440
 big breakthroughs that have happened in machine learning this year, you know,

07:03.440 --> 07:09.440
 from dancing robots, you know, is pretty fantastic, not just because it's cool,

07:09.440 --> 07:14.440
 but if you just think about not that many years ago, this YouTube video at

07:14.440 --> 07:18.440
 this DARPA challenge where the MIT robot comes out of the car and face

07:18.440 --> 07:23.440
 plants, how far we've come in just a few years.

07:23.440 --> 07:30.440
 Similarly, Alpha Fold 2, you know, crushing the protein folding problem,

07:30.440 --> 07:34.440
 we can talk more about implications for medical research and stuff,

07:34.440 --> 07:38.440
 but hey, you know, that's huge progress.

07:38.440 --> 07:46.440
 You can look at the GPT3, they can spout off English texts,

07:46.440 --> 07:49.440
 which sometimes really, really blows you away.

07:49.440 --> 07:53.440
 You can look at the Google, at DeepMind's Mu Zero,

07:53.440 --> 07:58.440
 which doesn't just kick our butt and go and chest and chogi,

07:58.440 --> 08:01.440
 but also in all these Atari games, and you don't even have to teach it

08:01.440 --> 08:02.440
 the rules now.

08:02.440 --> 08:07.440
 You know, what all of those have in common is besides being powerful is

08:07.440 --> 08:10.440
 we don't fully understand how they work.

08:10.440 --> 08:13.440
 And that's fine if it's just some dancing robots,

08:13.440 --> 08:16.440
 and the worst thing that can happen is they face plant, right?

08:16.440 --> 08:19.440
 Or if they're playing Go, and the worst thing that can happen is

08:19.440 --> 08:22.440
 that they make a bad move and lose the game, right?

08:22.440 --> 08:26.440
 It's less fine if that's what's controlling your self driving car

08:26.440 --> 08:28.440
 or your nuclear power plant.

08:28.440 --> 08:34.440
 And we've seen already that even though Hollywood had all these movies

08:34.440 --> 08:37.440
 where they try to make us worry about the wrong things like machines

08:37.440 --> 08:42.440
 turning evil, the actual bad things that have happened with automation

08:42.440 --> 08:45.440
 have not been machines turning evil.

08:45.440 --> 08:49.440
 They've been caused by over trust in things we didn't understand

08:49.440 --> 08:51.440
 as well as we thought we did, right?

08:51.440 --> 08:58.440
 Even very simple automated systems like what Boeing put into the 737 Max, right?

08:58.440 --> 08:59.440
 Yes.

08:59.440 --> 09:00.440
 Killed a lot of people.

09:00.440 --> 09:02.440
 Was it that that little simple system was evil?

09:02.440 --> 09:05.440
 Of course not, but we didn't understand it

09:05.440 --> 09:07.440
 as well as we should have, right?

09:07.440 --> 09:10.440
 And we trusted without understanding.

09:10.440 --> 09:11.440
 Exactly.

09:11.440 --> 09:12.440
 And over trust.

09:12.440 --> 09:15.440
 We didn't even understand that we didn't understand, right?

09:15.440 --> 09:19.440
 The humility is really at the core of being a scientist.

09:19.440 --> 09:21.440
 I think step one, if you want to be a scientist,

09:21.440 --> 09:24.440
 is don't ever fool yourself into thinking you understand things

09:24.440 --> 09:26.440
 when you actually don't, right?

09:26.440 --> 09:29.440
 That's probably good advice for humans in general.

09:29.440 --> 09:31.440
 I think humility in general can do us good.

09:31.440 --> 09:33.440
 In science, it's so spectacular.

09:33.440 --> 09:37.440
 Why did we have the wrong theory of gravity ever from Aristotle

09:37.440 --> 09:40.440
 onward and close to Galileo's time?

09:40.440 --> 09:42.440
 Why would we believe something so dumb

09:42.440 --> 09:44.440
 as that if I throw this water bottle,

09:44.440 --> 09:47.440
 it's going to go up with constant speed

09:47.440 --> 09:49.440
 until it realizes that its natural motion is down.

09:49.440 --> 09:51.440
 It changes its mind.

09:51.440 --> 09:54.440
 Because people just kind of assumed

09:54.440 --> 09:55.440
 Aristotle was right.

09:55.440 --> 09:56.440
 He's an authority.

09:56.440 --> 09:57.440
 We understand that.

09:57.440 --> 10:01.440
 Why did we believe things like that the sun is going around the Earth?

10:01.440 --> 10:04.440
 Why did we believe that time flows at the same rate

10:04.440 --> 10:06.440
 for everyone until Einstein?

10:06.440 --> 10:08.440
 Same exact mistake over and over again.

10:08.440 --> 10:11.440
 We just weren't humble enough to acknowledge

10:11.440 --> 10:13.440
 that we actually didn't know for sure.

10:13.440 --> 10:15.440
 We assumed we knew.

10:15.440 --> 10:17.440
 So we didn't discover the truth

10:17.440 --> 10:20.440
 because we assumed there was nothing there to be discovered, right?

10:20.440 --> 10:24.440
 There was something to be discovered about the 737 Max.

10:24.440 --> 10:26.440
 And if you had been a bit more suspicious

10:26.440 --> 10:28.440
 and tested it better, we would have found it.

10:28.440 --> 10:30.440
 And it's the same thing with most harm

10:30.440 --> 10:33.440
 that's been done by automation so far, I would say.

10:33.440 --> 10:36.440
 Did you hear of a company called Night Capital?

10:36.440 --> 10:37.440
 No.

10:37.440 --> 10:38.440
 So good.

10:38.440 --> 10:41.440
 That means you didn't invest in them earlier.

10:41.440 --> 10:44.440
 They deployed this automated rating system.

10:44.440 --> 10:45.440
 Yes.

10:45.440 --> 10:46.440
 All nice and shiny.

10:46.440 --> 10:49.440
 They didn't understand it as well as they thought.

10:49.440 --> 10:52.440
 And it went about losing 10 million bucks per minute

10:52.440 --> 10:54.440
 for 44 minutes straight.

10:54.440 --> 10:56.440
 No.

10:56.440 --> 10:58.440
 Until someone presumably was like, oh, no, shut this up.

10:58.440 --> 11:00.440
 You know, was it evil?

11:00.440 --> 11:01.440
 No.

11:01.440 --> 11:03.440
 It was, again, misplaced trust,

11:03.440 --> 11:05.440
 something they didn't fully understand, right?

11:05.440 --> 11:08.440
 And there have been so many,

11:08.440 --> 11:10.440
 even when people have been killed by robots,

11:10.440 --> 11:12.440
 it's just quite rare still.

11:12.440 --> 11:14.440
 But in factory accidents,

11:14.440 --> 11:17.440
 it's in every single case been not malice,

11:17.440 --> 11:19.440
 just that the robot didn't understand that,

11:19.440 --> 11:22.440
 hey, a human is different from an auto part or whatever.

11:22.440 --> 11:27.440
 So this is where I think there's so much opportunity

11:27.440 --> 11:29.440
 for a physics approach,

11:29.440 --> 11:33.440
 where you just aim for a higher level of understanding.

11:33.440 --> 11:37.440
 And if you look at all these systems that we talked about

11:37.440 --> 11:42.440
 from reinforcement learning systems and dancing robots

11:42.440 --> 11:46.440
 to all these neural networks that power GPT3

11:46.440 --> 11:49.440
 and go playing software stuff,

11:49.440 --> 11:52.440
 they're all basically black boxes,

11:52.440 --> 11:55.440
 much like not so different from if you teach a human something,

11:55.440 --> 11:57.440
 you have no idea how their brain works, right?

11:57.440 --> 12:01.440
 Except the human brain at least has been error corrected

12:01.440 --> 12:04.440
 during many, many centuries of evolution

12:04.440 --> 12:07.440
 in a way that some of these systems have not, right?

12:07.440 --> 12:10.440
 And my MIT research is entirely focused on

12:10.440 --> 12:12.440
 demystifying this black box.

12:12.440 --> 12:15.440
 Intelligible intelligence is my slogan.

12:15.440 --> 12:16.440
 That's a good line.

12:16.440 --> 12:18.440
 Intelligible intelligence.

12:18.440 --> 12:20.440
 Yeah, it's not that we shouldn't settle for something

12:20.440 --> 12:21.440
 that seems intelligent,

12:21.440 --> 12:23.440
 but it should be intelligible

12:23.440 --> 12:26.440
 so that we actually trust it because we understand it, right?

12:26.440 --> 12:28.440
 Like, again, Elon trusts his rockets

12:28.440 --> 12:30.440
 because he understands Newton's laws

12:30.440 --> 12:33.440
 and thrusts and how everything works.

12:33.440 --> 12:34.440
 And let me tell you,

12:34.440 --> 12:36.440
 can I tell you why I'm optimistic about this?

12:36.440 --> 12:37.440
 Yes.

12:37.440 --> 12:41.440
 I think we've made a bit of a mistake

12:41.440 --> 12:43.440
 where some people still think

12:43.440 --> 12:47.440
 that somehow we're never going to understand neural networks.

12:47.440 --> 12:49.440
 And we're just going to have to learn to live with this.

12:49.440 --> 12:51.440
 It's this very powerful black box.

12:51.440 --> 12:55.440
 Basically, for those who haven't spent time

12:55.440 --> 12:56.440
 building their own,

12:56.440 --> 12:58.440
 it's super simple what happens inside.

12:58.440 --> 13:00.440
 You send in a long list of numbers,

13:00.440 --> 13:04.440
 and then you do a bunch of operations on them,

13:04.440 --> 13:06.440
 multiply by matrices, et cetera, et cetera,

13:06.440 --> 13:07.440
 and some other numbers come out.

13:07.440 --> 13:09.440
 That's the output of it.

13:09.440 --> 13:13.440
 And then there are a bunch of knobs you can tune.

13:13.440 --> 13:14.440
 And when you change them,

13:14.440 --> 13:17.440
 it affects the computation, the input output relation.

13:17.440 --> 13:20.440
 And then you just give the computer some definition of good,

13:20.440 --> 13:22.440
 and it keeps optimizing these knobs

13:22.440 --> 13:24.440
 until it performs as good as possible.

13:24.440 --> 13:26.440
 And often you go, like, wow, that's really good.

13:26.440 --> 13:28.440
 This robot can dance.

13:28.440 --> 13:31.440
 Or this machine is beating me at chest now.

13:31.440 --> 13:33.440
 And in the end, you have something,

13:33.440 --> 13:35.440
 which even though you can look inside it,

13:35.440 --> 13:38.440
 you have very little idea of how it works.

13:38.440 --> 13:42.440
 You can print out tables of all the millions of parameters in there.

13:42.440 --> 13:44.440
 Is it crystal clear now how it's working?

13:44.440 --> 13:46.440
 And of course not, right?

13:46.440 --> 13:48.440
 Many of my colleagues seem willing to settle for that.

13:48.440 --> 13:50.440
 And I'm like, no.

13:50.440 --> 13:53.440
 That's like the halfway point.

13:53.440 --> 13:57.440
 Some have even gone as far as sort of guessing

13:57.440 --> 14:00.440
 that the mystery, the inscrutability of this

14:00.440 --> 14:02.440
 is where some of the power comes from

14:02.440 --> 14:04.440
 and some sort of mysticism.

14:04.440 --> 14:06.440
 I think that's total nonsense.

14:06.440 --> 14:10.440
 I think the real power of neural networks

14:10.440 --> 14:12.440
 comes not from inscrutability,

14:12.440 --> 14:14.440
 but from differentiability.

14:14.440 --> 14:18.440
 And what I mean by that is simply that

14:18.440 --> 14:21.440
 the output changes only smoothly

14:21.440 --> 14:23.440
 if you tweak your knobs.

14:23.440 --> 14:26.440
 And then you can use all these powerful methods

14:26.440 --> 14:28.440
 we have for optimization in science.

14:28.440 --> 14:29.440
 We can just tweak them a little bit

14:29.440 --> 14:31.440
 and see, did that get better or worse?

14:31.440 --> 14:33.440
 That's the fundamental idea of machine learning,

14:33.440 --> 14:35.440
 that the machine itself can keep optimizing

14:35.440 --> 14:37.440
 until it gets better.

14:37.440 --> 14:40.440
 Suppose you wrote this algorithm instead

14:40.440 --> 14:43.440
 in Python or some other programming language.

14:43.440 --> 14:45.440
 And then what the knobs did was

14:45.440 --> 14:48.440
 they just changed random letters in your code.

14:48.440 --> 14:51.440
 Now it would just epically fail, right?

14:51.440 --> 14:53.440
 You change one thing and instead of saying print,

14:53.440 --> 14:56.440
 it says synth, syntax error.

14:56.440 --> 14:58.440
 You don't even know, was that for the better

14:58.440 --> 15:00.440
 or for the worse, right?

15:00.440 --> 15:02.440
 This to me is, this is what I believe

15:02.440 --> 15:05.440
 is the fundamental power of neural networks.

15:05.440 --> 15:07.440
 Just to clarify, the changing the different letters

15:07.440 --> 15:10.440
 in a program would not be a differentiable process.

15:10.440 --> 15:13.440
 It would make it an invalid program typically

15:13.440 --> 15:15.440
 and then you wouldn't even know

15:15.440 --> 15:16.440
 if you changed more letters,

15:16.440 --> 15:18.440
 if it would make it work again, right?

15:18.440 --> 15:21.440
 So that's the magic of neural networks,

15:21.440 --> 15:23.440
 the inscrubility.

15:23.440 --> 15:25.440
 The differentiability, that every setting

15:25.440 --> 15:27.440
 of the parameters is a program

15:27.440 --> 15:29.440
 and you can tell is it better or worse, right?

15:29.440 --> 15:32.440
 So you don't like the poetry of the mystery

15:32.440 --> 15:34.440
 of neural networks as the source of its power?

15:34.440 --> 15:37.440
 I generally like poetry, but...

15:37.440 --> 15:38.440
 Not in this case.

15:38.440 --> 15:41.440
 It's so misleading and above all,

15:41.440 --> 15:44.440
 it shortchanges us, it makes us underestimate

15:44.440 --> 15:46.440
 the good things we can accomplish

15:46.440 --> 15:48.440
 because so what we've been doing in my group

15:48.440 --> 15:50.440
 is basically step one,

15:50.440 --> 15:52.440
 train the mysterious neural network

15:52.440 --> 15:54.440
 to do something well.

15:54.440 --> 15:58.440
 And then step two, do some additional AI techniques

15:58.440 --> 16:02.440
 to see if we can now transform this black box

16:02.440 --> 16:04.440
 into something equally intelligent

16:04.440 --> 16:06.440
 that you can actually understand.

16:06.440 --> 16:08.440
 So for example, I'll give you one example

16:08.440 --> 16:11.440
 of this AI Feynman project that we just published, right?

16:11.440 --> 16:15.440
 So we took the 100 most famous

16:15.440 --> 16:17.440
 or complicated equations

16:17.440 --> 16:20.440
 from one of my favorite physics textbooks,

16:20.440 --> 16:22.440
 in fact the one that got me into physics

16:22.440 --> 16:25.440
 in the first place, the Feynman lectures on physics.

16:25.440 --> 16:28.440
 And so you have a formula, you know,

16:28.440 --> 16:31.440
 maybe it has what goes into the formula

16:31.440 --> 16:33.440
 as six different variables

16:33.440 --> 16:35.440
 and then what comes out as one.

16:35.440 --> 16:37.440
 So then you can make like a giant Excel spreadsheet

16:37.440 --> 16:39.440
 with seven columns.

16:39.440 --> 16:41.440
 You put in just random numbers for the six columns

16:41.440 --> 16:43.440
 for those six input variables

16:43.440 --> 16:45.440
 and then you calculate with the formula

16:45.440 --> 16:47.440
 the seventh column, the output.

16:47.440 --> 16:49.440
 So maybe it's like the force equals

16:49.440 --> 16:51.440
 in the last column some function of the other.

16:51.440 --> 16:53.440
 And now the task is, okay,

16:53.440 --> 16:55.440
 if I don't tell you what the formula was,

16:55.440 --> 16:57.440
 can you figure that out

16:57.440 --> 16:59.440
 from looking at my spreadsheet that I gave you?

16:59.440 --> 17:03.440
 This problem is called symbolic regression.

17:03.440 --> 17:05.440
 If I tell you that the formula

17:05.440 --> 17:07.440
 is what we call a linear formula.

17:07.440 --> 17:09.440
 So it's just that the output is

17:11.440 --> 17:13.440
 some sum of all the things

17:13.440 --> 17:15.440
 input at the time, some constants.

17:15.440 --> 17:17.440
 That's the famous easy problem we can solve.

17:17.440 --> 17:20.440
 We do it all the time in science and engineering.

17:20.440 --> 17:22.440
 But the general one,

17:22.440 --> 17:24.440
 if it's more complicated functions

17:24.440 --> 17:27.440
 with logarithms or cosines or other math,

17:27.440 --> 17:29.440
 it's a very, very hard one

17:29.440 --> 17:32.440
 and probably impossible to do fast in general

17:32.440 --> 17:34.440
 just because the number of formulas

17:34.440 --> 17:37.440
 with n symbols just grows exponentially.

17:37.440 --> 17:39.440
 Just like the number of passwords you can make

17:39.440 --> 17:41.440
 grow dramatically with length.

17:41.440 --> 17:44.440
 So we had this idea that

17:44.440 --> 17:46.440
 if you first have a neural network

17:46.440 --> 17:48.440
 that can actually approximate the formula,

17:48.440 --> 17:51.440
 you just train that even if you don't understand how it works,

17:51.440 --> 17:54.440
 that can be the first step

17:54.440 --> 17:56.440
 towards actually understanding how it works.

17:56.440 --> 17:59.440
 So that's what we do first.

17:59.440 --> 18:02.440
 And then we study that neural network now

18:02.440 --> 18:04.440
 and put in all sorts of other data

18:04.440 --> 18:06.440
 that wasn't in the original training data

18:06.440 --> 18:08.440
 and use that to discover

18:08.440 --> 18:10.440
 simplifying properties of the formula.

18:10.440 --> 18:12.440
 And that lets us break it apart

18:12.440 --> 18:14.440
 often into many simpler pieces

18:14.440 --> 18:16.440
 in a kind of divide and conquer approach.

18:16.440 --> 18:19.440
 So we were able to solve all of those 100 formulas,

18:19.440 --> 18:21.440
 discover them automatically,

18:21.440 --> 18:23.440
 plus a whole bunch of other ones.

18:23.440 --> 18:26.440
 But it's actually kind of humbling to say

18:26.440 --> 18:29.440
 that this code, which anyone who wants now

18:29.440 --> 18:32.440
 is listening to this can type pip install

18:32.440 --> 18:34.440
 AI Feynman on the computer and run it.

18:34.440 --> 18:37.440
 It can actually do what Johannes Kepler

18:37.440 --> 18:40.440
 spent four years doing when he stared at Mars data

18:40.440 --> 18:43.440
 until he was like, finally, Eureka, this is an ellipse.

18:43.440 --> 18:46.440
 This will do it automatically for you in one hour, right?

18:46.440 --> 18:48.440
 Or Max Planck.

18:48.440 --> 18:51.440
 He was looking at how much radiation comes out

18:51.440 --> 18:53.440
 at different wavelengths from a hot object

18:53.440 --> 18:56.440
 and discovered the famous black body formula.

18:56.440 --> 18:59.440
 This discovers it automatically.

18:59.440 --> 19:04.440
 I'm actually excited about

19:04.440 --> 19:08.440
 seeing if we can discover not just old formulas again,

19:08.440 --> 19:11.440
 but new formulas that no one has seen before.

19:11.440 --> 19:14.440
 And do you like this process of using kind of a neural network

19:14.440 --> 19:17.440
 to find some basic insights

19:17.440 --> 19:19.440
 and then dissecting the neural network

19:19.440 --> 19:23.440
 and gain the final so that that's in that way

19:23.440 --> 19:29.440
 you've forcing the explainability issue,

19:29.440 --> 19:33.440
 really trying to analyze the neural network

19:33.440 --> 19:36.440
 for the things it knows in order to come up

19:36.440 --> 19:39.440
 with the final beautiful simple theory

19:39.440 --> 19:42.440
 underlying the initial system that you were looking at.

19:42.440 --> 19:44.440
 I love that.

19:44.440 --> 19:48.440
 And the reason I'm so optimistic that it can be generalized

19:48.440 --> 19:53.440
 is because that's exactly what we do as human scientists.

19:53.440 --> 19:55.440
 Think of Galileo whom we mentioned, right?

19:55.440 --> 19:57.440
 I bet when he was a little kid,

19:57.440 --> 20:00.440
 if his dad threw him an apple, he would catch it.

20:00.440 --> 20:02.440
 Why?

20:02.440 --> 20:04.440
 Because he had a neural network in his brain

20:04.440 --> 20:07.440
 that he had trained to predict the parabolic orbit

20:07.440 --> 20:10.440
 of apples that are thrown under gravity.

20:10.440 --> 20:12.440
 If you throw a tennis ball to a dog,

20:12.440 --> 20:15.440
 it also has this same ability of deep learning

20:15.440 --> 20:18.440
 to figure out how the ball is going to move and catch it.

20:18.440 --> 20:21.440
 But Galileo went one step further when he got older.

20:21.440 --> 20:25.440
 He went back and was like, wait a minute.

20:25.440 --> 20:27.440
 I can write down a formula for this.

20:27.440 --> 20:31.440
 Y equals X squared, a parabola.

20:31.440 --> 20:36.440
 And he helped revolutionize physics as we know it, right?

20:36.440 --> 20:39.440
 So there was a basic neural network in there from childhood

20:39.440 --> 20:44.440
 that captured the experiences of observing

20:44.440 --> 20:46.440
 different kinds of trajectories.

20:46.440 --> 20:50.440
 And then he was able to go back in with another extra little neural network

20:50.440 --> 20:54.440
 and analyze all those experiences and be like, wait a minute.

20:54.440 --> 20:56.440
 There's a deeper rule here.

20:56.440 --> 21:00.440
 Exactly. He was able to distill out in symbolic form

21:00.440 --> 21:03.440
 what that complicated black box neural network was doing.

21:03.440 --> 21:06.440
 Not only did he, the formula he got,

21:06.440 --> 21:08.440
 ultimately become more accurate.

21:08.440 --> 21:11.440
 And similarly, this is how Newton got Newton's laws,

21:11.440 --> 21:15.440
 which is why Elon can send rockets to the space station now, right?

21:15.440 --> 21:19.440
 So it's not only more accurate, but it's also simpler, much simpler.

21:19.440 --> 21:23.440
 And it's so simple that we can actually describe it to our friends

21:23.440 --> 21:25.440
 and each other, right?

21:25.440 --> 21:28.440
 We've talked about it just in the context of physics now,

21:28.440 --> 21:32.440
 but hey, isn't this what we're doing when we're talking to each other also?

21:32.440 --> 21:36.440
 We go around with our neural networks just like dogs and cats

21:36.440 --> 21:38.440
 and chipmunks and blue jays.

21:38.440 --> 21:41.440
 And we experience things in the world.

21:41.440 --> 21:44.440
 But then we humans do this additional step on top of that,

21:44.440 --> 21:48.440
 where we then distill out certain high level knowledge

21:48.440 --> 21:52.440
 that we've extracted from this in a way that can communicate it to each other

21:52.440 --> 21:56.440
 in a symbolic form in English in this case, right?

21:56.440 --> 22:02.440
 So if we can do it and we believe that we are information processing entities,

22:02.440 --> 22:06.440
 then we should be able to make machine learning that does it also.

22:06.440 --> 22:09.440
 Well, do you think the entire thing could be learning?

22:09.440 --> 22:13.440
 Because this dissection process, like for AI Feynman,

22:13.440 --> 22:18.440
 the secondary stage feels like something like reasoning.

22:18.440 --> 22:24.440
 And the initial step feels like more like the more basic kind of differentiable learning.

22:24.440 --> 22:28.440
 Do you think the whole thing could be differentiable learning?

22:28.440 --> 22:31.440
 Do you think the whole thing could be basically neural networks on top of each other?

22:31.440 --> 22:33.440
 It's like turtles all the way down.

22:33.440 --> 22:35.440
 Could it be neural networks all the way down?

22:35.440 --> 22:37.440
 I mean, that's a really interesting question.

22:37.440 --> 22:40.440
 We know that in your case, it is neural networks all the way down

22:40.440 --> 22:45.440
 because that's all you'll have in your skull as a bunch of neurons doing their thing, right?

22:45.440 --> 22:49.440
 But if you ask the question more generally,

22:49.440 --> 22:53.440
 what algorithms are being used in your brain,

22:53.440 --> 22:55.440
 I think it's super interesting to compare.

22:55.440 --> 22:58.440
 I think we've gotten a little bit backwards historically

22:58.440 --> 23:02.440
 because we humans first discovered good old fashioned AI,

23:02.440 --> 23:08.440
 the logic based AI that we often call GOFI for good old fashioned AI.

23:08.440 --> 23:12.440
 And then more recently, we did machine learning

23:12.440 --> 23:15.440
 because it required bigger computers, so we had to discover it later.

23:15.440 --> 23:20.440
 So we think of machine learning with neural networks as the modern thing

23:20.440 --> 23:23.440
 and the logic based AI as the old fashioned thing.

23:23.440 --> 23:27.440
 But if you look at evolution on Earth, right,

23:27.440 --> 23:29.440
 it's actually been the other way around.

23:29.440 --> 23:35.440
 I would say that, for example, an eagle has a better vision system

23:35.440 --> 23:42.440
 than I have using, and dogs are just as good at casting tennis balls as I am.

23:42.440 --> 23:45.440
 All this stuff which is done by training in neural network

23:45.440 --> 23:49.440
 and not interpreting it in words, you know,

23:49.440 --> 23:53.440
 is something so many of our animal friends can do, at least as well as us, right?

23:53.440 --> 23:58.440
 What is it that we humans can do that the chipmunks and the eagles cannot?

23:58.440 --> 24:01.440
 It's more to do with this logic based stuff, right,

24:01.440 --> 24:07.440
 where we can extract out information in symbols, in language,

24:07.440 --> 24:11.440
 and now even with equations if you're a scientist, right?

24:11.440 --> 24:14.440
 So basically what happened was first we built these computers

24:14.440 --> 24:17.440
 that could multiply numbers real fast and manipulate symbols

24:17.440 --> 24:19.440
 and we felt they were pretty dumb.

24:19.440 --> 24:24.440
 And then we made neural networks that can see as well as a cat can

24:24.440 --> 24:29.440
 and do a lot of this inscrutable black box neural networks.

24:29.440 --> 24:33.440
 What we humans can do also is put the two together in a useful way.

24:33.440 --> 24:35.440
 Yes, in our own brain.

24:35.440 --> 24:37.440
 Yes, in our own brain.

24:37.440 --> 24:40.440
 So if we ever want to get artificial general intelligence

24:40.440 --> 24:44.440
 that can do all jobs as well as humans can, right,

24:44.440 --> 24:52.440
 then that's what's going to be required to be able to combine the neural networks with symbolic.

24:52.440 --> 24:55.440
 Combine the old AI with a new AI in a good way.

24:55.440 --> 25:00.440
 We do it in our brains and there seems to be basically two strategies I see in industry now.

25:00.440 --> 25:05.440
 One scares the heebie jeebies out of me and the other one I find much more encouraging.

25:05.440 --> 25:09.440
 Can we break them apart? Which other two?

25:09.440 --> 25:12.440
 The one that scares the heebie jeebies out of me is this attitude

25:12.440 --> 25:15.440
 that we're just going to make ever bigger systems that we still don't understand

25:15.440 --> 25:18.440
 until they can be as smart as humans.

25:18.440 --> 25:21.440
 What could possibly go wrong?

25:21.440 --> 25:25.440
 I think it's just such a reckless thing to do and unfortunately,

25:25.440 --> 25:29.440
 and if we actually succeed as a species to build artificial general intelligence,

25:29.440 --> 25:31.440
 then we still have no clue how it works.

25:31.440 --> 25:36.440
 I think at least 50% chance we're going to be extinct before too long.

25:36.440 --> 25:40.440
 It's just going to be an utter epic own goal.

25:40.440 --> 25:46.440
 Plus that 44 minute losing money problem or like the paperclip problem

25:46.440 --> 25:51.440
 where we don't understand how it works and it's just in a matter of seconds runs away

25:51.440 --> 25:54.440
 in some kind of direction that's going to be very problematic.

25:54.440 --> 25:58.440
 Even long before you have to worry about the machines themselves

25:58.440 --> 26:06.440
 somehow deciding to do things and to us that we have to worry about people using machines.

26:06.440 --> 26:09.440
 They're short of AI, AGI and power to do bad things.

26:09.440 --> 26:17.440
 I mean, just take a moment and if anyone who's not worried particularly about advanced AI,

26:17.440 --> 26:23.440
 just take 10 seconds and just think about your least favorite leader on the planet right now.

26:23.440 --> 26:24.440
 Don't tell me who it is.

26:24.440 --> 26:26.440
 I want to keep this apolitical.

26:26.440 --> 26:30.440
 But just see the face in front of you, that person for 10 seconds.

26:30.440 --> 26:36.440
 Now imagine that that person has this incredibly powerful AI under their control

26:36.440 --> 26:39.440
 and can use it to impose their will on the whole planet.

26:39.440 --> 26:42.440
 How does that make you feel?

26:42.440 --> 26:44.440
 Yeah.

26:44.440 --> 26:49.440
 Can we break that apart just briefly?

26:49.440 --> 26:53.440
 For the 50% chance that we'll run into trouble with this approach,

26:53.440 --> 27:00.440
 do you see the bigger worry in that leader or humans using the system to do damage

27:00.440 --> 27:05.440
 or are you more worried and I think I'm in this camp

27:05.440 --> 27:10.440
 more worried about accidental unintentional destruction of everything.

27:10.440 --> 27:17.440
 So humans trying to do good and in a way where everyone agrees it's kind of good,

27:17.440 --> 27:19.440
 it's just they're trying to do good without understanding.

27:19.440 --> 27:24.440
 Because I think every evil leader in history thought they're, to some degree,

27:24.440 --> 27:25.440
 thought they're trying to do good.

27:25.440 --> 27:26.440
 Oh yeah.

27:26.440 --> 27:29.440
 I'm sure Hitler thought he was doing a good job.

27:29.440 --> 27:31.440
 I've been reading a lot about Stalin.

27:31.440 --> 27:36.440
 He legitimately thought that communism was good for the world

27:36.440 --> 27:37.440
 and that he was doing good.

27:37.440 --> 27:41.440
 I think Mao Zedong thought what he was doing with a great leap forward was good too.

27:41.440 --> 27:45.440
 I'm actually concerned about both of those.

27:45.440 --> 27:49.440
 Before, I promised to answer this in detail, but before we do that,

27:49.440 --> 27:51.440
 let me finish answering the first question,

27:51.440 --> 27:53.440
 because I told you that there were two different routes

27:53.440 --> 27:57.440
 we could get to artificial general intelligence and one scares the FPGVs out of me,

27:57.440 --> 27:59.440
 which is this one where we build something,

27:59.440 --> 28:02.440
 we just say bigger neural networks, ever more hardware,

28:02.440 --> 28:07.440
 and it's just trying to get more data and poof, now it's very powerful.

28:07.440 --> 28:11.440
 That, I think, is the most unsafe and reckless approach.

28:11.440 --> 28:17.440
 The alternative to that is the intelligible intelligence approach instead,

28:17.440 --> 28:26.440
 where we say neural networks is just a tool for the first step to get the intuition,

28:26.440 --> 28:33.440
 but then we're going to spend also serious resources on other AI techniques

28:33.440 --> 28:37.440
 for demystifying this black box and figuring out what it's actually doing

28:37.440 --> 28:41.440
 so we can convert it into something that's equally intelligent,

28:41.440 --> 28:44.440
 but that we actually understand what it's doing.

28:44.440 --> 28:46.440
 Maybe we can even prove theorems about it,

28:46.440 --> 28:50.440
 that this car here will never be hacked when it's driving,

28:50.440 --> 28:53.440
 because here is a proof.

28:53.440 --> 28:56.440
 There is a whole science of this, but it doesn't work for neural networks.

28:56.440 --> 29:02.440
 There are big black boxes, but it works well and certain other kinds of codes.

29:02.440 --> 29:05.440
 That approach, I think, is much more promising.

29:05.440 --> 29:07.440
 That's exactly why I'm working on it, frankly,

29:07.440 --> 29:09.440
 not just because I think it's cool for science,

29:09.440 --> 29:14.440
 but because I think the more we understand these systems,

29:14.440 --> 29:18.440
 the better the chances that we can make them do the things that are good for us

29:18.440 --> 29:21.440
 that are actually intended, not unintended.

29:21.440 --> 29:27.440
 You think it's possible to prove things about something as complicated as a neural network?

29:27.440 --> 29:28.440
 That's the hope?

29:28.440 --> 29:34.440
 Well, ideally, there's no reason there has to be a neural network in the end, either.

29:34.440 --> 29:39.440
 We discovered that Newton's laws of gravity with neural network in Newton's head,

29:39.440 --> 29:46.440
 but that's not the way it's programmed into the navigation system of Elon Musk's rocket anymore.

29:46.440 --> 29:50.440
 It's written in C++, or I don't know what language he uses exactly.

29:50.440 --> 29:53.440
 And then there are software tools called symbolic verification.

29:53.440 --> 29:59.440
 DARPA and the US military has done a lot of really great research on this,

29:59.440 --> 30:03.440
 because they really want to understand that when they build weapon systems,

30:03.440 --> 30:06.440
 they don't just go fire at random or malfunction, right?

30:06.440 --> 30:12.440
 And there's even a whole operating system called Cell 3 that's been developed by a DARPA grant

30:12.440 --> 30:17.440
 where you can actually mathematically prove that this thing can never be hacked.

30:17.440 --> 30:24.440
 Well, one day, I hope that will be something you can say about the OS that's running on our laptops, too,

30:24.440 --> 30:26.440
 as you know, but we're not there.

30:26.440 --> 30:29.440
 But I think we should be ambitious, frankly.

30:29.440 --> 30:35.440
 And if we can use machine learning to help do the proofs and so on as well, right,

30:35.440 --> 30:42.440
 then it's much easier to verify that a proof is correct than to come up with a proof in the first place.

30:42.440 --> 30:44.440
 That's really the core idea here.

30:44.440 --> 30:49.440
 If someone comes on your podcast and says they proved the Riemann hypothesis

30:49.440 --> 30:59.440
 or some sensational new theorem, it's much easier for someone else to take some smart math grad students

30:59.440 --> 31:03.440
 and check, oh, there's an error here on equation 5, or this really checks out

31:03.440 --> 31:06.440
 than it was to discover the proof.

31:06.440 --> 31:12.440
 Yeah, although some of those proofs are pretty complicated, but yes, it's still nevertheless much easier to verify the proof.

31:12.440 --> 31:14.440
 I love the optimism.

31:14.440 --> 31:22.440
 You know, even with the security of systems, there's a kind of cynicism that pervades people who think about this,

31:22.440 --> 31:24.440
 which is like, oh, it's hopeless.

31:24.440 --> 31:27.440
 I mean, in the same sense, exactly like you're saying when you own networks,

31:27.440 --> 31:29.440
 oh, it's hopeless to understand what's happening.

31:29.440 --> 31:37.440
 With security, people are just like, well, there's always going to be attack vectors

31:37.440 --> 31:40.440
 and waste to attack the system.

31:40.440 --> 31:43.440
 But you're right, we're just very new with these computational systems.

31:43.440 --> 31:49.440
 We're even new with these intelligence systems, and it's not out of the realm of possibility.

31:49.440 --> 31:53.440
 Just like people that understand the movement of the stars and the planets and so on.

31:53.440 --> 31:54.440
 Yeah.

31:54.440 --> 31:59.440
 It's entirely possible that within, hopefully soon, but it could be within 100 years,

31:59.440 --> 32:04.440
 we start to have an obvious laws of gravity about intelligence.

32:04.440 --> 32:05.440
 Yeah.

32:05.440 --> 32:10.440
 And God forbid, well, consciousness too, that one.

32:10.440 --> 32:11.440
 Agreed.

32:11.440 --> 32:15.440
 You know, I think, of course, if you're selling computers that get hacked a lot,

32:15.440 --> 32:19.440
 that's in your interest as a company that people think it's impossible to make it safe.

32:19.440 --> 32:21.440
 So, you know, but he's going to get the idea of suing you.

32:21.440 --> 32:23.440
 But I want to really inject optimism here.

32:23.440 --> 32:30.440
 It's absolutely possible to do much better than we're doing now.

32:30.440 --> 32:34.440
 And you know, your laptop does so much stuff.

32:34.440 --> 32:41.440
 You don't need the music player to be super safe in your future self driving car, right?

32:41.440 --> 32:47.440
 If someone hacks it and starts playing music, you don't like the world on end.

32:47.440 --> 32:52.440
 But what you can do is you can break out and say the drive computer that controls your safety

32:52.440 --> 32:57.440
 must be completely physically decoupled entirely from the entertainment system.

32:57.440 --> 33:02.440
 And it must physically be such that it can't take on over the air updates while you're driving.

33:02.440 --> 33:09.440
 And it can be, it can have, it's not that, it can have ultimately some operating system on it,

33:09.440 --> 33:17.440
 which is symbolically verified and proven that it's always going to do what it's supposed to do, right?

33:17.440 --> 33:20.440
 We can basically have, and companies should take that attitude too.

33:20.440 --> 33:24.440
 They should look at everything they do and say, what are the few systems in our company

33:24.440 --> 33:28.440
 that threaten the whole life of the company if they get hacked, you know,

33:28.440 --> 33:32.440
 and have the highest standards for them, and then they can save money

33:32.440 --> 33:36.440
 by going for the El Chippo poorly understood stuff for the rest, you know.

33:36.440 --> 33:38.440
 This is very feasible, I think.

33:38.440 --> 33:42.440
 And coming back to the bigger question about, that you worried about,

33:42.440 --> 33:47.440
 that there'll be unintentional failures, I think, there are two quite separate risks here, right?

33:47.440 --> 33:52.440
 We talked a lot about one of them, which is that the goals are noble of the human.

33:52.440 --> 33:56.440
 The human says, I want this airplane to not crash,

33:56.440 --> 34:00.440
 because this is not Muhammad Atta now flying the airplane, right?

34:00.440 --> 34:05.440
 And now there's this technical challenge of making sure that the autopilot

34:05.440 --> 34:10.440
 is actually going to behave as the pilot wants.

34:10.440 --> 34:13.440
 If you set that aside, there's also the separate question.

34:13.440 --> 34:19.440
 How do you make sure that the goals of the pilot are actually aligned with the goals of the passenger?

34:19.440 --> 34:24.440
 How do you make sure very much more broadly that if we can all agree as a species

34:24.440 --> 34:27.440
 that we would like things to kind of go well for humanity as a whole,

34:27.440 --> 34:31.440
 that the goals are aligned here, the alignment problem.

34:31.440 --> 34:41.440
 And yeah, there's been a lot of progress in the sense that there's suddenly huge amounts of research going on about it.

34:41.440 --> 34:44.440
 I'm very grateful to Elon Musk for giving us that money five years ago,

34:44.440 --> 34:49.440
 so we could launch the first research program on technical AI safety and alignment.

34:49.440 --> 34:51.440
 There's a lot of stuff happening.

34:51.440 --> 34:57.440
 I think we need to do more than just make sure little machines do always what their owners do.

34:57.440 --> 35:00.440
 That wouldn't have prevented September 11.

35:00.440 --> 35:06.440
 Muhammad Atta said, OK, autopilot, please fly into World Trade Center.

35:06.440 --> 35:10.440
 And it's like, OK, that even happened.

35:10.440 --> 35:15.440
 In a different situation, there was this depressed pilot named Andreas Lubitz,

35:15.440 --> 35:18.440
 who told his German wings passenger jet to fly into the Alps.

35:18.440 --> 35:23.440
 He just told the computer to change the altitude to 100 meters or something like that.

35:23.440 --> 35:25.440
 And you know what the computer said?

35:25.440 --> 35:26.440
 OK.

35:26.440 --> 35:29.440
 And it had the frigging topographical map of the Alps in there.

35:29.440 --> 35:31.440
 It had GPS, everything.

35:31.440 --> 35:36.440
 No one had bothered teaching it even the basic kindergarten ethics of, like, no.

35:36.440 --> 35:42.440
 We never want airplanes to fly into mountains under any circumstances.

35:42.440 --> 35:49.440
 And so we have to think beyond just the technical issues

35:49.440 --> 35:54.440
 and think about how do we align, in general, incentives on this planet for the greater good.

35:54.440 --> 35:58.440
 So starting with simple stuff like that, every airplane that has a computer in it

35:58.440 --> 36:03.440
 should be taught whatever kindergarten ethics it's smart enough to understand.

36:03.440 --> 36:08.440
 Like, no, don't fly into fixed objects if the pilot tells you to do so.

36:08.440 --> 36:13.440
 And then go on autopilot mode, send an email to the cops

36:13.440 --> 36:16.440
 and land at the latest airport, nearest airport.

36:16.440 --> 36:22.440
 Any car with a forward facing camera should just be programmed by the manufacturers

36:22.440 --> 36:26.440
 so that it will never accelerate into a human ever.

36:26.440 --> 36:30.440
 That would avoid things like the niece attack

36:30.440 --> 36:35.440
 and many horrible terrorist vehicle attacks where they deliberately did that, right?

36:35.440 --> 36:39.440
 There's not some sort of thing, oh, you know, US and China, different views.

36:39.440 --> 36:45.440
 No, there was not a single car manufacturer in the world who wanted the cars to do this.

36:45.440 --> 36:47.440
 They just hadn't thought to do the alignment.

36:47.440 --> 36:52.440
 And if you look at, more broadly, problems that happen on this planet,

36:52.440 --> 36:55.440
 the vast majority have to do a poor alignment.

36:55.440 --> 37:01.440
 I mean, think about, let's go back really big, because I know you're so good at that.

37:01.440 --> 37:07.440
 So long ago in evolution, we had these genes and they wanted to make copies of themselves.

37:07.440 --> 37:09.440
 That's really all they cared about.

37:09.440 --> 37:15.440
 So some genes said, hey, I'm going to build a brain on this body I'm in

37:15.440 --> 37:18.440
 so that I can get better at making copies to myself.

37:18.440 --> 37:22.440
 And then they decided for their benefit to get copied more,

37:22.440 --> 37:25.440
 to align your brain's incentives with their incentives.

37:25.440 --> 37:29.440
 So it didn't want you to starve to death.

37:29.440 --> 37:32.440
 So it gave you an incentive to eat.

37:32.440 --> 37:36.440
 And it wanted you to make copies of the genes.

37:36.440 --> 37:41.440
 So it gave you an incentive to fall in love and do all sorts of naughty things

37:41.440 --> 37:44.440
 to make copies of itself, right?

37:44.440 --> 37:48.440
 So that was successful value alignment done on the genes.

37:48.440 --> 37:51.440
 They created something more intelligent than themselves,

37:51.440 --> 37:53.440
 but they made sure to try to align the values.

37:53.440 --> 37:59.440
 But then something went a little bit wrong against the idea of what the genes wanted

37:59.440 --> 38:05.440
 because a lot of humans discovered, hey, we really like this business about sex

38:05.440 --> 38:09.440
 that the genes have made us enjoy, but we don't want to have babies right now.

38:09.440 --> 38:14.440
 So we're going to hack the genes and use birth control.

38:14.440 --> 38:19.440
 And I really feel like drinking a Coca Cola right now,

38:19.440 --> 38:22.440
 but I don't want to get a potbelly, so I'm going to drink Diet Coke.

38:22.440 --> 38:26.440
 We have all these things we've figured out because we're smarter than the genes,

38:26.440 --> 38:29.440
 how we can actually subvert their intentions.

38:29.440 --> 38:34.440
 So it's not surprising that we humans now, when we're in the role of these genes,

38:34.440 --> 38:39.440
 creating other nonhuman entities with a lot of power have to face the same exact challenge.

38:39.440 --> 38:44.440
 How do we make other powerful entities have incentives that are aligned with ours

38:44.440 --> 38:46.440
 so they won't hack them?

38:46.440 --> 38:48.440
 Corporations, for example, right?

38:48.440 --> 38:53.440
 We humans decided to create corporations because it can benefit us greatly.

38:53.440 --> 38:56.440
 Now all of a sudden there's a supermarket. I can go buy food there.

38:56.440 --> 38:59.440
 I don't have to hunt. Awesome.

38:59.440 --> 39:04.440
 And then to make sure that this corporation would do things that were good for us

39:04.440 --> 39:08.440
 and not bad for us, we created institutions to keep them in check.

39:08.440 --> 39:12.440
 Like if the local supermarket sells poisonous food,

39:12.440 --> 39:21.440
 then the owners of the supermarket have to spend some years reflecting behind bars, right?

39:21.440 --> 39:25.440
 So we created incentives to get to align them.

39:25.440 --> 39:28.440
 But of course, just like we were able to see through this thing,

39:28.440 --> 39:31.440
 well, birth control, if you're a powerful corporation,

39:31.440 --> 39:36.440
 you also have an incentive to try to hack the institutions that are supposed to govern you

39:36.440 --> 39:40.440
 because you ultimately as a corporation have an incentive to maximize your profit.

39:40.440 --> 39:45.440
 Just like you have an incentive to maximize the enjoyment your brain has, not for your genes.

39:45.440 --> 39:51.440
 So if they can figure out a way of bribing regulators, then they're going to do that.

39:51.440 --> 39:57.440
 In the US, we kind of caught on to that and made laws against corruption and bribery.

39:57.440 --> 40:03.440
 Then in the late 1800s, Teddy Roosevelt realized that,

40:03.440 --> 40:08.440
 no, we were still being kind of hacked because the Massachusetts railroad companies had like a bigger budget

40:08.440 --> 40:13.440
 than the state of Massachusetts and they were doing a lot of very corrupt stuff.

40:13.440 --> 40:18.440
 So he did the whole trust busting thing to try to align these other nonhuman entities,

40:18.440 --> 40:23.440
 the companies, again, more with the incentives of Americans as a whole.

40:23.440 --> 40:27.440
 It's not surprising though that this is a battle you have to keep fighting.

40:27.440 --> 40:31.440
 Now we have even larger companies than we ever had before.

40:31.440 --> 40:38.440
 And of course, they're going to try to, again, subvert the institutions.

40:38.440 --> 40:47.440
 Not because, you know, I think people make a mistake of getting all too black thinking about things in terms of good and evil.

40:47.440 --> 40:53.440
 Like arguing about whether corporations are good or evil or whether robots are good or evil.

40:53.440 --> 40:57.440
 A robot isn't good or evil. It's tool.

40:57.440 --> 41:01.440
 And you can use it for great things like robotic surgery or for bad things.

41:01.440 --> 41:04.440
 And a corporation also is a tool, of course.

41:04.440 --> 41:10.440
 And if you have good incentives to the corporation, it'll do great things like start a hospital or a grocery store.

41:10.440 --> 41:19.440
 If you have really bad incentives, then it's going to start maybe marketing addictive drugs to people and you'll have an opioid epidemic.

41:19.440 --> 41:30.440
 It's all about, we should not make a mistake of getting into some sort of fairytale, good, evil thing about corporations or robots.

41:30.440 --> 41:33.440
 We should focus on putting the right incentives in place.

41:33.440 --> 41:38.440
 My optimistic vision is that if we can do that, then we can really get good things.

41:38.440 --> 41:46.440
 We're not doing so great with that right now, either on AI, I think, or on other intelligent, nonhuman entities like big companies.

41:46.440 --> 41:50.440
 We just have a new secretary of defense.

41:50.440 --> 41:58.440
 There's going to start up now in the Biden administration who was an active member of the board of Raytheon.

41:58.440 --> 42:04.440
 I have nothing against Raytheon.

42:04.440 --> 42:14.440
 I'm not a pacifist, but there's an obvious conflict of interest if someone is in the job where they decide who they're going to contract with.

42:14.440 --> 42:23.440
 I think somehow we have, maybe we need another Teddy Roosevelt to come along again and say, hey, we want what's good for all Americans.

42:23.440 --> 42:32.440
 We need to go do some serious realigning again of the incentives that we're giving to these big companies.

42:32.440 --> 42:34.440
 Then we're going to be better off.

42:34.440 --> 42:45.440
 Naturally, with human beings, just like you beautifully described the history of this whole thing, it all started with the genes and they're probably pretty upset by all the unintended consequences that happened since.

42:45.440 --> 42:48.440
 It seems that it kind of works out.

42:48.440 --> 42:53.440
 It's in this collective intelligence that emerges at the different levels.

42:53.440 --> 43:02.440
 It seems to find, sometimes last minute, a way to realign the values or keep the values aligned.

43:02.440 --> 43:04.440
 It finds a way.

43:04.440 --> 43:13.440
 Different leaders, different humans pop up all over the place that reset the system.

43:13.440 --> 43:15.440
 Do you have an explanation why that is?

43:15.440 --> 43:17.440
 Or is that just survivor bias?

43:17.440 --> 43:33.440
 Also, is that somehow fundamentally different than with the AI systems where you're no longer dealing with something that was a direct, maybe companies are the same, a direct byproduct of the evolutionary process?

43:33.440 --> 43:36.440
 I think there is one thing which has changed.

43:36.440 --> 43:51.440
 That's why I'm not all optimistic. That's why I think there's about a 50% chance if we take the dumb route with artificial intelligence that humanity will be extinct in this century.

43:51.440 --> 43:53.440
 First, just the big picture.

43:53.440 --> 43:57.440
 Companies need to have the right incentives.

43:57.440 --> 43:59.440
 Even governments, right?

43:59.440 --> 44:07.440
 We used to have governments, usually there were just some king who was the king because his dad was the king.

44:07.440 --> 44:18.440
 Then there were some benefits of having this powerful kingdom or empire of any sort because then it could prevent a lot of local squabbles.

44:18.440 --> 44:21.440
 So at least everybody in that region would stop warring against each other.

44:21.440 --> 44:25.440
 Their incentives of different cities in the kingdom became more aligned.

44:25.440 --> 44:27.440
 That was the whole selling point.

44:27.440 --> 44:28.440
 Harari.

44:28.440 --> 44:35.440
 Harari has a beautiful piece on how empires were collaboration enablers.

44:35.440 --> 44:43.440
 Harari has invented money for that reason so we could have better alignment and trade even with people we didn't know.

44:43.440 --> 44:47.440
 This sort of stuff has been playing out since time immemorial.

44:47.440 --> 44:51.440
 What's changed is that it happens on ever larger scales.

44:51.440 --> 44:54.440
 Technology keeps getting better because science gets better.

44:54.440 --> 44:59.440
 So now we can communicate over larger distances, transport things faster over larger distances.

44:59.440 --> 45:04.440
 So the entities get ever bigger but our planet is not getting bigger anymore.

45:04.440 --> 45:20.440
 So in the past, you could have one experiment that just totally screwed up like Easter Island where they actually managed to have such poor alignment that when they went extinct, people there, there was no one else to come back and replace them.

45:20.440 --> 45:28.440
 If Elon Musk doesn't get us to Mars and then we go extinct on a global scale, then we're not coming back.

45:28.440 --> 45:30.440
 That's the fundamental difference.

45:30.440 --> 45:35.440
 And that's a mistake I would rather we don't make for that reason.

45:35.440 --> 45:41.440
 In the past, of course, history is full of fiascos, but it was never the whole planet.

45:41.440 --> 45:45.440
 And then, okay, now there's this nice uninhabited land here.

45:45.440 --> 45:48.440
 Some other people could move in and organize things better.

45:48.440 --> 45:50.440
 This is different.

45:50.440 --> 46:00.440
 The second thing which is also different is that technology gives us so much more empowerment both to do good things and also to screw up.

46:00.440 --> 46:04.440
 In the Stone Age, even if you had someone whose goals were really poorly aligned,

46:04.440 --> 46:12.440
 maybe he was really pissed off because his Stone Age girlfriend dumped him and he just wanted to kill as many people as he could.

46:12.440 --> 46:16.440
 How many could he really take out with a rock and a stick before he was overpowered?

46:16.440 --> 46:18.440
 Right, just handful, right?

46:18.440 --> 46:27.440
 Now, with today's technology, if we have an accidental nuclear war between Russia and the US,

46:27.440 --> 46:32.440
 which we almost have about a dozen times and then we have a nuclear winter,

46:32.440 --> 46:36.440
 it could take out 7 billion people or 6 billion people, we don't know.

46:36.440 --> 46:40.440
 So the scale of damage is bigger that we can do.

46:40.440 --> 46:51.440
 And if there's obviously no law of physics that says that technology will never get powerful enough that we could wipe out our species entirely,

46:51.440 --> 46:57.440
 that would just be fantasy to think that science is somehow doomed not to get more powerful than that, right?

46:57.440 --> 47:04.440
 And it's not at all unfeasible in our lifetime that someone could design a designer pandemic which spreads as easily as COVID,

47:04.440 --> 47:06.440
 but just basically kills everybody.

47:06.440 --> 47:12.440
 We already had smallpox, it killed one third of everybody who got it.

47:12.440 --> 47:19.440
 What do you think of the, here's an intuition, maybe it's completely naive and this optimistic intuition I have,

47:19.440 --> 47:23.440
 which it seems, and maybe it's a biased experience that I have,

47:23.440 --> 47:33.440
 but it seems like the most brilliant people I've met in my life all are really fundamentally good human beings.

47:33.440 --> 47:41.440
 And not like naive, good, like they really want to do good for the world in a way that well maybe is aligned to my sense of what good means.

47:41.440 --> 47:50.440
 And so I have a sense that the people that will be defining the very cutting edge of technology,

47:50.440 --> 47:55.440
 there will be much more of the ones that are doing good versus the ones that are doing evil.

47:55.440 --> 48:03.440
 So the race, I'm optimistic on us always like last minute coming up with a solution.

48:03.440 --> 48:11.440
 So if there's an engineered pandemic that has the capability to destroy most of the human civilization,

48:11.440 --> 48:17.440
 it feels like to me either leading up to that before or as it's going on,

48:17.440 --> 48:23.440
 there will be, we're able to rally the collective genius of the human species.

48:23.440 --> 48:29.440
 I could tell by your smile that you're at least some percentage doubtful,

48:29.440 --> 48:37.440
 but could that be a fundamental law of human nature that evolution only creates,

48:37.440 --> 48:43.440
 like karma is beneficial, good is beneficial and therefore will be alright?

48:43.440 --> 48:46.440
 I hope you're right.

48:46.440 --> 48:48.440
 I would really love it if you're right,

48:48.440 --> 48:52.440
 if there's some sort of law of nature that says that we always get lucky in the last second

48:52.440 --> 49:02.440
 because of karma, but I prefer not playing it so close and gambling on that.

49:02.440 --> 49:07.440
 And I think, in fact, I think it can be dangerous to have too strong faith in that

49:07.440 --> 49:10.440
 because it makes us complacent.

49:10.440 --> 49:13.440
 Like if someone tells you you never have to worry about your house burning down,

49:13.440 --> 49:16.440
 then you're not going to put in a smoke detector because why would you need to, right?

49:16.440 --> 49:20.440
 Even if it's sometimes very simple precautions, we don't take them.

49:20.440 --> 49:24.440
 If you're like, oh, the government is going to take care of everything for us.

49:24.440 --> 49:26.440
 I can always trust my politicians.

49:26.440 --> 49:28.440
 We abdicate our own responsibility.

49:28.440 --> 49:31.440
 I think it's a healthier attitude to say, yeah, maybe things will work out,

49:31.440 --> 49:37.440
 but maybe I'm actually going to have to myself step up and take responsibility.

49:37.440 --> 49:39.440
 And the stakes are so huge.

49:39.440 --> 49:44.440
 I mean, if we do this right, we can develop all this ever more powerful technology

49:44.440 --> 49:49.440
 and cure all diseases and create a future where humanity is healthy and wealthy

49:49.440 --> 49:53.440
 or not just the next election cycle, but like billions of years throughout our universe.

49:53.440 --> 49:58.440
 That's really worth working hard for and not just, you know, sitting and hoping

49:58.440 --> 50:00.440
 for some sort of fairytale karma.

50:00.440 --> 50:02.440
 Well, I just mean, so you're absolutely right.

50:02.440 --> 50:04.440
 From the perspective of the individual, like for me,

50:04.440 --> 50:07.440
 like the primary thing should be to take responsibility

50:07.440 --> 50:12.440
 and to build the solutions that your skill set allows to build.

50:12.440 --> 50:13.440
 Which is a lot.

50:13.440 --> 50:16.440
 I think we underestimate often very much how much good we can do.

50:16.440 --> 50:23.440
 If you or anyone listening to this is completely confident that our government

50:23.440 --> 50:28.440
 would do a perfect job on handling any future crisis with engineered pandemics

50:28.440 --> 50:30.440
 or future AI.

50:30.440 --> 50:32.440
 The one or two people out there.

50:32.440 --> 50:36.440
 On what actually happened in 2020.

50:36.440 --> 50:42.440
 Do you feel that government by and large around the world is handled flawlessly?

50:42.440 --> 50:48.440
 That's a really sad and disappointing reality that hopefully is a wake up call for everybody.

50:48.440 --> 50:54.440
 For the scientists, for the engineers, for the researchers and AI especially.

50:54.440 --> 51:04.440
 It was disappointing to see how inefficient we were at collecting the right amount of data

51:04.440 --> 51:07.440
 in a privacy preserving way and spreading that data

51:07.440 --> 51:10.440
 and utilizing that data to make decisions, all that kind of stuff.

51:10.440 --> 51:17.440
 I think when something bad happens to me, I made myself a promise many years ago

51:17.440 --> 51:21.440
 that I would not be a whiner.

51:21.440 --> 51:27.440
 So when something bad happens to me, of course it's just a process of disappointment.

51:27.440 --> 51:30.440
 But then I try to focus on what did I learn from this

51:30.440 --> 51:32.440
 that can make me a better person in the future.

51:32.440 --> 51:35.440
 And there's usually something to be learned when I fail.

51:35.440 --> 51:41.440
 And I think we should all ask ourselves, what can we learn from the pandemic

51:41.440 --> 51:43.440
 about how we can do better in the future?

51:43.440 --> 51:46.440
 And you mentioned there's a really good lesson.

51:46.440 --> 51:49.440
 We were not as resilient as we thought we were.

51:49.440 --> 51:53.440
 And we were not as prepared maybe as we wish we were.

51:53.440 --> 51:56.440
 You can even see very stark contrast around the planet.

51:56.440 --> 52:01.440
 South Korea, they have over 50 million people.

52:01.440 --> 52:05.440
 Do you know how many deaths they have from COVID last time I checked?

52:05.440 --> 52:08.440
 It's about 500.

52:08.440 --> 52:10.440
 Why is that?

52:10.440 --> 52:16.440
 Well, the short answer is that they had prepared.

52:16.440 --> 52:21.440
 They were incredibly quick, incredibly quick to get on it

52:21.440 --> 52:25.440
 with very rapid testing and contact tracing and so on,

52:25.440 --> 52:30.440
 which is why they never had more cases than they could contract trace effectively, right?

52:30.440 --> 52:33.440
 They even had to have the kind of big lockdowns we had in the West.

52:33.440 --> 52:39.440
 But the deeper answer to it's not just Koreans are just somehow better people.

52:39.440 --> 52:45.440
 The reason I think they were better prepared was because they had already had a pretty bad hit

52:45.440 --> 52:49.440
 from the SARS pandemic, which never became a pandemic.

52:49.440 --> 52:52.440
 Something like 17 years ago, I think.

52:52.440 --> 52:56.440
 So it was kind of a fresh memory that we need to be prepared for pandemics.

52:56.440 --> 52:58.440
 So they were, right?

52:58.440 --> 53:03.440
 So maybe this is a lesson here for all of us to draw from COVID

53:03.440 --> 53:07.440
 that rather than just wait for the next pandemic or the next problem

53:07.440 --> 53:10.440
 with AI getting out of control or anything else,

53:10.440 --> 53:16.440
 maybe we should just actually set aside a tiny fraction of our GDP

53:16.440 --> 53:20.440
 to have people very systematically do some horizon scanning

53:20.440 --> 53:22.440
 and say, okay, what are the things that could go wrong?

53:22.440 --> 53:25.440
 And let's do get out and see which are the more likely ones

53:25.440 --> 53:31.440
 and which are the ones that are actually actionable and then be prepared.

53:31.440 --> 53:38.440
 So one of the observations as one little ant slash human that I am of disappointment

53:38.440 --> 53:47.440
 is the political division over information that has been observed that I observed this year

53:47.440 --> 53:56.440
 that it seemed the discussion was less about sort of what happened

53:56.440 --> 54:03.440
 and understanding what happened deeply and more about there's different truths out there.

54:03.440 --> 54:07.440
 And it's like an argument, my truth is better than your truth.

54:07.440 --> 54:10.440
 And it's like red versus blue or different.

54:10.440 --> 54:16.440
 It was like this ridiculous discourse that doesn't seem to get at any kind of notion of the truth.

54:16.440 --> 54:18.440
 It's not like there's some kind of scientific process.

54:18.440 --> 54:23.440
 Even science got politicized in ways that's very heartbreaking to me.

54:23.440 --> 54:34.440
 You have an exciting project on the AI front of trying to rethink one of the mentioned corporations.

54:34.440 --> 54:38.440
 There's one of the other collective intelligence systems that have emerged

54:38.440 --> 54:46.440
 from this is social networks and just the spread of information on the internet,

54:46.440 --> 54:48.440
 our ability to share that information.

54:48.440 --> 54:50.440
 There's all different kinds of news sources and so on.

54:50.440 --> 54:53.440
 And so you said like that's from first principles.

54:53.440 --> 54:59.440
 Let's rethink how we think about the news, how we think about information.

54:59.440 --> 55:03.440
 Can you talk about this amazing effort that you're undertaking?

55:03.440 --> 55:04.440
 Oh, I'd love to.

55:04.440 --> 55:11.440
 But this has been my big COVID project has been nights and weekends on ever since the lockdown.

55:11.440 --> 55:14.440
 To segue into this, actually, let me come back to what you said earlier,

55:14.440 --> 55:18.440
 that you had this hope that in your experience, people who you felt were very talented,

55:18.440 --> 55:21.440
 often idealistic and wanted to do good.

55:21.440 --> 55:25.440
 Frankly, I feel the same about all people by and large.

55:25.440 --> 55:29.440
 There are always exceptions, but I think the vast majority of everybody,

55:29.440 --> 55:33.440
 regardless of education and whatnot, really are fundamentally good, right?

55:33.440 --> 55:37.440
 So how can it be that people still do so much nasty stuff?

55:37.440 --> 55:42.440
 I think it has everything to do with the information that we're given.

55:42.440 --> 55:49.440
 If you go into Sweden 500 years ago and you start telling all the farmers that those Danes in Denmark,

55:49.440 --> 55:55.440
 they're so terrible people and we have to invade them because they've done all these terrible things

55:55.440 --> 55:57.440
 that you can't fact check yourself.

55:57.440 --> 55:59.440
 A lot of people in Sweden did that.

55:59.440 --> 56:09.440
 And we've seen so much of this today in the world, both geopolitically,

56:09.440 --> 56:14.440
 where we are told that China is bad and Russia is bad and Venezuela is bad

56:14.440 --> 56:17.440
 and people in those countries are often told that we are bad.

56:17.440 --> 56:22.440
 And we also see it at a micro level, where people are told that,

56:22.440 --> 56:25.440
 oh, those who voted for the other party are bad people.

56:25.440 --> 56:30.440
 It's not just an intellectual disagreement, but they're bad people

56:30.440 --> 56:33.440
 and we're getting ever more divided.

56:33.440 --> 56:40.440
 And so how do you reconcile this with intrinsic goodness in people?

56:40.440 --> 56:43.440
 I think it's pretty obvious that it has again to do with this,

56:43.440 --> 56:46.440
 with information that we're fed and given, right?

56:46.440 --> 56:52.440
 We evolved to live in small groups where you might know 30 people in total, right?

56:52.440 --> 56:57.440
 So you then had a system that was quite good for assessing who you could trust

56:57.440 --> 56:58.440
 and who you could not.

56:58.440 --> 57:03.440
 And if someone told you that Joe there is a jerk,

57:03.440 --> 57:06.440
 but you had interacted with him yourself and seen him in action,

57:06.440 --> 57:11.440
 you would quickly realize maybe that that's actually not quite accurate, right?

57:11.440 --> 57:15.440
 But now that most people on the planet are people we've never met,

57:15.440 --> 57:19.440
 it's very important that we have a way of trusting information we're given.

57:19.440 --> 57:23.440
 So, okay, so where does the news project come in?

57:23.440 --> 57:27.440
 Well, throughout history, you can go read Machiavelli from the 1400s

57:27.440 --> 57:31.440
 and you'll see how already then there were busy manipulating people with propaganda and stuff.

57:31.440 --> 57:35.440
 Propaganda is not new at all.

57:35.440 --> 57:39.440
 And the incentives to manipulate people is just not new at all.

57:39.440 --> 57:41.440
 What is it that's new?

57:41.440 --> 57:45.440
 What's new is machine learning meets propaganda.

57:45.440 --> 57:46.440
 That's what's new.

57:46.440 --> 57:48.440
 That's why this has gotten so much worse.

57:48.440 --> 57:53.440
 Some people like to blame certain individuals like in my liberal university bubble,

57:53.440 --> 57:57.440
 many people blame Donald Trump and say it was his fault.

57:57.440 --> 57:59.440
 I see it differently.

57:59.440 --> 58:05.440
 I think Donald Trump just had this extreme skill at playing this game

58:05.440 --> 58:09.440
 in the machine learning algorithm age.

58:09.440 --> 58:12.440
 A game he couldn't have played 10 years ago.

58:12.440 --> 58:13.440
 So what's changed?

58:13.440 --> 58:17.440
 What's changed is, well, Facebook and Google and other companies.

58:17.440 --> 58:19.440
 I'm not a bad man, I think them.

58:19.440 --> 58:22.440
 I have a lot of friends who work for these companies, good people.

58:22.440 --> 58:27.440
 They deployed machine learning algorithms just to increase their profit a little bit

58:27.440 --> 58:31.440
 to just maximize the time people spent watching ads.

58:31.440 --> 58:35.440
 And they had totally underestimated how effective they were going to be.

58:35.440 --> 58:39.440
 This was, again, the black box, non intelligible intelligence.

58:39.440 --> 58:42.440
 They just noticed, oh, we're getting more ad revenue, great.

58:42.440 --> 58:47.440
 It took a long time until even realize why and how damaging this was for society.

58:47.440 --> 58:51.440
 Because, of course, what the machine learning figured out was

58:51.440 --> 58:56.440
 that the by far most effective way of gluing you to your little rectangle

58:56.440 --> 59:02.440
 was to show you things that triggered strong emotions, anger, et cetera, resentment.

59:02.440 --> 59:07.440
 And if it was true or not, it didn't really matter.

59:07.440 --> 59:10.440
 It was also easier to find stories that weren't true.

59:10.440 --> 59:13.440
 If you weren't limited, that's just a limitation to show people.

59:13.440 --> 59:15.440
 That's a very limiting fact.

59:15.440 --> 59:21.440
 And before long, we got these amazing filter bubbles on a scale we had never seen before.

59:21.440 --> 59:28.440
 A couple of this to the fact that also the online news media were so effective

59:28.440 --> 59:30.440
 that they killed a lot of print journalism.

59:30.440 --> 59:35.440
 There's less than half as many journalists now in America, I believe,

59:35.440 --> 59:39.440
 as there was a generation ago.

59:39.440 --> 59:42.440
 He just couldn't compete with the online advertising.

59:42.440 --> 59:48.440
 So, all of a sudden, most people are not getting even reading newspapers.

59:48.440 --> 59:50.440
 They get their news from social media.

59:50.440 --> 59:55.440
 And most people only get news in their little bubble.

59:55.440 --> 59:59.440
 So, along comes now some people like Donald Trump who figured out

59:59.440 --> 1:00:03.440
 among the first successful politicians to figure out how to really play this new game

1:00:03.440 --> 1:00:05.440
 and become very, very influential.

1:00:05.440 --> 1:00:10.440
 But I think Donald Trump took advantage of it.

1:00:10.440 --> 1:00:16.440
 He didn't create the fundamental conditions were created by machine learning

1:00:16.440 --> 1:00:18.440
 taking over the news media.

1:00:18.440 --> 1:00:23.440
 So, this is what motivated my little COVID project here.

1:00:23.440 --> 1:00:27.440
 I said before, machine learning and tech in general is not evil,

1:00:27.440 --> 1:00:28.440
 but it's also not good.

1:00:28.440 --> 1:00:32.440
 It's just a tool that you can use for good things or bad things.

1:00:32.440 --> 1:00:37.440
 And as it happens, machine learning and news was mainly used by the big players,

1:00:37.440 --> 1:00:42.440
 big tech, to manipulate people and to watch as many ads as possible,

1:00:42.440 --> 1:00:46.440
 which had this unintended consequence of really screwing up our democracy

1:00:46.440 --> 1:00:49.440
 and fragmenting it into filter bubbles.

1:00:49.440 --> 1:00:53.440
 So, I thought, well, machine learning algorithms are basically free.

1:00:53.440 --> 1:00:57.440
 They can run on your smartphone for free also if someone gives them away to you, right?

1:00:57.440 --> 1:01:02.440
 There's no reason why they only have to help the big guy to manipulate the little guy.

1:01:02.440 --> 1:01:07.440
 They can just as well help the little guy to see through all the manipulation attempts

1:01:07.440 --> 1:01:08.440
 from the big guy.

1:01:08.440 --> 1:01:12.440
 So, did this project, you can go to improvethenews.org.

1:01:12.440 --> 1:01:16.440
 The first thing we've built is this little news aggregator.

1:01:16.440 --> 1:01:19.440
 Looks a bit like Google News except it has these sliders on it

1:01:19.440 --> 1:01:21.440
 to help you break out of your filter bubble.

1:01:21.440 --> 1:01:26.440
 So, if you're reading, you can click click and go to your favorite topic.

1:01:26.440 --> 1:01:32.440
 And then, if you just slide the left right slider all the way over to the left.

1:01:32.440 --> 1:01:33.440
 There's two sliders, right?

1:01:33.440 --> 1:01:34.440
 Yeah.

1:01:34.440 --> 1:01:38.440
 There's the one, the most obvious one is the one that has left to right labeled on us.

1:01:38.440 --> 1:01:41.440
 You go to left, you get one set of articles, you go to the right,

1:01:41.440 --> 1:01:43.440
 you see a very different truth appearing.

1:01:43.440 --> 1:01:47.440
 Well, that's literally left and right on the political spectrum.

1:01:47.440 --> 1:01:54.440
 Yeah, so if you're reading about immigration, for example, it's very, very noticeable.

1:01:54.440 --> 1:01:58.440
 And I think step one, always if you want to not get manipulated,

1:01:58.440 --> 1:02:02.440
 it's just to be able to recognize the techniques people use.

1:02:02.440 --> 1:02:06.440
 So, it's very helpful to just see how they spin things on the two sides.

1:02:06.440 --> 1:02:13.440
 I think many people are under the misconception that the main problem is fake news.

1:02:13.440 --> 1:02:14.440
 It's not.

1:02:14.440 --> 1:02:19.440
 I had an amazing team of MIT students where we did an academic project,

1:02:19.440 --> 1:02:24.440
 used machine learning to detect the main kinds of bias over the summer.

1:02:24.440 --> 1:02:30.440
 Yes, of course, sometimes there's fake news where someone just claims something that's false, right?

1:02:30.440 --> 1:02:33.440
 Like, oh, Hillary Clinton just got divorced or something.

1:02:33.440 --> 1:02:34.440
 Yes.

1:02:34.440 --> 1:02:38.440
 But what we see much more of is actually just omissions.

1:02:38.440 --> 1:02:45.440
 If you go to, there's some stories which just won't be mentioned by the left or the right

1:02:45.440 --> 1:02:47.440
 because it doesn't suit their agenda.

1:02:47.440 --> 1:02:50.440
 And then they also mentioned other ones very, very, very much.

1:02:50.440 --> 1:03:00.440
 So, for example, we've had a number of stories about the Trump family's financial dealings.

1:03:00.440 --> 1:03:06.440
 And then there's been a bunch of stories about the Biden family's, Hunter Biden's financial dealings, right?

1:03:06.440 --> 1:03:10.440
 Surprise, surprise, they don't get equal coverage on the left and the right.

1:03:10.440 --> 1:03:14.440
 One side loves to cover the Biden, Hunter Biden's stuff.

1:03:14.440 --> 1:03:18.440
 And one side loves to cover the Trump, you can never guess which is which, right?

1:03:18.440 --> 1:03:25.440
 But the great news is if you're a normal American citizen and you dislike corruption in all its forms,

1:03:25.440 --> 1:03:33.440
 then slide, slide, you can just look at both sides and you'll see all those political corruption stories.

1:03:33.440 --> 1:03:40.440
 It's really liberating to just take in the both sides, the spin on both sides.

1:03:40.440 --> 1:03:47.440
 It somehow unlocks your mind to think on your own, to realize that, I don't know,

1:03:47.440 --> 1:03:58.440
 it's the same thing that was useful in the Soviet Union times for when everybody was much more aware that they're surrounded by propaganda.

1:03:58.440 --> 1:04:01.440
 That is so interesting what you're saying, actually.

1:04:01.440 --> 1:04:08.440
 So, Noam Chomsky used to be our MIT colleague once said that propaganda is to democracy.

1:04:08.440 --> 1:04:12.440
 What violence is to totalitarianism.

1:04:12.440 --> 1:04:20.440
 And what he means by that is if you have a really totalitarian government, you don't need propaganda.

1:04:20.440 --> 1:04:24.440
 People will do what you want them to do anyway out of fear, right?

1:04:24.440 --> 1:04:28.440
 But otherwise, you need propaganda.

1:04:28.440 --> 1:04:34.440
 So, I would say actually that the propaganda is much higher quality in democracies, much more believable.

1:04:34.440 --> 1:04:42.440
 And it's really striking when I talk to colleagues, science colleagues like from Russia and China and so on,

1:04:42.440 --> 1:04:47.440
 I notice they are actually much more aware of the propaganda in their own media

1:04:47.440 --> 1:04:51.440
 than many of my American colleagues are about the propaganda in Western media.

1:04:51.440 --> 1:04:55.440
 That's brilliant. That means the propaganda in the Western media is just better.

1:04:55.440 --> 1:04:57.440
 Yes, that's so brilliant.

1:04:57.440 --> 1:05:05.440
 Even the propaganda.

1:05:05.440 --> 1:05:10.440
 But once you realize that, you realize there's also something very optimistic there that you can do about it, right?

1:05:10.440 --> 1:05:13.440
 Because, first of all, omissions.

1:05:13.440 --> 1:05:19.440
 As long as there's no outright censorship, you can just look at both sides

1:05:19.440 --> 1:05:25.440
 and pretty quickly piece together a much more accurate idea of what's actually going on, right?

1:05:25.440 --> 1:05:28.440
 And develop a natural skepticism too.

1:05:28.440 --> 1:05:33.440
 Just an analytical scientific mind about what you're taking information from.

1:05:33.440 --> 1:05:40.440
 And I think, I have to say, sometimes I feel that some of us in the academic bubble are too arrogant about this

1:05:40.440 --> 1:05:45.440
 and somehow think, oh, it's just people who aren't as educated as us for a fool.

1:05:45.440 --> 1:05:51.440
 When we are often just as gullible also, we read only our media and don't see through things.

1:05:51.440 --> 1:05:58.440
 Anyone who looks at both sides like this in comparison will immediately start noticing the shenanigans being pulled at.

1:05:58.440 --> 1:06:07.440
 And I think what I try to do with this app is that big tech has to some extent tried to blame the individual

1:06:07.440 --> 1:06:13.440
 for being manipulated much like big tobacco tried to blame the individuals entirely for smoking.

1:06:13.440 --> 1:06:20.440
 And later on, our government stepped up and said, actually, you can't just blame little kids for starting to smoke.

1:06:20.440 --> 1:06:23.440
 You have to have more responsible advertising and this and that.

1:06:23.440 --> 1:06:27.440
 I think it's a bit the same here. It's very convenient for a big tech to blame.

1:06:27.440 --> 1:06:32.440
 So it's just people who are so dumb and get fooled.

1:06:32.440 --> 1:06:36.440
 The blame usually comes in saying, oh, it's just human psychology.

1:06:36.440 --> 1:06:38.440
 People just want to hear what they already believe.

1:06:38.440 --> 1:06:46.440
 But Professor David Rand at MIT actually partly debunked that with a really nice study showing that people tend to be interested

1:06:46.440 --> 1:06:52.440
 in hearing things that go against what they believe if it's presented in a respectful way.

1:06:52.440 --> 1:07:00.440
 Suppose, for example, that you have a company and you're just about to launch this project and you're convinced it's going to work.

1:07:00.440 --> 1:07:05.440
 And someone says, you know, Lex, I hate to tell you this, but this is going to fail.

1:07:05.440 --> 1:07:09.440
 And here's why. Would you be like, shut up. I don't want to hear it.

1:07:09.440 --> 1:07:12.440
 Would you? You would be interested, right?

1:07:12.440 --> 1:07:18.440
 And also, if you're on an airplane back in the pre COVID times, you know,

1:07:18.440 --> 1:07:24.440
 and the guy next to you is clearly from the opposite side of the political spectrum,

1:07:24.440 --> 1:07:27.440
 but is very respectful and polite to you.

1:07:27.440 --> 1:07:32.440
 Wouldn't you be kind of interested to hear a bit about how he or she thinks about things?

1:07:32.440 --> 1:07:33.440
 Of course.

1:07:33.440 --> 1:07:37.440
 But it's not so easy to find out respectful disagreement now,

1:07:37.440 --> 1:07:43.440
 because like, for example, if you are a Democrat and you're like, oh, I want to see something on the other side.

1:07:43.440 --> 1:07:45.440
 So you just go bright bar.com.

1:07:45.440 --> 1:07:50.440
 And then after the first 10 seconds, you feel deeply insulted by something.

1:07:50.440 --> 1:07:53.440
 It's not going to work.

1:07:53.440 --> 1:08:00.440
 Or if you take someone who votes Republican and they go to something on the left and they just get very offended very quickly

1:08:00.440 --> 1:08:06.440
 by them having put a deliberately ugly picture of Donald Trump on the front page or something, it doesn't really work.

1:08:06.440 --> 1:08:12.440
 So this news aggregator also has a nuanced slider, which you can pull to the right.

1:08:12.440 --> 1:08:22.440
 And then to make it easier to get exposed to actually more sort of academic style or more respectful portrayals of different views.

1:08:22.440 --> 1:08:28.440
 And finally, the one kind of bias I think people are mostly aware of is the left right,

1:08:28.440 --> 1:08:33.440
 because it's so obvious because both left and right are very powerful here, right?

1:08:33.440 --> 1:08:38.440
 Both of them have well funded TV stations and newspapers and it's kind of hard to miss.

1:08:38.440 --> 1:08:44.440
 But there's another one, the establishment slider, which is also really fun.

1:08:44.440 --> 1:08:45.440
 I love to play with it.

1:08:45.440 --> 1:08:47.440
 And that's more about corruption.

1:08:47.440 --> 1:08:59.440
 Because if you have a society where almost all the powerful entities want you to believe a certain thing,

1:08:59.440 --> 1:09:04.440
 that's what you're going to read in both the big mainstream media on the left and on the right, of course.

1:09:04.440 --> 1:09:08.440
 And powerful companies can push back very hard.

1:09:08.440 --> 1:09:15.440
 Like tobacco companies push back very hard back in the day when some newspaper started writing articles about tobacco being dangerous.

1:09:15.440 --> 1:09:18.440
 So it was hard to get a lot of coverage about it initially.

1:09:18.440 --> 1:09:20.440
 And also if you look geopolitically, right?

1:09:20.440 --> 1:09:27.440
 Of course, in any country when you read their media, you're mainly going to be reading a lot about articles about how our country is the good guy

1:09:27.440 --> 1:09:30.440
 and the other countries are the bad guys, right?

1:09:30.440 --> 1:09:38.440
 So if you want to have a really more nuanced understanding, you know, like the Germans used to be told that the British used to be told that the French were the bad guys

1:09:38.440 --> 1:09:41.440
 and the French used to be told that the British were the bad guys.

1:09:41.440 --> 1:09:47.440
 Now they visit each other's countries a lot and have a much more nuanced understanding.

1:09:47.440 --> 1:09:50.440
 I don't think there's going to be any more wars between France and Germany.

1:09:50.440 --> 1:09:57.440
 On the geopolitical scale, it's just as much as ever, you know, big Cold War now, US, China, and so on.

1:09:57.440 --> 1:10:05.440
 And if you want to get a more nuanced understanding of what's happening geopolitically, then it's really fun to look at this establishment slider

1:10:05.440 --> 1:10:13.440
 because it turns out there are tons of little newspapers, both on the left and on the right, who sometimes challenge establishment

1:10:13.440 --> 1:10:17.440
 and say, you know, maybe we shouldn't actually invade Iraq right now.

1:10:17.440 --> 1:10:20.440
 Maybe this weapons and mass destruction thing is BS.

1:10:20.440 --> 1:10:24.440
 If you look at journalism research afterwards, you can actually see that.

1:10:24.440 --> 1:10:28.440
 Clearly, both CNN and Fox were very pro.

1:10:28.440 --> 1:10:30.440
 Let's get rid of Saddam.

1:10:30.440 --> 1:10:32.440
 There are weapons and mass destruction.

1:10:32.440 --> 1:10:34.440
 Then there were a lot of smaller newspapers.

1:10:34.440 --> 1:10:39.440
 They were like, wait a minute, this evidence seems a bit sketchy and maybe we...

1:10:39.440 --> 1:10:41.440
 But of course, they were so hard to find.

1:10:41.440 --> 1:10:44.440
 Most people didn't even know they existed, right?

1:10:44.440 --> 1:10:49.440
 Yet, it would have been better for American national security if those voices had also come up.

1:10:49.440 --> 1:10:53.440
 I think it harmed America's national security, actually, that we invaded Iraq.

1:10:53.440 --> 1:11:00.440
 And arguably, there's a lot more interest in that kind of thinking, too, from those small sources.

1:11:00.440 --> 1:11:08.440
 So, like, when you say big, it's more about kind of the reach of the broadcast.

1:11:08.440 --> 1:11:11.440
 But it's not big in terms of the interest.

1:11:11.440 --> 1:11:18.440
 I think there's a lot of interest in that kind of antiestablishment or skepticism towards...

1:11:18.440 --> 1:11:21.440
 Out of the box thinking, there's a lot of interest in that kind of thing.

1:11:21.440 --> 1:11:32.440
 Do you see this news project or something like it being basically taken over the world as the main way we consume information?

1:11:32.440 --> 1:11:35.440
 Like, how do we get there?

1:11:35.440 --> 1:11:43.440
 So, okay, the idea is brilliant. You're calling it your little project in 2020.

1:11:43.440 --> 1:11:48.440
 But how does that become the new way we consume information?

1:11:48.440 --> 1:11:50.440
 I hope, first of all, just to plant a little seed there.

1:11:50.440 --> 1:11:56.440
 Because normally, the big barrier of doing anything in media is you need a ton of money.

1:11:56.440 --> 1:11:58.440
 But this costs no money at all.

1:11:58.440 --> 1:12:00.440
 I've just been paying myself.

1:12:00.440 --> 1:12:04.440
 You pay a tiny amount of money each month to Amazon to run the thing in their cloud.

1:12:04.440 --> 1:12:06.440
 There will never be any ads.

1:12:06.440 --> 1:12:09.440
 The point is not to make any money off of it.

1:12:09.440 --> 1:12:13.440
 And we just train machine learning algorithms to classify the articles and stuff.

1:12:13.440 --> 1:12:15.440
 So, it just kind of runs by itself.

1:12:15.440 --> 1:12:20.440
 So, if it actually gets good enough at some point that it starts catching on, it could scale.

1:12:20.440 --> 1:12:28.440
 And if other people carbon copy it and make other versions that are better, that's the more the merrier.

1:12:28.440 --> 1:12:39.440
 I think there's a real opportunity for machine learning to empower the individual against the powerful players.

1:12:39.440 --> 1:12:43.440
 As I said in the beginning here, it's been mostly the other way around so far,

1:12:43.440 --> 1:12:49.440
 that the big players have the AI and then they tell people this is the truth, this is how it is.

1:12:49.440 --> 1:12:52.440
 But it can just as well go the other way around.

1:12:52.440 --> 1:12:57.440
 When the internet was born, actually, a lot of people had this hope that maybe this will be a great thing for democracy,

1:12:57.440 --> 1:12:59.440
 make it easier to find out about things.

1:12:59.440 --> 1:13:03.440
 And maybe machine learning and things like this can actually help again.

1:13:03.440 --> 1:13:06.440
 And I have to say, I think it's more important than ever now,

1:13:06.440 --> 1:13:13.440
 because this is very linked also to the whole future of life as we discussed earlier.

1:13:13.440 --> 1:13:16.440
 We're getting this ever more powerful tack.

1:13:16.440 --> 1:13:21.440
 Frank, it's pretty clear if you look on the one or two generation, three generation timescale,

1:13:21.440 --> 1:13:24.440
 that there are only two ways this can end, geopolitically.

1:13:24.440 --> 1:13:31.440
 Either it ends great for all humanity, or it ends terribly for all of us.

1:13:31.440 --> 1:13:33.440
 There's really no way in between.

1:13:33.440 --> 1:13:38.440
 And we're so stuck in, because technology knows no borders.

1:13:38.440 --> 1:13:46.440
 And you can't have people fighting when the weapons just keep getting ever more powerful indefinitely.

1:13:46.440 --> 1:13:50.440
 Eventually, the luck runs out.

1:13:50.440 --> 1:13:58.440
 And right now we have, I love America, but the fact of the matter is,

1:13:58.440 --> 1:14:04.440
 what's good for America is not opposites in the long term to what's good for other countries.

1:14:04.440 --> 1:14:10.440
 It would be if this was some sort of zero sum game like it was thousands of years ago,

1:14:10.440 --> 1:14:15.440
 when the only way one country could get more resources was to take land from other countries,

1:14:15.440 --> 1:14:17.440
 because that was basically the resource.

1:14:17.440 --> 1:14:22.440
 Look at the map of Europe, some countries kept getting bigger and smaller, endless wars.

1:14:22.440 --> 1:14:29.440
 But then, since 1945, there hasn't been any war in Western Europe, and they all got way richer, because of tech.

1:14:29.440 --> 1:14:37.440
 So the optimistic outcome is that the big winner in this century is going to be America and China,

1:14:37.440 --> 1:14:41.440
 and Russia, and everybody else, because technology just makes us all healthier and wealthier.

1:14:41.440 --> 1:14:46.440
 And we just find some way of keeping the peace on this planet.

1:14:46.440 --> 1:14:50.440
 But I think, unfortunately, there are some pretty powerful forces right now

1:14:50.440 --> 1:14:54.440
 that are pushing in exactly the opposite direction and trying to demonize other countries,

1:14:54.440 --> 1:15:02.440
 which just makes it more likely that this ever more powerful tech we're building is going to be in disastrous ways.

1:15:02.440 --> 1:15:05.440
 Yeah, for aggression versus cooperation, that kind of thing.

1:15:05.440 --> 1:15:08.440
 Yeah, even look at just military AI now, right?

1:15:08.440 --> 1:15:12.440
 It was so awesome to see these dancing robots.

1:15:12.440 --> 1:15:19.440
 I loved it, right? But one of the biggest growth areas in robotics now is, of course, autonomous weapons.

1:15:19.440 --> 1:15:24.440
 And 2020 was like the best marketing year ever for autonomous weapons,

1:15:24.440 --> 1:15:34.440
 because in both Libya, Civil War, and in Nagorno Karabakh, they made the decisive difference, right?

1:15:34.440 --> 1:15:38.440
 And everybody else is like watching this, oh yeah, we want to build autonomous weapons too.

1:15:38.440 --> 1:15:46.440
 In Libya, you had, on one hand, our ally, the United Arab Emirates,

1:15:46.440 --> 1:15:51.440
 that were flying their autonomous weapons that they bought from China, bombing Libyans.

1:15:51.440 --> 1:15:56.440
 And on the other side, you had our other ally, Turkey, flying their drones.

1:15:56.440 --> 1:16:01.440
 They had no skin in the game, any of these other countries.

1:16:01.440 --> 1:16:04.440
 And of course, it was the Libyans who really got screwed.

1:16:04.440 --> 1:16:12.440
 In Nagorno Karabakh, you had actually, again, now Turkey is sending drones built by this company

1:16:12.440 --> 1:16:18.440
 that was actually founded by a guy who went to MIT AeroAstrode, you know that?

1:16:18.440 --> 1:16:22.440
 So MIT has a direct responsibility for ultimately this.

1:16:22.440 --> 1:16:25.440
 And a lot of civilians were killed there.

1:16:25.440 --> 1:16:31.440
 So because it was militarily so effective, now suddenly there's like a huge push.

1:16:31.440 --> 1:16:37.440
 Yeah, yeah, let's go build ever more autonomy into these weapons.

1:16:37.440 --> 1:16:39.440
 And it's going to be great.

1:16:39.440 --> 1:16:46.440
 And I think actually people who are obsessed about some sort of future terminers,

1:16:46.440 --> 1:16:52.440
 NATO scenario right now, should start focusing on the fact that we have two

1:16:52.440 --> 1:16:54.440
 much more urgent threats happening for machine learning.

1:16:54.440 --> 1:16:58.440
 One of them is the whole destruction of democracy that we've talked about now,

1:16:58.440 --> 1:17:03.440
 where our flow of information is being manipulated by machine learning.

1:17:03.440 --> 1:17:08.440
 And the other one is that right now, you know, this is the year when the big arms race

1:17:08.440 --> 1:17:14.440
 out of control arms race in at least Thomas weapons is going to start or it's going to stop.

1:17:14.440 --> 1:17:23.440
 So you have a sense that there is, like 2020 was an instrumental catalyst for the race of the autonomous weapons race.

1:17:23.440 --> 1:17:27.440
 Yeah, because it was the first year when they proved decisive in the battlefield.

1:17:27.440 --> 1:17:32.440
 And these ones are still not fully autonomous, mostly they're remote controlled, right?

1:17:32.440 --> 1:17:41.440
 But, you know, we could very quickly make things about, you know, the size and cost of a smartphone,

1:17:41.440 --> 1:17:45.440
 which you just put in the GPS coordinates or the face of the one you want to kill,

1:17:45.440 --> 1:17:48.440
 a skin color or whatever and it flies away and does it.

1:17:48.440 --> 1:17:56.440
 And the real good reason why the US and all the other superpowers should put the kibosh on this

1:17:56.440 --> 1:18:01.440
 is the same reason we decided to put the kibosh on bio weapons.

1:18:01.440 --> 1:18:06.440
 So, you know, we gave the future of life award that we can talk more about later.

1:18:06.440 --> 1:18:10.440
 Matthew Messelsen from Harvard before for convincing Nixon to ban bio weapons.

1:18:10.440 --> 1:18:13.440
 And I asked him, how did you do it?

1:18:13.440 --> 1:18:20.440
 And he was like, well, I just said, look, we don't want there to be a $500 weapon of mass destruction

1:18:20.440 --> 1:18:26.440
 that even all our enemies can afford, even non state actors.

1:18:26.440 --> 1:18:31.440
 And Nixon was like, good point.

1:18:31.440 --> 1:18:36.440
 You know, it's in America's interest that the powerful weapons are all really expensive.

1:18:36.440 --> 1:18:40.440
 So only we can afford them or maybe some more stable adversaries, right?

1:18:40.440 --> 1:18:42.440
 Nuclear weapons are like that.

1:18:42.440 --> 1:18:44.440
 But bio weapons were not like that.

1:18:44.440 --> 1:18:46.440
 That's why we banned them.

1:18:46.440 --> 1:18:48.440
 And that's why you never hear about them now.

1:18:48.440 --> 1:18:50.440
 That's why we love biology.

1:18:50.440 --> 1:18:58.440
 So you have a sense that it's possible for the big powerhouses in terms of the big nations in the world

1:18:58.440 --> 1:19:02.440
 to agree that autonomous weapons is not a race we want to be on.

1:19:02.440 --> 1:19:04.440
 That it doesn't end well.

1:19:04.440 --> 1:19:06.440
 Yeah, because we know it's just going to end in mass proliferation

1:19:06.440 --> 1:19:12.440
 and every terrorist everywhere is going to have these super cheap weapons that they will use against us.

1:19:12.440 --> 1:19:18.440
 And our politicians have to constantly worry about being assassinated every time they go outdoors

1:19:18.440 --> 1:19:20.440
 by some anonymous little mini drone.

1:19:20.440 --> 1:19:22.440
 We don't want that.

1:19:22.440 --> 1:19:28.440
 And even if the U.S. and China and everyone else could just agree that you can only build these weapons

1:19:28.440 --> 1:19:34.440
 if they cost at least 10 million bucks, that would be a huge win for the superpowers.

1:19:34.440 --> 1:19:40.440
 And frankly for everybody, people often push back and say,

1:19:40.440 --> 1:19:42.440
 well, it's so hard to prevent cheating.

1:19:42.440 --> 1:19:45.440
 But hey, you can say the same about bioweapons.

1:19:45.440 --> 1:19:49.440
 Take any of your RMIT colleagues in biology.

1:19:49.440 --> 1:19:53.440
 Of course they could build some nasty bioweapon if they really wanted to.

1:19:53.440 --> 1:19:57.440
 But first of all, they don't want to because they think it's disgusting because of the stigma.

1:19:57.440 --> 1:20:01.440
 And second, even if there's some sort of nutcase and want to,

1:20:01.440 --> 1:20:05.440
 it's very likely that some of their grad students or someone would rat them out

1:20:05.440 --> 1:20:07.440
 because everyone else thinks it's so disgusting.

1:20:07.440 --> 1:20:12.440
 And in fact, we now know there was even a fair bit of cheating on the bioweapons ban.

1:20:12.440 --> 1:20:17.440
 But none, no countries used them because it was so stigmatized

1:20:17.440 --> 1:20:22.440
 that it just wasn't worth revealing that they had cheated.

1:20:22.440 --> 1:20:28.440
 You talk about drones, but you kind of think that drones is the remote operation.

1:20:28.440 --> 1:20:30.440
 Which they are mostly still.

1:20:30.440 --> 1:20:36.440
 But you're not taking the next intellectual step of like, where does this go?

1:20:36.440 --> 1:20:42.440
 You're kind of saying the problem with drones is that you're removing yourself from direct violence.

1:20:42.440 --> 1:20:45.440
 Therefore, you're not able to sort of maintain the common humanity

1:20:45.440 --> 1:20:48.440
 required to make the proper decisions strategically.

1:20:48.440 --> 1:20:52.440
 But that's the criticism as opposed to like, if this is automated,

1:20:52.440 --> 1:20:58.440
 and just exactly as you said, if you automate it and there's a race,

1:20:58.440 --> 1:21:01.440
 then the technology is going to get better and better and better,

1:21:01.440 --> 1:21:03.440
 which means getting cheaper and cheaper and cheaper.

1:21:03.440 --> 1:21:10.440
 And unlike perhaps nuclear weapons, which is connected to resources in a way,

1:21:10.440 --> 1:21:13.440
 like it's hard to get the, it's hard to engineer.

1:21:13.440 --> 1:21:19.440
 It feels like it's, you know, there's too much overlap between the tech industry

1:21:19.440 --> 1:21:24.440
 and autonomous weapons to where you could have smartphone type of cheapness.

1:21:24.440 --> 1:21:29.440
 If you look at drones, you know, it's a, you know, for $1,000,

1:21:29.440 --> 1:21:34.440
 you have an incredible system that's able to maintain flight autonomously for you

1:21:34.440 --> 1:21:36.440
 and take pictures and stuff.

1:21:36.440 --> 1:21:41.440
 You could see that going into the autonomous weapon space that's,

1:21:41.440 --> 1:21:45.440
 but like, why is that not thought about or discussed enough in the public?

1:21:45.440 --> 1:21:52.440
 Do you think you see those dancing Boston Dynamics robots and everybody has this kind of,

1:21:52.440 --> 1:21:55.440
 like as if this is like a far future.

1:21:55.440 --> 1:21:59.440
 They have this like fear, like, oh, this will be Terminator in like some,

1:21:59.440 --> 1:22:02.440
 I don't know, unspecified 20, 30, 40 years.

1:22:02.440 --> 1:22:11.440
 And they don't think about, well, this is like some much less dramatic version of that is actually happening now.

1:22:11.440 --> 1:22:14.440
 It's not going to have, it's not going to be legged, it's not going to be dancing,

1:22:14.440 --> 1:22:20.440
 but it's already has the capability to use artificial intelligence to kill humans.

1:22:20.440 --> 1:22:24.440
 Yeah, the Boston Dynamics leg robots, I think the reason we imagine them holding guns

1:22:24.440 --> 1:22:28.440
 is just because you've all seen Arnold Schwarzenegger, right?

1:22:28.440 --> 1:22:30.440
 That's our reference point.

1:22:30.440 --> 1:22:32.440
 That's pretty useless.

1:22:32.440 --> 1:22:35.440
 That's not going to be the main military use of them.

1:22:35.440 --> 1:22:38.440
 They might be useful in law enforcement in the future.

1:22:38.440 --> 1:22:42.440
 And there's a whole debate about you want robots showing up at your house with guns

1:22:42.440 --> 1:22:47.440
 telling you who'll be perfectly obedient to whatever dictator controls them.

1:22:47.440 --> 1:22:51.440
 But let's leave that aside for a moment and look at what's actually relevant now.

1:22:51.440 --> 1:22:55.440
 There's a spectrum of things you can do with AI in the military.

1:22:55.440 --> 1:22:58.440
 And again, to put my card on the table, I'm not the pacifist.

1:22:58.440 --> 1:23:01.440
 I think we should have good defense.

1:23:01.440 --> 1:23:10.440
 So, for example, a predator drone is basically a fancy little remote controlled airplane.

1:23:10.440 --> 1:23:18.440
 There's a human piloting it and the decision ultimately about whether to kill somebody with it is made by a human still.

1:23:18.440 --> 1:23:23.440
 And this is a line I think we should never cross.

1:23:23.440 --> 1:23:25.440
 There's a current DOD policy.

1:23:25.440 --> 1:23:27.440
 Again, you have to have a human in the loop.

1:23:27.440 --> 1:23:31.440
 I think algorithms should never make life or death decisions.

1:23:31.440 --> 1:23:33.440
 They should be left to humans.

1:23:33.440 --> 1:23:37.440
 Now, why might we cross that line?

1:23:37.440 --> 1:23:40.440
 Well, first of all, these are expensive, right?

1:23:40.440 --> 1:23:47.440
 So, for example, when Azerbaijan had all these drones and Armenia didn't have any,

1:23:47.440 --> 1:23:51.440
 they started trying to jerry rig little cheap things, fly around.

1:23:51.440 --> 1:23:53.440
 But then, of course, the Armenians would jam them.

1:23:53.440 --> 1:23:55.440
 The Azeris would jam them.

1:23:55.440 --> 1:23:57.440
 And remote controlled things can be jammed.

1:23:57.440 --> 1:23:59.440
 That makes them inferior.

1:23:59.440 --> 1:24:05.440
 Also, there's a bit of a time delay between, you know, if we're piloting something far away,

1:24:05.440 --> 1:24:09.440
 speed of light, and the human has a reaction time as well,

1:24:09.440 --> 1:24:14.440
 it would be nice to eliminate that jamming possibility in the time delay by having it fully autonomous.

1:24:14.440 --> 1:24:19.440
 But now you might be crossing that exact line.

1:24:19.440 --> 1:24:25.440
 You might program it to just, oh, yeah, dear drone, go hover over this country for a while

1:24:25.440 --> 1:24:30.440
 and whenever you find someone who is a bad guy, you know, kill them.

1:24:30.440 --> 1:24:33.440
 Now, the machine is making these sort of decisions.

1:24:33.440 --> 1:24:37.440
 And some people who defend this still say, well, that's morally fine

1:24:37.440 --> 1:24:43.440
 because we are the good guys and we will tell it the definition of bad guy

1:24:43.440 --> 1:24:45.440
 that we think is moral.

1:24:45.440 --> 1:24:51.440
 But now it would be very naive to think that if ISIS buys that same drone

1:24:51.440 --> 1:24:54.440
 that they're going to use our definition of bad guy.

1:24:54.440 --> 1:24:58.440
 Maybe for them, bad guy is someone wearing a U.S. Army uniform.

1:24:58.440 --> 1:25:06.440
 Or maybe there will be some weird ethnic group

1:25:06.440 --> 1:25:10.440
 who decides that someone of an other ethnic group, they are the bad guys, right?

1:25:10.440 --> 1:25:14.440
 The thing is, human soldiers, with all of our faults, right,

1:25:14.440 --> 1:25:17.440
 we still have some basic wiring in us.

1:25:17.440 --> 1:25:22.440
 Like, no, it's not okay to kill kids and civilians.

1:25:22.440 --> 1:25:24.440
 And Thomas Reppin has none of that.

1:25:24.440 --> 1:25:26.440
 It's just going to do whatever is programmed.

1:25:26.440 --> 1:25:30.440
 It's like the perfect Adolf Eichmann on steroids.

1:25:30.440 --> 1:25:34.440
 Like, they told him, Adolf Eichmann, you know, you want you to do this and this and this

1:25:34.440 --> 1:25:36.440
 to make the Holocaust more efficient.

1:25:36.440 --> 1:25:40.440
 And he was like, yeah, and off he went and did it, right?

1:25:40.440 --> 1:25:45.440
 Do we really want to make machines that are like that, like completely amoral

1:25:45.440 --> 1:25:48.440
 and will take the user's definition of who is the bad guy?

1:25:48.440 --> 1:25:52.440
 And do we then want to make them so cheap that all our adversaries can have them?

1:25:52.440 --> 1:25:55.440
 Like, what could possibly go wrong?

1:25:55.440 --> 1:26:03.440
 That's the big argument for why we want to, this year, really put the kibosh on this.

1:26:03.440 --> 1:26:08.440
 And I think you can tell there's a lot of very active debate even going on

1:26:08.440 --> 1:26:12.440
 within the U.S. military and undoubtedly in other militaries around the world also

1:26:12.440 --> 1:26:15.440
 about whether we should have some sort of international agreement

1:26:15.440 --> 1:26:21.440
 to at least require that these weapons have to be above a certain size and cost,

1:26:21.440 --> 1:26:29.440
 so that things just don't totally spiral out of control.

1:26:29.440 --> 1:26:33.440
 And finally, just for your question now, but is it possible to stop it?

1:26:33.440 --> 1:26:36.440
 Because some people tell me, oh, just give up, you know.

1:26:36.440 --> 1:26:44.440
 But again, so Matthew Messelsen again from Harvard, right, who, the bio weapons hero,

1:26:44.440 --> 1:26:47.440
 he had exactly this criticism also with bio weapons.

1:26:47.440 --> 1:26:52.440
 People were like, how can you check for sure that the Russians aren't cheating?

1:26:52.440 --> 1:27:00.440
 And he told me this, I think really ingenious insight, he said, you know, Max,

1:27:00.440 --> 1:27:03.440
 some people think you have to have inspections and things,

1:27:03.440 --> 1:27:08.440
 and you have to make sure that people, you can catch the cheaters with 100% chance.

1:27:08.440 --> 1:27:10.440
 You don't need 100%, he said.

1:27:10.440 --> 1:27:13.440
 1% is usually enough.

1:27:13.440 --> 1:27:18.440
 Because if it's another big state,

1:27:18.440 --> 1:27:24.440
 I suppose China and the US have signed a treaty, drawing a certain line and saying,

1:27:24.440 --> 1:27:28.440
 yeah, these kind of drones are okay, but these fully autonomous ones are not.

1:27:28.440 --> 1:27:35.440
 Now suppose you are China and you have cheated and secretly developed some clandestine little thing,

1:27:35.440 --> 1:27:39.440
 or you're thinking about doing it, you know, what's your calculation that you do?

1:27:39.440 --> 1:27:44.440
 Well, you're like, okay, what's the probability that we're going to get caught?

1:27:44.440 --> 1:27:48.440
 If the probability is 100%, of course, we're not going to do it.

1:27:48.440 --> 1:27:52.440
 But if the probability is 5% that we're going to get caught,

1:27:52.440 --> 1:27:55.440
 then it's going to be like a huge embarrassment for us.

1:27:55.440 --> 1:28:04.440
 And we still have our nuclear weapons anyway, so it doesn't really make any enormous difference

1:28:04.440 --> 1:28:07.440
 in terms of deterring the US, you know.

1:28:07.440 --> 1:28:14.440
 And that feeds the stigma that you kind of establish, like this fabric, this universal stigma over the thing.

1:28:14.440 --> 1:28:15.440
 Exactly.

1:28:15.440 --> 1:28:18.440
 It's very reasonable for them to say, well, you know, we probably get away with it,

1:28:18.440 --> 1:28:21.440
 but if we don't, then the US will know we cheated,

1:28:21.440 --> 1:28:24.440
 and then they're going to go full tilt with their program and say, look, the Chinese are cheaters,

1:28:24.440 --> 1:28:27.440
 and now we have all these weapons against us, and that's bad.

1:28:27.440 --> 1:28:31.440
 So the stigma alone is very, very powerful.

1:28:31.440 --> 1:28:34.440
 And again, look what happened with bioweapons, right?

1:28:34.440 --> 1:28:36.440
 It's been 50 years now.

1:28:36.440 --> 1:28:39.440
 When was the last time you read about a bioterrorism attack?

1:28:39.440 --> 1:28:43.440
 The only deaths I really know about with bioweapons that have happened,

1:28:43.440 --> 1:28:46.440
 when we Americans managed to kill some of our own with anthrax,

1:28:46.440 --> 1:28:50.440
 you know, the idiot who sent them to Tom Daschel and others in letters, right?

1:28:50.440 --> 1:28:55.440
 And similarly, in Sverlovsk in the Soviet Union,

1:28:55.440 --> 1:28:57.440
 they had some anthrax in some lab there.

1:28:57.440 --> 1:28:59.440
 Maybe they were cheating or who knows,

1:28:59.440 --> 1:29:01.440
 and it leaked out and killed a bunch of Russians.

1:29:01.440 --> 1:29:04.440
 I'd say that's a pretty good success, right?

1:29:04.440 --> 1:29:09.440
 50 years, just two own goals by the superpowers, and then nothing.

1:29:09.440 --> 1:29:13.440
 And that's why whenever I ask anyone what they think about biology,

1:29:13.440 --> 1:29:15.440
 they think it's great.

1:29:15.440 --> 1:29:19.440
 They associate it with new cures, new diseases, maybe a good vaccine.

1:29:19.440 --> 1:29:22.440
 This is how I want to think about AI in the future.

1:29:22.440 --> 1:29:24.440
 And I want others to think about AI too,

1:29:24.440 --> 1:29:27.440
 as a source of all these great solutions to our problems,

1:29:27.440 --> 1:29:30.440
 not as, oh, AI.

1:29:30.440 --> 1:29:34.440
 Oh, yeah, that's the reason I feel scared going outside these days.

1:29:34.440 --> 1:29:39.440
 Yeah, it's kind of brilliant that the bio weapons and nuclear weapons,

1:29:39.440 --> 1:29:43.440
 we've figured out, I mean, of course, there's still a huge source of danger,

1:29:43.440 --> 1:29:50.440
 but we figured out some way of creating rules and social stigma

1:29:50.440 --> 1:29:54.440
 over these weapons that then creates a stability to our,

1:29:54.440 --> 1:29:56.440
 whatever that game theoretic stability there, of course.

1:29:56.440 --> 1:29:57.440
 Exactly, exactly.

1:29:57.440 --> 1:30:01.440
 And we don't have that with AI, and you're kind of screaming from the top

1:30:01.440 --> 1:30:05.440
 of the mountain about this, that we need to find that,

1:30:05.440 --> 1:30:10.440
 because just like, it's very possible with the future of life,

1:30:10.440 --> 1:30:17.440
 as you've pointed out, Institute Awards pointed out that with nuclear weapons,

1:30:17.440 --> 1:30:21.440
 we could have destroyed ourselves quite a few times.

1:30:21.440 --> 1:30:28.440
 And it's a learning experience that is very costly.

1:30:28.440 --> 1:30:33.440
 We gave this Future Life Award, we gave it the first time to this guy,

1:30:33.440 --> 1:30:37.440
 Vasily Arkhipov, he was on, most people haven't even heard of him.

1:30:37.440 --> 1:30:38.440
 Yeah, can you say who he is?

1:30:38.440 --> 1:30:43.440
 Vasily Arkhipov, he has, in my opinion,

1:30:43.440 --> 1:30:49.440
 made the greatest positive contribution to humanity of any human in modern history.

1:30:49.440 --> 1:30:53.440
 And maybe it sounds like hyperbole here, like I'm just over the top,

1:30:53.440 --> 1:30:55.440
 but let me tell you the story, and I think maybe you'll agree.

1:30:55.440 --> 1:31:01.440
 So during the Cuban Missile Crisis, we Americans first didn't know

1:31:01.440 --> 1:31:06.440
 that the Russians had sent four submarines, but we caught two of them,

1:31:06.440 --> 1:31:11.440
 and we didn't know that, so we dropped practice depth charges on the one that he was on,

1:31:11.440 --> 1:31:14.440
 trying to force it to the surface.

1:31:14.440 --> 1:31:18.440
 But we didn't know that this nuclear submarine actually was a nuclear submarine

1:31:18.440 --> 1:31:20.440
 with a nuclear torpedo.

1:31:20.440 --> 1:31:24.440
 We also didn't know that they had an authorization to launch it without clearance from Moscow.

1:31:24.440 --> 1:31:28.440
 And we also didn't know that they were running out of electricity,

1:31:28.440 --> 1:31:31.440
 their batteries were almost dead, they were running out of oxygen,

1:31:31.440 --> 1:31:34.440
 sailors were fainting left and right.

1:31:34.440 --> 1:31:39.440
 The temperature was about 110, 120 Fahrenheit on board,

1:31:39.440 --> 1:31:42.440
 it was really hellish conditions, really just a kind of doomsday.

1:31:42.440 --> 1:31:46.440
 And at that point, these giant explosions start happening

1:31:46.440 --> 1:31:48.440
 from Americans dropping these.

1:31:48.440 --> 1:31:50.440
 The captain thought World War III had begun.

1:31:50.440 --> 1:31:53.440
 They decided that they were going to launch the nuclear torpedo.

1:31:53.440 --> 1:31:56.440
 And one of them shouted, you know, we're all going to die,

1:31:56.440 --> 1:31:58.440
 but we're not going to disgrace our navy.

1:31:58.440 --> 1:32:03.440
 We don't know what would have happened if there had been a giant mushroom cloud all of a sudden

1:32:03.440 --> 1:32:08.440
 against Americans, but since everybody had their hands on the triggers,

1:32:08.440 --> 1:32:12.440
 you don't have to be too creative to think that it could have led to an all out nuclear war,

1:32:12.440 --> 1:32:15.440
 in which case we wouldn't be having this conversation now, right?

1:32:15.440 --> 1:32:20.440
 What actually took place was they needed three people to approve this.

1:32:20.440 --> 1:32:24.440
 The captain had said yes, there was the Communist Party political officer,

1:32:24.440 --> 1:32:26.440
 he also said yes, let's do it.

1:32:26.440 --> 1:32:29.440
 And the third man was this guy Vasily Arkhipov, who said,

1:32:29.440 --> 1:32:34.440
 yeah, for some reason he was just more chill than the others

1:32:34.440 --> 1:32:36.440
 and he was the right man at the right time.

1:32:36.440 --> 1:32:41.440
 I don't want us as a species rely on the right person being there at the right time.

1:32:41.440 --> 1:32:48.440
 You know, we tracked down his family living in relative poverty outside Moscow.

1:32:48.440 --> 1:32:54.440
 When he flew his daughter, he had passed away and flew them to London.

1:32:54.440 --> 1:32:55.440
 They had never been to the West even.

1:32:55.440 --> 1:32:58.440
 It was incredibly moving to get to honor them for this.

1:32:58.440 --> 1:33:03.440
 The next year we gave this future life award to Stanislav Petrov.

1:33:03.440 --> 1:33:04.440
 Have you heard of him?

1:33:04.440 --> 1:33:05.440
 Yes.

1:33:05.440 --> 1:33:12.440
 He was in charge of the Soviet early warning station which was built with Soviet technology

1:33:12.440 --> 1:33:14.440
 and honestly not that reliable.

1:33:14.440 --> 1:33:17.440
 It said that there were five US missiles coming in.

1:33:17.440 --> 1:33:23.440
 Again, if they had launched at that point, we probably wouldn't be having this conversation.

1:33:23.440 --> 1:33:32.440
 He decided based on just mainly gut instinct to just not escalate this.

1:33:32.440 --> 1:33:37.440
 I'm very glad he wasn't replaced by an AI that was just automatically falling orders.

1:33:37.440 --> 1:33:39.440
 Then we gave the third one to Matthew Messelsen.

1:33:39.440 --> 1:33:46.440
 Last year we gave this award to these guys who actually used technology for good,

1:33:46.440 --> 1:33:49.440
 not avoiding something bad, but for something good.

1:33:49.440 --> 1:33:54.440
 The guys who eliminated this disease, which is way worse than COVID,

1:33:54.440 --> 1:33:58.440
 that had killed half a billion people in its violent century.

1:33:58.440 --> 1:33:59.440
 Smallpox.

1:33:59.440 --> 1:34:02.440
 You mentioned it earlier.

1:34:02.440 --> 1:34:05.440
 COVID on average kills less than 1% of people who get it.

1:34:05.440 --> 1:34:08.440
 Smallpox, about 30%.

1:34:08.440 --> 1:34:17.440
 Ultimately, Viktor Zhdanov and Bill Fagy, most of my colleagues have never heard of either of them,

1:34:17.440 --> 1:34:22.440
 one American, one Russian, they did this amazing effort.

1:34:22.440 --> 1:34:26.440
 Not only was Zhdanov able to get the US and the Soviet Union to team up against smallpox

1:34:26.440 --> 1:34:28.440
 during the Cold War,

1:34:28.440 --> 1:34:33.440
 but Fagy came up with this ingenious strategy for making it actually go all the way

1:34:33.440 --> 1:34:38.440
 to defeat the disease without funding for vaccinating everyone.

1:34:38.440 --> 1:34:44.440
 As a result, we went from 15 million deaths the year I was born in smallpox.

1:34:44.440 --> 1:34:46.440
 So what do we have in COVID now?

1:34:46.440 --> 1:34:47.440
 A little bit short of 2 million, right?

1:34:47.440 --> 1:34:48.440
 Yes.

1:34:48.440 --> 1:34:50.440
 To zero deaths, of course, this year.

1:34:50.440 --> 1:34:54.440
 And forever, there have been 200 million people,

1:34:54.440 --> 1:34:58.440
 who would have died since then by smallpox had it not been for this.

1:34:58.440 --> 1:35:01.440
 So isn't science awesome when you use it for good?

1:35:01.440 --> 1:35:05.440
 And the reason we want to celebrate these sort of people is to remind them of this.

1:35:05.440 --> 1:35:09.440
 Science is so awesome when you use it for good.

1:35:09.440 --> 1:35:14.440
 And those awards actually, the variety there, it's a very interesting picture.

1:35:14.440 --> 1:35:22.440
 So the first two are looking at, it's kind of exciting to think that these average humans,

1:35:22.440 --> 1:35:29.440
 in some sense, there are products of billions of other humans that came before them, evolution.

1:35:29.440 --> 1:35:40.440
 And some little, you said gut, but there's something in there that stopped the annihilation of the human race.

1:35:40.440 --> 1:35:44.440
 And that's a magical thing, but that's like this deeply human thing.

1:35:44.440 --> 1:35:49.440
 And then there's the other aspect where it's also very human,

1:35:49.440 --> 1:35:55.440
 which is to build solution to the existential crises that we're facing,

1:35:55.440 --> 1:36:00.440
 to build it, to take responsibility, to come up with different technologies and so on.

1:36:00.440 --> 1:36:03.440
 And both of those are deeply human.

1:36:03.440 --> 1:36:07.440
 The gut and the mind, whatever that is.

1:36:07.440 --> 1:36:08.440
 The best is when they work together.

1:36:08.440 --> 1:36:12.440
 Archipelago, I wish I could have met him, of course, but he had passed away.

1:36:12.440 --> 1:36:20.440
 He was really a fantastic military officer, combining all the best traits that we in America admire in our military.

1:36:20.440 --> 1:36:23.440
 Because first of all, he was very loyal, of course.

1:36:23.440 --> 1:36:28.440
 He never even told anyone about this during his whole life, even though you think he had some bragging rights, right?

1:36:28.440 --> 1:36:31.440
 But he just was like, this is just business, just doing my job.

1:36:31.440 --> 1:36:33.440
 It only came out later after his death.

1:36:33.440 --> 1:36:39.440
 And second, the reason he did the right thing was not because he was some sort of liberal,

1:36:39.440 --> 1:36:46.440
 not because he was just, oh, you know, peace and love.

1:36:46.440 --> 1:36:52.440
 It was partly because he had been the captain on another submarine that had a nuclear reactor meltdown.

1:36:52.440 --> 1:36:57.440
 And it was his heroism that helped contain this.

1:36:57.440 --> 1:36:59.440
 That's why he died of cancer later also.

1:36:59.440 --> 1:37:01.440
 But he's seen many of his crew members die.

1:37:01.440 --> 1:37:04.440
 And I think for him, that gave him this gut feeling that, you know,

1:37:04.440 --> 1:37:10.440
 if there's a nuclear war between the US and the Soviet Union, the whole world is going to go through

1:37:10.440 --> 1:37:13.440
 what I saw my dear crew members suffer through.

1:37:13.440 --> 1:37:15.440
 It wasn't just an abstract thing for him.

1:37:15.440 --> 1:37:17.440
 I think it was real.

1:37:17.440 --> 1:37:20.440
 And second, though, not just the gut, the mind, right?

1:37:20.440 --> 1:37:25.440
 He was, for some reason, very level headed personality and very smart guy,

1:37:25.440 --> 1:37:29.440
 which is exactly what we want our best fighter pilots to be also.

1:37:29.440 --> 1:37:34.440
 I never forget Neil Armstrong when he's landing on the moon and almost running out of gas.

1:37:34.440 --> 1:37:37.440
 And he doesn't even change, let me say 30 seconds.

1:37:37.440 --> 1:37:39.440
 He doesn't even change the tone of voice, just keeps going.

1:37:39.440 --> 1:37:41.440
 Archipelago, I think, was just like that.

1:37:41.440 --> 1:37:46.440
 So when the explosions start going off and his captain is screaming and we should nuke them and all,

1:37:46.440 --> 1:37:53.440
 he's like, I don't think the Americans are trying to sink us.

1:37:53.440 --> 1:37:57.440
 I think they're trying to send us a message.

1:37:57.440 --> 1:37:59.440
 That's pretty badass.

1:37:59.440 --> 1:38:00.440
 Coolness.

1:38:00.440 --> 1:38:05.440
 Because he said, if they wanted to sink us, he said, listen, listen,

1:38:05.440 --> 1:38:12.440
 it's alternating one loud explosion on the left, one on the right, one on the left, one on the right.

1:38:12.440 --> 1:38:15.440
 He was the only one to notice this pattern.

1:38:15.440 --> 1:38:22.440
 And he's like, I think this is them trying to send us a signal that they wanted to surface

1:38:22.440 --> 1:38:25.440
 and they're not going to sink us.

1:38:25.440 --> 1:38:34.440
 And somehow this is how he then managed it ultimately with his combination of gut

1:38:34.440 --> 1:38:40.440
 and also just cool analytical thinking, was able to deescalate the whole thing.

1:38:40.440 --> 1:38:44.440
 And yeah, so this is some of the best in humanity.

1:38:44.440 --> 1:38:47.440
 I guess coming back to what we talked about earlier is the combination of the neural network,

1:38:47.440 --> 1:38:51.440
 the instinctive, you know, with I'm tearing up here, getting emotional.

1:38:51.440 --> 1:39:00.440
 But he is one of my superheroes having both the heart and the mind combined.

1:39:00.440 --> 1:39:04.440
 And especially in that time, there's something about the, I mean, this is a very,

1:39:04.440 --> 1:39:11.440
 in America, people are used to this kind of idea of being the individual of like on your own thinking.

1:39:11.440 --> 1:39:17.440
 I think in the Soviet Union under communism, it's actually much harder to do that.

1:39:17.440 --> 1:39:23.440
 Oh yeah, he didn't even, he even got, he didn't get any accolades either when he came back for this, right?

1:39:23.440 --> 1:39:25.440
 They just wanted to hush the whole thing up.

1:39:25.440 --> 1:39:32.440
 Yeah, there's echoes of that with Chernobyl, there's all kinds of, that's one,

1:39:32.440 --> 1:39:37.440
 that's a really hopeful thing that amidst big centralized powers,

1:39:37.440 --> 1:39:43.440
 whether it's companies or states, there's still the power of the individual to think on their own to act.

1:39:43.440 --> 1:39:49.440
 But I think we need to think of people like this, not as a panacea we can always count on,

1:39:49.440 --> 1:39:54.440
 but rather as a wake up call, you know.

1:39:54.440 --> 1:40:00.440
 So because of them, because of Arkhipov, we are alive to learn from this lesson,

1:40:00.440 --> 1:40:03.440
 to learn from the fact that we shouldn't keep playing Russian roulette

1:40:03.440 --> 1:40:06.440
 and almost have a nuclear war by mistake now and then,

1:40:06.440 --> 1:40:09.440
 because relying on luck is not a good long term strategy.

1:40:09.440 --> 1:40:11.440
 If you keep playing Russian roulette over and over again,

1:40:11.440 --> 1:40:14.440
 the probability of surviving just drops exponentially with time.

1:40:14.440 --> 1:40:18.440
 And if you have some probability of having an accidental nuclear war every year,

1:40:18.440 --> 1:40:21.440
 the probability of not having one also drops exponentially.

1:40:21.440 --> 1:40:23.440
 I think we can do better than that.

1:40:23.440 --> 1:40:27.440
 So I think the message is very clear, once in a while shit happens,

1:40:27.440 --> 1:40:36.440
 and there's a lot of very concrete things we can do to reduce the risk of things like that happening in the first place.

1:40:36.440 --> 1:40:40.440
 On the AI front, if we just link on that for a second.

1:40:40.440 --> 1:40:45.440
 So you're friends with, you often talk with Elon Musk throughout history.

1:40:45.440 --> 1:40:48.440
 You've did a lot of interesting things together.

1:40:48.440 --> 1:40:55.440
 He has a set of fears about the future of artificial intelligence, AGI.

1:40:55.440 --> 1:41:01.440
 Do you have a sense, we've already talked about the things we should be worried about with AI.

1:41:01.440 --> 1:41:05.440
 Do you have a sense of the shape of his fears in particular,

1:41:05.440 --> 1:41:13.440
 about AI, which subset of what we've talked about, whether it's creating,

1:41:13.440 --> 1:41:19.440
 it's that direction of creating these giant computational systems that are not explainable,

1:41:19.440 --> 1:41:26.440
 they're not intelligible intelligence, or is it the...

1:41:26.440 --> 1:41:31.440
 And then as a branch of that, is it the manipulation by big corporations of that,

1:41:31.440 --> 1:41:37.440
 or individual evil people to use that for destruction, or the unintentional consequences.

1:41:37.440 --> 1:41:40.440
 Do you have a sense of where his thinking is on this?

1:41:40.440 --> 1:41:47.440
 From my many conversations with Elon, I certainly have a model of how he thinks.

1:41:47.440 --> 1:41:51.440
 It's actually very much like the way I think also, I'll elaborate on it a bit.

1:41:51.440 --> 1:41:54.440
 I just want to push back on when you said evil people.

1:41:54.440 --> 1:41:59.440
 I don't think it's a very helpful concept, evil people.

1:41:59.440 --> 1:42:05.440
 Sometimes people do very, very bad things, but they usually do it because they think it's a good thing,

1:42:05.440 --> 1:42:08.440
 because somehow other people had told them that that was a good thing,

1:42:08.440 --> 1:42:15.440
 or given them incorrect information, or whatever.

1:42:15.440 --> 1:42:21.440
 I believe in the fundamental goodness of humanity that if we educate people well,

1:42:21.440 --> 1:42:28.440
 and they find out how things really are, people generally want to do good and be good.

1:42:28.440 --> 1:42:30.440
 There's a sense of value alignment.

1:42:30.440 --> 1:42:35.440
 It's about information, it's about knowledge, and then once we have that,

1:42:35.440 --> 1:42:42.440
 we'll likely be able to do good in the way that's aligned with everybody else who thinks it's good.

1:42:42.440 --> 1:42:45.440
 Yeah, and it's not just the individual people we have to align,

1:42:45.440 --> 1:42:51.440
 so we don't just want people to be educated to know the way things actually are,

1:42:51.440 --> 1:42:56.440
 and to treat each other well, but we also need to align other nonhuman entities.

1:42:56.440 --> 1:42:58.440
 We've talked about corporations, there has to be institutions,

1:42:58.440 --> 1:43:01.440
 so that what they do is actually good for the country they're in,

1:43:01.440 --> 1:43:07.440
 and we should make sure that what countries do is actually good for the species as a whole, etc.

1:43:07.440 --> 1:43:15.440
 Coming back to Elon, my understanding of how Elon sees this is really quite similar to my own,

1:43:15.440 --> 1:43:19.440
 which is one of the reasons I like him so much, and enjoy talking with him so much.

1:43:19.440 --> 1:43:29.440
 He's quite different from most people in that he thinks much more than most people about their really big picture,

1:43:29.440 --> 1:43:32.440
 not just what's going to happen in the next election cycle,

1:43:32.440 --> 1:43:36.440
 and millennia, millions and billions of years from now.

1:43:36.440 --> 1:43:42.440
 When you look in this more cosmic perspective, it's so obvious that we're gazing out into this universe

1:43:42.440 --> 1:43:49.440
 that as far as we can tell is mostly dead, with life being almost imperceptibly tiny perturbation, right?

1:43:49.440 --> 1:43:53.440
 And he sees this enormous opportunity for our universe to come alive,

1:43:53.440 --> 1:43:55.440
 first to become an interplanetary species.

1:43:55.440 --> 1:44:01.440
 Mars is obviously just first stop on this cosmic journey,

1:44:01.440 --> 1:44:08.440
 and precisely because he thinks more long term, it's much more clear to him than to most people

1:44:08.440 --> 1:44:14.440
 that what we do with this Russian roulette thing we keep playing with our nukes is a really poor strategy,

1:44:14.440 --> 1:44:19.440
 a really reckless strategy, and also that we're just building these ever more powerful AI systems

1:44:19.440 --> 1:44:23.440
 that we don't understand is also a really reckless strategy.

1:44:23.440 --> 1:44:30.440
 I feel Elon is a humanist in the sense that he wants an awesome future for humanity.

1:44:30.440 --> 1:44:38.440
 He wants it to be us that control the machines, rather than the machines that control us.

1:44:38.440 --> 1:44:44.440
 And why shouldn't we insist on that? We're building them after all, right?

1:44:44.440 --> 1:44:48.440
 Why should we build things that just make us into some little cog in the machinery

1:44:48.440 --> 1:44:50.440
 that has no further say in the matter, right?

1:44:50.440 --> 1:44:54.440
 It's not my idea of an inspiring future either.

1:44:54.440 --> 1:45:03.440
 Yeah, if you think on the cosmic scale in terms of both time and space, so much is put into perspective.

1:45:03.440 --> 1:45:09.440
 Whenever I have a bad day, that's what I think about. It immediately makes me feel better.

1:45:09.440 --> 1:45:16.440
 It makes me sad that for us individual humans, at least for now, the ride ends too quickly.

1:45:16.440 --> 1:45:20.440
 We don't get to experience the cosmic scale.

1:45:20.440 --> 1:45:25.440
 I mean, I think of our universe sometimes as an organism that has only begun to wake up a tiny bit.

1:45:25.440 --> 1:45:31.440
 Just like the very first little glimmers of consciousness you have in the morning when you start coming around.

1:45:31.440 --> 1:45:32.440
 Before the coffee.

1:45:32.440 --> 1:45:39.440
 Before the coffee. Even before you get out of bed, before you even open your eyes, start to wake up a little bit.

1:45:39.440 --> 1:45:46.440
 There's something here, you know. That's very much how I think of where we are.

1:45:46.440 --> 1:45:50.440
 All those galaxies out there, I think they're really beautiful.

1:45:50.440 --> 1:45:52.440
 But why are they beautiful?

1:45:52.440 --> 1:45:59.440
 They're beautiful because conscious entities are actually observing them and experiencing them through our telescopes.

1:45:59.440 --> 1:46:05.440
 I define consciousness as subjective experience.

1:46:05.440 --> 1:46:09.440
 Whether it be colors or emotions or sounds.

1:46:09.440 --> 1:46:12.440
 So beauty is an experience.

1:46:12.440 --> 1:46:13.440
 Meaning is an experience.

1:46:13.440 --> 1:46:15.440
 Purpose is an experience.

1:46:15.440 --> 1:46:19.440
 If there was no conscious experience observing these galaxies, they wouldn't be beautiful.

1:46:19.440 --> 1:46:28.440
 If we do something dumb with advanced AI in the future here and Earth originating, life goes extinct.

1:46:28.440 --> 1:46:30.440
 And that was it for this.

1:46:30.440 --> 1:46:37.440
 If there is nothing else with telescopes in our universe, then it's kind of game over for beauty and meaning and purpose in our whole universe.

1:46:37.440 --> 1:46:41.440
 And I think that would be just such an opportunity lost, frankly.

1:46:41.440 --> 1:46:52.440
 And I think when Elon points this out, he gets very unfairly maligned in the media for all the dumb media bias reasons we talked about, right?

1:46:52.440 --> 1:46:58.440
 They want to print precisely the things about Elon out of context that are really clickbaity.

1:46:58.440 --> 1:47:04.440
 Like he has gotten so much flak for this summoning the demon statement.

1:47:04.440 --> 1:47:11.440
 I happen to know exactly the context because I was in the front row when he gave that talk. It was at MIT, you'll be pleased to know.

1:47:11.440 --> 1:47:13.440
 It was the AeroAstro anniversary.

1:47:13.440 --> 1:47:20.440
 They had Buzz Aldrin there from the moon landing, a whole house, a Kresge auditorium packed with MIT students.

1:47:20.440 --> 1:47:22.440
 And he had this amazing Q&A.

1:47:22.440 --> 1:47:26.440
 It might have gone for an hour and they talked about rockets and Mars and everything.

1:47:26.440 --> 1:47:32.440
 At the very end, this one student who has actually hit my class asked him, what about AI?

1:47:32.440 --> 1:47:39.440
 Elon makes this one comment and they take this out of context, print it, goes viral.

1:47:39.440 --> 1:47:42.440
 Was it like with AI where summoning the demons and stuff like that?

1:47:42.440 --> 1:47:46.440
 And try to cast him as some sort of doom and gloom dude.

1:47:46.440 --> 1:47:48.440
 You know Elon.

1:47:48.440 --> 1:47:51.440
 He's not the doom and gloom dude.

1:47:51.440 --> 1:47:53.440
 He is such a positive visionary.

1:47:53.440 --> 1:47:59.440
 And the whole reason he warns about this is because he realizes more than most what the opportunity cost is of screwing up.

1:47:59.440 --> 1:48:07.440
 That there is so much awesomeness in the future that we can and our descendants can enjoy if we don't screw up, right?

1:48:07.440 --> 1:48:14.440
 I get so pissed off when people try to cast him as some sort of technophobic Luddite.

1:48:14.440 --> 1:48:24.440
 And at this point, it's kind of ludicrous when I hear people say that people who worry about artificial general intelligence are Luddites.

1:48:24.440 --> 1:48:32.440
 Because of course, if you look more closely, you have some of the most outspoken people making warnings.

1:48:32.440 --> 1:48:38.440
 Are people like Professor Stuart Russell from Berkeley who's written the best selling AI textbook, you know.

1:48:38.440 --> 1:48:46.440
 So claiming that he is a Luddite who doesn't understand AI is the joke is really on the people who said it.

1:48:46.440 --> 1:48:50.440
 But I think more broadly, this message is really not sunk in at all.

1:48:50.440 --> 1:48:52.440
 What it is that people worry about.

1:48:52.440 --> 1:49:03.440
 They think that Elon and Stuart Russell and others are worried about the dancing robots picking up an AR15 and going on a rampage, right?

1:49:03.440 --> 1:49:07.440
 They think they're worried about robots turning evil.

1:49:07.440 --> 1:49:09.440
 They're not. I'm not.

1:49:09.440 --> 1:49:15.440
 The risk is not malice. It's competence.

1:49:15.440 --> 1:49:21.440
 The risk is just that we build some systems that are incredibly competent, which means they're always going to get their goals accomplished.

1:49:21.440 --> 1:49:23.440
 Even if they clash with our goals.

1:49:23.440 --> 1:49:25.440
 That's the risk.

1:49:25.440 --> 1:49:31.440
 Why did we humans, you know, drive the West African black rhino extinct?

1:49:31.440 --> 1:49:35.440
 Is it because we're malicious, evil rhinoceros haters?

1:49:35.440 --> 1:49:42.440
 No, it's just because our goals didn't align with the goals of those rhinos and tough luck for the rhinos, you know.

1:49:42.440 --> 1:49:51.440
 So the point is just we don't want to put ourselves in the position of those rhinos creating these something more powerful than us.

1:49:51.440 --> 1:49:55.440
 If we haven't first figured out how to align the goals and I am optimistic.

1:49:55.440 --> 1:50:02.440
 I think we could do it if we worked really hard on it because I spent a lot of time around intelligent entities that were more intelligent than me.

1:50:02.440 --> 1:50:11.440
 My mom and my dad and I was little and that was fine because their goals were actually aligned with mine quite well.

1:50:11.440 --> 1:50:17.440
 But we've seen today many examples of where the goals of our powerful systems are not so aligned.

1:50:17.440 --> 1:50:24.440
 So those click through optimization algorithms that are polarized social media, right?

1:50:24.440 --> 1:50:29.440
 They were actually pretty poorly aligned with what was good for democracy turned out.

1:50:29.440 --> 1:50:35.440
 And again, almost all problems we've had in machine learning again came so far, not from Alice, but from poor alignment.

1:50:35.440 --> 1:50:39.440
 And it's that's exactly why that's why we should be concerned about in the future.

1:50:39.440 --> 1:50:49.440
 Do you think it's possible that with systems like Neuralink and brain computer interfaces, you know, again, thinking of the cosmic scale,

1:50:49.440 --> 1:50:59.440
 Elon's talked about this, but others have as well throughout history of figuring out how the exact mechanism of how to achieve that kind of alignment.

1:50:59.440 --> 1:51:06.440
 So one of them is having a symbiosis with AI, which is like coming up with clever ways where we're like stuck together.

1:51:06.440 --> 1:51:18.440
 And this weird relationship, whether it's biological or in some kind of other way, do you think there's that's a possibility of having that kind of symbiosis?

1:51:18.440 --> 1:51:31.440
 Or do we want to instead kind of focus on this distinct entities of us humans talking to these intelligible, self doubting AIs,

1:51:31.440 --> 1:51:39.440
 maybe like Stuart Russell thinks about it, like these, we're we're self doubting and full of uncertainty and have our AI systems are full of uncertainty.

1:51:39.440 --> 1:51:44.440
 We communicate back and forth and in that way achieves symbiosis.

1:51:44.440 --> 1:51:51.440
 I honestly don't know. I would say that because we don't know for sure what if any of our which of any of our ideas will work.

1:51:51.440 --> 1:52:03.440
 But we do know that if we don't, I'm pretty convinced that if we don't get any of these things to work and just barge ahead, then our species is, you know, probably going to go extinct this century.

1:52:03.440 --> 1:52:11.440
 I think this century, you think like you think we're facing this crisis is a 21st century crisis.

1:52:11.440 --> 1:52:20.440
 This century will be remembered on a hard drive somewhere or maybe by future generations is like,

1:52:20.440 --> 1:52:28.440
 like there will be future future of life as a two awards for people that have done something about AI.

1:52:28.440 --> 1:52:34.440
 It could also end even worse, whether we're not superseded by leaving any AI behind either.

1:52:34.440 --> 1:52:37.440
 We just totally wipe out, you know, like on Easter Island.

1:52:37.440 --> 1:52:38.440
 Our century is long.

1:52:38.440 --> 1:52:42.440
 No, there are still 79 years left of it.

1:52:42.440 --> 1:52:46.440
 Think about how far we've come just in the last 30 years.

1:52:46.440 --> 1:52:52.440
 So we can talk more about what might go wrong.

1:52:52.440 --> 1:52:55.440
 But you asked me this really good question about what's the best strategy?

1:52:55.440 --> 1:52:59.440
 Is it Neuralink or Russell's approach or whatever?

1:52:59.440 --> 1:53:12.440
 I think, you know, when we did the Manhattan Project, we didn't know if any of our four ideas for enriching uranium and getting out the uranium 235 were going to work.

1:53:12.440 --> 1:53:16.440
 But we felt this was really important to get it before Hitler did.

1:53:16.440 --> 1:53:17.440
 So you know what we did?

1:53:17.440 --> 1:53:19.440
 We tried all four of them here.

1:53:19.440 --> 1:53:28.440
 I think it's analogous where there's the greatest threat that's ever faced our species and of course US National Security by implication.

1:53:28.440 --> 1:53:34.440
 We don't know if we don't have any method that's guaranteed to work, but we have a lot of ideas.

1:53:34.440 --> 1:53:40.440
 So we should invest pretty heavily in pursuing all of them with an open mind and hope that one of them at least works.

1:53:40.440 --> 1:53:50.440
 These are the good news is the century is long, you know, and it might take decades until we have artificial general intelligence.

1:53:50.440 --> 1:53:57.440
 So we have some time, hopefully, but it takes a long time to solve these very, very difficult problems.

1:53:57.440 --> 1:54:01.440
 It's going to actually be the most difficult problem we were ever trying to solve as a species.

1:54:01.440 --> 1:54:03.440
 So we have to start now.

1:54:03.440 --> 1:54:10.440
 So we don't want to have it rather than, you know, begin thinking about it the night before some people who've had too much Red Bull switch it on.

1:54:10.440 --> 1:54:19.440
 And we have to coming back to your question, we have to pursue all of these different avenues and see if you're my investment advisor and I was trying to invest in the future.

1:54:19.440 --> 1:54:29.440
 How do you think the human species is most likely to destroy itself in the century?

1:54:29.440 --> 1:54:51.440
 Yeah, so if the crises, many of the crises we're facing are really before us within the next hundred years, how do we make explicit, make known the unknowns and solve those problems to avoid the biggest starting with the biggest existential crisis.

1:54:51.440 --> 1:54:56.440
 So as your investment advisor, how are you planning to make money on us destroying ourselves?

1:54:56.440 --> 1:54:57.440
 I don't know.

1:54:57.440 --> 1:55:02.440
 It might be the Russian origins that somehow is involved.

1:55:02.440 --> 1:55:08.440
 At the micro level of detailed strategies, of course, these are unsolved problems.

1:55:08.440 --> 1:55:13.440
 For AI alignment, we can break it into three sub problems that are all unsolved.

1:55:13.440 --> 1:55:23.440
 I think you want first to make machines understand our goals, then adopt our goals and then retain our goals.

1:55:23.440 --> 1:55:27.440
 So hit on all three real quickly.

1:55:27.440 --> 1:55:38.440
 The problem when Andreas Lubitz told his autopilot to fly into the Alps was that the computer didn't even understand anything about his goals, right?

1:55:38.440 --> 1:55:40.440
 It was too dumb.

1:55:40.440 --> 1:55:48.440
 It could have understood actually, but we would have had to put some effort in as a system designer to don't fly into mountains.

1:55:48.440 --> 1:55:55.440
 So that's the first challenge. How do you program into computers human values, human goals?

1:55:55.440 --> 1:56:01.440
 Rather than saying, oh, it's so hard, we should start with the simple stuff, as I said.

1:56:01.440 --> 1:56:16.440
 Self driving cars, airplanes, just put in all the goals that we all agree on already and then have a habit of whenever a machine gets smarter so they can understand one level higher goals, you know, put them into.

1:56:16.440 --> 1:56:20.440
 The second challenge is getting them to adopt the goals.

1:56:20.440 --> 1:56:37.440
 It's easy for situations like that where you just program it in, but when you have self learning systems like children, you know, any parent knows that there is a difference between getting our kids to understand what we want them to do and to actually adopt our goals, right?

1:56:37.440 --> 1:56:55.440
 With humans, with children, fortunately, they go through this phrase, first they're too dumb to understand what we want our goals are, and then they have this period of some years when they're both smart enough to understand them and malleable enough that we have a chance to raise them well,

1:56:55.440 --> 1:57:06.440
 and then they become teenagers, kind of too late, but we have this window with machines, the challenges, the intelligence might grow so fast that that window is pretty short.

1:57:06.440 --> 1:57:08.440
 So that's a research problem.

1:57:08.440 --> 1:57:11.440
 The third one is how do you make sure they keep the goals?

1:57:11.440 --> 1:57:14.440
 If they keep learning more and getting smarter.

1:57:14.440 --> 1:57:27.440
 Many sci fi movies are about how you have something which initially was aligned, but then things kind of go off keel and, you know, my kids were very, very excited about their Legos when they were little.

1:57:27.440 --> 1:57:39.440
 And now they're just gathering dust in the basement, you know, if we create machines that are really on board with a goal of taking care of humanity, we don't want them to get as bored with us and as my kids got with Legos.

1:57:39.440 --> 1:57:41.440
 So this is another research challenge.

1:57:41.440 --> 1:57:47.440
 How can you make some sort of recursively self improving system retain certain basic goals?

1:57:47.440 --> 1:57:53.440
 That said, a lot of adult people still play with Legos, so maybe we succeeded with Legos.

1:57:53.440 --> 1:57:55.440
 I like your optimism.

1:57:55.440 --> 1:57:58.440
 So not all AI systems have to maintain the goals, right?

1:57:58.440 --> 1:58:00.440
 Some just some fraction.

1:58:00.440 --> 1:58:01.440
 Yeah.

1:58:01.440 --> 1:58:07.440
 So there's there's a lot of talented AI researchers now who have heard of this and want to work on it.

1:58:07.440 --> 1:58:10.440
 Not so much funding for it yet.

1:58:10.440 --> 1:58:14.440
 Of the billions that go into building AI more powerful.

1:58:14.440 --> 1:58:16.440
 It's only a miniscule fraction.

1:58:16.440 --> 1:58:24.440
 So for going into the safety research, my attitude is generally we should not try to slow down the technology, but we should greatly accelerate the investment in this sort of safety research.

1:58:24.440 --> 1:58:33.440
 And also make sure it's been it's this was very embarrassing last year, but you know, the NSF decided to give out six of these big institutes.

1:58:33.440 --> 1:58:35.440
 We got one of them for AI and science.

1:58:35.440 --> 1:58:39.440
 You asked me about another one was supposed to be for a safety research.

1:58:39.440 --> 1:58:44.440
 And they gave it to people studying oceans and climate and stuff.

1:58:44.440 --> 1:58:46.440
 Yeah.

1:58:46.440 --> 1:58:53.440
 So I'm all for studying oceans and climates, but we need to actually have some money that actually goes into a safety research also and doesn't just get grabbed.

1:58:53.440 --> 1:58:55.440
 By whatever.

1:58:55.440 --> 1:58:57.440
 That's a fantastic investment.

1:58:57.440 --> 1:59:01.440
 And then at the higher level, you asked this question, OK, what can we do?

1:59:01.440 --> 1:59:04.440
 You know, what are the biggest risks?

1:59:04.440 --> 1:59:10.440
 I think I think we cannot just consider this to be only a technical problem.

1:59:10.440 --> 1:59:14.440
 Again, because if you solve only the technical problem, can I play with your robot?

1:59:14.440 --> 1:59:16.440
 Yes, please.

1:59:16.440 --> 1:59:22.440
 Get our machines, you know, to just blindly obey the orders we give them.

1:59:22.440 --> 1:59:25.440
 So we can always trust that it will do what we want.

1:59:25.440 --> 1:59:28.440
 That might be great for the owner of the robot.

1:59:28.440 --> 1:59:36.440
 It might not be so great for the rest of humanity if if that person is that least favorite world leader or whatever you imagine, right?

1:59:36.440 --> 1:59:44.440
 So we have to also take a look at the apply alignment, not just to machines, but to all the other powerful structures.

1:59:44.440 --> 1:59:46.440
 That's why it's so important to strengthen our democracy.

1:59:46.440 --> 1:59:58.440
 Again, as I said, to have institutions make sure that the playing field is not rigged so that corporations are given the right incentives to do the things that both make profit and are good for people.

1:59:58.440 --> 2:00:06.440
 To make sure that countries have incentives to do things that are both good for their people and don't screw up the rest of the world.

2:00:06.440 --> 2:00:10.440
 And this is not just something for AI nerds to geek out on.

2:00:10.440 --> 2:00:16.440
 This is an interesting challenge for political scientists, economists, and so many other thinkers.

2:00:16.440 --> 2:00:28.440
 So one of the magical things that perhaps makes this earth quite unique is that it's home to conscious beings.

2:00:28.440 --> 2:00:30.440
 So you mentioned consciousness.

2:00:30.440 --> 2:00:41.440
 Perhaps as a small aside, because we didn't really get specific to what how we might do the alignment, like you said, is there just a really important research problem?

2:00:41.440 --> 2:00:49.440
 But do you think engineering consciousness into AI systems is a possibility?

2:00:49.440 --> 2:01:02.440
 Is something that we might one day do or is there something fundamental to consciousness that is fundamental to humans and humans only?

2:01:02.440 --> 2:01:04.440
 I think it's possible.

2:01:04.440 --> 2:01:12.440
 I think both consciousness and intelligence are information processing, certain types of information processing.

2:01:12.440 --> 2:01:26.440
 And that fundamentally, it doesn't matter whether the information is processed by carbon atoms in neurons and brains or by silicon atoms and so on in our technology.

2:01:26.440 --> 2:01:28.440
 Some people disagree.

2:01:28.440 --> 2:01:37.440
 This is what I think is as physicists that I and the consciousness is the same kind of you said consciousness is information processing.

2:01:37.440 --> 2:01:47.440
 So meaning, I think you had a quote of something like it's information knowing itself, that kind of thing.

2:01:47.440 --> 2:01:53.440
 I think consciousness is the way information feels when it's being processed in certain complex ways.

2:01:53.440 --> 2:01:55.440
 We don't know exactly what those complex ways are.

2:01:55.440 --> 2:02:01.440
 It's clear that most of the information processing in our brains does not create an experience.

2:02:01.440 --> 2:02:03.440
 We're not even aware of it.

2:02:03.440 --> 2:02:10.440
 For example, you're not aware of your heartbeat regulation right now, even though it's clearly being done by your body.

2:02:10.440 --> 2:02:12.440
 It's just kind of doing its own thing.

2:02:12.440 --> 2:02:17.440
 When you go jogging, there's a lot of complicated stuff about how you put your foot down.

2:02:17.440 --> 2:02:19.440
 And we know it's hard.

2:02:19.440 --> 2:02:21.440
 That's why robots used to fall over so much.

2:02:21.440 --> 2:02:23.440
 But you're mostly unaware about it.

2:02:23.440 --> 2:02:29.440
 Your brain, your CEO consciousness module just sends an email, hey, I want to keep jogging along this path.

2:02:29.440 --> 2:02:31.440
 The rest is on autopilot.

2:02:31.440 --> 2:02:33.440
 So most of it is not conscious.

2:02:33.440 --> 2:02:41.440
 But somehow there is some of the information processing, which is we don't know what exactly.

2:02:41.440 --> 2:02:48.440
 I think this is a science problem that I hope one day we'll have some equation for or something.

2:02:48.440 --> 2:02:52.440
 So we can be able to build a consciousness detector and say, yeah, here there is some consciousness.

2:02:52.440 --> 2:02:53.440
 Here there is not.

2:02:53.440 --> 2:02:59.440
 Oh, don't boil that lobster because it's feeling pain or it's okay because it's not feeling pain.

2:02:59.440 --> 2:03:02.440
 Right now we treat this as sort of just metaphysics.

2:03:02.440 --> 2:03:10.440
 But it would be very useful in emergency rooms to know if a patient has locked in syndrome and is conscious,

2:03:10.440 --> 2:03:14.440
 or if they are actually just out.

2:03:14.440 --> 2:03:19.440
 And in the future, if you build a very, very intelligent helper robot to take care of you,

2:03:19.440 --> 2:03:23.440
 I think you'd like to know if you should feel guilty about shutting it down,

2:03:23.440 --> 2:03:29.440
 or if it's just like a zombie going through the motions like a fancy tape recorder.

2:03:29.440 --> 2:03:38.440
 And once we can make progress on the science of consciousness and figure out what is conscious and what isn't,

2:03:38.440 --> 2:03:47.440
 then assuming we want to create positive experiences and not suffering,

2:03:47.440 --> 2:03:55.440
 we'll probably choose to build some machines that are deliberately unconscious that do incredibly boring,

2:03:55.440 --> 2:03:59.440
 repetitive jobs in an iron mine somewhere or whatever.

2:03:59.440 --> 2:04:07.440
 And maybe we'll choose to create helper robots for the elderly that are conscious so that people don't just feel creeped out,

2:04:07.440 --> 2:04:12.440
 that the robot is just faking it when it acts like it's sad or happy.

2:04:12.440 --> 2:04:17.440
 Like you said, elderly, I think everybody gets pretty deeply lonely in this world.

2:04:17.440 --> 2:04:21.440
 And so there's a place, I think, for everybody to have a connection with conscious beings,

2:04:21.440 --> 2:04:24.440
 whether they're human or otherwise.

2:04:24.440 --> 2:04:32.440
 But I know for sure that I would, if I had a robot, if I was going to develop any kind of personal emotional connection with it,

2:04:32.440 --> 2:04:36.440
 I would be very creeped out if I knew it in intellectual level that the whole thing was just a fraud.

2:04:36.440 --> 2:04:42.440
 You know, today you can buy a little talking doll for a kid,

2:04:42.440 --> 2:04:47.440
 which will say things and the little child will often think that this is actually conscious

2:04:47.440 --> 2:04:52.440
 and even real secrets to it that then go on the internet with all sorts of creepy repercussions.

2:04:52.440 --> 2:04:57.440
 You know, I would not want to be just hacked and tricked like this.

2:04:57.440 --> 2:05:02.440
 If I was going to be developing real emotional connections with a robot,

2:05:02.440 --> 2:05:06.440
 I would want to know that this is actually real.

2:05:06.440 --> 2:05:09.440
 It's acting conscious, acting happy because it actually feels it.

2:05:09.440 --> 2:05:12.440
 And I think this is not sci fi.

2:05:12.440 --> 2:05:15.440
 It's possible to measure, to come up with tools.

2:05:15.440 --> 2:05:21.440
 After we understand the science of consciousness, you're saying we'll be able to come up with tools that can measure consciousness

2:05:21.440 --> 2:05:27.440
 and definitively say like this thing is experiencing the things it says it's experiencing.

2:05:27.440 --> 2:05:31.440
 Kind of by definition, if it is a physical phenomena, information processing,

2:05:31.440 --> 2:05:34.440
 and we know that some information processing is conscious and some isn't,

2:05:34.440 --> 2:05:37.440
 well then there is something there to be discovered with the methods of science.

2:05:37.440 --> 2:05:43.440
 Giulio Tononi has stuck his neck out the farthest and written down some equations for a theory.

2:05:43.440 --> 2:05:46.440
 Maybe that's right, maybe it's wrong, we certainly don't know.

2:05:46.440 --> 2:05:50.440
 But I applaud that kind of efforts to sort of take this,

2:05:50.440 --> 2:05:56.440
 say this is not just something that philosophers can have beer and muse about,

2:05:56.440 --> 2:06:00.440
 but something we can measure and study and coming, being that back to us.

2:06:00.440 --> 2:06:05.440
 I think what we would probably choose to do, as I said, is if we cannot figure this out,

2:06:05.440 --> 2:06:14.440
 choose to be quite mindful about what sort of consciousness, if any, we put in different machines that we have.

2:06:14.440 --> 2:06:23.440
 And certainly, we wouldn't want to make, we should not be making much of machines that suffer without us even knowing it, right?

2:06:23.440 --> 2:06:31.440
 And if at any point someone decides to upload themselves like Ray Kurzweil wants to do, I don't know if you've had him on your show.

2:06:31.440 --> 2:06:34.440
 We agree, but then COVID happens, so we're waiting it out a little bit.

2:06:34.440 --> 2:06:42.440
 You know, suppose he uploads himself into this robo array, and it talks like him, and acts like him, and laughs like him,

2:06:42.440 --> 2:06:49.440
 and before he powers off his biological body, he would probably be pretty disturbed if he realized that there's no one home.

2:06:49.440 --> 2:06:53.440
 This robot is not having any subjective experience, right?

2:06:53.440 --> 2:07:05.440
 If humanity gets replaced by machine descendants, do all these cool things, and build spaceships, and go to intergalactic rock concerts,

2:07:05.440 --> 2:07:11.440
 and it turns out that they are all unconscious, just going through the motions.

2:07:11.440 --> 2:07:17.440
 Wouldn't that be like the ultimate zombie apocalypse, right? Just a play for empty benches?

2:07:17.440 --> 2:07:22.440
 Yeah, I have a sense that there's some kind of, once we understand consciousness better,

2:07:22.440 --> 2:07:27.440
 we'll understand that there's some kind of continuum, and it would be a greater appreciation.

2:07:27.440 --> 2:07:31.440
 And we'll probably understand, just like you said, it'd be unfortunate if it's a trick.

2:07:31.440 --> 2:07:38.440
 We'll probably definitely understand that love is indeed a trick that will play on each other, that we humans are.

2:07:38.440 --> 2:07:46.440
 We convince ourselves we're conscious, but we're really, you know, awesome trees and dolphins are all the same kind of consciousness.

2:07:46.440 --> 2:07:50.440
 Can I try to cheer you up a little bit with a philosophical thought here about the love part?

2:07:50.440 --> 2:07:52.440
 Yes, let's do it.

2:07:52.440 --> 2:08:01.440
 You might say, okay, love is just a collaboration enabler, and then maybe you can go and get depressed about that.

2:08:01.440 --> 2:08:04.440
 But I think that would be the wrong conclusion, actually.

2:08:04.440 --> 2:08:13.440
 I know that the only reason I enjoy food is because my genes hacked me, and they don't want me to starve to death,

2:08:13.440 --> 2:08:20.440
 not because they care about me consciously enjoying succulent delights of pistachio ice cream,

2:08:20.440 --> 2:08:23.440
 but they just want me to make copies of them.

2:08:23.440 --> 2:08:28.440
 So in a sense, the whole enjoyment of food is also a scam, like this.

2:08:28.440 --> 2:08:32.440
 But does that mean I shouldn't take pleasure in this pistachio ice cream?

2:08:32.440 --> 2:08:38.440
 I love pistachio ice cream, and I can tell you, I know this is an experimental fact.

2:08:38.440 --> 2:08:46.440
 I enjoy pistachio ice cream every bit as much, even though I scientifically know exactly what kind of scam this was.

2:08:46.440 --> 2:08:50.440
 Your genes really appreciate that you like the pistachio ice cream.

2:08:50.440 --> 2:08:55.440
 Well, but I, my mind appreciates it too, you know, and I have a conscious experience right now.

2:08:55.440 --> 2:09:00.440
 Ultimately, all of my brain is also just something the genes built to copy themselves.

2:09:00.440 --> 2:09:04.440
 But so what, you know, I'm grateful that, yeah, thanks genes for doing this,

2:09:04.440 --> 2:09:09.440
 but, you know, now it's my brain that's in charge here, and I'm going to enjoy my conscious experience.

2:09:09.440 --> 2:09:10.440
 Thank you very much.

2:09:10.440 --> 2:09:19.440
 And not just the pistachio ice cream, but also the love I feel for my amazing wife and all the other delights of being conscious.

2:09:19.440 --> 2:09:24.440
 I don't, actually Richard Feynman, I think said this so well.

2:09:24.440 --> 2:09:28.440
 He is also the guy, you know, really got me into physics.

2:09:28.440 --> 2:09:34.440
 Some art friend said that, oh, science kind of just is the party pooper.

2:09:34.440 --> 2:09:36.440
 It kind of ruins the fun, right?

2:09:36.440 --> 2:09:43.440
 When like, you have a beautiful flowers as the artist, and then the scientist is going to deconstruct that into just a blob of quarks and electrons.

2:09:43.440 --> 2:09:52.440
 And Feynman pushed back on that in such a beautiful way, which I think also can be used to push back and make you not feel guilty about falling in love.

2:09:52.440 --> 2:09:54.440
 So here's what Feynman basically said.

2:09:54.440 --> 2:09:59.440
 He said to his friend, you know, yeah, I can also, as a scientist, see that this is a beautiful flower.

2:09:59.440 --> 2:10:00.440
 Thank you very much.

2:10:00.440 --> 2:10:06.440
 Maybe I can't draw as good a painting as you because I'm not as talented an artist, but yeah, I can really see the beauty in it.

2:10:06.440 --> 2:10:08.440
 And it just, it also looks beautiful to me.

2:10:08.440 --> 2:10:16.440
 But in addition to that, Feynman said, as a scientist, I see even more beauty that the artist did not see, right?

2:10:16.440 --> 2:10:25.440
 Suppose this is a flower on a blossoming apple tree, you could say this tree has more beauty in it than just the colors and the fragrance.

2:10:25.440 --> 2:10:28.440
 This tree is made of air, Feynman wrote.

2:10:28.440 --> 2:10:30.440
 This is one of my favorite Feynman quotes ever.

2:10:30.440 --> 2:10:37.440
 And it took the carbon out of the air and bound it in using the flaming heat of the sun, you know, to turn the air into tree.

2:10:37.440 --> 2:10:44.440
 And when you burn logs in your fireplace, it's really beautiful to think that this is being reversed.

2:10:44.440 --> 2:10:58.440
 Now the tree is going, the wood is going back into air and in this flaming, beautiful dance of the fire that the artist can see is the flaming light of the sun that was bound in to turn the air into tree.

2:10:58.440 --> 2:11:03.440
 And then the ashes is the little residue that didn't come from the air, that the tree sucked out of the ground, you know.

2:11:03.440 --> 2:11:07.440
 Feynman said, these are beautiful things and science just adds.

2:11:07.440 --> 2:11:09.440
 It doesn't subtract.

2:11:09.440 --> 2:11:15.440
 And I feel exactly that way about love and about pistachio ice cream also.

2:11:15.440 --> 2:11:20.440
 I can understand that there is even more nuance to the whole thing, right?

2:11:20.440 --> 2:11:27.440
 At this very visceral level, you can fall in love just as much as someone who knows nothing about neuroscience.

2:11:27.440 --> 2:11:32.440
 But you can also appreciate this even greater beauty in it.

2:11:32.440 --> 2:11:42.440
 Isn't it remarkable that it came about from this completely lifeless universe, just a bunch of hot blob of plasma expanding?

2:11:42.440 --> 2:11:53.440
 And then over the eons, you know, gradually, first the strong nuclear force decided to combine quarks together into nuclei and then the electric force bound in electrons and made atoms.

2:11:53.440 --> 2:11:57.440
 And then they clustered it from gravity and you've got planets and stars and this and that.

2:11:57.440 --> 2:12:06.440
 And then natural selection came along and the genes had their little thing and you started getting what went from seeming like a completely pointless universe.

2:12:06.440 --> 2:12:11.440
 So we're just trying to increase entropy and approach heat depth into something that looked more goal oriented.

2:12:11.440 --> 2:12:13.440
 Isn't that kind of beautiful?

2:12:13.440 --> 2:12:18.440
 And then this goal orientedness through evolution got ever more sophisticated where you got ever more.

2:12:18.440 --> 2:12:31.440
 And then you started getting this thing which is kind of like DeepMind's Mu Zero and steroids, the ultimate self play is not what DeepMind's AI does against itself to get better at the go.

2:12:31.440 --> 2:12:38.440
 It's what all these little cork blobs did against each other in the game of survival of the fittest.

2:12:38.440 --> 2:12:50.440
 Now, when you had really dumb bacteria living in a simple environment, there wasn't much incentive to get intelligent, but then the life made environment more complex.

2:12:50.440 --> 2:12:53.440
 And then there was more incentive to get even smarter.

2:12:53.440 --> 2:12:57.440
 And that gave the other organisms more incentive to also get smarter.

2:12:57.440 --> 2:13:08.440
 And then here we are now just like Mu Zero learned to become world master at the go and chess from playing against itself by just playing against itself.

2:13:08.440 --> 2:13:17.440
 All the quarks here on our planet and electrons have created giraffes and elephants and humans and love.

2:13:17.440 --> 2:13:20.440
 I just find that really beautiful.

2:13:20.440 --> 2:13:23.440
 And I think that just adds to the enjoyment of love.

2:13:23.440 --> 2:13:25.440
 It doesn't subtract anything.

2:13:25.440 --> 2:13:27.440
 Do you feel a little more careful now?

2:13:27.440 --> 2:13:28.440
 I feel way better.

2:13:28.440 --> 2:13:30.440
 That was incredible.

2:13:30.440 --> 2:13:43.440
 So this self play of quarks, taking back to the beginning of our conversation a little bit, there's so many exciting possibilities about artificial intelligence understanding the basic laws of physics.

2:13:43.440 --> 2:13:46.440
 Do you think AI will help us unlock?

2:13:46.440 --> 2:13:57.440
 There's been quite a bit of excitement throughout the history of physics of coming up with more and more general simple laws that explain the nature of our reality.

2:13:57.440 --> 2:14:03.440
 And then the ultimate of that would be a theory of everything that combines everything together.

2:14:03.440 --> 2:14:16.440
 Do you think it's possible that one we humans, but perhaps AI systems will figure out a theory of physics that unifies all the laws of physics?

2:14:16.440 --> 2:14:19.440
 Yeah, I think it's absolutely possible.

2:14:19.440 --> 2:14:24.440
 I think it's very clear that we're going to see a great boost to science.

2:14:24.440 --> 2:14:33.440
 We're already seeing a boost actually from machine learning, helping science. Alpha fold was an example, you know, decades old protein folding problem.

2:14:33.440 --> 2:14:53.440
 And gradually, yeah, unless we go extinct by doing something dumb like we discussed, I think it's very likely that our understanding of physics will become so good that our technology will no longer be limited by human

2:14:53.440 --> 2:14:57.440
 intelligence, but instead be limited by the laws of physics.

2:14:57.440 --> 2:15:01.440
 So our tech today is limited by what we've been able to invent, right?

2:15:01.440 --> 2:15:14.440
 I think as AI progresses, it'll just be limited by the speed of light and other physical limits, which would mean it's going to be just dramatically beyond, you know, where we are now.

2:15:14.440 --> 2:15:25.440
 Do you think it's a fundamentally mathematical pursuit of trying to understand like the laws of the governing our universe from a mathematical perspective?

2:15:25.440 --> 2:15:32.440
 It's almost like if it's AI, it's exploring the space of like theorems and those kinds of things.

2:15:32.440 --> 2:15:40.440
 Or is there some other more computational ideas, more sort of empirical ideas?

2:15:40.440 --> 2:15:47.440
 They're both, I would say, it's really interesting to look out at the landscape of everything we call science today.

2:15:47.440 --> 2:15:55.440
 So here you come now with this big new hammer, it says machine learning on it and ask, you know, where are there some nails that you can help with here that you can hammer?

2:15:55.440 --> 2:16:05.440
 Ultimately, if machine learning gets the point that it can do everything better than us, it will be able to help across the whole space of science.

2:16:05.440 --> 2:16:11.440
 But maybe we can anchor it by starting a little bit right now near term and see how we kind of move forward.

2:16:11.440 --> 2:16:28.440
 So like right now, first of all, you have a lot of big data science where, for example, with telescopes, we are able to collect way more data every hour than a grad student can just pour over like in the old times, right?

2:16:28.440 --> 2:16:38.440
 And machine learning is already being used very effectively, even at MIT, to find planets around other stars, to detect exciting new signatures of new particle physics in the sky,

2:16:38.440 --> 2:16:49.440
 to detect the ripples in the fabric of space time that we call gravitational waves caused by enormous black holes crashing into each other halfway across the observable universe.

2:16:49.440 --> 2:16:58.440
 Machine learning is running and taking it right now, you know, doing all these things and it's really helping all these experimental fields.

2:16:58.440 --> 2:17:05.440
 There is a separate front of physics, computational physics, which is getting an enormous boost also.

2:17:05.440 --> 2:17:09.440
 So we had to do all our computations by hand, right?

2:17:09.440 --> 2:17:19.440
 People would have these giant books with tables of logarithms and oh my God, it pains me to even think how long time it would have taken to do simple stuff.

2:17:19.440 --> 2:17:26.440
 Then we started to get calculators and computers that could do some basic math for us.

2:17:26.440 --> 2:17:40.440
 Now what we're starting to see is kind of a shift from go fi computational physics to neural network computational physics.

2:17:40.440 --> 2:17:52.440
 What I mean by that is most computational physics would be done by humans programming in the intelligence of how to do the computation into the computer.

2:17:52.440 --> 2:17:59.440
 Just as when Gary Kasparov got his posterior kicked by IBM's Deep Blue in chess, humans had programmed in exactly how to play chess.

2:17:59.440 --> 2:18:03.440
 Intelligence came from the humans, it wasn't learned, right?

2:18:03.440 --> 2:18:15.440
 Mu zero can be not only Kasparov in chess, but also stock fish, which is the best go fi chess program by learning.

2:18:15.440 --> 2:18:22.440
 And we're seeing more of that now, that shift beginning to happen in physics. So let me give you an example.

2:18:22.440 --> 2:18:32.440
 So lattice QCD is an area of physics whose goal is basically to take the periodic table and just compute the whole thing from first principles.

2:18:32.440 --> 2:18:35.440
 This is not the search for theory of everything.

2:18:35.440 --> 2:18:44.440
 We already know the theory that's supposed to produce as output the periodic table, which atoms are stable, how heavy they are, all that good stuff.

2:18:44.440 --> 2:18:54.440
 These are spectral lines. It's a theory, lattice QCD, you can put it on your t shirt, our colleague Frank Wilczek got the Nobel Prize for working on it.

2:18:54.440 --> 2:19:02.440
 But the math is just too hard for us to solve. We have not been able to start with these equations and solve them to the extent that we can predict, oh yeah.

2:19:02.440 --> 2:19:07.440
 And then there is carbon, and this is what the spectrum of the carbon atom looks like.

2:19:07.440 --> 2:19:20.440
 But awesome people are building these super computer simulations where you just put in these equations and you make a big cubic lattice of space.

2:19:20.440 --> 2:19:26.440
 Or actually it's a very small lattice because you're going down to the subatomic scale and you try to solve it.

2:19:26.440 --> 2:19:34.440
 But it's just so computationally expensive that we still haven't been able to calculate things as accurately as we measure them in many cases.

2:19:34.440 --> 2:19:37.440
 And now machine learning is really revolutionizing this.

2:19:37.440 --> 2:19:47.440
 So my colleague Fiola Shanahan at MIT, for example, she's been using this really cool machine learning technique called normalizing flows,

2:19:47.440 --> 2:19:55.440
 where she's realized she can actually speed up the calculation dramatically by having the AI learn how to do things faster.

2:19:55.440 --> 2:20:06.440
 Another area like this where we suck up an enormous amount of super computer time to do physics is black hole collisions.

2:20:06.440 --> 2:20:14.440
 So now that we've done the sexy stuff of detecting a bunch of this with LIGO and other experiments, we want to be able to know what we're seeing.

2:20:14.440 --> 2:20:19.440
 And so it's a very simple conceptual problem. It's the two body problem.

2:20:19.440 --> 2:20:27.440
 Newton solved it for classical gravity hundreds of years ago, but the two body problem is still not fully solved.

2:20:27.440 --> 2:20:28.440
 For black holes.

2:20:28.440 --> 2:20:33.440
 Yes, a nice thing is gravity because they won't just orbit each other forever anymore, two things.

2:20:33.440 --> 2:20:37.440
 They give off gravitational waves and make sure they crash into each other.

2:20:37.440 --> 2:20:45.440
 And the game, what you want to do is you want to figure out, okay, what kind of wave comes out as a function of the masses of the two black holes,

2:20:45.440 --> 2:20:49.440
 as a function of how they're spinning relative to each other, etc.

2:20:49.440 --> 2:20:51.440
 And that is so hard.

2:20:51.440 --> 2:20:56.440
 It can take months of super computer time on massive numbers of cores to do it, you know.

2:20:56.440 --> 2:21:03.440
 Wouldn't it be great if you can use machine learning to greatly speed that up, right?

2:21:03.440 --> 2:21:14.440
 Now you can use the expensive old go fi calculation as the truth and then see if machine learning can figure out a smarter, faster way of getting the right answer.

2:21:14.440 --> 2:21:23.440
 Yet another area, like computational physics, these are probably the big three that suck up the most computer time.

2:21:23.440 --> 2:21:33.440
 Lattice QCD, black hole collisions and cosmological simulations, where you take not a subatomic thing and try to figure out the mass of the proton,

2:21:33.440 --> 2:21:40.440
 but you take something that's enormous and try to look at how all the galaxies get formed in there.

2:21:40.440 --> 2:21:49.440
 There again, there are a lot of very cool ideas right now about how you can use machine learning to do this sort of stuff better.

2:21:49.440 --> 2:21:56.440
 The difference between this and the big data is you kind of make the data yourself, right?

2:21:56.440 --> 2:22:02.440
 And then finally, we're looking over the physical landscape and seeing what can we hammer with machine learning, right?

2:22:02.440 --> 2:22:09.440
 So we talked about experimental data, big data, discovering cool stuff that we humans then look more closely at.

2:22:09.440 --> 2:22:18.440
 Then we talked about taking the expensive computations we're doing now and figuring out how to do the much faster and better with AI.

2:22:18.440 --> 2:22:21.440
 And finally, let's go really theoretical.

2:22:21.440 --> 2:22:28.440
 So things like discovering equations, having deep fundamental insights.

2:22:28.440 --> 2:22:33.440
 This is something closest to what I've been doing in my group.

2:22:33.440 --> 2:22:42.440
 We talked earlier about the whole AI Feynman project, where if you just have some data, how do you automatically discover equations that seem to describe this well,

2:22:42.440 --> 2:22:47.440
 that you can then go back as a human and work with and test and explore.

2:22:47.440 --> 2:22:54.440
 And you asked a really good question also about if this is sort of a search problem in some sense.

2:22:54.440 --> 2:22:57.440
 That's very deep, actually, what you said, because it is.

2:22:57.440 --> 2:23:02.440
 Suppose I asked you to prove some mathematical theorem.

2:23:02.440 --> 2:23:08.440
 What is a proof in math? It's just a long string of steps, logical steps that you can write out with symbols.

2:23:08.440 --> 2:23:15.440
 And once you find it, it's very easy to write a program to check whether it's a valid proof or not.

2:23:15.440 --> 2:23:17.440
 So why is it so hard to prove it then?

2:23:17.440 --> 2:23:22.440
 Well, because there are ridiculously many possible candidate proofs you could write down, right?

2:23:22.440 --> 2:23:33.440
 If the proof contains 10,000 symbols, even if there are only 10 options for what each symbol could be, that's 10 to the power of 1,000 possible proofs,

2:23:33.440 --> 2:23:36.440
 which is way more than there are atoms in our universe, right?

2:23:36.440 --> 2:23:38.440
 So you could say it's trivial to prove these things.

2:23:38.440 --> 2:23:44.440
 You just write a computer, generate all strings, and then check, is this a valid proof? No.

2:23:44.440 --> 2:23:47.440
 Is this a valid proof? No.

2:23:47.440 --> 2:23:51.440
 And then you just keep doing this forever.

2:23:51.440 --> 2:23:54.440
 But it is fundamentally a search problem.

2:23:54.440 --> 2:24:02.440
 You just want to search the space of all strings of symbols to find the one that is the proof, right?

2:24:02.440 --> 2:24:08.440
 And there's a whole area of machine learning called search.

2:24:08.440 --> 2:24:12.440
 How do you search through some giant space to find the needle in the haystack?

2:24:12.440 --> 2:24:18.440
 It's easier in cases where there's a clear measure of good, like you're not just right or wrong,

2:24:18.440 --> 2:24:23.440
 but this is better and this is worse, so you can maybe get some hints as to which direction to go in.

2:24:23.440 --> 2:24:27.440
 That's why we talked about neural networks work so well.

2:24:27.440 --> 2:24:37.440
 I mean, that's such a human thing of that moment of genius of figuring out the intuition of good, essentially.

2:24:37.440 --> 2:24:38.440
 I mean, we thought that that was...

2:24:38.440 --> 2:24:39.440
 What is it?

2:24:39.440 --> 2:24:42.440
 Maybe it's not, right? We thought that about chess, right?

2:24:42.440 --> 2:24:43.440
 Exactly.

2:24:43.440 --> 2:24:51.440
 That the ability to see 10, 15, sometimes 20 steps ahead was not a calculation that humans were performing.

2:24:51.440 --> 2:24:58.440
 It was some kind of weird intuition about different patterns, about board positions, about the relative positions.

2:24:58.440 --> 2:24:59.440
 Exactly.

2:24:59.440 --> 2:25:03.440
 Somehow stitching stuff together and a lot of it is just intuition.

2:25:03.440 --> 2:25:10.440
 But then you have Alpha, I guess, Zero be the first one that did the self play.

2:25:10.440 --> 2:25:15.440
 It just came up with this. It was able to learn through self play mechanism, this kind of intuition.

2:25:15.440 --> 2:25:16.440
 Exactly.

2:25:16.440 --> 2:25:28.440
 But just as you said, it's so fascinating to think whether in the space of totally new ideas, can that be done in developing theorems?

2:25:28.440 --> 2:25:36.440
 We know it can be done by neural networks because we did it with the neural networks in the craniums of the great mathematicians of humanity, right?

2:25:36.440 --> 2:25:40.440
 And I'm so glad you brought up Alpha, Zero, because that's the counter example.

2:25:40.440 --> 2:25:45.440
 It turned out we were flattering ourselves when we said intuition is something different.

2:25:45.440 --> 2:25:48.440
 It's only humans can do it. It's not the information processing.

2:25:48.440 --> 2:25:54.440
 If you... If it used to be that way, again, it's very...

2:25:54.440 --> 2:26:04.440
 It's really instructive, I think, to compare the chess computer, Deep Blue, that beat Kasparov with Alpha, Zero, that beat Lisa Dahl at the go.

2:26:04.440 --> 2:26:08.440
 Because for Deep Blue, there was no intuition.

2:26:08.440 --> 2:26:11.440
 There was some pro... Humans had programmed in some intuition.

2:26:11.440 --> 2:26:20.440
 After humans had played a lot of games, they told the computer, you know, count the pawn as one point, the bishop is three points, the rook is five points, and so on.

2:26:20.440 --> 2:26:27.440
 You add it all up and then you add some extra points for past pawns and subtract if the opponent has it and blah, blah, blah, blah.

2:26:27.440 --> 2:26:32.440
 And then what Deep Blue did was just search.

2:26:32.440 --> 2:26:42.440
 Just very brute force tried many, many moves ahead, all these combinations and a prune tree search, and it could think much faster than Kasparov and it won, right?

2:26:42.440 --> 2:26:51.440
 And that, I think, inflated our egos in a way it shouldn't have, because people started to say, yeah, yeah, it's just brute force search, but it has no intuition.

2:26:51.440 --> 2:27:00.440
 Alpha, Zero really popped our bubble there, because what Alpha, Zero does...

2:27:00.440 --> 2:27:14.440
 Yes, it does also do some of that tree search, but it also has this intuition module, which in GeekSpeak is called a value function, where it just looks at the board and comes up with a number for how good is that position.

2:27:14.440 --> 2:27:19.440
 The difference was no human told it how good the position is.

2:27:19.440 --> 2:27:21.440
 It just learned it.

2:27:21.440 --> 2:27:44.440
 And Mu Zero is the coolest or scariest of all, depending on your mood, because the same basic AI system will learn what the good board position is, regardless of whether it's chess or Go or Shogi or Pacman or Lady Pacman or Breakout or Space Invaders or any number, a bunch of other games.

2:27:44.440 --> 2:27:49.440
 You don't tell it anything and it gets this intuition after a while for what's good.

2:27:49.440 --> 2:28:02.440
 So this is very hopeful for science, I think, because if it can get intuition for what's a good position there, maybe it can also get intuition for what are some good directions to go if you're trying to prove something.

2:28:02.440 --> 2:28:19.440
 I often, one of the most fun things in my science career is when I've been able to prove some theorem about something, and it's very heavily intuition guided, of course. I don't sit and try all random strings. I have a hunch that, you know, this reminds me a little bit about this other proof I've seen for this thing.

2:28:19.440 --> 2:28:38.440
 So maybe I, first, what if I try this? No, that didn't work out. But this reminds me, actually, the way this failed reminds me of that. So combining the intuition with all these brute force capabilities, I think it's going to be able to help physics too.

2:28:38.440 --> 2:28:54.440
 Do you think there will be a day when an AI system being the primary contributor, let's say 90% plus wins the Nobel Prize in physics? Obviously, they'll give it to the humans because we humans don't like to give prizes to machines.

2:28:54.440 --> 2:29:03.440
 It'll give it to the humans behind the system. You could argue that AI has already been involved in some Nobel prizes, probably maybe something with black holes and stuff like that.

2:29:03.440 --> 2:29:11.440
 Yeah, we don't like giving prizes to other life forms. If someone wins a horse racing contest, they don't give the prize to the horse either.

2:29:11.440 --> 2:29:31.440
 That's true. But do you think that we might be able to see something like that in our lifetimes when AI? So like the first system, I would say, that makes us think about a Nobel Prize seriously is like Alpha Fold is making us think about in medicine, physiology, a Nobel Prize.

2:29:31.440 --> 2:29:41.440
 Perhaps discoveries that are direct result of something that's discovered by Alpha Fold. Do you think in physics, we might be able to see that in our lifetimes?

2:29:41.440 --> 2:29:57.440
 I think what's probably going to happen is more of a blurring of the distinctions. So today, if somebody uses a computer to do a computation that gives them the Nobel Prize, nobody's going to dream of giving the prize to the computer.

2:29:57.440 --> 2:30:11.440
 Maybe like that was just a tool. I think for these things also, people are just going to for a long time view the computer as a tool. But what's going to change is the ubiquity of machine learning.

2:30:11.440 --> 2:30:30.440
 I think at some point in my lifetime, finding a human physicist who knows nothing about machine learning is going to be about almost as hard as it is today finding a human physicist who doesn't says, Oh, I don't know anything about computers, or I don't use math.

2:30:30.440 --> 2:30:33.440
 It would just be a ridiculous concept.

2:30:33.440 --> 2:30:52.440
 But the thing is, there is a magic moment, though, like with Alpha Zero, when the system surprises us in a way where the best people in the world truly learn something from the system in a way where you feel like it's another entity.

2:30:52.440 --> 2:31:08.440
 The way people, the way Magnus Carlson, the way certain people are looking at the work of Alpha Zero, it truly is no longer a tool in the sense that it doesn't feel like a tool. It feels like some other entity.

2:31:08.440 --> 2:31:29.440
 So there is a magic difference where you're like, if an AI system is able to come up with an insight that surprises everybody in some major way that's a phase shift in our understanding of some particular science or some particular aspect of physics,

2:31:29.440 --> 2:31:38.440
 I feel like that is no longer a tool. And then you can start to say that it perhaps deserves the prize.

2:31:38.440 --> 2:31:50.440
 So for sure, the more important, the more fundamental transformation of the 21st century science is exactly what you're saying, which is probably everybody will be doing machine learning.

2:31:50.440 --> 2:32:10.440
 It's to some degree, like if you want to be successful at unlocking the mysteries of science, you should be doing machine learning. But it's just exciting to think about like whether there'll be one that comes along that's super surprising and they'll make a question like who the real inventors are in this world.

2:32:10.440 --> 2:32:25.440
 Yeah. Yeah, I think the question of isn't if it's going to happen, but when and but it's important also in my mind, the time when that happens is also more or less the same time when we get artificial general intelligence.

2:32:25.440 --> 2:32:43.440
 And then we have a lot bigger things to worry about than whether it should get the Nobel Prize or not, right? Because when you have machines that can outperform our best scientists at science, they can probably outperform us at a lot of other stuff as well,

2:32:43.440 --> 2:33:01.440
 which can at a minimum, you know, make them incredibly powerful agents in the world, you know. And I think it's a mistake to think we only have to start worrying about loss of control when machines get to AGI across the board where they can do everything, all our jobs.

2:33:01.440 --> 2:33:22.440
 Long before that, they'll be hugely influential. We talked at length about how the hacking of our minds with algorithms trying to get us glued to our screens, right, has already had a big impact on society.

2:33:22.440 --> 2:33:38.440
 There was an incredibly dumb algorithm in the grand scheme of things, right, just supervised machine learning, yet it had had huge impact. So I just don't want us to be lulled into false sense of security and think there won't be any societal impact until things reach human level because it's happening already.

2:33:38.440 --> 2:33:50.440
 And I was just thinking the other week, you know, when I see some scaremonger going, oh, the robots are coming. The implication is always that they're coming to kill us.

2:33:50.440 --> 2:34:05.440
 And maybe you should have worried about that if you were in Nagorno Karabakh during the recent war there. But more seriously, the robots are coming right now, but they're mainly not coming to kill us. They're coming to hack us.

2:34:05.440 --> 2:34:24.440
 They're coming to hack our minds into buying things that maybe we didn't need to vote for people who may not have our best interest in mind. And it's kind of humbling, I think, actually, as a human being to admit that it turns out that our minds are actually much more hackable than we thought.

2:34:24.440 --> 2:34:40.440
 And the ultimate insult is that we are actually getting hacked by the machine learning algorithms that are in some objective sense much dumber than us, you know. But maybe we shouldn't be so surprised because, you know, how do you feel about the cute puppies?

2:34:40.440 --> 2:34:41.440
 Love them.

2:34:41.440 --> 2:34:54.440
 So, you know, you would probably argue that in some across the board measure, you're more intelligent than they are. But boy, our cute puppies good at hacking us, right? Yeah, they move into our house, persuade us to feed them and do all these things.

2:34:54.440 --> 2:35:08.440
 What do they ever do for us? Yeah, other than being cute and making us feel good, right? So if puppies can hack us, maybe we shouldn't be so surprised if pretty dumb machine learning algorithms can hack us too.

2:35:08.440 --> 2:35:22.440
 Not to speak of cats, which is another level. And I think we should, to counter your previous point about there, let us not think about evil creatures in this world. We can all agree that cats are as close to objective evil as we can get.

2:35:22.440 --> 2:35:41.440
 But that's just me saying that. Have you seen the cartoon? I think it's maybe the onion with this incredibly cute kitten. And it just says underneath something that thinks about murder all day. Exactly.

2:35:41.440 --> 2:35:47.440
 That's accurate. You mentioned offline that there might be a link between post biological AGI and SETI.

2:35:47.440 --> 2:36:06.440
 So last time we talked, you've talked about this intuition that we humans might be quite unique in our galactic neighborhood, perhaps our galaxy, perhaps the entirety of the observable universe.

2:36:06.440 --> 2:36:33.440
 You might be the only intelligence civilization here, which is, and you argue pretty well for that thought. So I have a few little questions around this one, the scientific question, in which way would you be, if you were wrong in that intuition,

2:36:33.440 --> 2:36:56.440
 in which way do you think you would be surprised? Like why were you wrong? We find out that you ended up being wrong. Like in which dimension? So like, is it because we can't see them? Is it because the nature of their intelligence or the nature of their life is totally different than we can possibly imagine?

2:36:56.440 --> 2:37:20.440
 Is it because the, I mean, something about the great filters and surviving them? Or maybe because we're being protected from signals? All those explanations for why we haven't heard a big loud like red light that says we're here.

2:37:20.440 --> 2:37:47.440
 Yeah. So there are actually two separate things there that I could be wrong about, two separate claims that I made, right? One of them is, I made the claim, I think most civilizations, going from simple bacteria like things to space colonizing civilizations,

2:37:47.440 --> 2:38:03.440
 they spend only a very, very tiny fraction of their other life being where we are. That I could be wrong about. The other one I could be wrong about is quite different statement that I think that actually I'm guessing that we are the only

2:38:03.440 --> 2:38:14.440
 civilization in our observable universe from which light has reached us so far, that's actually gotten far enough to invent telescopes. So let's talk about maybe both of them in turn because they really are different.

2:38:14.440 --> 2:38:27.440
 The first one, if you look at the n equals one, the data point we have on this planet, right? So we spent four and a half billion years flexing around on this planet with life, right?

2:38:27.440 --> 2:38:46.440
 We got, and most of it was pretty lame stuff from an intelligence perspective, you know, as bacteria and then the dinosaurs spent, then the things gradually accelerated, right? Then the dinosaurs spent over 100 million years stomping around here without even inventing smartphones.

2:38:46.440 --> 2:39:02.440
 And then very recently, you know, it's only, we've only spent 400 years going from Newton to us, right? In terms of technology and we've looked at what we've done even, you know, when I was a little kid, there was no internet even.

2:39:02.440 --> 2:39:18.440
 So it's, I think it's pretty likely for in this case of this planet, right, that we're either going to really get our act together and start spreading life into space the century and doing all sorts of great things or we're going to wipe out.

2:39:18.440 --> 2:39:37.440
 It's a little hard. If I could be wrong in the sense that maybe what happened on this earth is very atypical. And for some reason, what's more common on other planets is that they spend an enormously long time futzing around with the ham radio and things, but they just never really take it to the next level for reasons I don't have.

2:39:37.440 --> 2:39:50.440
 I haven't understood and I'm humble and open to that. But I would bet at least 10 to one that our situation is more typical because the whole thing with Moore's law and accelerating technology, it's pretty obvious why it's happening.

2:39:50.440 --> 2:40:01.440
 Everything that grows exponentially, we call it an explosion, whether it's a population explosion or a nuclear explosion, it's always caused by the same thing. It's that the next step triggers a step after that.

2:40:01.440 --> 2:40:16.440
 Today's technology enables tomorrow's technology and that enables the next level because the technology is always better, of course, the steps can come faster and faster.

2:40:16.440 --> 2:40:22.440
 On the other question that I might be wrong about, that's the much more controversial one, I think.

2:40:22.440 --> 2:40:43.440
 Before we close out on this thing about the first one, if it's true that most civilizations spend only a very short amount of their total time in the stage, say, between inventing telescopes or mastering electricity and doing space travel,

2:40:43.440 --> 2:40:58.440
 if that's actually generally true, but then that should apply also elsewhere out there. So we should be very, very surprised if we find some random civilization and we happen to catch them exactly in that very, very short stage.

2:40:58.440 --> 2:41:02.440
 It's much more likely that we find a planet full of bacteria.

2:41:02.440 --> 2:41:13.440
 Yes. Or that we find some civilization that's already post biological and has done some really cool galactic construction projects in their galaxy.

2:41:13.440 --> 2:41:23.440
 Would we be able to recognize them, do you think? Is it possible that we just can't? I mean, this post biological world, could it be just existing in some other dimension?

2:41:23.440 --> 2:41:32.440
 Could it be just all a virtual reality game for them or something? I don't know. That it changes completely where we won't be able to detect?

2:41:32.440 --> 2:41:44.440
 We have to be, honestly, very humble about this. I think I said earlier the number one principle being scientists is you have to be humble and willing to acknowledge that everything we think, guess, might be totally wrong.

2:41:44.440 --> 2:41:57.440
 Of course, you could imagine some civilization where they all decide to become Buddhists and very inward looking and just move into their little virtual reality and not disturb the flora and fauna around them and we might not notice them.

2:41:57.440 --> 2:42:13.440
 But this is a numbers game, right? If you have millions of civilizations out there or billions of them, all it takes is one with a more ambitious mentality that decides, hey, we are going to go out and settle

2:42:13.440 --> 2:42:22.440
 for a bunch of other solar systems and maybe galaxies. And then it doesn't matter if they're a bunch of quiet Buddhists, we're still going to notice that expansionist one, right?

2:42:22.440 --> 2:42:38.440
 And it seems like quite the stretch to assume that, you know, we know even in our own galaxy that there are probably a billion or more planets that are pretty Earth like and many of them were formed over a billion years before ours.

2:42:38.440 --> 2:42:58.440
 So I had a big head start. So if you actually assume also that life happens kind of automatically on an Earth like planet, I think it's pretty quite the stretch to then go and say, okay, so we have other billions of and other billion civilizations out there that also have our level of tech and they all decided to become Buddhists

2:42:58.440 --> 2:43:10.440
 and not a single one decided to go like go Hitler on the galaxy and say we need to go out and colonize or and or not and not a single one decided for more benevolent reasons to go out and get more resources.

2:43:10.440 --> 2:43:21.440
 That seems seems like a bit of a stretch, frankly, and this leads into the the second thing you challenge me to be that I might be wrong about how rare or common is life.

2:43:21.440 --> 2:43:29.440
 You know, so Francis Drake, when he wrote down the Drake equation, multiplied together a huge number of factors, and then we don't know any of them.

2:43:29.440 --> 2:43:34.440
 So we know even less about what you get when you multiply together the whole product.

2:43:34.440 --> 2:43:38.440
 Since then, a lot of those factors have become much better known.

2:43:38.440 --> 2:43:43.440
 One of his big uncertainties was how common is it that a solar system even has a planet.

2:43:43.440 --> 2:43:44.440
 Right.

2:43:44.440 --> 2:43:47.440
 Well, now we know a very common Earth like planets.

2:43:47.440 --> 2:43:51.440
 We know where but our diamond doesn't there are many, many of them even in our galaxy.

2:43:51.440 --> 2:43:58.440
 At the same time, you know, we have thanks to I'm a big supporter of the set the project and its cousins.

2:43:58.440 --> 2:44:01.440
 And I think we should keep doing this and we've learned a lot.

2:44:01.440 --> 2:44:07.440
 We've we've learned that so far, all we have is still unconvincing hints, nothing more.

2:44:07.440 --> 2:44:08.440
 Right.

2:44:08.440 --> 2:44:12.440
 And there are certainly many scenarios where it will be dead obvious.

2:44:12.440 --> 2:44:22.440
 If there were 100 million other human like civilizations in our galaxy, it would not be that hard to notice some of them with today's technology.

2:44:22.440 --> 2:44:23.440
 And we haven't.

2:44:23.440 --> 2:44:24.440
 Right.

2:44:24.440 --> 2:44:32.440
 So so what we can what we can say is, well, OK, we can rule out that there is a human level of civilization on the moon.

2:44:32.440 --> 2:44:41.440
 And in fact, the many nearby solar systems where we we cannot rule out, of course, that there is something like Earth sitting in a galaxy.

2:44:41.440 --> 2:44:45.440
 Five billion light years away.

2:44:45.440 --> 2:44:47.440
 But we've ruled out a lot.

2:44:47.440 --> 2:44:52.440
 And that's already kind of shocking, given that there are all these planets there, you know, so like, where are they?

2:44:52.440 --> 2:44:53.440
 Where are they all?

2:44:53.440 --> 2:44:55.440
 That's the that's the classic Fermi paradox.

2:44:55.440 --> 2:44:56.440
 Yeah.

2:44:56.440 --> 2:45:02.440
 And so my argument, which might very wrong, it's very simple, really just goes like this.

2:45:02.440 --> 2:45:06.440
 OK, we have no clue about this.

2:45:06.440 --> 2:45:18.440
 It could be the probability of getting life on a random planet that could be 10 to the minus one a priori or 10 to the minus 10, 10 to minus 20, 10 to minus 30, 10 to minus 40.

2:45:18.440 --> 2:45:22.440
 Basically, every order of magnitude is about equally likely.

2:45:22.440 --> 2:45:26.440
 When then do the math and ask how close is our nearest neighbor?

2:45:26.440 --> 2:45:32.440
 It's again equally likely that it's 10 to the 10 meters away, 10 to 20 meters away, 10 to the 30 meters away.

2:45:32.440 --> 2:45:38.440
 We can we have some nerdy ways of talking about this with Bayesian statistics and a uniform log prior, but that's irrelevant.

2:45:38.440 --> 2:45:41.440
 This is the simple basic argument.

2:45:41.440 --> 2:45:48.440
 And now comes the data so we can say, OK, how many were there are all these orders of magnitude 10 to the 26 meters away?

2:45:48.440 --> 2:45:51.440
 There's the edge of our observable universe.

2:45:51.440 --> 2:45:54.440
 If it's farther than that light hasn't even reached us yet.

2:45:54.440 --> 2:46:01.440
 If it's less than 10 to the 16 meters away, well, it's within Earth's rate.

2:46:01.440 --> 2:46:03.440
 It's no farther away than the sun.

2:46:03.440 --> 2:46:05.440
 We can definitely rule that out.

2:46:05.440 --> 2:46:08.440
 So I think about it like this.

2:46:08.440 --> 2:46:16.440
 A priori, before we looked with telescopes, it could be 10 meters, 10 to the 20, 10 to the 30, 10 to the 40, 10 to the 50, 10 to the blah blah blah.

2:46:16.440 --> 2:46:18.440
 Equally likely anywhere here.

2:46:18.440 --> 2:46:22.440
 And now we've ruled out this chunk.

2:46:22.440 --> 2:46:25.440
 And most of it is outside.

2:46:25.440 --> 2:46:28.440
 And here is the edge of our observable universe already.

2:46:28.440 --> 2:46:32.440
 So I'm certainly not saying I don't think there's any life elsewhere in space.

2:46:32.440 --> 2:46:36.440
 If space is infinite, then you're basically 100% guaranteed that there is.

2:46:36.440 --> 2:46:48.440
 But the probability that there is life, that the nearest neighbor happens to be in this little region between where we would have seen it already and where we will never see it.

2:46:48.440 --> 2:46:51.440
 There's actually significantly less than one, I think.

2:46:51.440 --> 2:46:55.440
 And I think there's a moral lesson from this, which is really important.

2:46:55.440 --> 2:47:01.440
 Which is to be good stewards of this planet and this shot we've had.

2:47:01.440 --> 2:47:10.440
 It can be very dangerous to say, oh, it's fine if we nuke our planet or ruin the climate or mess it up with unaligned AI.

2:47:10.440 --> 2:47:15.440
 Because I know there is this nice Star Trek fleet out there.

2:47:15.440 --> 2:47:18.440
 They're going to swoop in and take over where we failed.

2:47:18.440 --> 2:47:23.440
 It wasn't the big deal that the Easter Island losers wiped themselves out.

2:47:23.440 --> 2:47:27.440
 It's a dangerous way of loading yourself into false sense of security.

2:47:27.440 --> 2:47:43.440
 If it's actually the case that it might be up to us and only us, the whole future of intelligent life in our observable universe, then I think it really puts a lot of responsibility on our shoulders.

2:47:43.440 --> 2:47:46.440
 It's a little bit terrifying, but it's also inspiring.

2:47:46.440 --> 2:47:48.440
 But it's empowering, I think, most of all.

2:47:48.440 --> 2:47:52.440
 Because the biggest problem today is, I see this even when I teach.

2:47:52.440 --> 2:47:57.440
 So many people feel that it doesn't matter what they do or we do.

2:47:57.440 --> 2:47:58.440
 We feel disempowered.

2:47:58.440 --> 2:48:02.440
 Oh, it makes no difference.

2:48:02.440 --> 2:48:16.440
 This is about as far from that as you can come up and realize that what we do on our little spinning ball here in our lifetime could make the difference for the entire future of life in our universe.

2:48:16.440 --> 2:48:18.440
 How empowering is that?

2:48:18.440 --> 2:48:22.440
 Yeah, survival of consciousness.

2:48:22.440 --> 2:48:32.440
 A very similar kind of empowering aspect of the Drake equation is, say there is a huge number of intelligent civilizations that spring up everywhere.

2:48:32.440 --> 2:48:39.440
 But because of the Drake equation, which is the lifetime of a civilization, maybe many of them hit a wall.

2:48:39.440 --> 2:48:50.440
 And just like you said, it's clear that for us, the great filter, the one possible great filter seems to be coming in the next 100 years.

2:48:50.440 --> 2:48:57.440
 So it's also empowering to say, okay, well, we have a chance to not.

2:48:57.440 --> 2:49:01.440
 I mean, the way great filters work, it just gets most of them.

2:49:01.440 --> 2:49:02.440
 Exactly.

2:49:02.440 --> 2:49:05.440
 Nick Bostrom has articulated this really beautifully too.

2:49:05.440 --> 2:49:13.440
 You know, every time yet another search for life on Mars comes back negative or something, I'm like, yes!

2:49:13.440 --> 2:49:14.440
 Yes!

2:49:14.440 --> 2:49:17.440
 Our odds for us surviving is the best.

2:49:17.440 --> 2:49:20.440
 You already made the argument in broad brush there, right?

2:49:20.440 --> 2:49:22.440
 But just the uncockets, right?

2:49:22.440 --> 2:49:29.440
 The point is, we already know there is a crap ton of planets out there that are Earth like.

2:49:29.440 --> 2:49:34.440
 And we also know that most of them do not seem to have anything like our kind of life on them.

2:49:34.440 --> 2:49:36.440
 So what went wrong?

2:49:36.440 --> 2:49:44.440
 There's clearly one step along the evolutionary, at least one filter roadblock in going from no life to spacefaring life.

2:49:44.440 --> 2:49:47.440
 And where is it?

2:49:47.440 --> 2:49:50.440
 Is it in front of us or is it behind us, right?

2:49:50.440 --> 2:50:01.440
 If there's no filter behind us and we keep finding all sorts of little mice on Mars and whatever, right?

2:50:01.440 --> 2:50:05.440
 That's actually very depressing because that makes it much more likely that the filter is in front of us.

2:50:05.440 --> 2:50:15.440
 And what actually is going on is like the ultimate dark joke that whenever a civilization invents sufficiently powerful tech,

2:50:15.440 --> 2:50:21.440
 it's just set their clock and then after a while it goes poof for one reason or other and wipes itself out.

2:50:21.440 --> 2:50:25.440
 Wouldn't that be like utterly depressing if we're actually doomed?

2:50:25.440 --> 2:50:35.440
 Whereas if it turns out that there is a great filter early on that for whatever reason seems to be really hard to get to the stage of

2:50:35.440 --> 2:50:43.440
 sexually reproducing organisms or even the first ribosome or whatever, right?

2:50:43.440 --> 2:50:50.440
 Or maybe you have lots of planets with dinosaurs and cows, but for some reason they tend to get stuck there and never invent smartphones.

2:50:50.440 --> 2:50:58.440
 All of those are huge boosts for our own odds because being there done that, you know.

2:50:58.440 --> 2:51:05.440
 It doesn't matter how hard or unlikely it was that we got past that roadlock because we already did.

2:51:05.440 --> 2:51:11.440
 And then that makes it likely that the filter is in our own hands. We're not doomed.

2:51:11.440 --> 2:51:21.440
 So that's why I think the fact that life is rare in the universe is not just something that there is some evidence for,

2:51:21.440 --> 2:51:26.440
 but also something we should actually hope for.

2:51:26.440 --> 2:51:36.440
 So that's the end, the mortality, the death of human civilization that we've been discussing in life, maybe prospering beyond any kind of great filter.

2:51:36.440 --> 2:51:45.440
 Do you think about your own death? Does it make you sad that you may not witness some of the...

2:51:45.440 --> 2:51:53.440
 You lead a research group on working some of the biggest questions in the universe, actually, both on the physics and the AI side.

2:51:53.440 --> 2:52:00.440
 Does it make you sad that you may not be able to see some of these exciting things come to fruition that we've been talking about?

2:52:00.440 --> 2:52:07.440
 Of course. Of course it sucks, the fact that I'm going to die. I remember when I was much younger,

2:52:07.440 --> 2:52:11.440
 my dad made this remark that life is fundamentally tragic and I'm like,

2:52:11.440 --> 2:52:13.440
 why are you talking about that again?

2:52:13.440 --> 2:52:17.440
 Many years later, now I feel I totally understand what he means.

2:52:17.440 --> 2:52:25.440
 We grow up, we're little kids and everything is infinite and it's so cool and then suddenly we find out that actually,

2:52:25.440 --> 2:52:35.440
 you're going to get game over at some point. So of course it's something that's sad.

2:52:35.440 --> 2:52:42.440
 Are you afraid?

2:52:42.440 --> 2:52:48.440
 No, not in the sense that I think anything terrible is going to happen after I die or anything like that.

2:52:48.440 --> 2:52:59.440
 I think it's really going to be a game over, but it's more that it makes me very acutely aware of what a wonderful gift this is that it gets to be alive right now

2:52:59.440 --> 2:53:07.440
 and is a steady reminder to just live life to the fullest and really enjoy it because it is finite.

2:53:07.440 --> 2:53:19.440
 We all get regular reminders when someone near and dear to us dies that one day it's going to be our turn.

2:53:19.440 --> 2:53:25.440
 It adds this kind of focus. I wonder what it would feel like actually to be an immortal being

2:53:25.440 --> 2:53:33.440
 if they might even enjoy some of the wonderful things of life a little bit less because there isn't that...

2:53:33.440 --> 2:53:41.440
 finiteness. Do you think that could be a feature, not a bug, the fact that we beings are finite?

2:53:41.440 --> 2:53:48.440
 Maybe there's lessons for engineering and artificial intelligence systems as well that are conscious.

2:53:48.440 --> 2:53:59.440
 Do you think it makes... Is it possible that the reason the pistachio ice cream is delicious is the fact that you're going to die one day

2:53:59.440 --> 2:54:06.440
 and you will not have all the pistachio ice cream that you could eat because of that fact?

2:54:06.440 --> 2:54:10.440
 Well, let me say two things. First of all, it's actually quite profound what you're saying.

2:54:10.440 --> 2:54:17.440
 I do think I appreciate the pistachio ice cream a lot more knowing that there's only a finite number of times I get to enjoy that

2:54:17.440 --> 2:54:21.440
 and I can only remember a finite number of times in the past.

2:54:21.440 --> 2:54:28.440
 Moreover, my life is not so long that it just starts to feel like things are repeating themselves in general.

2:54:28.440 --> 2:54:30.440
 It's so new and fresh.

2:54:30.440 --> 2:54:43.440
 I also think, though, that death is a little bit overrated in the sense that it comes from an outdated view of physics

2:54:43.440 --> 2:54:49.440
 and what life actually is because if you ask, okay, what is it that's going to die exactly?

2:54:49.440 --> 2:54:51.440
 What am I really?

2:54:51.440 --> 2:54:58.440
 When I say I feel sad about the idea of myself dying, am I really sad that the skin cell here is going to die?

2:54:58.440 --> 2:55:03.440
 Of course not because it's going to die next week anyway and I'll grow a new one, right?

2:55:03.440 --> 2:55:10.440
 And it's not any of my cells that I'm associating really with who I really am

2:55:10.440 --> 2:55:14.440
 nor is it any of my atoms or quarks or electrons.

2:55:14.440 --> 2:55:20.440
 In fact, basically all of my atoms get replaced on a regular basis, right?

2:55:20.440 --> 2:55:24.440
 So what is it that's really me from a more modern physics perspective?

2:55:24.440 --> 2:55:28.440
 It's the information in processing Amy.

2:55:28.440 --> 2:55:39.440
 That's where my memories, that's my memories, that's my values, my dreams, my passion, my love.

2:55:39.440 --> 2:55:49.440
 That's what's really fundamentally me and frankly, not all of that will die when my body dies.

2:55:49.440 --> 2:55:54.440
 Like Richard Feynman, for example, his body died of cancer, you know?

2:55:54.440 --> 2:56:00.440
 But many of his ideas that he felt made him very him actually live on.

2:56:00.440 --> 2:56:03.440
 This is my own little personal tribute to Richard Feynman, right?

2:56:03.440 --> 2:56:06.440
 I try to keep a little bit of him alive in myself.

2:56:06.440 --> 2:56:08.440
 I've even quoted him today, right?

2:56:08.440 --> 2:56:12.440
 Yeah, he almost came alive for a brief moment in this conversation.

2:56:12.440 --> 2:56:16.440
 Yeah, and this honestly gives me some solace.

2:56:16.440 --> 2:56:25.440
 You know, when I work as a teacher, I feel if I can actually share a bit about myself,

2:56:25.440 --> 2:56:35.440
 that my students feel worthy enough to copy and adopt a part of things that they know or they believe or aspire to.

2:56:35.440 --> 2:56:39.440
 Now I live on also a little bit in them, right?

2:56:39.440 --> 2:56:48.440
 And so being a teacher is a little bit of what I...

2:56:48.440 --> 2:56:55.440
 That's something also that contributes to making me a little teeny bit less mortal, right?

2:56:55.440 --> 2:56:59.440
 Because I'm not at least not all going to die all at once, right?

2:56:59.440 --> 2:57:02.440
 And I find that a beautiful tribute to people we do not respect.

2:57:02.440 --> 2:57:12.440
 If we can remember them and carry in us the things that we felt was the most awesome about them, right?

2:57:12.440 --> 2:57:14.440
 Then they live on.

2:57:14.440 --> 2:57:19.440
 And I'm getting a bit emotional here, but it's a very beautiful idea you bring up there.

2:57:19.440 --> 2:57:27.440
 I think we should stop this old fashioned materialism and just equate who we are with our quirks and electrons.

2:57:27.440 --> 2:57:30.440
 There's no scientific basis for that, really.

2:57:30.440 --> 2:57:34.440
 And it's also very uninspiring.

2:57:34.440 --> 2:57:40.440
 Now, if you look a little bit towards the future, right?

2:57:40.440 --> 2:57:49.440
 One thing which really sucks about humans dying is that even though some of their teachings and memories and stories and ethics and so on

2:57:49.440 --> 2:57:55.440
 will be copied by those around them, hopefully, a lot of it can't be copied and just dies with them, with a brain.

2:57:55.440 --> 2:58:07.440
 And that really sucks. That's the fundamental reason why we find it so tragic when someone goes from having all this information there to just gone ruined, right?

2:58:07.440 --> 2:58:14.440
 With more post biological intelligence, that's going to shift a lot, right?

2:58:14.440 --> 2:58:21.440
 The only reason it's so hard to make a backup of your brain in its entirety is exactly because it wasn't built for that, right?

2:58:21.440 --> 2:58:36.440
 If you have a future machine intelligence, there's no reason for why it has to die at all if it wants to copy it into some other quirk blob, right?

2:58:36.440 --> 2:58:39.440
 You can copy not just some of it, but all of it, right?

2:58:39.440 --> 2:58:51.440
 And so in that sense, you can get immortality because all the information can be copied out of any individual entity.

2:58:51.440 --> 2:58:56.440
 And it's not just mortality that will change if we get more post biological life.

2:58:56.440 --> 2:59:03.440
 It's also with that very much the whole individualism we have now, right?

2:59:03.440 --> 2:59:10.440
 The reason that we make such a big difference between me and you is exactly because we're a little bit limited in how much we can copy.

2:59:10.440 --> 2:59:16.440
 Like, I would just love to go like this and copy your Russian skills, Russian speaking skills.

2:59:16.440 --> 2:59:23.440
 Wouldn't it be awesome? But I can't. I have to actually work for years to get better on it.

2:59:23.440 --> 2:59:34.440
 But if we were robots, just copy and paste freely, then that loses completely. It washes away the sense of what immortality is.

2:59:34.440 --> 2:59:39.440
 And also individuality a little bit, right? We would start feeling much more…

2:59:39.440 --> 2:59:44.440
 Maybe we would feel much more collaborative with each other if we can just say,

2:59:44.440 --> 2:59:54.440
 hey, you can give me your Russian and I'll give you whatever. And suddenly you can speak Swedish. Maybe that's a bad trade for you, but whatever else you want from my brain, right?

2:59:54.440 --> 3:00:04.440
 And there have been a lot of sci fi stories about hive minds and so on where experiences can be more broadly shared.

3:00:04.440 --> 3:00:16.440
 And I think we don't… I don't pretend to know what it would feel like to be a super intelligent machine,

3:00:16.440 --> 3:00:24.440
 but I'm quite confident that however it feels about mortality and individuality will be very, very different from how it is for us.

3:00:24.440 --> 3:00:34.440
 Well, for us, mortality and finiteness seems to be pretty important at this particular moment.

3:00:34.440 --> 3:00:39.440
 And so all good things must come to an end just like this conversation, Max.

3:00:39.440 --> 3:00:40.440
 I saw that coming.

3:00:40.440 --> 3:00:48.440
 Sorry, this is the world's worst transition. I could talk to you forever. It's such a huge honor that you've spent time with me.

3:00:48.440 --> 3:00:55.440
 My honor is mine. Thank you so much for getting me essentially to start this podcast by doing the first conversation,

3:00:55.440 --> 3:01:00.440
 making me realize falling in love with conversation in itself.

3:01:00.440 --> 3:01:09.440
 And thank you so much for inspiring so many people in the world with your books, with your research, with your talking and with other…

3:01:09.440 --> 3:01:17.440
 like this ripple effect of friends including Elon and everybody else that you inspire. So thank you so much for talking today.

3:01:17.440 --> 3:01:28.440
 Thank you. I feel so fortunate that you're doing this podcast and getting so many interesting voices out there into the ether,

3:01:28.440 --> 3:01:32.440
 and not just the five second sound bites, but so many of the interviews of what you do.

3:01:32.440 --> 3:01:41.440
 You really let people go into depth in a way which we sorely need in this day and age, and that I got to be number one. I feel super honored.

3:01:41.440 --> 3:01:44.440
 Yeah, you started it. Thank you so much, Max.

3:01:44.440 --> 3:01:51.440
 Thanks for listening to this conversation with Max Tegmark, and thank you to our sponsors, The Jordan Harbinger Show,

3:01:51.440 --> 3:01:58.440
 ForSigmatic Mushroom Coffee, BetterHelp Online Therapy, and ExpressVPN.

3:01:58.440 --> 3:02:05.440
 So the choice is Wisdom, Caffeine, Sanity, or Privacy. Choose wisely, my friends.

3:02:05.440 --> 3:02:11.440
 And if you wish, click the sponsor links below to get a discount and to support this podcast.

3:02:11.440 --> 3:02:14.440
 And now let me leave you with some words from Max Tegmark.

3:02:14.440 --> 3:02:23.440
 If consciousness is the way that information feels when it's processed in certain ways, then it must be substrate independent.

3:02:23.440 --> 3:02:31.440
 It's only the structure of information processing that matters, not the structure of the matter doing the information processing.

3:02:31.440 --> 3:02:42.440
 Thank you for listening, and hope to see you next time.

