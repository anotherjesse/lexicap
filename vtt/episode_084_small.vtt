WEBVTT

00:00.000 --> 00:03.600
 The following is a conversation with William McCaskill.

00:03.600 --> 00:06.960
 He's a philosopher, ethicist, and one of the originators

00:06.960 --> 00:09.280
 of the effective altruism movement.

00:09.280 --> 00:11.480
 His research focuses on the fundamentals

00:11.480 --> 00:14.960
 of effective altruism or the use of evidence and reason

00:14.960 --> 00:17.480
 to help others by as much as possible

00:17.480 --> 00:21.160
 with our time and money, with a particular concentration

00:21.160 --> 00:24.360
 on how to act given moral uncertainty.

00:24.360 --> 00:28.600
 He's the author of Doing Good, Better, Effective Altruism,

00:28.600 --> 00:31.200
 and a radical new way to make a difference.

00:31.200 --> 00:33.880
 He is a cofounder and the president

00:33.880 --> 00:37.120
 of the Center of Effective Altruism, CEA,

00:37.120 --> 00:40.680
 that encourages people to commit to donate at least 10%

00:40.680 --> 00:43.960
 of their income to the most effective charities.

00:43.960 --> 00:47.200
 He cofounded 80,000 Hours, which is a nonprofit

00:47.200 --> 00:50.200
 that provides research and advice on how you can best

00:50.200 --> 00:52.640
 make a difference through your career.

00:52.640 --> 00:55.600
 This conversation was recorded before the outbreak

00:55.600 --> 00:57.880
 of the coronavirus pandemic.

00:57.880 --> 01:00.360
 For everyone feeling the medical, psychological,

01:00.360 --> 01:02.400
 and financial burden of this crisis,

01:02.400 --> 01:04.280
 I'm sending love your way.

01:04.280 --> 01:07.720
 Stay strong, we're in this together, we'll beat this thing.

01:09.200 --> 01:12.000
 This is the Artificial Intelligence Podcast.

01:12.000 --> 01:14.120
 If you enjoy it, subscribe on YouTube,

01:14.120 --> 01:16.280
 review it with five stars on Apple Podcast,

01:16.280 --> 01:19.480
 support on Patreon, or simply connect with me on Twitter,

01:19.480 --> 01:23.240
 Alex Friedman, spelled F R I D M A N.

01:23.240 --> 01:25.840
 As usual, I'll do one or two minutes of ads now,

01:25.840 --> 01:27.360
 and never any ads in the middle

01:27.360 --> 01:29.800
 that can break the flow of the conversation.

01:29.800 --> 01:31.200
 I hope that works for you

01:31.200 --> 01:33.480
 and doesn't hurt the listening experience.

01:34.800 --> 01:36.800
 This show was presented by Cash App,

01:36.800 --> 01:39.080
 the number one finance app in the App Store.

01:39.080 --> 01:42.160
 When you get it, use code LEX Podcast.

01:42.160 --> 01:44.160
 Cash App lets you send money to your friends,

01:44.160 --> 01:46.320
 buy Bitcoin, and invest in the stock market

01:46.320 --> 01:48.040
 with as little as $1.

01:48.960 --> 01:50.520
 Since Cash App allows you to send

01:50.520 --> 01:52.880
 and receive money digitally, peer to peer,

01:52.880 --> 01:56.200
 and security in all digital transactions is very important,

01:56.200 --> 01:59.360
 let me mention that PCI data security standard

01:59.360 --> 02:01.440
 that Cash App is compliant with.

02:01.440 --> 02:04.360
 I'm a big fan of standards for safety and security.

02:04.360 --> 02:07.240
 PCI DSS is a good example of that.

02:07.240 --> 02:10.120
 Where a bunch of competitors got together and agreed

02:10.120 --> 02:11.880
 that there needs to be a global standard

02:11.880 --> 02:14.520
 around the security of transactions.

02:14.520 --> 02:17.400
 Now, we just need to do the same for autonomous vehicles

02:17.400 --> 02:19.360
 and AI systems in general.

02:19.360 --> 02:21.800
 So again, if you get Cash App from the App Store,

02:21.800 --> 02:25.040
 Google Play, and use the code LEX Podcast,

02:25.040 --> 02:28.880
 you get $10, and Cash App will also donate $10 to FIRST,

02:28.880 --> 02:31.600
 an organization that is helping to advance robotics

02:31.600 --> 02:34.640
 and STEM education for young people around the world.

02:34.640 --> 02:38.360
 And now, here's my conversation with William McCaskill.

02:39.240 --> 02:42.080
 What does utopia for humans and all life on earth

02:42.080 --> 02:43.480
 look like for you?

02:43.480 --> 02:45.400
 That's a great question.

02:45.400 --> 02:49.320
 What I wanna say is that we don't know,

02:49.320 --> 02:52.280
 and the utopia we want to get to

02:52.280 --> 02:55.520
 is an indirect one that I call the long reflection.

02:55.520 --> 02:57.920
 So a period of post scarcity,

02:57.920 --> 03:01.280
 no longer have the kind of urgent problems we have today,

03:01.280 --> 03:04.240
 but instead can spend perhaps it's tens of thousands

03:04.240 --> 03:08.120
 of years debating, engaging in ethical reflection

03:08.120 --> 03:11.200
 in order before we take any kind of drastic lock in

03:12.160 --> 03:14.560
 actions like spreading to the stars,

03:14.560 --> 03:19.000
 and then we can figure out what is right,

03:19.000 --> 03:20.560
 what is of kind of moral value.

03:20.560 --> 03:25.160
 The long reflection, that's a really beautiful term.

03:25.160 --> 03:28.280
 So if we look at Twitter for just a second,

03:29.680 --> 03:34.400
 do you think human beings are able to reflect

03:34.400 --> 03:35.960
 in a productive way?

03:37.440 --> 03:39.600
 I don't mean to make it sound bad

03:39.600 --> 03:42.720
 because there is a lot of fights and politics

03:42.720 --> 03:45.040
 and division in our discourse.

03:45.040 --> 03:48.960
 Maybe if you zoom out, it actually is civilized discourse.

03:48.960 --> 03:51.800
 It might not feel like it, but when you zoom out.

03:51.800 --> 03:55.200
 So I don't wanna say that Twitter is not civilized discourse.

03:55.200 --> 03:57.000
 I actually believe it's more civilized

03:57.000 --> 03:58.520
 than people give it credit for.

03:58.520 --> 04:03.720
 But do you think the long reflection can actually be stable

04:03.720 --> 04:08.440
 where we as human beings with our descendants of a brains

04:08.440 --> 04:11.360
 would be able to sort of rationally discuss things

04:11.360 --> 04:13.200
 together and arrive at ideas?

04:13.200 --> 04:17.600
 I think overall, we're pretty good

04:17.600 --> 04:19.840
 at discussing things rationally

04:19.840 --> 04:24.840
 and at least in the earliest stages of our lives

04:26.280 --> 04:28.520
 being open to many different ideas

04:28.520 --> 04:33.400
 and being able to be convinced and change our views.

04:33.400 --> 04:36.480
 I think that Twitter is designed almost

04:36.480 --> 04:38.800
 to bring out all of the worst tendencies.

04:38.800 --> 04:42.120
 So if the long reflection were conducted on Twitter,

04:43.320 --> 04:46.280
 maybe it would be better just not even to bother.

04:46.280 --> 04:50.320
 But I think the challenge really is getting to a stage

04:50.320 --> 04:55.320
 where we have a society that is as conducive as possible

04:55.760 --> 04:59.080
 to rational reflection, to deliberation.

04:59.080 --> 05:01.320
 I think we're actually very lucky

05:01.320 --> 05:04.680
 to be in a liberal society where people are able

05:04.680 --> 05:06.960
 to discuss a lot of ideas and so on.

05:06.960 --> 05:08.160
 I think when we look to the future,

05:08.160 --> 05:12.440
 that's not at all guaranteed that society would be like that

05:12.440 --> 05:16.000
 rather than a society where there's a fixed canon

05:16.000 --> 05:20.720
 of values that are being imposed on all of society

05:20.720 --> 05:22.400
 and where you aren't able to question that.

05:22.400 --> 05:24.040
 That would be very bad for my perspective

05:24.040 --> 05:25.840
 because it means we wouldn't be able

05:25.840 --> 05:28.000
 to figure out what the truth is.

05:28.000 --> 05:30.720
 I can already sense we're gonna go down a million

05:30.720 --> 05:35.720
 tangents, but what do you think is the,

05:36.840 --> 05:40.200
 if Twitter's not optimal, what kind of mechanism

05:40.200 --> 05:44.480
 in this modern age of technology can we design

05:44.480 --> 05:48.560
 where the exchange of ideas could be both civilized

05:48.560 --> 05:52.640
 and productive and yet not be too constrained

05:52.640 --> 05:55.360
 where there's rules of what you can say and can't say,

05:55.360 --> 05:57.880
 which is, as you say, is not desirable,

05:57.880 --> 06:00.640
 but yet not have some limits

06:00.640 --> 06:02.800
 of what can be said or not and so on.

06:02.800 --> 06:05.760
 Do you have any ideas, thoughts on the possible future?

06:05.760 --> 06:07.240
 Of course, nobody knows how to do it,

06:07.240 --> 06:08.760
 but do you have thoughts

06:08.760 --> 06:10.880
 of what a better Twitter might look like?

06:10.880 --> 06:14.640
 I think that text based media are intrinsically

06:14.640 --> 06:19.640
 gonna be very hard to be conducive to rational discussion

06:20.000 --> 06:22.000
 because if you think about it

06:22.000 --> 06:24.160
 from an informational perspective,

06:24.160 --> 06:27.320
 if I just send you a text of less than,

06:27.320 --> 06:31.760
 what is it now, 240 characters, 280 characters, I think,

06:31.760 --> 06:33.880
 that's a tiny amount of information

06:33.880 --> 06:36.120
 compared to, say, you and I talking now

06:36.120 --> 06:38.440
 where you have access to the words I say,

06:38.440 --> 06:40.200
 which is the same as in text,

06:40.200 --> 06:42.760
 but also my tone, also my body language

06:43.840 --> 06:47.840
 and we're very poorly designed to be able to assess.

06:47.840 --> 06:49.400
 I have to read all of this context

06:49.400 --> 06:52.920
 into anything you say, so I say,

06:52.920 --> 06:54.520
 maybe your partner sends you a text

06:54.520 --> 06:56.600
 and has a full stop at the end.

06:56.600 --> 06:58.040
 Are they mad at you?

06:58.040 --> 07:00.960
 You don't know, you have to infer everything

07:00.960 --> 07:02.440
 about this person's mental state

07:02.440 --> 07:04.720
 from whether they put a full stop at the end of a text or not.

07:04.720 --> 07:07.840
 Well, the flip side of that is it truly text

07:07.840 --> 07:08.800
 that's the problem here

07:08.800 --> 07:13.800
 because there's a viral aspect to the text

07:14.760 --> 07:17.280
 where it's you could just post text nonstop,

07:17.280 --> 07:18.680
 it's very immediate.

07:19.680 --> 07:23.120
 The times before Twitter, before the internet,

07:23.120 --> 07:27.680
 the way you would exchange text is you would write books.

07:28.520 --> 07:30.880
 And that, while it doesn't get body language,

07:30.880 --> 07:33.720
 it doesn't get tone, it doesn't, so on,

07:33.720 --> 07:36.320
 but it does actually boil down after some time

07:36.320 --> 07:39.440
 thinking some editing, so on, boil down ideas.

07:39.440 --> 07:44.440
 So is the immediacy and the viral nature

07:45.480 --> 07:47.840
 which produces the outrage mobs and so on

07:47.840 --> 07:49.440
 the potential problem?

07:49.440 --> 07:51.120
 I think that is a big issue.

07:51.120 --> 07:53.240
 I think there's gonna be the strong selection effect

07:53.240 --> 07:57.760
 where something that provokes outrage,

07:57.760 --> 07:59.000
 well, that's high arousal,

07:59.000 --> 08:03.200
 you're more likely to retweet that

08:03.200 --> 08:06.000
 where there's kind of sober analysis

08:06.000 --> 08:08.800
 is not as sexy, not as viral.

08:08.800 --> 08:10.640
 I do agree that long form content

08:11.640 --> 08:16.440
 is much better to productive discussion.

08:16.440 --> 08:19.440
 In terms of the media that are very popular at the moment,

08:19.440 --> 08:21.720
 I think that podcasting is great

08:21.720 --> 08:25.400
 where like your podcasts are two hours long,

08:25.400 --> 08:28.960
 so they're much more in depth than Twitter are.

08:28.960 --> 08:33.440
 And you are able to convey so much more nuance,

08:33.440 --> 08:36.800
 so much more caveat because it's an actual conversation.

08:36.800 --> 08:38.880
 It's more like the sort of communication

08:38.880 --> 08:41.640
 that we've evolved to do rather than kind of

08:41.640 --> 08:43.880
 these very small little snippets of ideas

08:43.880 --> 08:46.920
 that when also combined with bad incentives

08:46.920 --> 08:49.800
 just clearly aren't designed for helping us get to the truth.

08:49.800 --> 08:51.600
 It's kind of interesting that it's not just

08:51.600 --> 08:53.760
 the length of the podcast medium,

08:53.760 --> 08:56.960
 but it's the fact that it was started by people

08:56.960 --> 09:00.680
 that don't give a damn about, quote unquote, demand.

09:00.680 --> 09:05.680
 There's a relaxed sort of the style like that Joe Rogan does.

09:08.120 --> 09:12.840
 There's a freedom to express ideas

09:12.840 --> 09:15.360
 in an unconstrained way that's very real.

09:15.360 --> 09:18.760
 It's kind of funny in that it feels

09:18.760 --> 09:22.160
 so refreshingly real to us today.

09:22.160 --> 09:24.960
 And I wonder what the future looks like.

09:24.960 --> 09:27.480
 It's a little bit sad now that quite a lot

09:27.480 --> 09:31.680
 of sort of more popular people are getting into podcasting.

09:31.680 --> 09:36.040
 And they try to sort of create,

09:36.040 --> 09:37.400
 they try to control it,

09:37.400 --> 09:40.280
 they try to constrain it in different kinds of ways.

09:40.280 --> 09:42.160
 People I love like Conan Obron and so on,

09:42.160 --> 09:43.440
 different comedians.

09:43.440 --> 09:48.240
 And I'd love to see where the real aspects

09:48.240 --> 09:50.640
 of this podcasting medium persists,

09:50.640 --> 09:52.560
 maybe in TV, maybe in YouTube,

09:52.560 --> 09:55.640
 maybe Netflix is pushing those kind of ideas.

09:55.640 --> 09:58.440
 And it's kind of, it's a really exciting word,

09:58.440 --> 10:00.280
 that kind of sharing of knowledge.

10:00.280 --> 10:02.200
 Yeah, I mean, I think it's a double edged sword

10:02.200 --> 10:05.320
 as it becomes more popular and more profitable where

10:05.320 --> 10:08.440
 on the one hand you'll get a lot more creativity,

10:08.440 --> 10:10.720
 people doing more interesting things with the medium,

10:10.720 --> 10:12.720
 but also perhaps you get this place to the bottom

10:12.720 --> 10:16.920
 where suddenly maybe it'll be hard to find good content

10:16.920 --> 10:21.080
 on podcasts because it'll be so overwhelmed

10:21.080 --> 10:24.360
 by the latest bit of vital outage.

10:24.360 --> 10:27.280
 So speaking of that, jumping on effective altruism

10:27.280 --> 10:32.280
 for a second, so much of that internet content

10:33.800 --> 10:36.240
 is funded by advertisements.

10:36.240 --> 10:39.000
 Just in the context of effective altruism,

10:39.840 --> 10:44.160
 we're talking about the richest companies in the world,

10:44.160 --> 10:45.800
 they're funded by advertisements essentially,

10:45.800 --> 10:48.840
 Google, that's their primary source of income.

10:48.840 --> 10:53.360
 Do you see that as, do you have any criticism

10:53.360 --> 10:55.200
 of that source of income?

10:55.200 --> 10:57.520
 Do you see that source of money

10:57.520 --> 10:59.480
 as a potentially powerful source of money

10:59.480 --> 11:03.200
 that could be used, well, certainly could be used for good,

11:03.200 --> 11:05.920
 but is there something bad about that source of money?

11:05.920 --> 11:08.080
 I think there's significant worries with it

11:08.080 --> 11:13.080
 where it means that the incentives of the company

11:13.200 --> 11:15.400
 might be quite misaligned with,

11:15.400 --> 11:20.400
 are making people's lives better,

11:20.520 --> 11:25.200
 where again, perhaps the incentives

11:25.200 --> 11:29.000
 are towards increasing drama and debate

11:29.000 --> 11:32.280
 on your social news, social media feed

11:32.280 --> 11:36.320
 in order that more people are going to be engaged,

11:36.320 --> 11:41.320
 perhaps kind of compulsively involved with the platform,

11:41.320 --> 11:44.840
 whereas there are other business models

11:44.840 --> 11:48.360
 like having an opt in subscription service,

11:48.360 --> 11:50.760
 where perhaps they have other issues,

11:50.760 --> 11:54.800
 but there's much more of an incentive

11:54.800 --> 11:58.440
 to provide a product that its users are just

11:58.440 --> 12:02.160
 really wanting, because now I'm paying for this product,

12:02.160 --> 12:04.440
 I'm paying for this thing that I wanna buy,

12:04.440 --> 12:08.440
 rather than I'm trying to use this thing

12:08.440 --> 12:11.560
 and it's gonna get a profit mechanism

12:11.560 --> 12:13.560
 that is somewhat orthogonal to me,

12:13.560 --> 12:17.880
 actually just wanting to use the product.

12:19.000 --> 12:21.840
 And so, I mean, in some cases,

12:21.840 --> 12:23.000
 it'll work better than others.

12:23.000 --> 12:27.040
 I can imagine, I can in theory imagine Facebook

12:27.040 --> 12:28.800
 having a subscription service,

12:28.800 --> 12:32.280
 but I think it's unlikely to happen anytime soon.

12:32.280 --> 12:34.240
 Well, it's interesting, it's weird,

12:34.240 --> 12:36.240
 now that you bring it up that it's unlikely.

12:36.240 --> 12:38.680
 This example, I pay, I think 10 bucks a month

12:38.680 --> 12:43.280
 for YouTube Red, and that's,

12:43.280 --> 12:45.320
 and I don't think I get it much for that,

12:45.320 --> 12:50.200
 except just, so no ads,

12:50.200 --> 12:52.880
 but in general, it's just a slightly better experience.

12:52.880 --> 12:56.480
 And I would gladly, now I'm not wealthy in fact,

12:56.480 --> 12:59.160
 I'm operating very close to zero dollars,

12:59.160 --> 13:01.840
 but I would pay 10 bucks a month to Facebook

13:01.840 --> 13:03.920
 and 10 bucks a month to Twitter

13:03.920 --> 13:07.480
 for some kind of more control

13:07.480 --> 13:09.120
 in terms of advertisements and so on.

13:09.120 --> 13:13.720
 But the other aspect of that is data, personal data.

13:13.720 --> 13:16.240
 People are really sensitive about this.

13:16.240 --> 13:21.240
 And I as one who hopes to one day create a company

13:21.600 --> 13:26.600
 that may use people's data to do good for the world,

13:27.520 --> 13:28.960
 wonder about this,

13:28.960 --> 13:32.400
 won the psychology of why people are so paranoid.

13:32.400 --> 13:35.240
 Well, I understand why, but they seem to be more paranoid

13:35.240 --> 13:37.720
 than is justified at times.

13:37.720 --> 13:39.480
 And the other is how do you do it right?

13:39.480 --> 13:43.520
 So it seems that Facebook is,

13:43.520 --> 13:46.260
 it seems that Facebook is doing it wrong.

13:47.400 --> 13:49.560
 That's certainly the popular narrative.

13:49.560 --> 13:52.040
 It's unclear to me actually how wrong,

13:53.040 --> 13:55.440
 like I tend to give them more benefit of the doubt

13:55.440 --> 13:57.360
 because they're, you know,

13:57.360 --> 14:00.040
 it's a really hard thing to do right.

14:00.040 --> 14:01.400
 And people don't necessarily realize it,

14:01.400 --> 14:06.000
 but how do we respect in your view people's privacy?

14:06.000 --> 14:06.840
 Yeah.

14:06.840 --> 14:10.800
 I mean, in the case of how worried are people

14:10.800 --> 14:12.400
 about using their data?

14:12.400 --> 14:15.280
 I mean, there's a lot of public debate

14:15.280 --> 14:16.640
 and criticism about it.

14:18.680 --> 14:21.720
 When we look at people's revealed preferences,

14:21.720 --> 14:24.320
 you know, people's continuing massive use

14:24.320 --> 14:26.480
 of these sorts of services,

14:27.680 --> 14:30.560
 it's not clear to me how much people really do care.

14:30.560 --> 14:31.520
 Perhaps they care a bit,

14:31.520 --> 14:35.560
 but they're happy to in effect kind of sell their data

14:35.560 --> 14:37.600
 in order to be able to use a certain service.

14:37.600 --> 14:39.360
 That's a great term, revealed preferences.

14:39.360 --> 14:40.560
 So these aren't preferences,

14:40.560 --> 14:42.600
 you're self report in the survey,

14:42.600 --> 14:44.600
 this is like your actions speak.

14:44.600 --> 14:45.440
 Yeah, exactly.

14:45.440 --> 14:48.000
 So you might say, oh yeah, I hate the idea

14:48.000 --> 14:51.040
 of Facebook having my data,

14:51.040 --> 14:52.800
 but then when it comes to it,

14:52.800 --> 14:55.160
 you actually are willing to give that data

14:55.160 --> 14:59.000
 in exchange for being able to use the service.

14:59.000 --> 15:01.640
 And if that's the case,

15:01.640 --> 15:05.400
 then I think unless we have some explanation

15:05.400 --> 15:10.400
 about why there's some negative externality from that

15:11.120 --> 15:13.520
 or why there's some coordination failure,

15:15.920 --> 15:18.080
 or if there's something that consumers

15:18.080 --> 15:19.760
 are just really misled about

15:19.760 --> 15:22.680
 where they don't realize why giving away data

15:22.680 --> 15:25.360
 like this is a really bad thing to do,

15:25.360 --> 15:30.360
 then ultimately I kind of want to respect

15:31.520 --> 15:32.360
 people's preferences,

15:32.360 --> 15:34.480
 they can give away their data if they want.

15:35.520 --> 15:36.520
 I think there's a big difference

15:36.520 --> 15:41.520
 between companies use of data and governments having data

15:41.960 --> 15:45.840
 where looking at the record of history,

15:45.840 --> 15:48.880
 governments knowing a lot about their people

15:50.400 --> 15:54.160
 can be very bad if the government chooses to do

15:54.160 --> 15:55.000
 bad things with it.

15:55.000 --> 15:57.120
 And that's more worrying, I think.

15:57.120 --> 15:58.880
 So let's jump into it a little bit.

15:59.720 --> 16:03.920
 Most people know, but actually I two years ago

16:03.920 --> 16:07.040
 had no idea what effective altruism was

16:07.040 --> 16:09.120
 until I saw there was a cool looking event

16:09.120 --> 16:11.280
 in an MIT group here.

16:11.280 --> 16:15.960
 They, I think it's called the effective altruism club

16:15.960 --> 16:17.960
 or a group.

16:17.960 --> 16:19.880
 I was like, what the heck is that?

16:19.880 --> 16:21.440
 Yeah.

16:21.440 --> 16:23.240
 And one of my friends said,

16:23.240 --> 16:27.200
 I mean, he said that they're just

16:27.200 --> 16:30.000
 a bunch of eccentric characters.

16:30.000 --> 16:31.600
 So I was like, hell yes, I'm in.

16:31.600 --> 16:32.800
 So I went to one of their events

16:32.800 --> 16:34.360
 and looked up what's it about.

16:34.360 --> 16:37.040
 This is quite a fascinating philosophical

16:37.040 --> 16:38.880
 and just a movement of ideas.

16:38.880 --> 16:42.600
 So can you tell me what is effective altruism?

16:42.600 --> 16:43.440
 Great.

16:43.440 --> 16:44.800
 So the core of effective altruism

16:44.800 --> 16:46.480
 is about trying to answer this question,

16:46.480 --> 16:49.360
 which is how can I do as much good as possible

16:49.360 --> 16:53.200
 with my scarce resources, my time and with my money?

16:53.200 --> 16:57.120
 And then once we have our best guess answers to that,

16:57.120 --> 17:00.120
 trying to take those ideas and put that into practice

17:00.120 --> 17:03.000
 and do those things that we believe will do the most good.

17:03.000 --> 17:04.960
 And we're now a community of people,

17:06.040 --> 17:08.040
 many thousands of us around the world

17:08.040 --> 17:11.480
 who really are trying to answer that question as best we can

17:11.480 --> 17:15.200
 and then use our time and money to make the world better.

17:15.200 --> 17:17.200
 So what's the difference between

17:17.200 --> 17:22.200
 sort of classical general idea of altruism

17:22.200 --> 17:24.600
 and effective altruism?

17:24.600 --> 17:28.240
 So normally when people decide to do good,

17:28.240 --> 17:33.240
 they often just aren't so reflective about those attempts.

17:34.040 --> 17:36.200
 So someone might approach you on the street

17:36.200 --> 17:38.480
 asking you to give to charity.

17:38.480 --> 17:42.080
 And if you're feeling altruistic,

17:42.080 --> 17:44.360
 you'll give to the person on the street.

17:44.360 --> 17:47.960
 Or if you think, oh, I wanna do some good in my life,

17:47.960 --> 17:49.920
 you might volunteer at a local place

17:49.920 --> 17:52.800
 or perhaps you'll decide pursue a career

17:52.800 --> 17:56.400
 where you're working in a field

17:56.400 --> 17:58.120
 that's kind of more obviously beneficial

17:58.120 --> 18:02.160
 like being a doctor or a nurse or a healthcare professional.

18:03.880 --> 18:07.840
 But it's very rare that people apply the same level

18:07.840 --> 18:11.760
 of rigor and analytical thinking

18:11.760 --> 18:14.360
 to lots of other areas we think about.

18:14.360 --> 18:16.400
 So take the case of someone approaching you on the street.

18:16.400 --> 18:18.680
 Imagine if that person instead was saying,

18:18.680 --> 18:20.160
 hey, I've got this amazing company,

18:20.160 --> 18:22.320
 do you want to invest in it?

18:22.320 --> 18:24.840
 It would be insane for, no one would ever think,

18:24.840 --> 18:26.360
 oh, of course, I'm just a company,

18:26.360 --> 18:27.840
 like you'd think it was a scam.

18:29.160 --> 18:31.280
 But somehow we don't have that same level of rigor

18:31.280 --> 18:32.320
 when it comes to doing good,

18:32.320 --> 18:34.560
 even though the stakes are more important

18:34.560 --> 18:36.000
 when it comes to trying to help others

18:36.000 --> 18:38.800
 than trying to make money for ourselves.

18:38.800 --> 18:40.600
 First of all, so there is a psychology

18:40.600 --> 18:44.800
 at the individual level of doing good just feels good.

18:44.800 --> 18:49.800
 And so in some sense, on that pure psychological part,

18:51.720 --> 18:52.960
 it doesn't matter.

18:52.960 --> 18:56.480
 In fact, you don't want to know if it does good or not

18:56.480 --> 19:00.080
 because most of the time it won't.

19:01.640 --> 19:04.920
 So like in a certain sense,

19:04.920 --> 19:06.960
 it's understandable why altruism

19:06.960 --> 19:09.920
 without the effective part is so appealing

19:09.920 --> 19:11.400
 to a certain population.

19:11.400 --> 19:15.440
 By the way, let's zoom out for a second.

19:15.440 --> 19:18.840
 Do you think most people, two questions,

19:18.840 --> 19:21.040
 do you think most people are good?

19:21.040 --> 19:22.360
 Question number two is,

19:22.360 --> 19:25.000
 do you think most people want to do good?

19:25.000 --> 19:26.720
 So are most people good?

19:26.720 --> 19:28.080
 I think it's just super dependent

19:28.080 --> 19:31.760
 on the circumstances that someone is in.

19:31.760 --> 19:34.880
 I think that the actions people take

19:34.880 --> 19:37.760
 and their moral worth is just much more dependent

19:37.760 --> 19:40.800
 on circumstance than it is on someone's

19:40.800 --> 19:41.960
 intrinsic character.

19:41.960 --> 19:43.840
 So is it evil within all of us?

19:43.840 --> 19:47.920
 It seems like the better angels of our nature,

19:47.920 --> 19:50.400
 there's a tendency of us as a society

19:50.400 --> 19:53.280
 to tend towards good, less war,

19:53.280 --> 19:55.600
 I mean with all these metrics.

19:55.600 --> 20:00.080
 What is that us becoming who we want to be?

20:00.080 --> 20:03.240
 Or is that some kind of societal force?

20:03.240 --> 20:05.240
 What's the nature versus nurture thing here?

20:05.240 --> 20:07.280
 Yeah, so in that case, I just think, yeah,

20:07.280 --> 20:10.560
 so violence has massively declined over time.

20:10.560 --> 20:14.160
 I think that's a slow process of cultural evolution,

20:14.160 --> 20:15.440
 institutional evolution,

20:15.440 --> 20:18.760
 such that now the incentives for you and I

20:18.760 --> 20:21.720
 to be violent are very, very small indeed.

20:21.720 --> 20:23.680
 In contrast, when we were hunter gatherers,

20:23.680 --> 20:25.840
 the incentives were quite large.

20:25.840 --> 20:30.840
 If there was someone who was potentially disturbing

20:31.960 --> 20:35.320
 the social order and hunter gatherer setting,

20:35.320 --> 20:37.840
 there was a very strong incentive to kill that person

20:37.840 --> 20:38.680
 and people did.

20:38.680 --> 20:41.440
 And it was just regarded 10% of deaths

20:41.440 --> 20:44.840
 among hunter gatherers were murders.

20:44.840 --> 20:48.720
 After hunter gatherers, when you have actual societies

20:48.720 --> 20:51.360
 is when violence can probably go up

20:51.360 --> 20:54.320
 because there's more incentive to do mass violence, right?

20:54.320 --> 20:58.840
 To take over, conquer other people's lands

20:58.840 --> 21:01.240
 and murder everybody in place and so on.

21:01.240 --> 21:03.840
 Yeah, I mean, I think total death rate

21:03.840 --> 21:07.000
 from human causes does go down,

21:07.000 --> 21:10.480
 but you're like that if you're in a hunter gatherer situation,

21:10.480 --> 21:15.040
 you're kind of a group that you're part of is very small,

21:15.040 --> 21:17.360
 then you can't have massive wars

21:17.360 --> 21:19.640
 that just massive communities don't exist.

21:19.640 --> 21:21.360
 But anyway, the second question,

21:21.360 --> 21:23.400
 do you think most people want to do good?

21:23.400 --> 21:26.160
 Yeah, and then I think that is true for most people.

21:26.160 --> 21:28.720
 I think you see that with the fact that,

21:29.960 --> 21:33.840
 most people donate a large proportion of people volunteer.

21:33.840 --> 21:35.560
 If you give people opportunities

21:35.560 --> 21:38.760
 to easily help other people, they will take it.

21:38.760 --> 21:43.760
 But at the same time where a product of our circumstances

21:43.800 --> 21:47.440
 and if it were more socially rewarded to be doing more good,

21:47.440 --> 21:49.640
 if it were more socially rewarded to do good effectively,

21:49.640 --> 21:51.360
 rather than not effectively,

21:51.360 --> 21:53.720
 then we would see that behavior a lot more.

21:55.120 --> 21:58.760
 So why should we do good?

21:58.760 --> 22:01.440
 Yeah, my answer to this is,

22:01.440 --> 22:04.120
 there's no kind of deeper level of explanation.

22:04.120 --> 22:08.560
 So my answer to kind of why should you do good is,

22:08.560 --> 22:11.320
 well, there is someone whose life is on the line,

22:11.320 --> 22:13.720
 for example, whose life you can save

22:15.000 --> 22:17.960
 via donating just actually a few thousand dollars

22:17.960 --> 22:20.160
 to an effective nonprofit,

22:20.160 --> 22:21.960
 like the Against Malaria Foundation.

22:21.960 --> 22:24.120
 That is a sufficient reason to do good.

22:24.120 --> 22:27.240
 And then if you ask, well, why ought I to do that?

22:27.240 --> 22:29.920
 I'm like, I just show you the same facts again.

22:29.920 --> 22:32.240
 It's that fact that is the reason to do good.

22:32.240 --> 22:34.840
 There's nothing more fundamental than that.

22:34.840 --> 22:38.360
 I'd like to sort of make more concrete

22:38.360 --> 22:41.200
 the thing we're trying to make better.

22:41.200 --> 22:43.240
 So you just mentioned malaria.

22:43.240 --> 22:45.800
 So there's a huge amount of suffering in the world.

22:46.840 --> 22:50.200
 Are we trying to remove,

22:50.200 --> 22:53.640
 so ultimately the goal, not ultimately,

22:53.640 --> 22:58.640
 but the first step is to remove the worst of the suffering.

22:59.200 --> 23:01.760
 So there's some kind of threshold of suffering

23:01.760 --> 23:04.600
 that we want to make sure does not exist in the world.

23:06.640 --> 23:11.280
 Or do we really naturally want to take a much further step

23:11.280 --> 23:13.880
 and look at things like income inequality.

23:14.840 --> 23:17.200
 So not just getting everybody above a certain threshold,

23:17.200 --> 23:19.440
 but making sure that there's some,

23:21.680 --> 23:23.800
 that broadly speaking,

23:23.800 --> 23:27.560
 there's less injustice in the world, unfairness.

23:27.560 --> 23:29.320
 In some definition, of course,

23:29.320 --> 23:31.360
 very difficult to define a fairness.

23:31.360 --> 23:32.200
 Yeah.

23:32.200 --> 23:35.680
 So the metric I use is how many people do we affect

23:35.680 --> 23:37.480
 and by how much do we affect them?

23:37.480 --> 23:42.480
 And so that can, often that means eliminating suffering,

23:43.360 --> 23:44.360
 but it doesn't have to,

23:44.360 --> 23:47.960
 could be helping promote a flourishing life instead.

23:47.960 --> 23:52.960
 And so if I was comparing reducing income inequality

23:53.160 --> 23:58.160
 or getting people from the very pits of suffering

23:58.480 --> 23:59.880
 to a higher level,

23:59.880 --> 24:03.240
 the question I would ask is just a quantitative one

24:03.240 --> 24:06.320
 of just if I do this first thing or the second thing,

24:06.320 --> 24:08.200
 how many people am I going to benefit

24:08.200 --> 24:10.120
 and by how much am I going to benefit?

24:10.120 --> 24:13.480
 Am I going to move that one person from kind of 10%,

24:13.480 --> 24:17.320
 0% well being to 10% well being?

24:17.320 --> 24:20.360
 Perhaps that's just not as good as moving 100 people

24:20.360 --> 24:22.880
 from 10% well being to 50% well being.

24:22.880 --> 24:25.960
 And the idea is the diminishing returns

24:25.960 --> 24:30.960
 is the idea of when you're in terrible poverty,

24:33.000 --> 24:38.000
 then the $1 that you give goes much further

24:38.400 --> 24:40.160
 than if you were in the middle class

24:40.160 --> 24:41.840
 in the United States, for example.

24:41.840 --> 24:42.680
 Absolutely.

24:42.680 --> 24:44.640
 And this fact is really striking.

24:44.640 --> 24:49.640
 So if you take even just quite a conservative estimate

24:49.640 --> 24:54.640
 of how we are able to turn money into well being,

24:55.720 --> 24:59.160
 the economists put it as like a log curve.

24:59.160 --> 25:00.880
 That's all steeper,

25:00.880 --> 25:05.880
 but that means that any proportional increase in your income

25:05.920 --> 25:08.200
 has the same impact on your well being.

25:08.200 --> 25:12.200
 And so someone moving from $1,000 a year to $2,000 a year

25:12.200 --> 25:17.200
 has the same impact to someone moving from $100,000 a year

25:17.200 --> 25:20.680
 to $200,000 a year.

25:20.680 --> 25:22.320
 And then when you combine that with the fact

25:22.320 --> 25:26.320
 that we in middle class members of rich countries

25:27.240 --> 25:29.760
 are 100 times richer than financial terms

25:29.760 --> 25:31.160
 in the global poor,

25:31.160 --> 25:33.080
 that means we can do 100 times to benefit

25:33.080 --> 25:34.520
 the poorest people in the world

25:34.520 --> 25:37.600
 as we can to benefit people of our income level.

25:37.600 --> 25:39.400
 And that's this astonishing fact.

25:39.400 --> 25:41.120
 Yeah, it's quite incredible.

25:41.120 --> 25:43.760
 A lot of these facts and ideas are just

25:43.760 --> 25:47.640
 difficult to think about

25:47.640 --> 25:52.640
 because there's an overwhelming amount of suffering

25:53.800 --> 25:58.800
 in the world and even acknowledging it is difficult.

26:00.680 --> 26:02.320
 I'm not exactly sure why that is.

26:02.320 --> 26:05.320
 I mean, I mean, it's difficult

26:05.320 --> 26:07.680
 because you have to bring to mind,

26:07.680 --> 26:09.640
 you know, it's an unpleasant experience

26:09.640 --> 26:11.720
 thinking about other people suffering.

26:11.720 --> 26:14.760
 It's unpleasant to be empathizing with it, firstly.

26:14.760 --> 26:16.360
 And then secondly, thinking about it

26:16.360 --> 26:19.040
 means that maybe we'd have to change our lifestyles.

26:19.040 --> 26:22.920
 And if you're very attached to the income that you've got,

26:22.920 --> 26:25.920
 perhaps you don't want to be confronting ideas

26:25.920 --> 26:28.560
 or arguments that might cause you

26:28.560 --> 26:31.480
 to use some of that money to help others.

26:31.480 --> 26:34.720
 So it's quite understandable in the psychological terms,

26:34.720 --> 26:38.200
 even if it's not the right thing that we ought to be doing.

26:38.200 --> 26:40.160
 So how can we do better?

26:40.160 --> 26:42.480
 How can we be more effective?

26:42.480 --> 26:44.760
 How does data help?

26:44.760 --> 26:47.560
 In general, how can we do better?

26:47.560 --> 26:48.840
 It's definitely hard.

26:48.840 --> 26:52.240
 And we have spent the last 10 years engaged

26:52.240 --> 26:54.800
 in kind of some deep research projects

26:54.800 --> 26:59.480
 to try and answer kind of two questions.

26:59.480 --> 27:02.560
 One is of all the many problems the world is facing,

27:02.560 --> 27:04.720
 what are the problems we ought to be focused on?

27:04.720 --> 27:06.840
 And then within those problems that we judge

27:06.840 --> 27:08.600
 to be kind of the most pressing

27:08.600 --> 27:11.280
 where we use this idea of focusing on problems

27:11.280 --> 27:15.640
 that are the biggest in scale, that are the most tractable,

27:15.640 --> 27:20.240
 where we can do have the kind of make the most progress

27:20.240 --> 27:23.800
 on that problem, and that are the most neglected.

27:23.800 --> 27:26.480
 Within them, what are the things that

27:26.480 --> 27:29.120
 have the kind of best evidence, or we

27:29.120 --> 27:32.040
 have the best guess that will do the most good?

27:32.040 --> 27:34.480
 And so we have a bunch of organizations.

27:34.480 --> 27:37.720
 So GiveWell, for example, is focused

27:37.720 --> 27:39.320
 on global health and development,

27:39.320 --> 27:42.360
 and has a list of seven top recommended charities.

27:42.360 --> 27:44.640
 So the idea in general, and sorry to interrupt,

27:44.640 --> 27:47.680
 is so we'll talk about sort of poverty and animal welfare

27:47.680 --> 27:48.640
 and existential risk.

27:48.640 --> 27:49.920
 There's all fascinating topics.

27:49.920 --> 27:56.320
 But in general, the idea is there should be a group.

27:56.320 --> 27:59.160
 Sorry, there's a lot of groups that

27:59.160 --> 28:04.240
 seek to convert money into good.

28:04.240 --> 28:11.640
 And then you also, on top of that, want to have a counting

28:11.640 --> 28:16.000
 of how good they actually perform that conversion,

28:16.000 --> 28:18.440
 how well they did in converting money to good.

28:18.440 --> 28:20.480
 So ranking of these different groups,

28:20.480 --> 28:24.080
 ranking these charities.

28:24.080 --> 28:28.520
 So does that apply across basically all aspects

28:28.520 --> 28:29.680
 of effective altruism?

28:29.680 --> 28:31.840
 So there should be a group of people,

28:31.840 --> 28:34.560
 and they should report on certain metrics

28:34.560 --> 28:35.760
 of how well they've done.

28:35.760 --> 28:38.440
 And you should only give your money to groups

28:38.440 --> 28:40.000
 that do a good job.

28:40.000 --> 28:42.360
 That's the core idea.

28:42.360 --> 28:43.600
 I'd make two comments.

28:43.600 --> 28:45.400
 One is just it's not just about money.

28:45.400 --> 28:48.400
 So we're also trying to encourage people

28:48.400 --> 28:51.400
 to work in areas where they'll have the biggest impact.

28:51.400 --> 28:52.040
 Absolutely.

28:52.040 --> 28:56.480
 And in some areas, they're really people heavy, but money poor.

28:56.480 --> 28:59.800
 Other areas are kind of money rich and people poor.

28:59.800 --> 29:02.960
 And so whether it's better to focus time or money

29:02.960 --> 29:05.360
 depends on the cause area.

29:05.360 --> 29:08.400
 And then the second is that you mentioned metrics.

29:08.400 --> 29:11.440
 And while that's the ideal, and in some areas,

29:11.440 --> 29:15.200
 we are able to get somewhat quantitative information

29:15.200 --> 29:19.040
 about how much impact an area is having,

29:19.040 --> 29:21.800
 that's not always true for some of the issues,

29:21.800 --> 29:23.920
 like you mentioned, existential risks.

29:23.920 --> 29:30.560
 Well, we're not able to measure in any sort of precise way

29:30.560 --> 29:32.520
 like how much progress we're making.

29:32.520 --> 29:35.120
 And so you have to instead fall back

29:35.120 --> 29:38.640
 on just a regular argument and evaluation,

29:38.640 --> 29:41.160
 even in the absence of data.

29:41.160 --> 29:47.520
 So let's first sort of linger on your own story for a second.

29:47.520 --> 29:50.360
 How do you yourself practice effective altruism

29:50.360 --> 29:51.200
 in your own life?

29:51.200 --> 29:54.720
 Because I think that's a really interesting place to start.

29:54.720 --> 29:56.960
 So I've tried to build effective altruism

29:56.960 --> 30:00.240
 into at least many components of my life.

30:00.240 --> 30:03.640
 So on the donation side, my plan is

30:03.640 --> 30:07.560
 to give away most of my income over the course of my life.

30:07.560 --> 30:09.440
 I've set a bar I feel happy with,

30:09.440 --> 30:12.480
 and I just donate above that bar.

30:12.480 --> 30:14.920
 So at the moment, I donate about 20% of my income.

30:17.960 --> 30:20.280
 Then on the career side, I've also

30:20.280 --> 30:24.320
 shifted kind of what I do, where I was initially

30:24.320 --> 30:28.520
 planning to work on very esoteric topics

30:28.520 --> 30:30.880
 in the philosophy of logic, philosophy of language,

30:30.880 --> 30:33.040
 things that are intellectually extremely interesting,

30:33.040 --> 30:35.280
 but the path by which they really

30:35.280 --> 30:38.160
 make a difference to the world is, let's just say,

30:38.160 --> 30:40.600
 it's very unclear at best.

30:40.600 --> 30:43.400
 And so I switched instead to the searching ethics

30:43.400 --> 30:46.480
 to actually just working on this question of how we can do

30:46.480 --> 30:48.520
 as much good as possible.

30:48.520 --> 30:51.680
 And then I've also spent a very large chunk of my life

30:51.680 --> 30:55.320
 over the last 10 years creating a number of nonprofits

30:55.320 --> 30:58.040
 who, again, in different ways, are tackling

30:58.040 --> 31:00.160
 this question of how we can do the most good

31:00.160 --> 31:02.120
 and helping them to grow over time too.

31:02.120 --> 31:05.440
 Yeah, we'll mention a few of them with the career selection,

31:05.440 --> 31:07.640
 80,000 hours.

31:07.640 --> 31:11.200
 80,000 hours is a really interesting group.

31:11.200 --> 31:16.800
 So maybe also just a quick pause on the origins

31:16.800 --> 31:19.480
 of effective altruism, because you painted a picture

31:19.480 --> 31:23.120
 who the key figures are, including yourself,

31:23.120 --> 31:26.920
 in the effective altruism movement today.

31:26.920 --> 31:30.360
 Yeah, there are two main strands that

31:30.360 --> 31:34.920
 kind of came together to form the effective altruism movement.

31:34.920 --> 31:40.480
 So one was two philosophers, myself and Toby Ord at Oxford.

31:40.480 --> 31:44.000
 And we had been very influenced by the work of Peter Singer,

31:44.000 --> 31:45.920
 an Australian model philosopher, who

31:45.920 --> 31:50.240
 had argued for many decades that because one can do so much good

31:50.240 --> 31:52.960
 at such a little cost to oneself,

31:52.960 --> 31:55.640
 we have an obligation to give away most of our income,

31:55.640 --> 31:58.280
 to benefit those who are actually in poverty,

31:58.280 --> 32:00.960
 just in the same way that we have an obligation

32:00.960 --> 32:04.800
 to run in and save a child from a drowning in a shallow pond

32:04.800 --> 32:06.560
 if it were just to ruin your suit that

32:06.560 --> 32:10.400
 cost a few thousand dollars.

32:10.400 --> 32:13.200
 And we set up Giving What We Can in 2009,

32:13.200 --> 32:16.040
 which is encouraging people to give at least 10% of their income

32:16.040 --> 32:18.200
 to the most effective charities.

32:18.200 --> 32:21.400
 And the second main strand was the formation of Give Well,

32:21.400 --> 32:26.360
 which was originally based in New York and started in about 2007.

32:26.360 --> 32:30.280
 And that was set up by Holden Karnosi and Ellie Hassenfeld,

32:30.280 --> 32:36.280
 who were two hedge fund dudes who were making good money

32:36.280 --> 32:38.440
 and thinking, well, where should I donate?

32:38.440 --> 32:40.640
 And in the same way as if they wanted

32:40.640 --> 32:42.200
 to buy a product for themselves, they

32:42.200 --> 32:44.160
 would look at Amazon reviews.

32:44.160 --> 32:46.600
 They were like, well, what are the best charities?

32:46.600 --> 32:49.280
 Found they just weren't really good answers to that question,

32:49.280 --> 32:51.240
 certainly not that they were satisfied with.

32:51.240 --> 32:52.800
 And so they formed Give Well in order

32:52.800 --> 32:57.520
 to try and work out what are those charities where they can

32:57.520 --> 32:59.040
 have the biggest impact.

32:59.040 --> 33:02.280
 And then from there and some other influences,

33:02.280 --> 33:05.200
 kind of community glue and spread.

33:05.200 --> 33:08.640
 Can we explore the philosophical and political space

33:08.640 --> 33:11.440
 that effective altruism occupies a little bit?

33:11.440 --> 33:16.600
 So from the little and distant in my own lifetime

33:16.600 --> 33:18.640
 that I've read of Ayn Rand's work,

33:18.640 --> 33:22.080
 Ayn Rand's philosophy of Objectivism, espouses.

33:22.080 --> 33:26.760
 And it's interesting to put her philosophy in contrast

33:26.760 --> 33:28.040
 with effective altruism.

33:28.040 --> 33:32.760
 So it espouses selfishness as the best thing you can do.

33:32.760 --> 33:37.600
 And it's not actually against altruism.

33:37.600 --> 33:40.480
 It's just you have that choice, but you

33:40.480 --> 33:43.680
 should be selfish in it, or not.

33:43.680 --> 33:44.760
 Maybe you can disagree here.

33:44.760 --> 33:48.280
 But so it can be viewed as the complete opposite

33:48.280 --> 33:51.760
 of effective altruism, or it can be viewed as similar

33:51.760 --> 33:55.520
 because the word effective is really interesting.

33:55.520 --> 34:00.600
 Because if you want to do good, then you should be damn good

34:00.600 --> 34:03.520
 at doing good.

34:03.520 --> 34:06.960
 I think that would fit within the morality that's

34:06.960 --> 34:08.640
 defined by Objectivism.

34:08.640 --> 34:11.120
 So do you see a connection between these two philosophies

34:11.120 --> 34:16.400
 and other, perhaps, other in this complicated space

34:16.400 --> 34:22.840
 of beliefs that effective altruism is positioned as opposing

34:22.840 --> 34:24.800
 or aligned with?

34:24.800 --> 34:27.160
 I would definitely say that Objectivism Ayn Rand's

34:27.160 --> 34:31.080
 philosophy is a philosophy that's quite fundamentally

34:31.080 --> 34:37.040
 opposed to effective altruism in so far as Ayn Rand's philosophy

34:37.040 --> 34:39.200
 is about championing egoism and saying

34:39.200 --> 34:41.600
 that I'm never quite sure whether the philosophy is

34:41.600 --> 34:46.360
 meant to say that just you ought to do whatever will best

34:46.360 --> 34:48.680
 benefit yourself as ethical egoism,

34:48.680 --> 34:50.760
 no matter what the consequences are.

34:50.760 --> 34:54.960
 Or second, if there's this alternative view, which is,

34:54.960 --> 34:57.560
 well, you ought to try and benefit yourself

34:57.560 --> 35:02.960
 because that's actually the best way of benefiting society.

35:02.960 --> 35:07.560
 Certainly, Atlas Shilaguchi is presenting her philosophy

35:07.560 --> 35:09.800
 as a way that's actually going to bring

35:09.800 --> 35:12.080
 about a flourishing society.

35:12.080 --> 35:15.200
 And if it's the former, then well, effective altruism

35:15.200 --> 35:17.120
 is all about promoting the idea of altruism.

35:17.120 --> 35:21.000
 So it's saying, in fact, we ought to really be trying to help

35:21.000 --> 35:23.920
 others as much as possible so it's opposed there.

35:23.920 --> 35:27.800
 And then on the second side, I would just dispute

35:27.800 --> 35:28.720
 the empirical premise.

35:28.720 --> 35:31.480
 It would seem, given the major problems in the world today,

35:31.480 --> 35:34.160
 it would seem like this remarkable coincidence,

35:34.160 --> 35:37.440
 quite suspicious, one might say, if benefiting myself

35:37.440 --> 35:41.040
 was actually the best way to bring about a better world.

35:41.040 --> 35:44.120
 So in that point, and I think that connects also

35:44.120 --> 35:48.080
 with career selection that we'll talk about,

35:48.080 --> 35:51.800
 but let's consider not objectives, but capitalism.

35:53.080 --> 35:56.840
 So, and the idea that you focusing on the thing

35:56.840 --> 36:01.840
 that you damn are damn good at, whatever that is,

36:02.400 --> 36:05.720
 may be the best thing for the world.

36:05.720 --> 36:08.600
 Sort of part of it is also mindset, right?

36:08.600 --> 36:13.080
 Sort of like the thing I love is robots.

36:13.080 --> 36:17.400
 So maybe I should focus on building robots

36:17.400 --> 36:19.800
 and never even think about the idea

36:19.800 --> 36:23.160
 of effective altruism, which is kind

36:23.160 --> 36:25.000
 of the capitalist notion.

36:25.000 --> 36:27.400
 Is there any value in that idea and just finding

36:27.400 --> 36:28.520
 the thing you're good at

36:28.520 --> 36:31.520
 and maximizing your productivity in this world

36:31.520 --> 36:34.960
 and thereby sort of lifting all boats

36:34.960 --> 36:38.640
 and benefiting society as a result?

36:38.640 --> 36:41.000
 Yeah, I think there's two things I'd wanna say on that.

36:41.000 --> 36:43.560
 So one is what your comparative advantages,

36:43.560 --> 36:45.400
 what your strengths are when it comes to career.

36:45.400 --> 36:46.840
 There's obviously super important

36:46.840 --> 36:50.720
 because there's lots of career paths I would be terrible at

36:50.720 --> 36:53.840
 if I thought being an artist was the best thing one could do.

36:53.840 --> 36:58.440
 Well, I'd be doomed, just really quite astonishingly bad.

36:59.320 --> 37:01.680
 And so I do think, at least within the realm

37:01.680 --> 37:05.760
 of things that could plausibly be very high impact,

37:05.760 --> 37:08.360
 choose the thing that you think you're gonna be able

37:08.360 --> 37:12.400
 to really be passionate at and excel at

37:12.400 --> 37:13.720
 kind of over the long term.

37:15.120 --> 37:17.960
 Then on this question of like, should one just do that

37:17.960 --> 37:19.680
 in an unrestricted way and not even think

37:19.680 --> 37:22.280
 about what the most important problems are?

37:22.280 --> 37:26.600
 I do think that in a kind of perfectly designed society,

37:26.600 --> 37:27.840
 that might well be the case.

37:27.840 --> 37:29.960
 That would be a society where we've corrected

37:29.960 --> 37:33.840
 all market failures, we've internalized all externalities

37:34.760 --> 37:37.000
 and then we've managed to set up incentives

37:37.000 --> 37:41.720
 such that people just pursuing their own strengths

37:41.720 --> 37:44.120
 is the best way of doing good,

37:44.120 --> 37:46.200
 but we're very far from that society.

37:46.200 --> 37:51.200
 So if one did that, then it'd be very unlikely

37:51.200 --> 37:55.000
 that you would focus on improving the lives

37:55.000 --> 37:57.880
 of non human animals that aren't participating in markets

37:57.880 --> 38:00.000
 or ensuring the long run future goes well,

38:00.000 --> 38:02.480
 where future people certainly aren't participating

38:02.480 --> 38:05.360
 in markets or benefiting the global poor

38:05.360 --> 38:09.680
 who do participate but have so much less kind of power

38:09.680 --> 38:12.120
 from a starting perspective that their views

38:12.120 --> 38:17.120
 aren't accurately kind of represented by market forces too.

38:17.120 --> 38:21.120
 Got it, so yeah, and sort of pure definition capitalism

38:21.120 --> 38:24.120
 just may very well ignore the people

38:24.120 --> 38:27.120
 that are suffering the most, the white swath of them.

38:27.120 --> 38:32.120
 So if you could allow me this line of thinking here,

38:33.720 --> 38:37.120
 so I've listened to a lot of your conversations online.

38:37.120 --> 38:42.120
 I find, if I can compliment you,

38:42.360 --> 38:44.360
 they're very interesting conversations.

38:44.360 --> 38:48.560
 Your conversation on Rogan, on Joe Rogan

38:48.560 --> 38:53.560
 was really interesting with Sam Harris and so on, whatever.

38:55.640 --> 38:57.920
 There's a lot of stuff that's really good out there.

38:57.920 --> 39:00.240
 And yet when I look at the internet,

39:00.240 --> 39:04.240
 I look at YouTube, which has certain mobs,

39:04.240 --> 39:08.280
 certain swaths of right leaning folks

39:08.280 --> 39:13.280
 whom I dearly love, I love all people.

39:13.280 --> 39:17.320
 All, especially people with ideas.

39:19.000 --> 39:21.320
 They seem to not like you very much.

39:22.680 --> 39:26.240
 So I don't understand why exactly.

39:26.240 --> 39:31.240
 So my own sort of hypothesis is there is a right left divide

39:31.240 --> 39:36.120
 that absurdly so caricatured in politics,

39:36.120 --> 39:38.320
 at least in the United States.

39:38.320 --> 39:42.720
 And maybe you're somehow pigeonholed into one of those sides

39:42.720 --> 39:46.600
 and maybe that's what it is.

39:46.600 --> 39:49.560
 Maybe your message is somehow politicized.

39:49.560 --> 39:51.320
 Yeah, I mean.

39:51.320 --> 39:52.240
 How do you make sense of that?

39:52.240 --> 39:54.400
 Because you're extremely interesting.

39:54.400 --> 39:57.680
 Like you got the comments I see on Joe Rogan,

39:58.640 --> 40:00.360
 there's a bunch of negative stuff.

40:00.360 --> 40:03.200
 And yet if you listen to it, the conversation is fascinating.

40:03.200 --> 40:08.200
 I'm not speaking, I'm not some kind of lefty extremist,

40:08.320 --> 40:10.120
 but just this fascinating conversation.

40:10.120 --> 40:13.760
 So why are you getting some small amount of hate?

40:13.760 --> 40:17.560
 So I'm actually pretty glad that effective altruism

40:17.560 --> 40:22.160
 has managed to stay relatively unpoliticized

40:22.160 --> 40:24.000
 because I think the core message

40:24.000 --> 40:25.880
 to just use some of your time and money

40:25.880 --> 40:27.160
 to do as much good as possible

40:27.160 --> 40:29.000
 to fight some of the problems in the world

40:29.000 --> 40:31.760
 can be appealing across the political spectrum.

40:31.760 --> 40:35.000
 And we do have a diversity of political viewpoints

40:35.000 --> 40:37.720
 among people who have engaged in effective altruism.

40:37.720 --> 40:40.640
 We do, however, do get some criticism

40:40.640 --> 40:42.720
 from the left and the right.

40:42.720 --> 40:43.560
 Oh, interesting.

40:43.560 --> 40:44.400
 What's the criticism?

40:44.400 --> 40:45.840
 Both will be interesting to hear.

40:45.840 --> 40:47.800
 Yeah, so criticism from the left

40:47.800 --> 40:49.280
 is that we're not focused enough

40:49.280 --> 40:52.520
 on dismantling the capitalist system

40:52.520 --> 40:55.600
 that they see as the root of most of the problems

40:55.600 --> 40:56.800
 that we're talking about.

40:58.480 --> 41:03.480
 And there I kind of disagree on partly of the premise

41:03.480 --> 41:08.480
 where I don't think relevant alternative systems

41:08.480 --> 41:11.560
 would say to the animals or to the global poor

41:11.560 --> 41:14.200
 or to the future generations, kind of much better.

41:14.200 --> 41:16.840
 And then also the tactics where I think

41:16.840 --> 41:19.600
 there are particular ways we can change society

41:19.600 --> 41:21.400
 that would massively benefit,

41:21.400 --> 41:23.720
 be massively beneficial on those things

41:23.720 --> 41:27.680
 that don't go via dismantling the entire system

41:27.680 --> 41:30.920
 which is perhaps a million times harder to do.

41:30.920 --> 41:32.400
 Then criticism on the right,

41:32.400 --> 41:34.400
 there's definitely like in the sponsor,

41:34.400 --> 41:36.400
 the Joe Rogan podcast.

41:36.400 --> 41:38.400
 There definitely were a number of A&L fans

41:38.400 --> 41:42.400
 who weren't keen on the idea of promoting altruism.

41:43.400 --> 41:46.400
 There was a remarkable set of ideas,

41:46.400 --> 41:48.400
 just the idea that effective altruism,

41:48.400 --> 41:51.400
 unmanly, I think, was driving a lot of criticism.

41:53.400 --> 41:56.400
 Okay, so I love fighting.

41:56.400 --> 41:58.400
 I've been in street fights my whole life.

41:58.400 --> 42:03.400
 I'm as alpha in everything I do as it gets.

42:03.400 --> 42:06.400
 And the fact that I and Joe Rogan said

42:06.400 --> 42:09.400
 that I thought Scent of a Woman is a better movie

42:09.400 --> 42:14.400
 than John Wick put me into this beta category

42:14.400 --> 42:19.400
 amongst people who are basically saying that,

42:19.400 --> 42:21.400
 yeah, unmanly or it's not tough,

42:21.400 --> 42:25.400
 it's not some principled view of strength

42:25.400 --> 42:28.400
 that is represented by it's possible.

42:28.400 --> 42:30.400
 So actually, how do you think about this?

42:30.400 --> 42:36.400
 Because to me, altruism, especially effective altruism,

42:36.400 --> 42:42.400
 is, I don't know what the female version of that is,

42:42.400 --> 42:46.400
 but on the male side, manliest fuck, if I may say so.

42:46.400 --> 42:51.400
 So how do you think about that kind of criticism?

42:51.400 --> 42:53.400
 I think people who would make that criticism

42:53.400 --> 42:56.400
 are just occupying a state of mind

42:56.400 --> 42:59.400
 that I think is just so different from my state of mind

42:59.400 --> 43:02.400
 that I kind of struggle to maybe even understand it,

43:02.400 --> 43:06.400
 where if something's manly or unmanly or feminine

43:06.400 --> 43:08.400
 or unfeminine, I'm like, I don't care.

43:08.400 --> 43:11.400
 Is it the right thing to do or the wrong thing to do?

43:11.400 --> 43:14.400
 Let me put it not in terms of man or woman,

43:14.400 --> 43:16.400
 because I don't think that's useful.

43:16.400 --> 43:21.400
 But I think there's a notion of acting out of fear

43:21.400 --> 43:26.400
 or as opposed to out of principle and strength.

43:26.400 --> 43:27.400
 Yeah.

43:27.400 --> 43:28.400
 So, okay, yeah.

43:28.400 --> 43:32.400
 Here's something that I do feel as an intuition

43:32.400 --> 43:35.400
 and that I think drives some people who do find

43:35.400 --> 43:39.400
 kind of a land detective and so on as a philosophy,

43:39.400 --> 43:43.400
 which is a kind of taking control of your own life

43:43.400 --> 43:48.400
 and having power over how you're steering your life

43:48.400 --> 43:53.400
 and not kind of toutowing to others,

43:53.400 --> 43:55.400
 really thinking things through.

43:55.400 --> 43:58.400
 I find that set of ideas just very compelling

43:58.400 --> 44:00.400
 and inspirational.

44:00.400 --> 44:02.400
 But I actually think of effective altruism

44:02.400 --> 44:05.400
 as really that side of my personality.

44:05.400 --> 44:07.400
 It's like, scratch that itch,

44:07.400 --> 44:11.400
 where you are just not taking the kind of priorities

44:11.400 --> 44:14.400
 that society is giving you as granted.

44:14.400 --> 44:18.400
 Instead, you're choosing to act in accordance with

44:18.400 --> 44:22.400
 the priorities that you think are most important in the world.

44:22.400 --> 44:29.400
 And often that involves then doing quite unusual things

44:29.400 --> 44:30.400
 from a societal perspective,

44:30.400 --> 44:33.400
 like donating a large chunk of your earnings

44:33.400 --> 44:36.400
 or working on these weird issues about AI

44:36.400 --> 44:39.400
 and so on that other people might not understand.

44:39.400 --> 44:42.400
 Yeah, I think that's a really gutsy thing to do.

44:42.400 --> 44:45.400
 Just taking control at least at this stage.

44:45.400 --> 44:52.400
 I mean, that's you taking ownership not of just yourself

44:52.400 --> 44:57.400
 but your presence in this world that's full of suffering

44:57.400 --> 45:00.400
 and saying as opposed to being paralyzed by that notion,

45:00.400 --> 45:03.400
 it's taking control and saying I could do something.

45:03.400 --> 45:04.400
 Yeah, exactly.

45:04.400 --> 45:05.400
 I mean, that's really powerful.

45:05.400 --> 45:11.400
 But the one thing I personally hate too about the left

45:11.400 --> 45:14.400
 currently that I think those folks to detect

45:14.400 --> 45:17.400
 is the social signaling.

45:17.400 --> 45:21.400
 When you look at yourself sort of late at night,

45:21.400 --> 45:23.400
 would you do everything you're doing

45:23.400 --> 45:27.400
 in terms of effective altruism if your name,

45:27.400 --> 45:28.400
 because you're quite popular,

45:28.400 --> 45:31.400
 but if your name was totally unattached to it,

45:31.400 --> 45:32.400
 if it was in secret?

45:32.400 --> 45:35.400
 Yeah, I mean, I think I would.

45:35.400 --> 45:39.400
 To be honest, I think the kind of popularity is like,

45:39.400 --> 45:43.400
 you know, it's a mixed bag but there are serious costs

45:43.400 --> 45:47.400
 and I don't particularly, I don't like love it.

45:47.400 --> 45:49.400
 Like it means you get all these people calling you a cock

45:49.400 --> 45:50.400
 on Joe Rogan.

45:50.400 --> 45:52.400
 It's like not the most fun thing.

45:52.400 --> 45:55.400
 But you also get a lot of sort of brownie points

45:55.400 --> 45:57.400
 for doing good for the world.

45:57.400 --> 45:58.400
 Yeah, you do.

45:58.400 --> 46:01.400
 But I think my ideal life, I would be like in some library

46:01.400 --> 46:04.400
 solving logic puzzles all day

46:04.400 --> 46:07.400
 and I'd like really be like learning maths and so on.

46:07.400 --> 46:11.400
 And have a good body of friends and so on.

46:11.400 --> 46:14.400
 So your instinct for effective altruism is something deep.

46:14.400 --> 46:19.400
 It's not one that is communicating socially.

46:19.400 --> 46:23.400
 It's more in your heart you want to do good for the world.

46:23.400 --> 46:27.400
 Yeah, I mean, so we can look back to early giving what we can.

46:27.400 --> 46:32.400
 So, you know, we're setting this up for me and Toby.

46:32.400 --> 46:36.400
 And I really thought that doing this would be a big hit

46:36.400 --> 46:39.400
 in my academic career because I was now spending, you know,

46:39.400 --> 46:42.400
 at that time more than half my time setting up this nonprofit

46:42.400 --> 46:45.400
 at the crucial time when you should be like producing

46:45.400 --> 46:47.400
 your best academic work and so on.

46:47.400 --> 46:51.400
 And it was also the case at the time, it was kind of like

46:51.400 --> 46:53.400
 the Toby Ord Club.

46:53.400 --> 46:55.400
 You know, he was the most popular.

46:55.400 --> 46:57.400
 There was this personal interest story around him

46:57.400 --> 46:59.400
 and his plans to donate.

46:59.400 --> 47:02.400
 Sorry to interrupt, but Toby was donating a large amount.

47:02.400 --> 47:05.400
 Can you tell just briefly what he was doing?

47:05.400 --> 47:09.400
 Yeah, so he made this public commitment to give everything here

47:09.400 --> 47:14.400
 and above £20,000 per year to the most effective causes.

47:14.400 --> 47:17.400
 And even as a graduate student, he was still donating

47:17.400 --> 47:21.400
 about 15, 20% of his income, which is quite significant

47:21.400 --> 47:24.400
 given that graduate students are not known for being super wealthy.

47:24.400 --> 47:25.400
 That's right.

47:25.400 --> 47:27.400
 And when we launched giving what we can,

47:27.400 --> 47:31.400
 the media just loved this as like a personal interest story.

47:31.400 --> 47:36.400
 So the story about him and his pledge was the most,

47:36.400 --> 47:40.400
 yeah, it was actually the most popular news story of the day.

47:40.400 --> 47:42.400
 And we kind of ran the same story a year later,

47:42.400 --> 47:44.400
 and it was the most popular news story of the day

47:44.400 --> 47:46.400
 a year later too.

47:46.400 --> 47:52.400
 And so it really was kind of several years before

47:52.400 --> 47:54.400
 then I was also kind of giving more talks

47:54.400 --> 47:55.400
 and starting to do more writing,

47:55.400 --> 47:57.400
 and then especially with, you know,

47:57.400 --> 47:59.400
 I wrote this book, Doing Good Better,

47:59.400 --> 48:03.400
 that then there started to be kind of attention and so on.

48:03.400 --> 48:07.400
 But deep inside your own relationship with effective altruism

48:07.400 --> 48:12.400
 was, I mean, it had nothing to do with the publicity.

48:12.400 --> 48:17.400
 Did you see yourself, how did the publicity connect with it?

48:17.400 --> 48:19.400
 Yeah, I mean, that's kind of what I'm saying

48:19.400 --> 48:22.400
 is I think the publicity came like several years afterwards.

48:22.400 --> 48:25.400
 I mean, at the early stage when we set up giving what we can,

48:25.400 --> 48:29.400
 it was really just every person we get to pledge 10% is,

48:29.400 --> 48:34.400
 you know, something like $100,000 over their lifetime.

48:34.400 --> 48:35.400
 That's huge.

48:35.400 --> 48:38.400
 And so it was just we had started with 23 members.

48:38.400 --> 48:42.400
 Every single person was just this like kind of huge accomplishment.

48:42.400 --> 48:45.400
 And at the time I just really thought, you know,

48:45.400 --> 48:47.400
 maybe over time we'll have 100 members

48:47.400 --> 48:49.400
 and that'll be like amazing.

48:49.400 --> 48:51.400
 Whereas now we have, you know,

48:51.400 --> 48:53.400
 over 4,000 and one and a half billion dollars pledged.

48:53.400 --> 48:58.400
 That's just unimaginable to me at the time when I was first

48:58.400 --> 49:01.400
 kind of getting this, you know, getting this stuff off the ground.

49:01.400 --> 49:09.400
 So can we talk about poverty and the biggest problems

49:09.400 --> 49:13.400
 that you think in the near term effective altruism

49:13.400 --> 49:15.400
 can attack in each one.

49:15.400 --> 49:18.400
 So poverty obviously is a huge one.

49:18.400 --> 49:19.400
 Yeah.

49:19.400 --> 49:21.400
 How can we help?

49:21.400 --> 49:24.400
 Great. Yeah. So poverty absolutely this huge problem,

49:24.400 --> 49:29.400
 700 million people in extreme poverty living in less than $2 per day

49:29.400 --> 49:35.400
 where that's what that means is what $2 would buy in the US.

49:35.400 --> 49:36.400
 So think about that.

49:36.400 --> 49:38.400
 It's like some rice, maybe some beans.

49:38.400 --> 49:41.400
 It's very, you know, really not much.

49:41.400 --> 49:44.400
 And at the same time we can do an enormous amount

49:44.400 --> 49:47.400
 to improve the lives of people in extreme poverty.

49:47.400 --> 49:49.400
 So the things that we tend to focus on

49:49.400 --> 49:52.400
 are interventions in global health.

49:52.400 --> 49:55.400
 And that's for a couple of reasons.

49:55.400 --> 49:58.400
 One is that global health just has this amazing track record.

49:58.400 --> 50:03.400
 Life expectancy globally is up 50% relative to 60 or 70 years ago.

50:03.400 --> 50:07.400
 We've eradicated smallpox, which killed 2 million lives every year,

50:07.400 --> 50:09.400
 almost eradicated polio.

50:09.400 --> 50:13.400
 Second is that we just have great data on what works

50:13.400 --> 50:15.400
 when it comes to global health.

50:15.400 --> 50:19.400
 So we just know that bed nets protect children

50:19.400 --> 50:22.400
 and prevent them from dying from malaria.

50:22.400 --> 50:26.400
 And then the third is just that it's extremely cost effective.

50:26.400 --> 50:29.400
 So it costs $5 to buy one bed net,

50:29.400 --> 50:32.400
 protects two children for two years against malaria.

50:32.400 --> 50:34.400
 If you spend about $3,000 on bed nets,

50:34.400 --> 50:38.400
 then statistically speaking you're going to save a child's life.

50:38.400 --> 50:41.400
 And there are other interventions too.

50:41.400 --> 50:44.400
 And so given the people in such suffering

50:44.400 --> 50:48.400
 and we have this opportunity to, you know,

50:48.400 --> 50:52.400
 do such huge good for such low cost, well, yeah, why not?

50:52.400 --> 50:55.400
 So the individuals, so for me today,

50:55.400 --> 51:00.400
 if I wanted to deal with the poverty, how would I help?

51:00.400 --> 51:04.400
 And I wanted to say, I think donating 10% of your income

51:04.400 --> 51:06.400
 is a very interesting idea or some percentage

51:06.400 --> 51:10.400
 or some setting a bar instead of sticking to it.

51:10.400 --> 51:15.400
 So how do we then take the step towards the effective part?

51:15.400 --> 51:20.400
 So you've conveyed some notions, but who do you give the money to?

51:20.400 --> 51:24.400
 Yeah, so Give Well, this organization I mentioned is...

51:24.400 --> 51:25.400
 Give Well.

51:25.400 --> 51:27.400
 Well, it makes charity recommendations

51:27.400 --> 51:29.400
 and some of its top recommendations.

51:29.400 --> 51:32.400
 So Against Malaria Foundation is this organization

51:32.400 --> 51:37.400
 that buys and distributes these insecticide seeded bed nets.

51:37.400 --> 51:40.400
 And then it has a total of seven charities

51:40.400 --> 51:42.400
 that it recommends very highly.

51:42.400 --> 51:47.400
 So that recommendation, is it almost like a star of approval?

51:47.400 --> 51:49.400
 Or is there some metrics?

51:49.400 --> 51:53.400
 So what are the ways that Give Well conveys

51:53.400 --> 51:57.400
 that this is a great charity organization?

51:57.400 --> 52:00.400
 Yeah, so Give Well is looking at metrics

52:00.400 --> 52:03.400
 and it's trying to compare charities ultimately

52:03.400 --> 52:06.400
 in the number of lives that you can save

52:06.400 --> 52:08.400
 without an equivalent benefit.

52:08.400 --> 52:10.400
 So one of the charities that it recommends

52:10.400 --> 52:14.400
 is Give Directly, which simply just transfers cash

52:14.400 --> 52:16.400
 to the poorest families,

52:16.400 --> 52:20.400
 where a poor family will get a cash transfer of $1,000.

52:20.400 --> 52:23.400
 And they kind of regard that as the baseline intervention

52:23.400 --> 52:25.400
 because it's so simple and people, you know,

52:25.400 --> 52:29.400
 they know what to do with how to benefit themselves.

52:29.400 --> 52:31.400
 That's quite powerful, by the way.

52:31.400 --> 52:34.400
 So before Give Well, before the effective altruism movement,

52:34.400 --> 52:38.400
 was there, I imagine there's a huge amount of corruption,

52:38.400 --> 52:41.400
 funny enough, in charity organizations,

52:41.400 --> 52:43.400
 or misuse of money.

52:43.400 --> 52:46.400
 So there was nothing like Give Well before that?

52:46.400 --> 52:49.400
 No, I mean, there were some, so I mean, the charity corruption,

52:49.400 --> 52:51.400
 I mean, obviously there's some,

52:51.400 --> 52:54.400
 I don't think it's a huge issue,

52:54.400 --> 52:57.400
 they're also just focusing on the long things.

52:57.400 --> 52:59.400
 Prior to Give Well, there were some organizations

52:59.400 --> 53:02.400
 like Charity Navigator, which were more aimed

53:02.400 --> 53:04.400
 at worrying about corruption and so on.

53:04.400 --> 53:06.400
 So they weren't saying, these are the charities

53:06.400 --> 53:08.400
 where you're going to do the most good.

53:08.400 --> 53:12.400
 Instead, it was like, how good are the charity's financials?

53:12.400 --> 53:14.400
 How good is its health? Are they transparent?

53:14.400 --> 53:16.400
 And yeah, so that would be more useful

53:16.400 --> 53:18.400
 for weeding out some of those worst charities.

53:18.400 --> 53:21.400
 So Give Well is just taking this step further.

53:21.400 --> 53:24.400
 Sort of in this 21st century of data,

53:24.400 --> 53:28.400
 it's actually looking at the effective part.

53:28.400 --> 53:31.400
 Yeah, so it's like, you know, if you know the wire cutter

53:31.400 --> 53:33.400
 if you want to buy a pair of headphones,

53:33.400 --> 53:35.400
 they will just look at all the headphones and be like,

53:35.400 --> 53:37.400
 these are the best headphones you can buy.

53:37.400 --> 53:39.400
 That's the idea with Give Well.

53:39.400 --> 53:44.400
 Okay, so do you think there's a bar of what suffering is?

53:44.400 --> 53:47.400
 And do you think one day we can eradicate suffering

53:47.400 --> 53:50.400
 in our world amongst humans?

53:50.400 --> 53:52.400
 Let's talk humans for now.

53:52.400 --> 53:55.400
 Talk humans, but in general, yeah, actually.

53:55.400 --> 53:59.400
 So there's a colleague of mine,

53:59.400 --> 54:01.400
 kind of term abolitionism for the idea

54:01.400 --> 54:03.400
 that we should just be trying to abolish suffering.

54:03.400 --> 54:05.400
 And in the long run, I mean,

54:05.400 --> 54:08.400
 I don't expect it anytime soon, but I think we can.

54:08.400 --> 54:10.400
 I think that would require, you know,

54:10.400 --> 54:14.400
 quite drastic changes to the way society is structured

54:14.400 --> 54:19.400
 and perhaps even the, you know,

54:19.400 --> 54:22.400
 in fact, even changes to human nature.

54:22.400 --> 54:25.400
 But I do think that suffering whenever that occurs is bad

54:25.400 --> 54:28.400
 and we should want it to not occur.

54:28.400 --> 54:31.400
 So there's a line.

54:31.400 --> 54:33.400
 There's a gray area between suffering.

54:33.400 --> 54:38.400
 Now I'm Russian, so I romanticize some aspects of suffering.

54:38.400 --> 54:40.400
 There's a gray line between struggle,

54:40.400 --> 54:44.400
 gray area between struggle and suffering.

54:44.400 --> 54:51.400
 So one, do we want to eradicate all struggle in the world?

54:51.400 --> 54:59.400
 So there's an idea, you know, that the human condition

54:59.400 --> 55:04.400
 inherently has suffering in it and it's a creative force.

55:04.400 --> 55:09.400
 It's a struggle of our lives and we somehow grow from that.

55:09.400 --> 55:13.400
 How do you think about that?

55:13.400 --> 55:15.400
 I agree that's true.

55:15.400 --> 55:21.400
 So, you know, often, you know, great artists can be also suffering from,

55:21.400 --> 55:24.400
 you know, major health conditions or depression and so on.

55:24.400 --> 55:26.400
 Or they come from abusive parents.

55:26.400 --> 55:27.400
 Yeah, for example.

55:27.400 --> 55:29.400
 The most great artists they think come from abusive parents.

55:29.400 --> 55:32.400
 Yeah, that seems to be at least commonly the case.

55:32.400 --> 55:37.400
 But I want to distinguish between suffering as being instrumentally good,

55:37.400 --> 55:40.400
 you know, it causes people to produce good things

55:40.400 --> 55:42.400
 and whether it's intrinsically good.

55:42.400 --> 55:44.400
 And I think intrinsically it's always bad.

55:44.400 --> 55:47.400
 And so if we can produce these, you know, great achievements

55:47.400 --> 55:53.400
 via some other means where, you know, if we look at the scientific enterprise,

55:53.400 --> 55:55.400
 we've produced incredible things.

55:55.400 --> 55:59.400
 Often from people who aren't suffering have, you know,

55:59.400 --> 56:00.400
 pretty good lives.

56:00.400 --> 56:02.400
 They're just, they're driven instead of, you know,

56:02.400 --> 56:04.400
 being pushed by a sense of anguish.

56:04.400 --> 56:06.400
 They're being driven by intellectual curiosity.

56:06.400 --> 56:11.400
 If we can instead produce a society where it's all carrot and no stick,

56:11.400 --> 56:13.400
 that's better from my perspective.

56:13.400 --> 56:17.400
 Yeah, but I'm going to have to disagree with the notion that that's possible.

56:17.400 --> 56:22.400
 But I would say most of the suffering in the world is not productive.

56:22.400 --> 56:27.400
 So I would dream of effective altruism curing that suffering.

56:27.400 --> 56:28.400
 Yeah.

56:28.400 --> 56:31.400
 But then I would say that there is some suffering that is productive

56:31.400 --> 56:36.400
 that we want to keep the, because, but that's not even the focus of,

56:36.400 --> 56:39.400
 because most of the suffering is just absurd.

56:39.400 --> 56:40.400
 Yeah.

56:40.400 --> 56:42.400
 It needs to be eliminated.

56:42.400 --> 56:46.400
 So let's not even romanticize this usual notion I have,

56:46.400 --> 56:53.400
 but nevertheless struggle has some kind of inherent value that to me at least.

56:53.400 --> 56:54.400
 Yeah.

56:54.400 --> 56:55.400
 You're right.

56:55.400 --> 56:58.400
 There's some elements of human nature that also have to be modified

56:58.400 --> 57:00.400
 in order to cure all suffering.

57:00.400 --> 57:01.400
 Yeah.

57:01.400 --> 57:03.400
 I mean, there's an interesting question of whether it's possible.

57:03.400 --> 57:06.400
 So at the moment, you know, most of the time we're kind of neutral,

57:06.400 --> 57:10.400
 and then we burn ourselves and that's negative and that's really good

57:10.400 --> 57:14.400
 that we get that negative signal because it means we won't burn ourselves again.

57:14.400 --> 57:19.400
 There's a question like, could you design agents, humans,

57:19.400 --> 57:22.400
 such that you're not hovering around the zero level,

57:22.400 --> 57:23.400
 you're hovering at like bliss.

57:23.400 --> 57:24.400
 Yeah.

57:24.400 --> 57:26.400
 And then you touch the flame and you're like, oh no,

57:26.400 --> 57:27.400
 you're just slightly worse bliss.

57:27.400 --> 57:28.400
 Yeah.

57:28.400 --> 57:32.400
 But that's really bad compared to the bliss you are normally in.

57:32.400 --> 57:35.400
 So that you can have like a gradient of bliss instead of like pain and pleasure.

57:35.400 --> 57:40.400
 Well, on that point, I think it's a really important point on the experience

57:40.400 --> 57:45.400
 of suffering, the relative nature of it.

57:45.400 --> 57:48.400
 I mean, having grown up in the Soviet Union,

57:48.400 --> 57:57.400
 we're quite poor by any measure in when I was in my childhood,

57:57.400 --> 58:01.400
 but it didn't feel like you were poor because everybody around you were poor.

58:01.400 --> 58:06.400
 And then in America, I feel, for the first time,

58:06.400 --> 58:11.400
 beginning to feel poor because of the, there's different.

58:11.400 --> 58:16.400
 There's some cultural aspects to it that really emphasize that it's good to be rich.

58:16.400 --> 58:20.400
 And then there's just the notion that there is a lot of income inequality

58:20.400 --> 58:22.400
 and therefore you experience that inequality.

58:22.400 --> 58:23.400
 That's where suffering comes.

58:23.400 --> 58:27.400
 So what do you think about the inequality of suffering

58:27.400 --> 58:31.400
 that we have to think about?

58:31.400 --> 58:37.400
 Do you think we have to think about that as part of effective altruism?

58:37.400 --> 58:38.400
 Yeah.

58:38.400 --> 58:43.400
 I think there are just things vary in terms of whether you get benefits

58:43.400 --> 58:46.400
 or costs from them just in relative terms or in absolute terms.

58:46.400 --> 58:49.400
 So a lot of the time, yeah, there's this hedonic treadmill

58:49.400 --> 58:58.400
 where there's money is useful because it helps you buy things

58:58.400 --> 59:00.400
 or good for you because it helps you buy things,

59:00.400 --> 59:02.400
 but there's also a status component too.

59:02.400 --> 59:05.400
 And that status component is kind of zero sum.

59:05.400 --> 59:10.400
 If you were saying like in Russia, no one else felt poor

59:10.400 --> 59:13.400
 because everyone around you was poor,

59:13.400 --> 59:18.400
 whereas now you've got this, these other people who are super rich

59:18.400 --> 59:24.400
 and maybe that makes you feel less good about yourself.

59:24.400 --> 59:28.400
 There are some other things, however, which are just instantaneously good or bad.

59:28.400 --> 59:33.400
 So commuting, for example, is just people hate it.

59:33.400 --> 59:34.400
 It doesn't really change.

59:34.400 --> 59:40.400
 Knowing that other people are commuting too doesn't make it any kind of less bad.

59:40.400 --> 59:43.400
 But to push back on that for a second, I mean, yes,

59:43.400 --> 59:49.400
 but also if some people are on horseback,

59:49.400 --> 59:52.400
 your commute on the train might feel a lot better.

59:52.400 --> 59:58.400
 There is a relative, I mean, everybody's complaining about society today,

59:58.400 --> 1:00:04.400
 forgetting how much better it is, the better angels of our nature,

1:00:04.400 --> 1:00:09.400
 how the technology is fundamentally improving most of the world's lives.

1:00:09.400 --> 1:00:16.400
 And actually there's some psychological research on the well being benefits of volunteering,

1:00:16.400 --> 1:00:21.400
 where people who volunteer tend to just feel happier about their lives.

1:00:21.400 --> 1:00:25.400
 And one of the suggested explanations is it because it extends your reference class.

1:00:25.400 --> 1:00:30.400
 So no longer you comparing yourself to the Joneses who have their slightly better car,

1:00:30.400 --> 1:00:34.400
 but you realize that people are in much worse conditions than you.

1:00:34.400 --> 1:00:37.400
 And so now your life doesn't seem so bad.

1:00:37.400 --> 1:00:39.400
 That's actually on the psychological level.

1:00:39.400 --> 1:00:45.400
 One of the fundamental benefits of effective altruism is, I mean,

1:00:45.400 --> 1:00:48.400
 I guess it's the altruism part of effective altruism,

1:00:48.400 --> 1:00:56.400
 is exposing yourself to the suffering in the world allows you to be more, yeah, happier

1:00:56.400 --> 1:01:01.400
 and actually allows you in a sort of meditative, introspective way,

1:01:01.400 --> 1:01:07.400
 realize that you don't need most of the wealth you have to be happy.

1:01:07.400 --> 1:01:11.400
 Absolutely. I mean, I think effective altruism has been this huge benefit for me.

1:01:11.400 --> 1:01:14.400
 And I really don't think that if I had more money that I was living on,

1:01:14.400 --> 1:01:17.400
 that that would change my level of well being at all.

1:01:17.400 --> 1:01:21.400
 Whereas engaging in something that I think is meaningful,

1:01:21.400 --> 1:01:26.400
 that I think is steering humanity in a positive direction, that's extremely rewarding.

1:01:26.400 --> 1:01:32.400
 And so, yeah, I mean, despite my best attempts at sacrifice,

1:01:32.400 --> 1:01:38.400
 I think I've actually ended up happier as a result of engaging in effective altruism than I would have done.

1:01:38.400 --> 1:01:40.400
 That's an interesting idea.

1:01:40.400 --> 1:01:43.400
 So let's talk about animal welfare.

1:01:43.400 --> 1:01:46.400
 Easy question. What is consciousness?

1:01:46.400 --> 1:01:50.400
 Especially as it has to do with the capacity to suffer.

1:01:50.400 --> 1:01:55.400
 I think there seems to be a connection between how conscious something is,

1:01:55.400 --> 1:01:59.400
 the amount of consciousness and its ability to suffer.

1:01:59.400 --> 1:02:05.400
 And that all comes into play about us thinking how much suffering there is in the world with regard to animals.

1:02:05.400 --> 1:02:08.400
 So how do you think about animal welfare and consciousness?

1:02:08.400 --> 1:02:11.400
 Okay. Well, consciousness, easy question.

1:02:11.400 --> 1:02:14.400
 Yeah, I mean, I think we don't have a good understanding of consciousness.

1:02:14.400 --> 1:02:16.400
 My best guess is it's got.

1:02:16.400 --> 1:02:20.400
 And by consciousness, I'm meaning what it feels like to be you,

1:02:20.400 --> 1:02:26.400
 the subjective experience that seems to be different from everything else we know about in the world.

1:02:26.400 --> 1:02:29.400
 Yeah, I think it's clear, it's very poorly understood at the moment.

1:02:29.400 --> 1:02:32.400
 I think it has something to do with information processing.

1:02:32.400 --> 1:02:36.400
 So the fact that the brain is a computer or something like a computer.

1:02:36.400 --> 1:02:41.400
 So that would mean that very advanced AI could be conscious.

1:02:41.400 --> 1:02:46.400
 Information processors in general could be conscious with some suitable complexity.

1:02:46.400 --> 1:02:53.400
 But that also, some suitable complexity, it's a question whether greater complexity creates some kind of greater consciousness,

1:02:53.400 --> 1:02:55.400
 which relates to animals.

1:02:55.400 --> 1:03:00.400
 If it's an information processing system and it's smaller and smaller,

1:03:00.400 --> 1:03:06.400
 is an ant less conscious than a cow, less conscious than a monkey?

1:03:06.400 --> 1:03:12.400
 Yeah, and again, this super hard question, but I think my best guess is yes.

1:03:12.400 --> 1:03:17.400
 Like if I think, well, consciousness, it's not some magical thing that appears out of nowhere.

1:03:17.400 --> 1:03:21.400
 It's not, you know, Descartes thought it was just comes in from this other realm

1:03:21.400 --> 1:03:28.400
 and then enters through the pineal gland in your brain and that's kind of soul and it's conscious.

1:03:28.400 --> 1:03:33.400
 So it's got something to do with what's going on in your brain.

1:03:33.400 --> 1:03:38.400
 A chicken has one three hundredths of the size of the brain that you have.

1:03:38.400 --> 1:03:42.400
 Ants, I don't know how small it is, maybe it's a millionth the size.

1:03:42.400 --> 1:03:47.400
 My best guess, which I may well be wrong about because this is so hard,

1:03:47.400 --> 1:03:54.400
 is that in some relevant sense, the chicken is experiencing consciousness to a lesser degree than the human

1:03:54.400 --> 1:03:56.400
 and the ants significantly less again.

1:03:56.400 --> 1:04:00.400
 I don't think it's as little as three hundredths as much, I think.

1:04:00.400 --> 1:04:06.400
 There's evolutionary reasons for thinking that like the ability to feel pain comes on the scene relatively early on.

1:04:06.400 --> 1:04:11.400
 And we have lots of our brain that's dedicated to stuff that doesn't seem to have to do anything to do with consciousness,

1:04:11.400 --> 1:04:13.400
 language processing and so on.

1:04:13.400 --> 1:04:20.400
 So it seems like the easy, so there's a lot of complicated questions there that we can't ask the animals about.

1:04:20.400 --> 1:04:29.400
 But it seems that there's easy questions in terms of suffering, which is things like factory farming that could be addressed.

1:04:29.400 --> 1:04:36.400
 Is that the lowest hanging fruit, if I may use crude terms here, of animal welfare?

1:04:36.400 --> 1:04:38.400
 Absolutely, I think that's the lowest hanging fruit.

1:04:38.400 --> 1:04:44.400
 So at the moment we kill, we raise and kill about 50 billion animals every year.

1:04:44.400 --> 1:04:45.400
 So how many?

1:04:45.400 --> 1:04:47.400
 50 billion.

1:04:47.400 --> 1:04:53.400
 So for every human on the planet, several times that number are being killed.

1:04:53.400 --> 1:04:59.400
 And the vast majority of them are raised in factory farms where basically whatever your view on animals,

1:04:59.400 --> 1:05:03.400
 I think you should agree, even if you think, well, maybe it's not bad to kill an animal,

1:05:03.400 --> 1:05:05.400
 maybe if the animal was raised in good conditions.

1:05:05.400 --> 1:05:07.400
 That's just not the empirical reality.

1:05:07.400 --> 1:05:12.400
 The empirical reality is that they are kept in incredible cage confinement.

1:05:12.400 --> 1:05:24.400
 They are debeaked or detailed without an aesthetic.

1:05:24.400 --> 1:05:30.400
 I think when a chicken gets killed, that's the best thing that happened to the chicken in the course of its life.

1:05:30.400 --> 1:05:32.400
 And it's also completely unnecessary.

1:05:32.400 --> 1:05:37.400
 This is in order to save a few pence for the price of meat or price of eggs.

1:05:37.400 --> 1:05:43.400
 And we have indeed found it's also just inconsistent with consumer preference as well.

1:05:43.400 --> 1:05:52.400
 People who buy the products, when you do surveys, are extremely against suffering in factory farms.

1:05:52.400 --> 1:05:57.400
 It's just they don't appreciate how bad it is and just tend to go with easy options.

1:05:57.400 --> 1:06:04.400
 And so then the best, the most effective programs I know of at the moment are nonprofits that go to companies

1:06:04.400 --> 1:06:11.400
 and work with companies to get them to take a pledge to cut certain sorts of animal products,

1:06:11.400 --> 1:06:15.400
 like eggs from cage confinement out of their supply chain.

1:06:15.400 --> 1:06:22.400
 And it's now the case that the top 50 food retailers and fast food companies

1:06:22.400 --> 1:06:25.400
 have all made these kind of cage for the pledges.

1:06:25.400 --> 1:06:30.400
 And when you do the numbers, you get the conclusion that every dollar you're giving to these nonprofits,

1:06:30.400 --> 1:06:33.400
 there's hundreds of chickens being spared from cage confinement.

1:06:33.400 --> 1:06:39.400
 And then they're working to other types of animals, other products too.

1:06:39.400 --> 1:06:44.400
 So is that the most effective way to have a ripple effect essentially?

1:06:44.400 --> 1:06:51.400
 It's supposed to directly having regulation from on top that says you can't do this.

1:06:51.400 --> 1:06:56.400
 So I would be more open to the regulation approach, but at least in the U.S.

1:06:56.400 --> 1:07:01.400
 there's quite intense regulatory capture from the agricultural industry.

1:07:01.400 --> 1:07:09.400
 And so attempts that we've seen to try and change regulation, it's been a real uphill struggle.

1:07:09.400 --> 1:07:16.400
 There are some examples of ballot initiatives where the people have been able to vote in a ballot

1:07:16.400 --> 1:07:21.400
 to say we want to ban eggs from cage conditions, and that's been huge, that's been really good.

1:07:21.400 --> 1:07:24.400
 But beyond that, it's much more limited.

1:07:24.400 --> 1:07:32.400
 So I've been really interested in the idea of hunting in general and wild animals and seeing nature

1:07:32.400 --> 1:07:43.400
 as a form of cruelty that I am ethically more okay with, just from my perspective.

1:07:43.400 --> 1:07:47.400
 And then I read about wild animal suffering.

1:07:47.400 --> 1:08:00.400
 I'm just giving you the notion of how I felt because animal factory farming is so bad that living in the woods seemed good.

1:08:00.400 --> 1:08:11.400
 And yet when you actually start to think about it, all of the animals in the animal world are living in terrible poverty.

1:08:11.400 --> 1:08:18.400
 So you have all the medical conditions, all of that, I mean, they're living horrible lives that could be improved.

1:08:18.400 --> 1:08:26.400
 That's a really interesting notion that I think may not even be useful to talk about because factory farming is such a big thing to focus on.

1:08:26.400 --> 1:08:34.400
 But it's nevertheless an interesting notion to think of all the animals in the wild as suffering in the same way that humans in poverty are suffering.

1:08:34.400 --> 1:08:46.400
 Yeah, I mean, and often even worse, so many animals are produced via our selection, so you have a very large number of children in the expectation that only small numbers survive.

1:08:46.400 --> 1:08:52.400
 And so for those animals, almost all of them just live short lives where they starve to death.

1:08:52.400 --> 1:08:54.400
 So yeah, there's huge amounts of suffering in nature.

1:08:54.400 --> 1:09:04.400
 I don't think we should pretend that it's this kind of wonderful paradise for most animals.

1:09:04.400 --> 1:09:10.400
 Yeah, their life is filled with hunger and fear and disease.

1:09:10.400 --> 1:09:16.400
 I agree with you entirely that when it comes to focusing on animal welfare, we should focus on factory farming.

1:09:16.400 --> 1:09:23.400
 But we also should be aware to the reality of what life for most animals is like.

1:09:23.400 --> 1:09:38.400
 So let's talk about a topic I've talked a lot about, and you've actually quite eloquently talked about, which is the third priority that effective altruism considers as really important is existential risks.

1:09:38.400 --> 1:09:45.400
 When you think about the existential risks that are facing our civilization, what's before us?

1:09:45.400 --> 1:09:46.400
 What concerns you?

1:09:46.400 --> 1:09:51.400
 What should we be thinking about, especially from an effective altruism perspective?

1:09:51.400 --> 1:10:02.400
 Great, so the reason I started getting concerned about this was thinking about future generations, where the key idea is just while future people matter morally,

1:10:02.400 --> 1:10:05.400
 there are vast numbers of future people.

1:10:05.400 --> 1:10:11.400
 If we don't cause our own extinction, there's no reason why civilization might not last a million years.

1:10:11.400 --> 1:10:14.400
 I mean, we last as long as a typical mammalian species.

1:10:14.400 --> 1:10:23.400
 A billion years is when the earth is no longer habitable, or if we can take to the stars, then perhaps it's trillions of years beyond that.

1:10:23.400 --> 1:10:29.400
 So the future could be very big indeed, and it seems like we're potentially very early on in civilization.

1:10:29.400 --> 1:10:37.400
 Then the second idea is just, well, maybe there are things that are going to really derail that, things that actually could prevent us from having this long, wonderful civilization.

1:10:37.400 --> 1:10:50.400
 And instead, could cause our own extinction, or otherwise perhaps lock ourselves into a very bad state.

1:10:50.400 --> 1:10:53.400
 And what ways could that happen?

1:10:53.400 --> 1:11:06.400
 Well, causing our own extinction, development of nuclear weapons in the 20th century, at least put on the table that we now had weapons that were powerful enough that you could very significantly destroy society.

1:11:06.400 --> 1:11:09.400
 Perhaps an all out nuclear war would cause a nuclear winter.

1:11:09.400 --> 1:11:14.400
 Perhaps that would be enough for the human race to go extinct.

1:11:14.400 --> 1:11:16.400
 Why do you think we haven't done it?

1:11:16.400 --> 1:11:17.400
 Sorry to interrupt.

1:11:17.400 --> 1:11:19.400
 Why do you think we haven't done it yet?

1:11:19.400 --> 1:11:33.400
 Is it surprising to you that having always, for the past few decades, several thousand of active ready to launch nuclear weapons warheads,

1:11:33.400 --> 1:11:43.400
 and yet we have not launched them ever since the initial launch on Hiroshima and Nagasaki?

1:11:43.400 --> 1:11:46.400
 I think it's a mix of luck.

1:11:46.400 --> 1:11:49.400
 So I think it's definitely not inevitable that we haven't used them.

1:11:49.400 --> 1:11:58.400
 So John F. Kennedy during the Cuban Missile Crisis put the estimate of nuclear exchange between the US and USSR that somewhere between one in three and even.

1:11:58.400 --> 1:12:02.400
 So, you know, we really did come close.

1:12:02.400 --> 1:12:08.400
 At the same time, I do think mutually assured destruction is a reason why people don't go to war.

1:12:08.400 --> 1:12:11.400
 It would be, you know, why nuclear powers don't go to war.

1:12:11.400 --> 1:12:20.400
 Do you think that holds, if you can link around that for a second, like my dad is a physicist amongst other things.

1:12:20.400 --> 1:12:31.400
 And he believes that nuclear weapons are actually just really hard to build, which is one of the really big benefits of them currently.

1:12:31.400 --> 1:12:38.400
 So that you don't have, it's very hard if you're crazy to build, to acquire a nuclear weapon.

1:12:38.400 --> 1:12:52.400
 So the mutually assured destruction really works when you talk, seems to work better when it's nation states, when it's serious people, even if they're a little bit, you know, dictatorial and so on.

1:12:52.400 --> 1:13:01.400
 Do you think this mutually assured destruction idea will carry, how far will it carry us in terms of different kinds of weapons?

1:13:01.400 --> 1:13:13.400
 Oh, yeah, I think it's your point that nuclear weapons are very hard to build and relatively easy to control because you can control fissile material is a really important one.

1:13:13.400 --> 1:13:18.400
 And future technology that's equally destructive might not have those properties.

1:13:18.400 --> 1:13:31.400
 So for example, if in the future, people are able to design viruses, perhaps using a DNA printing kit that's on that, you know, one can just buy.

1:13:31.400 --> 1:13:41.400
 In fact, there are companies in the process of creating home DNA printing kits.

1:13:41.400 --> 1:13:53.400
 Well, then perhaps that's just totally democratized, perhaps the power to reap huge destructive potential is in the hands of most people in the world, or certainly most people with effort.

1:13:53.400 --> 1:14:03.400
 And then, yeah, I no longer trust mutually assured destruction because some for some people, the idea that they would die is just not a disincentive.

1:14:03.400 --> 1:14:12.400
 There was a Japanese cult, for example, Om Shinrikyo in the 90s that had, what they believed was that Armageddon was coming.

1:14:12.400 --> 1:14:19.400
 If you died before Armageddon, you would get good karma, you wouldn't go to hell.

1:14:19.400 --> 1:14:23.400
 If you died during Armageddon, maybe you would go to hell.

1:14:23.400 --> 1:14:36.400
 And they had a biological weapons program, a chemical weapons program, when they were finally apprehended, they hadn't stocks of southern gas that were sufficient to kill 4 million people engaged in multiple terrorist acts.

1:14:36.400 --> 1:14:42.400
 If they had had the ability to thinter virus at home, that would have been very scary.

1:14:42.400 --> 1:14:57.400
 So it's not impossible to imagine groups of people that hold that kind of belief of death as a suicide as a good thing for passage into the next world and so on.

1:14:57.400 --> 1:15:06.400
 And then connect them with some weapons, then ideology and weaponry create serious problems for us.

1:15:06.400 --> 1:15:13.400
 Let me ask you a quick question. What do you think is the line between killing most humans and killing all humans?

1:15:13.400 --> 1:15:19.400
 How hard is it to kill everybody? Have you thought about this?

1:15:19.400 --> 1:15:22.400
 I've thought about it a bit. I think it is very hard to kill everybody.

1:15:22.400 --> 1:15:37.400
 So in the case of, let's say, an all out nuclear exchange, and let's say that leads to nuclear winter, we don't really know, but it might well happen. That would, I think, result in billions of deaths.

1:15:37.400 --> 1:15:45.400
 Would it kill everybody? It's quite hard to see how it would kill everybody for a few reasons.

1:15:45.400 --> 1:15:55.400
 One is just, there's just so many people, seven and a half billion people. So this bad event has to kill all, almost all of them.

1:15:55.400 --> 1:16:09.400
 Secondly, live in such diversity of locations. So a nuclear exchange or the virus, it has to kill people who live in the coast of New Zealand, which is going to be climatically much more stable than other areas in the world.

1:16:09.400 --> 1:16:16.400
 Or people who are on submarines or who have access to bunkers. So there's a very...

1:16:16.400 --> 1:16:25.400
 I'm sure there's two guys in Siberia, just bad ass. There's just human nature, somehow just perseveres.

1:16:25.400 --> 1:16:31.400
 And then the second thing is just, if there's some catastrophic event, people really don't want to die.

1:16:31.400 --> 1:16:37.400
 So there's going to be huge amounts of effort to ensure that it doesn't affect everyone.

1:16:37.400 --> 1:16:47.400
 Have you thought about what it takes to rebuild a society with smaller, smaller numbers, like how big of a setback these kinds of things are?

1:16:47.400 --> 1:16:58.400
 Yeah. So then that's something where there's a real uncertainty, I think, where at some point you just lose sufficient genetic diversity, such that you can't come back.

1:16:58.400 --> 1:17:09.400
 It's unclear how small that population is, but if you've only got, say, a thousand people or fewer than a thousand, then maybe that's small enough.

1:17:09.400 --> 1:17:11.400
 What about human knowledge?

1:17:11.400 --> 1:17:29.400
 And then there's human knowledge. I mean, it's striking how short on geological timescales or evolutionary timescales the progress in, or how quickly the progress in human knowledge has been like agriculture, we only invented in 10,000 BC.

1:17:29.400 --> 1:17:37.400
 Cities were only, you know, 3,000 BC, whereas typical animal species is half a million years to a million years.

1:17:37.400 --> 1:17:51.400
 Do you think it's inevitable in some sense, the agriculture, everything that came, the industrial revolution, cars, planes, the internet, that level of innovation you think is inevitable?

1:17:51.400 --> 1:18:07.400
 I think given how quickly it arose, so in the case of agriculture, I think that was dependent on climate. So it was the kind of glacial period was over, the earth warmed up a bit.

1:18:07.400 --> 1:18:12.400
 That made it much more likely that humans would develop agriculture.

1:18:12.400 --> 1:18:21.400
 When it comes to the industrial revolution, it's just, you know, again, only took a few thousand years from cities to industrial revolution.

1:18:21.400 --> 1:18:32.400
 If we think, okay, we've gone back to this, even let's say agricultural era, but there's no reason why we wouldn't go extinct in the coming tens of thousands of years or hundreds of thousands of years.

1:18:32.400 --> 1:18:39.400
 It seems just that it would be very surprising if we didn't rebound unless there's some special reason that makes things different.

1:18:39.400 --> 1:18:48.400
 So perhaps we just have a much greater disease burden now. So HIV exists, it didn't exist before.

1:18:48.400 --> 1:19:04.400
 And perhaps that's kind of latent in being suppressed by modern medicine and sanitation and so on, but would be a much bigger problem for some utterly destroyed society that was trying to rebound.

1:19:04.400 --> 1:19:08.400
 Or there's just maybe there's something we don't know about.

1:19:08.400 --> 1:19:17.400
 So another existential risk comes from the mysterious, the beautiful artificial intelligence.

1:19:17.400 --> 1:19:22.400
 So what's the shape of your concerns about AI?

1:19:22.400 --> 1:19:30.400
 I think there are quite a lot of concerns about AI and sometimes the different risks don't get distinguished enough.

1:19:30.400 --> 1:19:44.400
 So the kind of classic worry most is closely associated with Nick Bossam and Elias Jukowski is that we at some point move from having narrow AI systems to artificial general intelligence.

1:19:44.400 --> 1:19:53.400
 You get this very fast feedback effect where AI is able to build artificial intelligence helps you to build greater artificial intelligence.

1:19:53.400 --> 1:20:05.400
 We have this one system that's suddenly very powerful, far more powerful than others than perhaps far more powerful than, you know, the rest of the world combined.

1:20:05.400 --> 1:20:10.400
 And then secondly, it has goals that are misaligned with human goals.

1:20:10.400 --> 1:20:12.400
 And so it pursues its own goals.

1:20:12.400 --> 1:20:22.400
 It realizes, hey, there's this competition, namely from humans, it would be better if we eliminated them in just the same way as Homo sapiens eradicated the Neanderthals.

1:20:22.400 --> 1:20:30.400
 In fact, it in fact killed off most large animals on the planet that walked the planet.

1:20:30.400 --> 1:20:33.400
 So that's kind of one set of worries.

1:20:33.400 --> 1:20:40.400
 I think that's not my, I think these shouldn't be dismissed as science fiction.

1:20:40.400 --> 1:20:44.400
 I think it's something we should be taking very seriously.

1:20:44.400 --> 1:20:49.400
 But it's not the thing you visualize when you're concerned about the biggest near term.

1:20:49.400 --> 1:20:55.400
 Yeah, I think it's, I think it's like one possible scenario that would be astronomically bad.

1:20:55.400 --> 1:21:01.400
 I think that other scenarios that would also be extremely bad, comparably bad, are more likely to occur.

1:21:01.400 --> 1:21:05.400
 So one is just we are able to control AI.

1:21:05.400 --> 1:21:09.400
 So we're able to get it to do what we want it to do.

1:21:09.400 --> 1:21:19.400
 And perhaps there's not like this fast takeoff of AI capabilities within a single system, it's distributed across many systems that do somewhat different things.

1:21:19.400 --> 1:21:30.400
 But you do get very rapid economic and technological progress as a result that concentrates power into the hands of a very small number of individuals, perhaps a single dictator.

1:21:30.400 --> 1:21:46.400
 And secondly, that single individual is or small group of individuals or single country is then able to like lock in their values indefinitely via transmitting those values to artificial systems that have no reason to die.

1:21:46.400 --> 1:21:49.400
 Like, you know, their code is copyable.

1:21:49.400 --> 1:22:06.400
 Perhaps, you know, Donald Trump or Xi Jinping creates their kind of AI progeny and an image. And once you have a system that's content, once you have a society that's controlled by AI, you no longer have one of the main drivers of change

1:22:06.400 --> 1:22:12.400
 historically, which is the fact that human life spans are, you know, only 100 years give or take.

1:22:12.400 --> 1:22:28.400
 That's really interesting. So as opposed to sort of killing off all humans is locking in and creating a hell on earth, basically a set of principles under which the society operates that's extremely undesirable.

1:22:28.400 --> 1:22:30.400
 So everybody is suffering indefinitely.

1:22:30.400 --> 1:22:46.400
 Or it doesn't. I mean, it also doesn't need to be hell on earth. It could just be the wrong values. So we talked at the very beginning about how I want to see this kind of diversity of different values and exploration so that we can just work out what is kind of morally like

1:22:46.400 --> 1:22:49.400
 what is good, what is bad, and then pursue the thing that's best.

1:22:49.400 --> 1:23:01.400
 So actually, so the idea of wrong values is actually probably the beautiful thing is there's no such thing as right and wrong values because we don't know the right answer.

1:23:01.400 --> 1:23:12.400
 We just kind of have a sense of which value is more right, which is more wrong. So any kind of lock in makes a value wrong, because it prevents exploration of this kind.

1:23:12.400 --> 1:23:23.400
 Yeah. And just, you know, imagine if fascist value, you know, imagine if there was Hitler's utopia or Stalin's utopia or Donald Trump's or Xi Jinping's forever.

1:23:23.400 --> 1:23:25.400
 Yeah.

1:23:25.400 --> 1:23:36.400
 You know, how, how good or bad would that be compared to the best possible future we could create. And my suggestion is it really suck compared to the best possible future we could create.

1:23:36.400 --> 1:23:45.400
 And you're just one individual. There's some individuals for whom Donald Trump is perhaps the best possible future.

1:23:45.400 --> 1:23:50.400
 And so that's the whole point of us individuals exploring the space together.

1:23:50.400 --> 1:23:51.400
 Exactly. Yeah.

1:23:51.400 --> 1:23:56.400
 And what's trying to figure out which is the path that will make America great again.

1:23:56.400 --> 1:23:57.400
 Yeah, exactly.

1:23:57.400 --> 1:24:16.400
 So how can effective altruism help? I mean, this is a really interesting notion they actually describing of artificial intelligence being used as extremely powerful technology in the hands of very few potentially one person to create some very undesirable effect.

1:24:16.400 --> 1:24:25.400
 So as opposed to AI, and again, the source of the undesirableness there is the human AI is just a really powerful tool.

1:24:25.400 --> 1:24:31.400
 So whether it's that or whether AI is AI just runs away from us completely.

1:24:31.400 --> 1:24:40.400
 How, as individuals, as, as people in the effective altruism movement, how can we think about something like this?

1:24:40.400 --> 1:24:42.400
 Understand poverty and welfare.

1:24:42.400 --> 1:24:47.400
 But this is a far out incredibly mysterious and difficult problem.

1:24:47.400 --> 1:24:50.400
 Great. Well, I think there's three paths as an individual.

1:24:50.400 --> 1:24:55.400
 So if you're thinking about, you know, career paths, you can pursue.

1:24:55.400 --> 1:24:58.400
 So one is going down the line of technical AI safety.

1:24:58.400 --> 1:25:11.400
 So this is most relevant to the kind of AI winning AI taking over scenarios where this is just technical work on current machine learning systems.

1:25:11.400 --> 1:25:21.400
 Often sometimes going more theoretical to on how we can ensure that an AI is able to learn human values and able to act in the way that you want it to act.

1:25:21.400 --> 1:25:27.400
 And that's a pretty mainstream issue and approach in machine learning today.

1:25:27.400 --> 1:25:31.400
 So, you know, we definitely need more people doing that.

1:25:31.400 --> 1:25:40.400
 Second is on the policy side of things, which I think is even more important at the moment, which is how should developments in AI be managed?

1:25:40.400 --> 1:25:47.400
 On a political level, how can you ensure that the benefits of AI are very distributed?

1:25:47.400 --> 1:25:55.400
 Power isn't being concentrated in the hands of a small set of individuals.

1:25:55.400 --> 1:26:06.400
 How do you ensure that there aren't arms races between different AI companies that might result in them, you know, cutting corners with respect to safety?

1:26:06.400 --> 1:26:13.400
 And so there the input as individuals who can have is this, we're not talking about money, we're talking about effort.

1:26:13.400 --> 1:26:15.400
 We're talking about career choices.

1:26:15.400 --> 1:26:17.400
 Yeah, we're talking about career choice. Yeah.

1:26:17.400 --> 1:26:23.400
 But then it is the case that supposing, you know, you're like, I've already decided my career and I'm doing something quite different.

1:26:23.400 --> 1:26:30.400
 You can contribute with money to where at the center for the effect of autism, we set up the long term future fund.

1:26:30.400 --> 1:26:46.400
 So if you go on to effectiveautism.org, you can donate where a group of individuals will then work out what's the highest value place they can donate to work on existential risk issues with a particular focus on AI.

1:26:46.400 --> 1:26:48.400
 And what's path number three?

1:26:48.400 --> 1:26:49.400
 This was path number three.

1:26:49.400 --> 1:26:53.400
 This is the donations with the third option I was thinking of.

1:26:53.400 --> 1:27:08.400
 And then, yeah, you can also donate directly to organizations working on this like Center for Human Compatible AI at Berkeley, Future of Humanity Institute at Oxford, or other organizations too.

1:27:08.400 --> 1:27:10.400
 Does AI keep you up at night?

1:27:10.400 --> 1:27:12.400
 This kind of concern?

1:27:12.400 --> 1:27:19.400
 Yeah, it's kind of a mix where I think it's very likely things are going to go well.

1:27:19.400 --> 1:27:25.400
 I think we're going to be able to solve these problems. I think that's by far the most likely outcome, at least over the next.

1:27:25.400 --> 1:27:26.400
 By far the most likely.

1:27:26.400 --> 1:27:41.400
 So if you look at all the trajectories running away from our current moment in the next 100 years, you see AI creating destructive consequences as a small subset of those possible trajectories.

1:27:41.400 --> 1:27:46.400
 Or at least, yeah, kind of eternal, disruptive consequences. I think that being a small subset.

1:27:46.400 --> 1:27:48.400
 At the same time, it still freaks me out.

1:27:48.400 --> 1:27:57.400
 I mean, when we're talking about the entire future of civilization, then small probabilities, 1% probability, that's terrifying.

1:27:57.400 --> 1:28:05.400
 What do you think about Elon Musk's strong worry that we should be really concerned about existential risks of AI?

1:28:05.400 --> 1:28:09.400
 Yeah, I mean, I think, broadly speaking, I think he's right.

1:28:09.400 --> 1:28:16.400
 I think if we talked, we would probably have very different probabilities on how likely it is that we're doomed.

1:28:16.400 --> 1:28:23.400
 But again, when it comes to talking about the entire future of civilization, it doesn't really matter if it's 1% or if it's 50%.

1:28:23.400 --> 1:28:29.400
 We ought to be taking every possible safeguard we can to ensure that things go well rather than poorly.

1:28:29.400 --> 1:28:35.400
 Last question. If you yourself could eradicate one problem from the world, what would that problem be?

1:28:35.400 --> 1:28:45.400
 That's a great question. I don't know if I'm cheating in saying this, but I think the thing I would most want to change is just the fact that people...

1:28:45.400 --> 1:28:50.400
 don't actually care about ensuring the long run future goes well.

1:28:50.400 --> 1:28:54.400
 People don't really care about future generations. They don't think about it. It's not part of their aims.

1:28:54.400 --> 1:29:05.400
 Well, in some sense, you're not cheating at all because in speaking the way you do and writing the things you're writing, you're addressing exactly this aspect.

1:29:05.400 --> 1:29:06.400
 Exactly.

1:29:06.400 --> 1:29:10.400
 That is your input into the effective altruism movement.

1:29:10.400 --> 1:29:14.400
 So for that, well, thank you so much. It's an honor to talk to you. I really enjoyed it.

1:29:14.400 --> 1:29:16.400
 Thanks so much for having me on.

1:29:16.400 --> 1:29:22.400
 Thanks for listening to this conversation with William McCaskill and thank you to our presenting sponsor, Cash App.

1:29:22.400 --> 1:29:28.400
 Please consider supporting the podcast by downloading Cash App and using code lexpodcast.

1:29:28.400 --> 1:29:39.400
 If you enjoy this podcast, subscribe on YouTube, review it with 5 stars on Apple Podcast, support on Patreon, or simply connect with me on Twitter at lexfreedman.

1:29:39.400 --> 1:29:44.400
 And now, let me leave you with some words from William McCaskill.

1:29:44.400 --> 1:29:56.400
 One additional unit of income can do 100 times as much to benefit the extreme poor as it can to benefit you or I, earning the typical US wage of $28,000 a year.

1:29:56.400 --> 1:30:01.400
 It's not often that you have two options, one of which is 100 times better than the other.

1:30:01.400 --> 1:30:09.400
 Imagine a happy hour where you can either buy yourself a beer for $5 or buy someone else a beer for $0.05.

1:30:09.400 --> 1:30:14.400
 If that were the case, we'd probably be pretty generous next rounds on me.

1:30:14.400 --> 1:30:18.400
 But that's effectively the situation we're in all the time.

1:30:18.400 --> 1:30:23.400
 It's like a 99% off sale or buy one get 99 free.

1:30:23.400 --> 1:30:27.400
 It might be the most amazing deal you'll see in your life.

1:30:27.400 --> 1:30:32.400
 Thank you for listening and hope to see you next time.

