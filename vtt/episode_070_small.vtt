WEBVTT

00:00.000 --> 00:10.000
 The following is a conversation with Jim Keller, legendary microprocessor engineer who has worked at AMD, Apple, Tesla, and now Intel.

00:10.000 --> 00:18.000
 He's known for his work on AMD K7, K8, K12, and Zen microarchitectures, Apple A4 and A5 processors,

00:18.000 --> 00:26.000
 and coauthor of the specification for the X8664 instruction set and hypertransport interconnect.

00:26.000 --> 00:33.000
 He's a brilliant first principles engineer and out of the box thinker and just an interesting and fun human being to talk to.

00:33.000 --> 00:36.000
 This is the Artificial Intelligence Podcast.

00:36.000 --> 00:45.000
 If you enjoy it, subscribe on YouTube, give it 5 stars on Apple Podcast, follow on Spotify, support it on Patreon, or simply connect with me on Twitter.

00:45.000 --> 00:49.000
 Alex Friedman, spelled F R I D M A N.

00:49.000 --> 00:59.000
 I recently started doing ads at the end of the introduction. I'll do one or two minutes after introducing the episode and never any ads in the middle that can break the flow of the conversation.

00:59.000 --> 01:03.000
 I hope that works for you. It doesn't hurt the listening experience.

01:03.000 --> 01:08.000
 This show is presented by Cash App, the number one finance app in the App Store.

01:08.000 --> 01:15.000
 I personally use Cash App to send money to friends, but you can also use it to buy, sell, and deposit Bitcoin in just seconds.

01:15.000 --> 01:23.000
 Cash App also has a new investing feature. You can buy fractions of a stock, say $1 worth, no matter what the stock price is.

01:23.000 --> 01:29.000
 Roker's services are provided by Cash App Investing, a subsidiary of Square and member SIPC.

01:29.000 --> 01:38.000
 I'm excited to be working with Cash App to support one of my favorite organizations called FIRST, best known for their first robotics and legal competitions.

01:38.000 --> 01:50.000
 They educate and inspire hundreds of thousands of students in over 110 countries and have a perfect rating on charity navigator, which means that donated money is used to maximum effectiveness.

01:50.000 --> 01:38.000
 When you get Cash App from the App Store, Google Play, and use code LEX

01:38.000 --> 02:07.000
 You'll get $10 and Cash App will also donate $10 to FIRST, which again is an organization that I've personally seen inspire girls and boys to dream of engineering a better world.

02:08.000 --> 02:12.000
 And now, here's my conversation with Jim Keller.

02:12.000 --> 02:22.000
 What are the differences and similarities between the human brain and a computer with the microprocessor at its core? Let's start with the philosophical question, perhaps.

02:22.000 --> 02:29.000
 Well, since people don't actually understand how human brains work, I think that's true.

02:29.000 --> 02:30.000
 I think that's true.

02:30.000 --> 02:54.000
 So it's hard to compare them.

02:54.000 --> 02:58.000
 Computers are, you know, there's really two things.

02:58.000 --> 03:24.000
 You think in the human brain, everything's a mesh, a mess that's combined together.

03:24.000 --> 03:50.000
 I don't know that the understanding of that is super deep.

03:50.000 --> 04:18.000
 What is a microprocessor, what is a microarchitecture? What's an instruction set architecture?

04:18.000 --> 04:23.840
 transistors. On top of that, we build logic gates, right, and

04:23.840 --> 04:27.480
 then functional units, like an adder, a subtractor, an

04:27.480 --> 04:30.480
 instruction parsing unit, and then we assemble those into,

04:31.000 --> 04:34.040
 you know, processing elements, modern computers are built out

04:34.040 --> 04:39.840
 of, you know, probably 10 to 20 locally, you know, organic

04:39.840 --> 04:42.880
 processing elements or coherent processing elements, and then

04:42.880 --> 04:47.560
 that runs computer programs. Right. So there's abstraction

04:47.560 --> 04:50.840
 layers, and then software, you know, there's an instruction set

04:50.840 --> 04:54.360
 you run. And then there's assembly language C, C plus

04:54.360 --> 04:57.800
 plus Java JavaScript, you know, there's abstraction layers,

04:58.200 --> 05:02.000
 you know, essentially from the atom to the data center. Right.

05:02.440 --> 05:07.160
 So when you when you build a computer, you know, first, there's

05:07.160 --> 05:09.920
 a target like what's it for, like how fast does it have to be,

05:09.920 --> 05:12.480
 which, you know, today, there's a whole bunch of metrics about

05:12.480 --> 05:16.720
 what that is. And then in an organization of, you know, 1000

05:16.720 --> 05:20.600
 people who build a computer, there's lots of different

05:20.600 --> 05:24.920
 disciplines that you have to operate on. Does that make sense?

05:25.480 --> 05:26.200
 And so

05:27.160 --> 05:32.400
 there's a bunch of levels of abstraction of in an organization

05:32.400 --> 05:36.320
 I can tell, and in your own vision, there's a lot of

05:36.320 --> 05:40.080
 brilliance that comes in at every one of those layers. Some of

05:40.080 --> 05:42.760
 it is science, some of it is engineering, some of it is art.

05:42.760 --> 05:46.360
 What's the most, if you could pick favorites, what's the most

05:46.360 --> 05:50.480
 important, your favorite layer on these layers of abstractions?

05:50.480 --> 05:53.280
 Where does the magic enter this hierarchy?

05:54.760 --> 05:58.920
 I don't really care. That's the fun, you know, I'm somewhat

05:58.920 --> 06:04.480
 agnostic to that. So I would say, for relatively long periods

06:04.480 --> 06:09.160
 of time, instruction sets are stable. So the x86 instruction

06:09.160 --> 06:10.720
 set, the arm instruction set.

06:10.720 --> 06:14.320
 What's an instruction set? So it says, how do you encode the

06:14.320 --> 06:17.440
 basic operations, load, store, multiply, add, subtract,

06:17.440 --> 06:21.440
 conditional branch, you know, there aren't that many

06:21.440 --> 06:24.240
 interesting instructions. Look, if you look at a program and it

06:24.240 --> 06:28.920
 runs, you know, 90% of the execution is on 25 op codes,

06:28.960 --> 06:32.960
 you know, 25 instructions on those are stable. Right?

06:32.960 --> 06:34.320
 What does it mean stable?

06:34.320 --> 06:36.920
 Intel architecture has been around for 25 years.

06:36.920 --> 06:40.000
 It works. It works. And that's because the

06:40.000 --> 06:45.520
 basics, you know, are defined a long time ago. Right? Now, the

06:45.520 --> 06:49.480
 way an old computer ran is you fetched instructions and you

06:49.480 --> 06:54.720
 executed them in order. Do the load, do the add, do the

06:54.720 --> 06:59.440
 compare. The way a modern computer works is you fetch large

06:59.440 --> 07:04.480
 numbers of instructions, say 500. And then you find the

07:04.480 --> 07:09.520
 dependency graph between the instructions. And then you execute

07:09.520 --> 07:15.440
 in independent units, those little micrographs. So a modern

07:15.440 --> 07:18.400
 computer, like people like to say computers should be simple

07:18.400 --> 07:22.320
 and clean. But it turns out the market for a simple complete

07:22.320 --> 07:27.040
 clean slow computers is zero. Right? We don't sell any simple

07:27.040 --> 07:31.840
 clean computers. Now you can, there's how you build it can

07:31.840 --> 07:36.960
 be clean, but the computer people want to buy, let's say in a

07:36.960 --> 07:42.320
 phone or data center, fetches a large number of instructions,

07:42.320 --> 07:47.040
 computes the dependency graph, and then executes it in a way

07:47.040 --> 07:48.720
 that gets the right answers.

07:48.720 --> 07:50.400
 And optimize that graph somehow.

07:50.400 --> 07:54.480
 Yeah, they run deeply out of order. And then there's

07:54.480 --> 07:57.440
 semantics around how memory ordering works and other

07:57.440 --> 08:00.400
 things work. So the computer sort of has a bunch of

08:00.400 --> 08:04.160
 bookkeeping tables that says what order cities operations

08:04.160 --> 08:09.200
 should finish in or appear to finish in. But to go fast, you

08:09.200 --> 08:11.840
 have to fetch a lot of instructions and find all the

08:11.840 --> 08:16.160
 parallelism. Now there's a second kind of computer, which we

08:16.160 --> 08:20.640
 call GPUs today. And I call it the difference. There's found

08:20.640 --> 08:22.800
 parallelism, like you have a program with a lot of

08:22.800 --> 08:26.000
 dependent instructions, you fetch a bunch and then you go

08:26.000 --> 08:28.640
 figure out the dependency graph and you issues instructions

08:28.640 --> 08:32.240
 at order. That's because you have one serial narrative to

08:32.240 --> 08:36.000
 use, which in fact is in can be done out of order.

08:36.000 --> 08:37.840
 Did you call it a narrative? Yeah.

08:37.840 --> 08:41.440
 Wow. So yeah, so humans think of serial narrative. So read a

08:41.440 --> 08:44.480
 book, right? There's a you know, there's a sentence after

08:44.480 --> 08:47.280
 sentence after sentence, and there's paragraphs. Now you

08:47.280 --> 08:51.200
 could diagram that. Imagine you diagrammed it properly and

08:51.200 --> 08:56.080
 you said, which sentences could be read in anti order, any

08:56.080 --> 09:00.080
 order without changing the meaning, right?

09:00.080 --> 09:02.800
 It's a fascinating question to ask of a book. Yeah. Yeah, you

09:02.800 --> 09:05.600
 could do that. Right? So some paragraphs could be

09:05.600 --> 09:10.400
 reordered, some sentences can be reordered. You could say he

09:10.400 --> 09:17.200
 is tall and smart and X, right? And it doesn't matter the order

09:17.200 --> 09:22.160
 of tall and smart. But if you say the tall man is wearing a

09:22.160 --> 09:26.240
 red shirt, what colors, you know, like you can create

09:26.240 --> 09:31.920
 dependencies, right? Right. And so GPUs, on the other hand,

09:31.920 --> 09:36.480
 run simple programs on pixels, but you're given a million of

09:36.480 --> 09:40.080
 them. And the first order, the screen you're looking at

09:40.080 --> 09:43.280
 doesn't care which order you do it in. So I call that given

09:43.280 --> 09:46.880
 parallelism, simple narratives around the large numbers of

09:46.880 --> 09:51.360
 things where you can just say it's parallel because you told

09:51.360 --> 09:55.920
 me it was. So found parallelism where the narrative

09:55.920 --> 10:00.160
 is sequential, but you discover like little pockets

10:00.160 --> 10:02.880
 of parallelism versus. Turns out large pockets of

10:02.880 --> 10:05.840
 parallelism. Large. So how hard is it to discover?

10:05.840 --> 10:08.720
 Well, how hard is it? That's just transistor count, right?

10:08.720 --> 10:12.000
 So once you crack the problem, you say here's how you fetch

10:12.000 --> 10:14.720
 10 instructions at a time, here's how you calculate the

10:14.720 --> 10:17.360
 dependencies between them, here's how you describe the

10:17.360 --> 10:21.680
 dependencies, here's, you know, these are pieces, right?

10:21.680 --> 10:26.080
 So, Anna, once you describe the dependencies, then it's just

10:26.080 --> 10:31.520
 a graph, sort of, it's an algorithm that finds, what is

10:31.520 --> 10:34.320
 that? I'm sure there's a graph theory, a theoretical answer

10:34.320 --> 10:40.560
 here that's solvable. In general, programs, modern programs

10:40.560 --> 10:44.320
 that human beings write, how much found parallelism is

10:44.320 --> 10:47.120
 there in them? About 10x. What does 10x mean?

10:47.120 --> 10:52.000
 Well, you execute it in order. Versus, yeah. You would get

10:52.000 --> 10:54.480
 what's called cycles per instruction and it would be

10:54.480 --> 10:58.640
 about, you know, three instructions, three cycles

10:58.640 --> 11:01.520
 per instruction because of the latency of the operations

11:01.520 --> 11:05.440
 and stuff. And in a modern computer, execute it like

11:05.440 --> 11:09.600
.2,.25 cycles per instruction. So it's about,

11:09.600 --> 11:14.000
 we today find 10x. And there's two things. One is

11:14.000 --> 11:17.840
 the found parallelism in the narrative, right? And the other

11:17.840 --> 11:23.120
 is the predictability of the narrative, right? So certain

11:23.120 --> 11:26.320
 operations, they do a bunch of calculations and if greater

11:26.320 --> 11:31.360
 than one, do this, else do that. That, that decision is

11:31.360 --> 11:37.120
 predicted in modern computers to high 90% accuracy. So

11:37.120 --> 11:39.920
 branches happen a lot. So imagine you have, you have a

11:39.920 --> 11:42.160
 decision to make every six instructions, which is about

11:42.160 --> 11:44.800
 the average, right? But you want to fetch 500

11:44.800 --> 11:46.880
 instructions, figure out the graph and execute them all

11:46.880 --> 11:51.680
 in parallel. That means you have, let's say, if you

11:51.680 --> 11:55.440
 affect 600 instructions and it's every six, you have to

11:55.440 --> 11:58.640
 fetch, you have to predict 99 out of 100 branches

11:58.640 --> 12:02.240
 correctly for that window to be effective.

12:02.240 --> 12:06.640
 Okay. So parallelism, you can't parallelize branches

12:06.640 --> 12:10.080
 or you can. You can predict. What does predict a branch

12:10.080 --> 12:12.880
 mean? What's predicted? So imagine you do a computation

12:12.880 --> 12:16.800
 over and over. You're in a loop. Yep. So while n is greater

12:16.800 --> 12:20.640
 than one, do. And you go through that loop a million

12:20.640 --> 12:23.440
 times. So every time you look at the branch, you say, it's

12:23.440 --> 12:26.320
 probably still greater than one. And you're saying you could

12:26.320 --> 12:29.280
 do that accurately. Very accurately. Modern computer. My

12:29.280 --> 12:32.480
 mind is blown. How the heck do you do that? Wait a minute.

12:32.480 --> 12:36.720
 Well, you want to know? This is really sad. 20 years ago.

12:36.720 --> 12:40.560
 Yes. You simply recorded which way the branch went last time

12:40.560 --> 12:45.040
 and predicted the same thing. Right. Okay. What's the accuracy

12:45.040 --> 12:50.160
 of that? 85%. So then somebody said, hey, let's keep a

12:50.160 --> 12:53.440
 couple of bits and have a little counter. So when it

12:53.440 --> 12:57.280
 predicts one way, we count up and then pins. So say you have

12:57.280 --> 13:00.320
 a three bit counter. So you count up and then you count

13:00.320 --> 13:02.720
 down. And if it's, you know, you can use the top bit as a

13:02.720 --> 13:05.360
 signed bit. So you have a signed two bit number. So if it's

13:05.360 --> 13:09.200
 greater than one, you predict taken and less than one, you

13:09.200 --> 13:12.640
 predict not taken, right? Or less than zero, whatever the

13:12.640 --> 13:18.160
 thing is. And that got us to 92%. Oh. Okay, you know, it's

13:18.160 --> 13:23.280
 better. This branch depends on how you got there. So if you

13:23.280 --> 13:26.720
 came down the code one way, you're talking about Bob and

13:26.720 --> 13:30.720
 Jane, right? And then said is just Bob like Jane, it went

13:30.720 --> 13:33.280
 one way. But if you're talking about Bob and Jill, just Bob

13:33.280 --> 13:36.240
 like Jane, you go a different way, right? So that's called

13:36.240 --> 13:40.080
 history. So you take the history and a counter. That's

13:40.080 --> 13:43.440
 cool. But that's not how anything works today. They

13:43.440 --> 13:48.160
 use something that looks a little like a neural network. So

13:48.160 --> 13:53.200
 modern, you take all the execution flows. And then you

13:53.200 --> 13:57.520
 do basically deep pattern recognition of how the program

13:57.520 --> 14:03.840
 is executing. And you do that multiple different ways. And

14:03.840 --> 14:07.440
 you have something that chooses what the best result is.

14:07.440 --> 14:10.640
 There's a little supercomputer inside the computer. That's

14:10.640 --> 14:14.400
 trying to predict that calculates which way branches go. So

14:14.400 --> 14:17.600
 the effective window that it's worth finding grass and gets

14:17.600 --> 14:22.240
 bigger. Why was that gonna make me sad? Because that's

14:22.240 --> 14:25.680
 amazing. It's amazingly complicated. Oh, well, here's

14:25.680 --> 14:30.160
 the here's the funny thing. So to get to 85% took a

14:30.160 --> 14:38.960
 thousand bits. To get to 99% takes tens of megabits. So

14:38.960 --> 14:42.960
 this is one of those to get the result, you know, to get

14:42.960 --> 14:48.080
 from a window of say, 50 instructions to 500. It took

14:48.080 --> 14:50.560
 three orders of magnitude or four orders of magnitude

14:50.560 --> 14:54.320
 toward bits. Now, if you get the prediction of a branch

14:54.320 --> 14:58.080
 wrong, what happens then flush the pipe flush the pipe. So

14:58.080 --> 15:01.280
 it's just a performance cost. But it gets even better. Yeah.

15:01.280 --> 15:04.640
 So we're starting to look at stuff that says so executed

15:04.640 --> 15:10.720
 down this path. And then you had two ways to go. But far, far

15:10.720 --> 15:12.960
 away, there's something that doesn't matter which path

15:12.960 --> 15:17.680
 you went. So you missed you took the wrong path, you

15:17.680 --> 15:21.040
 executed a bunch of stuff. Then you had the

15:21.040 --> 15:23.360
 mispredicting to backed it up. But you remembered all the

15:23.360 --> 15:26.480
 results you already calculated. Some of those are just

15:26.480 --> 15:29.600
 fine. Like if you read a book and you misunderstand a

15:29.600 --> 15:32.480
 paragraph, your understanding of the next paragraph,

15:32.480 --> 15:36.640
 sometimes is invariant to the understanding. Sometimes it

15:36.640 --> 15:41.760
 depends on it. And you can kind of anticipate that

15:41.760 --> 15:46.080
 invariance. Yeah, well, you can keep track of whether the

15:46.080 --> 15:48.880
 data changed. And so when you come back to me a piece of

15:48.880 --> 15:51.760
 code, should you calculate it again or do the same thing?

15:51.760 --> 15:55.440
 Okay, how much of this is art and how much of it is science?

15:55.440 --> 15:59.440
 Because it sounds pretty complicated. Well, how do you

15:59.440 --> 16:02.000
 describe a situation? So imagine you come to a point in the

16:02.000 --> 16:05.360
 road where you have to make a decision. And you have a

16:05.360 --> 16:07.520
 bunch of knowledge about which way to go. Maybe you have a

16:07.520 --> 16:11.840
 map. So you want to go the shortest way. Or do you want to

16:11.840 --> 16:14.240
 go the fastest way? Or do you want to take the nicest

16:14.240 --> 16:18.400
 road? So there's some set of data. So imagine you're

16:18.400 --> 16:21.920
 doing something complicated like building a computer. And

16:21.920 --> 16:25.600
 there's hundreds of decision points, all with hundreds of

16:25.600 --> 16:29.680
 possible ways to go. And the ways you pick interact in a

16:29.680 --> 16:34.400
 complicated way. Right. And then you have to pick the right

16:34.400 --> 16:37.360
 spot. Right. So that's an order of science. I don't know.

16:37.360 --> 16:40.880
 You avoided the question. You just described the Robert Frost

16:40.880 --> 16:45.040
 poem of road less taken. I described the Robin Frost

16:45.040 --> 16:49.600
 problem. That's what we do as computer designers. It's all

16:49.600 --> 16:54.080
 poetry. Okay. Great. Yeah, I don't know how to describe that

16:54.080 --> 16:57.040
 because some people are very good at making those

16:57.040 --> 16:59.760
 intuitive leaps. It seems like the this combinations of

16:59.760 --> 17:02.800
 things. Some people are less good at it, but they're really

17:02.800 --> 17:06.080
 good at evaluating the alternatives. Right. And

17:06.080 --> 17:10.800
 everybody has a different way to do it. And some people can't

17:10.800 --> 17:14.160
 make those leaps, but they're really good at analyzing it.

17:14.160 --> 17:16.720
 So when you see computers are designed by teams of people

17:16.720 --> 17:22.320
 who have very different skill sets and a good team has lots

17:22.320 --> 17:25.600
 of different kinds of people. I suspect you would describe

17:25.600 --> 17:30.320
 some of them as artistic. Right. But not very many.

17:30.320 --> 17:35.760
 Unfortunately. Or fortunately. Well, you know, computer

17:35.760 --> 17:41.440
 design is hard. It's 99% perspiration. And the 1%

17:41.440 --> 17:45.680
 perspiration is really important. But you still need the 99.

17:45.680 --> 17:49.520
 Yeah, you gotta do a lot of work. And then there's there are

17:49.520 --> 17:54.320
 interesting things to do at every level that stack. So at the

17:54.320 --> 17:58.720
 end of the day, if you run the same program multiple times,

17:58.720 --> 18:02.800
 does it always produce the same result? Is is is there some

18:02.800 --> 18:07.120
 room for fuzziness there? That's a math problem. So if you

18:07.120 --> 18:11.120
 run a correct C program, the definition is every time you

18:11.120 --> 18:13.680
 run it, you get the same answer. Yeah, well, that's a math

18:13.680 --> 18:15.840
 statement. But that's a that's a language

18:15.840 --> 18:20.080
 definitional statement. So for years, when people did when we

18:20.080 --> 18:25.440
 first did 3D acceleration of graphics, you could run the

18:25.440 --> 18:29.600
 same scene multiple times and get different answers. Right.

18:29.600 --> 18:32.400
 Right. And then some people thought that was okay. And

18:32.400 --> 18:35.760
 some people thought it was a bad idea. And then when the

18:35.760 --> 18:39.520
 HPC world used GPUs for calculations, they thought it

18:39.520 --> 18:44.320
 was a really bad idea. Okay. Now, in modern AI stuff,

18:44.320 --> 18:49.200
 people are looking at networks where the precision of the

18:49.200 --> 18:53.920
 data is low enough that the data is somewhat noisy. And the

18:53.920 --> 18:57.600
 observation is the input data is unbelievably noisy. So why

18:57.600 --> 19:00.640
 should the calculation be not noisy? And people have

19:00.640 --> 19:04.320
 experimented with algorithms that say can get faster answers

19:04.320 --> 19:08.160
 by being noisy. Like as the network starts to converge, if

19:08.160 --> 19:10.240
 you look at the computation graph, it starts out really

19:10.240 --> 19:13.200
 wide and it gets narrower. And you can say is that last

19:13.200 --> 19:15.680
 little bit that important? Or should I start the graph on

19:15.680 --> 19:19.680
 the next rev before we whittle it all the way down to the

19:19.680 --> 19:22.640
 answer? Right. So you can create algorithms that are

19:22.640 --> 19:25.760
 noisy. Now, if you're developing something and every

19:25.760 --> 19:27.920
 time you run it, you get a different answer. It's really

19:27.920 --> 19:34.240
 annoying. And so most people think even today, every time

19:34.240 --> 19:37.040
 you run the program, you get the same answer. No, I know.

19:37.040 --> 19:40.960
 But the question is, that's the formal definition of a

19:40.960 --> 19:44.640
 programming language. There is a definition of languages that

19:44.640 --> 19:48.640
 don't get the same answer. But people who use those, you

19:48.640 --> 19:50.960
 always want something because you get a bad answer. And then

19:50.960 --> 19:54.320
 you're wondering, is it because of something in the algorithm

19:54.320 --> 19:56.720
 or because of this? And so everybody wants a little switch

19:56.720 --> 20:00.560
 that says no matter what, do it deterministically. And it's

20:00.560 --> 20:03.200
 really weird because almost everything going into modern

20:03.200 --> 20:07.440
 calculations is noisy. So the answers have to be so clear.

20:07.440 --> 20:10.960
 Right. So where do you stand? I design computers for people who

20:10.960 --> 20:15.440
 run programs. So somebody says, I want a deterministic

20:15.440 --> 20:19.200
 answer. Like most people want that. Can you deliver a

20:19.200 --> 20:22.160
 deterministic answer? I guess is the question. Like when you

20:22.160 --> 20:25.840
 Yeah, hopefully, sure. What people don't realize is you get

20:25.840 --> 20:28.880
 a deterministic answer even though the execution flow is

20:28.880 --> 20:33.040
 very undeterministic. So you run this program 100 times.

20:33.040 --> 20:37.040
 It never runs the same way twice, ever. And the answer, it

20:37.040 --> 20:38.800
 arrives at the same answer. But it gets the same answer

20:38.800 --> 20:43.680
 every time. It's just, it's just amazing. Okay, you've

20:43.680 --> 20:50.800
 achieved in the eyes of many people, legend status as a

20:50.800 --> 20:55.360
 chip art architect, what design creation are you most proud

20:55.360 --> 21:00.000
 of? Perhaps because it was challenging, because of its

21:00.000 --> 21:04.960
 impact or because of the set of brilliant ideas that that

21:04.960 --> 21:10.160
 were involved in. Well, I find that description odd and I

21:10.160 --> 21:14.720
 have two small children and I promise you they think it's

21:14.720 --> 21:20.560
 hilarious. This question. Yeah. So I do it for them. So I am

21:20.560 --> 21:24.160
 I'm really interested in building computers and I've worked

21:24.160 --> 21:28.400
 with really, really smart people. I'm not unbelievable

21:28.400 --> 21:33.360
 to be smart. I'm fascinated by how they go together both as a

21:33.360 --> 21:38.400
 as a thing to do and as endeavor that people do. How

21:38.400 --> 21:41.360
 people and computers go together. Yeah, like how people

21:41.360 --> 21:45.600
 think and build a computer. And I find sometimes that the

21:45.600 --> 21:49.360
 best computer architects aren't that interested in people or

21:49.360 --> 21:52.320
 the best people managers aren't that good at designing

21:52.320 --> 21:56.400
 computers. So the whole stack of human beings is

21:56.400 --> 21:59.760
 fascinating. So the managers, the individual engineers. So

21:59.760 --> 22:02.640
 yeah, I said I realized after a lot of years of building

22:02.640 --> 22:04.560
 computers, where you sort of build them out of

22:04.560 --> 22:07.520
 transistors, logic gates, functional units, computational

22:07.520 --> 22:10.880
 elements, that you could think of people the same way. So

22:10.880 --> 22:13.440
 people are functional units. Yes. And then you can think of

22:13.440 --> 22:16.800
 organizational design as a computer architectural problem.

22:16.800 --> 22:20.000
 And then it's like, oh, that's super cool because the people

22:20.000 --> 22:22.400
 are all different just like the computational elements are

22:22.400 --> 22:26.320
 all different. And they like to do different things and

22:26.320 --> 22:29.760
 so I had a lot of fun like reframing how I think about

22:29.760 --> 22:34.320
 organizations. Just like with with computers, we were

22:34.320 --> 22:36.880
 saying execution paths, you can have a lot of different

22:36.880 --> 22:41.600
 paths that end up at a at at the same good destination. So

22:41.600 --> 22:45.840
 what have you learned about the human abstractions from

22:45.840 --> 22:50.240
 individual functional human units to the the broader

22:50.240 --> 22:53.440
 organization? What what does it take to create something

22:53.440 --> 23:00.080
 special? Well, most people don't think simple enough.

23:00.080 --> 23:02.640
 All right. So do you know the difference between a recipe

23:02.640 --> 23:07.760
 and the understanding? There's probably a philosophical

23:07.760 --> 23:10.000
 description of this. So imagine you're gonna make a loaf

23:10.000 --> 23:13.360
 for bread. Yep. The recipe says get some flour, add some

23:13.360 --> 23:17.120
 water, add some yeast, mix it up, let it rise, put it in a

23:17.120 --> 23:21.200
 pan, put it in the oven. It's a recipe. Right.

23:21.200 --> 23:25.040
 Understanding bread. You can understand biology, supply

23:25.040 --> 23:32.000
 chains, you know, rain grinders, yeast, physics, you

23:32.000 --> 23:35.520
 know, thermodynamics, like there's so many levels of

23:35.520 --> 23:39.200
 understanding there. And then when people build and design

23:39.200 --> 23:42.640
 things, they frequently are executing some stack of

23:42.640 --> 23:46.800
 recipes. Right. And the problem with that is the recipes

23:46.800 --> 23:49.840
 all have limited scope. Look, if you have a really good

23:49.840 --> 23:52.160
 recipe book for making bread, it won't tell you anything

23:52.160 --> 23:55.200
 about how to make an omelet. Right. Right. But if you

23:55.200 --> 24:00.720
 have a deep understanding of cooking, right, then bread,

24:00.720 --> 24:04.400
 omelets, you know, sandwich, you know, there's there's a

24:04.400 --> 24:08.800
 different, you know, way of viewing everything. And most

24:08.800 --> 24:13.040
 people, when you get to be an expert at something, you

24:13.040 --> 24:16.320
 know, you're you're hoping to achieve deeper understanding

24:16.320 --> 24:21.200
 not just a large set of recipes to go execute. And it's

24:21.200 --> 24:24.080
 interesting to walk groups of people because executing

24:24.080 --> 24:28.240
 recipes is unbelievably efficient. If it's what you

24:28.240 --> 24:32.320
 want to do. If it's not what you want to do, you're

24:32.320 --> 24:37.120
 really stuck. And that difference is crucial. And

24:37.120 --> 24:39.760
 everybody has a balance of, let's say, deeper

24:39.760 --> 24:41.920
 understanding recipes. And some people are really good

24:41.920 --> 24:45.360
 at recognizing when the problem is to understand

24:45.360 --> 24:49.120
 something deeply, deeply. Does that make sense? It

24:49.120 --> 24:52.720
 totally makes sense. Does it every stage of development

24:52.720 --> 24:55.920
 deep understanding on the team needed? Oh, this goes

24:55.920 --> 24:59.520
 back to the art versus science question. Sure. If you

24:59.520 --> 25:02.080
 constantly unpacked everything for deeper understanding,

25:02.080 --> 25:04.800
 you never get anything done. Right. And if you don't

25:04.800 --> 25:07.520
 unpack understanding when you need to, you'll do the

25:07.520 --> 25:11.600
 wrong thing. And then at every juncture, like human

25:11.600 --> 25:14.160
 beings are these really weird things because

25:14.160 --> 25:16.320
 everything you tell them has a million possible

25:16.320 --> 25:19.360
 outputs, right? And then they all interact in a

25:19.360 --> 25:23.360
 hilarious way. And then having some intuition about

25:23.360 --> 25:25.360
 what do you tell them, what do you do, when do you

25:25.360 --> 25:28.720
 intervene, when do you not, it's, it's complicated.

25:28.720 --> 25:31.680
 Right. So it's, it's, it's, you know, essentially

25:31.680 --> 25:34.480
 computationally unsolvable. Yeah, it's an intractable

25:34.480 --> 25:40.480
 problem. Sure. Humans are a mess. But with deep

25:40.480 --> 25:44.000
 understanding, do you mean also sort of fundamental

25:44.000 --> 25:52.240
 questions of things like what is a computer? Like, or

25:52.240 --> 25:55.760
 why? Like, the why questions, why are we even

25:55.760 --> 26:00.080
 building this? Like, of purpose? Or do you mean

26:00.080 --> 26:03.760
 more like going towards the fundamental limits of

26:03.760 --> 26:06.800
 physics sort of really getting into the core of

26:06.800 --> 26:09.520
 the science? Well, in terms of building a computer,

26:09.520 --> 26:12.880
 think simple, think a little simpler. So common

26:12.880 --> 26:15.120
 practices, you build a computer. And then when

26:15.120 --> 26:17.920
 somebody says, I want to make it 10% faster, you'll

26:17.920 --> 26:19.920
 go in and say, all right, I need to make this

26:19.920 --> 26:23.520
 buffer bigger. And maybe I'll add an ad unit. Or, you

26:23.520 --> 26:25.040
 know, I have this thing that's three instructions

26:25.040 --> 26:27.680
 wide, I'm going to make it four instructions wide.

26:27.680 --> 26:31.440
 And what you see is each piece gets incrementally

26:31.440 --> 26:35.280
 more complicated. Right. And then at some point, you

26:35.280 --> 26:38.640
 hit this limit, like adding another feature or

26:38.640 --> 26:41.280
 buffer doesn't seem to make it any faster. And

26:41.280 --> 26:43.440
 then people will say, well, that's because it's a

26:43.440 --> 26:46.320
 fundamental limit. And then somebody else will look

26:46.320 --> 26:48.320
 at it and say, well, actually the way you divided

26:48.320 --> 26:50.720
 the problem up and the way that different features

26:50.720 --> 26:53.520
 are interacting is limiting you and it has to be

26:53.520 --> 26:57.360
 rethought, rewritten. Right. So then you refactor

26:57.360 --> 27:00.160
 and rewrite it and what people commonly find is

27:00.160 --> 27:02.480
 the rewrite is not only faster, but half is

27:02.480 --> 27:06.640
 complicated. From scratch? Yes. So how often in

27:06.640 --> 27:09.600
 your career, but just have you seen as needed,

27:09.600 --> 27:12.640
 maybe more generally, to just throw the whole

27:12.640 --> 27:15.760
 out, the whole thing out? This is where I'm on one

27:15.760 --> 27:19.520
 end of it, every three to five years. Which end

27:19.520 --> 27:23.280
 are you on? Wait. Rewrite more often. Rewrite. And

27:23.280 --> 27:25.840
 three to five years is... If you want to really

27:25.840 --> 27:28.000
 make a lot of progress in computer architecture,

27:28.000 --> 27:31.920
 every five years you should do one from scratch.

27:31.920 --> 27:36.800
 So where does the x86, x64 standard come in?

27:36.800 --> 27:40.960
 How often do you... I was the coauthor of that spec

27:40.960 --> 27:44.800
 in 98. That's 20 years ago. Yeah. So that's still

27:44.800 --> 27:47.600
 around. The instruction set itself has been

27:47.600 --> 27:51.440
 extended quite a few times. Yes. And instruction sets

27:51.440 --> 27:53.360
 are less interesting than the implementation

27:53.360 --> 27:56.320
 underneath. There's been... Interesting. On x86

27:56.320 --> 27:59.040
 architecture, Intel's designed a few, AIM just

27:59.040 --> 28:02.400
 designed a few very different architectures.

28:02.400 --> 28:06.400
 And I don't want to go into too much of the detail

28:06.400 --> 28:10.640
 about how often, but there's a tendency to

28:10.640 --> 28:12.960
 rewrite it every 10 years and it really

28:12.960 --> 28:16.240
 should be every five. So you're saying you're an

28:16.240 --> 28:18.800
 outlier in that sense in the... Rewrite more often.

28:18.800 --> 28:22.000
 Rewrite more often. Well, and here's... Isn't that scary?

28:22.000 --> 28:26.160
 Yeah, of course. Well, scary to who? To everybody

28:26.160 --> 28:28.640
 involved, because like you said, repeating the

28:28.640 --> 28:34.080
 recipe is efficient. Companies want to make money...

28:34.080 --> 28:37.600
 No. Individual engineers want to succeed. So you want to

28:37.600 --> 28:41.120
 incrementally improve, increase the buffer from 3 to 4.

28:41.120 --> 28:45.280
 Well, this is where you get into diminishing return curves.

28:45.280 --> 28:48.880
 I think Steve Jobs said this, right? So every... You have a project

28:48.880 --> 28:50.800
 and you start here and it goes up and they have

28:50.800 --> 28:53.840
 diminishing return. And to get to the next level, you have to

28:53.840 --> 28:56.640
 do a new one and the initial starting point

28:56.640 --> 29:00.000
 will be lower than the old optimization point.

29:00.000 --> 29:03.440
 But it'll get higher. So now you have two kinds of fear.

29:03.440 --> 29:07.440
 Short term disaster and long term disaster.

29:07.440 --> 29:12.720
 And you're... So grown ups, right? Like, you know, people with a

29:12.720 --> 29:16.400
 quarter by quarter business objective are terrified about

29:16.400 --> 29:19.920
 changing everything. And people who are trying to run

29:19.920 --> 29:23.920
 a business or build a computer for a long term objective

29:23.920 --> 29:29.200
 know that the short term limitations block them from the long term success.

29:29.200 --> 29:33.520
 So if you look at leaders of companies that had really good

29:33.520 --> 29:36.960
 long term success, every time they saw that they had to redo

29:36.960 --> 29:40.880
 something, they did. And so somebody has to speak up?

29:40.880 --> 29:44.720
 Or you do multiple projects in parallel. Like, you optimize the old one while you

29:44.720 --> 29:48.160
 build a new one and... But the marketing guys are always like,

29:48.160 --> 29:52.560
 make promise me that the new computer is faster on every single thing.

29:52.560 --> 29:55.440
 And the computer architect says, well, the new computer will be faster on the

29:55.440 --> 29:58.560
 average. But there's a distribution of results

29:58.560 --> 30:01.760
 and performance. And you'll have some outliers that are slower.

30:01.760 --> 30:05.200
 And that's very hard because they have one customer cares about that one.

30:05.200 --> 30:11.280
 So speaking of the long term, for over 50 years now, Moore's Law has served a...

30:11.280 --> 30:14.960
 for me and millions of others as an inspiring beacon

30:14.960 --> 30:19.280
 of what kind of amazing future brilliant engineers can build.

30:19.280 --> 30:23.520
 I'm just making your kids laugh all of today. That's great.

30:23.520 --> 30:28.800
 So first, in your eyes, what is Moore's Law if you could define for people who

30:28.800 --> 30:33.360
 don't know? Well, the simple statement was,

30:33.360 --> 30:37.760
 from Gordon Moore, was double the number of transistors every two years.

30:37.760 --> 30:44.240
 Something like that. And then my operational model is we increase the

30:44.240 --> 30:48.400
 performance of computers by 2x every two or three years.

30:48.400 --> 30:51.360
 And it's wiggled around substantially over time.

30:51.360 --> 30:56.960
 And also, in how we deliver performance has changed.

30:56.960 --> 31:02.800
 But the foundational idea was 2x the transistors every two years.

31:02.800 --> 31:07.840
 The current cadence is something like, they call it a shrink factor,

31:07.840 --> 31:11.760
 like 0.6 every two years, which is not 0.5.

31:11.760 --> 31:14.720
 But that's referring strictly again to the original definition.

31:14.720 --> 31:16.480
 Yeah, a transistor count.

31:16.480 --> 31:18.880
 A shrink factor, just getting them small and small and small.

31:18.880 --> 31:24.080
 Well, as you use for a constant chip area, if you make the transistors smaller by 0.6,

31:24.080 --> 31:27.040
 then you get one over 0.6 more transistors.

31:27.040 --> 31:30.560
 So can you linger a little longer? What's the broader,

31:30.560 --> 31:33.760
 what do you think should be the broader definition of Moore's Law

31:33.760 --> 31:36.720
 when you mentioned how you think of performance?

31:37.760 --> 31:41.200
 Just broadly, what's a good way to think about Moore's Law?

31:42.240 --> 31:46.880
 Well, first of all, I've been aware of Moore's Law for 30 years.

31:46.880 --> 31:48.880
 In which sense?

31:48.880 --> 31:52.160
 Well, I've been designing computers for 40.

31:52.160 --> 31:55.280
 You're just watching it before your eyes kind of thing.

31:55.280 --> 31:58.000
 Well, and somewhere where I became aware of it,

31:58.000 --> 32:01.440
 I was also informed that Moore's Law was going to die in 10 to 15 years.

32:02.000 --> 32:03.760
 And then I thought that was true at first.

32:03.760 --> 32:07.360
 But then after 10 years, it was going to die in 10 to 15 years.

32:07.360 --> 32:09.600
 And then at one point, it was going to die in five years.

32:09.600 --> 32:11.120
 And then it went back up to 10 years.

32:11.120 --> 32:15.280
 And at some point, I decided not to worry about that particular product

32:15.280 --> 32:19.520
 agnostication for the rest of my life, which is fun.

32:19.520 --> 32:22.640
 And then I joined Intel and everybody said Moore's Law is dead.

32:22.640 --> 32:24.960
 And I thought that's sad because it's the Moore's Law company.

32:25.520 --> 32:26.800
 And it's not dead.

32:26.800 --> 32:28.320
 And it's always been going to die.

32:29.120 --> 32:33.520
 And humans like these apocryphal kind of statements like,

32:34.080 --> 32:37.040
 we'll run out of food or we'll run out of air or run out of room

32:37.040 --> 32:38.960
 or run out of something.

32:39.840 --> 32:43.920
 Right. But it's still incredible that it's lived for as long as it has.

32:43.920 --> 32:49.200
 And yes, there's many people who believe now that Moore's Law is dead.

32:49.200 --> 32:52.960
 You know, they can join the last 50 years of people who had the same idea.

32:52.960 --> 32:54.240
 Yeah, there's a long tradition.

32:54.240 --> 33:01.760
 But why do you think, if you can try to understand it, why do you think it's not dead?

33:01.760 --> 33:06.240
 Well, first, let's just think, people think Moore's Law is one thing.

33:06.240 --> 33:07.600
 Transistors get smaller.

33:08.240 --> 33:11.440
 But actually under the sheets, there's literally thousands of innovations.

33:11.440 --> 33:16.320
 And almost all those innovations have their own diminishing return curves.

33:17.200 --> 33:20.720
 So if you graph it, it looks like a cascade of diminishing return curves.

33:21.280 --> 33:22.560
 I don't know what to call that.

33:22.560 --> 33:25.600
 But the result is an exponential curve.

33:26.320 --> 33:27.440
 But at least it has been.

33:29.040 --> 33:30.720
 And we keep inventing new things.

33:30.720 --> 33:34.320
 So if you're an expert in one of the things on a diminishing return curve,

33:35.440 --> 33:39.440
 right, and you can see it's plateau, you will probably tell people,

33:39.440 --> 33:41.440
 well, this is done.

33:42.000 --> 33:45.600
 Meanwhile, some other pilot people are doing something different.

33:46.240 --> 33:48.160
 So that's just normal.

33:48.160 --> 33:52.800
 So then there's the observation of how small could a switching device be?

33:53.920 --> 33:59.040
 So a modern transistor is something like 1,000 by 1,000 by 1,000 atoms, right?

33:59.760 --> 34:03.520
 And you get quantum effects down around 2 to 10 atoms.

34:04.480 --> 34:08.080
 So you can imagine a transistor as small as 10 by 10 by 10.

34:08.080 --> 34:11.200
 So that's a million times smaller.

34:11.920 --> 34:16.480
 And then the quantum computational people are working away at how to use quantum effects.

34:17.360 --> 34:21.840
 So 1,000 by 1,000 by 1,000.

34:21.840 --> 34:22.320
 Atoms.

34:23.520 --> 34:26.480
 That's a really clean way of putting it.

34:26.480 --> 34:31.840
 Well, a fan, like a modern transistor, if you look at the fan, it's like 120 atoms wide.

34:31.840 --> 34:33.200
 But we can make that thinner.

34:33.200 --> 34:36.400
 And then there's a gate wrapped around it and then there's spacing.

34:36.400 --> 34:37.920
 There's a whole bunch of geometry.

34:38.640 --> 34:45.840
 And a competent transistor designer could count both atoms in every single direction.

34:47.760 --> 34:51.600
 Like there's techniques now to already put down atoms in a single atomic layer.

34:52.880 --> 34:54.720
 And you can place atoms if you want to.

34:55.680 --> 35:01.120
 It's just, from a manufacturing process, if placing an atom takes 10 minutes,

35:01.120 --> 35:07.680
 and you need to put 10 to the 23rd atoms together to make a computer, it would take a long time.

35:08.640 --> 35:12.480
 So the methods are both shrinking things.

35:13.120 --> 35:16.880
 And then coming up with effective ways to control what's happening.

35:17.760 --> 35:19.440
 Manufacture stably and cheaply.

35:19.920 --> 35:20.160
 Yeah.

35:21.200 --> 35:23.680
 So the innovation stock's pretty broad.

35:23.680 --> 35:25.120
 There's equipment.

35:25.120 --> 35:25.840
 There's optics.

35:25.840 --> 35:26.720
 There's chemistry.

35:26.720 --> 35:27.440
 There's physics.

35:27.440 --> 35:29.040
 There's material science.

35:29.040 --> 35:33.520
 There's metallurgy. There's lots of ideas about when you put different materials together,

35:33.520 --> 35:34.320
 how do they interact?

35:34.320 --> 35:35.280
 Are they stable?

35:35.280 --> 35:36.640
 Is it stable over temperature?

35:38.720 --> 35:39.840
 Like are they repeatable?

35:40.800 --> 35:44.800
 There's literally thousands of technologies involved.

35:44.800 --> 35:50.000
 But just for the shrinking, you don't think we're quite yet close to the fundamental limits of physics.

35:50.720 --> 35:54.800
 I did a talk on Mars Law and I asked for a roadmap to a path of about 100.

35:54.800 --> 35:57.840
 And after two weeks, they said, we only got to 50.

35:57.840 --> 35:59.520
 100, what say?

35:59.520 --> 36:00.480
 100 X shrink.

36:00.480 --> 36:01.680
 100 X shrink.

36:01.680 --> 36:02.960
 We only got to 50.

36:02.960 --> 36:04.720
 And I said, why don't you give us another two weeks?

36:07.520 --> 36:09.520
 Well, here's the thing about Mars Law.

36:09.520 --> 36:15.120
 So I believe that the next 10 or 20 years of shrinking is going to happen.

36:16.240 --> 36:20.240
 Now, as a computer designer, you have two stances.

36:20.800 --> 36:26.000
 You think it's going to shrink, in which case you're designing and thinking about architecture

36:26.000 --> 36:32.720
 in a way that you'll use more transistors, or conversely, not be swamped by the complexity

36:32.720 --> 36:34.400
 of all the transistors you get.

36:36.000 --> 36:37.600
 You have to have a strategy.

36:39.200 --> 36:44.160
 So you're open to the possibility and waiting for the possibility of a whole new army of

36:44.160 --> 36:45.840
 transistors ready to work.

36:45.840 --> 36:51.920
 I'm expecting more transistors every two or three years by a number large enough

36:51.920 --> 36:56.160
 that how you think about design, how you think about architecture has to change.

36:56.960 --> 37:02.720
 Like, imagine you build buildings out of bricks, and every year the bricks are half the size,

37:04.240 --> 37:05.120
 or every two years.

37:05.680 --> 37:10.000
 Well, if you kept building bricks the same way, so many bricks per person per day,

37:11.120 --> 37:14.720
 the amount of time to build a building would go up exponentially.

37:15.600 --> 37:16.000
 Right.

37:16.000 --> 37:16.720
 Right.

37:16.720 --> 37:20.960
 But if you said, I know that's coming, so now I'm going to design equipment.

37:20.960 --> 37:24.960
 I move bricks faster, use them better, because maybe you're getting something out of the smaller

37:24.960 --> 37:30.160
 bricks, more strengths, thinner walls, you know, less material efficiency out of that.

37:30.160 --> 37:34.720
 So once you have a roadmap with what's going to happen, transistors, they're going to get,

37:34.720 --> 37:39.440
 we're going to get more of them, then you design all this collateral around it to take advantage

37:39.440 --> 37:41.360
 of it, and also to cope with it.

37:42.240 --> 37:43.600
 Like, that's the thing people don't understand.

37:43.600 --> 37:48.080
 It's like, if I didn't believe in Moore's Law, and then Moore's Law Transistors showed up,

37:48.080 --> 37:51.440
 my design teams were all drowned.

37:51.440 --> 37:57.200
 So what's the hardest part of this influx of new transistors?

37:57.200 --> 38:01.280
 I mean, even if you just look historically throughout your career,

38:02.080 --> 38:06.880
 what's the thing, what fundamentally changes when you add more transistors

38:06.880 --> 38:09.680
 in the task of designing an architecture?

38:10.560 --> 38:12.320
 Well, there's two constants, right?

38:12.320 --> 38:13.840
 One is people don't get smarter.

38:13.840 --> 38:20.000
 By the way, there's some science shown that we do get smarter because of nutrition, whatever.

38:21.120 --> 38:22.000
 Sorry to bring that up.

38:22.000 --> 38:22.480
 In effect.

38:22.480 --> 38:22.880
 Yes.

38:22.880 --> 38:23.760
 Yeah, familiar with it.

38:23.760 --> 38:24.640
 Nobody understands it.

38:24.640 --> 38:26.160
 Nobody knows if it's still going on.

38:26.160 --> 38:27.040
 So that's a...

38:27.040 --> 38:28.400
 Or whether it's real or not.

38:28.400 --> 38:31.200
 But yeah, I sort of...

38:31.200 --> 38:32.000
 Anyway, but not exponentially.

38:32.000 --> 38:34.800
 I would believe for the most part, people aren't getting much smarter.

38:35.360 --> 38:36.720
 The evidence doesn't support it.

38:36.720 --> 38:37.440
 That's right.

38:37.440 --> 38:39.360
 And then teams can't grow that much.

38:39.920 --> 38:40.560
 Right.

38:40.560 --> 38:47.120
 So human beings, we're really good in teams of 10, up to teams of 100.

38:47.120 --> 38:48.000
 They can know each other.

38:48.000 --> 38:50.160
 Beyond that, you have to have organizational boundaries.

38:51.920 --> 38:53.520
 Those are pretty hard constraints.

38:54.480 --> 38:56.320
 So then you have to divide and conquer.

38:56.320 --> 38:59.280
 Like as the designs get bigger, you have to divide it into pieces.

39:00.480 --> 39:03.040
 The power of abstraction layers is really high.

39:03.040 --> 39:05.200
 We used to build computers out of transistors.

39:06.000 --> 39:08.720
 Now we have a team that turns transistors into logic cells,

39:08.720 --> 39:10.480
 and our team that turns them into functional units.

39:10.480 --> 39:12.240
 Another one that turns them into computers.

39:12.960 --> 39:14.960
 So we have abstraction layers in there.

39:15.920 --> 39:20.480
 And you have to think about when do you shift gears on that.

39:21.120 --> 39:24.160
 We also use faster computers to build faster computers.

39:24.160 --> 39:27.040
 So some algorithms run twice as fast on new computers,

39:27.600 --> 39:29.360
 but a lot of algorithms are n squared.

39:30.320 --> 39:33.360
 So a computer with twice as many transistors,

39:33.360 --> 39:35.760
 and it might take four times as long to run.

39:36.320 --> 39:38.320
 So you have to refactor the software.

39:38.320 --> 39:40.880
 Like simply using faster computers

39:40.880 --> 39:42.640
 to build bigger computers doesn't work.

39:44.080 --> 39:46.160
 So you have to think about all these things.

39:46.160 --> 39:47.760
 So in terms of computing performance

39:47.760 --> 39:50.880
 and the exciting possibility that more powerful computers bring

39:51.520 --> 39:54.720
 is shrinking the thing we've just been talking about.

39:56.400 --> 39:59.680
 One of the, for you, one of the biggest exciting possibilities

39:59.680 --> 40:01.280
 of advancement in performance.

40:01.280 --> 40:03.840
 Or is there other directions that you're interested in?

40:03.840 --> 40:09.280
 Like in the direction of sort of enforcing given parallelism,

40:09.280 --> 40:14.000
 or doing massive parallelism in terms of many, many CPUs,

40:15.120 --> 40:19.600
 stacking CPUs on top of each other, that kind of parallelism,

40:19.600 --> 40:20.640
 or any kind of parallelism?

40:20.640 --> 40:22.160
 Well, think about it in a different way.

40:22.160 --> 40:25.120
 So all of computers, slow computers,

40:25.120 --> 40:27.680
 you said a equal b plus c times d.

40:28.400 --> 40:29.840
 Pretty simple, right?

40:30.480 --> 40:33.360
 And then we made faster computers with vector units,

40:33.360 --> 40:38.400
 and you can do proper equations and matrices, right?

40:38.400 --> 40:40.960
 And then modern like AI computations,

40:40.960 --> 40:43.280
 or like convolutional neural networks,

40:43.280 --> 40:46.160
 where you convolve one large data set against another.

40:46.960 --> 40:50.960
 And so there's sort of this hierarchy of mathematics,

40:50.960 --> 40:53.920
 you know, from simple equation to linear equations

40:53.920 --> 40:58.000
 to matrix equations to deeper kind of computation.

40:58.640 --> 41:00.480
 And the data sets are getting so big

41:00.480 --> 41:04.160
 that people are thinking of data as a topology problem.

41:04.160 --> 41:07.120
 You know, data is organized in some immense shape.

41:07.840 --> 41:10.480
 And then the computation, which sort of wants to be

41:11.040 --> 41:14.480
 get data from immense shape and do some computation on it.

41:15.200 --> 41:18.000
 So what computers have a lot of people to do

41:18.000 --> 41:21.040
 is have algorithms go much, much further.

41:22.320 --> 41:25.840
 So that paper you referenced, the Sutton paper,

41:26.480 --> 41:28.960
 they talked about, you know, like when AI started,

41:28.960 --> 41:30.880
 it was apply rule sets to something.

41:31.680 --> 41:35.040
 That's a very simple computational situation.

41:35.680 --> 41:37.680
 And then when they did first chess thing,

41:37.680 --> 41:39.840
 they solved deep searches.

41:39.840 --> 41:44.560
 So have a huge database of moves and results, deep search,

41:44.560 --> 41:48.000
 but it's still just a search, right?

41:48.000 --> 41:50.320
 Now we take large numbers of images,

41:51.040 --> 41:54.240
 and we use it to train these weight sets

41:54.240 --> 41:56.080
 that we convolve across.

41:56.080 --> 41:58.160
 It's a completely different kind of phenomena.

41:58.160 --> 41:59.360
 We call that AI.

41:59.360 --> 42:01.760
 And now they're doing the next generation.

42:01.760 --> 42:03.040
 And if you look at it,

42:03.040 --> 42:06.720
 they're going up this mathematical graph, right?

42:06.720 --> 42:10.480
 And then computations, both computation and data sets,

42:10.480 --> 42:13.040
 support going up that graph.

42:13.040 --> 42:14.880
 Yeah, the kind of computation though might,

42:14.880 --> 42:18.400
 I mean, I would argue that all of it is still a search, right?

42:19.200 --> 42:22.080
 Just like you said, a topology problem of data sets,

42:22.080 --> 42:26.320
 you're searching the data sets for valuable data

42:26.320 --> 42:29.360
 and also the actual optimization of neural networks

42:29.360 --> 42:31.920
 is a kind of search for the...

42:31.920 --> 42:32.640
 I don't know.

42:32.640 --> 42:34.960
 If you had looked at the inner layers of finding a cat,

42:36.320 --> 42:37.200
 it's not a search.

42:38.240 --> 42:40.400
 It's a set of endless projections.

42:40.400 --> 42:44.480
 So a projection, here's a shadow of this phone, right?

42:44.480 --> 42:44.800
 Yeah.

42:44.800 --> 42:46.960
 And then you can have a shadow of that on the something,

42:46.960 --> 42:48.320
 a shadow on that or something.

42:48.320 --> 42:49.680
 And if you look in the layers,

42:49.680 --> 42:52.800
 you'll see this layer actually describes pointy ears

42:52.800 --> 42:55.120
 and round eyedness and fuzziness and...

42:55.120 --> 43:02.880
 But the computation to tease out the attributes is not search.

43:02.880 --> 43:03.600
 Right.

43:03.600 --> 43:04.240
 I mean, well...

43:04.240 --> 43:05.760
 Like the inference part might be search,

43:05.760 --> 43:07.200
 but the training's not search.

43:07.200 --> 43:07.840
 Okay, well, technically...

43:07.840 --> 43:10.640
 And then in deep networks, they look at layers

43:10.640 --> 43:12.720
 and they don't even know it's represented.

43:14.160 --> 43:16.320
 And yet, if you take the layers out, it doesn't work.

43:16.320 --> 43:17.120
 Okay, so refer...

43:17.120 --> 43:18.160
 So I don't think it's search.

43:18.800 --> 43:19.360
 All right, well...

43:19.360 --> 43:20.880
 But you'll have to talk to my mathematician

43:20.880 --> 43:21.920
 about what that actually is.

43:21.920 --> 43:27.200
 Well, we could disagree, but it's just semantics, I think.

43:27.200 --> 43:28.000
 It's not...

43:28.000 --> 43:28.960
 But it's certainly not...

43:28.960 --> 43:31.120
 I would say it's absolutely not semantics, but...

43:31.760 --> 43:32.080
 Okay.

43:33.440 --> 43:35.120
 All right, well, if you want to go there.

43:36.880 --> 43:38.880
 So optimization, to me, is search.

43:38.880 --> 43:42.720
 And we're trying to optimize the ability

43:42.720 --> 43:44.720
 of in your own network to detect cat ears.

43:45.760 --> 43:50.160
 And the difference between chess and the space,

43:50.160 --> 43:53.280
 the incredibly multidimensional,

43:53.280 --> 43:56.080
 100,000 dimensional space that, you know,

43:56.080 --> 43:57.680
 networks are trying to optimize over,

43:57.680 --> 44:01.040
 is nothing like the chess board database.

44:01.040 --> 44:03.440
 So it's a totally different kind of thing.

44:03.440 --> 44:05.440
 And, okay, in that sense, you can say...

44:05.440 --> 44:06.000
 Yeah, yeah.

44:06.000 --> 44:06.880
 It loses the meaning.

44:06.880 --> 44:08.560
 I can see how you might say.

44:08.560 --> 44:09.440
 If you...

44:10.400 --> 44:12.640
 The funny thing is, it's the difference between

44:13.200 --> 44:15.760
 given search space and found search space.

44:15.760 --> 44:16.400
 Right, exactly.

44:16.400 --> 44:17.760
 Yeah, maybe that's the different way to describe it.

44:17.760 --> 44:19.040
 That's a beautiful way to put it, okay.

44:19.040 --> 44:22.480
 But you're saying, what's your sense in terms of the basic

44:22.480 --> 44:26.800
 mathematical operations and the architectures

44:26.800 --> 44:29.760
 computer hardware that enables those operations?

44:29.760 --> 44:34.960
 Do you see the CPUs of today still being a really core part

44:34.960 --> 44:37.520
 of executing those mathematical operations?

44:37.520 --> 44:37.760
 Yes.

44:38.320 --> 44:40.080
 Well, the operations, you know,

44:40.080 --> 44:42.960
 continue to be at subtract, load, store,

44:42.960 --> 44:43.840
 compare, and branch.

44:43.840 --> 44:46.000
 It's remarkable.

44:46.000 --> 44:48.640
 So it's interesting that the building blocks

44:48.640 --> 44:50.880
 of, you know, computers or transistors,

44:50.880 --> 44:52.480
 and, you know, under that atoms.

44:52.480 --> 44:55.120
 So you've got atoms, transistors, logic gates, computers,

44:55.760 --> 44:57.520
 right, you know, functional units and computers.

44:58.160 --> 45:00.880
 The building blocks of mathematics at some level

45:00.880 --> 45:04.240
 are things like ads and subtracts and multiplies.

45:04.240 --> 45:08.800
 But the space mathematics can describe as,

45:08.800 --> 45:10.240
 I think, essentially infinite.

45:11.120 --> 45:13.920
 But the computers that run the algorithms

45:13.920 --> 45:15.840
 are still doing the same things.

45:15.840 --> 45:19.520
 Now, a given algorithm may say, I need sparse data,

45:19.520 --> 45:23.200
 or I need 32 bit data, or I need, you know,

45:24.000 --> 45:26.960
 like a convolution operation that naturally takes

45:26.960 --> 45:30.320
 8 bit data, multiplies it and sums it up a certain way.

45:30.800 --> 45:34.880
 So the, like the data types in TensorFlow imply

45:34.880 --> 45:36.880
 an optimization set.

45:37.360 --> 45:39.680
 But when you go right down and look at the computers,

45:39.680 --> 45:41.680
 it's Ann and Orgade still and add some multiplies.

45:41.680 --> 45:44.800
 Like, that hasn't changed much.

45:44.800 --> 45:47.520
 Now, the quantum researchers think they're going

45:47.520 --> 45:48.640
 to change that radically.

45:48.640 --> 45:50.960
 And then there's people who think about analog computing,

45:50.960 --> 45:53.360
 because you look in the brain, and it seems to be more analogish.

45:54.240 --> 45:57.280
 You know, that may be just a way to do that more efficiently.

45:57.760 --> 46:01.360
 But we have a million X on computation.

46:02.080 --> 46:04.560
 And I don't know the repris...

46:05.440 --> 46:08.400
 The relationship between computational, let's say,

46:08.400 --> 46:12.960
 intensity and ability to hit mathematical abstractions.

46:12.960 --> 46:17.680
 I don't know anybody's described that, but just like you saw in AI,

46:17.680 --> 46:21.680
 you went from rule sets, the simple search, the complex search,

46:21.680 --> 46:23.680
 to, say, found search.

46:23.680 --> 46:26.720
 Like, those are, you know, orders of magnitude,

46:26.720 --> 46:28.720
 more computation to do.

46:28.720 --> 46:31.200
 And as we get to the next two orders of magnitude,

46:32.400 --> 46:34.000
 like a friend, Roger Gadori, said,

46:34.000 --> 46:36.880
 like every order of magnitude changes to computation.

46:37.680 --> 46:40.480
 Fundamentally changes what the computation is doing.

46:40.480 --> 46:43.040
 Fundamentally changes what the computation is doing.

46:43.040 --> 46:46.720
 Oh, you know, the expression of the difference in quantity

46:46.720 --> 46:47.840
 is a difference in kind.

46:49.280 --> 46:52.880
 You know, the difference between ant and anthill, right?

46:52.880 --> 46:54.240
 Or neuron and brain.

46:55.760 --> 47:00.560
 You know, there's indefinable place where the quantity changed,

47:00.560 --> 47:02.320
 the quality, right?

47:02.320 --> 47:04.880
 And we've seen that happen in mathematics multiple times.

47:04.880 --> 47:08.480
 And, you know, my guess is it's going to keep happening.

47:08.480 --> 47:12.080
 So your sense is, yeah, if you focus head down

47:12.080 --> 47:14.000
 and shrinking the transistor.

47:14.640 --> 47:15.600
 Well, it's not just head down.

47:15.600 --> 47:18.240
 We're aware of the software stacks

47:18.240 --> 47:20.240
 that are running in the computational loads.

47:20.240 --> 47:22.560
 And we're kind of pondering, what do you do

47:22.560 --> 47:25.440
 with a petabyte of memory that wants to be accessed

47:25.440 --> 47:28.080
 in a sparse way and have, you know,

47:28.080 --> 47:31.360
 the kind of calculations AI programmers want?

47:32.560 --> 47:34.640
 So there's a dialogue and interaction.

47:34.640 --> 47:36.720
 But when you go in the computer chip,

47:36.720 --> 47:41.280
 you know, you find addersons, subtractors, and multipliers.

47:41.280 --> 47:45.440
 And so if you zoom out, then with, as you mentioned,

47:45.440 --> 47:48.480
 the idea that most of the development

47:49.120 --> 47:51.440
 in the last many decades in AI research

47:51.440 --> 47:54.240
 came from just leveraging computation

47:54.240 --> 47:58.160
 and just simple algorithms waiting

47:58.160 --> 47:59.840
 for the computation to improve.

47:59.840 --> 48:02.400
 Well, software guys have a thing that they call it

48:03.600 --> 48:06.080
 the problem of early optimization.

48:06.080 --> 48:06.960
 Right.

48:06.960 --> 48:09.040
 So you write a big software stack

48:09.040 --> 48:12.240
 and if you start optimizing like the first thing you write,

48:12.240 --> 48:14.640
 the odds of that being the performance limit is low.

48:15.200 --> 48:16.800
 But when you get the whole thing working,

48:16.800 --> 48:19.760
 can you make it 2x faster by optimizing the right things?

48:19.760 --> 48:22.400
 Sure. While you're optimizing that,

48:22.400 --> 48:24.160
 could you've written a new software stack,

48:24.160 --> 48:25.440
 which would have been a better choice?

48:26.000 --> 48:28.000
 Maybe. Now you have creative tension.

48:29.440 --> 48:30.080
 So.

48:30.080 --> 48:32.560
 But the whole time as you're doing the writing,

48:32.560 --> 48:34.800
 the, that's the software we're talking about,

48:34.800 --> 48:36.640
 the hardware underneath gets faster and faster.

48:36.640 --> 48:38.000
 Well, it goes back to the Moore's Law.

48:38.000 --> 48:39.840
 If Moore's Law is going to continue,

48:39.840 --> 48:45.680
 then your AI research should expect that to show up.

48:45.680 --> 48:48.000
 And then you make a slightly different set of choices than

48:48.560 --> 48:50.640
 we've hit the wall, nothing's going to happen.

48:50.640 --> 48:51.200
 Yeah.

48:51.200 --> 48:54.000
 And from here, it's just us rewriting algorithms.

48:54.880 --> 48:56.400
 Like that seems like a failed strategy

48:56.400 --> 48:58.800
 for the last 30 years of Moore's Law's death.

48:59.600 --> 49:00.000
 So.

49:00.000 --> 49:01.680
 So can you just linger on it?

49:01.680 --> 49:04.320
 I think you've answered it,

49:04.320 --> 49:06.880
 but I'll just ask the same dumb question over and over.

49:06.880 --> 49:11.200
 So why do you think Moore's Law is not going to die?

49:12.560 --> 49:15.600
 Which is the most promising, exciting possibility

49:15.600 --> 49:17.920
 of why it won't die in the next 5, 10 years?

49:17.920 --> 49:20.560
 So is it the continued shrinking of the transistor,

49:20.560 --> 49:23.840
 or is it another S curve that steps in,

49:23.840 --> 49:25.360
 and a totally sort of.

49:25.360 --> 49:29.520
 Well, shrinking the transistor is literally thousands of innovations.

49:30.080 --> 49:30.320
 Right.

49:30.320 --> 49:35.760
 So there's a whole bunch of S curves just kind of running

49:35.760 --> 49:40.240
 their course and being reinvented and new things.

49:41.600 --> 49:45.440
 The semiconductor fabricators and technologists

49:45.440 --> 49:47.280
 have all announced what's called nanowires.

49:47.280 --> 49:51.040
 So they took a fin, which had a gate around it

49:51.040 --> 49:52.480
 and turned that into a little wire.

49:52.480 --> 49:54.640
 So you have better control that and they're smaller.

49:55.200 --> 49:57.120
 And then from there, there are some obvious steps

49:57.120 --> 49:58.480
 about how to shrink that.

49:58.480 --> 50:02.720
 So the metallurgy around wire stacks and stuff

50:03.600 --> 50:06.080
 has very obvious abilities to shrink.

50:07.120 --> 50:10.240
 And there's a whole combination of things there to do.

50:10.880 --> 50:12.560
 Your sense is that we're going to get a lot

50:13.360 --> 50:15.440
 if this innovation from just that shrinking.

50:16.480 --> 50:18.160
 Yeah, like a factor of a hundred's a lot.

50:19.360 --> 50:22.080
 Yeah, I would say that's incredible.

50:22.080 --> 50:23.600
 And it's totally unknown.

50:23.600 --> 50:25.040
 It's only 10 or 15 years.

50:25.040 --> 50:26.320
 Now, you're smarter, you might know,

50:26.320 --> 50:29.200
 but to me, it's totally unpredictable of what that 100x

50:29.200 --> 50:33.200
 would bring in terms of the nature of the computation

50:33.200 --> 50:36.240
 that people would be familiar with Bell's law.

50:37.120 --> 50:39.280
 So for a long time, it was mainframes,

50:39.280 --> 50:41.520
 many's workstation, PC, mobile.

50:42.400 --> 50:45.040
 Moore's law drove faster, smaller computers.

50:46.160 --> 50:48.560
 And then when we were thinking about Moore's law,

50:49.360 --> 50:53.200
 Roger Gadori said every 10x generates a new computation.

50:53.200 --> 50:58.800
 So scalar, vector, matrix, topological computation.

51:00.960 --> 51:03.680
 And if you go look at the industry trends,

51:03.680 --> 51:06.640
 there was mainframes and many computers and PCs.

51:07.280 --> 51:10.640
 And then the internet took off and then we got mobile devices.

51:10.640 --> 51:14.080
 And now we're building 5G wireless with one millisecond latency.

51:14.800 --> 51:17.040
 And people are starting to think about the smart world

51:17.040 --> 51:20.720
 where everything knows you, recognizes you.

51:20.720 --> 51:26.640
 Like the transformations are gonna be unpredictable.

51:27.360 --> 51:32.400
 How does it make you feel that you're one of the key architects

51:33.440 --> 51:35.040
 of this kind of future?

51:35.040 --> 51:37.040
 So you're not, we're not talking about the architects

51:37.040 --> 51:43.760
 of the high level people who build the Angry Bird apps and Snapchat.

51:43.760 --> 51:44.560
 Angry Bird apps.

51:44.560 --> 51:46.960
 Who knows, maybe that's the whole point of the universe.

51:46.960 --> 51:48.160
 I'm gonna take a stand at that.

51:48.160 --> 51:52.160
 And the attention distracting nature of mobile phones.

51:52.160 --> 51:53.040
 I'll take a stand.

51:53.040 --> 52:00.400
 But anyway, in terms of the side effects of smartphones

52:00.400 --> 52:03.040
 or the attention distraction, which part?

52:03.040 --> 52:05.440
 Well, who knows, you know, where this is all leading.

52:05.440 --> 52:06.640
 It's changing so fast.

52:06.640 --> 52:07.600
 Wait, so back to the...

52:07.600 --> 52:09.920
 My parents used to yell at my sisters for hiding in the closet

52:09.920 --> 52:11.840
 with a wired phone with a dial on it.

52:12.400 --> 52:13.840
 Stop talking to your friends all day.

52:13.840 --> 52:13.840
 Right.

52:13.840 --> 52:17.040
 Now, my wife yells at my kids for talking to her friends

52:17.040 --> 52:18.160
 all day on text.

52:18.160 --> 52:19.840
 It looks the same to me.

52:19.840 --> 52:21.840
 It's always, it echoes at the same time.

52:21.840 --> 52:25.840
 Okay, but you are the one of the key people architecting

52:25.840 --> 52:27.440
 the hardware of this future.

52:27.440 --> 52:28.560
 How does that make you feel?

52:28.560 --> 52:29.840
 Do you feel responsible?

52:31.840 --> 52:33.040
 Do you feel excited?

52:34.240 --> 52:36.240
 So we're in a social context.

52:36.240 --> 52:38.240
 So there's billions of people on this planet.

52:39.040 --> 52:42.640
 There are literally millions of people working on technology.

52:42.640 --> 52:47.440
 I feel lucky to be, you know, doing what I do

52:47.440 --> 52:48.640
 and getting paid for it.

52:48.640 --> 52:50.640
 And there's an interest in it.

52:50.640 --> 52:53.040
 But there's so many things going on in parallel.

52:53.040 --> 52:56.240
 It's like the actions are so unpredictable.

52:56.240 --> 52:58.640
 If I wasn't here, somebody else would do it.

52:58.640 --> 53:01.040
 The vectors of all these different things

53:01.040 --> 53:02.640
 are happening all the time.

53:03.840 --> 53:08.240
 You know, there's a, I'm sure some philosopher,

53:08.240 --> 53:14.240
 a metaphilosopher is, you know, wondering about how we transform our world.

53:16.240 --> 53:19.040
 So you can't deny the fact that these tools,

53:19.040 --> 53:24.240
 whether these tools are changing our world.

53:24.240 --> 53:25.040
 That's right.

53:25.040 --> 53:28.640
 So do you think it's changing for the better?

53:28.640 --> 53:31.040
 Somebody, I read this thing recently.

53:31.040 --> 53:36.240
 It said the two disciplines with the highest GRE scores in college

53:36.240 --> 53:39.600
 are physics and philosophy, right?

53:39.600 --> 53:41.680
 And they're both sort of trying to answer the question,

53:41.680 --> 53:43.920
 why is there anything, right?

53:43.920 --> 53:47.600
 And the philosophers, you know, are on the kind of theological side

53:47.600 --> 53:51.440
 and the physicists are obviously on the, you know, the material side.

53:52.480 --> 53:56.800
 And there's a hundred billion galaxies with a hundred billion stars.

53:56.800 --> 53:59.680
 It seems, well, repetitive at best.

53:59.680 --> 54:05.440
 So, you know, there's on our way to 10 billion people.

54:05.440 --> 54:08.960
 I mean, it's hard to say what it's all for, if that's what you're asking.

54:08.960 --> 54:10.560
 Yeah, I guess, I guess I am.

54:10.560 --> 54:14.720
 I mean, things do tend to significantly increase this in complexity.

54:16.160 --> 54:23.120
 And I'm curious about how computation, like our world, our physical world,

54:23.840 --> 54:25.680
 inherently generates mathematics.

54:25.680 --> 54:26.720
 It's kind of obvious, right?

54:26.720 --> 54:30.000
 So we have XYZ coordinates, you take a sphere, you make it bigger,

54:30.000 --> 54:33.920
 you get a surface that falls, you know, grows by R squared.

54:33.920 --> 54:37.360
 Like, it generally generates mathematics and the mathematicians

54:37.360 --> 54:40.560
 and the physicists have been having a lot of fun talking to each other for years.

54:41.120 --> 54:46.000
 And computation has been, let's say, relatively pedestrian.

54:46.000 --> 54:51.840
 Like, computation in terms of mathematics has been doing binary algebra,

54:51.840 --> 54:58.000
 while those guys have been galavanting through the other realms of possibility, right?

54:58.000 --> 55:05.680
 Now, recently, the computation lets you do mathematical computations that are sophisticated

55:05.680 --> 55:09.920
 enough that nobody understands how the answers came out, right?

55:09.920 --> 55:10.640
 Machine learning.

55:10.640 --> 55:11.440
 Machine learning.

55:11.440 --> 55:11.760
 Yeah, yeah.

55:11.760 --> 55:15.920
 Right, it used to be, you get data set, you guess at a function.

55:16.640 --> 55:20.960
 The function is considered physics if it's predictive of new functions,

55:20.960 --> 55:22.960
 new data sets.

55:22.960 --> 55:29.840
 Modern, you can take a large data set with no intuition about what it is

55:29.840 --> 55:34.160
 and use machine learning to find a pattern that has no function, right?

55:34.160 --> 55:38.320
 And it can arrive at results that I don't know if they're completely

55:38.320 --> 55:39.840
 mathematically describable.

55:39.840 --> 55:46.000
 So computation has kind of done something interesting compared to A equal B plus C.

55:46.000 --> 55:53.360
 There's something reminiscent of that step from the basic operations of addition

55:54.640 --> 55:58.880
 to taking a step towards neural networks that's reminiscent of what life on earth

55:58.880 --> 56:00.320
 at its origins was doing.

56:00.880 --> 56:04.160
 Do you think we're creating sort of the next step in our evolution

56:04.720 --> 56:07.920
 in creating artificial intelligence systems that will?

56:07.920 --> 56:08.480
 I don't know.

56:08.480 --> 56:11.680
 I mean, there's so much in the universe already, it's hard to say.

56:12.560 --> 56:13.920
 What we stand in this whole thing.

56:13.920 --> 56:18.320
 Are human beings working on additional abstraction layers and possibilities?

56:18.320 --> 56:19.200
 Yeah, it appears so.

56:20.160 --> 56:22.880
 Does that mean that human beings don't need dogs?

56:22.880 --> 56:23.680
 You know, no.

56:24.640 --> 56:30.240
 Like there's so many things that are all simultaneously interesting and useful.

56:30.240 --> 56:33.760
 What you've seen throughout your career, you've seen great and greater level

56:33.760 --> 56:38.800
 abstractions built in artificial machines, right?

56:38.800 --> 56:45.360
 When you look at humans, you think of all life on earth as a single organism building

56:45.360 --> 56:49.680
 this thing, this machine with greater and greater levels of abstraction.

56:49.680 --> 56:56.480
 Do you think humans are the peak, the top of the food chain in this long arc of history

56:57.200 --> 56:58.160
 on earth?

56:58.160 --> 57:00.240
 Or do you think we're just somewhere in the middle?

57:00.240 --> 57:05.120
 Are we the basic functional operations of a CPU?

57:05.120 --> 57:10.080
 Are we the C++ program, the Python program, or with the neural network?

57:11.760 --> 57:14.720
 People have calculated like how many operations does the brain do.

57:15.920 --> 57:19.840
 I've seen the number 10 to the 18th about a bunch of times arrive different ways.

57:20.480 --> 57:23.680
 So could you make a computer that did 10 to the 20th operations?

57:23.680 --> 57:24.240
 Yes.

57:24.240 --> 57:24.400
 Sure.

57:25.040 --> 57:25.600
 So you think?

57:25.600 --> 57:26.240
 We're going to do that.

57:26.960 --> 57:31.440
 Now, is there something magical about how brains compute things?

57:31.440 --> 57:31.920
 I don't know.

57:31.920 --> 57:37.920
 My personal experience is interesting because you think you know how you think and then you

57:37.920 --> 57:40.320
 have all these ideas and you can't figure out how they happened.

57:41.280 --> 57:47.600
 And if you meditate, what you can be aware of is interesting.

57:48.400 --> 57:50.480
 So I don't know if brains are magical or not.

57:52.080 --> 57:53.680
 The physical evidence says no.

57:54.560 --> 57:56.720
 Lots of people's personal experience says yes.

57:56.720 --> 58:03.520
 So what would be funny is if brains are magical and yet we can make brains with more computation.

58:04.480 --> 58:06.160
 You know, I don't know what to say about that, but.

58:06.880 --> 58:09.840
 What do you think magic is an emergent phenomena?

58:10.720 --> 58:11.280
 It could be.

58:11.280 --> 58:13.200
 I have no explanation for it.

58:13.200 --> 58:17.440
 Let me ask Jim Keller of what in your view is consciousness?

58:19.120 --> 58:20.400
 With consciousness?

58:20.400 --> 58:28.160
 Yeah, like what consciousness, love, things that are these deeply human things that seems

58:28.160 --> 58:29.440
 to emerge from our brain?

58:29.440 --> 58:37.040
 Is that something that we'll be able to make in code in chips that get faster and faster

58:37.040 --> 58:38.000
 and faster and faster?

58:38.000 --> 58:40.000
 That's like a 10 hour conversation.

58:40.000 --> 58:40.880
 Nobody really knows.

58:40.880 --> 58:43.600
 Can you summarize it in a couple of sentences?

58:43.600 --> 58:50.720
 Many people have observed that organisms run at lots of different levels, right?

58:51.280 --> 58:55.040
 If you had two neurons, somebody said you'd have one sensory neuron and one motor neuron,

58:56.000 --> 58:56.640
 right?

58:56.640 --> 59:01.520
 So we move towards things and away from things and we have physical integrity and safety or not,

59:02.240 --> 59:03.040
 right?

59:03.040 --> 59:08.160
 And then if you look at the animal kingdom, you can see brains that are a little more complicated.

59:08.160 --> 59:12.560
 And at some point there's a planning system and then there's an emotional system that's,

59:12.560 --> 59:17.120
 you know, happy about being safe or unhappy about being threatened, right?

59:17.120 --> 59:24.080
 And then our brains have massive numbers of structures, you know, like planning and movement

59:24.080 --> 59:27.040
 and thinking and feeling and drives and emotions.

59:27.760 --> 59:30.320
 And we seem to have multiple layers of thinking systems.

59:30.960 --> 59:36.080
 And we have a brain, a dream system that nobody understands whatsoever, which I find completely

59:36.080 --> 59:46.400
 hilarious and you can think in a way that those systems are more independent and you can observe,

59:46.400 --> 59:48.560
 you know, the different parts of yourself can observe them.

59:49.360 --> 59:51.200
 I don't know which one's magical.

59:51.200 --> 59:53.200
 I don't know which one's not computational.

59:55.280 --> 59:58.800
 So is it possible that it's all computation?

59:58.800 --> 59:59.280
 Probably.

59:59.920 --> 1:00:01.440
 Is there a limit to computation?

1:00:01.440 --> 1:00:02.080
 I don't think so.

1:00:02.080 --> 1:00:07.120
 Do you think the universe is a computer?

1:00:07.120 --> 1:00:14.320
 It's a weird kind of computer because if it was a computer, right, like when they do calculations

1:00:14.320 --> 1:00:19.760
 on what it, how much calculation it takes to describe quantum effects is unbelievably high.

1:00:20.800 --> 1:00:24.720
 So if it was a computer, when you built it out of something that was easier to compute,

1:00:25.840 --> 1:00:28.800
 right, that's, that's a funny, it's a funny system.

1:00:28.800 --> 1:00:32.000
 But then the simulation guys have pointed out that the rules are kind of interesting.

1:00:32.000 --> 1:00:36.000
 Like when you look really close, it's uncertain and the speed of light says you can only look

1:00:36.000 --> 1:00:40.960
 so far and things can't be simultaneous except for the odd entanglement problem where they seem

1:00:40.960 --> 1:00:43.360
 to be like the rules are all kind of weird.

1:00:44.320 --> 1:00:50.560
 And somebody said physics is like having 50 equations with 50 variables to define 50 variables.

1:00:50.560 --> 1:00:58.320
 Like, you know, it's, you know, like physics itself has been a shit show for thousands of years.

1:00:58.880 --> 1:01:02.480
 It seems odd when you get to the corners of everything, you know, it's either

1:01:02.480 --> 1:01:06.400
 uncomputable or undefinable or uncertain.

1:01:07.040 --> 1:01:11.680
 It's almost like the designers of the simulation are trying to prevent us from understanding it

1:01:11.680 --> 1:01:12.160
 perfectly.

1:01:12.720 --> 1:01:18.880
 But, but also the things that require calculations requires so much calculation that our idea of

1:01:18.880 --> 1:01:23.680
 the universe of a computer is absurd because every single little bit of it takes all the

1:01:23.680 --> 1:01:25.520
 computation in the universe to figure out.

1:01:26.480 --> 1:01:27.920
 That's a weird kind of computer.

1:01:27.920 --> 1:01:32.880
 You know, you say the simulation is running in the computer, which has, by definition,

1:01:32.880 --> 1:01:34.400
 infinite computation.

1:01:34.400 --> 1:01:35.200
 Not infinite.

1:01:35.200 --> 1:01:36.880
 Oh, you mean if the universe is infinite?

1:01:37.600 --> 1:01:43.120
 Yeah, well, every little piece of our universe seems to take infinite computation and figure out.

1:01:43.120 --> 1:01:44.080
 Just a lot.

1:01:44.080 --> 1:01:45.920
 Well, a lot, some pretty big number.

1:01:45.920 --> 1:01:52.560
 Computer's little teeny spot takes all the mass in the local one light year by one light

1:01:52.560 --> 1:01:53.280
 year space.

1:01:53.280 --> 1:01:54.720
 It's close enough to infinite.

1:01:54.720 --> 1:01:56.480
 Oh, it's a heck of a computer if it is one.

1:01:56.480 --> 1:02:01.520
 I know it's, it's, it's a weird, it's a weird description because the simulation description

1:02:01.520 --> 1:02:04.160
 seems to break when you look closely at it.

1:02:04.720 --> 1:02:07.280
 But the rules of the universe seem to imply something's up.

1:02:08.640 --> 1:02:10.080
 That seems a little arbitrary.

1:02:10.800 --> 1:02:14.320
 The whole, the universe, the whole thing, the laws of physics.

1:02:14.320 --> 1:02:14.800
 Yeah.

1:02:14.800 --> 1:02:20.000
 It just seems like, like, how did it come out to be the way it is?

1:02:20.000 --> 1:02:21.200
 Well, lots of people talk about that.

1:02:21.200 --> 1:02:24.960
 It's, you know, it's, like I said, the two smartest groups of humans are working on the

1:02:24.960 --> 1:02:26.000
 same problem.

1:02:26.000 --> 1:02:26.720
 From different aspects.

1:02:26.720 --> 1:02:29.040
 From different aspects, and they're both complete failures.

1:02:29.920 --> 1:02:31.120
 So that's, that's kind of cool.

1:02:32.480 --> 1:02:33.920
 They might succeed eventually.

1:02:35.120 --> 1:02:38.000
 Well, after 2000 years, the trend isn't good.

1:02:38.000 --> 1:02:41.360
 Oh, 2000 years is nothing in the span of the history of the universe.

1:02:41.360 --> 1:02:41.920
 Not for sure.

1:02:41.920 --> 1:02:43.120
 We have some time.

1:02:43.120 --> 1:02:45.200
 But the next 1000 years doesn't look good either.

1:02:45.200 --> 1:02:48.800
 So that's what everybody says at every stage.

1:02:48.800 --> 1:02:52.960
 But with Moore's law, as you've just described, not being dead, the exponential

1:02:53.600 --> 1:02:57.520
 growth of technology, the future seems pretty incredible.

1:02:57.520 --> 1:02:58.400
 Well, it'll be interesting.

1:02:58.400 --> 1:02:59.440
 That's for sure.

1:02:59.440 --> 1:03:00.240
 That's right.

1:03:00.240 --> 1:03:05.120
 So what are your thoughts on Ray Kurzweil's sense that exponential improvement and

1:03:05.120 --> 1:03:07.200
 technology will continue indefinitely?

1:03:07.200 --> 1:03:09.840
 That, is that how you see Moore's law?

1:03:09.840 --> 1:03:16.560
 Do you see Moore's law more broadly in the sense that technology of all kinds has a way

1:03:16.560 --> 1:03:22.320
 of stacking S curves on top of each other, where it'll be exponential, and then we'll

1:03:22.320 --> 1:03:23.280
 see all kinds of.

1:03:23.280 --> 1:03:25.360
 What does an exponential of a million mean?

1:03:26.320 --> 1:03:28.080
 That's a pretty amazing number.

1:03:28.080 --> 1:03:30.240
 And that's just for a local little piece of silicon.

1:03:30.800 --> 1:03:38.480
 Now, let's imagine you, say, decided to get 1000 tons of silicon to collaborate in one

1:03:38.480 --> 1:03:41.600
 computer at a million times the density.

1:03:42.880 --> 1:03:48.720
 Now you're talking, I don't know, 10 to the 20th more computation power than our

1:03:48.720 --> 1:03:51.200
 current already unbelievably fast computers.

1:03:52.480 --> 1:03:54.080
 Nobody knows what that's going to mean.

1:03:54.080 --> 1:03:56.880
 The sci fi guy is called competronium.

1:03:56.880 --> 1:04:02.240
 Like when a local civilization turns the nearby star into a computer.

1:04:02.240 --> 1:04:04.720
 I don't know if that's true.

1:04:04.720 --> 1:04:12.480
 So just even when you shrink a transistor, that's only one dimension.

1:04:12.480 --> 1:04:14.160
 The ripple effects of that.

1:04:14.160 --> 1:04:17.520
 People tend to think about computers as a cost problem, right?

1:04:17.520 --> 1:04:21.120
 So computers are made out of silicon and minor amounts of metals.

1:04:21.920 --> 1:04:26.240
 And you know, this and that, none of those things cost any money.

1:04:26.800 --> 1:04:28.480
 Like there's plenty of sand.

1:04:29.760 --> 1:04:33.280
 Like you could just turn the beach and a little bit of ocean water into computers.

1:04:33.280 --> 1:04:36.560
 So all the cost is in the equipment to do it.

1:04:36.560 --> 1:04:40.480
 And the trend on equipment is once you figure out how to build the equipment,

1:04:40.480 --> 1:04:41.680
 the trend of cost is zero.

1:04:41.680 --> 1:04:46.720
 Elon said, first you figure out what configuration you want the atoms in

1:04:47.360 --> 1:04:48.640
 and then how to put them there.

1:04:49.680 --> 1:04:50.160
 Right?

1:04:50.160 --> 1:04:50.720
 Yeah.

1:04:50.720 --> 1:04:56.320
 Because well, what you hear that, you know, his, his great insight is people are how constrained.

1:04:56.320 --> 1:04:57.040
 I have this thing.

1:04:57.040 --> 1:04:58.000
 I know how it works.

1:04:58.560 --> 1:05:03.200
 And then little tweaks to that will generate something as opposed to what

1:05:03.200 --> 1:05:06.880
 do I actually want and then figure out how to build it.

1:05:06.880 --> 1:05:11.040
 It's a very different mindset and almost nobody has it, obviously.

1:05:12.720 --> 1:05:14.480
 Well, let me ask on that topic.

1:05:15.600 --> 1:05:20.000
 You were one of the key early people in the development of autopilot,

1:05:20.000 --> 1:05:21.200
 at least in the hardware side.

1:05:22.320 --> 1:05:26.560
 Elon Musk believes that autopilot and vehicle autonomy, if you just look at that problem,

1:05:26.560 --> 1:05:30.800
 can follow this kind of exponential improvement in terms of the hot,

1:05:30.800 --> 1:05:32.400
 the how question that we're talking about.

1:05:32.400 --> 1:05:34.000
 There's no reason why you can't.

1:05:34.560 --> 1:05:39.200
 What are your thoughts on this particular space of vehicle autonomy?

1:05:39.920 --> 1:05:46.640
 And you're a part of it and Elon Musk's and Tesla's vision for the computer you need to build

1:05:46.640 --> 1:05:47.840
 was straightforward.

1:05:48.640 --> 1:05:53.200
 And you could argue, well, doesn't need to be two times faster or five times or 10 times.

1:05:54.400 --> 1:05:58.320
 But that's just a matter of time or price in the short run.

1:05:58.320 --> 1:06:00.080
 So that's, that's not a big deal.

1:06:00.080 --> 1:06:02.400
 You don't have to be especially smart to drive a car.

1:06:03.120 --> 1:06:05.520
 So it's not like a super hard problem.

1:06:05.520 --> 1:06:10.720
 I mean, the big problem of safety is attention, which computers are really good at, not skills.

1:06:12.800 --> 1:06:15.120
 Well, let me push back on one.

1:06:15.120 --> 1:06:20.880
 You say everything you said is correct, but we as humans tend to,

1:06:23.040 --> 1:06:26.800
 tend to take for granted how, how incredible our vision system is.

1:06:26.800 --> 1:06:33.120
 So you can drive a car with 2050 vision and you can train a neural network to extract the

1:06:33.120 --> 1:06:37.680
 distance of any object in the shape of any surface from a video and data.

1:06:38.640 --> 1:06:40.080
 But that's really simple.

1:06:40.080 --> 1:06:41.120
 No, it's not simple.

1:06:42.000 --> 1:06:43.280
 That's a simple data problem.

1:06:44.320 --> 1:06:45.760
 It's not, it's not simple.

1:06:46.480 --> 1:06:50.320
 It's because you, because it's not just detecting objects.

1:06:50.320 --> 1:06:55.520
 It's understanding the scene and it's being able to do it in a way that doesn't make errors.

1:06:55.520 --> 1:07:00.400
 So the, the beautiful thing about the human vision system and our entire brain around the

1:07:00.400 --> 1:07:03.040
 whole thing is we're able to fill in the gaps.

1:07:04.240 --> 1:07:06.800
 It's not just about perfectly detecting cars.

1:07:06.800 --> 1:07:08.560
 It's inferring the occluded cars.

1:07:08.560 --> 1:07:11.360
 It's trying to, it's, it's understanding the statistics.

1:07:11.360 --> 1:07:12.640
 I think that's mostly a data problem.

1:07:13.360 --> 1:07:19.040
 So you think what data would compute with improvement of computation with improvement

1:07:19.040 --> 1:07:19.600
 in collection?

1:07:19.600 --> 1:07:22.400
 Well, there's a, you know, when you're driving a car and somebody cuts you off,

1:07:22.400 --> 1:07:24.720
 your brain has theories about why they did it.

1:07:24.720 --> 1:07:27.840
 You know, they're a bad person, they're distracted, they're dumb.

1:07:28.560 --> 1:07:30.720
 You know, you can listen to yourself, right?

1:07:31.360 --> 1:07:36.800
 So, you know, if you think that narrative is important to be able to successfully drive

1:07:36.800 --> 1:07:40.320
 a car, then current autopilot systems can't do it.

1:07:40.320 --> 1:07:45.680
 But if cars are ballistic things with tracks and probabilistic changes of speed and direction

1:07:45.680 --> 1:07:51.920
 and roads are fixed and given, by the way, they don't change dynamically, right?

1:07:51.920 --> 1:07:55.520
 Right, you can map the world really thoroughly.

1:07:56.240 --> 1:08:01.360
 You can place every object really thoroughly, right?

1:08:01.360 --> 1:08:06.400
 You can calculate trajectories of things really thoroughly, right?

1:08:06.400 --> 1:08:11.840
 But everything you said about really thoroughly has a different degree of difficulty.

1:08:12.480 --> 1:08:12.960
 So.

1:08:12.960 --> 1:08:18.240
 And you could say at some point, computer autonomous systems will be way better at

1:08:18.240 --> 1:08:19.920
 things that humans are lousy at.

1:08:19.920 --> 1:08:22.320
 Like, they'll be better at attention.

1:08:22.320 --> 1:08:26.560
 They'll always remember there was a pothole on the road that humans keep forgetting about.

1:08:27.120 --> 1:08:31.920
 They'll remember that this set of roads has these weirdo lines on it that the computers

1:08:31.920 --> 1:08:36.880
 figured out once and especially if they get updates so that somebody changes a given.

1:08:37.840 --> 1:08:42.560
 Like, the key to robots and stuff somebody said is to maximize the givens.

1:08:43.760 --> 1:08:45.040
 Right, right.

1:08:45.040 --> 1:08:50.960
 So having a robot pick up this bottle cap is way easier to put a red dot on the top because

1:08:50.960 --> 1:08:54.320
 then you'll have to figure out, you know, and if you want to do a certain thing with it,

1:08:54.320 --> 1:08:56.320
 you know, maximize the givens is the thing.

1:08:56.960 --> 1:08:59.840
 And autonomous systems are happily maximizing the givens.

1:09:00.880 --> 1:09:06.160
 Like humans, when you drive someplace new, you remember it because you're processing it the

1:09:06.160 --> 1:09:10.240
 whole time and after the 50th time you drove to work, you get to work, you don't know how you got

1:09:10.240 --> 1:09:11.280
 there, right?

1:09:11.280 --> 1:09:13.760
 You're on autopilot, right?

1:09:13.760 --> 1:09:20.160
 Autonomous cars are always on autopilot, but the cars have no theories about why they got cut off

1:09:20.160 --> 1:09:21.280
 or why they're in traffic.

1:09:22.000 --> 1:09:24.080
 So that's never stopped paying attention.

1:09:24.640 --> 1:09:25.200
 Right.

1:09:25.200 --> 1:09:29.200
 So I tend to believe you do have deaf theories met the models of other people,

1:09:29.760 --> 1:09:32.560
 especially with pedestrian cyclists, but also with other cars.

1:09:32.560 --> 1:09:38.800
 So everything you said is like, is actually essential to driving.

1:09:38.800 --> 1:09:41.600
 Driving is a lot more complicated than people realize.

1:09:41.600 --> 1:09:46.320
 I think so sort of to push back slightly, but to cut into traffic, right?

1:09:46.320 --> 1:09:46.880
 Yeah.

1:09:46.880 --> 1:09:48.240
 You can't just wait for a gap.

1:09:48.240 --> 1:09:50.080
 You have to be somewhat aggressive.

1:09:50.080 --> 1:09:52.560
 You'll be surprised how simple the calculation for that is.

1:09:53.600 --> 1:10:00.160
 I may be on that particular point, but there's a, maybe I should have to push back.

1:10:00.160 --> 1:10:01.520
 I would be surprised.

1:10:01.520 --> 1:10:01.840
 You know what?

1:10:01.840 --> 1:10:02.880
 Yeah, I'll just say where I stand.

1:10:02.880 --> 1:10:08.320
 I would be very surprised, but I think it's, you might be surprised how complicated it is.

1:10:08.320 --> 1:10:13.680
 I'd say I tell people like progress disappoints in the short run surprises in the long run.

1:10:13.680 --> 1:10:14.720
 It's very possible.

1:10:14.720 --> 1:10:18.880
 Yeah, I suspect in 10 years it'll be just like taken for granted.

1:10:18.880 --> 1:10:19.680
 Yeah, probably.

1:10:19.680 --> 1:10:21.360
 But you're probably right.

1:10:21.360 --> 1:10:24.800
 It's going to be a $50 solution that nobody cares about.

1:10:24.800 --> 1:10:26.960
 It's like GPS is like, wow, GPS.

1:10:26.960 --> 1:10:30.880
 We have satellites in space that tell you where your location is.

1:10:30.880 --> 1:10:31.840
 It was a really big deal.

1:10:31.840 --> 1:10:33.360
 Now everything has a GPS in it.

1:10:33.360 --> 1:10:33.920
 Yeah, that's true.

1:10:33.920 --> 1:10:40.160
 But I do think that systems that involve human behavior are more complicated than we give them

1:10:40.160 --> 1:10:40.720
 credit for.

1:10:40.720 --> 1:10:44.800
 So we can do incredible things with technology that don't involve humans.

1:10:44.800 --> 1:10:45.360
 But when you...

1:10:45.360 --> 1:10:49.280
 I think humans are less complicated than people, you know, frequently

1:10:49.280 --> 1:10:49.760
 subscribe.

1:10:50.400 --> 1:10:51.040
 Maybe I feel...

1:10:51.040 --> 1:10:55.040
 We tend to operate out of large numbers of patterns and just keep doing it over and over.

1:10:55.600 --> 1:10:57.920
 But I can't trust you because you're a human.

1:10:57.920 --> 1:11:00.640
 That's something, something a human would say.

1:11:00.640 --> 1:11:06.880
 But my hope is on the point you've made is even if, no matter who's right,

1:11:08.080 --> 1:11:12.720
 there, I'm hoping that there's a lot of things that humans aren't good at that machines are

1:11:12.720 --> 1:11:13.280
 definitely good at.

1:11:13.280 --> 1:11:15.520
 Like you said, attention and things like that.

1:11:15.520 --> 1:11:21.280
 Well, there'll be so much better that the overall picture of safety and autonomy will be

1:11:21.280 --> 1:11:24.000
 obviously cars will be safer, even if they're not as good.

1:11:24.000 --> 1:11:26.160
 No, I'm a big believer in safety.

1:11:26.160 --> 1:11:30.960
 I mean, there are already the current safety systems like cruise control that doesn't let

1:11:30.960 --> 1:11:33.120
 you run into people and lane keeping.

1:11:33.120 --> 1:11:38.240
 There are so many features that you just look at the Pareto of accidents and knocking off

1:11:38.240 --> 1:11:41.360
 like 80% of them is super doable.

1:11:42.240 --> 1:11:46.480
 Just to linger on the autopilot team and the efforts there, the...

1:11:47.840 --> 1:11:53.440
 It seems to be that there is a very intense scrutiny by the media and the public in terms

1:11:53.440 --> 1:11:57.200
 of safety, the pressure, the bar put before autonomous vehicles.

1:11:57.760 --> 1:12:03.200
 What are your, sort of, as a person there working on the hardware and trying to build

1:12:03.200 --> 1:12:08.800
 a system that builds a safe vehicle and so on, what was your sense about that pressure?

1:12:08.800 --> 1:12:09.680
 Is it unfair?

1:12:09.680 --> 1:12:11.440
 Is it expected of new technology?

1:12:12.080 --> 1:12:13.360
 Yeah, it seems reasonable.

1:12:13.360 --> 1:12:14.000
 I was interested.

1:12:14.000 --> 1:12:21.280
 I talked to both American and European regulators and I was worried that the regulations would

1:12:21.280 --> 1:12:28.880
 write into the rules, technology solutions like modern brake systems imply hydraulic brakes.

1:12:29.840 --> 1:12:35.600
 So if you read the regulations to meet the letter of the law for brakes, it sort of has

1:12:35.600 --> 1:12:37.120
 to be hydraulic, right?

1:12:37.680 --> 1:12:43.600
 And the regulator said they're interested in the use cases like a head on crash, an offset

1:12:43.600 --> 1:12:49.280
 crash, don't hit pedestrians, don't run into people, don't leave the road, don't run a red

1:12:49.280 --> 1:12:55.760
 light or a stoplight. They were very much into the scenarios and they had all the data about

1:12:55.760 --> 1:13:02.560
 which scenarios injured or killed the most people and for the most part those conversations were

1:13:03.520 --> 1:13:07.600
 like what's the right thing to do to take the next step.

1:13:08.560 --> 1:13:13.280
 Now Elon's very interested in also in the benefits of autonomous driving or freeing

1:13:13.280 --> 1:13:20.400
 people's time and attention as well as safety. And I think that's also an interesting thing but

1:13:22.960 --> 1:13:26.640
 building autonomous systems so they're safe and safer than people seemed.

1:13:27.280 --> 1:13:32.560
 Since the goal is to be 10x safer than people, having the bar to be safer than people and

1:13:32.560 --> 1:13:38.000
 scrutinizing accidents seems philosophically correct.

1:13:39.120 --> 1:13:40.160
 So I think that's a good thing.

1:13:40.160 --> 1:13:48.480
 What are, it's different than the things that you worked at in teleMD,

1:13:48.480 --> 1:13:54.800
 Apple with autopilot chip design and hardware design. What are interesting or challenging

1:13:54.800 --> 1:13:58.880
 aspects of building this specialized kind of computing system in the automotive space?

1:14:00.080 --> 1:14:05.360
 I mean there's two tricks to building like an automotive computer. One is the software team,

1:14:05.360 --> 1:14:12.720
 the machine learning team is developing algorithms that are changing fast. So as you're building the

1:14:12.720 --> 1:14:18.400
 accelerator you have this you know worry or intuition that the algorithms will change enough

1:14:18.400 --> 1:14:25.040
 that the accelerator will be the wrong one, right? And there's the generic thing which is if you

1:14:25.040 --> 1:14:31.440
 build a really good general purpose computer say its performance is one and then GPU guys will

1:14:31.440 --> 1:14:36.800
 deliver about 5x to performance for the same amount of silicon because instead of discovering

1:14:36.800 --> 1:14:44.000
 parallelism you're given parallelism. And then special accelerators get another 2 to 5x on top

1:14:44.000 --> 1:14:50.960
 of a GPU because you say I know the math is always 8 bit integers into 32 bit accumulators

1:14:50.960 --> 1:14:58.240
 and the operations are the subset of mathematical possibilities. So auto you know AI accelerators

1:14:58.240 --> 1:15:04.160
 have a claimed performance benefit over GPUs because in the narrow math space

1:15:04.960 --> 1:15:11.920
 you're nailing the algorithm. Now you still try to make it programmable but the AI field is changing

1:15:11.920 --> 1:15:17.600
 really fast. So there's a you know there's a little creative tension there of I want the

1:15:17.600 --> 1:15:23.920
 acceleration afforded by specialization without being over specialized so that the new algorithm is

1:15:23.920 --> 1:15:28.960
 so much more effective that you would have been better off on a GPU. So there's a tension there

1:15:29.840 --> 1:15:36.080
 to build a good computer for an application like automotive. There's all kinds of sensor inputs

1:15:36.080 --> 1:15:41.520
 and safety processors and a bunch of stuff. So one of Elon's goals to make it super affordable.

1:15:42.080 --> 1:15:46.400
 So every car gets an autopilot computer. So some of the recent startups you look at and

1:15:46.400 --> 1:15:50.640
 they have a server and a trunk because they're saying I'm going to build this autopilot computer

1:15:50.640 --> 1:15:57.040
 replaces the driver. So their cost budget is $10,000 or $20,000 and Elon's constraint was I'm going

1:15:57.040 --> 1:16:02.480
 to put one every in every car whether people buy auto ton of striping or not. So the cost

1:16:02.480 --> 1:16:08.240
 constraint he had in mind was great. Right. And to hit that you had to think about the system design

1:16:08.240 --> 1:16:13.840
 that's complicated. It's fun. You know it's like it's like it's Crestman's work like a violin maker

1:16:13.840 --> 1:16:18.800
 right. You can say Strativarius is this incredible thing. The musicians are incredible but the guy

1:16:18.800 --> 1:16:25.280
 making the violin you know picked wood and sanded it and then he cut it you know and he glued it

1:16:25.280 --> 1:16:30.320
 you know and he waited for the right day so that when he put the finish on it it didn't you know

1:16:30.320 --> 1:16:35.760
 do something dumb. That's Crestman's work right. You may be a genius craftsman because you have

1:16:35.760 --> 1:16:42.320
 the best techniques and you discover a new one but most engineers Crestman's work and humans

1:16:42.320 --> 1:16:48.960
 really like to do that. Smart humans. No everybody. All humans. I don't know. I used to I dug ditches

1:16:48.960 --> 1:16:55.120
 when I was in college. I got really good at it. Satisfying. Digging ditches is also craftsman

1:16:55.120 --> 1:17:01.200
 work. Yeah of course. So there's an expression called complex mastery behavior. So when you're

1:17:01.200 --> 1:17:04.880
 learning something that's fine because you're learning something. When you do something it's

1:17:04.880 --> 1:17:09.680
 wrote and simple. It's not that satisfying but if the steps that you have to do are complicated

1:17:09.680 --> 1:17:16.880
 and you're good at them it's satisfying to do them. And then if you're intrigued by it all as

1:17:16.880 --> 1:17:22.160
 you're doing them you sometimes learn new things that you can raise your game but Crestman's work

1:17:22.160 --> 1:17:27.920
 is good. And engineers like engineering is complicated enough that you have to learn a lot

1:17:27.920 --> 1:17:33.280
 of skills and then a lot of what you do is then craftsman's work which is fun.

1:17:33.280 --> 1:17:39.360
 Autonomous driving building a very resource constrained computer. So a computer has to be cheap enough

1:17:39.360 --> 1:17:46.000
 that put in every single car. That's essentially boils down to craftsman's work. It's engineering.

1:17:46.000 --> 1:17:50.560
 You know there's thoughtful decisions and problems to solve and tradeoffs to make. You

1:17:50.560 --> 1:17:55.120
 need 10 camera imports or eight. You know you're building for the current car or the next one.

1:17:55.840 --> 1:17:59.680
 You know how do you do the safety stuff. You know there's a whole bunch of details

1:17:59.680 --> 1:18:05.200
 but it's fun. But it's not like I'm building a new type of neural network which has a new

1:18:05.200 --> 1:18:11.200
 mathematics and a new computer to work. You know that's like there's more invention than that.

1:18:12.240 --> 1:18:16.960
 But the rejection to practice once you pick the architecture you look inside and what do you see.

1:18:16.960 --> 1:18:24.320
 Adders and multipliers and memories and you know the basics. So computers is always this weird set

1:18:24.320 --> 1:18:31.120
 of abstraction layers of ideas and thinking that reduction to practice is transistors and wires and

1:18:32.000 --> 1:18:38.640
 you know pretty basic stuff and that's an interesting phenomena. By the way that like factory work

1:18:38.640 --> 1:18:44.080
 like lots of people think factory work is road assembly stuff. I've been on the assembly line

1:18:44.080 --> 1:18:48.800
 like the people work there really like it. It's a really great job. It's really complicated putting

1:18:48.800 --> 1:18:54.320
 cars together is hard right and the cars moving and the parts are moving and sometimes the parts are

1:18:54.320 --> 1:18:58.880
 damaged and you have to coordinate putting all the stuff together and people are good at it.

1:18:58.880 --> 1:19:03.840
 They're good at it. And I remember one day I went to work and the line was shut down for some reason

1:19:03.840 --> 1:19:08.720
 and some of the guys sitting around were really bummed because they had reorganized a bunch of

1:19:08.720 --> 1:19:12.880
 stuff and they were going to hit a new record for the number of cars built that day and they

1:19:12.880 --> 1:19:19.040
 were all gun hoe to do it and these are big tough buggers and you know but what they did was complicated

1:19:19.040 --> 1:19:23.840
 and you couldn't do it. Yeah and I mean well after a while you could but you'd have to work your way

1:19:23.840 --> 1:19:30.000
 up because you know like putting the bright what's called the brights the trim on a car

1:19:30.880 --> 1:19:35.360
 on a moving assembly line where it has to be attached 25 places in a minute and a half

1:19:35.360 --> 1:19:42.720
 is unbelievably complicated and human beings can do it. It's really good. I think that's

1:19:42.720 --> 1:19:50.080
 harder than driving a car by the way. Putting together working on a factory. Two smart people

1:19:50.080 --> 1:19:56.800
 can disagree. Yay. I think driving a car. We'll get you in the factory someday and then we'll see

1:19:56.800 --> 1:20:00.880
 how you do it. No not for us humans driving a car is easy. I'm saying building a machine

1:20:00.880 --> 1:20:07.680
 that drives a car is not easy. Okay. Okay. Driving a car is easy for humans because

1:20:07.680 --> 1:20:14.560
 we've been evolving for billions of years. Drive cars. Yeah I noticed the pale with the cars are

1:20:14.560 --> 1:20:22.240
 super cool. No. Now you join the rest of the unit and mocking me. Okay. I wasn't mocking you. I was

1:20:22.240 --> 1:20:29.200
 just intrigued by your you know your anthropology. Yeah. I have to go dig into that. There's some

1:20:29.200 --> 1:20:40.080
 inaccuracies there. Yes. Okay. But in general what have you learned in terms of thinking about

1:20:40.960 --> 1:20:51.840
 passion, craftsmanship, tension, chaos. Jesus. The whole mess of it. What have you learned

1:20:51.840 --> 1:20:58.560
 and have taken away from your time working with Elon Musk working at Tesla which is known to be

1:20:58.560 --> 1:21:04.640
 a place of chaos, innovation, craftsmanship and all those things. I really like the way you thought.

1:21:05.280 --> 1:21:10.160
 Like you think you have an understanding about what first principles of something is and then

1:21:10.160 --> 1:21:16.240
 you talk to Elon about it and you didn't scratch the surface. You know he has a deep

1:21:16.240 --> 1:21:22.320
 belief that no matter what you do is a local maximum. Right. I had a friend he invented a

1:21:22.320 --> 1:21:27.760
 better electric motor and it was like a lot better than what we were using and one day he came by

1:21:27.760 --> 1:21:32.480
 he said you know I'm a little disappointed because you know this is really great and you didn't seem

1:21:32.480 --> 1:21:37.840
 that impressed and I said you know when the super intelligent aliens come are they gonna be looking

1:21:37.840 --> 1:21:47.520
 for you. Like where is he? The guy built the motor. Yeah. Probably not you know like but doing

1:21:47.520 --> 1:21:52.160
 interesting work that's both innovative and let's say craftsman's work on the current thing is really

1:21:52.160 --> 1:21:58.880
 satisfying and it's good and that's cool and then Elon was good at taking everything apart

1:21:58.880 --> 1:22:04.400
 and like what's the deep first principle. Oh no what's really no what's really you know you know

1:22:04.400 --> 1:22:11.600
 you know that that you know ability to look at it without assumptions and and how constraints

1:22:11.600 --> 1:22:18.480
 is super wild. You know he built rocket ship and electric car and you know everything

1:22:19.840 --> 1:22:25.200
 and that's super fun and he's into it too. Like when they first landed two SpaceX rockets to Tesla

1:22:25.920 --> 1:22:30.320
 we had a video projector in the big room and like 500 people came down and when they landed

1:22:30.320 --> 1:22:36.400
 everybody cheered and some people cried it was so cool. All right but how did you do that? Well

1:22:38.080 --> 1:22:43.920
 it was super hard and then people say well it's chaotic really to get out of all your

1:22:43.920 --> 1:22:49.920
 assumptions you think that's not going to be unbelievably painful and it was Elon tough

1:22:49.920 --> 1:22:57.680
 yeah probably the people look back on it and say boy I'm really happy I had that experience to go

1:22:57.680 --> 1:23:04.640
 take apart that many layers of assumptions sometimes super fun sometimes painful.

1:23:05.200 --> 1:23:09.520
 So it could be emotionally and intellectually painful that whole process is just stripping away

1:23:09.520 --> 1:23:15.040
 assumptions. Yeah imagine 99% of your thought process is protecting your self conception

1:23:16.400 --> 1:23:23.600
 and 98% of that's wrong. Yeah. Now you got the math right. How do you think you're feeling when

1:23:23.600 --> 1:23:28.720
 you get back into that one bit that's useful and now you're open and you have the ability to do

1:23:28.720 --> 1:23:37.040
 something different. I don't know if I got the math right it might be 99.9 but it ain't 50.

1:23:38.560 --> 1:23:46.560
 Imagining it the 50% is hard enough. Yeah. Now for a long time I've suspected you could get better

1:23:48.240 --> 1:23:51.680
 like you can think better you can think more clearly you can take things apart

1:23:51.680 --> 1:23:56.080
 and there's lots of examples of that people who do that.

1:23:58.240 --> 1:24:03.120
 So and Elon is an example of that apparently you are an example so I don't know if I am

1:24:04.320 --> 1:24:10.000
 I'm fun to talk to certainly I've learned a lot of stuff right well here's the other thing is

1:24:10.000 --> 1:24:15.440
 like I joke like like I read books and people think oh you read books well no I've read a

1:24:15.440 --> 1:24:22.880
 couple of books a week for 55 years well maybe 50 because I didn't read learn read until I was

1:24:22.880 --> 1:24:31.120
 eight or something and it turns out when people write books they often take 20 years of their life

1:24:31.120 --> 1:24:38.160
 where they passionately did something reduce it to 200 pages that's kind of fun and then

1:24:38.160 --> 1:24:42.880
 you go online and you can find out who wrote the best books and who like you know that's kind of

1:24:42.880 --> 1:24:48.160
 wild so there's this wild selection process and then you can read it and for the most part understand

1:24:48.160 --> 1:24:54.880
 it and then you can go apply it like I went to one company I thought I haven't managed much before

1:24:54.880 --> 1:25:00.000
 so I read 20 management books and I started talking to him basically compared to all the

1:25:00.000 --> 1:25:05.040
 VPs running around I'd run night read 19 more management books than anybody else

1:25:05.040 --> 1:25:12.320
 wasn't even that hard yeah and half the stuff worked like first time it wasn't even rocket science

1:25:13.360 --> 1:25:19.840
 but at the core of that is questioning the assumptions or sort of entering the thinking

1:25:19.840 --> 1:25:26.720
 first person principles thinking sort of looking at the reality of the situation and using using

1:25:26.720 --> 1:25:31.920
 that knowledge applying that knowledge so yeah so I would say my brain has this idea that you can

1:25:31.920 --> 1:25:39.120
 question first assumptions and but I can go days at a time and forget that and you have to kind of

1:25:39.120 --> 1:25:46.400
 like circle back that observation because it is because it's hard well it's hard to just keep it

1:25:46.400 --> 1:25:50.400
 front and center because you know you're you operate on so many levels all the time and

1:25:51.040 --> 1:25:57.120
 you know getting this done takes priority or you know being happy takes priority or you know

1:25:57.120 --> 1:26:02.160
 you know screwing around takes priority like like like how you go through life is complicated

1:26:02.880 --> 1:26:07.120
 and then you remember oh yeah I could really uh think first principles you know shit that's

1:26:07.120 --> 1:26:11.120
 that's tiring you know but you do for a while and that's kind of cool

1:26:12.640 --> 1:26:18.400
 so just as a last question in your sense from the big picture from the first principles

1:26:19.440 --> 1:26:24.880
 do you think you kind of answered already but do you think autonomous driving is something

1:26:24.880 --> 1:26:33.680
 we can solve on a timeline of years so one two three five ten years as opposed to a century

1:26:33.680 --> 1:26:40.480
 yeah definitely just to linger on it a little longer where's the confidence coming from is it

1:26:40.480 --> 1:26:45.600
 the fundamentals of the problem the fundamentals of building the hardware and the software

1:26:45.600 --> 1:26:54.960
 where as a computational problem understanding ballistics roles topography it seems pretty

1:26:54.960 --> 1:27:00.160
 solvable I mean and you can see this you know like like speech recognition for a long time

1:27:00.160 --> 1:27:04.400
 people are doing you know frequency and domain analysis and and all kinds of stuff and that

1:27:04.400 --> 1:27:09.920
 didn't work for at all right and then they did deep learning about it and it worked great

1:27:09.920 --> 1:27:18.640
 and it took multiple iterations and you know autonomous driving is way past the frequency

1:27:18.640 --> 1:27:25.440
 analysis point you know use radar don't run into things and the data gathering is going up and the

1:27:25.440 --> 1:27:29.280
 computation is going up and the algorithm understanding is going up and there's a whole

1:27:29.280 --> 1:27:34.560
 bunch of problems getting solved like that the data side is really powerful but I disagree with

1:27:34.560 --> 1:27:41.200
 both you and you and I'll tell you on once again as I did before that that when you add human beings

1:27:41.200 --> 1:27:48.240
 into the picture the it's no longer a ballistics problem it's something more complicated but I

1:27:48.240 --> 1:27:54.080
 could be very well proven wrong and cars are highly damped in terms of rate of change like the

1:27:54.080 --> 1:27:59.360
 steering and the steering system is really slow compared to a computer the acceleration the acceleration

1:27:59.360 --> 1:28:05.120
 is really slow yeah on a certain time scale on a ballistics time scale but human behavior I don't

1:28:05.120 --> 1:28:11.520
 know yet I I shouldn't say the beans are really slow too we weirdly we operate you know half a

1:28:11.520 --> 1:28:18.640
 second behind reality I'll be really understands that one either it's pretty funny yeah yeah so

1:28:19.760 --> 1:28:25.440
 yeah I would be with very well could be surprised and I think with the rate of improvement in all

1:28:25.440 --> 1:28:30.400
 aspects on both the compute and the the software and the hardware there's going to be pleasant

1:28:30.400 --> 1:28:37.760
 surprises all over the place yeah speaking of unpleasant surprises many people have worries

1:28:37.760 --> 1:28:45.200
 about a singularity in the development of AI forgive me for such questions yeah when AI improves

1:28:45.200 --> 1:28:51.600
 the exponential and reaches a point of superhuman level general intelligence you know beyond the

1:28:51.600 --> 1:28:56.640
 point there's no looking back do you share this worry of existential threats from artificial

1:28:56.640 --> 1:29:04.640
 intelligence from computers becoming superhuman level intelligent no not really you know like we

1:29:04.640 --> 1:29:10.240
 already have a very stratified society and then if you look at the whole animal kingdom of capabilities

1:29:10.240 --> 1:29:16.880
 and abilities and interests and you know smart people have their niche and you know normal people

1:29:16.880 --> 1:29:22.880
 have their niche and craftsmen's have their niche and you know animals have their niche I

1:29:22.880 --> 1:29:29.680
 suspect that the domains of interest for things that you know astronomically different like the

1:29:29.680 --> 1:29:34.480
 whole something got 10 times smarter than us and wanted to track us all down because what

1:29:34.480 --> 1:29:40.560
 we like to have coffee at Starbucks like it doesn't seem plausible no is there an existential problem

1:29:40.560 --> 1:29:44.960
 that how do you live in a world where there's something way smarter than you and you based

1:29:44.960 --> 1:29:51.200
 your kind of self esteem on being the smartest local person well there's what 0.1 of the population

1:29:51.200 --> 1:29:55.680
 who thinks that because the rest of the population has been dealing with it since they were born

1:29:56.640 --> 1:30:03.280
 so the the breath of possible experience that can be interesting is really big

1:30:04.800 --> 1:30:13.040
 and you know superintelligence seems likely although we still don't know if we're magical

1:30:13.040 --> 1:30:19.520
 but I suspect we're not and it seems likely that will create possibilities that are interesting

1:30:19.520 --> 1:30:28.080
 for us and its its interests will be interesting for that for whatever it is it's not obvious why

1:30:28.080 --> 1:30:34.800
 its interests would somehow want to fight over some square foot of dirt or you know whatever

1:30:34.800 --> 1:30:40.160
 you know the usual fears are about so you don't think you'll inherit some of the darker aspects

1:30:40.160 --> 1:30:48.240
 of human nature depends on how you think reality is constructed so for for whatever reason human

1:30:48.240 --> 1:30:55.200
 beings are in let's say creative tension and opposition with both our good and bad forces

1:30:55.200 --> 1:31:01.680
 like there's lots of philosophical understanding of that right I don't know why that would be

1:31:01.680 --> 1:31:08.320
 different so you think the evil is is necessary for the good I mean the the tension I don't know

1:31:08.320 --> 1:31:13.840
 about evil but like we live in a competitive world where your good is somebody else's

1:31:14.960 --> 1:31:20.640
 you know evil you know there's there's the malignant part of it but that seems to be

1:31:21.760 --> 1:31:25.920
 self limiting although occasionally it's it's super horrible

1:31:27.280 --> 1:31:32.880
 but yes there's a debate over ideas and some people have different beliefs and that that

1:31:32.880 --> 1:31:39.200
 debate itself is a process so that at arriving at something yeah and why wouldn't that continue

1:31:39.200 --> 1:31:45.280
 yeah just you but you don't think that whole process will leave humans behind in a way that's

1:31:45.280 --> 1:31:51.280
 painful emotionally painful yes for the one for the point one percent they'll be you know why isn't

1:31:51.280 --> 1:31:55.920
 it already painful for a large percentage of the population and it is I mean society does have a

1:31:55.920 --> 1:32:01.120
 lot of stress in it about the one percent and the about to this and about to that but you know

1:32:01.120 --> 1:32:05.280
 everybody has a lot of stress in a life about what they find satisfying and

1:32:06.240 --> 1:32:12.400
 and you know know yourself seems to be the proper dictum and pursue something that makes

1:32:12.400 --> 1:32:19.920
 your life meaningful seems proper and there's so many avenues on that like there's so much

1:32:19.920 --> 1:32:28.480
 unexplored space at every single level you know I'm I'm somewhat of my nephew called me a jaded

1:32:28.480 --> 1:32:39.040
 optimist and you know so it's there's a beautiful tension that in that label but if you were to

1:32:39.040 --> 1:32:47.600
 look back at your life and could relive a moment a set of moments because there were the happiest

1:32:47.600 --> 1:32:56.960
 times of your life outside of family what would that be I don't want to relive any moments I like

1:32:56.960 --> 1:33:04.000
 that I like that situation where you have some amount of optimism and then the the anxiety of the

1:33:04.000 --> 1:33:11.600
 unknown so you love the unknown the the mystery of it I don't know about the mystery you sure get

1:33:11.600 --> 1:33:18.800
 your blood pumping what do you think is the meaning of this whole thing of life on this

1:33:18.800 --> 1:33:30.000
 pale blue dot it seems to be what it does like the universe for whatever reason makes atoms

1:33:30.000 --> 1:33:38.080
 which makes us which we do stuff and we figure out things and we explore things and that's

1:33:38.080 --> 1:33:46.480
 just what it is it's not just yeah it is you know Jim I don't think there's a better place to end

1:33:46.480 --> 1:33:52.640
 it it's a huge honor and uh well that was super fun thank you so much for talking today all right

1:33:52.640 --> 1:33:59.200
 great thanks for listening to this conversation and thank you to our presenting sponsor cash app

1:33:59.200 --> 1:34:05.600
 download it use code lex podcast you'll get $10 and $10 will go to first a stem education

1:34:05.600 --> 1:34:10.880
 nonprofit that inspires hundreds of thousands of young minds to become future leaders and

1:34:10.880 --> 1:34:16.880
 innovators if you enjoy this podcast subscribe on youtube give it five stars an apple podcast

1:34:16.880 --> 1:34:23.360
 follow on spotify support on patreon or simply connect with me on twitter and now let me leave

1:34:23.360 --> 1:34:30.240
 you with some words of wisdom from gordon more if everything you try works you aren't trying hard

1:34:30.240 --> 1:34:43.280
 enough thank you for listening and hope to see you next time

