WEBVTT

00:00.000 --> 00:02.800
 By the time we get to 2045,

00:02.800 --> 00:05.320
 we'll be able to multiply our intelligence

00:05.320 --> 00:07.680
 many millions full.

00:07.680 --> 00:10.800
 And it's just very hard to imagine what that will be like.

00:13.560 --> 00:16.840
 The following is a conversation with Ray Kurzweil,

00:16.840 --> 00:19.480
 author, inventor, and futurist,

00:19.480 --> 00:22.280
 who has an optimistic view of our future

00:22.280 --> 00:24.320
 as a human civilization,

00:24.320 --> 00:27.280
 predicting that exponentially improving technologies

00:27.280 --> 00:29.880
 will take us to a point of a singularity,

00:29.880 --> 00:33.480
 beyond which superintelligent, artificial intelligence

00:33.480 --> 00:38.400
 will transform our world in nearly unimaginable ways.

00:38.400 --> 00:41.320
 18 years ago, in the book Singularity is Near,

00:41.320 --> 00:44.000
 he predicted that the onset of the singularity

00:44.000 --> 00:47.360
 will happen in the year 2045.

00:47.360 --> 00:50.800
 He still holds to this prediction and estimate.

00:50.800 --> 00:53.440
 In fact, he's working on a new book on this topic

00:53.440 --> 00:55.640
 that will hopefully be out next year.

00:56.520 --> 00:58.280
 This is the Lex Friedman podcast.

00:58.280 --> 01:00.360
 To support it, please check out our sponsors

01:00.360 --> 01:01.640
 in the description.

01:01.640 --> 01:05.360
 And now, dear friends, here's Ray Kurzweil.

01:06.360 --> 01:10.960
 In your 2005 book titled The Singularity is Near,

01:10.960 --> 01:15.400
 you predicted that the singularity will happen in 2045.

01:15.400 --> 01:17.640
 So now 18 years later,

01:17.640 --> 01:20.880
 do you still estimate that the singularity will happen

01:20.880 --> 01:22.480
 on 2045?

01:22.480 --> 01:24.960
 And maybe first, what is the singularity,

01:24.960 --> 01:27.760
 the technological singularity, and when will it happen?

01:27.760 --> 01:31.640
 Singularity is where computers really change our view

01:31.640 --> 01:35.840
 of what's important and change who we are.

01:35.840 --> 01:39.560
 But we're getting close to some salient things

01:39.560 --> 01:42.840
 that will change who we are.

01:42.840 --> 01:45.680
 The key thing is 2029,

01:45.680 --> 01:49.080
 when computers will pass the Turing test.

01:50.120 --> 01:51.560
 And there's also some controversy

01:51.560 --> 01:55.080
 whether the Turing test is valid, I believe it is.

01:55.080 --> 01:57.920
 Most people do believe that,

01:57.920 --> 01:59.680
 but there's some controversy about that.

01:59.680 --> 02:04.680
 But Stanford got very alarmed at my prediction about 2029.

02:06.520 --> 02:10.520
 I made this in 1999, in my book.

02:10.520 --> 02:12.440
 The Age of Spiritual Machines.

02:12.440 --> 02:15.480
 And then you repeated the prediction in 2005.

02:15.480 --> 02:16.600
 In 2005.

02:16.600 --> 02:17.520
 Yeah.

02:17.520 --> 02:19.480
 So they held an international conference,

02:19.480 --> 02:21.160
 you might've been aware of it,

02:21.160 --> 02:23.840
 of AI experts in 1999

02:23.840 --> 02:26.600
 to assess this view.

02:26.600 --> 02:30.840
 So people gave different predictions and they took a poll.

02:30.840 --> 02:32.080
 It was really the first time

02:32.080 --> 02:36.600
 that AI experts worldwide were polled on this prediction.

02:37.720 --> 02:39.960
 And the average poll was 100 years.

02:41.400 --> 02:44.320
 20% believed it would never happen.

02:44.320 --> 02:48.120
 And that was the view in 1999.

02:48.120 --> 02:50.640
 80% believed it would happen,

02:50.640 --> 02:53.200
 but not within their lifetimes.

02:53.200 --> 02:55.840
 There's been so many advances in AI

02:56.920 --> 03:01.840
 that the poll of AI experts has come down over the years.

03:01.840 --> 03:05.400
 So a year ago, something called meticulous,

03:05.400 --> 03:07.120
 which you may be aware of,

03:07.120 --> 03:11.560
 assessed as different types of experts on the future.

03:11.560 --> 03:16.440
 They again assessed what AI experts then felt.

03:16.440 --> 03:18.960
 And they were saying 2042.

03:18.960 --> 03:20.440
 For the Turing test.

03:20.440 --> 03:22.440
 For the Turing test.

03:22.440 --> 03:23.640
 Yes, that's coming down.

03:23.640 --> 03:26.360
 And I was still saying 2029.

03:26.360 --> 03:30.240
 A few weeks ago, they again did another poll

03:30.240 --> 03:31.720
 and it was 2030.

03:33.000 --> 03:37.960
 So AI experts now basically agree with me.

03:37.960 --> 03:39.280
 I haven't changed at all.

03:39.280 --> 03:41.240
 I've stayed with 2029.

03:42.880 --> 03:44.560
 And AI experts now agree with me,

03:44.560 --> 03:46.880
 but they didn't agree at first.

03:46.880 --> 03:51.000
 So Alan Turing formulated the Turing test and...

03:51.000 --> 03:54.480
 Right, now what he said was very little about it.

03:54.480 --> 03:57.120
 I mean, the 1950 paper where he had articulated

03:57.120 --> 03:58.600
 the Turing test,

03:58.600 --> 04:03.600
 he just like a few lines that talk about the Turing test.

04:06.880 --> 04:11.880
 And it really wasn't very clear how to administer it.

04:12.040 --> 04:16.520
 And he said if they did it in like 15 minutes,

04:16.520 --> 04:17.680
 that would be sufficient,

04:17.680 --> 04:20.680
 which I don't really think is the case.

04:20.680 --> 04:22.920
 These large language models now,

04:22.920 --> 04:25.520
 some people are convinced by it already.

04:25.520 --> 04:28.440
 I mean, you can talk to it and have a conversation with you.

04:28.440 --> 04:30.440
 You can actually talk to it for hours.

04:31.720 --> 04:35.320
 So it requires a little more depth.

04:35.320 --> 04:38.080
 There's some problems with large language models,

04:38.080 --> 04:39.600
 which we can talk about.

04:41.800 --> 04:46.400
 But some people are convinced by the Turing test.

04:46.400 --> 04:50.120
 Now, if somebody passes the Turing test,

04:50.120 --> 04:52.120
 what are the implications of that?

04:52.120 --> 04:53.720
 Does that mean that they're sentient,

04:53.720 --> 04:55.960
 that they're conscious or not?

04:55.960 --> 05:00.840
 It's not necessarily clear what the implications are.

05:00.840 --> 05:05.840
 Anyway, I believe 2029, that's six, seven years from now,

05:07.640 --> 05:10.320
 we'll have something that passes the Turing test

05:10.320 --> 05:12.440
 and a valid Turing test,

05:12.440 --> 05:15.280
 meaning it goes for hours, not just a few minutes.

05:15.280 --> 05:16.560
 Can you speak to that a little bit?

05:16.560 --> 05:21.120
 What is your formulation of the Turing test?

05:21.120 --> 05:23.160
 You've proposed a very difficult version

05:23.160 --> 05:25.400
 of the Turing test, so what does that look like?

05:25.400 --> 05:28.560
 Basically, it's just to assess it over several hours

05:30.760 --> 05:35.760
 and also have a human judge that's fairly sophisticated

05:36.440 --> 05:39.200
 on what computers can do and can't do.

05:40.760 --> 05:43.800
 If you take somebody who's not that sophisticated

05:43.800 --> 05:47.200
 or even an average engineer,

05:48.360 --> 05:52.080
 they may not really assess various aspects of it.

05:52.080 --> 05:55.680
 So you really want the human to challenge the system?

05:55.680 --> 05:57.040
 Exactly, exactly.

05:57.040 --> 05:58.640
 On its ability to do things like

05:58.640 --> 06:00.800
 common sense reasoning, perhaps?

06:00.800 --> 06:04.680
 That's actually a key problem with large language models.

06:04.680 --> 06:10.160
 They don't do these kinds of tests

06:10.160 --> 06:15.040
 that would involve assessing chains of reasoning.

06:17.400 --> 06:18.960
 But you can lose track of that.

06:18.960 --> 06:21.480
 If you talk to them, they actually can talk to you

06:21.480 --> 06:24.840
 pretty well and you can be convinced by it.

06:24.840 --> 06:27.400
 But it's somebody that would really convince you

06:27.400 --> 06:31.960
 that it's a human, whatever that takes.

06:31.960 --> 06:34.800
 Maybe it would take days or weeks,

06:34.800 --> 06:38.720
 but it would really convince you that it's human.

06:40.880 --> 06:45.320
 Large language models can appear that way.

06:45.320 --> 06:49.760
 You can read conversations and they appear pretty good.

06:49.760 --> 06:52.240
 There are some problems with it.

06:52.240 --> 06:54.120
 It doesn't do math very well.

06:55.000 --> 06:58.160
 You can ask how many legs did 10 elephants have

06:58.160 --> 07:00.040
 and they'll tell you, well, okay,

07:00.040 --> 07:02.360
 each elephant has four legs and it's 10 elephants,

07:02.360 --> 07:03.720
 so it's 40 legs.

07:03.720 --> 07:05.840
 And you go, okay, that's pretty good.

07:05.840 --> 07:07.920
 How many legs do 11 elephants have?

07:07.920 --> 07:11.480
 And they don't seem to understand the question.

07:11.480 --> 07:14.120
 Do all humans understand that question?

07:14.120 --> 07:15.840
 No, that's the key thing.

07:15.840 --> 07:19.400
 I mean, how advanced the human do you want it to be?

07:19.400 --> 07:21.800
 But we do expect a human to be able

07:21.800 --> 07:23.960
 to do multi chain reasoning,

07:24.800 --> 07:28.280
 to be able to take a few facts and put them together.

07:28.280 --> 07:29.800
 Not perfectly.

07:29.800 --> 07:32.760
 And we see that in a lot of polls

07:32.760 --> 07:35.880
 that people don't do that perfectly at all, but.

07:39.200 --> 07:42.000
 So it's not very well defined,

07:42.000 --> 07:44.280
 but it's something where it really would convince you

07:44.280 --> 07:45.600
 that it's a human.

07:45.600 --> 07:48.840
 Is your intuition that large language models

07:48.840 --> 07:52.320
 will not be solely the kind of system

07:52.320 --> 07:55.560
 that passes the Turing test in 2029?

07:55.560 --> 07:56.760
 Do we need something else?

07:56.760 --> 07:58.680
 No, I think it will be a large language model,

07:58.680 --> 08:02.920
 but they have to go beyond what they're doing now.

08:02.920 --> 08:04.360
 I think we're getting there.

08:05.720 --> 08:09.200
 And another key issue is if somebody

08:09.200 --> 08:12.160
 actually passes the Turing test validly,

08:12.160 --> 08:13.600
 I would believe they're conscious.

08:13.600 --> 08:14.960
 And then not everybody would say that.

08:14.960 --> 08:17.400
 It's okay, we can pass the Turing test,

08:17.400 --> 08:20.040
 but we don't really believe that it's conscious.

08:20.040 --> 08:21.480
 That's a whole nother issue.

08:23.080 --> 08:24.880
 But if it really passes the Turing test,

08:24.880 --> 08:26.680
 I would believe that it's conscious.

08:26.680 --> 08:31.680
 But I don't believe that of large language models today.

08:32.760 --> 08:35.520
 If it appears to be conscious,

08:35.520 --> 08:38.240
 that's as good as being conscious, at least for you,

08:38.240 --> 08:40.720
 in some sense.

08:40.720 --> 08:45.280
 I mean, consciousness is not something that's scientific.

08:46.640 --> 08:48.880
 I mean, I believe you're conscious,

08:49.760 --> 08:51.120
 but it's really just a belief.

08:51.120 --> 08:52.800
 And we believe that about other humans

08:52.800 --> 08:57.400
 that at least appear to be conscious.

08:57.400 --> 09:00.480
 When you go outside of shared human assumption,

09:01.720 --> 09:03.640
 like our animals conscious,

09:04.520 --> 09:06.200
 some people believe they're not conscious,

09:06.200 --> 09:08.680
 some people believe they are conscious,

09:08.680 --> 09:13.680
 and would a machine that acts just like a human be conscious?

09:14.520 --> 09:16.240
 I mean, I believe it would be,

09:17.040 --> 09:20.720
 but that's really a philosophical belief.

09:20.720 --> 09:22.680
 It's not, you can't prove it.

09:22.680 --> 09:25.440
 I can't take an entity and prove that it's conscious.

09:25.440 --> 09:30.320
 There's nothing that you can do that would indicate that.

09:30.320 --> 09:32.760
 It's like saying a piece of art is beautiful.

09:32.760 --> 09:36.600
 You can say it, multiple people can experience

09:36.600 --> 09:41.280
 a piece of art is beautiful, but you can't prove it.

09:41.280 --> 09:44.800
 But it's also an extremely important issue.

09:44.800 --> 09:47.000
 I mean, imagine if you had something

09:47.000 --> 09:52.640
 with nobody's conscious, the world may as well not exist.

09:55.640 --> 10:00.000
 And so some people, like say, Marvin Rinsky,

10:02.600 --> 10:05.920
 said, well, consciousness is not logical,

10:05.920 --> 10:08.360
 it's not scientific, and therefore we should dismiss it.

10:08.360 --> 10:13.360
 And any talk about consciousness is just not to be believed.

10:13.360 --> 10:17.360
 But when he actually engaged with somebody who was conscious,

10:17.360 --> 10:20.360
 he actually acted as if they were conscious.

10:20.360 --> 10:22.200
 He didn't ignore that.

10:22.200 --> 10:24.760
 He acted as if consciousness does matter.

10:24.760 --> 10:27.960
 Exactly. Whereas he said it didn't matter.

10:27.960 --> 10:29.800
 Well, that's Marvin Rinsky.

10:29.800 --> 10:30.640
 Yeah.

10:30.640 --> 10:32.000
 He's full of contradictions.

10:32.000 --> 10:35.200
 But that's true of a lot of people as well.

10:35.200 --> 10:37.440
 But to you, consciousness matters.

10:37.440 --> 10:39.720
 But to me, it's very important,

10:39.720 --> 10:43.480
 but I would say it's not a scientific issue.

10:45.480 --> 10:47.320
 It's a philosophical issue.

10:47.320 --> 10:48.720
 And people have different views.

10:48.720 --> 10:50.520
 And some people believe that anything

10:50.520 --> 10:52.520
 that makes a decision is conscious.

10:52.520 --> 10:54.520
 So your light switch is conscious.

10:54.520 --> 10:57.120
 It's the level of consciousness that's low.

10:57.120 --> 11:01.120
 It's not very interesting, but that's a consciousness.

11:02.120 --> 11:04.920
 And anything, so a computer that makes

11:04.920 --> 11:08.520
 a more interesting decision, still not a human being.

11:08.520 --> 11:11.320
 Still not at human levels, but it's also conscious

11:11.320 --> 11:13.720
 and at a higher level than your light switch.

11:13.720 --> 11:15.720
 So that's one view.

11:15.720 --> 11:19.720
 There's many different views of what consciousness is.

11:19.720 --> 11:22.720
 So if a system passes the Turing test,

11:22.720 --> 11:25.720
 it's not scientific.

11:25.720 --> 11:29.720
 But in the issues of philosophy,

11:29.720 --> 11:31.720
 things like ethics start to enter the picture.

11:31.720 --> 11:34.720
 Do you think there would be...

11:34.720 --> 11:38.920
 We would start contending as a human species

11:38.920 --> 11:42.920
 about the ethics of turning off such a machine.

11:42.920 --> 11:46.920
 Yeah, I mean, that's definitely come up.

11:46.920 --> 11:48.920
 It hasn't come up in reality yet.

11:48.920 --> 11:51.920
 But I'm talking about 2029.

11:51.920 --> 11:54.920
 It's not that many years from now.

11:54.920 --> 11:58.920
 And so what are our obligations to it?

11:58.920 --> 12:00.920
 It has a different...

12:00.920 --> 12:02.920
 I mean, a computer that's conscious

12:02.920 --> 12:11.920
 has a little bit different connotations than a human.

12:11.920 --> 12:14.920
 We have a continuous consciousness.

12:14.920 --> 12:20.920
 We're in an entity that does not last forever.

12:20.920 --> 12:26.920
 Now, actually, a significant portion of humans still exist

12:26.920 --> 12:30.920
 and are therefore still conscious.

12:30.920 --> 12:36.920
 But anybody who is over a certain age doesn't exist anymore.

12:36.920 --> 12:39.920
 That wouldn't be true of a computer program.

12:39.920 --> 12:41.920
 You could completely turn it off

12:41.920 --> 12:45.920
 and a copy of it could be stored and you could recreate it.

12:45.920 --> 12:50.920
 And so it has a different type of validity.

12:50.920 --> 12:52.920
 You could actually take it back in time.

12:52.920 --> 12:55.920
 You could eliminate its memory and have it go over again.

12:55.920 --> 12:59.920
 I mean, it has a different kind of connotation

12:59.920 --> 13:01.920
 than humans do.

13:01.920 --> 13:03.920
 Well, perhaps you can do the same thing with humans.

13:03.920 --> 13:06.920
 It's just that we don't know how to do that yet.

13:06.920 --> 13:11.920
 It's possible that we figure out all of these things on the machine first.

13:11.920 --> 13:14.920
 But that doesn't mean the machine isn't conscious.

13:14.920 --> 13:17.920
 I mean, if you look at the way people react,

13:17.920 --> 13:24.920
 say three CPO or other machines that are conscious in movies,

13:24.920 --> 13:26.920
 they don't actually present how it's conscious.

13:26.920 --> 13:29.920
 We see that they are a machine

13:29.920 --> 13:32.920
 and people will believe that they are conscious

13:32.920 --> 13:36.920
 and they'll actually worry about it if they get in trouble and so on.

13:36.920 --> 13:42.920
 So, 2029 is going to be the first year when a major thing happens.

13:42.920 --> 13:43.920
 Right.

13:43.920 --> 13:49.920
 And that will shake our civilization to start to consider the role of AI.

13:49.920 --> 13:50.920
 Yes and no.

13:50.920 --> 13:57.920
 I mean, this one guy at Google claimed that the machine was conscious.

13:57.920 --> 13:59.920
 That's just one person.

13:59.920 --> 14:00.920
 Right.

14:00.920 --> 14:02.920
 When it starts to happen to scale.

14:02.920 --> 14:07.920
 Well, that's exactly right because most people have not taken that position.

14:07.920 --> 14:08.920
 I don't take that position.

14:08.920 --> 14:16.920
 I mean, I've used different things like this

14:16.920 --> 14:19.920
 and they don't appear to me to be conscious.

14:19.920 --> 14:25.920
 As we eliminate various problems of these large language models,

14:25.920 --> 14:29.920
 more and more people will accept that they're conscious.

14:29.920 --> 14:31.920
 So, when we get to 2029,

14:31.920 --> 14:37.920
 I think a large fraction of people will believe that they're conscious.

14:37.920 --> 14:40.920
 So, it's not going to happen all at once.

14:40.920 --> 14:43.920
 I believe that what actually happened gradually

14:43.920 --> 14:46.920
 and it's already started to happen.

14:46.920 --> 14:51.920
 And so, that takes us one step closer to the singularity.

14:51.920 --> 14:59.920
 Another step then is in the 2030s when we can actually connect our neocortex,

14:59.920 --> 15:04.920
 which is where we do our thinking, to computers.

15:04.920 --> 15:11.920
 And I mean, just as this actually gains a lot to being connected to computers

15:11.920 --> 15:14.920
 that will amplify its abilities.

15:14.920 --> 15:18.920
 I mean, if this did not have any connection, it would be pretty stupid.

15:18.920 --> 15:21.920
 It could not answer any of your questions.

15:21.920 --> 15:23.920
 If you're just listening to this, by the way,

15:23.920 --> 15:29.920
 we're just holding up the all powerful smartphone.

15:29.920 --> 15:32.920
 So, we're going to do that directly from our brains.

15:32.920 --> 15:34.920
 I mean, these are pretty good.

15:34.920 --> 15:36.920
 These already have amplified our intelligence.

15:36.920 --> 15:41.920
 I'm already much smarter than I would otherwise be if I didn't have this.

15:41.920 --> 15:48.920
 Because I remember my first book, The Age of Intelligent Machines,

15:48.920 --> 15:51.920
 there was no way to get information from computers.

15:51.920 --> 15:54.920
 I actually would go to a library, find a book,

15:54.920 --> 15:57.920
 find the page that had an information I wanted,

15:57.920 --> 16:03.920
 and I'd go to the copier and my most significant information tool

16:03.920 --> 16:07.920
 was a roll of quarters where I could see the copier.

16:07.920 --> 16:12.920
 So, we're already greatly advanced that we have these things.

16:12.920 --> 16:14.920
 There's a few problems with it.

16:14.920 --> 16:18.920
 First of all, I constantly put it down, and I don't remember where I put it.

16:18.920 --> 16:23.920
 I've actually never lost it, but you have to find it,

16:23.920 --> 16:25.920
 and then you have to turn it on.

16:25.920 --> 16:27.920
 So, there's a certain amount of steps.

16:27.920 --> 16:32.920
 It would actually be quite useful if someone would just listen to your conversation

16:32.920 --> 16:40.920
 and say, oh, that's so and so actress and tell you what you're talking about.

16:40.920 --> 16:45.920
 So, going from active to passive where it just permeates your whole life.

16:45.920 --> 16:46.920
 Yeah, exactly.

16:46.920 --> 16:48.920
 The way your brain does when you're awake.

16:48.920 --> 16:50.920
 Your brain is always there.

16:50.920 --> 16:51.920
 Right.

16:51.920 --> 16:55.920
 That's something that could actually just about be done today

16:55.920 --> 16:58.920
 where you would listen to your conversation, understand what you're saying,

16:58.920 --> 17:03.920
 understand what you're not missing and give you that information.

17:03.920 --> 17:06.920
 But another step is to actually go inside your brain.

17:06.920 --> 17:07.920
 Yeah.

17:07.920 --> 17:14.920
 And there are some prototypes where you can connect your brain.

17:14.920 --> 17:18.920
 They actually don't have the amount of bandwidth that we need.

17:18.920 --> 17:21.920
 They can work, but they work fairly slowly.

17:21.920 --> 17:26.920
 So, if it actually would connect to your neocortex,

17:26.920 --> 17:31.920
 and the neocortex which I described and how to create a mind,

17:31.920 --> 17:37.920
 the neocortex is actually, it has different levels.

17:37.920 --> 17:41.920
 And as you go up the levels, it's kind of like a pyramid.

17:41.920 --> 17:43.920
 The top level is fairly small.

17:43.920 --> 17:53.920
 And that's the level where you want to connect these brain extenders.

17:53.920 --> 17:55.920
 So, I believe that will happen in the 2030s.

17:55.920 --> 18:03.920
 So, just the way this is greatly amplified by being connected to the cloud,

18:03.920 --> 18:13.920
 we can connect our own brain to the cloud and just do what we can do by using this machine.

18:13.920 --> 18:18.920
 Do you think it would look like the brain computer interface of like Neuralink?

18:18.920 --> 18:21.920
 Well, Neuralink is an attempt to do that.

18:21.920 --> 18:26.920
 It doesn't have the bandwidth that we need yet.

18:26.920 --> 18:27.920
 Right.

18:27.920 --> 18:31.920
 But I think, I mean, they're going to get permission for this

18:31.920 --> 18:34.920
 because there are a lot of people who absolutely need it

18:34.920 --> 18:36.920
 because they can't communicate.

18:36.920 --> 18:41.920
 I know a couple of people like that who have ideas and they cannot,

18:41.920 --> 18:46.920
 they cannot move their muscles and so on, they can't communicate.

18:46.920 --> 18:51.920
 So, for them, this would be very valuable.

18:51.920 --> 18:54.920
 But we could all use it.

18:54.920 --> 19:02.920
 Basically, it would turn us into something that would be like we have a phone,

19:02.920 --> 19:06.920
 but it would be in our minds, it would be kind of instantaneous.

19:06.920 --> 19:13.920
 And maybe communication between two people would not require this low bandwidth mechanism of language.

19:13.920 --> 19:14.920
 Yes.

19:14.920 --> 19:16.920
 Exactly. We don't know what that would be.

19:16.920 --> 19:23.920
 Although we do know that computers can share information like language instantly.

19:23.920 --> 19:30.920
 They can share many, many books in a second, so we could do that as well.

19:30.920 --> 19:39.920
 If you look at what our brain does, it actually can manipulate different parameters.

19:39.920 --> 19:45.920
 So, we talk about these large language models.

19:45.920 --> 19:57.920
 I mean, I had written that it requires a certain amount of information in order to be effective

19:57.920 --> 20:03.920
 and that we would not see AI really being effective until it got to that level.

20:03.920 --> 20:08.920
 And we had large language models that were like 10 billion bytes, didn't work very well.

20:08.920 --> 20:12.920
 They finally got to 100 billion bytes and now they work fairly well.

20:12.920 --> 20:15.920
 And now we're going to a trillion bytes.

20:15.920 --> 20:22.920
 If you say lambda has 100 billion bytes, what does that mean?

20:22.920 --> 20:27.920
 Well, what if you had something that had one byte, one parameter?

20:27.920 --> 20:32.920
 Maybe you want to tell whether or not something is an elephant or not.

20:32.920 --> 20:36.920
 And so, you put in something that would detect its trunk.

20:36.920 --> 20:38.920
 If it has a trunk, it's an elephant.

20:38.920 --> 20:40.920
 If it doesn't have a trunk, it's not an elephant.

20:40.920 --> 20:43.920
 And that would work fairly well.

20:43.920 --> 20:45.920
 There's a few problems with it.

20:45.920 --> 20:49.920
 I really wouldn't be able to tell what a trunk is, but anyway.

20:49.920 --> 20:52.920
 And maybe other things other than elephants have trunks.

20:52.920 --> 20:54.920
 You might get really confused.

20:54.920 --> 20:55.920
 Yeah, exactly.

20:55.920 --> 20:59.920
 I'm not sure which animals have trunks, but you know.

20:59.920 --> 21:01.920
 How do you define a trunk?

21:01.920 --> 21:03.920
 But yeah, that's one parameter.

21:03.920 --> 21:05.920
 You can do okay.

21:05.920 --> 21:08.920
 So these things have 100 billion parameters.

21:08.920 --> 21:11.920
 So they're able to deal with very complex issues.

21:11.920 --> 21:13.920
 All kinds of trunks.

21:13.920 --> 21:15.920
 Human beings actually have a little bit more than that,

21:15.920 --> 21:21.920
 but they're getting to the point where they can emulate humans.

21:21.920 --> 21:26.920
 If we were able to connect this to our neocortex,

21:26.920 --> 21:34.920
 we would basically add more of these abilities to make distinctions.

21:34.920 --> 21:39.920
 And it could ultimately be much smarter and also be attached to information

21:39.920 --> 21:42.920
 that we feel is reliable.

21:42.920 --> 21:44.920
 So that's where we're headed.

21:44.920 --> 21:48.920
 So you think that there will be a merger in the 30s

21:48.920 --> 21:55.920
 and increasing amount of merging between the human brain and the AI brain.

21:55.920 --> 21:57.920
 Exactly.

21:57.920 --> 22:01.920
 And the AI brain is really an emulation of human beings.

22:01.920 --> 22:03.920
 I mean, that's why we're creating them.

22:03.920 --> 22:06.920
 Because human beings act the same way,

22:06.920 --> 22:08.920
 and this is basically to amplify them.

22:08.920 --> 22:11.920
 I mean, this amplifies our brain.

22:11.920 --> 22:14.920
 It's a little bit clumsy to interact with,

22:14.920 --> 22:20.920
 but it definitely is way beyond what we had 15 years ago.

22:20.920 --> 22:22.920
 But the implementation becomes different,

22:22.920 --> 22:25.920
 just like a bird versus the airplane.

22:25.920 --> 22:29.920
 Even though the AI brain is an emulation,

22:29.920 --> 22:33.920
 it starts adding features we might not otherwise have,

22:33.920 --> 22:37.920
 like ability to consume a huge amount of information quickly,

22:37.920 --> 22:42.920
 like look up thousands of Wikipedia articles in one take.

22:42.920 --> 22:43.920
 Exactly.

22:43.920 --> 22:47.920
 We can get, for example, to issues like simulated biology

22:47.920 --> 22:55.920
 where it can simulate many different things at once.

22:55.920 --> 22:59.920
 We already had one example of simulated biology,

22:59.920 --> 23:03.920
 which is the Moderna vaccine.

23:03.920 --> 23:10.920
 And that's going to be now the way in which we create medications.

23:10.920 --> 23:15.920
 But they were able to simulate what each example of an mRNA

23:15.920 --> 23:17.920
 would do to a human being,

23:17.920 --> 23:20.920
 and they were able to simulate that quite reliably.

23:20.920 --> 23:26.920
 And we actually simulated billions of different mRNA sequences,

23:26.920 --> 23:28.920
 and they found the ones that were the best

23:28.920 --> 23:30.920
 and they created the vaccine.

23:30.920 --> 23:33.920
 And they did, and talked about doing that quickly,

23:33.920 --> 23:35.920
 they did that in two days.

23:35.920 --> 23:37.920
 Now, how long would a human being take

23:37.920 --> 23:40.920
 to simulate billions of different mRNA sequences?

23:40.920 --> 23:42.920
 I don't know that we could do it at all,

23:42.920 --> 23:45.920
 but it would take many years.

23:45.920 --> 23:48.920
 They did it in two days.

23:48.920 --> 23:52.920
 One of the reasons that people didn't like vaccines

23:52.920 --> 23:55.920
 is because it was done too quickly.

23:55.920 --> 23:57.920
 It was done too fast.

23:57.920 --> 24:00.920
 And they actually included the time it took to test it out,

24:00.920 --> 24:02.920
 which was 10 months.

24:02.920 --> 24:05.920
 So they figured, okay, it took 10 months to create this.

24:05.920 --> 24:08.920
 Actually, it took us two days.

24:08.920 --> 24:11.920
 And we also will be able to ultimately do the tests

24:11.920 --> 24:13.920
 in a few days as well.

24:13.920 --> 24:16.920
 Oh, because we can simulate how the body will respond to it.

24:16.920 --> 24:18.920
 That's a little bit more complicated

24:18.920 --> 24:22.920
 because the body has a lot of different elements,

24:22.920 --> 24:24.920
 and we have to simulate all of that.

24:24.920 --> 24:26.920
 But that's coming as well.

24:26.920 --> 24:29.920
 So ultimately, we could create it in a few days

24:29.920 --> 24:32.920
 and then test it in a few days, and it would be done.

24:32.920 --> 24:37.920
 And we can do that with every type of medical insufficiency

24:37.920 --> 24:39.920
 that we have.

24:39.920 --> 24:41.920
 So curing all diseases,

24:41.920 --> 24:47.920
 improving certain functions of the body,

24:47.920 --> 24:53.920
 supplements, drugs, for recreation, for health,

24:53.920 --> 24:56.920
 for performance, for productivity, all that kind of stuff.

24:56.920 --> 24:58.920
 Well, that's where we're headed.

24:58.920 --> 25:00.920
 Because I mean, right now we have a very inefficient way

25:00.920 --> 25:03.920
 of creating these new medications.

25:03.920 --> 25:05.920
 But we've already shown it.

25:05.920 --> 25:08.920
 And the Moderna vaccine is actually the best

25:08.920 --> 25:11.920
 of the vaccines we've had.

25:11.920 --> 25:15.920
 And it literally took two days to create.

25:15.920 --> 25:19.920
 And we'll get to the point where we can test it out also quickly.

25:19.920 --> 25:25.920
 Are you impressed by AlphaFold and the solution to the protein folding,

25:25.920 --> 25:28.920
 which essentially is simulating,

25:28.920 --> 25:32.920
 modeling this primitive building block of life,

25:32.920 --> 25:35.920
 which is a protein, in its 3D shape.

25:35.920 --> 25:38.920
 It's pretty remarkable that they can actually predict

25:38.920 --> 25:41.920
 what the 3D shape of these things are.

25:41.920 --> 25:45.920
 But they did it with the same type of neural net,

25:45.920 --> 25:50.920
 the one, for example, the GO test.

25:50.920 --> 25:52.920
 So it's all the same.

25:52.920 --> 25:53.920
 It's all the same.

25:53.920 --> 25:54.920
 Awesome approaches.

25:54.920 --> 25:58.920
 They took that same thing and just changed the rules to chess.

25:58.920 --> 26:01.920
 And within a couple of days,

26:01.920 --> 26:08.920
 AlphaFold now played a master level of chess greater than any human being.

26:08.920 --> 26:12.920
 And the same thing then worked for AlphaFold,

26:12.920 --> 26:14.920
 which no human had done.

26:14.920 --> 26:16.920
 I mean, human beings could do,

26:16.920 --> 26:22.920
 the best humans could maybe do 15, 20 percent

26:22.920 --> 26:25.920
 of figuring out what the shape would be.

26:25.920 --> 26:31.920
 And after a few takes, it ultimately did just about 100 percent.

26:31.920 --> 26:36.920
 Do you still think the singularity will happen in 2045?

26:36.920 --> 26:39.920
 And what does that look like?

26:39.920 --> 26:45.920
 Once we can amplify our brain with computers directly,

26:45.920 --> 26:47.920
 which will happen in the 2030s,

26:47.920 --> 26:49.920
 that's going to keep growing.

26:49.920 --> 26:50.920
 It's another whole theme,

26:50.920 --> 26:54.920
 which is the exponential growth of computing power.

26:54.920 --> 26:59.920
 So looking at price performance of computation from 1939 to 2021.

26:59.920 --> 27:02.920
 So that starts with the very first computer

27:02.920 --> 27:05.920
 actually created by German during World War II.

27:05.920 --> 27:08.920
 And you might have thought that that might be significant,

27:08.920 --> 27:13.920
 but actually the Germans didn't think computers were significant

27:13.920 --> 27:15.920
 and they completely rejected it.

27:15.920 --> 27:19.920
 And the second one was also the ZUSA II.

27:19.920 --> 27:21.920
 And by the way, we're looking at a plot

27:21.920 --> 27:27.920
 with the X axis being the year from 1935 to 2025.

27:27.920 --> 27:31.920
 And on the Y axis and log scale is competition per second

27:31.920 --> 27:33.920
 per constant dollar.

27:33.920 --> 27:36.920
 So dollar normalized to inflation.

27:36.920 --> 27:39.920
 And it's growing linearly on the log scale,

27:39.920 --> 27:41.920
 which means it's growing exponentially.

27:41.920 --> 27:43.920
 The third one was the British computer,

27:43.920 --> 27:46.920
 which the Allies did take very seriously.

27:46.920 --> 27:50.920
 And it cracked the German code

27:50.920 --> 27:54.920
 and enables the British to win the Battle of Britain,

27:54.920 --> 27:56.920
 which otherwise absolutely would not have happened

27:56.920 --> 28:00.920
 if they hadn't cracked the code using that computer.

28:00.920 --> 28:02.920
 But that's an exponential graph.

28:02.920 --> 28:06.920
 So a straight line on that graph is exponential growth.

28:06.920 --> 28:10.920
 And you see 80 years of exponential growth.

28:10.920 --> 28:14.920
 And I would say about every five years,

28:14.920 --> 28:17.920
 and this happened shortly before the pandemic,

28:17.920 --> 28:20.920
 people saying, well, they call it Moore's law,

28:20.920 --> 28:24.920
 which is not the correct, because it's not all Intel.

28:24.920 --> 28:28.920
 In fact, it started decades before Intel was even created.

28:28.920 --> 28:33.920
 It wasn't with transistors formed into a grid.

28:33.920 --> 28:36.920
 So it's not just transistor count or transistor size.

28:36.920 --> 28:37.920
 Right.

28:37.920 --> 28:42.920
 It started with relays, then went to vacuum tubes,

28:42.920 --> 28:45.920
 then went to individual transistors,

28:45.920 --> 28:48.920
 and then to integrated circuits.

28:48.920 --> 28:53.920
 And the integrated circuits actually starts

28:53.920 --> 28:55.920
 like in the middle of this graph.

28:55.920 --> 28:58.920
 And it has nothing to do with Intel.

28:58.920 --> 29:01.920
 Intel actually was a key part of this.

29:01.920 --> 29:06.920
 But a few years ago, they stopped making the fastest chips.

29:06.920 --> 29:13.920
 But if you take the fastest chip of any technology in that year,

29:13.920 --> 29:15.920
 you get this kind of graph.

29:15.920 --> 29:18.920
 And it's definitely continuing for 80 years.

29:18.920 --> 29:23.920
 So you don't think Moore's law broadly defined is dead?

29:23.920 --> 29:28.920
 It's been declared dead multiple times throughout this process.

29:28.920 --> 29:30.920
 I don't like the term Moore's law,

29:30.920 --> 29:33.920
 because it has nothing to do with Moore or the Intel.

29:33.920 --> 29:40.920
 But yes, the exponential growth of computing is continuing

29:40.920 --> 29:42.920
 and has never stopped.

29:42.920 --> 29:43.920
 From various sources.

29:43.920 --> 29:45.920
 I mean, it went through World War II.

29:45.920 --> 29:48.920
 It went through global recessions.

29:48.920 --> 29:52.920
 It's just continuing.

29:52.920 --> 29:57.920
 And if you continue that out, along with software gains,

29:57.920 --> 30:02.920
 which is all another issue, and they really multiply,

30:02.920 --> 30:04.920
 whatever you get from software gains,

30:04.920 --> 30:07.920
 you multiply by the computer gains,

30:07.920 --> 30:10.920
 you get faster and faster speed.

30:10.920 --> 30:15.920
 This is actually the fastest computer models that have been created.

30:15.920 --> 30:19.920
 And that actually expands roughly twice a year.

30:19.920 --> 30:22.920
 Like every six months, it expands by two.

30:22.920 --> 30:27.920
 So we're looking at a plot from 2010 to 2022.

30:27.920 --> 30:30.920
 On the x axis is the publication date of the model,

30:30.920 --> 30:33.920
 and perhaps sometimes the actual paper associated with it.

30:33.920 --> 30:39.920
 And on the y axis is training compute and flops.

30:39.920 --> 30:44.920
 And so basically, this is looking at the increase in the,

30:44.920 --> 30:50.920
 not transistors, but the computational power of neural networks.

30:50.920 --> 30:54.920
 Yes, the computational power that created these models.

30:54.920 --> 30:56.920
 And that's doubled every six months.

30:56.920 --> 30:59.920
 Which is even faster than transistor division.

30:59.920 --> 31:01.920
 Yeah.

31:01.920 --> 31:05.920
 Actually, since it goes faster than the amount of cost,

31:05.920 --> 31:11.920
 this has actually become a greater investment to create these.

31:11.920 --> 31:15.920
 But at any rate, by the time you get to 2045,

31:15.920 --> 31:20.920
 we'll be able to multiply our intelligence many millions full.

31:20.920 --> 31:24.920
 And it's just very hard to imagine what that will be like.

31:24.920 --> 31:27.920
 And that's the singularity where we can't even imagine.

31:27.920 --> 31:29.920
 Right. That's why we call it the singularity.

31:29.920 --> 31:31.920
 It's a singularity in physics.

31:31.920 --> 31:36.920
 Something gets sucked into its singularity and you can't tell what's going on in there.

31:36.920 --> 31:39.920
 Because no information can get out of it.

31:39.920 --> 31:41.920
 There's various problems with that.

31:41.920 --> 31:43.920
 But that's the idea.

31:43.920 --> 31:48.920
 It's too much beyond what we can imagine.

31:48.920 --> 31:55.920
 Do you think it's possible we don't notice that what the singularity actually feels like?

31:55.920 --> 32:04.920
 Is we just live through it with exponentially increasing cognitive capabilities.

32:04.920 --> 32:08.920
 And we almost, because everything is moving so quickly,

32:08.920 --> 32:13.920
 aren't really able to introspect that our life has changed.

32:13.920 --> 32:18.920
 Yeah. But I mean, we will have that much greater capacity to understand things.

32:18.920 --> 32:20.920
 So we should be able to look back.

32:20.920 --> 32:22.920
 Looking at history, understand history.

32:22.920 --> 32:26.920
 But we will need people, basically like you and me,

32:26.920 --> 32:28.920
 to actually think about these things.

32:28.920 --> 32:29.920
 Think about it.

32:29.920 --> 32:33.920
 But we might be distracted by all the other sources of entertainment and fun.

32:33.920 --> 32:39.920
 Because the exponential power of intellect is growing.

32:39.920 --> 32:43.920
 But also, there'll be a lot of fun.

32:43.920 --> 32:46.920
 The amount of ways you can have, you know.

32:46.920 --> 32:51.920
 I mean, we already have a lot of fun with computer games and so on that are really quite remarkable.

32:51.920 --> 32:57.920
 Thinking about the digital world, the metaverse, virtual reality.

32:57.920 --> 33:01.920
 Will that have a component in this or will most of our advancement be in physical?

33:01.920 --> 33:04.920
 Well, that's a little bit like Second Life.

33:04.920 --> 33:09.920
 Although the Second Life actually didn't work very well because it couldn't actually handle too many people.

33:09.920 --> 33:14.920
 And I don't think the metaverse has come to being.

33:14.920 --> 33:16.920
 I think there will be something like that.

33:16.920 --> 33:20.920
 It won't necessarily be from that one company.

33:20.920 --> 33:22.920
 I mean, there's going to be competitors.

33:22.920 --> 33:25.920
 But yes, we're going to live increasingly online.

33:25.920 --> 33:30.920
 Particularly if our brains are online, I mean, how could we not be online?

33:30.920 --> 33:41.920
 Do you think it's possible that, given this merger with AI, most of our meaningful interactions will be in this virtual world?

33:41.920 --> 33:48.920
 Most of our life, we fall in love, we make friends, we come up with ideas, we do collaborations, we have fun.

33:48.920 --> 33:52.920
 I actually know somebody who's marrying somebody that they never met.

33:52.920 --> 33:56.920
 I think they just met her briefly before the wedding.

33:56.920 --> 34:04.920
 But she actually fell in love with this other person, never having met them.

34:04.920 --> 34:09.920
 And I think the love is real.

34:09.920 --> 34:10.920
 That's a beautiful story.

34:10.920 --> 34:21.920
 But do you think that story is one that might be experienced as opposed to by hundreds of thousands of people, but instead by hundreds of millions of people?

34:21.920 --> 34:27.920
 I mean, it really gives you appreciation for these virtual ways of communicating.

34:27.920 --> 34:33.920
 And if anybody can do it, then it's really not such a freak story.

34:33.920 --> 34:36.920
 So I think more and more people will do that.

34:36.920 --> 34:40.920
 But that's turning our back on our entire history of evolution.

34:40.920 --> 34:48.920
 The old days, we used to fall in love by holding hands and sitting by the fire, that kind of stuff.

34:48.920 --> 34:55.920
 Actually, I have five patents on where you can hold hands, even if you're separated.

34:55.920 --> 34:57.920
 Great.

34:57.920 --> 35:00.920
 So the touch, the sense, it's all just senses.

35:00.920 --> 35:06.920
 It's all just, it's not just that you're touching someone or not.

35:06.920 --> 35:09.920
 There's a whole way of doing it and it's very subtle.

35:09.920 --> 35:16.920
 But ultimately, we can emulate all of that.

35:16.920 --> 35:18.920
 Are you excited by that future?

35:18.920 --> 35:22.920
 Do you worry about that future?

35:22.920 --> 35:28.920
 I have certain worries about the future, but not that virtual touch.

35:28.920 --> 35:30.920
 Well, I agree with you.

35:30.920 --> 35:38.920
 You described six stages in the evolution of information processing in the universe as you started to describe.

35:38.920 --> 35:45.920
 Can you maybe talk through some of those stages from the physics and chemistry to DNA and brains

35:45.920 --> 35:51.920
 and then to the very end, to the very beautiful end of this process?

35:51.920 --> 35:54.920
 Well, it actually gets more rapid.

35:54.920 --> 35:59.920
 So physics and chemistry, that's how we started.

35:59.920 --> 36:01.920
 So from the very beginning of the universe.

36:01.920 --> 36:06.920
 We had lots of electrons and various things traveling around.

36:06.920 --> 36:15.920
 And that took actually many billions of years, kind of jumping ahead here to kind of some of the last stages

36:15.920 --> 36:18.920
 where we have things like love and creativity.

36:18.920 --> 36:22.920
 It's really quite remarkable that that happens.

36:22.920 --> 36:29.920
 But finally, physics and chemistry created biology and DNA.

36:29.920 --> 36:38.920
 And now you had actually one type of molecule that described the cutting edge of this process.

36:38.920 --> 36:43.920
 And we go from physics and chemistry to biology.

36:43.920 --> 36:47.920
 And finally, biology created brains.

36:47.920 --> 36:55.920
 Not everything that's created by biology has a brain, but eventually brains came along.

36:55.920 --> 36:59.920
 And all of this is happening faster and faster.

36:59.920 --> 37:03.920
 It created increasingly complex organisms.

37:03.920 --> 37:11.920
 Another key thing is actually not just brains, but our thumb.

37:11.920 --> 37:18.920
 Because there's a lot of animals with brains even bigger than humans.

37:18.920 --> 37:20.920
 Elephants have a bigger brain.

37:20.920 --> 37:22.920
 Whales have a bigger brain.

37:22.920 --> 37:28.920
 But they've not created technology because they don't have a thumb.

37:28.920 --> 37:33.920
 So that's one of the really key elements in the evolution of humans.

37:33.920 --> 37:40.920
 This physical manipulator device that's useful for puzzle solving in the physical reality.

37:40.920 --> 37:45.920
 So I could think, I could look at a tree and go, oh, I could actually trip that branch down

37:45.920 --> 37:49.920
 and eliminate the leaves and carve a tip on it.

37:49.920 --> 37:51.920
 And I would create technology.

37:51.920 --> 37:58.920
 And you can't do that if you don't have a thumb.

37:58.920 --> 38:03.920
 So thumbs then created technology.

38:03.920 --> 38:07.920
 And technology also had a memory.

38:07.920 --> 38:14.920
 And now those memories are competing with the scale and scope of human beings.

38:14.920 --> 38:17.920
 And ultimately we'll go beyond it.

38:17.920 --> 38:26.920
 And then we're going to merge human technology with human intelligence

38:26.920 --> 38:32.920
 and understand how human intelligence works, which I think we already do.

38:32.920 --> 38:38.920
 And we're putting that into our human technology.

38:38.920 --> 38:42.920
 So create the technology inspired by our own intelligence

38:42.920 --> 38:46.920
 and then that technology supersedes us in terms of its capabilities.

38:46.920 --> 38:48.920
 And we ride along.

38:48.920 --> 38:50.920
 Or do you ultimately see it as...

38:50.920 --> 38:52.920
 We ride along, but a lot of people don't see that.

38:52.920 --> 38:55.920
 They say, well, you've got humans and you've got machines

38:55.920 --> 38:59.920
 and there's no way we can ultimately compete with humans.

38:59.920 --> 39:02.920
 And you can already see that.

39:02.920 --> 39:06.920
 Lisa Dahl, who's like the best Go player in the world,

39:06.920 --> 39:09.920
 says he's not going to play Go anymore.

39:09.920 --> 39:14.920
 Because playing Go for humans, that was like the ultimate in intelligence

39:14.920 --> 39:17.920
 because no one else could do that.

39:17.920 --> 39:21.920
 But now a machine can actually go way beyond him.

39:21.920 --> 39:24.920
 And so he says, well, there's no point playing it anymore.

39:24.920 --> 39:29.920
 That may be more true for games than it is for life.

39:29.920 --> 39:33.920
 I think there's a lot of benefit to working together with AI in regular life.

39:33.920 --> 39:37.920
 So if you were to put a probability on it,

39:37.920 --> 39:42.920
 is it more likely that we merge with AI or AI replaces us?

39:42.920 --> 39:47.920
 A lot of people just think computers come along and they compete with them.

39:47.920 --> 39:51.920
 We can't really compete and that's the end of it.

39:51.920 --> 39:56.920
 As opposed to them increasing our abilities.

39:56.920 --> 40:01.920
 And if you look at most technology, it increases our abilities.

40:01.920 --> 40:06.920
 I mean, look at the history of work.

40:06.920 --> 40:10.920
 Look at what people did 100 years ago.

40:10.920 --> 40:12.920
 Does any of that exist anymore?

40:12.920 --> 40:18.920
 People, I mean, if you were to predict that all of these jobs would go away

40:18.920 --> 40:22.920
 and would be done by machines, people would say, well, there's going to be,

40:22.920 --> 40:28.920
 no one's going to have jobs and it's going to be massive unemployment.

40:28.920 --> 40:33.920
 But I show in this book that's coming out,

40:33.920 --> 40:39.920
 the amount of people that are working, even as a percentage of the population,

40:39.920 --> 40:41.920
 has gone way up.

40:41.920 --> 40:45.920
 We're looking at the x axis year from 1774 to 2024

40:45.920 --> 40:50.920
 and on the y axis, personal income per capita in constant dollars

40:50.920 --> 40:52.920
 and it's growing super linearly.

40:52.920 --> 40:57.920
 I mean, it's 2021 constant dollars and it's gone way up.

40:57.920 --> 41:00.920
 That's not what you would predict,

41:00.920 --> 41:04.920
 given that we would predict that all these jobs would go away.

41:04.920 --> 41:09.920
 But the reason it's gone up is because we've basically enhanced our own capabilities

41:09.920 --> 41:13.920
 by using these machines as opposed to them just competing with us.

41:13.920 --> 41:18.920
 That's a key way in which we're going to be able to become far smarter than we are now

41:18.920 --> 41:25.920
 by increasing the number of different parameters we can consider in making a decision.

41:25.920 --> 41:27.920
 I was very fortunate.

41:27.920 --> 41:34.920
 I am very fortunate to be able to get a glimpse preview of your upcoming book,

41:34.920 --> 41:37.920
 Singularity's Nearer.

41:37.920 --> 41:44.920
 One of the themes outside of just discussing the increasing exponential growth of technology,

41:44.920 --> 41:50.920
 one of the themes is that things are getting better in all aspects of life

41:50.920 --> 41:53.920
 and you talked just about this.

41:53.920 --> 41:55.920
 One of the things you're saying is with jobs,

41:55.920 --> 41:57.920
 so let me just ask about that.

41:57.920 --> 42:06.920
 There is a big concern that automation, especially powerful AI, will get rid of jobs.

42:06.920 --> 42:07.920
 People will lose jobs.

42:07.920 --> 42:13.920
 As you were saying, the senses throughout the history of the 20th century,

42:13.920 --> 42:16.920
 automation did not do that ultimately.

42:16.920 --> 42:20.920
 The question is, will this time be different?

42:20.920 --> 42:22.920
 That is the question.

42:22.920 --> 42:24.920
 Will this time be different?

42:24.920 --> 42:30.920
 It really has to do with how quickly we can merge with this type of intelligence.

42:30.920 --> 42:39.920
 The Lambda GPT3 is out there and maybe it has overcome some of its key problems.

42:39.920 --> 42:49.920
 We really have an enhanced human intelligence that might be a negative scenario.

42:49.920 --> 42:55.920
 That is why we create technologies to enhance ourselves.

42:55.920 --> 43:08.920
 I believe we will be enhanced when I'm just going to sit here with 300 million modules in our neocortex.

43:08.920 --> 43:15.920
 We're going to be able to go beyond that because that's useful,

43:15.920 --> 43:25.920
 so we can multiply that by 10, 100,000 million.

43:25.920 --> 43:29.920
 You might think, well, what's the point of doing that?

43:29.920 --> 43:36.920
 It's like asking somebody that's never heard music, well, what's the value of music?

43:36.920 --> 43:40.920
 I mean, you can't appreciate it until you've created it.

43:40.920 --> 43:47.920
 There's some worry that there will be a wealth disparity, a class or wealth disparity.

43:47.920 --> 43:54.920
 Only the rich people will be, basically, the rich people will first have access to this kind of thing,

43:54.920 --> 44:01.920
 and then because of this kind of thing, because the ability to merge will get richer exponentially faster.

44:01.920 --> 44:04.920
 I say that just like cell phones.

44:04.920 --> 44:09.920
 There's like four billion cell phones in the world today.

44:09.920 --> 44:14.920
 In fact, when cell phones first came out, you had to be fairly wealthy.

44:14.920 --> 44:16.920
 They weren't very inexpensive.

44:16.920 --> 44:19.920
 You had to have some wealth in order to afford them.

44:19.920 --> 44:21.920
 Yeah, there were these big, sexy phones.

44:21.920 --> 44:23.920
 And they didn't work very well.

44:23.920 --> 44:25.920
 They did almost nothing.

44:25.920 --> 44:34.920
 So you can only afford these things if you're wealthy at a point where they really don't work very well.

44:34.920 --> 44:41.920
 So achieving scale and making it inexpensive is part of making the thing work well.

44:41.920 --> 44:42.920
 Exactly.

44:42.920 --> 44:46.920
 So these are not totally cheap, but they're pretty cheap.

44:46.920 --> 44:51.920
 You can get them for a few hundred dollars.

44:51.920 --> 44:54.920
 Especially given the kind of things it provides for you.

44:54.920 --> 44:59.920
 There's a lot of people in the third world that have very little, but they have a smartphone.

44:59.920 --> 45:01.920
 Yeah, absolutely.

45:01.920 --> 45:03.920
 And the same will be true with AI.

45:03.920 --> 45:06.920
 I mean, I see homeless people have their own cell phones.

45:06.920 --> 45:13.920
 Yeah, so your sense is any kind of advanced technology will take the same trajectory.

45:13.920 --> 45:14.920
 Right.

45:14.920 --> 45:18.920
 It ultimately becomes cheap and will be affordable.

45:18.920 --> 45:27.920
 I probably would not be the first person to put something in my brain to connect to computers,

45:27.920 --> 45:29.920
 because I think it will have limitations.

45:29.920 --> 45:35.920
 But once it's really perfected, and at that point it will be pretty inexpensive,

45:35.920 --> 45:38.920
 I think it will be pretty affordable.

45:38.920 --> 45:44.920
 So in which other ways, as you outline your book, is life getting better?

45:44.920 --> 45:50.920
 Well, I mean, I have 50 charts in there where everything is getting better.

45:50.920 --> 45:57.920
 I think there's a kind of cynicism about, even if you look at extreme poverty, for example.

45:57.920 --> 46:02.920
 For example, this is actually a poll taken on extreme poverty,

46:02.920 --> 46:07.920
 and the people who were asked, has poverty gotten better or worse?

46:07.920 --> 46:13.920
 And the options are increased by 50%, increased by 25%, remain the same,

46:13.920 --> 46:16.920
 decreased by 25%, decreased by 50%.

46:16.920 --> 46:20.920
 If you're watching this or listening to this, try to vote for yourself.

46:20.920 --> 46:26.920
 70% thought it had gotten worse, and that's the general impression.

46:26.920 --> 46:31.920
 88% thought it had gotten worse and remained the same.

46:31.920 --> 46:36.920
 Only 1% thought it decreased by 50%, and that is the answer.

46:36.920 --> 46:38.920
 It actually decreased by 50%.

46:38.920 --> 46:44.920
 So only 1% of people got the right optimistic estimate of how poverty is...

46:44.920 --> 46:50.920
 Right, and this is the reality, and it's true of almost everything you look at.

46:50.920 --> 46:53.920
 You don't want to go back 100 years or 50 years.

46:53.920 --> 47:00.920
 Things were quite miserable then, but we tend not to remember that.

47:00.920 --> 47:06.920
 So literacy rate increasing over the past few centuries across all the different nations,

47:06.920 --> 47:11.920
 nearly to 100% across many of the nations in the world.

47:11.920 --> 47:14.920
 It's gone way up, average years of education have gone way up.

47:14.920 --> 47:17.920
 Life expectancy is also increasing.

47:17.920 --> 47:23.920
 Life expectancy was 48 in 1900.

47:23.920 --> 47:25.920
 And it's over 80 now.

47:25.920 --> 47:32.920
 And it's going to continue to go up, particularly as we get into more advanced stages of simulated biology.

47:32.920 --> 47:37.920
 For life expectancy, these trends are the same for at birth, age 1, age 5, age 10,

47:37.920 --> 47:39.920
 so it's not just the infamortality.

47:39.920 --> 47:44.920
 And I have 50 more graphs in the book about all kinds of things.

47:44.920 --> 47:51.920
 Even spread of democracy, which might bring up some sort of controversial issues,

47:51.920 --> 47:54.920
 it still has gone way up.

47:54.920 --> 47:58.920
 Well, that one is gone way up, but that one is a bumpy road, right?

47:58.920 --> 48:04.920
 Exactly, and somebody might represent democracy and go backwards,

48:04.920 --> 48:10.920
 but we basically had no democracies before the creation of the United States,

48:10.920 --> 48:16.920
 which is a little over two centuries ago, which is on the scale of human history isn't that long.

48:16.920 --> 48:22.920
 Do you think superintelligence systems will help with democracy?

48:22.920 --> 48:24.920
 So what is democracy?

48:24.920 --> 48:33.920
 Democracy is giving a voice to the populace and having their ideas, having their beliefs,

48:33.920 --> 48:37.920
 having their views represented.

48:37.920 --> 48:40.920
 Well, I hope so.

48:40.920 --> 48:48.920
 I mean, we've seen social networks can spread conspiracy theories,

48:48.920 --> 48:57.920
 which have been quite negative, being, for example, being against any kind of stuff that would help your health.

48:57.920 --> 49:09.920
 So those kinds of ideas have, on social media, what you notice is they increase engagement, so dramatic division increases engagement.

49:09.920 --> 49:15.920
 Do you worry about AI systems that will learn to maximize that division?

49:15.920 --> 49:29.920
 I mean, I do have some concerns about this, and I have a chapter in the book about the perils of advanced AI,

49:29.920 --> 49:35.920
 spreading misinformation on social networks is one of them, but there are many others.

49:35.920 --> 49:46.920
 What's the one that worries you the most that we should think about to try to avoid?

49:46.920 --> 49:49.920
 Well, it's hard to choose.

49:49.920 --> 49:56.920
 We do have the nuclear power that evolved when I was a child.

49:56.920 --> 50:10.920
 I remember we would actually do these drills against a nuclear war. We'd get under our desks and put our hands behind our heads to protect us from a nuclear war.

50:10.920 --> 50:14.920
 It seems to work. We're still around.

50:14.920 --> 50:16.920
 You're protected.

50:16.920 --> 50:26.920
 But that's still a concern, and there are dangerous situations that can take place in biology.

50:26.920 --> 50:41.920
 Someone could create a virus that's very, I mean, we have viruses that are hard to spread, and they can be very dangerous.

50:41.920 --> 50:49.920
 And we have viruses that are easy to spread, but they're not so dangerous.

50:49.920 --> 50:57.920
 Somebody could create something that would be very easy to spread and very dangerous and be very hard to stop.

50:57.920 --> 51:05.920
 It could be something that would spread without people noticing because people could get it, they'd have no symptoms,

51:05.920 --> 51:11.920
 and then everybody would get it, and then symptoms would occur maybe a month later.

51:11.920 --> 51:26.920
 And that actually doesn't occur normally because if we were to have a problem with that, we wouldn't exist.

51:26.920 --> 51:36.920
 So the fact that humans exist means that we don't have viruses that can spread easily and kill us because otherwise we wouldn't exist.

51:36.920 --> 51:43.920
 Yeah, viruses don't want to do that. They want to spread and keep the host alive somewhat.

51:43.920 --> 51:57.920
 So you can describe various dangers with biology, also nanotechnology, which we actually haven't experienced yet, but there are people that are creating nanotechnology and I described that in the book.

51:57.920 --> 52:06.920
 Now you're excited by the possibilities of nanotechnology, of nanobots, of being able to do things inside our body, inside our mind that's going to help.

52:06.920 --> 52:10.920
 What's exciting, what's terrifying about nanobots?

52:10.920 --> 52:25.920
 What's exciting is that that's a way to communicate with our neocortex because each neocortex is pretty small and you need a small entity that can actually get in there and establish a communication channel.

52:25.920 --> 52:38.920
 And that's going to really be necessary to connect our brains to AI within ourselves because otherwise it would be hard for us to compete with it.

52:38.920 --> 52:40.920
 It's a high bandwidth way.

52:40.920 --> 52:49.920
 Yeah, and that's key actually because a lot of the things like Neuralink are really not high bandwidth yet.

52:49.920 --> 52:55.920
 So nanobots is the way you achieve high bandwidth. How much intelligence would those nanobots have?

52:55.920 --> 53:04.920
 Yeah, they don't need a lot. Just enough to basically establish a communication channel to one nanobot.

53:04.920 --> 53:14.920
 It's primarily about communication between external computing devices and our biological thinking machine.

53:14.920 --> 53:19.920
 What worries you about nanobots? Is it similar to the viruses?

53:19.920 --> 53:40.920
 Well, I mean, this is the Great Goo Challenge. If you had a nanobot that wanted to create any kind of entity and repeat itself and was able to operate in a natural environment,

53:40.920 --> 53:50.920
 you could turn everything into that entity and basically destroy all biological life.

53:50.920 --> 53:53.920
 So you mentioned nuclear weapons.

53:53.920 --> 53:56.920
 Yeah.

53:56.920 --> 54:04.920
 I'd love to hear your opinion about the 21st century and whether you think we might destroy ourselves.

54:04.920 --> 54:20.920
 And maybe your opinion, if it has changed by looking at what's going on in Ukraine, that we could have a hot war with nuclear powers involved and the tensions building

54:20.920 --> 54:28.920
 and a seeming forgetting of how terrifying and destructive nuclear weapons are.

54:28.920 --> 54:37.920
 Do you think humans might destroy ourselves in the 21st century and if we do, how? And how do we avoid it?

54:37.920 --> 54:49.920
 I don't think that's going to happen, despite the terrors of that war. It is a possibility, but I mean, I don't...

54:49.920 --> 54:51.920
 It's unlikely in your mind.

54:51.920 --> 55:04.920
 Yeah. Even with the tensions we've had with this one nuclear power plant that's been taken over, it's very tense.

55:04.920 --> 55:11.920
 But I don't actually see a lot of people worrying that that's going to happen. I think we'll avoid that.

55:11.920 --> 55:20.920
 We had two nuclear bombs go off in 45, so now we're 77 years later.

55:20.920 --> 55:22.920
 Yeah, we're doing pretty good.

55:22.920 --> 55:26.920
 We've never had another one go off through anger.

55:26.920 --> 55:30.920
 People forget. People forget the lessons of history.

55:30.920 --> 55:36.920
 Well, yeah. I mean, I am worried about it. I mean, that is definitely a challenge.

55:36.920 --> 55:46.920
 But you believe that we'll make it out and ultimately, superintelligent AI will help us make it out, as opposed to destroy us.

55:46.920 --> 55:55.920
 I think so, but we do have to be mindful of these dangers, and there are other dangers besides nuclear weapons.

55:55.920 --> 56:10.920
 So to get back to merging with AI, will we be able to upload our mind in a computer in a way where we might even transcend the constraints of our bodies?

56:10.920 --> 56:15.920
 So copy our mind into a computer and leave the body behind?

56:15.920 --> 56:19.920
 Let me describe one thing I've already done with my father.

56:19.920 --> 56:21.920
 That's a great story.

56:21.920 --> 56:32.920
 So we created technology. This is public came out, I think, six years ago, where you could ask any question.

56:32.920 --> 56:39.920
 And the release product, which I think is still on the market, it would read 200,000 books.

56:39.920 --> 56:47.920
 And then find the one sentence in 200,000 books that best answered your question.

56:47.920 --> 56:55.920
 It's actually quite interesting. You can ask all kinds of questions and you get the best answer in 200,000 books.

56:55.920 --> 57:05.920
 But I was also able to take it and not go through 200,000 books, but go through a book that I put together,

57:05.920 --> 57:09.920
 which is basically everything my father had written.

57:09.920 --> 57:19.920
 So everything he had written, I had gathered, and we created a book, everything that Frederickers all had written.

57:19.920 --> 57:29.920
 Now, I didn't think this actually would work that well because stuff he had written was stuff about how to lay out.

57:29.920 --> 57:38.920
 I mean, he did directed choral groups and music groups.

57:38.920 --> 57:54.920
 And he would be laying out how the people should, where they should sit and how to fund this and all kinds of things that really didn't seem that interesting.

57:54.920 --> 58:03.920
 And yet, when you ask a question, it would go through it and it would actually give you a very good answer.

58:03.920 --> 58:07.920
 So I said, well, you know, who's the most interesting composer?

58:07.920 --> 58:09.920
 And he said, well, definitely Brahms.

58:09.920 --> 58:16.920
 He would go on about how Brahms was fabulous and talk about the importance of music education.

58:16.920 --> 58:21.920
 You could have an essential question and answer, a conversation with him.

58:21.920 --> 58:26.920
 And I have a conversation with him, which was actually more interesting than talking to him because if you talk to him,

58:26.920 --> 58:33.920
 he'd be concerned about how they're going to lay out this property to give a choral group.

58:33.920 --> 58:36.920
 He'd be concerned about the day to day versus the big questions.

58:36.920 --> 58:38.920
 Exactly, yeah.

58:38.920 --> 58:42.920
 And you did ask about the meaning of life and he answered love.

58:42.920 --> 58:45.920
 Yeah.

58:45.920 --> 58:48.920
 Do you miss him?

58:48.920 --> 58:50.920
 Yes, I do.

58:50.920 --> 59:06.920
 You know, you get used to missing somebody after 52 years and I didn't really have intelligent conversations with him until later in life.

59:06.920 --> 59:16.920
 In the last few years, he was sick, which meant he was home a lot and I was actually able to talk to him about different things like music and other things.

59:16.920 --> 59:19.920
 So I miss that very much.

59:19.920 --> 59:24.920
 What did you learn about life from your father?

59:24.920 --> 59:28.920
 What part of him is with you now?

59:28.920 --> 59:36.920
 He was devoted to music and when he would create something to music, he'd put them in a different world.

59:36.920 --> 59:48.920
 Otherwise, he was very shy and if people got together, he tended not to interact with people just because of his shyness.

59:48.920 --> 59:54.920
 But when he created music, he was like a different person.

59:54.920 --> 59:59.920
 Do you have that in you, that kind of light that shines?

59:59.920 --> 1:00:05.920
 I got involved with technology at like age five.

1:00:05.920 --> 1:00:08.920
 And you fell in love with it in the same way he did with music?

1:00:08.920 --> 1:00:10.920
 Yeah.

1:00:10.920 --> 1:00:22.920
 I remember this actually happened with my grandmother. She had a manual typewriter and she wrote a book, One Life is Not Enough.

1:00:22.920 --> 1:00:26.920
 It's actually a good title for a book I might write.

1:00:26.920 --> 1:00:29.920
 And it was about a school she had created.

1:00:29.920 --> 1:00:33.920
 Well, actually, her mother created it.

1:00:33.920 --> 1:00:42.920
 My mother's mother's mother created the school in 1868 and it was the first school in Europe that provided higher education for girls.

1:00:42.920 --> 1:00:45.920
 It went through 14th grade.

1:00:45.920 --> 1:00:52.920
 If you were a girl and you were lucky enough to get an education at all, it would go through like ninth grade.

1:00:52.920 --> 1:00:56.920
 And many people didn't have any education as a girl.

1:00:56.920 --> 1:01:00.920
 This went through 14th grade.

1:01:00.920 --> 1:01:03.920
 Her mother created it. She took it over.

1:01:03.920 --> 1:01:12.920
 And the book was about the history of the school and her involvement with it.

1:01:12.920 --> 1:01:19.920
 When she presented it to me, I was not so interested in the story of the school.

1:01:19.920 --> 1:01:24.920
 But I was totally amazed with this manual typewriter.

1:01:24.920 --> 1:01:32.920
 I mean, here was something you could put a blank piece of paper into and you could turn it into something that looked like it came from a book.

1:01:32.920 --> 1:01:38.920
 And you could actually type on it and it looked like it came from a book. It was just amazing to me.

1:01:38.920 --> 1:01:41.920
 And I could see actually how it worked.

1:01:41.920 --> 1:01:46.920
 And I was also interested in magic.

1:01:46.920 --> 1:01:51.920
 But in magic, if somebody actually knows how it works, the magic goes away.

1:01:51.920 --> 1:01:55.920
 The magic doesn't stay there if you actually understand how it works.

1:01:55.920 --> 1:02:00.920
 But he was technology. I didn't have that word when I was five or six.

1:02:00.920 --> 1:02:02.920
 And the magic was still there for you?

1:02:02.920 --> 1:02:06.920
 The magic was still there, even if you knew how it worked.

1:02:06.920 --> 1:02:17.920
 So I became totally interested in this and then went around, collected little pieces of mechanical objects from bicycles, from broken radios.

1:02:17.920 --> 1:02:25.920
 I went through the neighborhood. This was an era where you would allow five or six year olds who run through the neighborhood and do this.

1:02:25.920 --> 1:02:27.920
 We don't do that anymore.

1:02:27.920 --> 1:02:30.920
 But I didn't know how to put them together.

1:02:30.920 --> 1:02:36.920
 I said, if I could just figure out how to put these things together, I could solve any problem.

1:02:36.920 --> 1:02:44.920
 And I actually remember talking to these very old girls, I think they were 10,

1:02:44.920 --> 1:02:49.920
 and telling them, if I could just figure this out, we could fly, we could do anything.

1:02:49.920 --> 1:02:55.920
 And they said, well, you have quite an imagination.

1:02:55.920 --> 1:03:09.920
 And then when I was in third grade, so it was like eight, created like a virtual reality theater where people could come on stage and they could move their arms.

1:03:09.920 --> 1:03:16.920
 And all of it was controlled through one control box. It was all done with mechanical technology.

1:03:16.920 --> 1:03:20.920
 And it was a big hit in my third grade class.

1:03:20.920 --> 1:03:27.920
 And then I went on to do things in junior high school science fairs and high school science fairs.

1:03:27.920 --> 1:03:30.920
 I won the Westinghouse Science Talent Search.

1:03:30.920 --> 1:03:38.920
 So I mean, I became committed to technology when I was five or six years old.

1:03:38.920 --> 1:03:45.920
 You've talked about how you use lucid dreaming to think, to come up with ideas as a source of creativity.

1:03:45.920 --> 1:03:53.920
 Because you maybe talk through that, maybe the process of how to, you've invented a lot of things.

1:03:53.920 --> 1:03:57.920
 You've came up and thought through some very interesting ideas.

1:03:57.920 --> 1:04:04.920
 What advice would you give or can you speak to the process of thinking of how to think?

1:04:04.920 --> 1:04:06.920
 How to think creatively?

1:04:06.920 --> 1:04:11.920
 Well, I mean, sometimes I will think through in a dream and try to interpret that.

1:04:11.920 --> 1:04:29.920
 But I think the key issue that I would tell younger people is to put yourself in the position that what you're trying to create already exists.

1:04:29.920 --> 1:04:36.920
 And then you're explaining how it works.

1:04:36.920 --> 1:04:37.920
 Exactly.

1:04:37.920 --> 1:04:38.920
 That's really interesting.

1:04:38.920 --> 1:04:42.920
 You paint a world that you would like to exist.

1:04:42.920 --> 1:04:45.920
 You think it exists and reverse it.

1:04:45.920 --> 1:04:49.920
 And then you actually imagine you're giving a speech about how you created this.

1:04:49.920 --> 1:04:56.920
 Well, you'd have to then work backwards as to how you would create it in order to make it work.

1:04:56.920 --> 1:04:57.920
 That's brilliant.

1:04:57.920 --> 1:05:03.920
 And that requires some imagination to some first principles thinking.

1:05:03.920 --> 1:05:05.920
 You have to visualize that world.

1:05:05.920 --> 1:05:07.920
 That's really interesting.

1:05:07.920 --> 1:05:15.920
 And generally when I talk about things we're trying to invent, I would use the present tense as if it already exists.

1:05:15.920 --> 1:05:21.920
 Not just to give myself that confidence, but everybody else is working on it.

1:05:21.920 --> 1:05:30.920
 We just have to kind of do all the steps in order to make it actual.

1:05:30.920 --> 1:05:34.920
 How much of a good idea is about timing?

1:05:34.920 --> 1:05:40.920
 How much is it about your genius versus that its time has come?

1:05:40.920 --> 1:05:42.920
 Timing's very important.

1:05:42.920 --> 1:05:45.920
 I mean, that's really why I got into futurism.

1:05:45.920 --> 1:05:53.920
 I wasn't inherently a futurist, but that's not really my goal.

1:05:53.920 --> 1:05:57.920
 That's really to figure out when things are feasible.

1:05:57.920 --> 1:06:00.920
 We see that now with large scale models.

1:06:00.920 --> 1:06:08.920
 The large scale models like GPT3, it started two years ago.

1:06:08.920 --> 1:06:10.920
 Four years ago it wasn't feasible.

1:06:10.920 --> 1:06:17.920
 In fact, they did create GPT2, which didn't work.

1:06:17.920 --> 1:06:26.920
 So it required a certain amount of timing having to do with this exponential growth of computing power.

1:06:26.920 --> 1:06:30.920
 So futurism in some sense is a study of timing.

1:06:30.920 --> 1:06:37.920
 Trying to understand how the world will evolve and when will the capacity for certain ideas emerge.

1:06:37.920 --> 1:06:42.920
 And that's become a thing in itself and to try to time things in the future.

1:06:42.920 --> 1:06:49.920
 But really its original purpose was to time my products.

1:06:49.920 --> 1:07:00.920
 I mean, I did OCR in the 1970s because OCR doesn't require a lot of computation.

1:07:00.920 --> 1:07:02.920
 Optical character recognition.

1:07:02.920 --> 1:07:13.920
 So we were able to do that in the 70s and I waited until the 80s to address speech recognition since it requires more computation.

1:07:13.920 --> 1:07:20.920
 So you were thinking through timing when you're developing those things, has its time come?

1:07:20.920 --> 1:07:26.920
 And that's how you've developed that brain power to start to think in a futurist sense.

1:07:26.920 --> 1:07:33.920
 And how will the world look like in 2045 and work backwards and how it gets there.

1:07:33.920 --> 1:07:47.920
 But that has to become a thing in itself because looking at what things will be like in the future reflects such dramatic changes in how humans will live.

1:07:47.920 --> 1:07:50.920
 That was worth communicating also.

1:07:50.920 --> 1:08:01.920
 So you developed that muscle of predicting the future and then applied broadly and started to discuss how it changes the world of technology,

1:08:01.920 --> 1:08:05.920
 how it changes the world of human life on earth.

1:08:05.920 --> 1:08:15.920
 In Danielle, one of your books, you write about someone who has the courage to question assumptions that limit human imagination to solve problems.

1:08:15.920 --> 1:08:22.920
 And you also give advice on how each of us can have this kind of courage.

1:08:22.920 --> 1:08:27.920
 Well, it's good that you picked that quote because I think that symbolizes what Danielle is about.

1:08:27.920 --> 1:08:32.920
 Courage. So how can each of us have that courage to question assumptions?

1:08:32.920 --> 1:08:42.920
 I mean, we see that when people can go beyond the current realm and create something that's new.

1:08:42.920 --> 1:08:53.920
 I mean, take Uber, for example. Before that existed, you never thought that that was to be feasible and it did require changes in the way people work.

1:08:53.920 --> 1:09:03.920
 Is there practical advice, as you give in the book, about what each of us can do to be a Danielle?

1:09:03.920 --> 1:09:16.920
 Well, she looks at the situation and tries to imagine how she can overcome various obstacles and then she goes for it.

1:09:16.920 --> 1:09:23.920
 And she's a very good communicator so she can communicate these ideas to other people.

1:09:23.920 --> 1:09:33.920
 And there's practical advice of learning to program and recording your life and things of this nature. Become a physicist.

1:09:33.920 --> 1:09:38.920
 So you list a bunch of different suggestions of how to throw yourself into this world.

1:09:38.920 --> 1:09:51.920
 Yeah. I mean, it's kind of an idea how young people can actually change the world by learning all of these different skills.

1:09:51.920 --> 1:09:59.920
 And at the core of that is the belief that you can change the world, that your mind, your body can change the world.

1:09:59.920 --> 1:10:01.920
 Yeah. That's right.

1:10:01.920 --> 1:10:05.920
 And not letting anyone else tell you otherwise.

1:10:05.920 --> 1:10:07.920
 That's very good, exactly.

1:10:07.920 --> 1:10:20.920
 When we upload the story you told about your dad and having a conversation with him, we're talking about uploading your mind to the computer.

1:10:20.920 --> 1:10:24.920
 Do you think we'll have a future with something you call afterlife?

1:10:24.920 --> 1:10:32.920
 We'll have avatars that mimic increasingly better and better our behavior, our appearance, all that kind of stuff.

1:10:32.920 --> 1:10:35.920
 Even those are perhaps no longer with us.

1:10:35.920 --> 1:10:41.920
 Yes. I mean, we need some information about them.

1:10:41.920 --> 1:10:52.920
 I mean, think about my father. I have what he wrote. Now, he didn't have a word processor, so he didn't actually write that much.

1:10:52.920 --> 1:11:00.920
 And our memories of him aren't perfect. So how do you even know if you've created something that's satisfactory?

1:11:00.920 --> 1:11:06.920
 Now, you could do a Frederick Kurzweil Turing test. It seems like Frederick Kurzweil to me.

1:11:06.920 --> 1:11:13.920
 But the people who remember him, like me, don't have a perfect memory.

1:11:13.920 --> 1:11:24.920
 Is there such a thing as a perfect memory? Maybe the whole point is for him to make you feel a certain way.

1:11:24.920 --> 1:11:27.920
 Yeah. Well, I think that would be the goal.

1:11:27.920 --> 1:11:36.920
 And that's the connection we have with loved ones. It's not really based on very strict definition of truth. It's more about the experiences we share.

1:11:36.920 --> 1:11:40.920
 And they get morphed through memory. But ultimately, they make a smile.

1:11:40.920 --> 1:11:45.920
 I think we definitely can do that. And that would be very worthwhile.

1:11:45.920 --> 1:11:54.920
 So do you think we'll have a world of replicants, of copies? There'll be a bunch of records while I could hang out with one.

1:11:54.920 --> 1:12:00.920
 I can download it for five bucks and have a best friend, Ray.

1:12:00.920 --> 1:12:06.920
 And you, the original copy, wouldn't even know about it.

1:12:06.920 --> 1:12:12.920
 Do you think that world is, first of all, do you think that world is feasible?

1:12:12.920 --> 1:12:15.920
 And do you think there's ethical challenges there?

1:12:15.920 --> 1:12:21.920
 How would you feel about me hanging out with Ray Kurzweil and you not knowing about it?

1:12:21.920 --> 1:12:27.920
 It doesn't strike me as a problem.

1:12:27.920 --> 1:12:29.920
 Which you? The original?

1:12:29.920 --> 1:12:33.920
 Would that cause a problem for you?

1:12:33.920 --> 1:12:36.920
 No, I would really very much enjoy it.

1:12:36.920 --> 1:12:43.920
 No, not just hanging out with me, but if somebody hanging out with you, a replicant of you.

1:12:43.920 --> 1:12:54.920
 Well, I think I would start, it sounds exciting, but then what if they start doing better than me and take over my friend group?

1:12:54.920 --> 1:13:04.920
 And then because they may be an imperfect copy or there may be more social or these kinds of things.

1:13:04.920 --> 1:13:09.920
 And then I become like the old version that's not nearly as exciting.

1:13:09.920 --> 1:13:13.920
 Maybe they're a copy of the best version of me on a good day.

1:13:13.920 --> 1:13:24.920
 But if you hang out with a replicant of me and that turned out to be successful, I'd feel proud of that person because it's based on me.

1:13:24.920 --> 1:13:31.920
 But it is a kind of death of this version of you.

1:13:31.920 --> 1:13:35.920
 Well, not necessarily. I mean, you can still be alive, right?

1:13:35.920 --> 1:13:42.920
 Okay, so it's like having kids and you're proud that they've done even more than you were able to do.

1:13:42.920 --> 1:13:47.920
 Yeah, exactly.

1:13:47.920 --> 1:13:54.920
 It does bring up new issues, but it seems like an opportunity.

1:13:54.920 --> 1:13:59.920
 Well, that replicant should probably have the same rights as you do.

1:13:59.920 --> 1:14:09.920
 But that gets into a whole issue because when a replicant occurs, they're not necessarily going to have your rights.

1:14:09.920 --> 1:14:20.920
 And if a replicant occurs to somebody who's already dead, do they have all the obligations that the original person had?

1:14:20.920 --> 1:14:25.920
 Do they have all the agreements that they had?

1:14:25.920 --> 1:14:30.920
 I think you're going to have to have laws that say, yes.

1:14:30.920 --> 1:14:35.920
 If you want to create a replicant, they have to have all the same rights as human rights.

1:14:35.920 --> 1:14:41.920
 Well, you don't know. Somebody can create a replicant and say, well, it's a replicant, but I didn't bother getting their rights.

1:14:41.920 --> 1:14:47.920
 But that would be illegal. If you do that, you have to do that in the black market.

1:14:47.920 --> 1:14:49.920
 If you want to get an official replicant.

1:14:49.920 --> 1:14:55.920
 It's not so easy. It's supposed to create multiple replicants.

1:14:55.920 --> 1:15:04.920
 The original rights may be for one person and not for a whole group of people.

1:15:04.920 --> 1:15:08.920
 Sure.

1:15:08.920 --> 1:15:10.920
 So there has to be at least one.

1:15:10.920 --> 1:15:14.920
 And then all the other ones kind of share the rights.

1:15:14.920 --> 1:15:20.920
 I don't think that that's very difficult to conceive for us humans, the idea of this country.

1:15:20.920 --> 1:15:31.920
 You create a replicant that has certain... I mean, I've talked to people about this, including my wife, who would like to get back her father.

1:15:31.920 --> 1:15:37.920
 And she doesn't worry about who has rights to what.

1:15:37.920 --> 1:15:43.920
 She would have somebody that she could visit with and might give her some satisfaction.

1:15:43.920 --> 1:15:48.920
 And she wouldn't care about any of these other rights.

1:15:48.920 --> 1:15:52.920
 What does your wife think about multiple arrears as well?

1:15:52.920 --> 1:15:54.920
 Have you had that discussion?

1:15:54.920 --> 1:15:57.920
 I haven't addressed that with her.

1:15:57.920 --> 1:16:02.920
 I think ultimately that's an important question, loved ones, how they feel about...

1:16:02.920 --> 1:16:04.920
 There's something about love.

1:16:04.920 --> 1:16:11.920
 Well, that's the key thing, right? If the loved ones rejected, it's not going to work very well.

1:16:11.920 --> 1:16:18.920
 So the loved ones really are the key determinant whether or not this works or not.

1:16:18.920 --> 1:16:21.920
 But there's also ethical rules.

1:16:21.920 --> 1:16:27.920
 We have to contend with the idea and we have to contend with that idea with AI.

1:16:27.920 --> 1:16:29.920
 But what's going to motivate it is...

1:16:29.920 --> 1:16:39.920
 I mean, I talk to people who really miss people who are gone and they would love to get something back, even if it isn't perfect.

1:16:39.920 --> 1:16:46.920
 And that's what's going to motivate this.

1:16:46.920 --> 1:16:50.920
 And that person lives on in some form.

1:16:50.920 --> 1:16:58.920
 And the more data we have, the more we're able to reconstruct that person and allow them to live on.

1:16:58.920 --> 1:17:02.920
 And eventually as we go forward, we're going to have more and more of this data

1:17:02.920 --> 1:17:10.920
 because we're going to have nanobots that are inside our neocortex and we're going to collect a lot of data.

1:17:10.920 --> 1:17:15.920
 In fact, anything that's data is always collected.

1:17:15.920 --> 1:17:20.920
 There is something a little bit sad, which is becoming...

1:17:20.920 --> 1:17:26.920
 Or maybe it's hopeful, which is more and more common these days,

1:17:26.920 --> 1:17:34.920
 which when a person passes away, you'll have their Twitter account and you have the last tweet they tweeted.

1:17:34.920 --> 1:17:38.920
 And you can recreate them now with large language models and so on.

1:17:38.920 --> 1:17:44.920
 You can create somebody that's just like them and can actually continue to communicate.

1:17:44.920 --> 1:17:51.920
 I think that's really exciting because I think in some sense, like if I were to die today,

1:17:51.920 --> 1:17:55.920
 in some sense I would continue on if I continued tweeting.

1:17:55.920 --> 1:17:58.920
 I tweet there for I am.

1:17:58.920 --> 1:18:03.920
 Yeah, well, I mean, that's one of the advantages of a replicant.

1:18:03.920 --> 1:18:09.920
 They can recreate the communications of that person.

1:18:09.920 --> 1:18:16.920
 Do you hope, do you think, do you hope humans will become a multiplanetary species?

1:18:16.920 --> 1:18:23.920
 You've talked about the phases, the six epochs, and one of them is reaching out into the stars in part.

1:18:23.920 --> 1:18:35.920
 Yes, but the kind of attempts we're making now to go to other planetary objects doesn't excite me that much

1:18:35.920 --> 1:18:38.920
 because it's not really advancing anything.

1:18:38.920 --> 1:18:40.920
 It's not efficient enough?

1:18:40.920 --> 1:18:52.920
 Yeah, we're also putting out other human beings, which is a very inefficient way to explore these other objects.

1:18:52.920 --> 1:18:59.920
 What I'm really talking about in the sixth epoch, the universe wakes up.

1:18:59.920 --> 1:19:04.920
 It's where we can spread our superintelligence throughout the universe.

1:19:04.920 --> 1:19:10.920
 And that doesn't mean sending very soft, squishy creatures like humans.

1:19:10.920 --> 1:19:13.920
 The universe wakes up.

1:19:13.920 --> 1:19:28.920
 We would send intelligence masses of nanobots which can then go out and colonize these other parts of the universe.

1:19:28.920 --> 1:19:34.920
 Do you think there's intelligent alien civilizations out there that our bots might meet?

1:19:34.920 --> 1:19:38.920
 My hunch is no.

1:19:38.920 --> 1:19:41.920
 Most people say yes, absolutely.

1:19:41.920 --> 1:19:43.920
 It's too big.

1:19:43.920 --> 1:19:46.920
 And they'll cite the Drake equation.

1:19:46.920 --> 1:19:59.920
 And I think in singularities near, I have two analyses of the Drake equation, both with very reasonable assumptions.

1:19:59.920 --> 1:20:06.920
 And one gives you thousands of advanced civilizations in each galaxy.

1:20:06.920 --> 1:20:13.920
 And another one gives you one civilization, and we know of one.

1:20:13.920 --> 1:20:20.920
 A lot of the analyses are forgetting the exponential growth of computation

1:20:20.920 --> 1:20:29.920
 because we've gone from where the fastest way I could send a message to somebody was with a pony,

1:20:29.920 --> 1:20:37.920
 which was what, like a century and a half ago, to the advanced civilization we have today.

1:20:37.920 --> 1:20:42.920
 And if you accept what I've said, go forward a few decades.

1:20:42.920 --> 1:20:49.920
 You can have absolutely fantastic amount of civilization compared to a pony, and that's in a couple hundred years.

1:20:49.920 --> 1:20:57.920
 The speed and the scale of information transfer is growing exponentially in a blink of an eye.

1:20:57.920 --> 1:21:01.920
 Now think about these other civilizations.

1:21:01.920 --> 1:21:05.920
 They're going to be spread out at cosmic times.

1:21:05.920 --> 1:21:14.920
 So if something is like ahead of us or behind us, it could be ahead of us or behind us by maybe millions of years,

1:21:14.920 --> 1:21:16.920
 which isn't that much.

1:21:16.920 --> 1:21:23.920
 I mean, the world is billions of years old, 14 billion or something.

1:21:23.920 --> 1:21:33.920
 So even a thousand years, if two or three hundred years is enough to go from a pony to a fantastic amount of civilization,

1:21:33.920 --> 1:21:35.920
 we would see that.

1:21:35.920 --> 1:21:43.920
 So of other civilizations that have occurred, some might be behind us, but some might be ahead of us.

1:21:43.920 --> 1:21:51.920
 If they're ahead of us, they're ahead of us by thousands, millions of years, and they would be so far beyond us.

1:21:51.920 --> 1:21:59.920
 They would be doing galaxy wide engineering, but we don't see anything doing galaxy wide engineering.

1:21:59.920 --> 1:22:07.920
 So either they don't exist or this very universe is a construction of an alien species.

1:22:07.920 --> 1:22:11.920
 We're living inside a video game.

1:22:11.920 --> 1:22:18.920
 Well, that's another explanation that yes, you've got some teenage kids and other civilization.

1:22:18.920 --> 1:22:24.920
 Do you find compelling the simulation hypothesis as a thought experiment that we're living in a simulation?

1:22:24.920 --> 1:22:28.920
 The universe is computational.

1:22:28.920 --> 1:22:34.920
 So we are an example in a computational world.

1:22:34.920 --> 1:22:38.920
 Therefore, it is a simulation.

1:22:38.920 --> 1:22:44.920
 It doesn't necessarily mean an experiment by some high school kid in another world,

1:22:44.920 --> 1:22:57.920
 but it nonetheless is taking place in a computational world and everything that's going on is basically a form of computation.

1:22:57.920 --> 1:23:05.920
 So you really have to define what you mean by this whole world being a simulation.

1:23:05.920 --> 1:23:11.920
 Well, then it's the teenager that makes the video game.

1:23:11.920 --> 1:23:23.920
 You know, us humans with our current limited cognitive capability have strived to understand ourselves and we have created religions.

1:23:23.920 --> 1:23:31.920
 We think of God, whatever that is, do you think God exists?

1:23:31.920 --> 1:23:34.920
 And if so, who is God?

1:23:34.920 --> 1:23:52.920
 I alluded to this before and we started out with lots of particles going around and there's nothing that represents love and creativity.

1:23:52.920 --> 1:24:02.920
 And somehow we've gotten into a world where love actually exists and that has to do actually with consciousness because you can't have love without consciousness.

1:24:02.920 --> 1:24:15.920
 So to me, that's God, the fact that we have something where love where you can be devoted to someone else and really feel that love.

1:24:15.920 --> 1:24:18.920
 That's God.

1:24:18.920 --> 1:24:28.920
 And if you look at the Old Testament, it was actually created by several different rabbinids in there.

1:24:28.920 --> 1:24:33.920
 And I think they've identified three of them.

1:24:33.920 --> 1:24:47.920
 One of them dealt with God as a person that you can make deals with and he gets angry and he wrecks vengeance on various people.

1:24:47.920 --> 1:24:57.920
 But two of them actually talk about God as a symbol of love and peace and harmony and so forth.

1:24:57.920 --> 1:25:00.920
 That's how they describe God.

1:25:00.920 --> 1:25:08.920
 So that's my view of God, not as a person in the sky that you can make deals with.

1:25:08.920 --> 1:25:15.920
 It's whatever the magic that goes from basic elements to things like consciousness and love.

1:25:15.920 --> 1:25:23.920
 Do you think one of the things I find extremely beautiful and powerful is cellular automata, which you also touch on.

1:25:23.920 --> 1:25:32.920
 Do you think whatever the heck happens in cellular automata where interesting, complicated objects emerge, God is in there too?

1:25:32.920 --> 1:25:37.920
 The emergence of love in this seemingly primitive universe?

1:25:37.920 --> 1:25:47.920
 Well, that's the goal of creating a replicant is that they would love you and you would love them.

1:25:47.920 --> 1:25:51.920
 There wouldn't be much point of doing it if that didn't happen.

1:25:51.920 --> 1:26:02.920
 But all of it, I guess what I'm saying about cellular automata is it's a primitive building blocks and they somehow create beautiful things.

1:26:02.920 --> 1:26:07.920
 Is there some deep truth to that about how our universe works?

1:26:07.920 --> 1:26:12.920
 Is the emergence from simple rules, beautiful complex objects can emerge?

1:26:12.920 --> 1:26:20.920
 Is that the thing that made us as we went through all the six phases of reality?

1:26:20.920 --> 1:26:22.920
 That's a good way to look at it.

1:26:22.920 --> 1:26:30.920
 It just makes some point to the whole value of having a universe.

1:26:30.920 --> 1:26:35.920
 Do you think about your own mortality? Are you afraid of it?

1:26:35.920 --> 1:26:57.920
 Yes, but I keep going back to my idea of being able to expand human life quickly enough in advance of our getting there, longevity, escape velocity, which we're not quite at yet.

1:26:57.920 --> 1:27:05.920
 But I think we're actually pretty close, particularly with, for example, doing simulated biology.

1:27:05.920 --> 1:27:10.920
 I think we can probably get there within, say, by the end of this decade.

1:27:10.920 --> 1:27:12.920
 And that's my goal.

1:27:12.920 --> 1:27:19.920
 Do you hope to achieve the longevity, escape velocity? Do you hope to achieve immortality?

1:27:19.920 --> 1:27:25.920
 Well, immortality is hard to say. I can't really come on your program saying, I've done it.

1:27:25.920 --> 1:27:31.920
 I've achieved immortality because it's never forever.

1:27:31.920 --> 1:27:34.920
 A long time. A long time of living well.

1:27:34.920 --> 1:27:43.920
 But we'd like to actually advance human life expectancy, advance my life expectancy more than a year, every year.

1:27:43.920 --> 1:27:47.920
 And I think we can get there within, by the end of this decade.

1:27:47.920 --> 1:27:49.920
 How do you think we'd do it?

1:27:49.920 --> 1:27:57.920
 Practical things, in transcend the nine steps to living well forever, your book, you describe just that.

1:27:57.920 --> 1:28:01.920
 There's practical things like health, exercise, all those things.

1:28:01.920 --> 1:28:07.920
 Yeah, I mean, we live in a body that doesn't last forever.

1:28:07.920 --> 1:28:10.920
 There's no reason why it can't, though.

1:28:10.920 --> 1:28:16.920
 And we're discovering things, I think, that will extend it.

1:28:16.920 --> 1:28:22.920
 But you do have to deal with, I mean, I've got various issues.

1:28:22.920 --> 1:28:28.920
 Went to Mexico 40 years ago, developed Salmonella.

1:28:28.920 --> 1:28:36.920
 They created pancreatitis, which gave me a strange form of diabetes.

1:28:36.920 --> 1:28:44.920
 It's not type one diabetes because it's an autoimmune disorder that destroys your pancreas.

1:28:44.920 --> 1:28:46.920
 I don't have that.

1:28:46.920 --> 1:28:51.920
 But it's also not type two diabetes because type two diabetes is your pancreas works fine,

1:28:51.920 --> 1:28:55.920
 but your cells don't absorb the insulin well.

1:28:55.920 --> 1:28:57.920
 I don't have that either.

1:28:57.920 --> 1:29:05.920
 The pancreatitis I had partially damaged my pancreas, but it was a one time thing.

1:29:05.920 --> 1:29:08.920
 It didn't continue.

1:29:08.920 --> 1:29:11.920
 And I've learned now how to control it.

1:29:11.920 --> 1:29:18.920
 So that's just something that I had to do in order to continue to exist.

1:29:18.920 --> 1:29:22.920
 Since your particular biological system, you had to figure out a few hacks

1:29:22.920 --> 1:29:26.920
 and the idea is that science would be able to do that much better, actually.

1:29:26.920 --> 1:29:27.920
 Yeah.

1:29:27.920 --> 1:29:33.920
 So I mean, I do spend a lot of time just tinkering with my own body to keep it going.

1:29:33.920 --> 1:29:37.920
 So I do think I'll last till the end of this decade,

1:29:37.920 --> 1:29:40.920
 and I think we'll achieve longevity escape velocity.

1:29:40.920 --> 1:29:45.920
 I think that we'll start with people who are very diligent about this.

1:29:45.920 --> 1:29:50.920
 Eventually it'll become sort of routine that people will be able to do it.

1:29:50.920 --> 1:29:55.920
 So if you're talking about kids today or even people in their 20s or 30s,

1:29:55.920 --> 1:30:00.920
 it's really not a very serious problem.

1:30:00.920 --> 1:30:07.920
 I have had some discussions with relatives who were like almost 100

1:30:07.920 --> 1:30:11.920
 and saying, well, we're working on it as quickly as possible,

1:30:11.920 --> 1:30:15.920
 but I don't know if that's going to work.

1:30:15.920 --> 1:30:17.920
 Is there a case, this is a difficult question,

1:30:17.920 --> 1:30:22.920
 but is there a case to be made against living forever

1:30:22.920 --> 1:30:28.920
 that a finite life, that mortality is a feature, not a bug,

1:30:28.920 --> 1:30:36.920
 that living a shorter, so dying makes ice cream taste delicious,

1:30:36.920 --> 1:30:39.920
 makes life intensely beautiful.

1:30:39.920 --> 1:30:47.920
 Most people believe that way, except if you present a death of anybody

1:30:47.920 --> 1:30:54.920
 they care about or love, they find that extremely depressing.

1:30:54.920 --> 1:31:01.920
 And I know people who feel that way 20, 30, 40 years later,

1:31:01.920 --> 1:31:05.920
 they still want them back.

1:31:05.920 --> 1:31:10.920
 So I mean, death is not something to celebrate,

1:31:10.920 --> 1:31:15.920
 but we've lived in a world where people just accept this,

1:31:15.920 --> 1:31:18.920
 life is short, you see it all the time on TV, life's short,

1:31:18.920 --> 1:31:20.920
 you have to take advantage of it,

1:31:20.920 --> 1:31:26.920
 and nobody accepts the fact that you can actually go beyond normal lifetimes,

1:31:26.920 --> 1:31:30.920
 but any time we talk about death or death of a person,

1:31:30.920 --> 1:31:34.920
 even one death is a terrible tragedy.

1:31:34.920 --> 1:31:38.920
 If you have somebody that lives to 100 years old,

1:31:38.920 --> 1:31:42.920
 we still love them in return,

1:31:42.920 --> 1:31:46.920
 and there's no limitation to that.

1:31:46.920 --> 1:31:52.920
 In fact, these kinds of trends are going to provide greater and greater

1:31:52.920 --> 1:31:56.920
 opportunity for everybody, even if we have more people.

1:31:56.920 --> 1:31:59.920
 So let me ask about an alien species

1:31:59.920 --> 1:32:04.920
 or a super intelligent AI 500 years from now that will look back.

1:32:04.920 --> 1:32:09.920
 And remember Ray Kurzweil, version zero,

1:32:09.920 --> 1:32:12.920
 before the replicants spread.

1:32:12.920 --> 1:32:15.920
 How do you hope they remember you?

1:32:15.920 --> 1:32:20.920
 In a Hitchhiker's Guide to the Galaxy summary of Ray Kurzweil,

1:32:20.920 --> 1:32:22.920
 what do you hope your legacy is?

1:32:22.920 --> 1:32:25.920
 Well, I mean, I do hope to be around, so that's...

1:32:25.920 --> 1:32:27.920
 Some version of you, yes.

1:32:27.920 --> 1:32:29.920
 So...

1:32:29.920 --> 1:32:31.920
 Do you think you'll be the same person around?

1:32:31.920 --> 1:32:36.920
 I mean, am I the same person I was when I was 20 or 10?

1:32:36.920 --> 1:32:39.920
 You would be the same person in that same way,

1:32:39.920 --> 1:32:41.920
 but yes, we're different.

1:32:41.920 --> 1:32:43.920
 We're different.

1:32:43.920 --> 1:32:47.920
 All we have of that, all you have of that person is your memories,

1:32:47.920 --> 1:32:52.920
 which are probably distorted in some way.

1:32:52.920 --> 1:32:55.920
 Maybe you just remember the good parts,

1:32:55.920 --> 1:32:57.920
 depending on your psyche.

1:32:57.920 --> 1:32:59.920
 You might focus on the bad parts,

1:32:59.920 --> 1:33:02.920
 you might focus on the good parts.

1:33:02.920 --> 1:33:04.920
 Right, but...

1:33:04.920 --> 1:33:11.920
 I mean, I'd still have a relationship to the way I was when I was younger.

1:33:11.920 --> 1:33:15.920
 How will you and the other super intelligent AIs remember you of today,

1:33:15.920 --> 1:33:18.920
 from 500 years ago?

1:33:18.920 --> 1:33:22.920
 What do you hope to be remembered by this version of you,

1:33:22.920 --> 1:33:24.920
 before the singularity?

1:33:24.920 --> 1:33:27.920
 Well, I think it's expressed well in my books,

1:33:27.920 --> 1:33:31.920
 trying to create some new realities that people will accept.

1:33:31.920 --> 1:33:38.920
 I mean, that's something that gives me great pleasure,

1:33:38.920 --> 1:33:48.920
 and greater insight into what makes humans valuable.

1:33:48.920 --> 1:33:56.920
 I'm not the only person who's tempted to comment on that, but...

1:33:56.920 --> 1:33:59.920
 And optimism that permeates your work.

1:33:59.920 --> 1:34:02.920
 Optimism about the future.

1:34:02.920 --> 1:34:06.920
 It's ultimately that optimism paves the way for building a better future.

1:34:06.920 --> 1:34:09.920
 Yeah, I agree with that.

1:34:09.920 --> 1:34:14.920
 So you asked your dad about the meaning of life,

1:34:14.920 --> 1:34:18.920
 and he said, Love, let me ask you the same question.

1:34:18.920 --> 1:34:20.920
 What's the meaning of life?

1:34:20.920 --> 1:34:22.920
 Why are we here?

1:34:22.920 --> 1:34:27.920
 This beautiful journey they were on in Phase 4,

1:34:27.920 --> 1:34:33.920
 reaching for Phase 5 of this evolution and information processing.

1:34:33.920 --> 1:34:34.920
 Why?

1:34:34.920 --> 1:34:41.920
 Well, I think I'd give the same answers as my father.

1:34:41.920 --> 1:34:45.920
 Because if there was no love, and we didn't care about anybody,

1:34:45.920 --> 1:34:48.920
 there'd be no point existing.

1:34:48.920 --> 1:34:50.920
 Love is the meaning of life.

1:34:50.920 --> 1:34:54.920
 The AI version of your dad had a good point.

1:34:54.920 --> 1:34:57.920
 Well, I think that's a beautiful way to end it.

1:34:57.920 --> 1:35:00.920
 Right? Thank you for your work. Thank you for being who you are.

1:35:00.920 --> 1:35:05.920
 Thank you for dreaming about a beautiful future and creating it along the way.

1:35:05.920 --> 1:35:10.920
 And thank you so much for spending a really valuable time with me today.

1:35:10.920 --> 1:35:11.920
 This was awesome.

1:35:11.920 --> 1:35:15.920
 Well, this is my pleasure, and you have some great insights,

1:35:15.920 --> 1:35:18.920
 both into me and into humanity as well.

1:35:18.920 --> 1:35:20.920
 So I appreciate that.

1:35:20.920 --> 1:35:23.920
 Thanks for listening to this conversation with Ray Coorswell.

1:35:23.920 --> 1:35:27.920
 To support this podcast, please check out our sponsors in the description.

1:35:27.920 --> 1:35:31.920
 And now, let me leave you with some words from Isaac Asimov.

1:35:31.920 --> 1:35:36.920
 It is change, continuous change, inevitable change,

1:35:36.920 --> 1:35:40.920
 that is the dominant factor in society today.

1:35:40.920 --> 1:35:44.920
 No sensible decision could be made any longer without taking into account

1:35:44.920 --> 1:35:48.920
 not only the world as it is, but the world as it will be.

1:35:48.920 --> 1:35:54.920
 This, in turn, means that our statesmen, our businessmen, our everyman

1:35:54.920 --> 1:35:58.920
 must take on a science fictional way of thinking.

1:35:58.920 --> 1:36:10.920
 Thank you for listening, and hope to see you next time.

