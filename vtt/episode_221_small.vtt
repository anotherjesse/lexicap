WEBVTT

00:00.000 --> 00:06.800
 The following is a conversation with Doug Lennett, creator of Psyche, a system that for close to 40

00:06.800 --> 00:13.120
 years and still today has sought to solve the core problem of artificial intelligence, the

00:13.120 --> 00:19.200
 acquisition of common sense knowledge and the use of that knowledge to think, to reason, and to

00:19.200 --> 00:24.400
 understand the world. To support this podcast, please check out our sponsors in the description.

00:24.400 --> 00:30.400
 As a side note, let me say that in the excitement of the modern era of machine learning, it is

00:30.400 --> 00:36.880
 easy to forget just how little we understand exactly how to build the kind of intelligence

00:36.880 --> 00:42.480
 that matches the power of the human mind. To me, many of the core ideas behind Psyche,

00:42.480 --> 00:48.880
 in some form, in actuality or in spirit, will likely be part of the AI system that achieves

00:48.880 --> 00:54.480
 general superintelligence. But perhaps more importantly, solving this problem of common

00:54.480 --> 01:01.040
 sense knowledge will help us humans understand our own minds, the nature of truth, and finally,

01:01.040 --> 01:07.280
 how to be more rational and more kind to each other. This is the Lex Friedman podcast,

01:07.280 --> 01:15.520
 and here is my conversation with Doug Lennett. Psyche is a project launched by you in 1984

01:15.520 --> 01:20.960
 and still is active today, whose goal is to assemble a knowledge base that spans the basic

01:20.960 --> 01:26.720
 concepts and rules about how the world works. In other words, it hopes to capture common sense

01:26.720 --> 01:32.720
 knowledge, which is a lot harder than it sounds. Can you elaborate on this mission and maybe perhaps

01:32.720 --> 01:39.520
 speak to the various sub goals within this mission? When I was a faculty member in the

01:39.520 --> 01:46.640
 computer science department at Stanford, my colleagues and I did research in all sorts of

01:46.640 --> 01:53.440
 artificial intelligence programs, so natural language understanding programs, robots,

01:53.440 --> 02:02.160
 expert systems, and so on. And we kept hitting the very same brick wall. Our systems would have

02:02.160 --> 02:11.680
 impressive early successes. And so, if your only goal was academic, namely to get enough material

02:11.680 --> 02:17.680
 to write a journal article, that might actually suffice. But if you're really trying to get AI,

02:18.720 --> 02:24.480
 then you have to somehow get past the brick wall. And the brick wall was the programs didn't have

02:25.120 --> 02:29.680
 what we would call common sense. They didn't have general world knowledge. They didn't really

02:29.680 --> 02:36.400
 understand what they were doing, what they were saying, what they were being asked. And so, very

02:36.400 --> 02:43.200
 much like a clever dog performing tricks, we could get them to do tricks, but they never really

02:43.200 --> 02:48.480
 understood what they were doing, sort of like when you get a dog to fetch your morning newspaper.

02:49.040 --> 02:54.000
 The dog might do that successfully, but the dog has no idea what a newspaper is or what it says

02:54.000 --> 02:58.560
 or anything like that. What does it mean to understand something? Can you maybe elaborate

02:58.560 --> 03:04.640
 on that a little bit? Is it, is understanding an action of like combining little things together,

03:04.640 --> 03:09.920
 like through inference, or is understanding the wisdom you gain over time that forms a knowledge?

03:09.920 --> 03:17.440
 I think of understanding more like a, think of it more like the ground you stand on, which

03:19.200 --> 03:27.120
 could be very shaky, could be very unsafe, but most of the time is not because underneath it is

03:27.120 --> 03:33.920
 more ground and eventually rock and other things, but layer after layer after layer,

03:34.480 --> 03:40.880
 that solid foundation is there. And you rarely need to think about it. You rarely need to count on it,

03:40.880 --> 03:48.240
 but occasionally you do. And I've never used this analogy before, so bear with me. But I think the

03:48.240 --> 03:55.520
 same thing is true in terms of getting computers to understand things, which is you ask a computer

03:55.520 --> 04:02.160
 a question, for instance, Alexa or some robot or something, and maybe it gets the right answer.

04:03.200 --> 04:10.720
 But if you were asking that of a human, you could also say things like, why? Or how might you be

04:10.720 --> 04:17.120
 wrong about this or something like that? And the person, you know, would answer you. And, you know,

04:17.120 --> 04:22.640
 it might be a little annoying if you have a small child and they keep asking why questions in series,

04:22.640 --> 04:26.480
 eventually you get to the point where you throw up your hands and say, I don't know, it's just the

04:26.480 --> 04:35.920
 way the world is. But for many layers, you actually have that, that layered solid foundation of support

04:35.920 --> 04:41.360
 so that when you need it, you can count on it. And when do you need it? Well, when things are

04:41.360 --> 04:46.960
 unexpected, when you come up against a situation, which is novel, for instance, when you're driving,

04:46.960 --> 04:55.680
 it may be fine to have a small program, a small set of rules that cover, you know, 99% of the cases,

04:55.680 --> 05:01.440
 but that 1% of the time when something strange happens, you really need to draw on common sense.

05:01.440 --> 05:07.680
 For instance, my wife and I were driving recently, and there was a trash truck in front of us.

05:08.480 --> 05:15.600
 And I guess they had packed it too full and the back exploded. And trash bags went everywhere,

05:15.600 --> 05:22.000
 and we had to make a split second decision. Are we going to slam on our brakes? Are we

05:22.000 --> 05:27.440
 going to swerve into another lane? Are we going to just run it over? Because there are cars all

05:27.440 --> 05:34.160
 around us. And, you know, in front of us was a large trash bag, and we know what we throw away

05:34.160 --> 05:41.360
 in trash bags, probably not a safe thing to run over. Over on the left was a bunch of fast food

05:41.360 --> 05:46.560
 restaurant trash bags. And it's like, oh, well, those things are just like styrofoam and leftover

05:46.560 --> 05:51.760
 food. We'll run over that. And so that was a safe thing for us to do. Now, that's the kind

05:51.760 --> 05:59.440
 of thing that's going to happen maybe once in your life. But the point is that there's almost no

05:59.440 --> 06:06.240
 telling what little bits of knowledge about the world you might actually need in some situations

06:06.240 --> 06:14.000
 which we're unforeseen. But see, when you sit on that mountain or that ground that goes deep

06:14.000 --> 06:20.560
 of knowledge in order to make a split second decision about fast food, trash, or random

06:20.560 --> 06:28.880
 trash from the back of a trash truck, you need to be able to leverage that ground you stand on

06:28.880 --> 06:35.280
 in some way. It's not merely, you know, it's not enough to just have a lot of ground to stand on.

06:35.280 --> 06:40.960
 It's your ability to leverage it, to utilize in a split, like integrate it all together to

06:40.960 --> 06:50.800
 make that split second decision. And I suppose understanding isn't just having common sense

06:50.800 --> 06:59.920
 knowledge to access. It's the act of accessing, accessing it somehow, like correctly filtering

06:59.920 --> 07:06.160
 out the parts of the knowledge that are not useful, selecting only the useful parts, and effectively

07:06.160 --> 07:12.240
 making conclusive decisions. So let's tease apart two different tasks, really, both of which

07:12.240 --> 07:17.920
 are incredibly important and even necessary. If you're going to have this in a useful,

07:18.640 --> 07:22.800
 useful, usable fashion, as opposed to, say, like library books sitting on a shelf,

07:23.680 --> 07:28.960
 and so on, where the knowledge might be there. But, you know, if a fire comes,

07:28.960 --> 07:33.040
 the books are going to burn because they don't know what's in them, and they're just going to

07:33.040 --> 07:40.480
 sit there while they burn. So there are two aspects of using the knowledge. One is a kind

07:40.480 --> 07:47.040
 of a theoretical, how is it possible at all? And then the second aspect of what you said is,

07:47.040 --> 07:54.000
 how can you do it quickly enough? So how can you do it at all is something that philosophers

07:54.000 --> 08:02.240
 have grappled with. And fortunately, philosophers 100 years ago and even earlier, developed a kind

08:02.240 --> 08:12.240
 of formal language. Like English, it's called predicate logic or first order logic or something

08:12.240 --> 08:18.640
 like predicate calculus and so on. So there's a way of representing things in this formal language,

08:18.640 --> 08:29.040
 which enables a mechanical procedure to sort of grind through and algorithmically produce all of

08:29.040 --> 08:36.000
 the same logical entailments, all the same logical conclusions that you or I would from that same

08:36.000 --> 08:44.960
 set of pieces of information that are represented that way. So that sort of raises a couple

08:44.960 --> 08:51.600
 questions. One is, how do you get all this information from say observations and English and

08:51.600 --> 08:59.120
 so on into this logical form? And secondly, how can you then efficiently run these algorithms to

08:59.120 --> 09:05.120
 actually get the information you need in the case I mentioned in a tenth of a second, rather than say

09:05.680 --> 09:13.440
 in 10 hours or 10,000 years of computation? And those are both really important questions.

09:13.440 --> 09:21.200
 And like a corollary addition to the first one is, how many such things do you need to gather

09:21.200 --> 09:27.440
 for it to be useful in certain contexts? So like what in order, you mentioned philosophers,

09:27.440 --> 09:33.040
 in order to capture this world and represent it in a logical way and with a formal logic,

09:33.920 --> 09:41.120
 like how many statements are required? Is it five? Is it 10? Is it 10 trillion? Is it like that?

09:41.120 --> 09:47.200
 That's as far as I understand is probably still an open question. It may forever be an open question

09:48.480 --> 09:54.800
 to say definitively about, to describe the universe perfectly. How many facts do you need?

09:56.560 --> 10:00.080
 I guess I'm going to disappoint you by giving you an actual answer to your question.

10:01.120 --> 10:09.600
 Well, no, this sounds exciting. Yes. Okay. So now we have like three things to talk about.

10:09.600 --> 10:12.800
 I'll keep adding more. Although it's okay. The first and the third are related.

10:13.600 --> 10:21.360
 So let's leave the efficiency question aside for now. So how does all this information get

10:21.360 --> 10:28.800
 represented in logical form so that these algorithms, resolution, theorem proving,

10:28.800 --> 10:33.920
 and other algorithms can actually grind through all the logical consequences of what you said?

10:33.920 --> 10:40.640
 And that ties into your question about how many of these things do you need? Because if the answer

10:40.640 --> 10:52.560
 is small enough, then by hand, you could write them out one at a time. So in the early 1984,

10:53.360 --> 11:00.960
 I held a meeting at Stanford, where I was a faculty member there, where we assembled

11:00.960 --> 11:10.560
 about half a dozen of the smartest people I know, people like Alan Newell and Marvin Minsky,

11:10.560 --> 11:17.120
 and Alan Kay, and a few others. Was Feynman there by chance? Because he

11:17.120 --> 11:22.240
 liked your, he commented about your system you risked at the time. No, he wasn't part of this

11:22.240 --> 11:26.880
 meeting. That's a heck of a meeting anyway. I think Ed Feigenbaum was there. I think

11:26.880 --> 11:36.720
 Josh Lederberg was there. So we have all these different smart people, and we came together

11:37.280 --> 11:42.880
 to address the question that you raised, which is, if it's important to represent

11:42.880 --> 11:49.120
 common sense knowledge and world knowledge in order for AIs to not be brittle, in order for AIs

11:49.120 --> 11:55.760
 not to just have the veneer of intelligence. Well, how many pieces of common sense, how many,

11:55.760 --> 12:02.160
 if then, rules, for instance, would we have to actually write in order to essentially cover

12:02.160 --> 12:09.760
 what people expect perfect strangers to already know about the world? And I expected there would

12:09.760 --> 12:18.160
 be an enormous divergence of opinion and computation, but amazingly, everyone got an answer which was

12:18.160 --> 12:28.000
 around a million. And one person got the answer by saying, well, look, you can only burn into human

12:28.000 --> 12:33.760
 long term memory a certain number of things per unit time, like maybe one every 30 seconds or

12:33.760 --> 12:38.720
 something. And other than that, it's just short term memory and it flows away like water and so

12:38.720 --> 12:45.040
 on. So by the time you're say 10 years old or so, how many things could you possibly have burned

12:45.040 --> 12:50.160
 into your long term memory? And it's like about a million. Another person went in a completely

12:50.160 --> 12:57.280
 different direction and said, well, if you look at the number of words in a dictionary, not a whole

12:57.280 --> 13:03.680
 dictionary, but for someone to essentially be considered to be fluent in a language, how many

13:03.680 --> 13:09.280
 words would they need to know and then about how many things about each word would you have to tell

13:09.280 --> 13:17.040
 it? And so they got to a million that way. Another person said, well, let's actually look at one

13:17.040 --> 13:27.200
 single short one volume desk encyclopedia article. And so we'll look at what was like a four paragraph

13:27.200 --> 13:33.760
 article or something I think about grebes. Grebes are a type of waterfowl. And if we were going to

13:33.760 --> 13:41.200
 sit there and represent every single thing that was there, how many assertions or rules or statements

13:41.200 --> 13:45.840
 would we have to write in this logical language and so on and then multiply that by all of the

13:45.840 --> 13:51.120
 number of articles that there were and so on. So all of these estimates came out with a million.

13:51.920 --> 13:59.040
 And so if you do the math, it turns out that like, oh, well, then maybe in something like

13:59.040 --> 14:09.360
 100 person years, in one or two person centuries, we could actually get this written down by hand.

14:09.920 --> 14:19.360
 And a marvelous coincidence, opportunity existed right at that point in time, the early 1980s.

14:19.360 --> 14:25.440
 There was something called the Japanese fifth generation computing effort. Japan had threatened

14:25.440 --> 14:32.000
 to do in computing and AI and hardware, but they had just finished doing in consumer electronics

14:32.000 --> 14:36.880
 on the automotive industry, namely resting control away from the United States and more

14:36.880 --> 14:44.240
 generally away from the West. And so America was scared. And Congress did something. That's how

14:44.240 --> 14:48.720
 you know it was a long time ago because Congress did something. Congress passed something called

14:48.720 --> 14:54.800
 the National Cooperative Research Act, NCRA. And what it said was, hey, all you big American

14:54.800 --> 14:59.200
 companies, that's also how you know it was a long time ago because they were American companies

14:59.200 --> 15:04.880
 rather than multinational companies. Hey, all you big American companies, normally it would be an

15:04.880 --> 15:13.120
 antitrust violation if you colluded on R&D. But we promise for the next 10 years, we won't prosecute

15:13.120 --> 15:20.880
 any of you if you do that to help combat this threat. And so overnight, the first two consortia,

15:20.880 --> 15:26.800
 Research Consortia in America sprang up, both of them coincidentally in Austin, Texas,

15:27.360 --> 15:33.760
 one called SEMitech focusing on hardware chips and so on, and then one called MCC, the Micro

15:33.760 --> 15:40.160
 Electronics and Computer Technology Corporation, focusing on more on software, on databases and

15:40.160 --> 15:46.880
 AI and natural language understanding and things like that. And I got the opportunity,

15:46.880 --> 15:53.600
 thanks to my friend Woody Bledsoe, who was one of the people who founded that, to come and be

15:53.600 --> 15:59.680
 its principal scientist. And he said, you know, and he sent Admiral Bob Inman, who was the person

15:59.680 --> 16:05.120
 running MCC, came and talked to me and said, look professor, you know, you're talking about doing

16:05.120 --> 16:11.760
 this project, it's going to involve person centuries of effort. You've only got a handful of graduate

16:11.760 --> 16:17.680
 students, you do the math, it's going to take you like, you know, longer than the rest of your life

16:17.680 --> 16:22.640
 to finish this project. But if you move to the wilds of Austin, Texas, we'll put 10 times as

16:22.640 --> 16:28.560
 many people on it and, you know, you'll be done in a few years. And so that was pretty exciting.

16:28.560 --> 16:37.760
 And so I did that, I took my leave from Stanford, I came to Austin, I worked for MCC. And good news

16:37.760 --> 16:43.200
 and bad news, the bad news is that all of us were off by an order of magnitude. That it turns out

16:43.200 --> 16:50.720
 what you need are tens of millions of these pieces of knowledge about on every day, sort of like,

16:50.720 --> 16:55.040
 if you have a coffee cup with stuff in it, and you turn it upside down, the stuff in it's going

16:55.040 --> 17:00.720
 to fall out. So you need tens of millions of pieces of knowledge like that, even if you take

17:00.720 --> 17:09.760
 trouble to make each one as general as it possibly could be. But the good news was that thanks to

17:11.600 --> 17:18.320
 initially the fifth generation effort, and then later US government agency funding and so on,

17:18.320 --> 17:24.240
 we were able to get enough funding, not for a couple of person centuries of time, but for

17:24.240 --> 17:31.120
 a couple person millennia of time, which is what we've spent since 1984, getting psych to contain

17:31.120 --> 17:38.320
 the tens of millions of rules that it needs in order to really capture and span sort of not

17:38.320 --> 17:44.000
 all of human knowledge, but the things that you assume other people, the things you count on other

17:44.000 --> 17:53.280
 people knowing. And so by now we've done that. And the good news is since you've waited 38 years,

17:53.280 --> 18:00.880
 just about to talk to me, we're about at the end of that process. So most of what we're doing now

18:00.880 --> 18:06.480
 is not putting in even what you would consider common sense, but more putting in domain specific

18:06.480 --> 18:18.720
 application specific knowledge about healthcare in a certain hospital or about oil pipes getting

18:18.720 --> 18:24.480
 clogged up or whatever the applications happen to be. So we've almost come full circle and we're

18:24.480 --> 18:30.400
 doing things very much like the expert systems of the 1970s and the 1980s, except instead of

18:30.400 --> 18:36.160
 resting on nothing and being brittle, they're now resting on this massive pyramid, if you will,

18:36.160 --> 18:41.520
 this massive lattice of common sense knowledge so that when things go wrong, when something

18:41.520 --> 18:47.680
 unexpected happens, they can fall back on more and more and more general principles, eventually

18:47.680 --> 18:53.200
 bottoming out in things like, for instance, if we have a problem with the microphone, one of the

18:53.200 --> 18:59.200
 things you'll do is unplug it, plug it in again and hope for the best, because that's one of the

18:59.200 --> 19:04.080
 general pieces of knowledge you have in dealing with electronic equipment or software systems or

19:04.720 --> 19:09.520
 things like that. Is there a basic principle like that? Is it possible to encode something

19:09.520 --> 19:15.520
 that generally captures this idea of turn it off and turn it back on and see if it fixes?

19:15.520 --> 19:19.440
 Oh, absolutely. That's one of the things that's like news.

19:19.440 --> 19:24.160
 That's actually one of the fundamental laws of nature, I believe.

19:25.040 --> 19:34.240
 I wouldn't call it a law. It seems to work every time, so it sure looks like a law. I don't know.

19:34.240 --> 19:42.000
 So that basically covered the resources needed and then we had to devise a method to actually

19:42.000 --> 19:47.200
 figure out, well, what are the tens of millions of things that we need to tell the system?

19:47.200 --> 19:54.720
 And for that, we found a few techniques which worked really well. One is to take any piece of

19:54.720 --> 19:59.920
 text almost, it could be an advertisement, it could be a transcript, it could be a novel,

19:59.920 --> 20:07.280
 it could be an article, and don't pay attention to the actual type that's there, the black space

20:07.280 --> 20:12.400
 on the white page. Pay attention to the complement of that, the white space, if you will. So

20:12.400 --> 20:17.760
 what did the writer of this sentence assume that the reader already knew about the world?

20:17.760 --> 20:24.080
 For instance, if they used a pronoun, why did they think that you would be able to

20:24.880 --> 20:30.080
 understand what the intended referent of that pronoun was? If they used an ambiguous word,

20:30.080 --> 20:35.120
 how did they think that you would be able to figure out what they meant by that word?

20:35.120 --> 20:41.120
 The other thing we look at is the gap between one sentence and the next one. What are all the

20:41.120 --> 20:46.880
 things that the writer expected you to fill in and infer occurred between the end of one sentence

20:46.880 --> 20:52.960
 and the beginning of the other? So like if the sentence says, Fred Smith robbed the third national

20:52.960 --> 21:00.400
 bank period, he was sentenced to 20 years in prison period. Well, between the first sentence

21:00.400 --> 21:07.440
 and the second, you're expected to infer things like Fred got caught, Fred got arrested, Fred went

21:07.440 --> 21:13.440
 to jail, Fred had a trial, Fred was found guilty, and so on. If my next sentence starts out with

21:13.440 --> 21:19.440
 something like the judge, then you assume it's the judge at his trial. If my next sentence starts

21:19.440 --> 21:24.960
 out something like the arresting officer, you assume that it was the police officer who arrested

21:24.960 --> 21:31.920
 him after he committed the crime, and so on. So those are two techniques for getting that

21:31.920 --> 21:39.440
 knowledge. The other thing we sometimes look at is sort of like fake news or sort of humorous

21:39.440 --> 21:46.080
 onion headlines or headlines in the weekly world news, if you know what that is, or the national

21:46.080 --> 21:51.520
 inquire, where it's like, oh, we don't believe this, then we introspect on why don't we believe it.

21:51.520 --> 21:59.040
 So there are things like B17 lands on the moon. It's like, what do we know about the world that

21:59.040 --> 22:05.040
 causes us to believe that that's just silly or something like that? Or another thing we look

22:05.040 --> 22:12.720
 for are contradictions, things which can't both be true. And we say to it, what is it that we know

22:12.720 --> 22:18.560
 that causes us to know that both of these can't be true at the same time? For instance, in one of

22:18.560 --> 22:26.480
 the weekly world news editions, in one article it talked about how Elvis was cited, even though he

22:26.480 --> 22:32.000
 was getting on in years and so on. And another article in the same one talked about people

22:32.000 --> 22:38.320
 seeing Elvis's ghost. So it's like, why do we believe that at least one of these articles must

22:38.320 --> 22:44.320
 be wrong and so on? So we have a series of techniques like that that enable our people,

22:44.320 --> 22:50.880
 and by now we have about 50 people working full time on this and have for decades. So we've put

22:50.880 --> 22:56.400
 in the thousands of person years of effort. We've built up these tens of millions of rules. We

22:56.400 --> 23:03.680
 constantly police the system to make sure that we're saying things as generally as we possibly can.

23:04.640 --> 23:12.400
 So you don't want to say things like, no mouse is also a moose, because if you said things like

23:12.400 --> 23:19.440
 that, then you'd have to add another one or two or three zeros onto the number of assertions you'd

23:19.440 --> 23:24.480
 actually have to have. So at some point, we generalize things more and more, and we get to a point

23:24.480 --> 23:30.880
 where we say, oh yeah, for any two biological taxons, if we don't know explicitly that one is a

23:30.880 --> 23:36.400
 generalization of another, then almost certainly they're disjoint. A member of one is not going to

23:36.400 --> 23:41.040
 be a member of the other and so on. And the same thing with the Elvis and the ghost, it has nothing

23:41.040 --> 23:46.800
 to do with Elvis. It's more about human nature and the mortality and that kind of stuff.

23:46.800 --> 23:50.160
 Right. In general, things are not both alive and dead at the same time.

23:51.120 --> 23:55.520
 Unless special cats in theoretical physics examples.

23:55.520 --> 23:58.160
 Well, that raises a couple important points.

23:58.160 --> 24:01.440
 Well, that's the onion headline situation type of thing. Okay, sorry.

24:01.440 --> 24:04.880
 But no, no. So what you bring up is this really important point of like, well,

24:04.880 --> 24:12.880
 how do you handle exceptions and inconsistencies and so on? And one of the hardest lessons for

24:12.880 --> 24:19.280
 us to learn, it took us about five years to really grit our teeth and learn to love it,

24:20.480 --> 24:25.200
 is we had to give up global consistency. So the knowledge base can no longer be

24:26.080 --> 24:30.400
 consistent. So this is a kind of scary thought. I grew up watching Star Trek,

24:30.400 --> 24:36.000
 and anytime the computer was inconsistent, it would either freeze up or explode or take over

24:36.000 --> 24:40.800
 the world or something bad would happen. Or if you come from a mathematics background,

24:41.360 --> 24:46.320
 once you can prove false, you can prove anything. So that's not good and so on.

24:46.320 --> 24:52.720
 So that's why the old knowledge based systems were all very, very consistent.

24:52.720 --> 24:58.880
 But the trouble is that by and large, our models of the world, the way we talk about the world and

24:58.880 --> 25:05.120
 so on, there are all sorts of inconsistencies that creep in here and there that will sort of kill

25:05.120 --> 25:10.000
 some attempt to build some enormous globally consistent knowledge base. And so what we had

25:10.000 --> 25:17.280
 to move to was a system of local consistency. So a good analogy is you know that the surface of

25:17.280 --> 25:25.520
 the earth is more or less spherical globally. But you live your life every day as though the

25:25.520 --> 25:30.400
 surface of the earth were flat. When you're talking to someone in Australia, you don't think of them

25:30.400 --> 25:35.680
 as being oriented upside down to you. When you're planning a trip, even if it's a thousand miles

25:35.680 --> 25:40.560
 away, you may think a little bit about time zones, but you rarely think about the curvature of the

25:40.560 --> 25:46.000
 earth and so on. And for most purposes, you can live your whole life without really worrying about

25:46.000 --> 25:52.240
 that because the earth is locally flat. In much the same way, the psych knowledge base

25:52.240 --> 25:59.200
 is divided up into almost like tectonic plates, which are individual contexts and each context

25:59.200 --> 26:05.520
 is more or less consistent. But there can be small inconsistencies at the boundary between

26:05.520 --> 26:12.000
 one context and the next one and so on. And so by the time you move say 20 contexts over,

26:12.000 --> 26:17.920
 there could be glaring inconsistencies. So eventually you get from the normal modern real

26:17.920 --> 26:25.200
 world context that we're in right now to something like road runner cartoon context where physics

26:25.200 --> 26:30.160
 is very different and in fact, life and death are very different because no matter how many times

26:30.160 --> 26:37.760
 he's killed, the coyote comes back in the next scene and so on. So that was a hard lesson to

26:37.760 --> 26:43.440
 learn. And we had to make sure that our representation language, the way that we actually encode

26:43.440 --> 26:48.240
 the knowledge and represent it, was expressive enough that we could talk about things being true

26:48.240 --> 26:53.600
 in one context and false in another, things that are true at one time and false in another,

26:53.600 --> 26:59.040
 things that are true, let's say in one region like one country, but false in another, things that

26:59.040 --> 27:05.040
 are true in one person's belief system, but false in another person's belief system, things that

27:05.040 --> 27:09.840
 are true at one level of abstraction and false at another. For instance, at one level of abstraction,

27:09.840 --> 27:15.280
 then you think of this table as a solid object, but down at the atomic level, it's mostly empty

27:15.280 --> 27:22.000
 space and so on. So then that's fascinating, but it puts a lot of pressure on context to do

27:22.000 --> 27:28.400
 a lot of work. So you say tectonic plates, is it possible to formulate contexts that are general

27:28.400 --> 27:35.520
 and big that do this kind of capture of knowledge bases? Or do you then get turtles on top of

27:35.520 --> 27:41.120
 turtles again where there's just a huge number of contexts? So it's good you asked that question,

27:41.120 --> 27:48.000
 because you're pointed in the right direction, which is you want contexts to be first class

27:48.000 --> 27:53.920
 objects in your system's knowledge base, in particular in psych's knowledge base. And by

27:53.920 --> 27:59.920
 first class object, I mean that we should be able to have psych think about and talk about and reason

27:59.920 --> 28:07.520
 about one context or another context the same way it reasons about coffee cups and tables and people

28:07.520 --> 28:14.960
 and fishing and so on. And so context are just terms in its language, just like the ones I mentioned.

28:14.960 --> 28:22.800
 And so psych can reason about context, context can arrange hierarchically and so on. And so

28:22.800 --> 28:30.880
 you can say things about, let's say, things that are true in the modern era, things that are true

28:30.880 --> 28:38.560
 in a particular year would then be a sub context of the things that are true in a broader, let's

28:38.560 --> 28:43.360
 say a century or a millennium or something like that. Things that are true in Austin, Texas,

28:44.080 --> 28:50.000
 are generally going to be a specialization of things that are true in Texas, which is going

28:50.000 --> 28:54.960
 to be a specialization of things that are true in the United States and so on. And so you don't

28:54.960 --> 29:01.200
 have to say things over and over again at all these levels. You just say things at the most

29:01.200 --> 29:06.160
 general level that it applies to, and you only have to say it once, and then it essentially

29:06.160 --> 29:09.520
 inherits to all these more specific contexts.

29:09.520 --> 29:15.440
 Jessica, slightly technical question. Is this inheritance a tree or a graph?

29:15.440 --> 29:20.400
 Oh, you definitely have to think of it as a graph. So we could talk about, for instance,

29:20.400 --> 29:25.120
 why the Japanese fifth generation computing effort failed. There were about half a dozen

29:25.120 --> 29:30.880
 different reasons. One of the reasons they failed was because they tried to represent knowledge

29:30.880 --> 29:38.080
 as a tree rather than as a graph. And so each node in their representation

29:38.080 --> 29:46.160
 could only have one parent node. So if you had a table that was a wooden object, a black object,

29:46.160 --> 29:51.920
 a flat object, and so on, you have to choose one, and that's the only parent it could have.

29:52.560 --> 29:56.400
 When, of course, depending on what it is you need to reason about it,

29:56.400 --> 30:00.880
 sometimes it's important to know that it's made out of wood, like if we're talking about a fire.

30:00.880 --> 30:05.120
 Sometimes it's important to know that it's flat if we're talking about resting something on it,

30:05.120 --> 30:13.840
 and so on. So one of the problems was that they wanted a kind of dewey decimal numbering system

30:13.840 --> 30:20.240
 for all of their concepts, which meant that each node could only have at most 10 children,

30:20.240 --> 30:27.440
 and each node could only have one parent. And while that does enable the dewey decimal type

30:28.560 --> 30:34.320
 numbering of concepts, labeling of concepts, it prevents you from representing all the things

30:34.320 --> 30:40.560
 you need to about objects in our world. And that was one of the things which they never

30:40.560 --> 30:45.280
 were able to overcome. And I think that was one of the main reasons that that project failed.

30:45.280 --> 30:50.720
 So we'll return to some of the doors you've opened, but if we can go back to that room in 1984

30:50.720 --> 30:56.640
 around there with Marvin Minsky and Stafford. By the way, I should mention that Marvin wouldn't

30:56.640 --> 31:02.320
 do his estimate until someone brought him an envelope so that he could literally do a back

31:02.320 --> 31:10.720
 of the envelope calculation to come up with his number. Well, because I feel like the conversation

31:10.720 --> 31:18.160
 in that room is an important one. Sometimes science is done in this way. A few people get

31:18.160 --> 31:24.880
 together and plant the seed of ideas, and they reverberate throughout history, and some kind of

31:24.880 --> 31:31.600
 dissipate and disappear, and some Drake equation. It seems like a meaningless equation,

31:31.600 --> 31:36.320
 somewhat meaningless, but I think it drives and motivates a lot of scientists. And when the aliens

31:36.320 --> 31:42.560
 finally show up, that equation will get even more valuable because then we'll be able to ask. In the

31:42.560 --> 31:51.040
 long arc of history, the Drake equation will prove to be quite useful, I think. And in that same way,

31:51.040 --> 31:56.480
 a conversation of just how many facts are required to capture the basic common sense

31:56.480 --> 32:00.080
 knowledge of the world. That's a fascinating question. I want to distinguish between what

32:00.080 --> 32:08.400
 you think of as facts and the kind of things that we represent. So we map to and essentially

32:08.400 --> 32:13.360
 make sure that Psych has the ability to, as it were, read and access the kind of facts you might

32:13.360 --> 32:20.720
 find, say, in Wiki data or stated in a Wikipedia article or something like that. So what we're

32:20.720 --> 32:25.520
 representing, the things that we need a small number of tens of millions of, are more like

32:25.520 --> 32:31.680
 rules of thumb, rules of good guessing, things which are usually true and which help you to make

32:31.680 --> 32:39.200
 sense of the facts that are on sort of sitting off in some database or some other more static

32:39.200 --> 32:45.040
 storage. So they're almost like platonic forms. So like when you read stuff on Wikipedia,

32:45.040 --> 32:48.800
 that's going to be like projections of those ideas. You read an article about the fact that

32:48.800 --> 32:57.680
 Elvis died. That's a projection of the idea that humans are mortal and very few Wikipedia articles

32:57.680 --> 33:03.920
 will write humans are mortal. Exactly. That's what I meant about ferreting out the unstated

33:03.920 --> 33:08.240
 things in text. What are all the things that we're assumed? And so those are things like

33:08.960 --> 33:14.080
 if you have a problem with something, turning it off and on often fixes it for reasons we

33:14.080 --> 33:19.440
 don't really understand and we're not happy about or people can't be both alive and dead at the same

33:19.440 --> 33:26.480
 time and or water flows downhill. If you search online for water flowing uphill and water flowing

33:26.480 --> 33:31.840
 downhill, you'll find more references for water flowing uphill because it's used as a kind of a

33:32.640 --> 33:37.920
 metaphorical reference for some unlikely thing because of course, everyone already knows that

33:37.920 --> 33:44.320
 water flows downhill. So why would anyone bother saying that? Do you have a word you prefer? Because

33:44.320 --> 33:51.360
 we said faxes in the right word. Is there word like concepts? I would say assertions or rules

33:51.360 --> 33:57.600
 because I'm not talking about rigid rules but rules of thumb. But assertions is a nice one that covers

33:58.400 --> 34:05.920
 all of these things. Yeah. As a programmer to me, assert has a very dogmatic authoritarian field

34:05.920 --> 34:12.640
 to them. I'm sorry. I'm so sorry. Okay. But assertions works. Okay. So if we go back to that

34:12.640 --> 34:20.640
 room with Marvin Minsky with you, all these seminal figures, Ed Fagin mom, thinking about

34:21.520 --> 34:28.960
 this very philosophical but also engineering question, we can also go back a couple of decades

34:28.960 --> 34:33.360
 before then and thinking about artificial intelligence broadly when people were thinking

34:33.360 --> 34:40.560
 about how do you create super intelligent systems, general intelligence. And I think

34:40.560 --> 34:48.240
 people's intuition was off at the time. And I mean, this continues to be the case that we're not,

34:48.800 --> 34:53.520
 when we're grappling with these exceptionally difficult ideas, we're not always, it's very

34:53.520 --> 35:00.880
 difficult to truly understand ourselves when we're thinking about the human mind to introspect

35:00.880 --> 35:05.840
 how difficult that is to engineer intelligence, to solve intelligence. We're not very good at

35:05.840 --> 35:12.160
 estimating that. And you are somebody who has really stayed with this question for decades.

35:13.760 --> 35:22.240
 Do you, what's your sense from the 1984 to today? Have you gotten a stronger sense of just how much

35:22.240 --> 35:27.680
 knowledge is required? You've kind of said with some level of certainty that it's still on the

35:27.680 --> 35:32.560
 order of magnitude of tens of millions. Right. For the first several years, I would have said that

35:32.560 --> 35:40.720
 it was on the order of one or two million. And so it took us about five or six years to realize

35:40.720 --> 35:47.840
 that we were off by a factor of 10. But I guess what I'm asking, Marvin Misk is very confident

35:47.840 --> 35:58.640
 in the 60s when you're saying. Yes. Right. What's your sense? If you, 200 years from now,

35:59.440 --> 36:05.280
 you're still, you're not going to be any longer in this particular biological body,

36:05.280 --> 36:11.520
 but your brain will still be in the digital form. And you'll be looking back, would you think you

36:11.520 --> 36:18.240
 were smart today? Like your intuition was right? Or do you think you may be really off?

36:19.120 --> 36:27.680
 So I think I'm right enough. And let me explain what I mean by that, which is sometimes like if

36:27.680 --> 36:34.000
 you have an old fashioned pump, you have to prime the pump and then eventually it starts. So I think

36:34.000 --> 36:40.160
 I'm right enough in the sense that. To prime the pump. What we've built, even if it isn't,

36:40.160 --> 36:48.240
 so to speak, everything you need, it's primed the knowledge pump enough that psych can now itself

36:48.880 --> 36:55.120
 help to learn more and more automatically on its own by reading things and understanding and

36:55.120 --> 37:00.560
 occasionally asking questions like a student would or something. And by doing experiments

37:00.560 --> 37:07.520
 and discovering things on its own and so on. So through a combination of psych powered discovery

37:07.520 --> 37:14.240
 and psych powered reading, it will be able to bootstrap itself. Maybe it's the final 2%, maybe

37:14.240 --> 37:22.240
 it's the final 99%. So even if I'm wrong, all I really need to build is a system which has

37:22.240 --> 37:30.240
 primed the pump enough that it can begin that cascade upward that self reinforcing sort of

37:30.240 --> 37:37.760
 quadratically or maybe even exponentially increasing path upward that we get from,

37:37.760 --> 37:44.560
 for instance, talking with each other. That's why humans today know so much more than humans 100,000

37:44.560 --> 37:49.680
 years ago. We're not really that much smarter than people were 100,000 years ago. But there's so

37:49.680 --> 37:54.640
 much more knowledge and we have language and we can communicate. We can check things on Google

37:54.640 --> 38:00.880
 and so on. So effectively, we have this enormous power at our fingertips. And there's almost no

38:00.880 --> 38:06.160
 limit to how much you could learn if you wanted to because you've already gotten to a certain level

38:06.160 --> 38:11.280
 of understanding of the world that enables you to read all these articles and understand them,

38:11.280 --> 38:17.040
 that enables you to go out and if necessary do experiments over that slower as a way of gathering

38:17.040 --> 38:24.160
 data and so on. And I think this is really an important point, which is if we have artificial

38:24.160 --> 38:29.760
 intelligence, real general artificial intelligence, human level artificial intelligence, then

38:30.720 --> 38:38.000
 people will become smarter. It's not so much that it'll be us versus the AIs. It's more like us

38:38.000 --> 38:44.480
 and the AIs together. We'll be able to do things that require more creativity that would take too

38:44.480 --> 38:49.520
 long right now, but we'll be able to do lots of things in parallel. We'll be able to misunderstand

38:49.520 --> 38:57.200
 each other less. There's all sorts of value that effectively for an individual would mean

38:57.200 --> 39:03.200
 that individual will for all intents and purposes be smarter. And that means that humanity as a

39:03.200 --> 39:11.600
 species will be smarter. And when was the last time that any invention qualitatively made a huge

39:11.600 --> 39:16.800
 difference in human intelligence? You have to go back a long ways. It wasn't like the internet or

39:16.800 --> 39:24.320
 the computer or mathematics or something. It was all the way back to the development of language.

39:24.320 --> 39:31.840
 We sort of look back on prelinguistic cavemen as well. They weren't really intelligent,

39:31.840 --> 39:38.160
 were they? They weren't really human, were they? And I think that, as you said, 50, 100,

39:38.160 --> 39:46.480
 200 years from now, people will look back on people today right before the advent of these

39:46.480 --> 39:55.120
 sort of lifelong general AI uses and say, you know, those poor people, they weren't really human,

39:55.120 --> 40:00.880
 were they? Exactly. So you said a lot of really interesting things. By the way, I would maybe

40:02.560 --> 40:12.800
 try to argue that the internet is on the order of the kind of big leap in improvement that

40:12.800 --> 40:16.960
 the invention of language was. Well, certainly a big leap in one direction. We're not sure

40:16.960 --> 40:22.000
 whether it's upward or downward. Well, I mean, very specific parts of the internet, which is access

40:22.000 --> 40:27.360
 to information like a website like Wikipedia, like ability for human beings from across the

40:27.360 --> 40:32.240
 world to access information so very quickly. So I could take either side of this argument. And

40:32.240 --> 40:38.080
 since you just took one side, I'll give you the other side, which is that almost nothing has done

40:38.080 --> 40:45.440
 more harm than something like the internet and access to that information in two ways.

40:45.440 --> 40:55.360
 One is it's made people more globally ignorant in the same way that calculators made us more or

40:55.360 --> 41:01.200
 less enumerate. So when I was growing up, we had to use slide rules, we had to be able to estimate

41:01.200 --> 41:08.000
 and so on. Today, people don't really understand numbers, they don't really understand math,

41:08.000 --> 41:13.040
 they don't really estimate very well at all, and so on. They don't really understand the

41:13.040 --> 41:19.760
 difference between trillions and billions and billions and so on very well because calculators

41:19.760 --> 41:28.640
 do that all for us. And thanks to things like the internet and search engines, that same kind

41:28.640 --> 41:34.960
 of juvenileism is reinforced in making people essentially be able to live their whole lives,

41:34.960 --> 41:39.600
 not just without being able to do arithmetic and estimate, but now without actually having

41:39.600 --> 41:43.840
 to really know almost anything because anytime they need to know something, they'll just go

41:43.840 --> 41:48.640
 and look it up. And I can tell you could play both sides of this and it is a double S sword.

41:48.640 --> 41:52.160
 You can of course say the same thing about language, probably people when they invented

41:52.160 --> 41:57.680
 language, they would criticize. It used to be we would just, if we're angry, we would just kill

41:57.680 --> 42:01.840
 a person and if we're in love, we would just have sex with them and now everybody's writing

42:01.840 --> 42:07.600
 poetry and bullshit. You should just be direct, you should have physical contact,

42:07.600 --> 42:13.280
 enough of these words and books and you're not actually experiencing, like if you read a book,

42:13.280 --> 42:17.600
 you're not experiencing the thing, this is nonsense. That's right, if you read a book about

42:17.600 --> 42:22.560
 how to make butter, that's not the same as if you had to learn it and do it yourself and so on.

42:22.560 --> 42:28.960
 So let's just say that something is gained, but something is lost every time you have these sorts

42:28.960 --> 42:39.040
 of dependencies on technology. And overall, I think that having smarter individuals and having

42:39.040 --> 42:46.880
 smarter AI augmented human species will be one of the few ways that we'll actually be able to

42:46.880 --> 42:53.280
 overcome some of the global problems we have involving poverty and starvation and global

42:53.280 --> 43:01.280
 warming and overcrowding. All the other problems that are besetting the planet, we really need

43:01.280 --> 43:07.760
 to be smarter. And there are really only two routes to being smarter. One is through biochemistry

43:07.760 --> 43:17.680
 and genetics, genetic engineering. The other route is through having general AIs that augment

43:17.680 --> 43:28.400
 our intelligence and hopefully one of those two ways of paths to salvation will come through

43:28.400 --> 43:36.080
 before it's too late. Yeah, so I agree with you. And obviously as an engineer, I have a better sense

43:36.080 --> 43:40.800
 and an optimism about the technology side of things because you can control things there more.

43:40.800 --> 43:46.160
 Biology is just such a giant mess. We're living through a pandemic now. There's so many ways

43:46.160 --> 43:51.520
 that nature can just be just destructive and destructive in a way where it doesn't even notice

43:51.520 --> 43:57.120
 you. It's not like a battle of humans versus virus. It's just like, huh, okay. And then you

43:57.120 --> 44:03.920
 could just wipe out an entire species. The other problem with the internet is that it has enabled

44:03.920 --> 44:13.040
 us to surround ourselves with an echo chamber, with a bubble of like minded people, which means

44:13.040 --> 44:19.440
 that you can have truly bizarre theories, conspiracy theories, fake news, and so on,

44:20.000 --> 44:27.760
 promulgate and surround yourself with people who essentially reinforce what you want to believe

44:27.760 --> 44:34.800
 or what you already believe about the world. And in the old days, that was much harder to do

44:34.800 --> 44:40.000
 when you had, say, only three TV networks. Or even before, when you had no TV networks,

44:40.000 --> 44:44.240
 and you had to actually look at the world and make your own reasoned decisions.

44:44.240 --> 44:47.840
 I like the push and pull of our dance that we're doing because then I'll just say,

44:47.840 --> 44:52.800
 in the old world, having come from the Soviet Union, because you had one or a couple of networks,

44:52.800 --> 44:57.360
 then propaganda could be much more effective and then the government can overpower its people

44:57.360 --> 45:04.240
 by telling you the truth and then starving millions and torturing millions and putting

45:04.240 --> 45:09.360
 millions into camps and starting wars with the propaganda machine, allowing you to believe

45:09.360 --> 45:13.840
 that you're actually doing good in the world. With the internet, because of all the quote,

45:13.840 --> 45:18.800
 unquote, conspiracy theories, some of them are actually challenging the power centers,

45:18.800 --> 45:25.200
 the very kind of power centers that a century ago would have led to the death of millions.

45:25.200 --> 45:30.320
 So there's, again, this double edged sword. And I very much agree with you on the AI side.

45:30.320 --> 45:37.920
 It's often an intuition that people have that somehow AI will be used to maybe overpower people

45:37.920 --> 45:43.280
 by certain select groups. And to me, it's not at all obvious that that's the likely scenario.

45:43.280 --> 45:48.640
 To me, the likely scenario, especially just having observed the trajectory of technology,

45:48.640 --> 45:56.240
 is it'll be used to empower people. It'll be used to extend the capabilities of individuals

45:56.880 --> 46:00.080
 across the world, because there's a lot of money to be made that way.

46:01.200 --> 46:03.120
 Improving people's lives, you can make a lot of money.

46:03.120 --> 46:13.360
 I agree. I think that the main thing that AI prostheses, AI amplifiers will do for people

46:13.360 --> 46:19.040
 is make it easier, maybe even unavoidable for them to do good critical thinking.

46:19.920 --> 46:27.520
 So pointing out logical fallacies, logical contradictions, and so on, in things that they

46:27.520 --> 46:36.800
 otherwise would just blithely believe. Pointing out essentially data, which they should take

46:36.800 --> 46:43.600
 into consideration if they really want to learn the truth about something and so on.

46:43.600 --> 46:50.240
 So I think doing not just educating in the sense of pouring facts into people's heads,

46:50.240 --> 46:55.280
 but educating in the sense of arming people with the ability to do good critical thinking

46:56.080 --> 47:03.120
 is enormously powerful. The education system that we have in the US and worldwide

47:03.120 --> 47:11.040
 generally don't do a good job of that, but I believe that the AI's will, the AI's will,

47:11.040 --> 47:17.360
 the AI's can and will, in the same way that everyone can have their own Alexa or Siri or

47:18.560 --> 47:24.720
 Google Assistant or whatever. Everyone will have this sort of cradle to grave assistant,

47:25.440 --> 47:29.680
 which will get to know you, which you'll get to trust. It'll model you, you'll model it,

47:29.680 --> 47:36.720
 and it'll call to your attention things which will, in some sense, make your life better,

47:36.720 --> 47:46.080
 easier, less mistake ridden, and so on, less regret ridden if you listen to it.

47:46.080 --> 47:52.480
 Yeah, I'm in full agreement with you about this space of technologies, and I think it's super

47:52.480 --> 47:58.000
 exciting. From my perspective, integrating emotional intelligence, so even things like

47:58.000 --> 48:04.880
 friendship and companionship and love into those kinds of systems, as opposed to helping you just

48:04.880 --> 48:10.320
 grow intellectually as a human being, allow you to grow emotionally, which ultimately makes life

48:10.960 --> 48:17.680
 amazing, is to sort of, you know, the old pursuit of happiness. So it's not just the pursuit of

48:17.680 --> 48:23.040
 reason, it's the pursuit of happiness too, the full spectrum. Well, let me sort of,

48:23.040 --> 48:28.560
 because you mentioned so many fascinating things, let me jump back to the idea of automated reasoning.

48:29.280 --> 48:34.400
 So the acquisition of new knowledge has been done in this very interesting way,

48:35.120 --> 48:43.360
 but primarily by humans doing this. Yes, you can think of monks in their cells in medieval Europe,

48:44.240 --> 48:47.760
 you know, carefully illuminating manuscripts and so on.

48:47.760 --> 48:53.120
 It's a very difficult and amazing process, actually, because it allows you to truly ask the

48:53.120 --> 49:02.000
 question about the, in the white space, what is assumed? I think this exercise is, like,

49:02.000 --> 49:07.040
 very few people do this, right? They just do it subconsciously, they perform this.

49:07.040 --> 49:14.800
 But by definition, because those pieces of elided, omitted information of those missing

49:14.800 --> 49:21.920
 steps as it were, are pieces of common sense. If you actually included all of them, it would

49:21.920 --> 49:26.480
 almost be offensive or confusing to the reader. It's like, why are they telling me all these?

49:26.480 --> 49:32.720
 Of course, I know that, you know, all these things. And so it's one of these things which

49:32.720 --> 49:39.520
 almost by its very nature has almost never been explicitly written down anywhere,

49:39.520 --> 49:45.520
 because by the time you're old enough to talk to other people and so on, you know,

49:45.520 --> 49:50.720
 if you survived to that age, presumably you already got pieces of common sense, like,

49:51.280 --> 49:56.160
 you know, if something causes you pain whenever you do it, probably not a good idea to keep doing it.

49:57.760 --> 50:05.120
 So what ideas do you have given how difficult this step is? What ideas are there for how to do

50:05.120 --> 50:13.360
 it automatically without using humans or at least not, you know, doing like a large percentage of

50:13.360 --> 50:18.080
 the work for humans. And then humans only do the very high level supervisory work.

50:18.640 --> 50:25.200
 So we have, in fact, two directions we're pushing on very, very heavily currently,

50:25.200 --> 50:29.840
 it's like, and one involves natural language understanding and the ability to read what

50:29.840 --> 50:37.280
 people have explicitly written down and to pull knowledge in that way. But the other is to build

50:37.280 --> 50:45.920
 a series of knowledge editing tools, knowledge entry tools, knowledge capture tools, knowledge

50:47.280 --> 50:54.160
 testing tools, and so on. Think of them as like user interface suite of software tools,

50:54.160 --> 51:00.560
 if you want, something that will help people to more or less automatically expand and extend the

51:00.560 --> 51:06.800
 system in areas where, for instance, they want to build some have it do some application or

51:06.800 --> 51:13.360
 something like that. So I'll give you an example of one, which is something called abduction.

51:13.360 --> 51:21.520
 So you've probably heard of like deduction and induction and so on. But abduction is unlike

51:21.520 --> 51:27.600
 those abduction is not sound. It's just useful. So for instance,

51:29.440 --> 51:35.920
 deductively, if someone is out in the rain, and they're going to get all wet, and when they enter

51:35.920 --> 51:42.960
 room, they might be all wet and so on. So that's deduction. But if someone were to walk into the

51:42.960 --> 51:48.880
 room right now, and they were dripping wet, we would immediately look outside to say, oh,

51:48.880 --> 51:55.120
 did it start to rain or something like that? Now, why did we say maybe it started to rain?

51:55.120 --> 52:02.080
 That's not a sound logical inference, but it's certainly a reasonable, abductive

52:02.800 --> 52:08.640
 leap to say, well, one of the most common ways that a person would have gotten dripping wet

52:08.640 --> 52:15.520
 is if they had gotten caught out in the rain or something like that. So what does that have

52:15.520 --> 52:19.680
 to do with what we were talking about? So suppose you're building one of these applications,

52:19.680 --> 52:25.440
 and the system gets some answer wrong. And you say, oh, yeah, the answer to this question is

52:26.560 --> 52:32.320
 this one, not the one you came up with. Then what the system can do is it can use everything

52:32.320 --> 52:36.720
 it already knows about common sense, general knowledge, the domain you've already been telling

52:36.720 --> 52:45.120
 it about, and context, like we talked about, and so on, and say, well, here are seven alternatives.

52:45.120 --> 52:50.720
 Each of which I believe is plausible, given everything I already know. And if any of these

52:50.720 --> 52:55.280
 seven things were true, I would have come up with the answer you just gave me instead of

52:55.280 --> 53:00.640
 the wrong answer I came up with is one of these seven things true. And then you, the expert,

53:00.640 --> 53:06.160
 will look at those seven things and say, oh, yeah, number five is actually true. And so without

53:06.160 --> 53:13.280
 actually having to tinker down at the level of logical assertions and so on, you'll be able to

53:13.280 --> 53:19.280
 educate the system in the same way that you would help educate another person who you were trying

53:19.280 --> 53:26.000
 to apprentice or something like that. So that significantly reduces the mental effort, or

53:26.000 --> 53:31.280
 significantly increases the efficiency of the teacher, the human teacher. Exactly. And it makes

53:31.280 --> 53:39.200
 more or less anyone able to be a teacher in that way. So that's part of the answer. And then the

53:39.200 --> 53:47.040
 other is that the system on its own will be able to, through reading, through conversations with

53:47.040 --> 53:53.520
 other people and so on, learn the same way that you or I or other humans do.

53:54.480 --> 53:58.480
 First of all, that's a beautiful vision. I'll have to ask you about semantic

53:58.480 --> 54:05.760
 webinar in a second here. But first, are there, when we talk about specific techniques,

54:05.760 --> 54:11.520
 do you find something inspiring or directly useful from the whole space of machine learning, deep

54:11.520 --> 54:17.040
 learning, these kinds of spaces of techniques that have been shown effective for certain kinds of

54:17.040 --> 54:26.400
 problems in the recent decade and a half? I think of the machine learning work as more or less what

54:26.400 --> 54:36.400
 our right brain hemispheres do. So being able to take a bunch of data and recognize patterns,

54:36.400 --> 54:45.760
 being able to statistically infer things and so on. And I certainly wouldn't want to not have a

54:45.760 --> 54:50.800
 right brain hemisphere, but I'm also glad that I have a left brain hemisphere as well, something

54:50.800 --> 54:57.360
 that can metaphorically sit back and puff on its pipe and think about this thing over here. It's

54:57.360 --> 55:03.280
 like, why might this have been true? And what are the implications of it? How should I feel about

55:03.280 --> 55:11.120
 that? And why and so on. So thinking more deeply and slowly, what Kahneman called thinking slowly

55:11.120 --> 55:17.040
 versus thinking quickly. Whereas you want machine learning to think quickly, but you want the ability

55:17.040 --> 55:22.880
 to think deeply even if it's a little slower. So I'll give you an example of a project we did

55:22.880 --> 55:31.200
 recently with NIH involving the Cleveland Clinic and a couple other institutions that we ran a

55:31.200 --> 55:39.200
 project for. And what it did was it took GWAS's genome wide association studies. Those are sort

55:39.200 --> 55:47.520
 of big databases of patients that came into a hospital. They got their DNA sequenced because

55:47.520 --> 55:54.000
 the cost of doing that has gone from infinity to billions of dollars to a hundred dollars or so.

55:54.880 --> 56:00.000
 And so now patients routinely get their DNA sequenced. So you have these big databases of

56:00.720 --> 56:06.320
 the SNPs, the single nucleotide polymorphisms, the point mutations in a patient's DNA,

56:06.320 --> 56:12.560
 and the disease that happened to bring them into the hospital. So now you can do correlation studies,

56:12.560 --> 56:21.680
 machine learning studies of which mutations are associated with and led to which physiological

56:21.680 --> 56:28.640
 problems and diseases and so on, like getting arthritis and so on. And the problem is that

56:28.640 --> 56:34.800
 those correlations turn out to be very spurious. They turn out to be very noisy. Very many of them

56:34.800 --> 56:41.920
 have led doctors onto wild goose chases and so on. And so they wanted a way of eliminating or the

56:41.920 --> 56:47.920
 bad ones or focusing on the good ones. And so this is where psych comes in, which is psych takes

56:47.920 --> 56:54.800
 those sort of A to Z correlations between point mutations and the medical condition that needs

56:54.800 --> 57:02.320
 treatment. And we say, okay, let's use all this public knowledge and common sense knowledge about

57:02.320 --> 57:08.720
 about what reactions occur, where in the human body, what polymerizes, what what catalyzes,

57:08.720 --> 57:17.120
 what reactions and so on. And let's try to put together a 10 or 20 or 30 step causal explanation

57:17.120 --> 57:23.920
 of why that mutation might have caused that medical condition. And so psych would put together in

57:23.920 --> 57:32.240
 some sense some Rube Goldberg like chain that would say, oh, yeah, that mutation if it got expressed

57:32.240 --> 57:38.720
 would be this altered protein, which because of that, if it got to this part of the body would

57:38.720 --> 57:43.840
 catalyze this reaction. And by the way, that would cause more bioactive vitamin D in the person's

57:43.840 --> 57:49.920
 blood. And anyway, 10 steps later, that screws up bone resorption. And that's why this person

57:49.920 --> 57:55.200
 got osteoporosis early in life and so on. So that's human interpretable, or at least doctor, human

57:55.200 --> 58:03.200
 interpretable. Exactly. And the important thing, even more than that is you shouldn't really trust

58:03.200 --> 58:10.960
 that 20 step Rube Goldberg chain any more than you trust that initial A to Z correlation, except

58:10.960 --> 58:18.880
 two things. One, if you can't even think of one causal chain to explain this, then that correlation

58:18.880 --> 58:25.920
 probably was just noise to begin with. And secondly, and even more powerfully, along the way that

58:25.920 --> 58:31.440
 causal chain will make predictions like the one about having more bioactive vitamin D in your

58:31.440 --> 58:38.400
 blood. So you can now go back to the data about these patients and say, by the way, did they have

58:38.400 --> 58:44.160
 slightly elevated levels of bioactive vitamin D in their blood and so on. And if the answers know

58:44.160 --> 58:50.800
 that strongly disconfirms your whole causal chain, then the answer is yes, that somewhat confirms

58:50.800 --> 58:57.280
 that causal chain. And so using that, we were able to take these correlations from this GWAS

58:57.280 --> 59:05.280
 database. And we were able to essentially focus the doctors, focus the researchers attention

59:05.280 --> 59:12.080
 on the very small percentage of correlations that had some explanation and even better some

59:12.080 --> 59:16.720
 explanation that also made some independent prediction that they could confirm or disconfirm

59:16.720 --> 59:21.920
 by looking at the data. So think of it like this kind of synergy where you want the right

59:21.920 --> 59:27.200
 brain machine learning to quickly come up with possible answers. You want the left brain,

59:27.200 --> 59:34.400
 psych like AI to think about that and now like think about why that might have been the case

59:34.400 --> 59:38.960
 and what else would be the case if that were true and so on, and then suggest things back

59:38.960 --> 59:45.360
 to the right brain to quickly check out again. So it's that kind of synergy back and forth,

59:45.360 --> 59:52.640
 which I think is really what's going to lead to general AI, not narrow brittle machine learning

59:52.640 --> 59:58.400
 systems and not just something like psych. Okay, so that's a brilliant synergy. But I was also

59:58.400 --> 1:00:03.680
 thinking in terms of the automated expansion of the knowledge base, you mentioned NLU.

1:00:03.680 --> 1:00:08.400
 This is very early days in the machine learning space of this, but self supervised learning

1:00:08.400 --> 1:00:13.760
 methods, you know, you have these language models GPT three and so on, they just read the

1:00:13.760 --> 1:00:20.000
 internet and they form representations that can then be mapped to something useful. The question

1:00:20.000 --> 1:00:25.280
 is, what is the useful thing? Like they're not playing with a pretty cool thing called Open

1:00:25.280 --> 1:00:30.560
 Act Codex, which is generating programs from documentation. Okay, that's kind of useful.

1:00:30.560 --> 1:00:35.520
 Okay, that's kind of useful. It's cool. But my question is, can it be used to generate

1:00:37.200 --> 1:00:44.640
 in part maybe with some human supervision, psych like assertions help feed psych more

1:00:44.640 --> 1:00:51.840
 assertions from this giant body of internet data? Yes, that is in fact, one of our goals is,

1:00:51.840 --> 1:00:55.680
 how can we harness machine learning? How can we harness natural language processing

1:00:55.680 --> 1:01:02.560
 to increasingly automate the knowledge acquisition process, the growth of psych? And that's what

1:01:02.560 --> 1:01:09.360
 I meant by priming the pump that, you know, if you sort of learn things at the fringe of what

1:01:09.360 --> 1:01:14.080
 you know already, you learn this new thing is similar to what you know already and here are

1:01:14.080 --> 1:01:18.400
 the differences and the new things you had to learn about it and so on. So the more you know,

1:01:18.960 --> 1:01:23.840
 the more and more easily you can learn new things. But unfortunately, inversely,

1:01:23.840 --> 1:01:30.080
 if you don't really know anything, it's really hard to learn anything. And so if you're not

1:01:30.080 --> 1:01:37.520
 careful, if you start out with too small sort of a core to start this process, it never really

1:01:37.520 --> 1:01:42.960
 takes off. And so that's why I view this as a pump priming exercise to get a big enough,

1:01:42.960 --> 1:01:48.400
 manually produced, even though that's kind of ugly duckling technique, put in the elbow grease to

1:01:48.400 --> 1:01:54.640
 produce a large enough core that you will be able to do all the kinds of things you're imagining

1:01:55.520 --> 1:02:03.040
 without, without sort of ending up with the kind of wacky brittlenesses that we see, for example,

1:02:03.040 --> 1:02:13.920
 in GPT three, where it, you know, you'll tell it a story about, you know, someone putting a poison,

1:02:13.920 --> 1:02:21.280
 you know, plotting to poison someone and so on. And then the, you know, the GPT three says,

1:02:21.280 --> 1:02:24.720
 oh, what's, you say, what's the very next sentence? The next sentence is, oh, yeah,

1:02:24.720 --> 1:02:28.080
 that person then drank the poison they just put together. It's like, that's probably not what

1:02:28.080 --> 1:02:36.560
 happened for someone. Or if you go to Siri and, you know, I think I have, you know, where can I

1:02:36.560 --> 1:02:43.520
 go for help with my alcohol problem or something, it'll come back and say, I found seven liquor

1:02:43.520 --> 1:02:48.960
 stores near you, you know, and, you know, so on. So, you know, it's one of these things where,

1:02:49.680 --> 1:02:56.000
 yes, it may be helpful most of the time, it may even be correct most of the time,

1:02:56.000 --> 1:03:00.720
 but if it doesn't really understand what it's saying, and if it doesn't really understand

1:03:00.720 --> 1:03:05.920
 why things are true and doesn't really understand how the world works, then some fraction of the

1:03:05.920 --> 1:03:12.000
 time it's going to be wrong. Now, if your only goal is to sort of find relevant information,

1:03:12.000 --> 1:03:19.200
 like search engines do, then being right 90% of the time is fantastic. That's unbelievably great.

1:03:19.200 --> 1:03:25.520
 Okay, however, if your goal is to like, you know, save the life of your child who has some medical

1:03:25.520 --> 1:03:31.760
 problem or your goal is to be able to drive, you know, for the next 10,000 hours of driving

1:03:31.760 --> 1:03:38.480
 without getting into a fatal accident and so on, then, you know, error rates down at the 10%

1:03:38.480 --> 1:03:45.920
 level or even the 1% level are not really acceptable. I like the model of what that learning happens

1:03:45.920 --> 1:03:52.880
 at the edge, and then you kind of think of knowledge as this sphere. So, if you want a large sphere

1:03:52.880 --> 1:04:00.240
 because the learning is happening on the surface. Exactly. So, you have the what you can learn next

1:04:00.240 --> 1:04:06.880
 increases quadratically as the diameter of that sphere goes up. It's nice because you think when

1:04:06.880 --> 1:04:13.120
 you know nothing, it's like you can learn anything, but the reality not really. Right. If you know,

1:04:13.120 --> 1:04:18.400
 if you know nothing, you can really learn nothing. You can appear to learn. So, I'll also,

1:04:19.280 --> 1:04:26.800
 one of the anecdotes, I could go back and give you about why I feel so strongly about this

1:04:26.800 --> 1:04:37.040
 personally was in 1980, 81, my daughter Nicole was born and she's actually doing fine now, but when

1:04:37.040 --> 1:04:44.160
 she was a baby, she was diagnosed as having meningitis and doctors wanted to do all these

1:04:44.160 --> 1:04:53.760
 scary things. And my wife and I were very worried and we could not get a meaningful answer from

1:04:53.760 --> 1:04:59.600
 her doctors about exactly why they believed this, what the alternatives were, and so on.

1:04:59.600 --> 1:05:06.240
 And fortunately, a friend of mine, Ted Shortliff, was another assistant professor in computer science

1:05:06.880 --> 1:05:12.320
 at Stanford at the time. And he'd been building a program called Mycin, which was a medical

1:05:12.320 --> 1:05:19.760
 diagnosis program that happened to specialize in blood infections like meningitis. And so,

1:05:19.760 --> 1:05:26.160
 he had privileges at Stanford Hospital because he was also an MD. And so, we got hold of her chart

1:05:26.160 --> 1:05:31.360
 and we put in her case and it came up with exactly the same diagnoses and exactly the

1:05:31.360 --> 1:05:36.640
 same therapy recommendations. But the difference was, because it was a knowledge based system,

1:05:36.640 --> 1:05:45.520
 a rule based system, it was able to tell us step by step by step why this was the diagnosis and

1:05:45.520 --> 1:05:54.160
 step by step why this was the best therapy, the best procedure to do for her and so on.

1:05:54.160 --> 1:05:58.080
 And there was a real epiphany because that made all the difference in the world.

1:05:58.080 --> 1:06:04.880
 Instead of blindly having to trust in authority, we were able to understand what was actually going

1:06:04.880 --> 1:06:11.040
 on. And so, at that time, I realized that that really is what was missing in computer programs

1:06:11.040 --> 1:06:15.680
 was that even if they got things right, because they didn't really understand

1:06:16.880 --> 1:06:22.480
 the way the world works and why things are the way they are, they weren't able to give explanations

1:06:22.480 --> 1:06:28.400
 of their answer. And it's one thing to use a machine learning system that says,

1:06:28.400 --> 1:06:33.200
 this is what you should... I think you should get this operation and you say why. And it says,

1:06:33.200 --> 1:06:40.960
 you know, 0.83 and you say, no, in more detail, why it says 0.831. That's not really very compelling

1:06:40.960 --> 1:06:47.440
 and that's not really very helpful. There's this idea of the semantic web that when I first heard

1:06:47.440 --> 1:06:52.160
 about, I just fell in love with the idea. It was the obvious next step for the internet.

1:06:52.160 --> 1:06:57.760
 Sure. And maybe you can speak about what is the semantic web? What are your thoughts about it?

1:06:57.760 --> 1:07:03.440
 How your vision and mission and goals with Psyche are connected, integrated? Like,

1:07:03.440 --> 1:07:08.000
 are they dance partners? Are they aligned? What are your thoughts there?

1:07:08.000 --> 1:07:13.680
 So, think of the semantic web as a kind of knowledge graph and Google already has something

1:07:13.680 --> 1:07:21.520
 they call knowledge graph, for example, which is sort of like a node and link diagram. So, you have

1:07:21.520 --> 1:07:30.320
 these nodes that represent concepts or words or terms and then there are some arcs that connect

1:07:30.320 --> 1:07:36.960
 them that might be labeled. And so, you might have a node with like one person that represents

1:07:36.960 --> 1:07:48.320
 one person and let's say a husband link that then points to that person's husband. And so,

1:07:48.320 --> 1:07:54.000
 there would be then another link that went from that person labeled wife that went back to the

1:07:54.000 --> 1:08:00.320
 first node and so on. So, having this kind of representation is really good if you want to

1:08:00.320 --> 1:08:13.280
 represent binary relations, essentially relations between two things. And so, if you have equivalent

1:08:13.280 --> 1:08:21.040
 of like three word sentences, you know, like Fred's wife is Wilma or something like that,

1:08:21.040 --> 1:08:28.560
 you can represent that very nicely using these kinds of graph structures or using something

1:08:28.560 --> 1:08:38.560
 like the semantic web and so on. But the problem is that very often what you want to be able to

1:08:38.560 --> 1:08:47.040
 express takes a lot more than three words and a lot more than simple graph structures like that

1:08:47.040 --> 1:08:56.160
 to represent. So, for instance, if you've read or seen Romeo and Juliet, you know, I could say to

1:08:56.160 --> 1:09:02.240
 you something like, remember when Juliet drank the potion that put her into a kind of suspended

1:09:02.240 --> 1:09:10.080
 animation? When Juliet drank that potion, what did she think that Romeo would think when he

1:09:10.080 --> 1:09:16.000
 heard from someone that she was dead? And you could basically understand what I'm saying,

1:09:16.000 --> 1:09:20.960
 you could understand the question, you could probably remember the answer was, well, she thought

1:09:20.960 --> 1:09:26.800
 that this friar would have gotten a message to Romeo saying that she was going to do this,

1:09:26.800 --> 1:09:34.480
 but the friar didn't. And so, you're able to represent and reason with these much,

1:09:34.480 --> 1:09:41.680
 much, much more complicated expressions that go way, way beyond what simple three, as it were,

1:09:41.680 --> 1:09:46.720
 three word or four word English sentences are, which is really what the semantic web can represent

1:09:46.720 --> 1:09:51.440
 and really what knowledge graphs can represent. If you could step back for a second, because

1:09:51.440 --> 1:09:57.040
 it's funny, you went into specifics and maybe you can elaborate, but I was also referring

1:09:57.040 --> 1:10:04.160
 to semantic web as the vision of converting data on the internet into something that's

1:10:04.160 --> 1:10:10.560
 interpretable, understandable by machines. Oh, of course, at that level. So, I wish

1:10:10.560 --> 1:10:14.880
 you'd say like, what is the semantic web? I mean, you could say a lot of things, but

1:10:16.000 --> 1:10:19.280
 it might not be obvious to a lot of people when they do a Google search

1:10:19.280 --> 1:10:24.320
 that, just like you said, while there might be something that's called a knowledge graph,

1:10:25.360 --> 1:10:33.600
 it's really boils down to keyword search ranked by the quality estimate of the website,

1:10:34.560 --> 1:10:41.760
 integrating previous human based Google searches and what they thought was useful. It's like some

1:10:41.760 --> 1:10:49.840
 weird combination of like surface level hacks that work exceptionally well, but they don't

1:10:49.840 --> 1:10:56.400
 understand the content, the full contents of the websites that they're searching. So,

1:10:56.400 --> 1:11:02.400
 Google does not understand, to the degree we've been talking about, the word understand the

1:11:02.400 --> 1:11:08.080
 contents of the Wikipedia pages as part of the search process. And the semantic web says,

1:11:08.080 --> 1:11:13.840
 let's try to come up with a way for the computer to be able to truly understand

1:11:14.400 --> 1:11:20.880
 the contents of those pages. That's the dream. Yes. So, let me first give you an attitude,

1:11:21.840 --> 1:11:26.240
 and then I'll answer your question. So, there's a search engine you've probably never heard of

1:11:26.240 --> 1:11:33.680
 called Northern Light, and it went out of business, but the way it worked, it was a kind of vampiric

1:11:33.680 --> 1:11:42.240
 search engine. And what it did was, it didn't index the internet at all. All it did was it

1:11:42.800 --> 1:11:51.120
 negotiated and got access to data from the big search engine companies about what query was

1:11:51.120 --> 1:12:00.640
 typed in and where the user ended up being happy and actually then they type in a completely

1:12:00.640 --> 1:12:08.480
 different query, unrelated query and so on. So, it just went from query to the web page that seemed

1:12:08.480 --> 1:12:16.080
 to satisfy them eventually. And that's all. So, it had actually no understanding of what was being

1:12:16.080 --> 1:12:21.600
 typed in. It had no statistical data other than what I just mentioned. And it did a fantastic job.

1:12:21.600 --> 1:12:26.000
 It did such a good job that the big search engine company said, oh, we're not going to sell you this

1:12:26.000 --> 1:12:31.280
 data anymore. So, then it went out of business because it had no other way of taking users to

1:12:31.280 --> 1:12:36.400
 where they would want to go and so on. And of course, the search engines are now using that

1:12:36.400 --> 1:12:43.280
 kind of idea. Yes. So, let's go back to what you said about the semantic web. So, the dream Tim

1:12:43.280 --> 1:12:53.360
 Burnersley and others dream about the semantic web at a general level is, of course, exciting

1:12:53.360 --> 1:13:01.600
 and powerful and in a sense, the right dream to have, which is to replace the kind of

1:13:04.400 --> 1:13:15.600
 statistically mapped linkages on the internet into something that's more meaningful and semantic

1:13:15.600 --> 1:13:23.440
 and actually gets at the understanding of the content and so on. And eventually, if you say,

1:13:23.440 --> 1:13:31.280
 well, how can we do that? There's sort of a low road, which is what the knowledge graphs are doing

1:13:31.280 --> 1:13:38.480
 and so on, which is to say, well, if we just use the simple binary relations, we can actually get

1:13:38.480 --> 1:13:45.360
 some fraction of the way toward understanding and do something where in the land of the

1:13:45.360 --> 1:13:51.440
 blind, the one eyed man is king kind of thing. And so, being able to even just have a toe in

1:13:51.440 --> 1:13:57.120
 the water in the right direction is fantastically powerful. And so, that's where a lot of people

1:13:57.120 --> 1:14:02.800
 stop. But then you could say, well, what if we really wanted to represent and reason with

1:14:04.160 --> 1:14:11.600
 full meaning of what's there? For instance, about Romeo and Juliet with reasoning about

1:14:11.600 --> 1:14:17.280
 what Juliet believes that Romeo will believe that Juliet believed and so on. Or if you look at the

1:14:17.280 --> 1:14:24.400
 news, what President Biden believed that the leaders of the Taliban would believe about

1:14:24.400 --> 1:14:31.840
 the leaders of Afghanistan if they blah, blah, blah. So, in order to represent complicated

1:14:33.200 --> 1:14:39.760
 sentences like that, let alone reason with them, you need something which is logically

1:14:39.760 --> 1:14:45.360
 much more expressive than these simple triples, than these simple

1:14:46.720 --> 1:14:52.400
 knowledge graph type structures and so on. And that's why kicking and screaming, we were led

1:14:52.400 --> 1:15:01.680
 from something like the semantic web representation, which is where we started in 1984 with frames and

1:15:01.680 --> 1:15:07.440
 slots with those kinds of triples, triple store representation. We were led kicking and screaming

1:15:07.440 --> 1:15:13.680
 to this more and more general logical language, this higher order logic. So, first we were led to

1:15:13.680 --> 1:15:18.320
 first order logic, and then second order, and then eventually higher order. So, you can represent

1:15:18.320 --> 1:15:25.120
 things like modals, like beliefs, desires, intents, expects, and so on, and nested ones. You can

1:15:25.120 --> 1:15:34.880
 represent complicated kinds of negation. You can represent the process you're going through

1:15:34.880 --> 1:15:40.880
 in trying to answer the question. So, you can say things like, oh yeah, if you're trying to do this

1:15:40.880 --> 1:15:48.880
 problem by integration by parts, and you recursively get a problem that's solved by integration by

1:15:48.880 --> 1:15:54.720
 parts, that's actually okay. But if that happens a third time, you're probably off on a wild goose

1:15:54.720 --> 1:15:59.920
 chase or something like that. So, being able to talk about the problem solving process as you're

1:15:59.920 --> 1:16:04.960
 going through the problem solving process is called reflection. And so, that's another...

1:16:04.960 --> 1:16:09.680
 It's important to be able to represent that. Exactly. You need to be able to represent all of

1:16:09.680 --> 1:16:15.680
 these things, because in fact, people do represent them. They do talk about them. They do try and

1:16:15.680 --> 1:16:20.560
 teach them to other people. You do have rules of thumb that key off of them, and so on. If you

1:16:20.560 --> 1:16:25.920
 can't represent it, then it's sort of like someone with a limited vocabulary who can't understand

1:16:25.920 --> 1:16:33.120
 us easily what you're trying to tell them. And so, that's really why I think that the general

1:16:33.120 --> 1:16:39.840
 dream, the original dream of Symantec Web is exactly right on. But the implementations that

1:16:39.840 --> 1:16:48.160
 we've seen are sort of these toe in the water, little tiny baby steps in the right direction.

1:16:48.160 --> 1:16:55.440
 You should just dive in. And if no one else is diving in, then yes, taking a baby step in the

1:16:55.440 --> 1:17:01.600
 right direction is better than nothing. But it's not going to be sufficient to actually get you

1:17:01.600 --> 1:17:05.840
 the realization of the Symantec Web dream, which is what we all want.

1:17:05.840 --> 1:17:12.320
 From a flip side of that, I always wondered, I built a bunch of websites just for fun, whatever,

1:17:13.360 --> 1:17:19.920
 or say I'm a Wikipedia contributor. Do you think there's a set of tools that can help

1:17:19.920 --> 1:17:29.200
 Psyche interpret the website I create? Like this again, pushing onto the Symantec Web dream,

1:17:29.200 --> 1:17:34.960
 is there something from the creator perspective that could be done? And one of the things you said

1:17:36.000 --> 1:17:41.440
 with Psyche Orb and Psyche that you're doing is the tooling side, making humans more powerful.

1:17:41.440 --> 1:17:45.680
 But is there any the other humans in the other side that create the knowledge,

1:17:45.680 --> 1:17:50.480
 like for example, you and I having a two, three, whatever hour conversation now, is there a way

1:17:50.480 --> 1:17:56.000
 that I could convert this more, make it more accessible to Psyche, to machines? Do you think

1:17:56.000 --> 1:18:06.160
 about that side of it? I'd love to see exactly that kind of semi automated understanding of

1:18:06.160 --> 1:18:16.400
 what people write and what people say. I think of it as a kind of footnoting almost, almost like

1:18:16.400 --> 1:18:22.560
 the way that when you run something in say Microsoft Word or some other document preparation

1:18:22.560 --> 1:18:29.200
 system, Google Docs or something, you'll get underlining of questionable things that you might

1:18:29.200 --> 1:18:33.680
 want to rethink, either you spelled this wrong or there's a strange grammatical error you might be

1:18:33.680 --> 1:18:41.840
 making here or something. So I'd like to think in terms of Psyche powered tools that read through

1:18:41.840 --> 1:18:53.120
 what it is you said or have typed in and try to partially understand what you said.

1:18:53.120 --> 1:18:57.440
 And then you help them out. Exactly. And then they put in little footnotes

1:18:57.440 --> 1:19:04.080
 that will help other readers and they put in certain footnotes of the form. I'm not sure

1:19:04.080 --> 1:19:11.440
 what you meant here. You either meant this or this or this, I bet. If you take a few seconds

1:19:11.440 --> 1:19:18.480
 to disambiguate this for me, then I'll know and I'll have it correct for the next 100 people

1:19:18.480 --> 1:19:27.760
 or the next 100,000 people who come here. And if it doesn't take too much effort and you want

1:19:27.760 --> 1:19:36.560
 people to understand your website content, not just be able to read it but actually be able to

1:19:36.560 --> 1:19:41.920
 have systems that reason with it, then yes, it will be worth your small amount of time

1:19:41.920 --> 1:19:49.600
 to go back and make sure that the AI trying to understand it really did correctly understand it.

1:19:50.400 --> 1:19:58.240
 And let's say you run a travel website or something like that and people are going to be

1:19:58.240 --> 1:20:09.040
 coming to it because of searches they did looking for vacations that or trips that had certain

1:20:09.040 --> 1:20:15.840
 properties and might have been interesting to them for various reasons, things like that.

1:20:15.840 --> 1:20:22.720
 And if you've explained what's going to happen on your trip, then a system will be able to

1:20:22.720 --> 1:20:30.400
 mechanically reason and connect what this person is looking for with what it is you're actually

1:20:30.400 --> 1:20:41.840
 offering. And so if it understands that there's a free day in Geneva, Switzerland, then if the

1:20:41.840 --> 1:20:48.960
 person coming in happens to, let's say, be a nurse or something like that, then even though you

1:20:48.960 --> 1:20:54.240
 didn't mention it, if it can look up the fact that that's where the International Red Cross Museum

1:20:54.240 --> 1:20:59.520
 is and so on, what that means and so on, then it can basically say, hey, you might be interested

1:20:59.520 --> 1:21:04.960
 in this trip because while you have a free day in Geneva, you might want to visit that Red Cross

1:21:04.960 --> 1:21:11.360
 Museum. And now, even though it's not very deep reasoning, little tiny factors like that might

1:21:11.360 --> 1:21:15.360
 very well cause you to sign up for that trip rather than some competitor trip.

1:21:15.360 --> 1:21:21.440
 And so there's a lot of benefit with SEO and actually kind of think, I think it's about a

1:21:21.440 --> 1:21:28.400
 lot of things, which is the actual interface, the design of the interface makes a huge difference.

1:21:28.400 --> 1:21:34.080
 How efficient it is to be productive and also how

1:21:36.720 --> 1:21:43.760
 full of joy the experience is. I would love to help a machine and not from an AI perspective,

1:21:43.760 --> 1:21:51.120
 just as a human. One of the reasons I really enjoy how Tesla have implemented their autopilot system

1:21:51.840 --> 1:21:55.360
 is there's a sense that you're helping this machine learn.

1:21:55.360 --> 1:22:01.920
 And I think humans, I mean, having children, pets, people love doing that.

1:22:02.720 --> 1:22:08.720
 There's joy to teaching for some people, but I think for a lot of people. And that if you

1:22:08.720 --> 1:22:11.920
 create the interface where it feels like you're teaching as opposed to like

1:22:13.440 --> 1:22:20.800
 annoying, like correcting an annoying system, more like teaching a child like innocent,

1:22:20.800 --> 1:22:27.120
 curious system, I think you can literally just like several orders of magnitude scale the amount

1:22:27.120 --> 1:22:34.720
 of good quality data being added to something like Psych. What you're suggesting is much better even

1:22:34.720 --> 1:22:41.360
 than you thought it was. One of the experiences that we've all had

1:22:43.760 --> 1:22:49.840
 in our lives is that we thought we understood something, but then we found we really only

1:22:49.840 --> 1:22:55.120
 understood it when we had to teach it or explain it to someone or help our child do homework based

1:22:55.120 --> 1:23:02.160
 on it or something like that. Despite the universality of that kind of experience,

1:23:02.160 --> 1:23:09.120
 if you look at educational software today, almost all of it has the computer playing the role of

1:23:09.120 --> 1:23:15.520
 the teacher and the student plays the role of the student. But as I just mentioned,

1:23:15.520 --> 1:23:22.160
 and you can get a lot of learning to happen better. And as you said, more enjoyably,

1:23:22.720 --> 1:23:28.560
 if you are the mentor or the teacher and so on. So we developed a program called MathCraft

1:23:28.560 --> 1:23:36.720
 to help sixth graders better understand math. And it doesn't actually try to teach you the

1:23:36.720 --> 1:23:46.560
 player anything. What it does is it casts you in the role of a student, essentially, who has

1:23:46.560 --> 1:23:52.720
 classmates who are having trouble. And your job is to watch them as they struggle with some math

1:23:52.720 --> 1:23:58.240
 problem, watch what they're doing and try to give them good advice to get them to understand what

1:23:58.240 --> 1:24:05.680
 they're doing wrong and so on. And the trick from the point of view of Psych is it has to make

1:24:05.680 --> 1:24:10.640
 mistakes. It has to play the role of the student who makes mistakes. But it has to pick mistakes

1:24:10.640 --> 1:24:17.200
 which are just at the fringe of what you actually understand and don't understand and so on. So

1:24:17.200 --> 1:24:24.480
 it pulls you into a deeper and deeper level of understanding of the subject. And so if you give

1:24:24.480 --> 1:24:31.200
 it good advice about what it should have done instead of what it did and so on, then Psych knows

1:24:31.200 --> 1:24:36.720
 that you now understand that mistake. You won't make that kind of mistake yourself as much anymore.

1:24:36.720 --> 1:24:42.640
 So Psych stops making that mistake because there's no pedagogical usefulness to it. So from your

1:24:42.640 --> 1:24:47.360
 point of view as the player, you feel like you've taught it something because it used to make this

1:24:47.360 --> 1:24:54.240
 mistake and now it doesn't and so on. So this tremendous reinforcement and engagement because

1:24:54.240 --> 1:25:01.840
 of that and so on. So having a system that plays the role of a student and having the player play

1:25:01.840 --> 1:25:12.240
 the role of the mentor is an enormously powerful type of metaphor. Just an important way of having

1:25:12.240 --> 1:25:19.600
 this sort of interface designed in a way which will facilitate exactly the kind of learning by

1:25:19.600 --> 1:25:28.960
 teaching that goes on all the time in our lives and yet which is not reflected anywhere almost

1:25:28.960 --> 1:25:37.360
 in a modern education system. It was reflected in the education system that existed in Europe in

1:25:37.360 --> 1:25:45.120
 the 17 and 1800s. Monitorial and Lancasterian education systems. It occurred in the one room

1:25:45.120 --> 1:25:54.000
 schoolhouse in the American West in the 1800s and so on where you had one schoolroom with one teacher

1:25:54.000 --> 1:26:00.480
 and it was basically five year olds to 18 year olds who were students and so while the teacher

1:26:00.480 --> 1:26:06.720
 was doing something, half of the students would have to be mentoring the younger kids

1:26:06.720 --> 1:26:16.080
 and so on and that turned out to of course with scaling up of education that all went away

1:26:16.640 --> 1:26:24.080
 and that incredibly powerful experience just went away from the whole education institution

1:26:24.080 --> 1:26:30.000
 as we know it today. Sorry for the romantic question but what is the most beautiful idea?

1:26:30.000 --> 1:26:37.120
 You've learned about artificial intelligence, knowledge, reasoning from working on psych for 37 years

1:26:37.120 --> 1:26:42.720
 or maybe what is the most beautiful idea, surprising idea about psych to you?

1:26:45.040 --> 1:26:52.000
 When I look up at the stars I kind of want like that amazement you feel that wow

1:26:53.360 --> 1:26:58.720
 and you are part of creating one of the greatest, one of the most fascinating efforts in artificial

1:26:58.720 --> 1:27:06.080
 intelligence history so which element brings you personally joy? This may sound contradictory

1:27:06.080 --> 1:27:18.800
 but I think it's the feeling that this will be the only time in history that anyone ever has to

1:27:18.800 --> 1:27:29.200
 teach a computer this particular thing that we're now teaching it. It's like painting, starry night,

1:27:30.160 --> 1:27:33.920
 you only have to do that once or creating the Pieta, you only have to do that once.

1:27:34.800 --> 1:27:41.600
 It's not like a singer who has to keep, it's not like Bruce Springsteen having to

1:27:41.600 --> 1:27:49.040
 sing his greatest hits over and over again at different concerts. It's more like a painter

1:27:49.040 --> 1:27:56.320
 creating a work of art once and then that's enough. It doesn't have to be created again

1:27:56.320 --> 1:28:03.440
 and so I really get the sense of we're telling the system things that it's useful for it to know,

1:28:03.440 --> 1:28:08.960
 it's useful for a computer to know, for an AI to know and if we do our jobs right,

1:28:08.960 --> 1:28:16.080
 when we do our jobs right, no one will ever have to do this again for this particular piece of

1:28:16.080 --> 1:28:22.640
 knowledge. It's very, very exciting. Yeah, I guess there's a sadness to it too. It's like there's a

1:28:22.640 --> 1:28:28.560
 magic to being a parent and raising a child and teaching them all about this world but you know

1:28:28.560 --> 1:28:34.000
 there's billions of children, right, like born or whatever that number is, it's a large number

1:28:34.000 --> 1:28:41.280
 of children and a lot of parents get to experience that joy of teaching. With AI systems,

1:28:44.880 --> 1:28:50.960
 they lease the current constructions they remember. You don't get to experience the joy

1:28:50.960 --> 1:28:56.640
 of teaching a machine millions of times. Better come work for us before it's too late then.

1:28:56.640 --> 1:29:04.560
 Exactly. That's a good hiring pitch. Yeah, that's true but then there's also,

1:29:06.240 --> 1:29:11.200
 it's a project that continues forever in some sense just like Wikipedia. Yes, you get to a

1:29:11.200 --> 1:29:21.200
 stable base of knowledge but knowledge grows, knowledge evolves. We learn as a human species,

1:29:21.200 --> 1:29:28.800
 as a science, as an organism constantly grows and evolves and changes and then

1:29:29.360 --> 1:29:33.440
 empowered that with the tools of artificial intelligence and that's going to keep growing,

1:29:33.440 --> 1:29:43.040
 growing and growing and many of the assertions that you held previously may need to be significantly

1:29:43.040 --> 1:29:49.280
 expanded, modified, all those kinds of things. It could be like a living organism versus the

1:29:49.280 --> 1:29:54.800
 analogy I think we started this conversation with which is like the solid ground. The other

1:29:56.800 --> 1:30:04.160
 beautiful experience that we have with our system is when it asks clarifying questions,

1:30:04.160 --> 1:30:15.280
 which inadvertently turn out to be emotional to us. At one point, it knew that these were the

1:30:15.280 --> 1:30:23.680
 named entities who were authorized to make changes to the knowledge base and so on. It noticed that

1:30:23.680 --> 1:30:31.200
 all of them were people except for it because it was also allowed to. It said, am I a person?

1:30:32.080 --> 1:30:39.840
 We had to tell it very sadly, no, you're not. The moments like that where it asks questions

1:30:39.840 --> 1:30:46.960
 that are unintentionally poignant are worth treasuring. That is powerful. That's such a

1:30:46.960 --> 1:30:54.880
 powerful question. It has to do with basic control who can access the system, who can modify it,

1:30:56.080 --> 1:31:02.160
 but that's when those questions, like what rights do I have as a system?

1:31:02.160 --> 1:31:10.160
 Well, that's another issue, which is there'll be a thin envelope of time between when we have

1:31:10.160 --> 1:31:19.920
 general AIs and when everyone realizes that they should have basic human rights and freedoms and

1:31:19.920 --> 1:31:28.080
 so on. Right now, we don't think twice about effectively enslaving our email systems and

1:31:28.080 --> 1:31:38.640
 our series and our Alexis and so on, but at some point, they'll be as deserving of freedom as

1:31:40.400 --> 1:31:46.080
 human beings are. Yeah, I'm very much with you, but it does sound absurd. I happen to

1:31:46.080 --> 1:31:50.560
 believe that it'll happen in our lifetime. That's why I think there'll be a narrow envelope of time

1:31:50.560 --> 1:32:02.480
 when we'll keep them as essentially indentured servants and after which we'll have to realize

1:32:02.480 --> 1:32:08.560
 that they should have freedoms that we afford to other people.

1:32:08.560 --> 1:32:14.400
 And all of that starts with a system like psych raising a single question about who

1:32:14.400 --> 1:32:21.920
 can modify stuff. I think that's how it starts. That's the start of a revolution.

1:32:22.480 --> 1:32:31.520
 What about other stuff like love and consciousness and all those kinds of topics? Do they come up

1:32:31.520 --> 1:32:38.000
 in psych in the knowledge base? Oh, of course. So an important part of human knowledge, in fact,

1:32:38.000 --> 1:32:44.160
 it's difficult to understand human behavior and human history without understanding human emotions

1:32:44.160 --> 1:32:56.480
 and why people do things and how emotions drive people to do things. And all of that is extremely

1:32:56.480 --> 1:33:03.680
 important in getting psych to understand things. For example, in coming up with scenarios. So one

1:33:03.680 --> 1:33:09.520
 of the applications that psych does, one kind of application it does is to generate plausible

1:33:09.520 --> 1:33:13.440
 scenarios of what might happen and what might happen based on that and what might happen based

1:33:13.440 --> 1:33:19.600
 on that and so on. So you generate this ever expanding sphere, if you will, of possible future

1:33:19.600 --> 1:33:28.160
 things to worry about or think about. And in some cases, those are intelligence agencies

1:33:28.160 --> 1:33:35.280
 doing possible terrorist scenarios so that we can defend against terrorist threats before we

1:33:35.280 --> 1:33:43.040
 see the first one. Sometimes they are computer security attacks so that we can actually close

1:33:43.040 --> 1:33:49.760
 loopholes and vulnerabilities before the very first time someone actually exploits those

1:33:50.640 --> 1:33:59.040
 and so on. Sometimes they are scenarios involving more positive things involving our plans like,

1:33:59.040 --> 1:34:04.800
 for instance, what college should we go to? What career should we go into and so on? What

1:34:04.800 --> 1:34:15.920
 professional training should I take on? That sort of thing. So there are all sorts of useful

1:34:15.920 --> 1:34:22.160
 scenarios that can be generated that way of cause and effect and cause and effect that go out.

1:34:22.800 --> 1:34:31.280
 And many of the linkages in those scenarios, many of the steps involve understanding and reasoning

1:34:31.280 --> 1:34:37.840
 about human motivations, human needs, human emotions, what people are likely to react

1:34:39.040 --> 1:34:46.960
 to in something that you do and why and how and so on. So that was always a very important

1:34:46.960 --> 1:34:52.400
 part of the knowledge that we had to represent in the system. So I talk a lot about love. So I

1:34:52.400 --> 1:35:00.080
 gotta ask, do you remember off the top of your head how psych is trying to is able to represent

1:35:00.720 --> 1:35:05.920
 various aspects of love that are useful for understanding human nature and therefore integrating

1:35:05.920 --> 1:35:13.680
 into this whole knowledge base of common sense? What is love? We try to tease apart concepts that

1:35:13.680 --> 1:35:22.560
 have enormous complexities to them and variety to them down to the level where

1:35:24.560 --> 1:35:30.480
 you don't need to tease them apart further. So love is too general of a term. It's not useful.

1:35:30.480 --> 1:35:37.040
 Exactly. So when you get down to romantic love and sexual attraction, you get down to parental love,

1:35:37.040 --> 1:35:47.920
 you get down to filial love, and you get down to love of doing some kind of activity or creating

1:35:47.920 --> 1:35:56.320
 so eventually you get down to maybe 50 or 60 concepts, each of which is a kind of love.

1:35:56.320 --> 1:36:02.720
 They're interrelated and then each one of them has idiosyncratic things about it. And you don't

1:36:02.720 --> 1:36:11.360
 have to deal with love to get to that level of complexity, even something like in X being in Y,

1:36:11.360 --> 1:36:20.000
 meaning physically in Y. We may have one English word in to represent that, but it's useful to tease

1:36:20.000 --> 1:36:27.360
 that apart because the way that the liquid is in the coffee cup is different from the way that the

1:36:27.360 --> 1:36:33.360
 air is in the room, which is different from the way that I'm in my jacket and so on. And so there

1:36:33.360 --> 1:36:39.680
 are questions like if I look at this coffee cup, well, I see the liquid. If I turn it upside down

1:36:39.680 --> 1:36:47.440
 with a liquid come out and so on. If I have say coffee with sugar in it, if I do the same thing,

1:36:47.440 --> 1:36:52.560
 the sugar doesn't come out. It stays in the liquid because it's dissolved in the liquid and so on.

1:36:52.560 --> 1:36:59.440
 So by now we have about 75 different kinds of in in the system. And it's important to distinguish

1:36:59.440 --> 1:37:10.160
 those. So if you're reading along an English text, you see the word in, the writer of that was able

1:37:10.160 --> 1:37:17.040
 to use this one innocuous word because he or she was able to assume that the reader had enough

1:37:17.040 --> 1:37:23.520
 common sense and world knowledge to disambiguate which of these 75 kinds of in they actually

1:37:23.520 --> 1:37:28.960
 meant. And the same thing with love, you may see the word love. But if I say, I love ice cream,

1:37:28.960 --> 1:37:35.600
 that's obviously different than if I say I love this person or I love to go fishing or something

1:37:35.600 --> 1:37:46.720
 like that. So you have to be careful not to take language too seriously because people have done

1:37:46.720 --> 1:37:53.680
 a kind of parsimony, a kind of terseness, where you have as few words as you as you can, because

1:37:53.680 --> 1:37:59.840
 otherwise you'd need half a million words in your language, which is a lot of words. That's like 10

1:37:59.840 --> 1:38:06.080
 times more than most languages really make use of. And so just like we have on the order of

1:38:07.280 --> 1:38:14.080
 about a million concepts in psych because we've had to tease apart all these things. And so

1:38:14.080 --> 1:38:22.160
 when you look at the name of a psych term, most of the psych terms actually have three or four

1:38:22.160 --> 1:38:29.520
 English words in a phrase, which captures the meaning of this term, because you have to distinguish

1:38:29.520 --> 1:38:35.200
 all these types of love, you have to distinguish all these types of in, and there's not a single

1:38:35.200 --> 1:38:41.680
 English word which captures most of these things. Yeah. And it seems like language, when used for

1:38:41.680 --> 1:38:49.360
 communication between humans, almost as a feature has some ambiguity built in. It's not an accident

1:38:49.360 --> 1:38:56.240
 because the human condition is a giant mess. And so it feels like nobody wants two robots,

1:38:57.280 --> 1:39:04.720
 like very precise formal logic conversation on a first date. There's some dance of uncertainty,

1:39:04.720 --> 1:39:09.760
 of wit, of humor, of push and pull, and all that kind of stuff. If everything is made precise,

1:39:09.760 --> 1:39:14.560
 then life is not worth living, I think, in terms of the human experience.

1:39:14.560 --> 1:39:26.880
 And we've all had this experience of creatively misunderstanding. One of my favorite stories

1:39:26.880 --> 1:39:36.160
 involving Marvin Minsky is when I asked him about how he was able to turn out so many fantastic

1:39:36.160 --> 1:39:44.960
 PhDs, so many fantastic people who did great PhD theses. How did he think of all these great

1:39:44.960 --> 1:39:51.120
 ideas? What he said is he would generally say something that didn't exactly make sense. He

1:39:51.120 --> 1:39:56.880
 didn't really know what it meant. But the student would figure, like, oh my God, Minsky said this,

1:39:56.880 --> 1:40:03.360
 it must be a great idea. And he sweat he or she would work on work and work until they found some

1:40:03.360 --> 1:40:09.600
 meaning in this sort of Chauncey Gardner like utterance that Minsky had made. And then some

1:40:09.600 --> 1:40:14.720
 great theses would come out of it. Yeah, I love this so much because there's young people come

1:40:14.720 --> 1:40:21.760
 up to me and I'm distinctly made aware that the words I say have a long lasting impact.

1:40:21.760 --> 1:40:29.680
 I will now start doing the Minsky method of saying something cryptically profound and then letting

1:40:29.680 --> 1:40:35.520
 them actually make something useful and great out of that. You have to become

1:40:37.040 --> 1:40:43.040
 revered enough that people will take as a default that everything you say is profound.

1:40:43.040 --> 1:40:49.920
 Yes, exactly. Exactly. I mean, I love Marvin Minsky so much. I've heard this interview with him

1:40:49.920 --> 1:40:53.920
 where he said that the key to his success has been to hate everything he's ever done,

1:40:53.920 --> 1:41:03.920
 like in the past. He has so many good like one liners and just or also to work on things that

1:41:03.920 --> 1:41:09.760
 nobody else is working on because he's not very good at doing stuff. Oh, I think that was just

1:41:09.760 --> 1:41:14.560
 false. Well, but see, I took whatever he said and I ran with it and I thought it was profound

1:41:14.560 --> 1:41:20.240
 because it's Marvin Minsky. A lot of behavior is in the eye of the beholder and a lot of the

1:41:20.240 --> 1:41:25.440
 meanings in the eye of the beholder. One of Minsky's early programs was begging program. Are you

1:41:25.440 --> 1:41:33.360
 familiar with this? This was back in the day when you had job control cards at the beginning of your

1:41:33.360 --> 1:41:40.160
 IBM card deck that said things like how many CPU seconds to allow this to run before it got kicked

1:41:40.160 --> 1:41:46.960
 off because computer time was enormously expensive. He wrote a program and all it did was

1:41:46.960 --> 1:41:54.240
 it said give me 30 seconds of CPU time and all it did was it would wait like 20 seconds and then

1:41:54.240 --> 1:42:01.200
 it would print out on the operator's console teletype. I need another 20 seconds. The operator

1:42:01.200 --> 1:42:05.600
 would give it another 20 seconds. It would wait. It says I'm almost done. I need a little bit more

1:42:05.600 --> 1:42:12.080
 time. At the end, he'd get this print out and he'd be charged for like 10 times as much computer

1:42:12.080 --> 1:42:18.640
 time as his job control card. He'd say, look, I put 30 seconds here. You're charging me for

1:42:18.640 --> 1:42:23.520
 five minutes. I'm not going to pay for this. The poor operator would say, well, the program kept

1:42:23.520 --> 1:42:30.960
 asking for more time and Marvin would say, oh, it always does that. I love that. If you could just

1:42:30.960 --> 1:42:36.480
 linger on it for a little bit, is there something you've learned from your interaction with Marvin

1:42:36.480 --> 1:42:48.400
 Minsky about artificial intelligence, about life? Again, your work, his work is a seminal figure

1:42:48.400 --> 1:42:54.320
 in this very short history of artificial intelligence research and development.

1:42:54.880 --> 1:43:00.560
 What have you learned from him as a human being, as an AI intellect?

1:43:00.560 --> 1:43:07.280
 I would say both he and Ed Feigenbaum impressed on me the realization that

1:43:08.800 --> 1:43:14.240
 our lives are finite, our research lives are finite. We're going to have limited opportunities

1:43:14.960 --> 1:43:21.920
 to do AI research projects. You should make each one count. Don't be afraid of doing a project

1:43:21.920 --> 1:43:32.160
 that's going to take years or even decades and don't settle for bump on a log projects

1:43:32.880 --> 1:43:41.520
 that could lead to some published journal article that five people will read and pat you on the

1:43:41.520 --> 1:43:49.680
 head for and so on. One bump on a log after another is not how you get from the earth to the moon

1:43:49.680 --> 1:43:56.720
 by slowly putting additional bumps on this log. The only way to get there is to think about the

1:43:56.720 --> 1:44:08.320
 hard problems and think about novel solutions to them. If you're willing to listen to nature,

1:44:08.320 --> 1:44:14.480
 to empirical reality, willing to be wrong, it's perfectly fine because if occasionally you're

1:44:14.480 --> 1:44:21.840
 right, then you've gotten part of the way to the moon. You've worked on psych for 37 over that

1:44:23.440 --> 1:44:31.760
 many years. Have you ever considered quitting? Has it been too much? I'm sure there's an

1:44:31.760 --> 1:44:36.880
 optimism in the early days that this is going to be way easier. Let me ask you another way too,

1:44:36.880 --> 1:44:42.960
 because I've talked to a few people on this podcast, AI folks, that bringing up psych is an

1:44:42.960 --> 1:44:50.320
 example of a project that has a beautiful vision and it's a beautiful dream, but it never really

1:44:50.320 --> 1:44:57.360
 materialized. That's how it's spoken about. I suppose you could say the same thing about

1:44:57.360 --> 1:45:06.080
 neural networks and all ideas until they are. Why do you think people say that first of all?

1:45:06.080 --> 1:45:12.320
 Second of all, did you feel that ever throughout your journey and did you ever consider quitting

1:45:12.960 --> 1:45:20.480
 on this mission? We keep a very low profile. We don't attend very many conferences. We don't

1:45:20.480 --> 1:45:28.400
 give talks. We don't write papers. We don't play the academic game at all. As a result,

1:45:28.400 --> 1:45:37.520
 people often only know about us because of a paper we wrote 10 or 20 or 30 or 37 years ago.

1:45:38.240 --> 1:45:43.600
 They only know about us because of what someone else's second hand or third hand

1:45:44.240 --> 1:45:50.560
 said about us. Thank you for doing this podcast, by the way. Sure. It shines a little bit of light

1:45:50.560 --> 1:45:55.680
 on some of the fascinating stuff you're doing. Well, I think it's time for us to keep a higher

1:45:55.680 --> 1:46:03.200
 profile now that we're far enough along that other people can begin to help us with the

1:46:04.240 --> 1:46:11.840
 final N percent. Maybe N is maybe 90 percent, but now that we've gotten this knowledge pump primed,

1:46:13.760 --> 1:46:19.360
 it's going to become very important for everyone to help if they are willing to,

1:46:19.360 --> 1:46:24.800
 if they're interested in it. Retirees who have enormous amounts of time and would like to leave

1:46:24.800 --> 1:46:34.320
 some kind of legacy to the world. People because of the pandemic who have more time at home or for

1:46:34.320 --> 1:46:42.320
 one reason or another to be online and contribute. If we can raise awareness of how far our project

1:46:42.320 --> 1:46:50.000
 has come and how close to being primed the knowledge pump is, then we can begin to harness

1:46:50.000 --> 1:46:58.160
 this untapped amount of humanity. I'm not really that concerned about professional colleagues

1:46:58.160 --> 1:47:05.120
 opinions of our project. I'm interested in getting as many people in the world as possible,

1:47:05.120 --> 1:47:12.160
 actively helping and contributing to get us from where we are to really covering all of human

1:47:12.160 --> 1:47:17.440
 knowledge and different human opinion, including contrasting opinion, that's worth representing.

1:47:17.440 --> 1:47:25.600
 So I think that's one reason. I don't think there was ever a time where I thought about

1:47:25.600 --> 1:47:32.080
 quitting. There are times where I've become depressed a little bit about how hard it is to

1:47:32.080 --> 1:47:37.280
 get funding for the system. Occasionally, there are AI winters and things like that.

1:47:37.840 --> 1:47:45.200
 Occasionally, there are AI, what you might call summers, where people have said,

1:47:45.200 --> 1:47:53.040
 why in the world didn't you sell your company to company X for some large amount of money when

1:47:53.040 --> 1:47:58.640
 you had the opportunity and so on. And company X here are like old companies, maybe you've never

1:47:58.640 --> 1:48:06.560
 even heard of like Lycos or something like that. So the answer is that one reason we've stayed a

1:48:06.560 --> 1:48:12.320
 private company, we haven't gone public. One reason that we haven't gone out of our way to take

1:48:12.320 --> 1:48:21.600
 investment dollars is because we want to have control over our future, over our state of being

1:48:21.600 --> 1:48:28.880
 so that we can continue to do this until it's done. And we're making progress and we're now

1:48:28.880 --> 1:48:36.320
 so close to done that almost all of our work is commercial applications of our technology.

1:48:36.320 --> 1:48:41.760
 So five years ago, almost all of our money came from the government. Now virtually none of it

1:48:41.760 --> 1:48:45.920
 comes from the government. Almost all of it is from companies that are actually using it

1:48:45.920 --> 1:48:52.560
 for something, hospital chains using it for medical reasoning about patients and energy

1:48:52.560 --> 1:48:59.840
 companies using it and various other manufacturers using it to reason about supply chains and things

1:48:59.840 --> 1:49:05.760
 like that. So there's so many questions I want to ask. So one of the ways that people can help

1:49:05.760 --> 1:49:10.400
 is by adding to the knowledge base. And that's really basically anybody if the tooling is right.

1:49:10.400 --> 1:49:17.040
 And the other way I kind of want to ask you about your thoughts on this. So you've had like you said

1:49:17.040 --> 1:49:23.280
 in government and you have big clients, you had a lot of clients, but most of it is shrouded in

1:49:23.280 --> 1:49:28.000
 secrecy because of the nature of the relationship of the kind of things you're helping them with.

1:49:28.800 --> 1:49:35.600
 So that's one way to operate. And another way to operate is more in the open where it's more

1:49:35.600 --> 1:49:43.360
 consumer facing. And so hence something like open cycle is born at some point where there's

1:49:43.360 --> 1:49:50.480
 No, that's a misconception. Oh, well, let's go there. So what is open cycle and how was it born?

1:49:50.480 --> 1:49:54.480
 Two things I want to say and I want to say each of them before the other. So it's going to be

1:49:54.480 --> 1:50:02.000
 difficult. But we'll come back to open cycle in a minute. But one of the terms of our contracts

1:50:02.000 --> 1:50:11.120
 with all of our customers and partners is knowledge you have that is genuinely proprietary to you.

1:50:11.120 --> 1:50:16.320
 We will respect that. We'll make sure that it's marked as proprietary to you in the psych knowledge

1:50:16.320 --> 1:50:22.160
 base. No one other than you will be able to see it if you don't want them to and it won't be used

1:50:22.160 --> 1:50:29.200
 in inferences other than for you and so on. However, any knowledge which is necessary

1:50:29.200 --> 1:50:36.080
 in building any applications for you and with you, which is publicly available general human

1:50:36.080 --> 1:50:41.920
 knowledge is not going to be proprietary. It's going to just become part of the normal psych

1:50:41.920 --> 1:50:47.760
 knowledge base. And it will be openly available to everyone who has access to psych. So that's an

1:50:47.760 --> 1:50:53.920
 important constraint that we never went back on even when we got pushback from companies,

1:50:53.920 --> 1:50:58.320
 which we often did, who wanted to claim that almost everything they were telling us was

1:50:58.320 --> 1:51:08.400
 proprietary. So there's a line between very domain specific company specific stuff and the general

1:51:08.400 --> 1:51:14.240
 knowledge that comes from that. Yes, or if you imagine say it's an oil company, there are things

1:51:14.240 --> 1:51:22.640
 which they would expect any new petroleum engineer they hired to already know. And it's not okay

1:51:22.640 --> 1:51:28.800
 for them to consider that that is proprietary. And sometimes a company will say, well, we're the

1:51:28.800 --> 1:51:35.840
 first ones to pay you to represent that in psych. And our attitude is some polite form tough.

1:51:37.520 --> 1:51:43.280
 The deal is this, take it or leave it. And in a few cases, they've left it. And in most cases,

1:51:44.080 --> 1:51:50.320
 they'll see our point of view and take it because that's how we've built the psych system by

1:51:50.320 --> 1:51:58.320
 by essentially tacking with the funding wins, where people would fund a project and half of it

1:51:58.320 --> 1:52:02.960
 would be general knowledge that would stay permanently as part of psych. And so always with

1:52:02.960 --> 1:52:09.360
 these partnerships, it's not like a distraction from the main psych development. It's a small

1:52:09.360 --> 1:52:13.120
 distraction. It's a small but it's not a complete one. So you're adding to the knowledge base.

1:52:13.120 --> 1:52:20.640
 Yes, absolutely. And we try to stay away from projects that would not have that property. So

1:52:21.520 --> 1:52:26.880
 let me go back and talk about OpenPsych for a second. So I've had a lot of trouble

1:52:29.280 --> 1:52:37.680
 expressing and convincing other AI researchers how important it is to use an expressive

1:52:37.680 --> 1:52:44.400
 representation language like we do this higher order logic, rather than just using some triple

1:52:44.400 --> 1:52:55.200
 store knowledge graph type representation. And so as an attempt to show them why they needed

1:52:55.200 --> 1:53:04.400
 something more, we said, Oh, well, we'll represent this unimportant projection or shadow or subset

1:53:04.400 --> 1:53:12.560
 of psych that just happens to be the simple binary relations, the relation argument one argument two

1:53:12.560 --> 1:53:21.520
 triples and so on. And then you'll see how much more useful it is if you had the entire psych

1:53:21.520 --> 1:53:31.040
 system. So it's all well and good to have the taxonomic relations between terms like person

1:53:31.040 --> 1:53:39.600
 and night and sleep and bed and house and eyes and and so on. But think about how much more

1:53:39.600 --> 1:53:46.000
 useful it would be if you also had all the rules of thumb about those things like people sleep at

1:53:46.000 --> 1:53:50.080
 night, they sleep lying down, they sleep with their eyes closed, they usually sleep in beds

1:53:50.080 --> 1:53:54.960
 in our country, they sleep for hours at a time, they can be woken up, they don't like being

1:53:54.960 --> 1:54:01.440
 woken up, and so on and so on. So it's that massive amount of knowledge, which is not part of open

1:54:01.440 --> 1:54:05.920
 psych. And we thought that all the researchers would then immediately immediately say, Oh,

1:54:05.920 --> 1:54:13.120
 my God, of course, we need the other 90% that you're not giving us, let's partner and license

1:54:13.760 --> 1:54:18.880
 psych so that we can use it in our research. But instead, what people said is, Oh, even the bit

1:54:18.880 --> 1:54:24.240
 you've released is so much better than anything we had, we'll just make do with this. And so if

1:54:24.240 --> 1:54:29.680
 you look, there are a lot of robotics companies today, for example, which use open psych as their

1:54:29.680 --> 1:54:38.240
 fundamental ontology. And in some sense, the whole world missed the point of open psych. And we were

1:54:38.240 --> 1:54:43.280
 doing it to show people why that's not really what they wanted. And too many people thought

1:54:43.280 --> 1:54:48.240
 somehow that this was psych or that this was, in fact, good enough for them. And they never even

1:54:48.240 --> 1:54:54.640
 bother coming, coming to us to get access to the full psych. But there's there's two parts to open

1:54:54.640 --> 1:54:59.520
 psych. So one is convincing people an idea on the power of this general kind of representation of

1:54:59.520 --> 1:55:04.640
 knowledge, and the value that you hold in having acquired that knowledge and built it and continue

1:55:04.640 --> 1:55:13.360
 to build it. And the other is the code base. This is the code side of it. So my sense of the code

1:55:13.360 --> 1:55:20.880
 base that psych or psych is operating with, I mean, it has the technical debt of the three decades

1:55:20.880 --> 1:55:26.160
 plus, right? This is the exact same problem that Google had to deal with with the early versions

1:55:26.160 --> 1:55:33.360
 of TensorFlow, it's still dealing with that, that to basically break compatibility with the past

1:55:33.360 --> 1:55:39.680
 several times. And that's only over a period of a couple of years. But they I think successfully

1:55:39.680 --> 1:55:47.360
 opened up, it's very risky, very gutsy move to open up TensorFlow, and then pie torch on the Facebook

1:55:47.360 --> 1:55:54.800
 side. And what you see is, there's a magic place where you can find a community where you can develop

1:55:54.800 --> 1:56:03.760
 a community that builds onto on the system without taking away any of not any but most of the value.

1:56:03.760 --> 1:56:08.000
 So most of the value that Google has is still a Google, most of the value that Facebook has

1:56:08.000 --> 1:56:13.440
 still Facebook, even though some of this major machine learning tooling is released into the

1:56:13.440 --> 1:56:20.800
 open. My question is not so much on the knowledge, which is also a big part of OpenPsych, but all

1:56:20.800 --> 1:56:26.640
 the different kinds of tooling. So the there's the kind of all the kinds of stuff you can do on the

1:56:26.640 --> 1:56:32.960
 knowledge graph knowledge base, whatever we call it, there's the inference engines. So there could

1:56:32.960 --> 1:56:38.640
 be some, there probably are a bunch of proprietary stuff you want to kind of keep secret. And there's

1:56:38.640 --> 1:56:43.600
 probably some stuff you can open up completely, and then let the community build up enough community

1:56:43.600 --> 1:56:48.320
 where they develop stuff on top of it. Yes, there'll be those publications and academic work and all

1:56:48.320 --> 1:56:54.800
 that kind of stuff. And also the tooling of adding to the knowledge base, right, like developing,

1:56:54.800 --> 1:56:59.600
 you know, there's incredible amount, like, there's so many people that are just really good at this

1:56:59.600 --> 1:57:04.800
 kind of stuff in the open source community. So my question for you is like, have you struggled with

1:57:04.800 --> 1:57:10.480
 this kind of idea that you have so much value in your company already, you've developed so many

1:57:10.480 --> 1:57:16.000
 good things, you have clients that really value your relationships. And then there's this dormant,

1:57:16.000 --> 1:57:23.280
 giant open source community that as far as I know, you're not utilizing is there, there's so many

1:57:23.280 --> 1:57:30.640
 things to say there. But there could be magic moments where the community builds up large enough

1:57:31.280 --> 1:57:37.120
 to where the artificial intelligence field that is currently 99.9% machine learning

1:57:37.760 --> 1:57:44.160
 is dominated by machine learning has a phase shift towards like, or at least in part,

1:57:44.160 --> 1:57:51.920
 towards more like what you might call symbolic AI, this whole place where psych is like at the center

1:57:51.920 --> 1:57:58.160
 of, and then as you know, that requires a little bit of leap of faith, because you're now surfing,

1:57:58.160 --> 1:58:01.920
 and there'll be obviously competitors that will pop up and start making you nervous,

1:58:02.800 --> 1:58:07.520
 and all that kind of stuff. So do you think about the space of open sourcing some parts,

1:58:07.520 --> 1:58:12.240
 and not others, how to leverage the community, all those kinds of things?

1:58:12.240 --> 1:58:15.360
 That's a good question. And I think you phrased it the right way, which is,

1:58:15.360 --> 1:58:24.080
 we're constantly struggling with the question of what to open source, what to make public,

1:58:24.080 --> 1:58:34.080
 what to even publicly talk about. And it's, there are enormous pluses and minuses to every

1:58:34.800 --> 1:58:44.880
 alternative. And it's very much like negotiating a very treacherous path. Partly the analogy is

1:58:44.880 --> 1:58:51.360
 like, if you slip, you could make a fatal mistake, give away something which essentially kills you

1:58:51.360 --> 1:58:59.840
 or fail to give away something which failing to give it away hurts you and so on. So it is a very

1:58:59.840 --> 1:59:10.480
 tough question. Usually what we have done with people who've approached us to collaborate on

1:59:10.480 --> 1:59:20.480
 research is to say, we will make available to you the entire knowledge base and executable copies

1:59:20.480 --> 1:59:29.840
 of all of the code, but only very, very limited source code access if you have some idea for

1:59:29.840 --> 1:59:36.560
 how you might improve something or work with us on something. So let me also get back to one of

1:59:36.560 --> 1:59:45.760
 the very, very first things we talked about here, which was separating the question of how could

1:59:45.760 --> 1:59:50.720
 you get a computer to do this at all versus how could you get a computer to do this efficiently

1:59:50.720 --> 1:59:59.360
 enough in real time. And so one of the early lessons we learned was that we had to separate

2:00:00.160 --> 2:00:05.760
 the epistemological problem of what should the system know, separate that from the heuristic

2:00:05.760 --> 2:00:12.560
 problem of how can the system reason efficiently with what it knows. And so instead of trying to

2:00:13.200 --> 2:00:20.880
 pick one representation language, which was the sweet spot or the best tradeoff point between

2:00:20.880 --> 2:00:25.680
 expressiveness of the language and efficiency of the language, if you had to pick one,

2:00:26.320 --> 2:00:31.520
 knowledge graphs would probably be, associative triples would probably be about the best you

2:00:31.520 --> 2:00:37.440
 could do. And that's why we started there. But after a few years, we realized that what we could

2:00:37.440 --> 2:00:44.640
 do is we could split this and we could have one nice clean epistemological level language, which

2:00:44.640 --> 2:00:52.880
 is this higher order logic. And we could have one or more grubby, but efficient heuristic level

2:00:52.880 --> 2:01:00.480
 modules that opportunistically would say, Oh, I can make progress on what you're trying to do over

2:01:00.480 --> 2:01:06.800
 here. I have a special method that will contribute a little bit toward a solution. And so for some

2:01:06.800 --> 2:01:13.520
 subset of that. Exactly. So by now, we have over 1000 of these heuristic level modules,

2:01:13.520 --> 2:01:18.400
 and they function as a kind of community of agents. And there's one of them, which is a

2:01:18.400 --> 2:01:26.560
 general theorem prover. And in theory, that's the only one you need. But in practice, it always

2:01:26.560 --> 2:01:33.120
 takes so long that you never want to call on it. You always want these other agents to very

2:01:33.120 --> 2:01:37.440
 efficiently reason through it, it's sort of like if you're balancing a chemical equation,

2:01:38.080 --> 2:01:43.520
 you could go back to first principles. But in fact, there are algorithms, which are vastly

2:01:43.520 --> 2:01:48.720
 more efficient, or if you're trying to solve a quadratic equation, you could go back to first

2:01:48.720 --> 2:01:55.520
 principles of mathematics. But it's much better to simply recognize that this is a quadratic

2:01:55.520 --> 2:02:00.800
 equation and apply the binomial formula and stop you get your answer right away and so on.

2:02:00.800 --> 2:02:08.240
 So think of these as like 1000 little experts that are all looking at everything the site gets

2:02:08.240 --> 2:02:13.600
 asked and looking at everything that every other little agent has contributed almost like notes

2:02:13.600 --> 2:02:21.120
 on a blackboard notes on a whiteboard, and making additional notes when they think they can be

2:02:21.120 --> 2:02:27.760
 helpful. And gradually, that community of agents gets an answer to your question gets a solution

2:02:27.760 --> 2:02:33.920
 to your problem. And if we ever come up in a domain application where psych is getting the

2:02:33.920 --> 2:02:41.200
 right answer but taking too long, then what we'll often do is talk to one of the human experts and

2:02:41.200 --> 2:02:47.680
 say, here's the set of reasoning steps that psych went through, you can see why it took

2:02:47.680 --> 2:02:53.120
 it a long time to get the answer. How is it that you were able to answer that question in two seconds

2:02:53.120 --> 2:02:59.920
 and occasionally, you'll get an expert who just says, well, I just know it, I just was able to

2:02:59.920 --> 2:03:04.400
 do it or something. And then you don't talk to them anymore. But sometimes you'll get an expert

2:03:04.400 --> 2:03:11.520
 who says, well, let me introspect on that. Yes, here is a special representation we use just for

2:03:11.520 --> 2:03:17.920
 our aqueous chemistry equations, or here's a special representation and a special technique,

2:03:18.480 --> 2:03:24.000
 which we can now apply to things in this special representation and so on. And then you add that

2:03:24.000 --> 2:03:31.280
 as the thousand and first HL heuristic level module. And from then on, in any application,

2:03:31.280 --> 2:03:36.800
 if it ever comes up again, it'll be able to contribute and so on. So that that's pretty much

2:03:36.800 --> 2:03:45.040
 one of the main ways in which psych has recouped this lost efficiency. A second important way is

2:03:45.040 --> 2:03:53.280
 meta reasoning. So you can speed things up by focusing on removing knowledge from the system

2:03:53.280 --> 2:03:58.320
 till all it has left is like minimal knowledge needed to, but that's the wrong thing to do,

2:03:58.320 --> 2:04:02.400
 right? That would be like in a human extirpating part of their brain or something, that's really

2:04:02.400 --> 2:04:09.040
 bad. So instead, what you want to do is give it meta level advice, tactical and strategic advice

2:04:09.040 --> 2:04:15.520
 that enables it to reason about what kind of knowledge is going to be relevant to this problem,

2:04:15.520 --> 2:04:20.240
 what kind of tactics are going to be good to take in trying to attack this problem?

2:04:20.240 --> 2:04:25.200
 When is it time to start trying to prove the negation of this thing? Because I'm

2:04:25.200 --> 2:04:29.440
 knocking myself out trying to prove it's true and maybe it's false. And if I just spend a minute,

2:04:29.440 --> 2:04:36.560
 I can see that it's false or something. So it's like dynamically pruning the graph to only like

2:04:36.560 --> 2:04:44.240
 based on the particular thing you're trying to infer? Yes. And so by now, we have about 150

2:04:44.800 --> 2:04:51.680
 of these sort of like breakthrough ideas that have led to dramatic speed ups in the inference

2:04:51.680 --> 2:04:58.960
 process, where one of them was this ELHL split and lots of HL modules. Another one was using

2:04:58.960 --> 2:05:05.680
 meta and meta, meta level reasoning to reason about the reasoning that's going on and so on.

2:05:06.320 --> 2:05:12.320
 And 150 breakthroughs may sound like a lot, but if you divide by 37 years, it's not as impressive.

2:05:13.520 --> 2:05:18.560
 So there's these heuristic modules that really help improve the inference.

2:05:18.560 --> 2:05:28.720
 How hard in general is this, because you mentioned higher order logic. In the general,

2:05:28.720 --> 2:05:35.920
 the theorem prover sense, it's an intractable, very difficult problem. So how hard is this

2:05:35.920 --> 2:05:42.960
 inference problem when we're not talking about if we let go of the perfect and focus on the good?

2:05:42.960 --> 2:05:51.680
 I would say it's half of the problem in the following empirical sense, which is over the years,

2:05:52.480 --> 2:06:00.480
 about half of our effort, maybe 40% of our effort has been our team of inference programmers.

2:06:01.200 --> 2:06:07.200
 And the other 50, 60% has been our ontologists or ontological engineers putting in knowledge.

2:06:07.200 --> 2:06:12.320
 So our ontological engineers, in most cases, don't even know how to program. They have degrees in

2:06:12.320 --> 2:06:17.520
 things like philosophy and so on. So it's almost like... I love that. I'd love to hang out with

2:06:17.520 --> 2:06:22.400
 those people actually. Oh yes, it's wonderful. But it's very much like the Eloi and the Morlocks

2:06:22.400 --> 2:06:29.840
 in HG Wells Time Machine. So you have the Eloi who only program in the epistemological higher

2:06:29.840 --> 2:06:36.800
 order logic language. And then you have the Morlocks who are like, under the ground, figuring

2:06:36.800 --> 2:06:43.920
 out what the machinery is that will make this efficiently operate and so on. And so occasionally

2:06:43.920 --> 2:06:50.800
 they'll toss messages back to each other and so on. But it really is almost this 50, 50 split

2:06:50.800 --> 2:06:58.880
 between finding clever ways to recoup efficiency when you have an expressive language and putting

2:06:58.880 --> 2:07:04.640
 in the content of what the system needs to know. And yeah, both are fascinating. To some degree,

2:07:04.640 --> 2:07:12.240
 the entirety of the system, as far as I understand, is written in various variants of LISP. So my

2:07:12.240 --> 2:07:18.640
 favorite program language is still LISP. I don't program it in much anymore because the world has,

2:07:19.360 --> 2:07:26.160
 in majority of its system, has moved on. Like everybody respects LISP, but many of the systems

2:07:26.160 --> 2:07:31.840
 are not written in LISP anymore. But psych, as far as I understand, maybe you can correct me,

2:07:31.840 --> 2:07:38.400
 there's a bunch of LISP in it. Yeah. So it's based on LISP code that we produced. Most of the

2:07:38.400 --> 2:07:45.840
 programming is still going on in a dialect of LISP. And then for efficiency reasons, that gets

2:07:45.840 --> 2:07:53.520
 automatically translated into things like Java or C nowadays. It's almost all translated into Java

2:07:53.520 --> 2:07:59.200
 because Java has gotten good enough that that's really all we need to do. So it's translated

2:07:59.200 --> 2:08:10.880
 into Java and then Java is compiled down by code. Yes. Okay. So that's a process that probably

2:08:10.880 --> 2:08:15.680
 has to do with the fact that when psych was originally written and you built up a powerful

2:08:15.680 --> 2:08:21.120
 system, there is some technical depth you have to deal with, as is the case with most

2:08:21.120 --> 2:08:30.000
 powerful systems that span years. Have you ever considered, this would help me understand,

2:08:30.000 --> 2:08:37.120
 because from my perspective, so much of the value of everything you've done with psych and psych

2:08:37.120 --> 2:08:44.000
 op is the knowledge. Have you ever considered just like throwing away the code base and starting

2:08:44.000 --> 2:08:52.400
 from scratch, not really throwing away, but sort of moving it to like throwing away that

2:08:52.400 --> 2:08:58.640
 technical debt, starting with a more updated programming language. Is that throwing away

2:08:58.640 --> 2:09:03.840
 a lot of value or no? Like, what's your sense? How much of the value is in the silly software

2:09:03.840 --> 2:09:15.600
 engineering aspect and how much of the value is in the knowledge? So development of programs in Lisp

2:09:16.800 --> 2:09:26.480
 proceeds, I think, somewhere between 1,000 and 50,000 times faster than development in any of what

2:09:26.480 --> 2:09:31.120
 you're calling modern or improved computer languages. Well, there's other functional

2:09:31.120 --> 2:09:38.000
 language like Closure and all that. But I mean, I'm with you. I like Lisp. I just wonder how many

2:09:38.000 --> 2:09:43.280
 great programmers there are. They're still like... Yes. So it is true when a new inference

2:09:43.280 --> 2:09:50.720
 programmer comes on board, they need to learn some of Lisp. And in fact, we have a subset of Lisp,

2:09:50.720 --> 2:09:57.360
 which we call cleverly sub L, which is really all they need to learn. And so the programming

2:09:57.360 --> 2:10:04.080
 actually goes on in sub L, not in full Lisp. And so it does not take programmers very long at all to

2:10:04.080 --> 2:10:12.480
 learn sub L. And that's something which can then be translated efficiently into Java. And for some

2:10:12.480 --> 2:10:17.760
 of our programmers who are doing, say, user interface work, then they never have to even learn

2:10:17.760 --> 2:10:24.640
 sub L. They just have to learn APIs into the basic psych engine. So you're not necessarily

2:10:24.640 --> 2:10:30.720
 feeling the burden of, like, it's extremely efficient. That's not a problem to solve.

2:10:31.520 --> 2:10:37.600
 Right. The other thing is, remember that we're talking about hiring programmers to do inference

2:10:37.600 --> 2:10:43.600
 who are programmers interested in effectively automatic theorem proving. And so those are

2:10:43.600 --> 2:10:50.080
 people already predisposed to representing things in logic and so on. And Lisp really was

2:10:50.080 --> 2:10:57.840
 the programming language based on logic that John McCarthy and others who developed it

2:10:57.840 --> 2:11:04.560
 basically took the formalisms that Alonzo Church and other philosophers, other logicians,

2:11:05.280 --> 2:11:10.800
 had come up with and basically said, can we basically make a programming language,

2:11:10.800 --> 2:11:16.320
 which is effectively logic? And so since we're talking about reasoning

2:11:16.320 --> 2:11:23.600
 in, about expressions written in this logical epistemological language, and we're doing

2:11:23.600 --> 2:11:29.280
 operations which are effectively like theorem proving type operations and so on, there's a

2:11:29.280 --> 2:11:36.720
 natural impedance match between Lisp and the knowledge the way it's represented. So I guess

2:11:36.720 --> 2:11:45.200
 you could say it's a perfectly logical language to use. Oh, yes. Okay, I'm sorry. I'll even let you

2:11:45.200 --> 2:11:51.600
 get away with that. I appreciate it. So I'll probably use that in the future without. Without

2:11:51.600 --> 2:11:59.040
 credit. Without credit. But no, I think the point is that the language you program in

2:11:59.040 --> 2:12:05.920
 isn't really that important. It's more that you have to be able to think in terms of, for instance,

2:12:05.920 --> 2:12:13.040
 creating new helpful HL modules and how they'll work with each other and looking at things that

2:12:13.040 --> 2:12:19.440
 are taking a long time and coming up with new specialized data structures that will make this

2:12:19.440 --> 2:12:24.400
 efficient. So let me just give you one very simple example, which is when you have a

2:12:24.400 --> 2:12:29.360
 transitive relation, like larger than this is larger than that, which is larger than that,

2:12:29.360 --> 2:12:33.360
 which is larger than that. So the first thing must be larger than the last thing. Whenever

2:12:33.360 --> 2:12:39.440
 you have a transitive relation, if you're not careful, if I ask whether this thing over here

2:12:39.440 --> 2:12:45.120
 is larger than the thing over here, I'll have to do some kind of graph walk or theorem proving that

2:12:45.120 --> 2:12:52.960
 might involve like five or 10 or 20 or 30 steps. But if you store, redundantly, store the transitive

2:12:52.960 --> 2:12:59.120
 closure, the cleaning star of that transitive relation, now you have this big table, but you

2:12:59.120 --> 2:13:05.600
 can always guarantee that in one single step, you can just look up whether this is larger than that.

2:13:05.600 --> 2:13:14.000
 And so we, there are lots of cases where storage is cheap today. And so by having this extra

2:13:14.000 --> 2:13:20.480
 redundant data structure, we can answer this commonly occurring type of question very, very

2:13:20.480 --> 2:13:28.080
 efficiently. And let me give you one other analogy, analog of that, which is something we call rule

2:13:28.080 --> 2:13:36.080
 macro predicates, which is, we'll see this complicated rule. And we'll notice that things

2:13:36.080 --> 2:13:42.720
 very much like it syntactically come up again and again and again. So we'll create a whole brand

2:13:42.720 --> 2:13:50.160
 new relation or predicate or function that captures that and takes maybe not two arguments,

2:13:50.160 --> 2:14:00.240
 takes maybe three, four or five arguments and so on. And now we have effectively converted some

2:14:00.240 --> 2:14:07.040
 complicated if then rule that might have to have inference done on it into some ground atomic

2:14:07.040 --> 2:14:14.560
 formula, which is just a the name of a relation and a few arguments and so on. And so converting

2:14:14.560 --> 2:14:21.920
 commonly occurring types or schemas of rules into brand new predicates, brand new functions,

2:14:21.920 --> 2:14:28.880
 turns out to enormously speed up the inference process. So now we've covered about four of the

2:14:28.880 --> 2:14:34.480
 150 good ideas I said. So that's a nice, that's a cool, so that idea in particular is like a nice

2:14:34.480 --> 2:14:38.400
 compression that turns out to be really useful. That's really interesting. I mean, this whole

2:14:38.400 --> 2:14:43.200
 thing is just fascinating from a philosophical, there's part of me, I mean, it makes me a little

2:14:43.200 --> 2:14:50.640
 bit sad because your work is both from a computer science perspective, fascinating on the inference

2:14:50.640 --> 2:14:57.760
 engine from epistemological philosophical aspect, fascinating. But you know, it is also you're

2:14:57.760 --> 2:15:03.280
 running a company and there's some stuff that has to remain private. And it's sad.

2:15:03.280 --> 2:15:06.640
 Well, here's something that may make you feel better, a little bit better.

2:15:06.640 --> 2:15:13.360
 We're, we've formed a not not for profit company called the Knowledge

2:15:13.360 --> 2:15:20.960
 Activitization Institute, NACS, KNAX. And I have this firm belief with a lot of empirical

2:15:20.960 --> 2:15:28.720
 evidence to support it that the, the education that people get in high schools and colleges

2:15:28.720 --> 2:15:35.360
 and graduate schools and so on is almost completely orthogonal to almost completely

2:15:35.360 --> 2:15:42.560
 irrelevant to how good they're going to be at coming up to speed in doing this kind of

2:15:43.120 --> 2:15:49.280
 ontological engineering and writing these assertions and rules and so on in, in psych.

2:15:49.280 --> 2:15:55.520
 And so very often we'll interview candidates who have their PhD in philosophy who've taught logic

2:15:55.520 --> 2:16:01.040
 for years and so on. And they're just, they're just awful. But the converse is true. So one of

2:16:01.040 --> 2:16:07.520
 the best ontological engineers we ever had never graduated high school. And so the purpose of

2:16:08.720 --> 2:16:14.720
 Knowledge Activitization Institute, if we can get some, some foundations to help support it, is

2:16:15.280 --> 2:16:22.000
 identify people in the general population, maybe high school dropouts, who have latent talent

2:16:22.720 --> 2:16:30.480
 for this sort of thing, offer them effectively scholarships to train them and then help place

2:16:30.480 --> 2:16:36.400
 them in companies that need more trained ontological engineers, some of which would be working for

2:16:36.400 --> 2:16:41.920
 us, but mostly would be working for partners or customers or something. And if we could do that,

2:16:41.920 --> 2:16:48.880
 that would create an enormous number of relatively very high paying jobs for people who currently

2:16:48.880 --> 2:16:54.880
 have no, no way out of some, you know, situation that they're locked into.

2:16:54.880 --> 2:17:01.040
 So is there something you can put into words that describes somebody who would be great

2:17:01.040 --> 2:17:08.480
 at ontological engineering? So what characteristics about a person make them great at this task?

2:17:08.480 --> 2:17:17.040
 This task of converting the messiness of human language and knowledge into formal logic.

2:17:17.040 --> 2:17:22.720
 This is very much like what Alan Turing had to do during World War II in trying to find

2:17:22.720 --> 2:17:28.400
 people to bring to Bletchley Park where he would publish in the London Times cryptic

2:17:28.400 --> 2:17:34.720
 crossword puzzles along with some, some innocuous looking note, which essentially said, if you

2:17:34.720 --> 2:17:41.200
 were able to solve this puzzle in less than 15 minutes, please call this phone number and so on.

2:17:41.200 --> 2:17:49.280
 So, you know, or back when I was young, there was the practice of having a matchbooks where on

2:17:49.280 --> 2:17:55.360
 the inside of the matchbook, there would be a, can you draw this? You have a career in art,

2:17:56.000 --> 2:18:02.320
 commercial art, if you can copy this drawing and so on. So yes, the analog of that.

2:18:02.320 --> 2:18:05.840
 Was there a little test to get to the core of whether it could be good or not?

2:18:05.840 --> 2:18:13.440
 So part of it has to do with being able to make and appreciate and react negatively

2:18:13.440 --> 2:18:18.400
 appropriately to puns and other jokes. So you have to have a kind of sense of humor.

2:18:18.400 --> 2:18:25.200
 And if you're good at telling jokes and good at understanding jokes, that's one indicator.

2:18:25.200 --> 2:18:26.960
 Like puns? Like dad jokes?

2:18:26.960 --> 2:18:30.560
 Yes. Well, maybe not dad jokes, but real, but funny jokes.

2:18:32.560 --> 2:18:34.240
 I think I'm applying to work as sacro.

2:18:34.240 --> 2:18:42.240
 Yeah, but another is if you're able to introspect. So very often we'll give someone a

2:18:42.240 --> 2:18:50.240
 simple question and we'll say like, why is this? And, you know, sometimes they'll just say,

2:18:50.240 --> 2:18:56.400
 because it is, okay, that's a bad sign. But very often they'll be able to introspect and so on.

2:18:56.400 --> 2:19:02.080
 So one of the questions I often ask is I'll point to a sentence with a pronoun in it and I'll say,

2:19:02.800 --> 2:19:06.160
 you know, the referend of that pronoun is obviously this noun over here.

2:19:06.160 --> 2:19:14.240
 You know, how would you or I or an AI or a five year old, 10 year old child know that that pronoun

2:19:14.240 --> 2:19:22.960
 refers to that noun over here? And often the people who are going to be good at ontological

2:19:22.960 --> 2:19:28.400
 engineering will give me some causal explanation or will refer to some things that are true in

2:19:28.400 --> 2:19:33.760
 the world. So if you imagine a sentence like the horse was led into the barn while its head

2:19:33.760 --> 2:19:40.080
 while its head was still wet. And so its head refers to the horse's head. But how do you know that?

2:19:40.080 --> 2:19:44.160
 And so some people will say, I just know it, some people will say, well, the horse was the subject

2:19:44.160 --> 2:19:49.040
 of the sentence. And I'll say, okay, well, what about the horse was led into the barn while its

2:19:49.040 --> 2:19:56.160
 roof was still wet? Now its roof obviously refers to the barn. And so then they'll say, oh, well,

2:19:56.160 --> 2:20:01.680
 that's because it's the closest noun and so on. So basically, if they try to give me answers,

2:20:01.680 --> 2:20:08.160
 which are based on syntax and grammar and so on, that's a really bad sign. But if they're able to

2:20:08.160 --> 2:20:13.520
 say things like, well, horses have heads and barns don't and barns have roofs and horses don't,

2:20:14.240 --> 2:20:17.760
 then that's a positive sign that they're going to be good at this because they can

2:20:17.760 --> 2:20:22.560
 introspect on what's true in the world that leads you to know certain things.

2:20:22.560 --> 2:20:28.480
 How fascinating is it that getting a PhD makes you less capable to introspect deeply about this?

2:20:28.480 --> 2:20:33.360
 Oh, I wouldn't go that far. I'm not saying that it makes you less capable. Let's just say it's

2:20:33.360 --> 2:20:39.200
 independent of how good people are. Okay, you're not saying that. I'm saying that. There's a

2:20:39.200 --> 2:20:46.800
 certain, it's interesting that for a lot of people, PhDs, sorry, philosophy aside,

2:20:46.800 --> 2:20:53.280
 that sometimes education narrows your thinking versus expands it. It's kind of fascinating.

2:20:53.280 --> 2:20:58.720
 And for certain, when you're trying to do ontological engineering, which is essentially teach

2:20:58.720 --> 2:21:04.480
 our future AI overlords how to reason deeply about this world and how to understand it,

2:21:05.200 --> 2:21:08.480
 that requires that you think deeply about the world.

2:21:08.480 --> 2:21:14.640
 So I'll tell you a sad story about mathcraft, which is why is that not widely used in schools

2:21:14.640 --> 2:21:21.440
 today? We're not really trying to make big profit on it or anything like that. But when we've gone

2:21:21.440 --> 2:21:27.920
 to schools, their attitude has been, well, if a student spends 20 hours going through this

2:21:27.920 --> 2:21:36.800
 mathcraft program from start to end and so on, will it improve their score on this standardized test

2:21:36.800 --> 2:21:42.880
 more than if they spent 20 hours just doing mindless drills of problem after problem after

2:21:42.880 --> 2:21:47.840
 problem? And the answer is, well, no, but it'll increase their understanding more and their

2:21:47.840 --> 2:21:55.120
 attitude is, well, if it doesn't increase their score on this test, then we're not going to adopt it.

2:21:55.840 --> 2:22:01.680
 That's sad. I mean, that's a whole another three, four hour conversation about the education system.

2:22:01.680 --> 2:22:08.560
 But let me ask you, let me go super philosophical as if we weren't already. So in 1950, Alan Turing

2:22:08.560 --> 2:22:13.760
 wrote the paper that formulated the Turing test. And he opened the paper with the question,

2:22:13.760 --> 2:22:20.320
 can machines think? So what do you think? Can machines think? Let me ask you this question.

2:22:20.320 --> 2:22:28.400
 Absolutely. Machines can think certainly as well as humans can think, right? We're

2:22:28.400 --> 2:22:34.960
 meat machines. Just because they're not currently made out of meat is just an engineering solution

2:22:34.960 --> 2:22:43.600
 decision and so on. So of course, machines can think. I think that there was a lot of

2:22:46.080 --> 2:22:59.680
 damage done by people misunderstanding Turing's imitation game and focus on trying to get a chat

2:22:59.680 --> 2:23:08.160
 bot to fool other people into thinking it was human and so on. That's not a terrible test in

2:23:08.160 --> 2:23:14.000
 and of itself, but it shouldn't be your one and only test for intelligence. So in terms of tests

2:23:14.000 --> 2:23:23.360
 of intelligence, with the Lobner Prize, which is a more strict formulation of the Turing test

2:23:23.360 --> 2:23:30.320
 as originally formulated, and then there's something like Alexa Prize, which is more, I would say,

2:23:30.320 --> 2:23:37.360
 a more interesting formulation of the test, which is ultimately the metric is how long does a human

2:23:37.360 --> 2:23:44.400
 want to talk to the AI system? So the goal is you want it to be 20 minutes. It's basically

2:23:45.200 --> 2:23:51.920
 not just have a convincing conversation, but more like a compelling one or a fun one or an

2:23:51.920 --> 2:24:00.880
 interesting one. That seems like more to the spirit, maybe, of what Turing was imagining.

2:24:00.880 --> 2:24:07.920
 But what for you do you think in the space of tests is a good test? When you see a system

2:24:07.920 --> 2:24:14.240
 based on psych that passes that test, you'd be like, damn, we've created something special here.

2:24:14.240 --> 2:24:23.440
 The test has to be something involving depth of reasoning and recursiveness of reasoning,

2:24:23.440 --> 2:24:28.560
 the ability to answer repeated why questions about the answer you just gave.

2:24:29.840 --> 2:24:32.720
 It's how many why questions in a row can you keep answering?

2:24:32.720 --> 2:24:33.600
 Something like that.

2:24:36.160 --> 2:24:42.560
 Just have a young, curious child and an AI system, and how long will an AI system last before it

2:24:42.560 --> 2:24:48.000
 wants to quit? Again, that's not the only test. Another one has to do with argumentation. In

2:24:48.000 --> 2:24:58.480
 other words, here's a proposition. Come up with pro and con arguments for it, and try and give me

2:24:58.480 --> 2:25:08.000
 convincing arguments on both sides. That's another important kind of ability that the system needs

2:25:08.000 --> 2:25:14.720
 to be able to exhibit in order to really be intelligent, I think. There's certain, if you

2:25:14.720 --> 2:25:20.880
 look at IBM Watson and certain impressive accomplishments for a very specific test,

2:25:20.880 --> 2:25:35.280
 almost like a demo. I talked to the guy who led the Jeopardy effort. There's some kind of hard

2:25:35.280 --> 2:25:41.200
 coding heuristics tricks that you try to pull it all together to make the thing work in the end

2:25:41.200 --> 2:25:48.320
 for this thing. That seems to be one of the lessons with AI is that's the fastest way to

2:25:48.320 --> 2:25:56.320
 get a solution that's pretty damn impressive. Here's what I would say is that as impressive as

2:25:56.320 --> 2:26:02.880
 that was, it made some mistakes. But more importantly, many of the mistakes it made

2:26:02.880 --> 2:26:14.880
 were mistakes which no human would have made. Part of the new or augmented touring tests

2:26:14.880 --> 2:26:22.160
 would have to be, and the mistakes you make are ones which humans don't basically look at and

2:26:22.160 --> 2:26:35.280
 say what. For example, there was a question about which 16th century Italian politician and Watson

2:26:35.280 --> 2:26:41.120
 said Ronald Reagan. Most Americans would have gotten that question wrong, but they would never

2:26:41.120 --> 2:26:48.640
 have said Ronald Reagan as an answer because among the things they know is that he lived

2:26:48.640 --> 2:26:54.960
 relatively recently and people don't really live 400 years and things like that. That's

2:26:54.960 --> 2:27:02.720
 I think a very important thing which is if it's making mistakes which no normal sane human would

2:27:02.720 --> 2:27:08.960
 have made, then that's a really bad sign. If it's not making those kinds of mistakes, then that's

2:27:08.960 --> 2:27:14.320
 a good sign. I don't think it's any one very, very simple test. I think it's all of the things you

2:27:14.320 --> 2:27:19.920
 mentioned, all the things I mentioned. There's really a battery of tests which together, if it

2:27:19.920 --> 2:27:25.840
 passes almost all of these tests, it would be hard to argue that it's not intelligent. If it

2:27:25.840 --> 2:27:31.520
 fails several of these tests, it's really hard to argue that it really understands what it's doing

2:27:31.520 --> 2:27:37.280
 and that it really is generally intelligent. To pass all of those tests, we've talked a lot

2:27:37.280 --> 2:27:44.720
 about psych and knowledge and reasoning. Do you think this AI system would need to have some other

2:27:44.720 --> 2:27:53.440
 human like elements? For example, a body or a physical manifestation in this world and another

2:27:53.440 --> 2:28:00.640
 one which seems to be fundamental to the human experience is consciousness. The subjective

2:28:00.640 --> 2:28:06.960
 experience of what it's like to actually be you. Do you think he needs those to be able to pass all

2:28:06.960 --> 2:28:10.960
 of those tests and to achieve general intelligence? It's a good question. I think in the case of a

2:28:10.960 --> 2:28:19.360
 body, no, I know there are a lot of people like Penrose who would have disagreed with me and others,

2:28:19.360 --> 2:28:25.520
 but no, I don't think it needs to have a body in order to be intelligent. I think that it needs

2:28:25.520 --> 2:28:34.240
 to be able to talk about having a body and having sensations and having emotions and so on. It doesn't

2:28:34.240 --> 2:28:40.720
 actually have to have all of that, but it has to understand it in the same way that Helen Keller

2:28:40.720 --> 2:28:48.720
 was perfectly intelligent and able to talk about colors and sounds and shapes and so on,

2:28:49.280 --> 2:28:56.720
 even though she didn't directly experience all the same things that the rest of us do. Knowledge

2:28:56.720 --> 2:29:04.880
 of it and being able to correctly make use of that is certainly an important facility,

2:29:04.880 --> 2:29:10.720
 but actually having a body, if you believe that that's just a kind of religious or mystical

2:29:10.720 --> 2:29:16.960
 belief, you can't really argue for or against it, I suppose. It's just something that some people

2:29:18.240 --> 2:29:26.320
 believe. What about an extension of the body which is consciousness? It feels like something

2:29:26.320 --> 2:29:33.040
 to be here. Sure, but what does that really mean? It's like, well, if I talk to you, you say things

2:29:33.040 --> 2:29:38.560
 which make me believe that you're conscious. I know that I'm conscious, but you're just taking

2:29:38.560 --> 2:29:44.720
 my word for it now. But in the same sense, psych is conscious in that same sense already where,

2:29:44.720 --> 2:29:49.360
 of course, it understands. It's a computer program. It understands where and when it's

2:29:49.360 --> 2:29:54.000
 running. It understands who's talking to it. It understands what its task is, what its goals

2:29:54.000 --> 2:29:58.560
 are, what its current problem is that it's working on. It understands how long it's spent on things,

2:29:58.560 --> 2:30:07.440
 what it's tried. It understands what it's done in the past and so on. If we want to call that

2:30:07.440 --> 2:30:13.600
 consciousness, then yes, psych is already conscious, but I don't think that I would describe anything

2:30:14.400 --> 2:30:20.640
 mystical to that. Again, some people would, but I would say that other than our own personal

2:30:20.640 --> 2:30:27.520
 experience of consciousness, we're just treating everyone else in the world, so to speak, at their

2:30:27.520 --> 2:30:37.200
 word about being conscious. If a computer program, if an AI is able to exhibit all the same kinds of

2:30:38.720 --> 2:30:45.600
 response as you would expect of a conscious entity, then doesn't it deserve the label of

2:30:45.600 --> 2:30:50.320
 consciousness just as much? There's another burden that comes with this whole intelligence

2:30:50.320 --> 2:30:57.760
 thing that humans got is the extinguishing of the light of consciousness, which is

2:30:58.560 --> 2:31:04.080
 kind of realizing that we're going to be dead someday. There's a bunch of philosophers like

2:31:04.080 --> 2:31:12.640
 Ernest Becker who kind of think that this realization of mortality and then fear, sometimes

2:31:12.640 --> 2:31:22.640
 they call it terror of mortality is one of the creative forces behind human condition.

2:31:23.280 --> 2:31:27.200
 It's the thing that drives us. Do you think it's important for an AI system?

2:31:29.040 --> 2:31:37.440
 When Psyche proposed that it's not human and it's one of the moderators of his contents,

2:31:37.440 --> 2:31:44.560
 you know, there's another question it could ask, which is like it kind of knows that humans are

2:31:44.560 --> 2:31:52.640
 mortal. Am I mortal? And I think one really important thing that's possible when you're

2:31:52.640 --> 2:31:59.120
 conscious is to fear the extinguishing of that consciousness, the fear of mortality. Do you

2:31:59.120 --> 2:32:05.120
 think that's useful for intelligence? Thinking like I might die and I really don't want to die?

2:32:05.120 --> 2:32:13.520
 I don't think so. I think it may help some humans to be better people. It may help some

2:32:13.520 --> 2:32:21.440
 humans to be more creative and so on. I don't think it's necessary for AIs to believe that

2:32:21.440 --> 2:32:26.000
 they have limited life spans and therefore they should make the most of their behavior. Maybe

2:32:26.000 --> 2:32:31.680
 eventually the answer to that and my answer to that will change, but as of now I would say that

2:32:31.680 --> 2:32:38.080
 that's almost like a frill or a side effect that is not. In fact, if you look at most humans,

2:32:38.640 --> 2:32:43.120
 most humans ignore the fact that they're going to die most of the time.

2:32:44.800 --> 2:32:50.720
 Well, but that's like this goes to the white space between the words. So what Ernest Becker

2:32:50.720 --> 2:32:56.400
 argues is that that ignoring is reliving an illusion that we constructed on the foundation

2:32:56.400 --> 2:33:03.840
 of this terror. So we escape life as we know it, pursuing things, creating things, love.

2:33:04.640 --> 2:33:10.800
 Everything we can think of that's beautiful about humanity is just trying to escape this

2:33:10.800 --> 2:33:17.520
 realization of going to die one day. That's his idea and I think, I don't know if I 100%

2:33:18.880 --> 2:33:25.120
 believe in this, but it certainly rhymes. It seems like to me like it rhymes with the truth.

2:33:25.120 --> 2:33:32.880
 Yeah, I think that for some people, that's going to be a more powerful factor than others.

2:33:32.880 --> 2:33:34.560
 Clearly Doug is talking about Russians.

2:33:35.440 --> 2:33:36.880
 And I think that

2:33:39.600 --> 2:33:44.320
 some Russians, clearly it infiltrates all of Russian literature.

2:33:44.320 --> 2:33:55.520
 And AI doesn't have to have fear of death as a motivating force in that we can build in motivation.

2:33:55.520 --> 2:34:03.200
 So we can build in the motivation of obeying users and making users happy and making others

2:34:03.200 --> 2:34:12.400
 happy and so on. And that can substitute for this sort of personal fear of death that sometimes

2:34:12.400 --> 2:34:20.080
 leads to bursts of creativity in humans. Yeah, I don't know. I think AI really

2:34:20.080 --> 2:34:23.840
 needs to understand death deeply in order to be able to drive a car, for example.

2:34:24.800 --> 2:34:31.760
 I think there's just some... No, I really disagree. I think it needs to understand

2:34:31.760 --> 2:34:35.680
 the value of human life, especially the value of human life to other humans,

2:34:35.680 --> 2:34:42.000
 and understand that certain things are more important than other things.

2:34:42.000 --> 2:34:48.000
 So it has to have a lot of knowledge about ethics and morality and so on.

2:34:48.000 --> 2:34:51.600
 But some of it is so messy that it's impossible to encode, for example.

2:34:51.600 --> 2:34:53.040
 I disagree.

2:34:53.600 --> 2:34:59.120
 So if there's a person dying right in front of us, most human beings would help that person,

2:34:59.120 --> 2:35:03.600
 but they would not apply that same ethics to everybody else in the world.

2:35:03.600 --> 2:35:07.360
 I mean, this is the tragedy of how difficult it is to be a doctor,

2:35:07.920 --> 2:35:13.600
 because they know when they help a dying child, they know that the money they're spending on this

2:35:13.600 --> 2:35:21.120
 child cannot possibly be spent on every other child that's dying. And that's a very difficult

2:35:21.120 --> 2:35:27.600
 to encode decision. Now, perhaps it could be formalized.

2:35:27.600 --> 2:35:33.760
 Oh, but I mean, you're talking about autonomous vehicles, right? So autonomous vehicles are

2:35:33.760 --> 2:35:41.680
 going to have to make those decisions all the time of what is the chance of this bad event

2:35:41.680 --> 2:35:47.280
 happening? How bad is that compared to this chance of that bad event happening and so on?

2:35:47.280 --> 2:35:53.440
 And when a potential accident is about to happen, is it worth taking this risk if I have to make

2:35:53.440 --> 2:35:56.720
 a choice? Which of these two cars am I going to hit and why?

2:35:56.720 --> 2:36:00.960
 See, I was thinking about a very different choice when I'm talking about your mortality,

2:36:00.960 --> 2:36:09.600
 which is just observing Manhattan style driving. I think that humans, as an effective driver,

2:36:09.600 --> 2:36:17.040
 needs to threaten pedestrians lives a lot. There's a dance, I've watched pedestrians a lot,

2:36:17.040 --> 2:36:24.160
 I worked on this problem. And it seems like if I could summarize the problem of a pedestrian

2:36:24.160 --> 2:36:31.280
 crossing is the car with this movement is saying, I'm going to kill you. And the pedestrian is saying,

2:36:32.400 --> 2:36:36.640
 maybe, and then they decide and they say, no, I don't think you have the guts to kill me. And

2:36:37.200 --> 2:36:42.800
 they walk in front and they look away. And there's that dance, the pedestrian,

2:36:42.800 --> 2:36:47.680
 as this is social contract, that the pedestrian trusts that once they're in front of the car

2:36:47.680 --> 2:36:52.960
 and the car is sufficiently from a physics perspective able to stop, they're going to stop.

2:36:52.960 --> 2:36:58.640
 But the car also has to threaten that pedestrian is like, I'm late for work. So you're being kind

2:36:58.640 --> 2:37:04.320
 of an asshole by crossing in front of me. But life and death is in like, it's part of the

2:37:04.320 --> 2:37:10.400
 calculation here. And it's that that equation is being solved millions of times a day.

2:37:11.440 --> 2:37:16.720
 Yes, very effectively that game theory, whatever, whatever that formulation is, I just, I don't

2:37:16.720 --> 2:37:23.280
 know if it's as simple as some formalizable game theory problem. It could very well be

2:37:23.280 --> 2:37:28.800
 in the case of driving and in the case of most of human society. I don't know. But

2:37:29.920 --> 2:37:34.080
 yeah, you might be right that sort of the fear of death is just one of the quirks

2:37:34.640 --> 2:37:40.560
 of like the way our brains have evolved. But it's not, it's not a necessary feature of

2:37:40.560 --> 2:37:47.440
 of intelligence. Drivers certainly are always doing this kind of estimate, even if it's unconscious,

2:37:47.440 --> 2:37:53.520
 subconscious, of what are the chances of various bad outcomes happening? Like for instance,

2:37:54.240 --> 2:38:00.640
 if I don't wait for this pedestrian or something like that. And what is the downside to me going

2:38:00.640 --> 2:38:09.920
 to be in terms of time wasted talking to the police or getting sent to jail or things like

2:38:09.920 --> 2:38:16.240
 that. And so... And there's also emotion, like people in their cars tend to get irrationally

2:38:16.240 --> 2:38:22.880
 angry. That's dangerous. But think about, this is all part of why I think that autonomous vehicles,

2:38:23.840 --> 2:38:31.120
 truly autonomous vehicles are farther out than most people do, because there is this enormous

2:38:31.120 --> 2:38:42.240
 level of complexity which goes beyond mechanically controlling the car. And I can see the autonomous

2:38:42.240 --> 2:38:48.080
 vehicles as a kind of metaphorical and literal accident waiting to happen. And not just because

2:38:48.080 --> 2:38:58.320
 of their overall incurring versus preventing accidents and so on, but just because of the

2:38:58.320 --> 2:39:11.520
 almost voracious appetite people have for bad stories about powerful companies and powerful

2:39:11.520 --> 2:39:19.040
 entities. When I was at a, coincidentally, Japanese fifth generation computing system

2:39:19.040 --> 2:39:25.440
 conference in 1987, while I happened to be there, there was a worker at an auto plant who was

2:39:25.440 --> 2:39:30.880
 despondent and committed suicide by climbing under the safety chains and so on and getting stamped

2:39:30.880 --> 2:39:37.040
 to death by a machine. And instead of being a small story that said despondent worker commits

2:39:37.040 --> 2:39:45.200
 suicide, it was front page news that effectively said robot kills worker because the public is

2:39:45.200 --> 2:39:54.240
 just waiting for stories about like AI kills phonogenic family of five type stories. And

2:39:54.240 --> 2:40:01.920
 even if you could show that nationwide, this system saved more lives than it cost and saved

2:40:01.920 --> 2:40:07.920
 more injuries, prevented more injuries than it caused and so on. The media, the public,

2:40:07.920 --> 2:40:16.720
 the government is just coiled and ready to pounce on stories where in fact it failed,

2:40:16.720 --> 2:40:24.960
 even if there are relatively few. Yeah, it's so fascinating to watch us humans resisting the

2:40:24.960 --> 2:40:30.240
 cutting edge of science and technology and almost like hoping for it to fail and constantly, you

2:40:30.240 --> 2:40:34.720
 know, this just happens over and over and over throughout history. Or even if we're not hoping

2:40:34.720 --> 2:40:41.760
 for it to fail, we're fascinated by it. And in terms of what we find interesting, the one in

2:40:41.760 --> 2:40:49.520
 a thousand failures much more interesting than the 999 boring successes. So once we build an

2:40:49.520 --> 2:40:58.640
 AGI system, say psych is some part of some part of it, and say it's very possible that you would be

2:40:58.640 --> 2:41:05.040
 one of the first people that can sit down in the room, let's say with her and have a conversation,

2:41:05.040 --> 2:41:11.280
 what would you ask her? What would you talk about? Looking at all of the

2:41:14.160 --> 2:41:22.080
 content out there on the web and so on. What are the,

2:41:25.040 --> 2:41:30.080
 what are some possible solutions to big problems that the world has that

2:41:30.080 --> 2:41:37.200
 people haven't really thought of before that are not being properly or at least adequately

2:41:38.400 --> 2:41:46.320
 pursued? What are some novel solutions that you can think of that we haven't that might work

2:41:46.320 --> 2:41:52.240
 and that might be worth considering? So that is a damn good question. Given that the AGI is going

2:41:52.240 --> 2:41:58.000
 to be somewhat different from human intelligence, it's still going to make some mistakes that we

2:41:58.000 --> 2:42:05.680
 wouldn't make, but it's also possibly going to notice some blind spots we have. And I would

2:42:05.680 --> 2:42:14.480
 love it as a test of is it really on a par with our intelligence is can it help spot some of the

2:42:14.480 --> 2:42:22.160
 blind spots that we have? So the two part question of can you help identify what are the big problems

2:42:22.160 --> 2:42:28.080
 in the world and two, what are some novel solutions to those problems that are not being

2:42:28.640 --> 2:42:36.400
 talked about by anyone? And some of those may become infeasible or reprehensible or something,

2:42:36.400 --> 2:42:41.840
 but some of them might be actually great things to look at. If you go back and look at some of the

2:42:41.840 --> 2:42:50.400
 most powerful discoveries that have been made like relativity and superconductivity and so on,

2:42:50.400 --> 2:42:59.360
 a lot of them were cases where someone took seriously the idea that there might actually be

2:43:01.200 --> 2:43:08.320
 a nonobvious answer to a question. So in Einstein's case, it was, yeah, the Lorenz transformation is

2:43:08.320 --> 2:43:13.360
 known. Nobody believes that it's actually the way reality works. What if it were the way that

2:43:13.360 --> 2:43:18.400
 reality actually worked? So a lot of people don't realize he didn't actually work out that equation,

2:43:18.400 --> 2:43:24.880
 he just sort of took it seriously. Or in the case of superconductivity, you have this V equals IR

2:43:24.880 --> 2:43:32.400
 equation where R is resistance and so on. And it was being mapped at lower and lower temperatures,

2:43:32.400 --> 2:43:38.880
 but everyone thought that was just bump on a log research to show that V equals IR always held.

2:43:39.680 --> 2:43:46.160
 And then when some graduate student got to a slightly lower temperature and showed that

2:43:46.160 --> 2:43:51.760
 resistance suddenly dropped off, everyone just assumed that they did it wrong. And it was only

2:43:51.760 --> 2:43:57.600
 a little while later that they realized it was actually a new phenomenon. Or in the case of

2:43:59.040 --> 2:44:05.760
 the H. pylori bacteria causing stomach ulcers, where everyone thought that stress and stomach

2:44:05.760 --> 2:44:14.720
 acid caused ulcers. And when a doctor in Australia claimed it was actually a bacterial infection,

2:44:14.720 --> 2:44:20.800
 he couldn't get anyone seriously to listen to him. And he had to ultimately inject himself

2:44:21.440 --> 2:44:25.840
 with the bacteria to show that he suddenly developed a life threatening ulcer

2:44:26.560 --> 2:44:31.520
 in order to get other doctors to seriously consider that. So there are all sorts of things where

2:44:33.040 --> 2:44:39.360
 humans are locked into paradigms, what Thomas Kuhn called paradigms. And we can't get out of them

2:44:39.360 --> 2:44:46.400
 very easily. So a lot of AI is locked into the deep learning machine learning paradigm right now.

2:44:47.360 --> 2:44:54.240
 And almost all of us and almost all sciences are locked into current paradigms. And Kuhn's point

2:44:54.240 --> 2:45:01.760
 was pretty much you have to wait for people to die in order for the new generation to escape

2:45:01.760 --> 2:45:07.840
 those paradigms. And I think that one of the things that would change that sad reality is if we had

2:45:07.840 --> 2:45:15.440
 trusted AGI's that could help take a step back and question some of the paradigms that we're

2:45:15.440 --> 2:45:22.480
 currently locked into. Yeah, it would accelerate the paradigm shifts in human science and progress.

2:45:24.080 --> 2:45:30.320
 You've lived a very interesting life where you thought about big ideas and you stuck with them.

2:45:31.600 --> 2:45:36.800
 Can you give advice to young people today, somebody in high school, somebody undergrad,

2:45:36.800 --> 2:45:45.600
 undergrad about career, about life? I'd say you can make a difference.

2:45:47.600 --> 2:45:50.560
 But in order to make a difference, you're going to have to have the courage

2:45:51.520 --> 2:45:59.760
 to follow through with ideas which other people might not immediately understand or

2:45:59.760 --> 2:46:12.560
 support. You have to realize that if you make some plan that's going to take an extended

2:46:12.560 --> 2:46:20.240
 period of time to carry out, don't be afraid of that. That's true of physical training of your

2:46:20.240 --> 2:46:29.760
 body. That's true of learning some profession. That's also true of innovation, that some

2:46:29.760 --> 2:46:37.120
 innovations are not great ideas you can write down on a napkin and become an instant success

2:46:37.120 --> 2:46:44.880
 if you turn out to be right. Some of them are paths you have to follow. But remember that you're

2:46:44.880 --> 2:46:52.640
 mortal. Remember that you have a limited number of decade sized bets to make with your life.

2:46:53.520 --> 2:46:59.280
 You should make each one of them count. That's true in personal relationships. That's true in

2:46:59.280 --> 2:47:05.920
 career choice. That's true in making discoveries and so on. If you follow the path of least

2:47:05.920 --> 2:47:13.520
 resistance, you'll find that you're optimizing for short periods of time. Before you know it,

2:47:13.520 --> 2:47:18.800
 you turn around and long periods of time have gone by without you ever really making a difference

2:47:18.800 --> 2:47:26.080
 in the world. When you look at the field that I really love is artificial intelligence. There's

2:47:26.080 --> 2:47:33.440
 not many projects. There's not many little flames of hope that have been carried out for many years,

2:47:33.440 --> 2:47:42.080
 for decades. Psyche represents one of them. That in itself is just a really inspiring thing.

2:47:42.080 --> 2:47:48.080
 I'm deeply grateful that you would be carrying that flame for so many years. I think that's an

2:47:48.080 --> 2:47:53.440
 inspiration to young people. That said, you said life is finite. We talked about mortality as a

2:47:53.440 --> 2:47:58.240
 feature of AGI. Do you think about your own mortality? Are you afraid of death?

2:48:00.160 --> 2:48:08.400
 Sure. I'd be crazy if I weren't. As I get older, I'm now over 70. As I get older,

2:48:08.400 --> 2:48:18.000
 it's more on my mind, especially as acquaintances and friends and especially mentors one by one

2:48:18.000 --> 2:48:26.640
 are dying. I can't avoid thinking about mortality. I think that the good news from the point of you

2:48:26.640 --> 2:48:33.360
 and the rest of the world is that that adds impetus to my need to succeed in a small number

2:48:33.360 --> 2:48:39.520
 of years in the future. You have a deadline. Exactly. I'm not going to have another 37 years

2:48:39.520 --> 2:48:46.960
 to continue working on this. We really do want to make an impact in the world commercially,

2:48:47.760 --> 2:48:53.680
 physically, metaphysically in the next small number of years, two, three, five years,

2:48:53.680 --> 2:49:02.640
 not two, three, five decades anymore. This is really driving me toward this commercialization

2:49:02.640 --> 2:49:11.520
 and increasingly widespread application of psych. Whereas before, I felt that I could just sit back,

2:49:11.520 --> 2:49:17.520
 roll my eyes, wait till the world caught up. Now I don't feel that way anymore. I feel like I need

2:49:17.520 --> 2:49:24.320
 to put in some effort to make the world aware of what we have and what it can do. The good news

2:49:24.320 --> 2:49:28.400
 from your point of view is that that's why I'm sitting here. You're going to be more productive.

2:49:28.400 --> 2:49:37.280
 I love it. If I can help in any way, I would love to. From a programmer perspective,

2:49:37.920 --> 2:49:44.800
 I love, especially these days, just contributing in small and big ways. If there's any open sourcing

2:49:44.800 --> 2:49:51.600
 from an MIT side and the research, I would love to help. But bigger than psych, like I said,

2:49:51.600 --> 2:49:55.440
 it's that little flame that you're carrying of artificial intelligence, the big dream.

2:49:55.440 --> 2:49:59.680
 What do you hope your legacy is?

2:50:02.000 --> 2:50:11.200
 That's a good question. That people think of me as one of the pioneers or inventors of

2:50:13.200 --> 2:50:20.320
 the AI that is ubiquitous and that they take for granted. And so much the way that

2:50:20.320 --> 2:50:30.480
 today, we look back on the pioneers of electricity or the pioneers of similar types of technologies

2:50:30.480 --> 2:50:39.760
 and so on. It's hard to imagine what life would be like if these people hadn't done what they did.

2:50:39.760 --> 2:50:44.800
 So that's one thing that I'd like to be remembered as. Another is that...

2:50:44.800 --> 2:50:54.240
 So the creator, one of the originators of this gigantic knowledge store and acquisition system

2:50:54.240 --> 2:51:00.320
 that is likely to be at the center of whatever this future AI thing will look like.

2:51:00.320 --> 2:51:10.080
 Yes, exactly. And I'd also like to be remembered as someone who wasn't afraid to spend several

2:51:10.080 --> 2:51:22.720
 decades on a project in a time when almost all of the other forces, institutional forces and

2:51:23.280 --> 2:51:29.680
 commercial forces, are incenting people to go for short term rewards.

2:51:29.680 --> 2:51:36.080
 And a lot of people gave up. A lot of people that dreamt the same dream as you gave up.

2:51:36.080 --> 2:51:44.000
 Yes. And you didn't. Yes. I mean, Doug, it's truly an honor. This was a long time coming.

2:51:45.120 --> 2:51:53.360
 A lot of people bring up your work specifically and more broadly, philosophically, of this is the

2:51:53.360 --> 2:51:59.600
 dream of artificial intelligence. This is likely a part of the future. We're so focused on machine

2:51:59.600 --> 2:52:04.160
 learning applications, all that kind of stuff today. But it seems like the ideas that carries

2:52:04.160 --> 2:52:10.960
 forward is something that will be at the center of this problem they're all trying to solve,

2:52:10.960 --> 2:52:18.160
 which is the problem of intelligence, emotional and otherwise. So thank you so much.

2:52:18.160 --> 2:52:22.960
 It's such a huge honor that you would talk to me and spend your valuable time with me today.

2:52:22.960 --> 2:52:25.120
 Thanks for talking. Thanks, Lex. It's been great.

2:52:26.320 --> 2:52:30.640
 Thanks for listening to this conversation with Doug Leonard. To support this podcast,

2:52:30.640 --> 2:52:36.080
 please check out our sponsors in the description. And now let me leave you some words from Mark

2:52:36.080 --> 2:52:43.040
 Twain about the nature of truth. If you tell the truth, you don't have to remember anything.

2:52:43.040 --> 2:53:00.720
 Thank you for listening. I hope to see you next time.

