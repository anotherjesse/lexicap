WEBVTT

00:00.000 --> 00:07.280
 The following is a conversation with Elon Musk, part two. The second time we spoke on the podcast

00:07.280 --> 00:13.120
 with parallels, if not in quality, than an outfit to the objectively speaking greatest

00:13.120 --> 00:20.720
 sequel of all time, Godfather part two. As many people know, Elon Musk is a leader of Tesla,

00:20.720 --> 00:26.880
 SpaceX, Neolink, and the Boren company. What may be less known is that he's a world class

00:26.880 --> 00:32.480
 engineer and designer, constantly emphasizing first principles thinking and taking on big

00:32.480 --> 00:39.600
 engineering problems that many before him will consider impossible. As scientists and engineers,

00:39.600 --> 00:44.160
 most of us don't question the way things are done, we simply follow the momentum of the crowd.

00:44.880 --> 00:50.480
 But revolutionary ideas that change the world on the small and large scales happen

00:50.480 --> 00:57.600
 when you return to the fundamentals and ask, is there a better way? This conversation focuses

00:57.600 --> 01:03.040
 on the incredible engineering and innovation done in brain computer interfaces at Neolink.

01:04.160 --> 01:09.520
 This work promises to help treat neurobiological diseases to help us further understand the

01:09.520 --> 01:14.480
 connection between the individual neuron to the high level function of the human brain.

01:14.480 --> 01:20.320
 And finally, to one day expand the capacity of the brain through two way communication

01:20.320 --> 01:24.640
 with computational devices, the internet, and artificial intelligence systems.

01:25.440 --> 01:31.040
 This is the Artificial Intelligence Podcast. If you enjoy it, subscribe by new YouTube,

01:31.040 --> 01:36.400
 Apple Podcasts, Spotify, support on Patreon, or simply connect with me on Twitter,

01:36.400 --> 01:43.040
 Alex Friedman, spelled F R I D M A N. And now, as an anonymous YouTube commenter,

01:43.040 --> 01:48.480
 refer to our previous conversation as the quote, historical first video of two robots

01:48.480 --> 01:56.560
 conversing without supervision. Here's the second time, the second conversation with Elon Musk.

01:57.840 --> 02:03.120
 Let's start with an easy question about consciousness. In your view, is consciousness

02:03.120 --> 02:07.120
 something that's unique to humans, or is it something that permeates all matter,

02:07.120 --> 02:12.240
 almost like a fundamental force of physics? I don't think consciousness permeates all matter.

02:12.240 --> 02:18.160
 Panpsychists believe that. Yeah. There's a philosophical. How would you tell?

02:19.680 --> 02:22.880
 That's true. That's a good point. I believe in scientific methods. I don't

02:22.880 --> 02:26.480
 bother mine or anything, but the scientific method is like, if you cannot test the hypothesis,

02:26.480 --> 02:31.040
 then you cannot reach meaningful conclusion that it is true. Do you think consciousness,

02:31.760 --> 02:36.880
 understanding consciousness is within the reach of science of the scientific method?

02:36.880 --> 02:43.760
 We can dramatically improve our understanding of consciousness. I would be hard pressed to

02:43.760 --> 02:49.360
 say that we understand anything with complete accuracy, but can we dramatically improve our

02:49.360 --> 02:51.360
 understanding of consciousness? I believe the answer is yes.

02:53.360 --> 02:58.480
 Does an AI system, in your view, have to have consciousness in order to achieve human level

02:58.480 --> 03:03.120
 or superhuman level intelligence? Does it need to have some of these human qualities,

03:03.120 --> 03:09.600
 that consciousness, maybe a body, maybe a fear of mortality, capacity to love,

03:10.160 --> 03:12.080
 those kinds of silly human things?

03:16.160 --> 03:20.880
 It's different. There's this scientific method, which I very much believe in,

03:20.880 --> 03:25.200
 where something is true to the degree that it is, testably so.

03:25.200 --> 03:37.360
 Otherwise, you're really just talking about preferences or untestable beliefs or that kind

03:37.360 --> 03:45.120
 of thing. It ends up being somewhat of a semantic question, where we are conflating

03:45.120 --> 03:48.800
 a lot of things with the word intelligence. If we parse them out and say,

03:48.800 --> 04:00.240
 all we headed towards the future where an AI will be able to outthink us in every way,

04:01.520 --> 04:03.360
 then the answer is unequivocally yes.

04:05.600 --> 04:10.400
 In order for an AI system that needs to outthink us in every way, it also needs to have

04:11.600 --> 04:16.560
 a capacity to have consciousness, self awareness, and understanding.

04:16.560 --> 04:20.160
 It will be self aware, yes. That's different from consciousness.

04:21.120 --> 04:25.040
 I mean, to me, in terms of what consciousness feels like, it feels like consciousness is in

04:25.040 --> 04:34.320
 a different dimension. But this could be just an illusion. If you damage your brain in some way,

04:34.320 --> 04:40.480
 physically, you get, you damage your consciousness, which implies that consciousness is a physical

04:40.480 --> 04:48.640
 phenomenon, in my view. The things that I think are really quite likely is that digital

04:48.640 --> 04:55.120
 intelligence will outthink us in every way, and it will soon be able to simulate what we consider

04:55.120 --> 04:59.520
 consciousness. So to a degree that you would not be able to tell the difference.

04:59.520 --> 05:04.320
 And from the aspect of the scientific method, it might as well be consciousness if we can

05:04.320 --> 05:10.640
 simulate it perfectly. If you can't tell the difference, and this is sort of the Turing test,

05:10.640 --> 05:16.640
 but think of a more sort of advanced version of the Turing test. If you're talking to

05:17.680 --> 05:23.600
 digital superintelligence and can't tell if that is a computer or a human, like let's say you're

05:23.600 --> 05:30.480
 just having a conversation over a phone or a video conference or something where you think

05:30.480 --> 05:37.680
 you're talking, looks like a person makes all of the right inflections and movements and all the

05:37.680 --> 05:45.920
 small subtleties that constitute a human and talks like human makes mistakes like a human.

05:47.200 --> 05:53.440
 And you literally just can't tell. Are you video conferencing with a person or an AI?

05:53.440 --> 06:02.000
 Might as well. Might as well. Be human. So on a darker topic, you've expressed serious concern

06:02.000 --> 06:09.440
 about existential threats of AI. It's perhaps one of the greatest challenges our civilization faces.

06:09.440 --> 06:15.600
 But since I would say we're kind of an optimistic descendants of apes, perhaps we can find several

06:15.600 --> 06:21.520
 paths of escaping the harm of AI. So if I can give you three options, maybe you can comment,

06:21.520 --> 06:27.360
 which do you think is the most promising? So one is scaling up efforts on AI safety and

06:27.360 --> 06:33.520
 beneficial AI research in hope of finding an algorithmic or maybe a policy solution.

06:34.160 --> 06:41.280
 Two is becoming a multi planetary species as quickly as possible. And three is merging with AI

06:41.280 --> 06:47.200
 and riding the wave of that increasing intelligence as it continuously improves.

06:47.200 --> 06:51.440
 What do you think is the most promising, most interesting as a civilization that we should

06:51.440 --> 06:59.280
 invest in? I think there's a lot of investment going on in AI. Where there's a lack of investment

06:59.280 --> 07:06.080
 is in AI safety. And there should be, in my view, a government agency that oversees

07:07.040 --> 07:12.400
 anything related to AI to confirm that it does not represent a public safety risk,

07:12.400 --> 07:17.600
 just as there is a regulatory authority for the food and drug administration,

07:17.600 --> 07:23.280
 there's NHTSA for automotive safety, there's the FAA for aircraft safety.

07:24.160 --> 07:28.880
 We generally come to the conclusion that it is important to have a government referee or

07:28.880 --> 07:35.920
 referee that is serving the public interest in ensuring that things are safe when there's a

07:35.920 --> 07:43.440
 potential danger to the public. I would argue that AI is unequivocally something that has

07:43.440 --> 07:48.160
 potential to be dangerous to the public and therefore should have a regulatory agency just as

07:48.160 --> 07:52.960
 other things that are dangerous to the public have a regulatory agency. But let me tell you a

07:52.960 --> 08:00.000
 problem with this is that the government moves very slowly and the rate of the rate, the usually

08:00.000 --> 08:07.920
 way a regulatory agency comes into being is that something terrible happens. There's a huge public

08:07.920 --> 08:14.960
 outcry and years after that, there's a regulatory agency or a rule put in place. It takes something

08:14.960 --> 08:24.480
 like seat belts. It was known for a decade or more that seat belts would have a massive impact on

08:24.480 --> 08:31.840
 safety and save so many lives and serious injuries. The car industry fought the requirement to put

08:31.840 --> 08:40.000
 seat belts in tooth and nail. That's crazy. Hundreds of thousands of people probably died

08:40.000 --> 08:44.880
 because of that. They said people wouldn't buy cars if they had seat belts, which is obviously

08:44.880 --> 08:51.600
 absurd. Or look at the tobacco industry and how long they fought any thing about smoking.

08:51.600 --> 08:58.240
 That's part of why I helped make that movie, Thank You for Smoking. You can sort of see just

08:58.800 --> 09:04.160
 how pernicious it can be when you have these companies effectively

09:06.320 --> 09:14.960
 achieve regulatory capture of government is bad. People in the AI community refer to the advent of

09:14.960 --> 09:22.480
 digital superintelligence as a singularity. That is not to say that it is good or bad,

09:22.480 --> 09:28.880
 but that it is very difficult to predict what will happen after that point. There's some

09:28.880 --> 09:33.600
 probability it will be bad, some probably it will be good. We obviously want to affect that

09:33.600 --> 09:41.280
 probability and have it be more good than bad. Well, let me on the merger with AI question and

09:41.280 --> 09:46.640
 the incredible work that's being done at Neuralink. There's a lot of fascinating innovation here

09:46.640 --> 09:52.880
 across different disciplines going on. The flexible wires, the robotic sewing machine,

09:52.880 --> 10:00.560
 the responsive brain movement, everything around ensuring safety and so on. We currently

10:00.560 --> 10:07.360
 understand very little about the human brain. Do you also hope that the work at Neuralink will

10:07.360 --> 10:14.240
 help us understand more about the human mind, about the brain? Yeah, I think the work at Neuralink

10:14.240 --> 10:20.640
 will definitely shut a lot of insight into how the brain and the mind works. Right now,

10:20.640 --> 10:29.040
 just the data we have regarding how the brain works is very limited. We've got fMRI, which

10:29.040 --> 10:35.600
 is that that's kind of like putting a stethoscope on the outside of a factory wall and then putting

10:35.600 --> 10:39.360
 it like all over the factory wall and you can sort of hear the sounds, but you don't know what

10:39.360 --> 10:44.640
 the machines are doing really. It's hard. You can infer a few things, but it's a very broad

10:44.640 --> 10:49.040
 breaststroke. In order to really know what's going on in the brain, you really need, you have to have

10:49.040 --> 10:54.880
 high precision sensors and then you want to have stimulus and response. Like if you trigger Neuralink,

10:54.880 --> 10:59.440
 how do you feel? What do you see? How does it change your perception of the world?

10:59.440 --> 11:03.280
 You're speaking to physically just getting close to the brain, being able to measure signals from

11:03.280 --> 11:10.240
 the brain will give us sort of open the door inside the factory. Yes, exactly. Being able to

11:10.240 --> 11:17.440
 have high precision sensors that tell you what individual neurons are doing and then being

11:17.440 --> 11:23.840
 able to trigger Neuron and see what the response is in the brain so you can see the consequences of

11:25.600 --> 11:31.120
 if you fire this Neuron, what happens? How do you feel? What does it change? It'll be really

11:31.120 --> 11:38.160
 profound to have this in people because people can articulate their change. Like if there's a

11:38.160 --> 11:44.240
 change in mood or if they can tell you if they can see better or hear better or

11:45.840 --> 11:52.080
 be able to form sentences better or worse or their memories are jogged or that kind of thing.

11:52.720 --> 11:56.800
 So on the human side, there's this incredible general malleability,

11:56.800 --> 12:01.680
 plasticity of the human brain. The human brain adapts, adjusts and so on. So that's not that

12:01.680 --> 12:07.840
 plastic for totally frank. So there's a firm structure but there is some plasticity and the

12:07.840 --> 12:13.760
 open question is sort of if I could ask a broad question is how much that plasticity can be

12:13.760 --> 12:20.160
 utilized? Sort of on the human side, there's some plasticity in the human brain and on the machine

12:20.160 --> 12:27.200
 side, we have neural networks, machine learning, artificial intelligence that's able to adjust

12:27.200 --> 12:32.320
 and figure out signals. So there's a mysterious language that we don't perfectly understand

12:32.320 --> 12:37.680
 that's within the human brain and then we're trying to understand that language to communicate

12:37.680 --> 12:42.720
 both directions. So the brain is adjusting a little bit, we don't know how much and the

12:42.720 --> 12:48.800
 machine is adjusting. Where do you see as they try to sort of reach together almost like with an

12:48.800 --> 12:55.040
 alien species, try to find a protocol, communication protocol that works? Where do you see the biggest

12:56.320 --> 13:00.720
 the biggest benefit arriving from on the machine side or the human side? Do you see both of them

13:00.720 --> 13:05.200
 working together? I should think the machine side is far more malleable than the biological side

13:06.000 --> 13:13.280
 by a huge amount. So it will be the the machine that adapts to the brain. That's the only thing

13:13.280 --> 13:19.280
 that's possible. The brain can't adapt that well to the machine. You can't have neurons start to

13:19.280 --> 13:24.960
 regard an electrode as another neuron because a neuron just this is like the pulse and so

13:24.960 --> 13:31.440
 something else is pulsing. So there is that elasticity in the interface which we believe

13:31.440 --> 13:37.600
 is something that can happen but the vast majority of malleability will have to be on the machine

13:37.600 --> 13:43.840
 side. But it's interesting when you look at that synaptic plasticity at the interface side, there

13:43.840 --> 13:49.040
 might be like an emergent plasticity because it's a whole nother. It's not like in the brain. It's a

13:49.040 --> 13:54.240
 whole nother extension of the brain. You know, we might have to redefine what it means to be

13:54.800 --> 13:59.280
 malleable for the brain. So maybe the brain is able to adjust to external interfaces.

13:59.280 --> 14:02.320
 There will be some adjustments to the brain because there's going to be something

14:02.320 --> 14:09.600
 reading and simulating the brain and so it will adjust to that thing. But

14:10.880 --> 14:13.520
 most the vast majority of the adjustment will be on the machine side.

14:15.520 --> 14:20.720
 This is just it has to be that otherwise it will not work. Ultimately, like we currently

14:20.720 --> 14:25.120
 operate on two layers. We have sort of a limbic like prime primitive brain layer,

14:25.920 --> 14:30.640
 which is where all of our kind of impulses are coming from. It's sort of like we've got

14:30.640 --> 14:36.240
 we've got like a monkey brain with a computer stuck on it. That's the human brain. And a lot

14:36.240 --> 14:41.360
 of our impulses and everything are driven by the monkey brain. And the computer, the cortex,

14:42.160 --> 14:47.200
 is constantly trying to make the monkey brain happy. It's not the cortex that's steering the

14:47.200 --> 14:53.360
 monkey brain. It's the monkey brain steering the cortex. But the cortex is the part that tells

14:53.360 --> 14:58.160
 the story of the whole thing. So we convince ourselves it's more interesting than just the

14:58.160 --> 15:02.800
 monkey brain. The cortex is like what we call like human intelligence, you know, so it's like

15:02.800 --> 15:08.160
 that's like the advanced computer relative to other creatures. Other creatures do not have

15:08.160 --> 15:16.080
 either really, they don't have the computer, or they have a very weak computer relative to humans.

15:17.200 --> 15:23.200
 But but it's this, it's like, it sort of seems like surely the really smart thing should control

15:23.200 --> 15:28.800
 the dumb thing. But actually, the dumb thing controls the smart thing. So do you think some

15:28.800 --> 15:33.520
 of the same kind of machine learning methods, or whether that's natural language processing

15:33.520 --> 15:38.960
 applications are going to be applied for the communication between the machine and the brain

15:40.640 --> 15:46.560
 to learn how to do certain things like movement of the body, how to process visual stimuli and so

15:46.560 --> 15:53.440
 on. Do you see the value of using machine learning to understand the language of the two way

15:53.440 --> 15:59.680
 communication with the brain? Yeah, absolutely. I mean, we're neural net. And that, you know,

16:00.880 --> 16:05.760
 AI is basically neural net. So it's like digital neural net will interface with biological neural

16:05.760 --> 16:12.160
 net. And hopefully bring us along for the ride. Yeah. But the vast majority of our

16:12.160 --> 16:19.840
 of our intelligence will be digital. This is like, so like think of like the difference in

16:20.880 --> 16:26.800
 intelligence between your cortex and your limbic system is gigantic. Your limbic system really

16:26.800 --> 16:35.680
 has no comprehension of what the hell the cortex is doing. It's just literally hungry, you know,

16:35.680 --> 16:44.960
 or tired or angry or sexy or something, you know, and just and then in that case, that's

16:44.960 --> 16:51.840
 that impulse to the cortex and tells the cortex to go satisfy that. Then a lot of great deal of

16:51.840 --> 16:57.600
 like a massive amount of thinking, like truly stupendous amount of thinking has gone into sex

16:57.600 --> 17:06.800
 without purpose, without procreation, without procreation, which, which is actually quite

17:06.800 --> 17:14.480
 a silly action in the absence of procreation. It's a bit silly. Well, why are you doing it?

17:14.480 --> 17:19.840
 Because it makes the limbic system happy. That's why that's why. But it's pretty absurd, really.

17:21.280 --> 17:24.880
 Well, the whole of existence is pretty absurd in some kind of sense.

17:24.880 --> 17:30.400
 Yeah. But I mean, this is a lot of computation has gone into how can I do more of that

17:31.920 --> 17:37.440
 with procreation, not even being a factor. This is, I think, a very important era of research by NSFW.

17:40.240 --> 17:44.080
 Any agency that should receive a lot of funding, especially after this conversation?

17:44.080 --> 17:48.480
 If I propose a formation of a new agency. Oh, boy.

17:48.480 --> 17:57.040
 What is the most exciting or some of the most exciting things that you see in the future impact

17:57.040 --> 18:01.600
 of Neuralink, both in the science, the engineering and societal broad impact?

18:02.720 --> 18:07.360
 So Neuralink, I think, at first will solve a lot of brain related diseases. So

18:08.560 --> 18:14.560
 creating from like autism, schizophrenia, memory loss, like everyone experiences memory loss at

18:14.560 --> 18:18.480
 certain points in age. Parents can't remember their kid's names and that kind of thing.

18:19.280 --> 18:24.720
 So there's a tremendous amount of good that Neuralink can do in solving critical

18:27.840 --> 18:33.760
 damage to brain or the spinal cord. There's a lot that can be done to improve quality of life

18:33.760 --> 18:38.240
 of individuals. And that will be those will be steps along the way. And then ultimately,

18:38.240 --> 18:44.160
 it's intended to address the the risk, the existential risk associated with

18:45.120 --> 18:53.520
 digital superintelligence. Like we will not be able to be smarter than a digital supercomputer.

18:54.400 --> 18:59.280
 So therefore, if you cannot beat them, join them. And at least we won't have that option.

18:59.280 --> 19:07.760
 So you have hope that Neuralink will be able to be a kind of connection to allow us to

19:08.480 --> 19:11.680
 merge to ride the wave of the improving AI systems.

19:12.640 --> 19:14.640
 I think the chance is above zero percent.

19:15.600 --> 19:18.960
 So it's non zero. There's a chance. And that's

19:18.960 --> 19:20.720
 So what if you've seen Dumb and Dumber?

19:21.920 --> 19:22.560
 Yes.

19:22.560 --> 19:24.160
 So I'm saying there's a chance.

19:24.160 --> 19:28.240
 He's saying one in a billion or one in a million, whatever it was at Dumb and Dumber.

19:28.240 --> 19:32.320
 You know, it went from maybe one in a million to improving. Maybe it'll be one in a thousand

19:32.320 --> 19:37.200
 and then one in a hundred, then one in ten. Depends on the rate of improvement of Neuralink

19:37.200 --> 19:41.040
 and how fast we're able to do to make progress, you know.

19:41.040 --> 19:45.440
 Well, I've talked to a few folks here that are quite brilliant engineers. So I'm excited.

19:45.440 --> 19:47.200
 Yeah. I think it's like fundamentally good, you know,

19:48.160 --> 19:52.400
 you're giving somebody back full motor control after they've had a spinal cord injury,

19:52.400 --> 19:58.320
 you know, restoring brain functionality after a stroke, solving,

19:58.320 --> 20:03.760
 debilitating genetically orange brain diseases. These are all incredibly great, I think.

20:03.760 --> 20:08.720
 And in order to do these, you have to be able to interface with neurons at detail level and

20:08.720 --> 20:14.800
 need to be able to fire the right neurons, read the right neurons, and then effectively you can

20:14.800 --> 20:23.200
 create a circuit, replace what's broken with silicon, and essentially fill in the missing

20:23.200 --> 20:32.400
 functionality. And then over time, we can develop a tertiary layer. So if like the limbic system

20:32.400 --> 20:37.280
 is the primary layer, then the cortex is like the second layer. And as I said, you know,

20:37.280 --> 20:41.280
 obviously the cortex is vastly more intelligent than the limbic system. But people generally

20:41.280 --> 20:44.880
 like the fact that they have a limbic system and a cortex. I haven't met anyone who wants to

20:44.880 --> 20:48.640
 delete either one of them. They're like, okay, I'll keep them both. That's cool.

20:48.640 --> 20:50.240
 The limbic system is kind of fun.

20:50.240 --> 20:55.920
 Yeah, that's what the fun is. Absolutely. And then people generally don't want to lose the

20:55.920 --> 20:59.520
 cortex either, right? So they're like having the cortex and the limbic system.

21:00.560 --> 21:04.400
 And then there's a tertiary layer, which will be digital superintelligence.

21:04.400 --> 21:12.640
 And I think there's room for optimism given that the cortex is very intelligent and the limbic

21:12.640 --> 21:16.960
 system is not. And yet they work together well. Perhaps there can be a tertiary layer

21:18.320 --> 21:23.280
 where digital superintelligence lies. And that will be vastly more intelligent than the cortex,

21:23.280 --> 21:28.560
 but still coexist peacefully and in a benign manner with the cortex and limbic system.

21:29.280 --> 21:34.080
 That's a super exciting future, both in the low level engineering that I saw as being done here

21:34.080 --> 21:37.120
 and the actual possibility in the next few decades.

21:38.000 --> 21:42.240
 It's important that New Orleans solve this problem sooner rather than later, because

21:42.240 --> 21:46.480
 the point at which we have digital superintelligence, that's when we pass singularity.

21:46.480 --> 21:49.840
 And things become just very uncertain. It doesn't mean that they're necessarily bad or good.

21:49.840 --> 21:53.440
 But the point at which we pass singularity, things become extremely unstable.

21:53.440 --> 21:59.760
 So we want to have a human brain interface before the singularity, or at least not long after it,

21:59.760 --> 22:06.080
 to minimize existential risk for humanity and consciousness as we know it.

22:06.080 --> 22:10.080
 But there's a lot of fascinating, actual engineering, low level problems here at New

22:10.080 --> 22:16.320
 Orleans that are quite exciting. The problems that we face in New Orleans are

22:17.280 --> 22:23.680
 material science, electrical engineering, software, mechanical engineering, micro fabrication.

22:23.680 --> 22:28.080
 It's a bunch of engineering disciplines, essentially. That's what it comes down to,

22:28.080 --> 22:35.840
 is you have to have a tiny electrode. It's so small it doesn't hurt neurons,

22:36.880 --> 22:40.800
 but it's got to last for as long as a person. So it's got to last for decades.

22:41.760 --> 22:48.160
 And then you've got to take that signal, you've got to process that signal locally at low power.

22:48.160 --> 22:55.040
 So we need a lot of chip design engineers, because we've got to do signal processing.

22:55.040 --> 23:00.800
 And do so in a very power efficient way, so that we don't heat your brain up,

23:01.840 --> 23:05.520
 because the brain's very heat sensitive. And then we've got to take those signals,

23:05.520 --> 23:11.520
 we've got to do something with them, and then we've got to stimulate the back to,

23:12.320 --> 23:18.000
 so you could biirectional communication. So if somebody's good at material science,

23:18.000 --> 23:23.680
 software, mechanical engineering, electrical engineering, chip design, micro fabrication,

23:23.680 --> 23:28.800
 that's what those are the things we need to work on. We need to be good at material science,

23:28.800 --> 23:33.600
 so that we can have tiny electrodes that last a long time. And it's a tough thing with the

23:33.600 --> 23:38.080
 material science problem, it's a tough one, because you're trying to read and stimulate

23:38.080 --> 23:44.800
 electrically in an electrically active area. Your brain is very electrically active and

23:44.800 --> 23:51.360
 electrochemically active. So how do you have, say, a coating on the electrode that doesn't dissolve

23:51.360 --> 24:00.480
 over time, and is safe in the brain? This is a very hard problem. And then how do you

24:01.680 --> 24:07.440
 collect those signals in a way that is most efficient, because you really just have very

24:07.440 --> 24:13.040
 tiny amounts of power to process those signals. And then we need to automate the whole thing,

24:13.040 --> 24:21.040
 so it's like Lasik. If this is done by neurosurgeons, there's no way it can scale to

24:21.040 --> 24:24.880
 a large number of people. And it needs to scale to a large number of people, because I think

24:24.880 --> 24:31.040
 ultimately we want the future to be determined by a large number of humans.

24:32.080 --> 24:38.240
 Do you think that this has a chance to revolutionize surgery period, so neurosurgery and surgery

24:38.240 --> 24:43.680
 all across? Yeah, for sure. It's got to be like Lasik. If Lasik had to be hand done,

24:43.680 --> 24:53.440
 done by hand, by a person, that wouldn't be great. It's done by a robot. And the ophthalmologist

24:53.440 --> 24:58.480
 kind of just needs to make sure your head's in my position, and then they just press the button and go.

24:59.760 --> 25:05.680
 So smart summon and soon auto park takes on the full beautiful mess of parking lots and their

25:05.680 --> 25:13.280
 human to human nonverbal communication. I think it has actually the potential to have a profound

25:13.280 --> 25:19.120
 impact in changing how our civilization looks at AI and robotics, because this is the first time

25:19.120 --> 25:24.080
 human beings, people that don't own a Tesla may have never seen a Tesla, heard about a Tesla,

25:24.080 --> 25:30.880
 get to watch hundreds of thousands of cars without a driver. Do you see it this way almost like an

25:30.880 --> 25:36.080
 education tool for the world about AI? Do you feel the burden of that, the excitement of that,

25:36.080 --> 25:42.160
 or do you just think it's a smart parking feature? I do think you are getting at something

25:42.160 --> 25:48.160
 important, which is most people have never really seen a robot. And what is the car that is autonomous?

25:48.160 --> 25:54.240
 It's a four wheeled robot. Yeah, it communicates a certain sort of message with everything from

25:54.240 --> 26:00.720
 safety to the possibility of what AI could bring to its current limitations, its current challenges,

26:00.720 --> 26:05.440
 it's what's possible. Do you feel the burden of that almost like a communicator, educator to the

26:05.440 --> 26:11.680
 world about AI? We're just really trying to make people's lives easier with autonomy.

26:11.680 --> 26:16.320
 But now that you mention it, I think it will be an eye opener to people about robotics,

26:16.320 --> 26:23.520
 because most people have never seen a robot, and there are hundreds of thousands of Teslas,

26:23.520 --> 26:27.680
 won't be long before there's a million of them that have autonomous capability

26:27.680 --> 26:32.960
 and the drive without a person in it. And you can see the kind of evolution of the car's

26:32.960 --> 26:43.120
 personality and thinking with each iteration of autopilot. You can see it's uncertain about this,

26:43.120 --> 26:48.960
 or it gets, but now it's more certain. Now it's moving in a slightly different way.

26:50.000 --> 26:54.960
 Like I can tell immediately if a car is on Tesla Autopilot, because it's got just little nuances

26:54.960 --> 26:59.920
 of movement, it just moves in a slightly different way. Cars on Tesla Autopilot,

26:59.920 --> 27:03.760
 for example, on the highway are far more precise about being in the center of the lane

27:03.760 --> 27:10.640
 than a person. If you drive down the highway and look at where cars are, the human driven cars,

27:10.640 --> 27:14.240
 are within their lane, they're like bumper cars. They're like moving all over the place.

27:14.800 --> 27:20.880
 The car on autopilot, dead center. Yeah, so the incredible work that's going into that

27:20.880 --> 27:27.040
 neural network is learning fast. Autonomy is still very, very hard. We don't actually know

27:27.040 --> 27:33.840
 how hard it is fully, of course. You look at most problems you tackle, this one included

27:34.880 --> 27:39.440
 with an exponential lens. But even with an exponential improvement, things can take longer

27:39.440 --> 27:48.320
 than expected sometimes. So where does Tesla currently stand on its quest for full autonomy?

27:48.320 --> 27:54.720
 What's your sense? When can we see successful deployment of full autonomy?

27:54.720 --> 28:02.720
 Well, on the highway already, the probability of intervention is extremely low. So for highway

28:02.720 --> 28:09.120
 autonomy, with the latest release, especially the probability of needing to intervene

28:09.840 --> 28:17.440
 is really quite low. In fact, I'd say for stop and go traffic, it's as far safer than a person

28:17.440 --> 28:22.160
 right now. The probability of an injury or an impact is much, much lower for autopilot than a

28:22.160 --> 28:28.320
 person. And then with navigating autopilot, it can change lanes, take highway interchanges,

28:28.960 --> 28:32.960
 and then we're coming at it from the other direction, which is low speed, full autonomy.

28:33.760 --> 28:37.760
 And in a way, this is like, it's like, how does a person learn to drive? You learn to drive in

28:37.760 --> 28:42.640
 the parking lot. You know, the first time you learn to drive probably wasn't jumping on

28:42.640 --> 28:47.040
 Marcus Street in San Francisco. That'd be crazy. You learn to drive in the parking lot, get things

28:47.040 --> 28:54.960
 right at low speed. And then the missing piece that we're working on is traffic lights and

28:54.960 --> 29:01.280
 stop streets. Stop streets, I would say, are actually also relatively easy because you kind

29:01.280 --> 29:06.240
 of know where the stop street is, worst case in geocoders, and then use visualization to see

29:06.240 --> 29:12.080
 where the line is and stop at the line to eliminate the GPS error. So actually, I'd say

29:12.080 --> 29:21.120
 there's probably complex traffic lights and very windy roads are the two things that need to get

29:21.120 --> 29:25.360
 solved. What's harder, perception or control for these problems? So being able to perfectly

29:25.360 --> 29:31.040
 perceive everything or figuring out a plan once you perceive everything, how to interact with

29:31.040 --> 29:36.720
 all the agents in the environment, in your sense, from a learning perspective, is perception or

29:36.720 --> 29:44.000
 action harder? And that giant, beautiful multitask learning neural network? The hottest thing is

29:44.000 --> 29:51.040
 having accurate representation of the physical objects in vector space. So taking the visual

29:51.040 --> 29:59.120
 input, primarily visual input, some sonar and radar, and then creating an accurate vector

29:59.120 --> 30:05.440
 space representation of the objects around you. Once you have an accurate vector space representation,

30:05.440 --> 30:13.360
 the plan and control is relatively easier. Basically, once you have accurate vector space

30:13.360 --> 30:19.520
 representation, then you're kind of like a video game. Cars in like Grand Theft Auto or something,

30:19.520 --> 30:24.400
 they work pretty well. They drive down the road, they don't crash pretty much unless you crash

30:24.400 --> 30:28.800
 into them. That's because they've got an accurate vector space representation of where the cars

30:28.800 --> 30:36.400
 are, and then they're rendering that as the output. Do you have a sense high level that Tesla's on

30:36.400 --> 30:44.720
 track on being able to achieve full autonomy? So on the highway? Yeah, absolutely. And still no

30:44.720 --> 30:50.800
 driver state, driver sensing. And we have driver sensing with the torque on the wheel? That's right.

30:52.240 --> 30:58.400
 By the way, just a quick comment on karaoke. Most people think it's fun, but I also think it is

30:58.400 --> 31:01.920
 a driving feature. I've been saying for a long time, singing in the car is really good for

31:01.920 --> 31:07.760
 attention management and vigilance management. That's great. Tesla karaoke is great. It's one

31:07.760 --> 31:12.480
 of the most fun features of the car. Do you think of a connection between fun and safety sometimes?

31:12.480 --> 31:18.800
 Yeah, you can do both at the same time. That's great. I just met with Andrew and wife of Carl

31:18.800 --> 31:25.760
 Sagan, the director of Cosmos. I'm generally a big fan of Carl Sagan. He's super cool and had

31:25.760 --> 31:30.800
 a great way of doing things. All of our consciousness, all civilization, everything we've ever known

31:30.800 --> 31:36.560
 and done is on this tiny blue dot. People also get, they get too trapped in there. This is like

31:36.560 --> 31:42.480
 squabbles amongst humans. Let's not think of the big picture. They take a civilization

31:42.480 --> 31:48.160
 and our continued existence for granted. I shouldn't do that. Look at the history of civilizations.

31:48.160 --> 31:57.200
 They rise and they fall. And now civilization is all, it's globalized. And so civilization,

31:57.200 --> 32:04.960
 I think now rises and falls together. There's not geographic isolation. This is a big risk.

32:06.640 --> 32:10.800
 Things don't always go up. That should be, that's an important lesson of history.

32:10.800 --> 32:19.680
 In 1990, at the request of Carl Sagan, the Voyage 1 spacecraft, which is a spacecraft that's

32:19.680 --> 32:25.360
 reaching out farther than anything human made into space, turned around to take a picture of Earth

32:25.360 --> 32:32.160
 from 3.7 billion miles away. And as you're talking about the pale blue dot, that picture,

32:32.160 --> 32:35.520
 the Earth takes up less than a single pixel in that image. Yes.

32:35.520 --> 32:41.760
 Of course. Appearing as a tiny blue dot as pale blue dot as Carl Sagan called it.

32:42.320 --> 32:51.920
 So he spoke about this dot of ours in 1994. And if you could humor me, I was wondering if

32:51.920 --> 32:57.760
 in the last two minutes you could read the words that he wrote describing this pale blue dot.

32:57.760 --> 33:08.160
 Sure. Yes, it's funny, the universe appears to be 13.8 billion years old. Earth is like 4.5

33:08.160 --> 33:16.800
 billion years old. Another half billion years or so, the sun will expand and probably evaporate

33:16.800 --> 33:22.560
 the oceans and make life impossible on Earth. Which means that if it had taken consciousness

33:22.560 --> 33:28.000
 10% longer to evolve, it would never have evolved at all. It's just 10% longer.

33:30.640 --> 33:35.840
 And I wonder how many dead one planet civilizations that are out there in the cosmos

33:37.760 --> 33:41.120
 that never made it to the other planet and ultimately extinguished themselves or were

33:41.120 --> 33:51.600
 destroyed by external factors. Probably a few. It's only just possible to travel to Mars,

33:51.600 --> 33:56.320
 just barely. If G was 10% more, wouldn't work, really.

33:59.360 --> 34:01.520
 If FG was 10% lower, it would be easy.

34:03.760 --> 34:06.960
 Like you can go single stage from surface of Mars all the way to surface of the Earth.

34:07.520 --> 34:09.520
 Because Mars is 37% Earth's gravity.

34:12.880 --> 34:14.560
 We need a giant booster to get off Earth.

34:14.560 --> 34:28.960
 Look again at that dot. That's here. That's home. That's us. On it, everyone you love,

34:28.960 --> 34:34.960
 everyone you know, everyone you've ever heard of, every human being who ever was, lived out their

34:34.960 --> 34:40.720
 lives. The aggregate of our joy and suffering, thousands of confident religions, ideologies,

34:40.720 --> 34:46.240
 and economic doctrines. Every hunter and farger, every hero and coward, every creator and destroyer

34:46.240 --> 34:52.800
 of civilization. Every king and peasant, every young couple in love, every mother and father,

34:53.600 --> 35:00.720
 hopeful child, inventor and explorer. Every teacher of morals, every corrupt politician,

35:01.280 --> 35:08.800
 every superstar, every supreme leader, every saint and sinner in the history of our species

35:08.800 --> 35:16.000
 live there, on a mode of dust suspended in a sunbeam. Our planet is a lonely speck in the great

35:16.000 --> 35:22.640
 enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will

35:22.640 --> 35:27.280
 come from elsewhere to save us from ourselves. The Earth is the only world known so far to

35:27.280 --> 35:33.760
 harbor life. There is nowhere else, at least in the near future, to which our species could migrate.

35:33.760 --> 35:41.120
 This is not true. This is false. Mars. And I think Carl Sagan would agree with that. He

35:41.120 --> 35:46.400
 couldn't even imagine it at that time. So thank you for making the world dream.

35:46.400 --> 36:09.040
 And thank you for talking today. I really appreciate it. Thank you.

