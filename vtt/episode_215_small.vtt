WEBVTT

00:00.000 --> 00:03.480
 The following is a conversation with Wojciech Zaremba,

00:03.480 --> 00:05.520
 cofounder of OpenAI,

00:05.520 --> 00:08.480
 which is one of the top organizations in the world

00:08.480 --> 00:12.360
 doing artificial intelligence research and development.

00:12.360 --> 00:16.840
 Wojciech is the head of language and cogeneration teams,

00:16.840 --> 00:20.240
 building and doing research on GitHub copilot,

00:20.240 --> 00:25.160
 OpenAI codecs, and GPT3, and who knows,

00:25.160 --> 00:30.160
 four, five, six, n, and n plus one.

00:30.480 --> 00:34.800
 And he also previously led OpenAI's robotic efforts.

00:34.800 --> 00:37.680
 These are incredibly exciting projects to me

00:37.680 --> 00:41.480
 that deeply challenge and expand our understanding

00:41.480 --> 00:44.160
 of the structure and nature of intelligence.

00:44.160 --> 00:46.520
 The 21st century, I think,

00:46.520 --> 00:48.400
 may very well be remembered

00:48.400 --> 00:51.560
 for a handful of revolutionary AI systems

00:51.560 --> 00:53.320
 and their implementations.

00:53.320 --> 00:56.920
 GPT codecs and applications of language models

00:56.920 --> 00:58.560
 and transformers in general

00:58.560 --> 01:01.360
 to the language and visual domains

01:01.360 --> 01:05.860
 may very well be at the core of these AI systems.

01:05.860 --> 01:09.440
 To support this podcast, please check out our sponsors.

01:09.440 --> 01:11.800
 They're listed in the description.

01:11.800 --> 01:14.040
 This is the Lex Friedman podcast,

01:14.040 --> 01:17.420
 and here is my conversation with Wojciech Zaremba.

01:18.340 --> 01:21.320
 You mentioned that Sam Altman asked

01:21.320 --> 01:24.240
 about the Fermi Paradox,

01:24.240 --> 01:25.800
 and other people at OpenAI

01:25.800 --> 01:28.520
 had really sophisticated interesting answers.

01:28.520 --> 01:29.760
 So that's when you knew

01:29.760 --> 01:31.840
 this is the right team to be working with.

01:31.840 --> 01:35.620
 So let me ask you about the Fermi Paradox, about aliens.

01:36.720 --> 01:39.440
 Why have we not found overwhelming evidence

01:39.440 --> 01:41.560
 for aliens visiting Earth?

01:41.560 --> 01:43.160
 I don't have a conviction in the answer,

01:43.160 --> 01:45.680
 but rather kind of probabilistic perspective

01:45.680 --> 01:48.000
 on what might be, let's say, possible answers.

01:48.000 --> 01:50.600
 It's also interesting that the question itself,

01:50.600 --> 01:54.720
 even can't touch on your typical question

01:54.720 --> 01:55.880
 of what's the meaning of life,

01:55.880 --> 01:58.400
 because if you assume that we don't see aliens

01:58.400 --> 01:59.980
 because they destroy themselves,

01:59.980 --> 02:03.840
 that kind of upgrades the focus

02:03.840 --> 02:06.760
 on making sure that we won't destroy ourselves.

02:06.760 --> 02:10.720
 But at the moment, the place where I am actually

02:10.720 --> 02:14.160
 with my belief, and these things also change over the time,

02:14.160 --> 02:18.040
 is I think that we might be alone in the universe,

02:18.040 --> 02:21.640
 which actually makes life more, or let's say consciousness,

02:21.640 --> 02:23.500
 life more kind of valuable.

02:23.500 --> 02:26.560
 And that means that we should more appreciate it.

02:26.560 --> 02:27.720
 Have we always been alone?

02:27.720 --> 02:30.840
 So what's your intuition about our galaxy, our universe?

02:30.840 --> 02:34.120
 Is it just sprinkled with graveyards

02:34.120 --> 02:35.600
 of intelligent civilizations,

02:35.600 --> 02:40.200
 or is life, intelligent life truly unique?

02:40.200 --> 02:42.680
 At the moment, my belief that it is unique.

02:42.680 --> 02:45.200
 But I would say I could also,

02:45.200 --> 02:49.360
 there was like some footage released with UFO objects,

02:49.360 --> 02:51.800
 which makes me actually doubt my own belief.

02:51.800 --> 02:53.200
 Yes.

02:53.200 --> 02:56.240
 Yeah, I can tell you one crazy answer that I have heard.

02:56.240 --> 02:57.520
 Yes.

02:57.520 --> 03:00.440
 So apparently when you look actually

03:00.440 --> 03:02.660
 at the limits of computation,

03:02.660 --> 03:06.560
 you can compute more if the temperature

03:06.560 --> 03:08.560
 of the universe would drop down.

03:09.760 --> 03:13.240
 So one of the things that aliens might want to do

03:13.240 --> 03:16.600
 if they are truly optimizing to maximize amount of compute,

03:16.600 --> 03:20.480
 which maybe can lead to more, let's say simulations or so,

03:20.480 --> 03:24.640
 it's instead of wasting current entropy of the universe,

03:24.640 --> 03:25.880
 because we by living,

03:25.880 --> 03:28.920
 we are actually somewhat wasting entropy,

03:28.920 --> 03:31.680
 then you can wait for the universe to cool down,

03:31.680 --> 03:33.360
 such that you have more computation.

03:33.360 --> 03:34.560
 So that's kind of a funny answer.

03:34.560 --> 03:36.380
 I'm not sure if I believe in it,

03:36.380 --> 03:37.760
 but that would be one of the reasons

03:37.760 --> 03:39.800
 why you don't see aliens.

03:39.800 --> 03:43.120
 It's also possible, see some people say that,

03:43.120 --> 03:44.960
 maybe there is not that much point

03:44.960 --> 03:47.200
 in actually going to other galaxies,

03:47.200 --> 03:49.440
 if you can go inwards.

03:49.440 --> 03:53.360
 So there is no limits of what could be an experience

03:53.360 --> 03:57.320
 if we could connect machines to our brains,

03:57.320 --> 03:58.640
 while there are still some limits

03:58.640 --> 04:00.280
 if we want to explore the universe.

04:00.280 --> 04:04.960
 Yeah, there could be a lot of ways to go inwards too.

04:04.960 --> 04:07.560
 Once you figure out some aspect of physics

04:07.560 --> 04:09.000
 we haven't figured out yet,

04:09.000 --> 04:11.040
 maybe you can travel to different dimensions.

04:11.040 --> 04:16.040
 I mean, travel in three dimensional space

04:16.040 --> 04:19.120
 may not be the most fun kind of travel.

04:19.120 --> 04:21.000
 There may be like just a huge amount

04:21.000 --> 04:22.520
 of different ways to travel.

04:22.520 --> 04:26.280
 It doesn't require a spaceship going slowly

04:26.280 --> 04:28.360
 in 3D space to space time.

04:28.360 --> 04:31.000
 It also fills in one of the problems

04:31.000 --> 04:34.880
 is that speed of light is low and the universe is vast.

04:34.880 --> 04:37.920
 And it seems that actually most likely

04:37.920 --> 04:40.320
 if we want to travel very far,

04:40.320 --> 04:45.320
 then we would instead of actually sending spaceships

04:45.320 --> 04:47.000
 with humans that wait a lot,

04:47.000 --> 04:49.040
 we would send something similar

04:49.040 --> 04:51.120
 to what Uri Miller is working on.

04:51.120 --> 04:55.560
 These are like a huge sail which is at first powered.

04:55.560 --> 04:57.720
 There is a shot of laser from an air

04:57.720 --> 05:01.320
 and it can propel it to quarter of speed of light.

05:01.320 --> 05:06.320
 And the sail itself contains a few grams of equipment.

05:06.320 --> 05:11.320
 And that might be the way to actually transport matter

05:11.480 --> 05:12.400
 through the universe.

05:12.400 --> 05:14.800
 But then when you think what would it mean for humans,

05:14.800 --> 05:16.800
 it means that we would need to actually

05:16.800 --> 05:20.880
 put their 3D printer and 3D print a human on another planet.

05:20.880 --> 05:23.480
 I don't know, play them YouTube or let's say,

05:23.480 --> 05:26.120
 or like a 3D print like huge human right away

05:26.120 --> 05:29.080
 or maybe a womb or so, yeah.

05:29.080 --> 05:32.720
 With our current techniques of archeology,

05:32.720 --> 05:37.720
 if a civilization was born and died long enough to go

05:38.120 --> 05:40.600
 on Earth, we wouldn't be able to tell.

05:40.600 --> 05:43.440
 And so that makes me really sad.

05:43.440 --> 05:45.680
 And so I think about Earth in that same way.

05:45.680 --> 05:50.240
 How can we leave some remnants if we do destroy ourselves?

05:50.240 --> 05:52.160
 How can we leave remnants for aliens

05:52.160 --> 05:54.160
 in the future to discover?

05:54.160 --> 05:56.640
 Like here's some nice stuff we've done.

05:56.640 --> 05:58.360
 Like Wikipedia and YouTube.

05:58.360 --> 06:02.200
 Do we have it like in a satellite orbiting Earth

06:02.200 --> 06:03.640
 with a hard drive?

06:03.640 --> 06:05.640
 Like how do we say,

06:05.640 --> 06:09.640
 how do we back up human civilization for the good parts

06:09.640 --> 06:12.120
 or all of it is good parts

06:12.120 --> 06:16.640
 so that it can be preserved longer than our bodies can?

06:16.640 --> 06:20.400
 That's kind of a, it's a difficult question.

06:20.400 --> 06:22.920
 It also requires the difficult acceptance

06:22.920 --> 06:24.760
 of the fact that we may die.

06:24.760 --> 06:29.400
 And if we die, we may die suddenly as a civilization.

06:29.400 --> 06:33.240
 So let's see, I think it kind of depends on the cataclysm.

06:33.240 --> 06:35.320
 We have observed in other parts of the universe

06:35.320 --> 06:37.800
 that birds of gamma rays,

06:38.880 --> 06:41.960
 these are high energy rays of light

06:41.960 --> 06:45.320
 that actually can apparently kill entire galaxy.

06:46.600 --> 06:49.400
 So there might be actually nothing event to,

06:49.400 --> 06:51.280
 nothing to protect us from it.

06:51.280 --> 06:53.880
 I'm also, and I'm looking actually at the past civilizations.

06:53.880 --> 06:55.920
 So it's like Aztecs or so,

06:55.920 --> 06:59.200
 they disappear from the surface of the Earth

06:59.200 --> 07:02.560
 and one can ask, why is it the case?

07:02.560 --> 07:07.000
 And the way I'm thinking about it is,

07:07.000 --> 07:09.280
 you know, that definitely they had some problem

07:09.280 --> 07:11.040
 that they couldn't solve.

07:11.040 --> 07:12.720
 And maybe there was a flood

07:12.720 --> 07:14.520
 and all of a sudden they couldn't drink,

07:14.520 --> 07:17.520
 there was no potable water and they all died.

07:17.520 --> 07:22.120
 And I think that so far,

07:23.160 --> 07:27.720
 the best solution to such a problems is, I guess, technology.

07:27.720 --> 07:30.600
 So I mean, if they would know that you can just boil water

07:30.600 --> 07:32.160
 and then drink it after,

07:32.160 --> 07:34.200
 then that would save their civilization.

07:34.200 --> 07:37.840
 And even now when we look actually at the current pandemic,

07:37.840 --> 07:41.400
 it seems that once again, actually science comes to rescue

07:41.400 --> 07:45.200
 and somehow science increases size of the action space.

07:45.200 --> 07:47.480
 And I think that's a good thing.

07:47.480 --> 07:52.480
 Yeah, but nature has a vastly larger action space.

07:52.480 --> 07:54.680
 Yeah, but still it might be a good thing for us

07:54.680 --> 07:56.480
 to keep on increasing action space.

07:56.480 --> 08:01.480
 Well, okay, looking at past civilizations, yes.

08:01.880 --> 08:05.800
 But looking at the destruction of human civilization,

08:05.800 --> 08:08.320
 perhaps expanding the action space

08:08.320 --> 08:13.320
 will add actions that are easily acted upon,

08:14.600 --> 08:18.920
 easily executed and as a result destroy us.

08:18.920 --> 08:20.440
 So let's see.

08:20.440 --> 08:24.280
 I was pondering why actually even

08:24.280 --> 08:27.240
 we have negative impact on the globe.

08:27.240 --> 08:30.440
 Because if you ask every single individual,

08:30.440 --> 08:32.760
 they would like to have clean air.

08:32.760 --> 08:34.040
 They would like healthy planet,

08:34.040 --> 08:37.400
 but somehow it's not the case that as a collective,

08:37.400 --> 08:39.280
 we are not going in this direction.

08:40.200 --> 08:43.160
 I think that there exists very powerful system

08:43.160 --> 08:45.360
 to describe what we value, that's capitalism.

08:45.360 --> 08:49.160
 It assigns actually monetary values to various activities.

08:49.160 --> 08:51.800
 At the moment, the problem in the current system

08:51.800 --> 08:54.160
 is that there are some things which we value.

08:54.160 --> 08:55.800
 There is no cost assigned to it.

08:55.800 --> 08:58.360
 So even though we value clean air,

08:58.360 --> 09:03.360
 or maybe we also value lack of distraction

09:03.720 --> 09:05.320
 on the internet or so,

09:05.320 --> 09:08.360
 at the moment these quantities,

09:08.360 --> 09:11.880
 companies, corporations can pollute them for free.

09:12.800 --> 09:17.800
 So in some sense, I wished,

09:18.280 --> 09:21.080
 or like that's I guess purpose of politics

09:21.080 --> 09:24.000
 to align the incentive systems.

09:24.000 --> 09:26.720
 And we are kind of maybe even moving in this direction.

09:26.720 --> 09:28.920
 The first issue is even to be able to measure

09:28.920 --> 09:30.680
 the things that we value,

09:30.680 --> 09:34.320
 then we can actually assign the monetary value to them.

09:34.320 --> 09:36.840
 Yeah, and that's so it's getting the data

09:36.840 --> 09:39.600
 and also probably through technology,

09:39.600 --> 09:44.680
 enabling people to vote and to move money around

09:44.680 --> 09:47.000
 in a way that is aligned with their values.

09:47.000 --> 09:50.320
 And that's very much a technology question.

09:50.320 --> 09:55.200
 So like having one president and Congress

09:55.200 --> 09:57.440
 and voting that happens every four years

09:57.440 --> 09:59.040
 or something like that,

09:59.040 --> 10:00.640
 that's a very outdated idea.

10:00.640 --> 10:02.600
 There could be some technological improvements

10:02.600 --> 10:03.640
 to that kind of idea.

10:03.640 --> 10:06.720
 So I'm thinking from time to time about these topics,

10:06.720 --> 10:10.360
 but it also feels to me that it's a little bit like a,

10:10.360 --> 10:12.720
 it's hard for me to actually make correct predictions

10:12.720 --> 10:14.720
 what is the appropriate thing to do.

10:14.720 --> 10:17.560
 I extremely trust Sam Altman,

10:17.560 --> 10:21.640
 our CEO on these topics here.

10:21.640 --> 10:23.960
 Okay, I'm more on the side of being,

10:23.960 --> 10:27.960
 I guess, naïve hippie that, yeah.

10:29.920 --> 10:31.600
 That's your life philosophy.

10:32.680 --> 10:37.680
 Well, I think self doubt and I think hippie implies optimism.

10:39.640 --> 10:43.240
 Those two things are pretty good way to operate.

10:43.240 --> 10:47.160
 I mean, still it is hard for me to actually

10:47.160 --> 10:49.800
 understand how the politics works

10:49.800 --> 10:53.760
 or exactly how the things would play out.

10:54.960 --> 10:57.680
 And Sam is really excellent with it.

10:57.680 --> 11:00.080
 What do you think is rarest in the universe?

11:00.080 --> 11:02.040
 You said we might be alone.

11:02.040 --> 11:04.640
 What's hardest to build is another engineering way

11:04.640 --> 11:09.240
 to ask that, life, intelligence or consciousness.

11:09.240 --> 11:12.320
 So like you said that we might be alone,

11:12.320 --> 11:15.240
 which is the thing that's hardest to get to?

11:15.240 --> 11:17.200
 Is it just the origin of life?

11:17.200 --> 11:19.240
 Is it the origin of intelligence?

11:19.240 --> 11:21.480
 Is it the origin of consciousness?

11:22.680 --> 11:27.200
 So let me at first explain in my kind of mental model

11:27.200 --> 11:29.600
 what I think is needed for life to appear.

11:31.280 --> 11:35.720
 So I imagine that at some point there was this primordial

11:35.720 --> 11:40.720
 soup of amino acids and maybe some proteins in the ocean.

11:41.320 --> 11:44.840
 And some proteins were turning into some other proteins

11:44.840 --> 11:46.360
 through reaction.

11:46.360 --> 11:50.600
 And you can almost think about this cycle

11:50.600 --> 11:52.200
 of what turns into what,

11:52.200 --> 11:54.320
 as there is a graph essentially describing

11:54.320 --> 11:57.200
 which substance turns into some other substance.

11:57.200 --> 11:59.600
 And essentially life means that all the sudden

11:59.600 --> 12:02.120
 in the graph has been created that cycle,

12:02.120 --> 12:04.400
 such that the same thing keeps on happening

12:04.400 --> 12:05.520
 over and over again.

12:05.520 --> 12:07.280
 That's what is needed for life to happen.

12:07.280 --> 12:09.280
 And in some sense you can think almost

12:09.280 --> 12:11.120
 that you have this gigantic graph

12:11.120 --> 12:14.160
 and it needs like a sufficient number of edges

12:14.160 --> 12:15.560
 for the cycle to appear.

12:17.360 --> 12:21.640
 Then from perspective of intelligence and consciousness,

12:21.640 --> 12:26.400
 my current intuition is that they might be quite intertwined.

12:26.400 --> 12:28.960
 First of all, it might not be that it's like a binary thing

12:28.960 --> 12:30.800
 that you have intelligence or consciousness.

12:30.800 --> 12:35.800
 It seems to be a more continuous component.

12:36.920 --> 12:38.480
 Let's see, if we look for instance

12:38.480 --> 12:42.520
 on the event networks recognizing images,

12:42.520 --> 12:44.920
 people are able to show that the activations

12:44.920 --> 12:48.720
 of these networks correlate very strongly

12:48.720 --> 12:52.880
 with activations in visual cortex of some monkeys.

12:52.880 --> 12:56.640
 The same seems to be true about language models.

12:56.640 --> 13:01.640
 Also, if you for instance look,

13:01.640 --> 13:04.920
 if you train agent in 3D world,

13:06.040 --> 13:10.520
 at first it barely recognizes what is going on.

13:10.520 --> 13:14.400
 Over the time it recognizes foreground from background.

13:14.400 --> 13:17.000
 Over the time it knows where there is a foot

13:17.000 --> 13:19.600
 and it just follows it.

13:19.600 --> 13:22.960
 Over the time it actually starts having a 3D perception.

13:22.960 --> 13:25.200
 So it is possible for instance to look

13:25.200 --> 13:27.120
 inside of the head of an agent

13:27.120 --> 13:29.880
 and ask what would it see if it looks to the right.

13:29.880 --> 13:32.280
 And the crazy thing is initially

13:32.280 --> 13:34.200
 when the agents are barely trained

13:34.200 --> 13:35.920
 that these predictions are pretty bad,

13:35.920 --> 13:38.560
 over the time they become better and better,

13:38.560 --> 13:42.520
 you can still see that if you ask what happens

13:42.520 --> 13:45.200
 when the head is turned by 360 degrees,

13:45.200 --> 13:48.320
 for some time they think that the different thing appears

13:48.320 --> 13:51.040
 and then at some stage they understand actually

13:51.040 --> 13:52.760
 that the same thing supposed to appear.

13:52.760 --> 13:55.840
 So they get like an understanding of 3D structure.

13:55.840 --> 14:00.320
 It's also very likely that they have inside some level

14:00.320 --> 14:03.400
 of like a symbolic reasoning,

14:03.400 --> 14:06.840
 like they're particularly symbols for other agents.

14:06.840 --> 14:09.960
 So when you look at DOTA agents,

14:09.960 --> 14:11.720
 they collaborate together

14:11.720 --> 14:16.720
 and they have some anticipation of

14:16.720 --> 14:18.880
 if they would win battle,

14:18.880 --> 14:22.440
 they have some expectations with respect to other agents.

14:22.440 --> 14:24.840
 I might be too much anthropomorphizing

14:24.840 --> 14:28.520
 how the things look for me.

14:28.520 --> 14:33.520
 But then the fact that they have a symbol for other agents

14:33.520 --> 14:37.840
 makes me believe that at some stage,

14:37.840 --> 14:40.120
 as they are optimizing for skills,

14:40.120 --> 14:44.000
 they would have also symbol to describe themselves.

14:44.000 --> 14:46.480
 This is like a very useful symbol to have.

14:46.480 --> 14:48.200
 And this particularity I would call it

14:48.200 --> 14:52.040
 like a self consciousness or self awareness.

14:52.040 --> 14:55.400
 And still it might be different from the consciousness.

14:55.400 --> 14:58.960
 So I guess the way how I'm understanding

14:58.960 --> 15:00.320
 the word consciousness,

15:00.320 --> 15:02.400
 I'd say the experience of drinking a coffee

15:02.400 --> 15:05.160
 or let's say experience of being a bat.

15:05.160 --> 15:06.840
 That's the meaning of the word consciousness.

15:06.840 --> 15:08.200
 It doesn't mean to be awake.

15:09.800 --> 15:11.240
 Yeah, it feels,

15:11.240 --> 15:13.880
 it might be also somewhat related to memory

15:13.880 --> 15:15.440
 and recurrent connections.

15:15.440 --> 15:17.760
 So it's kind of like,

15:17.760 --> 15:19.800
 if you look at anesthetic drugs,

15:19.800 --> 15:24.720
 they might be like essentially,

15:24.720 --> 15:28.040
 they disturb brain waves

15:28.040 --> 15:33.040
 such that maybe memory is not formed.

15:34.040 --> 15:36.520
 And so there's a lessening of consciousness

15:36.520 --> 15:37.360
 when you do that.

15:37.360 --> 15:38.200
 Correct.

15:38.200 --> 15:41.120
 And so that's the one way to intuit what is consciousness.

15:41.120 --> 15:45.400
 There's also kind of another element here.

15:45.400 --> 15:50.400
 It could be that it's this kind of self awareness module

15:50.400 --> 15:52.320
 that you described.

15:52.320 --> 15:55.680
 Plus the actual subjective experience

15:55.680 --> 15:58.880
 is a storytelling module

15:58.880 --> 16:03.880
 that tells us a story about what we're experiencing.

16:05.240 --> 16:07.040
 The crazy thing.

16:07.040 --> 16:08.600
 So let's say, I mean, in meditation,

16:08.600 --> 16:12.840
 they teach people not to speak story inside of the head.

16:12.840 --> 16:15.480
 And there is also some fraction of population

16:15.480 --> 16:18.080
 who doesn't have actually narrator.

16:18.080 --> 16:21.120
 I know people who don't have a narrator

16:21.120 --> 16:23.880
 and they have to use external people

16:23.880 --> 16:27.600
 in order to kind of solve tasks

16:27.600 --> 16:30.800
 that require internal narrator.

16:30.800 --> 16:34.280
 So it seems that it's possible to have the experience

16:34.280 --> 16:35.240
 without the talk.

16:37.480 --> 16:38.760
 What are we talking about

16:38.760 --> 16:40.760
 when we talk about the internal narrator?

16:40.760 --> 16:42.400
 So is that the voice when you like read the book?

16:42.400 --> 16:45.040
 Yeah, I thought that that's what you are referring to.

16:45.040 --> 16:46.480
 Well, I was referring more on it,

16:46.480 --> 16:50.160
 like not an actual voice.

16:50.160 --> 16:55.160
 I meant like there's some kind of like subjective experience

16:57.000 --> 17:02.000
 feels like it's fundamentally about storytelling

17:03.000 --> 17:04.600
 to ourselves.

17:04.600 --> 17:09.600
 It feels like the feeling is a story

17:10.080 --> 17:15.080
 that is much simpler abstraction

17:15.120 --> 17:17.480
 than the raw sensory information.

17:17.480 --> 17:21.040
 So it feels like it's a very high level abstraction

17:21.040 --> 17:26.040
 that is useful for me to feel like entity in this world.

17:27.920 --> 17:32.920
 Most useful aspect of it is that because I'm conscious,

17:35.320 --> 17:37.040
 I think there's an intricate connection

17:37.040 --> 17:39.640
 to me not wanting to die.

17:40.840 --> 17:45.840
 So like it's a useful hack to really prioritize not dying.

17:45.840 --> 17:47.760
 Like those seem to be somehow connected.

17:47.760 --> 17:50.960
 So I'm telling the story of like it's richly feels

17:50.960 --> 17:54.520
 like something to be me and the fact that me exists

17:54.520 --> 17:56.960
 in this world, I want to preserve me.

17:56.960 --> 17:59.400
 And so that makes it a useful agent hack.

17:59.400 --> 18:02.560
 So I will just refer maybe to the first part,

18:02.560 --> 18:06.120
 as you said about that kind of story of describing who you are.

18:08.120 --> 18:11.400
 I was thinking about it even.

18:11.400 --> 18:15.760
 So, you know, obviously I like thinking about it

18:15.760 --> 18:17.840
 thinking about consciousness.

18:17.840 --> 18:19.720
 I like thinking about the AI as well.

18:19.720 --> 18:22.720
 And I'm trying to see analogies of these things in AI,

18:22.720 --> 18:24.640
 what would it correspond to?

18:24.640 --> 18:29.640
 So, you know, OpenAI trained a model called GPT

18:35.040 --> 18:40.040
 which can generate pretty amusing text on arbitrary topic.

18:40.040 --> 18:45.040
 And one way to control GPT

18:45.800 --> 18:50.600
 is by putting into prefix at the beginning of the text

18:50.600 --> 18:52.880
 some information what would be the story about.

18:53.880 --> 18:58.880
 You can have even chat with GPT by saying

18:58.960 --> 19:01.880
 that the chat is with Lex or Elon Musk or so.

19:01.880 --> 19:06.880
 And GPT would just pretend to be you or Elon Musk or so.

19:06.880 --> 19:11.880
 And it almost feels that this story

19:12.240 --> 19:15.560
 that we give ourselves to describe our life,

19:15.560 --> 19:18.960
 it's almost like things that you put into context of GPT.

19:18.960 --> 19:21.280
 Yeah, and it generates the,

19:21.280 --> 19:26.280
 but the context we provide to GPT is multimodal.

19:27.160 --> 19:29.440
 It's more, so GPT itself is multimodal.

19:29.440 --> 19:32.480
 GPT itself hasn't learned actually

19:32.480 --> 19:33.960
 from experience of single human,

19:33.960 --> 19:36.200
 but from the experience of humanity.

19:36.200 --> 19:37.320
 It's a chameleon.

19:37.320 --> 19:39.120
 You can turn it into anything.

19:39.120 --> 19:41.840
 And in some sense, by providing context,

19:43.480 --> 19:47.280
 it behaves as the thing that you wanted it to be.

19:47.280 --> 19:52.280
 It's interesting that people have a stories of who they are.

19:52.400 --> 19:54.200
 And as I said, these stories,

19:54.200 --> 19:56.400
 they help them to operate in the world.

19:57.320 --> 20:00.200
 But it's also interesting,

20:00.200 --> 20:03.080
 I guess various people find it out through meditation or so

20:03.080 --> 20:05.840
 that there might be some patterns

20:05.840 --> 20:08.280
 that you have learned when you were a kid

20:08.280 --> 20:10.760
 that actually are not serving you anymore.

20:10.760 --> 20:14.120
 And you also might be thinking that that's who you are

20:14.120 --> 20:15.680
 and that's actually just a story.

20:17.400 --> 20:18.640
 Yeah, so it's a useful hack,

20:18.640 --> 20:20.520
 but sometimes it gets us into trouble.

20:20.520 --> 20:21.640
 It's a local optima.

20:21.640 --> 20:23.040
 It's a local optima.

20:23.040 --> 20:26.160
 You wrote that Stephen Hawking, he tweeted,

20:26.160 --> 20:29.960
 Stephen Hawking asked what breathes fire into equations,

20:29.960 --> 20:32.840
 which meant what makes given mathematical equations

20:32.840 --> 20:35.960
 realize the physics of a universe.

20:35.960 --> 20:40.440
 Similarly, I wonder what breathes fire into computation.

20:40.440 --> 20:43.520
 What makes given computation conscious?

20:43.520 --> 20:47.320
 Okay, so how do we engineer consciousness?

20:47.320 --> 20:51.840
 How do you breathe fire and magic into the machine?

20:51.840 --> 20:54.480
 So it seems clear to me

20:54.480 --> 20:57.320
 that not every computation is conscious.

20:57.320 --> 20:58.680
 I mean, you can, let's say,

20:58.680 --> 21:02.280
 just keep on multiplying one matrix over and over again.

21:02.280 --> 21:04.000
 And it might be gigantic matrix.

21:04.000 --> 21:05.560
 You can put a lot of computation.

21:05.560 --> 21:07.160
 I don't think it would be conscious.

21:07.160 --> 21:09.600
 So in some sense, the question is,

21:09.600 --> 21:14.480
 what are the computations which could be conscious?

21:14.480 --> 21:17.160
 I mean, so one assumption is

21:17.160 --> 21:19.040
 that it has to do purely with computation

21:19.040 --> 21:20.760
 that you can abstract away matter.

21:20.760 --> 21:23.440
 Other possibilities, it's very important

21:23.440 --> 21:25.000
 was the realization of computation

21:25.000 --> 21:28.960
 that it has to do with some force fields or so

21:28.960 --> 21:30.600
 and they bring consciousness.

21:30.600 --> 21:32.480
 At the moment, my intuition is that it can be

21:32.480 --> 21:33.800
 fully abstracted away.

21:33.800 --> 21:36.480
 So in case of computation, you can ask yourself,

21:36.480 --> 21:39.280
 what are the mathematical objects

21:39.280 --> 21:41.880
 or so that could bring such a properties?

21:41.880 --> 21:46.880
 So for instance, if we think about the models,

21:47.280 --> 21:51.600
 AI models, what they truly try to do,

21:51.600 --> 21:54.240
 or like a models like GPT is,

21:54.240 --> 21:59.240
 they try to predict the next word or so.

22:00.880 --> 22:05.880
 And this turns out to be equivalent to compressing text.

22:07.320 --> 22:10.200
 And because in some sense, compression means

22:10.200 --> 22:12.840
 that you learn the model of reality

22:12.840 --> 22:17.040
 and you have just to remember what are your mistakes.

22:17.040 --> 22:19.160
 The better you are in predicting the,

22:19.160 --> 22:22.760
 and in some sense, when we look at our experience,

22:22.760 --> 22:24.720
 also when you look for instance at the car driving,

22:24.720 --> 22:26.440
 you know in which direction it will go.

22:26.440 --> 22:28.440
 You are good like in prediction.

22:28.440 --> 22:32.720
 And it might be the case that the consciousness

22:32.720 --> 22:35.000
 is intertwined with compression.

22:35.000 --> 22:37.840
 It might be also the case that self consciousness

22:39.120 --> 22:42.000
 has to do with compressor trying to compress itself.

22:42.000 --> 22:46.160
 So I was just wondering what are the objects

22:46.160 --> 22:49.560
 in mathematics or computer science,

22:49.560 --> 22:53.520
 which are mysterious that could have to do

22:53.520 --> 22:54.360
 with consciousness.

22:54.360 --> 22:59.360
 And then I thought, you know, you see in mathematics

23:00.000 --> 23:02.360
 there is something called Gadot theorem,

23:02.360 --> 23:05.520
 which means again, if you have sufficiently

23:05.520 --> 23:07.080
 complicated mathematical system,

23:07.080 --> 23:10.120
 it is possible to point the mathematical system

23:10.120 --> 23:11.400
 back on itself.

23:11.400 --> 23:13.840
 In computer science, there is something called

23:13.840 --> 23:17.440
 helping problem, it's somewhat similar construction.

23:17.440 --> 23:19.800
 So I thought that, you know, if we believe

23:19.800 --> 23:24.640
 that under assumption that consciousness

23:24.640 --> 23:26.960
 has to do with compression,

23:28.600 --> 23:31.920
 then you could imagine that as you are

23:31.920 --> 23:33.400
 keeping on compressing things,

23:33.400 --> 23:35.680
 then at some point it actually makes sense

23:35.680 --> 23:37.480
 for the compressor to compress itself.

23:37.480 --> 23:41.600
 Metacompression, consciousness is metacompression.

23:41.600 --> 23:47.600
 That's an idea, and in some sense, you know, the crazy.

23:47.720 --> 23:48.560
 Thank you.

23:48.560 --> 23:52.280
 So, but do you think if we think of a touring machine,

23:52.280 --> 23:54.360
 a universal touring machine,

23:54.360 --> 23:57.320
 can that achieve consciousness?

23:57.320 --> 24:01.680
 So is there something beyond our traditional definition

24:01.680 --> 24:03.600
 of computation that's required?

24:03.600 --> 24:05.480
 So it's a specific computation.

24:05.480 --> 24:08.560
 And I said this computation has to do with compression.

24:08.560 --> 24:12.560
 And the compression itself, maybe other way of putting it

24:12.560 --> 24:16.280
 is like you are internally creating the model of reality.

24:16.280 --> 24:20.080
 In order, it's like you try insight to simplify reality

24:20.080 --> 24:22.520
 in order to predict what's gonna happen.

24:22.520 --> 24:26.360
 And that also feels somewhat similar to how I think

24:26.360 --> 24:28.360
 actually about my own conscious experience.

24:28.360 --> 24:31.280
 So clearly I don't have access to reality.

24:31.280 --> 24:33.480
 The only access to reality is through, you know,

24:33.480 --> 24:35.040
 cable going to my brain.

24:35.040 --> 24:38.040
 And my brain is creating a simulation of reality.

24:38.040 --> 24:40.760
 And I have access to the simulation of reality.

24:40.760 --> 24:45.040
 Are you by any chance aware of the Hutter Prize,

24:45.040 --> 24:46.560
 Marcus Hutter?

24:46.560 --> 24:51.560
 He made this prize for compression of Wikipedia pages.

24:53.400 --> 24:56.240
 And there's a few qualities to it.

24:56.240 --> 24:58.480
 One, I think has to be perfect compression,

24:58.480 --> 25:01.520
 which makes, I think that little quirk

25:01.520 --> 25:06.320
 makes it much less applicable to the general task

25:06.320 --> 25:08.400
 of intelligence, because it feels like intelligence

25:08.400 --> 25:10.080
 is always going to be messy.

25:11.600 --> 25:16.600
 Like perfect compression feels like it's not the right goal,

25:17.000 --> 25:19.320
 but it's nevertheless a very interesting goal.

25:19.320 --> 25:22.680
 So for him, intelligence equals compression.

25:22.680 --> 25:26.160
 And so the smaller you make the file,

25:26.160 --> 25:29.120
 given a large Wikipedia page,

25:29.120 --> 25:31.240
 the more intelligent the system has to be.

25:31.240 --> 25:32.080
 Yeah, that makes sense.

25:32.080 --> 25:34.920
 So you can make perfect compression if you store errors.

25:34.920 --> 25:36.400
 And I think that actually what he meant

25:36.400 --> 25:38.480
 is you have algorithm plus errors.

25:38.480 --> 25:41.080
 By the way, Hutter is a,

25:41.080 --> 25:45.040
 he was a PhD advisor of Shanleck,

25:45.040 --> 25:48.640
 who is a deep mind cofounder.

25:48.640 --> 25:51.080
 Yeah, yeah, so there's an interesting,

25:51.080 --> 25:53.320
 and now he's a deep mind.

25:53.320 --> 25:55.760
 There's an interesting network of people.

25:55.760 --> 25:59.360
 And he's one of the people that I think

25:59.360 --> 26:02.760
 seriously took on the task of

26:02.760 --> 26:05.160
 what would an AGI system look like?

26:06.120 --> 26:08.640
 I think for the longest time,

26:08.640 --> 26:13.400
 the question of AGI was not taken seriously,

26:13.400 --> 26:15.640
 or rather rigorously.

26:15.640 --> 26:17.760
 And he did just that.

26:17.760 --> 26:19.600
 Like mathematically speaking,

26:19.600 --> 26:21.040
 what would the model look like?

26:21.040 --> 26:23.600
 If you remove the constraints of it having to be,

26:27.080 --> 26:30.680
 having to have a reasonable amount of memory,

26:30.680 --> 26:34.480
 reasonable amount of running time complexity,

26:34.480 --> 26:36.920
 computation time, what would it look like?

26:36.920 --> 26:40.120
 And essentially it's a half math,

26:40.120 --> 26:43.240
 half philosophical discussion of how would it,

26:43.240 --> 26:46.120
 like a reinforcement learning type of framework

26:46.120 --> 26:47.520
 look like for an AGI?

26:47.520 --> 26:49.520
 Yeah, so he developed the framework

26:49.520 --> 26:51.480
 even to describe what's optimal

26:51.480 --> 26:53.240
 with respect to reinforcement learning.

26:53.240 --> 26:54.800
 Like there is a theoretical framework,

26:54.800 --> 26:57.000
 which is as you said, under assumption,

26:57.000 --> 26:59.840
 there is infinite amount of memory and compute.

26:59.840 --> 27:01.760
 There was actually one person

27:01.760 --> 27:03.560
 before his name is Solomonov,

27:03.560 --> 27:07.760
 who extended Solomonov work to reinforcement learning,

27:07.760 --> 27:11.440
 but there exists a theoretical algorithm,

27:11.440 --> 27:14.880
 which is optimal algorithm to build intelligence.

27:14.880 --> 27:16.880
 And I can actually explain you the algorithm.

27:16.880 --> 27:20.200
 Yes, let's go, let's go, let's go.

27:20.200 --> 27:21.920
 So the task itself, you can...

27:21.920 --> 27:25.800
 Can I just pause how absurd it is

27:25.800 --> 27:27.880
 for a brain in a skull

27:27.880 --> 27:30.480
 trying to explain the algorithm for intelligence?

27:30.480 --> 27:31.400
 Just go ahead.

27:31.400 --> 27:32.440
 It is pretty crazy.

27:32.440 --> 27:34.400
 It is pretty crazy that the brain itself

27:34.400 --> 27:37.080
 is actually so small and it can ponder.

27:38.920 --> 27:40.360
 How to design algorithms

27:40.360 --> 27:42.840
 that optimally solve the problem of intelligence?

27:42.840 --> 27:45.240
 Okay, all right, so what's the algorithm?

27:45.240 --> 27:46.400
 So let's see.

27:46.400 --> 27:50.880
 So first of all, the task itself is describe us,

27:50.880 --> 27:54.560
 you have infinite sequence of zeros and ones, okay?

27:54.560 --> 27:57.680
 You read n bits and you are about to predict

27:57.680 --> 27:59.120
 n plus one bit.

27:59.120 --> 28:00.160
 So that's the task.

28:00.160 --> 28:02.520
 And you could imagine that every task

28:02.520 --> 28:04.440
 could be casted as such a task.

28:04.440 --> 28:07.480
 So if, for instance, you have images and labels,

28:07.480 --> 28:08.800
 you can just turn every image

28:08.800 --> 28:10.600
 into a sequence of zeros and ones,

28:10.600 --> 28:12.800
 then label, you concatenate labels,

28:12.800 --> 28:15.680
 and that's actually...

28:15.680 --> 28:18.400
 You could start by having training data first,

28:18.400 --> 28:21.200
 and then afterwards you have test data.

28:21.200 --> 28:24.240
 So theoretically, any problem could be casted

28:24.240 --> 28:27.160
 as a problem of predicting zeros and ones

28:27.160 --> 28:29.000
 on this infinite tape.

28:29.000 --> 28:34.000
 So let's say you read already n bits

28:34.280 --> 28:37.080
 and you want to predict n plus one bit.

28:37.080 --> 28:41.880
 And I will ask you to write every possible program

28:41.880 --> 28:44.440
 that generates these n bits, okay?

28:44.440 --> 28:48.520
 So, and you can have, you choose programming language.

28:48.520 --> 28:50.680
 It can be Python or C++.

28:50.680 --> 28:54.160
 And the difference between programming languages might be,

28:54.160 --> 28:57.240
 there is a difference by constant, asymptotically,

28:57.240 --> 28:58.920
 your predictions will be equivalent.

29:00.080 --> 29:04.240
 So you read n bits, you enumerate all the programs

29:04.240 --> 29:07.640
 that produce these n bits in their output.

29:07.640 --> 29:10.720
 And then in order to predict n plus one bit,

29:10.720 --> 29:15.720
 you actually weight the programs according to their length.

29:16.440 --> 29:19.880
 And there is some specific formula how you weight them.

29:19.880 --> 29:23.400
 And then the n plus one bit prediction

29:23.400 --> 29:26.520
 is the prediction from each of these program

29:26.520 --> 29:28.480
 according to their weight.

29:28.480 --> 29:29.880
 Like statistically, you pick.

29:29.880 --> 29:31.360
 Statistically, yeah.

29:31.360 --> 29:32.600
 So the smaller the program,

29:32.600 --> 29:36.920
 the more likely you are to pick its output.

29:36.920 --> 29:41.920
 So that algorithm is grounded in the hope

29:42.720 --> 29:45.960
 or the intuition that the simple answer is the right one.

29:45.960 --> 29:47.680
 It's a formalization of it.

29:47.680 --> 29:48.600
 Yeah.

29:48.600 --> 29:51.840
 It also means like if you would ask the question

29:51.840 --> 29:56.840
 after how many years would, you know, stand explode,

29:58.200 --> 30:00.320
 you can say, hmm, it's more likely the answer

30:00.320 --> 30:04.080
 is two to some power because they're shorter program.

30:04.080 --> 30:04.920
 Yeah.

30:04.920 --> 30:06.600
 And then other.

30:06.600 --> 30:08.360
 Well, I don't have a good intuition

30:08.360 --> 30:11.720
 about how different the space of short programs

30:11.720 --> 30:14.720
 are from the space of large programs.

30:14.720 --> 30:18.280
 Like what is the universe for short programs?

30:18.280 --> 30:21.200
 Like run things.

30:21.200 --> 30:24.600
 So, as I said, the things have to agree with n bits.

30:24.600 --> 30:27.560
 So even if you have, you need to start,

30:27.560 --> 30:29.800
 okay, if you have very short program

30:29.800 --> 30:32.080
 and they're like a still some has,

30:32.080 --> 30:34.240
 if it's not perfect with prediction of n bits,

30:34.240 --> 30:36.080
 you have to start errors.

30:36.080 --> 30:36.920
 What are the errors?

30:36.920 --> 30:38.200
 And that gives you the full program

30:38.200 --> 30:39.680
 that agrees on n bits.

30:40.520 --> 30:43.160
 Oh, so you don't agree perfectly with the n bits

30:43.160 --> 30:44.480
 and you store errors.

30:44.480 --> 30:46.280
 That's like a longer, a longer programs,

30:46.280 --> 30:48.400
 slightly longer program

30:48.400 --> 30:50.960
 because it can't take these extra bits of errors.

30:50.960 --> 30:52.040
 That's fascinating.

30:52.040 --> 30:57.040
 What's your intuition about the programs

30:58.000 --> 30:59.760
 that are able to do cool stuff

30:59.760 --> 31:01.760
 like intelligence and consciousness?

31:01.760 --> 31:06.160
 Are they perfectly, like is it,

31:07.200 --> 31:09.280
 is there if then statements in them?

31:09.280 --> 31:11.800
 So like, is there a lot of exceptions that they're storing?

31:11.800 --> 31:14.920
 So you could imagine if there would be tremendous

31:14.920 --> 31:16.600
 amount of each statements,

31:16.600 --> 31:18.040
 then they wouldn't be that short.

31:18.040 --> 31:19.960
 In case of neural networks,

31:19.960 --> 31:23.240
 you could imagine that what happens is

31:25.840 --> 31:29.480
 when you start with an uninitialized neural network,

31:29.480 --> 31:32.400
 it stores internally many possibilities

31:32.400 --> 31:35.360
 how the problem can be solved.

31:35.360 --> 31:40.360
 And HDD is kind of magnifying some paths

31:40.800 --> 31:44.560
 which are slightly similar to the correct answer.

31:44.560 --> 31:46.800
 So it's kind of magnifying correct programs.

31:46.800 --> 31:49.880
 And in some sense, HDD is a search algorithm

31:49.880 --> 31:51.320
 in the program space.

31:51.320 --> 31:53.440
 And the program space is represented

31:53.440 --> 31:57.880
 by kind of the wiring inside of the neural network.

31:57.880 --> 32:00.320
 And there's like an insane number of ways

32:00.320 --> 32:02.760
 how the features can be computed.

32:02.760 --> 32:05.480
 Let me ask you the high level basic question.

32:05.480 --> 32:07.320
 That's not so basic.

32:07.320 --> 32:10.120
 What is deep learning?

32:10.120 --> 32:12.400
 Is there a way you'd like to think of it

32:12.400 --> 32:16.000
 that is different than like a generic textbook definition?

32:16.000 --> 32:18.360
 The thing that I hinted just a second ago

32:18.360 --> 32:21.600
 is maybe the closest to how I'm thinking these days

32:21.600 --> 32:23.280
 about deep learning.

32:23.280 --> 32:26.640
 So now the statement is

32:28.080 --> 32:30.960
 neural networks can represent some programs.

32:32.280 --> 32:33.600
 Seems that various modules

32:33.600 --> 32:35.440
 that we are actually adding up to,

32:35.440 --> 32:38.320
 like we want networks to be deep

32:38.320 --> 32:42.760
 because we want multiple steps of the computation.

32:42.760 --> 32:47.360
 And deep learning provides the way

32:47.360 --> 32:50.360
 to represent space of programs which is searchable.

32:50.360 --> 32:53.320
 And it's searchable with stochastic gradient descent.

32:53.320 --> 32:55.680
 So we have an algorithm to search over

32:56.640 --> 32:58.640
 humongous number of programs.

32:58.640 --> 33:01.040
 And gradient descent kind of bubbles up

33:01.040 --> 33:04.440
 the things that tend to give correct answers.

33:04.440 --> 33:09.440
 So a neural network with fixed weights,

33:10.160 --> 33:11.880
 that's optimized.

33:11.880 --> 33:14.360
 Do you think of that as a single program?

33:14.360 --> 33:18.400
 So there is a work by Christopher Oleg

33:18.400 --> 33:21.960
 where he, so he works on interpretability

33:21.960 --> 33:23.120
 of neural networks.

33:23.120 --> 33:28.120
 And he was able to identify inside of the neural network

33:28.440 --> 33:31.840
 for instance, a detector of a wheel for a car

33:31.840 --> 33:34.040
 or the detector of a mask for a car.

33:34.040 --> 33:36.200
 And then he was able to separate them out

33:36.200 --> 33:39.120
 and assemble them together

33:39.120 --> 33:41.960
 using a simple program for the detector,

33:41.960 --> 33:43.360
 for a car detector.

33:43.360 --> 33:47.200
 That's like, if you think of traditionally defined programs,

33:47.200 --> 33:49.360
 that's like a function within a program

33:49.360 --> 33:52.280
 that this particular neural network was able to find.

33:52.280 --> 33:54.000
 And you can tear that out,

33:54.000 --> 33:56.560
 just like you can copy and paste from Stack Overflow.

33:58.960 --> 34:03.400
 So any program is a composition of smaller programs.

34:03.400 --> 34:05.880
 Yeah, I mean, the nice thing about the neural networks

34:05.880 --> 34:08.640
 is that it allows the things to be more fuzzy

34:08.640 --> 34:10.440
 than in case of programs.

34:10.440 --> 34:13.040
 In case of programs, you have this like a branch

34:13.040 --> 34:14.480
 in this way or that way.

34:14.480 --> 34:17.320
 And the neural networks, they have an easier way

34:17.320 --> 34:21.040
 to be somewhere in between or to share things.

34:22.520 --> 34:25.120
 What to use the most beautiful, surprising idea

34:25.120 --> 34:27.880
 in deep learning and the utilization

34:27.880 --> 34:29.520
 of these neural networks,

34:29.520 --> 34:32.320
 which by the way, for people who are not familiar,

34:32.320 --> 34:36.200
 neural networks is a bunch of, what would you say?

34:36.200 --> 34:37.920
 It's inspired by the human brain.

34:37.920 --> 34:40.400
 There's neurons, there's connection between those neurons.

34:40.400 --> 34:42.360
 There's inputs and there's outputs

34:42.360 --> 34:45.520
 and there's millions or billions of those neurons.

34:45.520 --> 34:50.520
 And the learning happens by adjusting the weights

34:51.680 --> 34:54.160
 on the edges that connect these neurons.

34:54.160 --> 34:56.360
 Thank you for giving the definition.

34:56.360 --> 34:59.040
 I supposed to do it, but I guess you have enough empathy

34:59.040 --> 35:02.760
 to listen there to actually know that that might be useful.

35:02.760 --> 35:05.680
 No, that's like, so I'm asking Plato

35:05.680 --> 35:07.480
 of like, what is the meaning of life?

35:07.480 --> 35:09.320
 He's not gonna answer.

35:09.320 --> 35:12.400
 You're being philosophical and deep and quite profound

35:12.400 --> 35:13.920
 talking about the space of programs,

35:13.920 --> 35:15.640
 which is very interesting,

35:15.640 --> 35:18.360
 but also for people who are just not familiar

35:18.360 --> 35:19.480
 with the hell we're talking about

35:19.480 --> 35:20.920
 when we talk about deep learning.

35:20.920 --> 35:23.720
 Anyway, sorry, what is the most beautiful

35:23.720 --> 35:27.600
 or surprising idea to you in all the time

35:27.600 --> 35:28.560
 you've worked at deep learning?

35:28.560 --> 35:31.960
 And you worked on a lot of fascinating projects,

35:31.960 --> 35:35.280
 the applications of neural networks.

35:35.280 --> 35:36.960
 It doesn't have to be big and profound.

35:36.960 --> 35:38.280
 It can be a cool trick.

35:38.280 --> 35:40.280
 Yeah, I mean, I'm thinking about the trick,

35:40.280 --> 35:44.720
 but like it's still amusing to me that it works at all.

35:44.720 --> 35:46.920
 That let's say that the extremely simple

35:46.920 --> 35:48.880
 algorithm stochastic gradient descent,

35:48.880 --> 35:52.160
 which is something that I would be able to derive

35:52.160 --> 35:55.840
 on the piece of paper to high school student

35:55.840 --> 36:00.080
 when put at the scale of thousands of machines

36:00.080 --> 36:05.000
 actually can create the behaviors,

36:05.000 --> 36:08.000
 which we call kind of human like behaviors.

36:08.000 --> 36:11.800
 So in general, any applications stochastic gradient descent

36:11.800 --> 36:14.640
 to neural networks is amazing to you.

36:14.640 --> 36:18.280
 So, or is there a particular application

36:18.280 --> 36:21.840
 in natural language, reinforcement learning?

36:21.840 --> 36:26.840
 And also, would you attribute that success to,

36:29.240 --> 36:31.360
 is it just scale?

36:31.360 --> 36:33.440
 What profound insight can we take from the fact

36:33.440 --> 36:38.440
 that the thing works for gigantic sets of variables?

36:40.880 --> 36:42.800
 I mean, the interesting thing is these algorithms,

36:42.800 --> 36:46.360
 they were invented decades ago

36:46.360 --> 36:51.160
 and people actually gave up on the idea.

36:51.160 --> 36:55.680
 And back then they thought that we need

36:55.680 --> 36:57.840
 profoundly different algorithms

36:57.840 --> 36:59.600
 and they spent a lot of cycles

36:59.600 --> 37:01.280
 on very different algorithms.

37:01.280 --> 37:06.080
 And I believe that we have seen that various innovations

37:06.080 --> 37:09.800
 that say like transformer or dropout

37:09.800 --> 37:13.200
 or so they can vastly help.

37:13.200 --> 37:15.280
 But it's also remarkable to me

37:15.280 --> 37:18.400
 that this algorithm from sixties or so,

37:18.400 --> 37:21.400
 or I mean, you can even say that the gradient descent

37:21.400 --> 37:25.480
 was invented by Leipniz in I guess 18th century or so.

37:25.480 --> 37:29.520
 That actually is the core of learning.

37:29.520 --> 37:32.760
 In the past people are, it's almost like a,

37:32.760 --> 37:35.280
 out of the maybe an ego,

37:35.280 --> 37:37.280
 people are saying that it cannot be the case

37:37.280 --> 37:40.240
 that such a simple algorithm is the,

37:42.240 --> 37:44.760
 could solve complicated problems.

37:44.760 --> 37:48.800
 So they were in search for the other algorithms.

37:48.800 --> 37:50.640
 And as I'm saying, like I believe that actually

37:50.640 --> 37:52.560
 we are in the game where there is,

37:52.560 --> 37:54.280
 there are actually frankly three levers.

37:54.280 --> 37:57.840
 There is compute, there are algorithms and there is data.

37:57.840 --> 38:00.240
 And if we want to build intelligent systems,

38:00.240 --> 38:02.800
 we have to pull all three levers

38:03.640 --> 38:05.520
 and they are actually multiplicative.

38:06.720 --> 38:07.720
 It's also interesting.

38:07.720 --> 38:09.520
 So you ask, is it only compute?

38:10.640 --> 38:13.000
 People internally, they did the studies

38:13.000 --> 38:15.560
 to determine how much gains they were coming

38:15.560 --> 38:16.920
 from different levers.

38:16.920 --> 38:19.680
 And so far we have seen that more gains came

38:19.680 --> 38:21.120
 from compute than algorithms.

38:21.120 --> 38:23.840
 But also we are in the world that in case of compute,

38:23.840 --> 38:27.000
 there is a kind of exponential increase in funding.

38:27.000 --> 38:29.800
 And at some point it's impossible to invest more.

38:29.800 --> 38:32.920
 It's impossible to invest 10 trillion dollars.

38:32.920 --> 38:37.040
 We are speaking about the, let's say all taxes in US.

38:38.240 --> 38:39.880
 But you're talking about money,

38:39.880 --> 38:42.960
 there could be innovation in the compute.

38:42.960 --> 38:44.840
 That's true as well.

38:44.840 --> 38:46.640
 So I mean, there are like a few pieces.

38:46.640 --> 38:50.480
 So one piece is human brain is an incredible supercomputer.

38:51.680 --> 38:53.240
 And they're like a,

38:55.280 --> 38:58.920
 it has a hundred trillion parameters.

39:00.080 --> 39:02.920
 Or like if you try to count the various quantities

39:02.920 --> 39:05.680
 in the brain, they're like a neuron, synapses.

39:05.680 --> 39:07.160
 They're small number of neurons.

39:07.160 --> 39:09.040
 There is a lot of synapses.

39:09.040 --> 39:13.080
 It's unclear even how to map synapses

39:13.080 --> 39:16.120
 to parameters of neural networks.

39:16.120 --> 39:18.400
 But it's clear that there are many more.

39:19.760 --> 39:22.160
 So it might be the case that our networks

39:22.160 --> 39:23.720
 are still somewhat small.

39:25.440 --> 39:27.440
 It also might be the case that they are more efficient

39:27.440 --> 39:31.160
 in brain or less efficient by some huge factor.

39:31.160 --> 39:33.520
 I also believe that there will be,

39:33.520 --> 39:35.600
 like at the moment we are at the stage

39:35.600 --> 39:37.520
 that these neural networks,

39:37.520 --> 39:41.160
 they require 1000X or like a huge factor

39:41.160 --> 39:43.240
 of more data than humans do.

39:43.240 --> 39:44.880
 And it will be a matter of,

39:46.320 --> 39:49.520
 there will be algorithms that vastly

39:49.520 --> 39:51.840
 decrease sample complexity, I believe so.

39:51.840 --> 39:54.680
 But the place where we are heading today is

39:54.680 --> 39:59.480
 there are domains which contains million X more data.

39:59.480 --> 40:02.680
 And even though computers might be 1000 times slower

40:02.680 --> 40:05.000
 than humans in learning, that's not the problem.

40:05.000 --> 40:10.000
 Like for instance, I believe that it should be possible

40:10.280 --> 40:15.120
 to create super human therapists by a,

40:15.120 --> 40:20.120
 and they're like even simple steps of doing it.

40:20.520 --> 40:24.120
 And the core reason is there is just,

40:24.120 --> 40:27.200
 machine will be able to read way more transcripts

40:27.200 --> 40:29.480
 of therapists and then it should be able to speak

40:29.480 --> 40:31.240
 simultaneously with many more people.

40:31.240 --> 40:35.480
 And it should be possible to optimize it all in parallel.

40:35.480 --> 40:36.320
 And then.

40:36.320 --> 40:38.360
 Well, there's now you're touching on something

40:38.360 --> 40:41.960
 I deeply care about and think is way harder than we imagine.

40:43.400 --> 40:45.320
 What's the goal of a therapist?

40:45.320 --> 40:47.600
 What's the goal of therapist?

40:47.600 --> 40:51.520
 So, okay, so one goal, now this is terrifying to me,

40:51.520 --> 40:55.280
 but there's a lot of people that contemplate suicide,

40:55.280 --> 40:56.740
 suffer from depression,

40:56.740 --> 41:01.740
 and they could significantly be helped with therapy.

41:02.660 --> 41:07.660
 And the idea that an AI algorithm might be in charge of that,

41:08.220 --> 41:10.660
 it's like a life and death task.

41:10.660 --> 41:14.100
 It's the stakes are high.

41:14.100 --> 41:19.100
 So one goal for a therapist, whether human or AI,

41:19.820 --> 41:23.940
 is to prevent suicide ideation, to prevent suicide.

41:23.940 --> 41:25.820
 How do you achieve that?

41:25.820 --> 41:27.940
 So, let's see.

41:27.940 --> 41:31.780
 So, to be clear, I don't think that the current models

41:31.780 --> 41:33.500
 are good enough for such a task

41:33.500 --> 41:36.700
 because it requires insane amount of understanding empathy

41:36.700 --> 41:40.420
 and the models are far from this place, but it's.

41:40.420 --> 41:43.260
 But do you think that understanding empathy,

41:43.260 --> 41:45.780
 that signal is in the data?

41:45.780 --> 41:47.700
 I think there is some signal in the data, yes.

41:47.700 --> 41:51.020
 I mean, there are plenty of transcripts of conversations

41:51.020 --> 41:55.540
 and it is possible from it to understand

41:55.540 --> 41:59.500
 personalities, it is possible from it to understand

41:59.500 --> 42:04.500
 if conversation is friendly, amicable, antagonistic.

42:06.020 --> 42:09.820
 It is, I believe that given the fact that the models

42:09.820 --> 42:14.820
 that we train now, they can have,

42:15.540 --> 42:18.340
 they are chameleons that they can have any personality.

42:18.340 --> 42:20.700
 They might turn out to be better in understanding

42:21.860 --> 42:24.300
 personality of other people than anyone else.

42:24.300 --> 42:25.660
 And they should. Be empathetic.

42:25.660 --> 42:26.580
 To be empathetic.

42:26.580 --> 42:28.940
 Yeah, interesting.

42:28.940 --> 42:33.140
 But I wonder if there's some level

42:33.140 --> 42:36.540
 of multiple modalities required

42:37.740 --> 42:42.060
 to be able to be empathetic of the human experience,

42:42.060 --> 42:45.060
 whether language is not enough, to understand death,

42:45.060 --> 42:49.900
 to understand fear, to understand childhood trauma,

42:49.900 --> 42:54.460
 to understand wit and humor required

42:54.460 --> 42:56.220
 when you're dancing with a person

42:56.220 --> 42:58.820
 who might be depressed or suffering,

42:58.820 --> 43:02.940
 both humor and hope and love and all those kinds of things.

43:02.940 --> 43:05.060
 So there's another underlying question

43:05.060 --> 43:08.580
 which is self supervised versus supervised.

43:09.580 --> 43:13.140
 So can you get that from the data

43:13.140 --> 43:16.420
 by just reading a huge number of transcripts?

43:16.420 --> 43:18.460
 I actually, so I think that reading

43:18.460 --> 43:20.980
 a huge number of transcripts is a step one.

43:20.980 --> 43:23.860
 It's like the same way as you cannot learn to dance

43:23.860 --> 43:25.540
 if just from YouTube.

43:25.540 --> 43:29.020
 By watching it, you have to actually try it out yourself.

43:29.020 --> 43:32.100
 So I think that here that's a similar situation.

43:32.100 --> 43:33.900
 I also wouldn't deploy the system

43:33.900 --> 43:36.660
 in the high stakes situations right away,

43:36.660 --> 43:40.140
 but kind of see gradually where it goes.

43:40.140 --> 43:45.140
 And obviously initially it would have to go hand in hand

43:45.580 --> 43:46.500
 with humans.

43:46.500 --> 43:49.020
 But at the moment we are in the situation

43:49.020 --> 43:51.820
 that actually there is many more people

43:51.820 --> 43:54.300
 who actually would like to have a therapy

43:54.300 --> 43:58.140
 or speak with someone than there are therapies out there.

43:58.140 --> 44:02.700
 Like I was so fundamentally I was thinking

44:02.700 --> 44:06.380
 what are the things that can vastly

44:06.380 --> 44:08.700
 increase people well being?

44:08.700 --> 44:10.260
 Therapy is one of them.

44:10.260 --> 44:12.180
 I think meditation is other one.

44:12.180 --> 44:14.380
 I guess maybe human connection is a third one.

44:14.380 --> 44:17.420
 And I guess pharmacologically it's also possible.

44:17.420 --> 44:19.900
 Maybe direct brain stimulation or something like that.

44:19.900 --> 44:22.140
 But these are pretty much options out there.

44:22.140 --> 44:25.780
 Then let's say the way I'm thinking about the AGI endeavor

44:25.780 --> 44:30.700
 is by default that's an endeavor to increase amount of wealth.

44:30.700 --> 44:32.620
 And I believe that we can vastly increase

44:32.620 --> 44:35.140
 amount of wealth for everyone.

44:35.140 --> 44:37.380
 And simultaneously, so I mean,

44:37.380 --> 44:39.980
 these are like two endeavors that make sense to me.

44:39.980 --> 44:42.660
 One is like essentially increase amount of wealth.

44:42.660 --> 44:47.060
 And second one is increase overall human well being.

44:47.060 --> 44:48.460
 And those are coupled together.

44:48.460 --> 44:51.900
 And they can, I would say these are different topics.

44:51.900 --> 44:53.100
 One can help another.

44:54.460 --> 44:57.900
 And you know, therapist is a funny word

44:57.900 --> 45:00.340
 because I see friendship and love as therapy.

45:00.340 --> 45:02.860
 I mean, so therapist broadly defined

45:02.860 --> 45:05.500
 as just friendship as a friend.

45:05.500 --> 45:09.980
 So like therapist has a very kind of clinical sense to it.

45:09.980 --> 45:13.020
 But what is human connection?

45:13.020 --> 45:18.020
 You're like, not to get all Camus and Dostoevsky on you,

45:18.340 --> 45:22.340
 but you know, life is suffering and we draw,

45:22.340 --> 45:25.260
 we seek connection with other humans

45:25.260 --> 45:29.660
 as we desperately try to make sense of this world

45:29.660 --> 45:34.620
 in a deep overwhelming loneliness that we feel inside.

45:34.620 --> 45:37.900
 So I think connection has to do with understanding.

45:37.900 --> 45:40.620
 And I think that almost like a lack of understanding

45:40.620 --> 45:42.460
 causes suffering if you speak with someone

45:42.460 --> 45:46.780
 and do you feel ignored that actually causes pain

45:46.780 --> 45:50.700
 if you are feeling deeply understood that actually

45:50.700 --> 45:53.900
 they might not even tell you what to do in life,

45:53.900 --> 45:56.020
 but like a pure understanding.

45:56.020 --> 45:57.540
 Or just being heard.

45:57.540 --> 46:00.740
 Understanding is a kind of, that's a lot, you know,

46:00.740 --> 46:03.860
 just being heard, feel like you're being heard.

46:03.860 --> 46:08.180
 Like somehow that's a alleviation temporarily

46:08.180 --> 46:13.180
 of the loneliness that if somebody knows you're here

46:14.580 --> 46:17.340
 with their body language, with the way they are,

46:17.340 --> 46:20.180
 with the way they look at you, with the way they talk,

46:20.180 --> 46:22.380
 you feel less alone for a brief moment.

46:23.900 --> 46:25.180
 Yeah, very much agree.

46:25.180 --> 46:29.460
 So I thought in the past about somewhat similar question

46:29.460 --> 46:31.980
 to yours, which is what is love?

46:31.980 --> 46:33.220
 Rather what is connection?

46:33.220 --> 46:34.060
 Yes.

46:34.060 --> 46:37.100
 And obviously I think about these things

46:37.100 --> 46:39.060
 from AI perspective, what would it mean?

46:40.180 --> 46:42.620
 So I said that the, you know,

46:42.620 --> 46:45.060
 intelligence has to do with some compression,

46:45.060 --> 46:46.740
 which is more or less like you can say

46:46.740 --> 46:49.180
 almost understanding of what is going around.

46:49.180 --> 46:51.780
 It seems to me that other aspect

46:51.780 --> 46:54.420
 is there seem to be reward functions.

46:54.420 --> 46:58.860
 And you can have, you know, reward for food,

46:58.860 --> 47:03.860
 for maybe human connection, for let's say warmth, sex,

47:04.580 --> 47:05.900
 and so on.

47:05.900 --> 47:10.900
 And it turns out that the various people

47:10.980 --> 47:13.660
 might be optimizing slightly different reward functions.

47:13.660 --> 47:16.460
 They essentially might care about different things.

47:16.460 --> 47:21.460
 And in case of love, at least the love between two people,

47:22.580 --> 47:25.020
 you can say that the, you know,

47:25.020 --> 47:27.260
 boundary between people dissolves to such extent

47:27.260 --> 47:30.660
 that they end up optimizing each other reward functions.

47:33.140 --> 47:34.180
 Yeah.

47:34.180 --> 47:35.700
 Oh, that's interesting.

47:37.140 --> 47:39.500
 Celebrate the success of each other.

47:39.500 --> 47:40.340
 Yeah.

47:40.340 --> 47:43.900
 Or in some sense, I would say love means helping others

47:43.900 --> 47:45.900
 to optimize their reward functions,

47:45.900 --> 47:47.060
 not your reward functions,

47:47.060 --> 47:49.020
 not the things that you think are important,

47:49.020 --> 47:51.260
 but the things that the person cares about,

47:51.260 --> 47:55.100
 you try to help them to optimize it.

47:55.100 --> 47:58.700
 So love is, if you think of two reward functions,

47:58.700 --> 48:00.180
 you just, it's a condition.

48:00.180 --> 48:01.020
 Yeah.

48:01.020 --> 48:01.860
 You combine them together.

48:01.860 --> 48:02.700
 Yeah, pretty much.

48:02.700 --> 48:03.540
 Maybe like with a weight,

48:03.540 --> 48:06.180
 and it depends like the dynamic of the relationship.

48:06.180 --> 48:08.740
 Yeah, I mean, you could imagine that if you are fully

48:08.740 --> 48:10.860
 optimizing someone's reward function without yours,

48:10.860 --> 48:13.260
 then maybe you are creating codependency

48:13.260 --> 48:14.100
 or something like that.

48:14.100 --> 48:15.140
 Yeah.

48:15.140 --> 48:17.180
 I'm not sure what's the appropriate weight.

48:17.180 --> 48:18.340
 But the interesting thing is,

48:18.340 --> 48:22.780
 I even think that the individual person,

48:22.780 --> 48:27.780
 we ourselves, we are actually less of a unified insight.

48:29.740 --> 48:32.180
 So for instance, if you look at the donut,

48:32.180 --> 48:33.540
 on the one level, you might think,

48:33.540 --> 48:35.820
 oh, this is like a look space that I would like to eat it.

48:35.820 --> 48:37.980
 On another level, you might tell yourself,

48:37.980 --> 48:42.020
 I shouldn't be doing it because I want to gain muscles.

48:42.020 --> 48:44.780
 So, and you know, you might do it regardless,

48:44.780 --> 48:46.140
 kind of against yourself.

48:46.140 --> 48:48.620
 So it seems that even within ourselves,

48:48.620 --> 48:51.820
 they're almost like a kind of intertwined personas.

48:51.820 --> 48:55.140
 And I believe that the self love

48:55.140 --> 48:58.620
 means that the love between all these personas,

48:58.620 --> 49:03.620
 which also means being able to love yourself

49:03.620 --> 49:06.300
 when we are angry or stressed or so.

49:06.300 --> 49:08.060
 Combining all those reward functions

49:08.060 --> 49:09.700
 of the different selves you have.

49:09.700 --> 49:11.380
 Yeah, and accepting that they are there.

49:11.380 --> 49:12.580
 Like, you know, often people,

49:12.580 --> 49:14.540
 they have a negative self talk,

49:14.540 --> 49:16.980
 or they say, I don't like when I'm angry.

49:16.980 --> 49:18.620
 And like I try to imagine,

49:18.620 --> 49:23.620
 if there would be like a small baby Lex,

49:23.620 --> 49:26.620
 like a five years old, angry,

49:26.620 --> 49:28.620
 and then you're like, you shouldn't be angry.

49:28.620 --> 49:29.940
 Like, stop being angry.

49:29.940 --> 49:33.020
 But like, instead, actually you want Lex to come over,

49:33.020 --> 49:35.940
 give him a hug, and this, like I say, it's fine.

49:35.940 --> 49:38.620
 Okay, you can be angry as long as you want.

49:38.620 --> 49:42.620
 And then he would stop, or maybe not.

49:42.620 --> 49:44.620
 Or maybe not, but you cannot expect it even.

49:44.620 --> 49:47.620
 Yeah, but still, that doesn't explain it.

49:47.620 --> 49:49.820
 Still, that doesn't explain the why of love.

49:49.820 --> 49:52.220
 Like, why is love part of the human condition?

49:52.220 --> 49:56.700
 Why is it useful to combine the reward functions?

49:56.700 --> 50:00.300
 It seems like that doesn't, I mean,

50:00.300 --> 50:02.140
 I don't think reinforcement learning frameworks

50:02.140 --> 50:04.060
 can give us answers to why.

50:04.060 --> 50:08.500
 Even the Hutter framework has an objective function

50:08.500 --> 50:09.500
 that's static.

50:09.500 --> 50:12.900
 So we came to existence as a consequence

50:12.900 --> 50:14.580
 of evolutionary process.

50:14.580 --> 50:17.700
 And in some sense, the purpose of evolution is survival.

50:17.700 --> 50:22.500
 And then this complicated optimization objective

50:22.500 --> 50:25.340
 baked into us, let's say compression,

50:25.340 --> 50:27.980
 which might help us operate in the reward,

50:27.980 --> 50:30.380
 and it baked into us various reward functions.

50:30.380 --> 50:31.780
 Yeah.

50:31.780 --> 50:33.220
 Then to be clear, at the moment,

50:33.220 --> 50:34.980
 we are operating in the regime,

50:34.980 --> 50:36.820
 which is somewhat out of distribution

50:36.820 --> 50:38.900
 where the even evolution optimized us.

50:38.900 --> 50:40.900
 It's almost like love is a consequence

50:40.900 --> 50:44.060
 of cooperation that we've discovered is useful.

50:44.060 --> 50:47.140
 Correct. In some way, it's even the case if you...

50:47.140 --> 50:51.380
 I just love the idea that love is out of distribution.

50:51.380 --> 50:52.580
 Or it's not out of distribution.

50:52.580 --> 50:55.460
 It's like, as you said, it evolved for cooperation.

50:55.460 --> 50:55.980
 Yes.

50:55.980 --> 50:58.460
 And I believe that in some sense,

50:58.460 --> 51:00.740
 cooperation ends up helping each of us individually.

51:00.740 --> 51:02.860
 So it makes sense evolutionary.

51:02.860 --> 51:06.300
 And there is, in some sense, and love means

51:06.300 --> 51:08.220
 there is this dissolution of boundaries

51:08.220 --> 51:10.380
 that you have shared the reward function.

51:10.380 --> 51:13.100
 And we evolved to actually identify ourselves

51:13.100 --> 51:14.340
 with larger groups.

51:14.340 --> 51:18.460
 So we can identify ourselves with a family.

51:18.460 --> 51:20.700
 We can identify ourselves with a country

51:20.700 --> 51:22.540
 to such extent that people are willing

51:22.540 --> 51:25.460
 to give away their life for country.

51:25.460 --> 51:30.100
 So we are wired, actually, even for love.

51:30.100 --> 51:35.660
 And at the moment, I guess, maybe it

51:35.660 --> 51:39.020
 would be somewhat more beneficial if we would identify

51:39.020 --> 51:41.700
 ourselves with all the humanity as a whole.

51:41.700 --> 51:44.660
 So you can clearly see, when people travel around the world,

51:44.660 --> 51:47.220
 when they run into a person from the same country,

51:47.220 --> 51:48.940
 they say, oh, which city you are.

51:48.940 --> 51:52.100
 And all of a sudden, they find all these similarities.

51:52.100 --> 51:56.820
 They befriend those folks earlier than others.

51:56.820 --> 51:59.540
 So there is some sense of the belonging.

51:59.540 --> 52:01.980
 And I would say, I think it would be overall good thing

52:01.980 --> 52:07.220
 to the world for people to move towards,

52:07.220 --> 52:09.940
 I think it's even called open individualism,

52:09.940 --> 52:13.940
 move toward the mindset of a larger and larger groups.

52:13.940 --> 52:17.620
 So the challenge there, that's a beautiful vision,

52:17.620 --> 52:21.100
 and I share it, to expand that circle of empathy,

52:21.100 --> 52:24.220
 that circle of love towards the entirety of humanity.

52:24.220 --> 52:27.380
 But then you start to ask, well, where do you draw the line?

52:27.380 --> 52:30.740
 Because why not expand it to other conscious beings?

52:30.740 --> 52:34.300
 And then finally, for our discussion,

52:34.300 --> 52:39.540
 something I think about is why not expand it to AI systems?

52:39.540 --> 52:41.580
 Like, we start respecting each other

52:41.580 --> 52:47.620
 when the entity on the other side has the capacity to suffer,

52:47.620 --> 52:52.420
 because then we develop a capacity to empathize.

52:52.420 --> 52:54.980
 And so I could see AI systems that

52:54.980 --> 52:57.580
 are interacting with humans more and more

52:57.580 --> 53:01.460
 having conscious like displays.

53:01.460 --> 53:04.420
 So they display consciousness through language

53:04.420 --> 53:05.940
 and through other means.

53:05.940 --> 53:09.860
 And so then the question is, well, is that consciousness?

53:09.860 --> 53:12.060
 Because they're acting conscious.

53:12.060 --> 53:17.900
 And so the reason we don't like torturing animals

53:17.900 --> 53:20.700
 is because they look like they're suffering

53:20.700 --> 53:22.500
 when they're tortured.

53:22.500 --> 53:29.220
 And if AI looks like it's suffering when it's tortured,

53:29.220 --> 53:35.060
 how is that not requiring of the same kind of empathy from us

53:35.060 --> 53:39.260
 and respect and rights that animals do and other humans do?

53:39.260 --> 53:40.980
 I think it requires empathy as well.

53:40.980 --> 53:43.900
 I mean, I would like, I guess, us or humanity

53:43.900 --> 53:47.940
 or so make a progressing understanding what consciousness

53:47.940 --> 53:50.100
 is, because I don't want just to be speaking

53:50.100 --> 53:53.340
 about the philosophy, but rather actually make a scientific.

53:56.220 --> 53:57.740
 There was a time that people thought

53:57.740 --> 54:03.700
 that there is a force of life and the things that

54:03.700 --> 54:07.020
 have this force, they are alive.

54:07.020 --> 54:09.620
 And I think that there is actually

54:09.620 --> 54:13.780
 a path to understand exactly what consciousness is.

54:13.780 --> 54:19.420
 And in some sense, it might require essentially putting

54:19.420 --> 54:21.820
 probes inside of a human brain.

54:21.820 --> 54:23.780
 What Neuralink does.

54:23.780 --> 54:25.060
 So the goal there, I mean, there's

54:25.060 --> 54:26.580
 several things with consciousness that

54:26.580 --> 54:29.300
 make it a real discipline, which is one,

54:29.300 --> 54:32.380
 is rigorous measurement of consciousness.

54:32.380 --> 54:33.900
 And then the other is the engineering

54:33.900 --> 54:36.460
 of consciousness, which may or may not be related.

54:36.460 --> 54:38.900
 I mean, you could also run into trouble,

54:38.900 --> 54:41.780
 like, for example, in the United States,

54:41.780 --> 54:44.940
 the Department of DOT, Department of Transportation,

54:44.940 --> 54:48.660
 and a lot of different places put a value on human life.

54:48.660 --> 54:54.260
 I think DOT's value is $9 million per person.

54:54.260 --> 54:57.820
 So in that same way, you can get into trouble

54:57.820 --> 55:01.100
 if you put a number on how conscious a being is,

55:01.100 --> 55:03.500
 because then you can start making policy.

55:03.500 --> 55:12.380
 If a cow is 0.1 or like 10% as conscious as a human,

55:12.380 --> 55:14.140
 then you can start making calculations

55:14.140 --> 55:15.500
 and it might get you into trouble.

55:15.500 --> 55:18.940
 But then again, that might be a very good way to do it.

55:18.940 --> 55:22.740
 I would like to move to that place that actually we

55:22.740 --> 55:25.180
 have scientific understanding what consciousness is.

55:25.180 --> 55:27.660
 And then we'll be able to actually assign value.

55:27.660 --> 55:30.060
 And I believe that there is even the path

55:30.060 --> 55:32.700
 for the experimentation in it.

55:32.700 --> 55:39.540
 So we said that you could put the probes inside of the brain.

55:39.540 --> 55:41.580
 There is actually a few other things

55:41.580 --> 55:44.620
 that you could do with devices like Neuralink.

55:44.620 --> 55:46.540
 So you could imagine that the way even

55:46.540 --> 55:49.540
 to measure if AI system is conscious

55:49.540 --> 55:53.380
 is by literally just plugging into the brain.

55:53.380 --> 55:55.380
 I mean, that assumes that it's kind of easy.

55:55.380 --> 55:57.700
 But plugging into the brain and asking

55:57.700 --> 56:01.060
 a person if they feel that their consciousness expanded.

56:01.060 --> 56:03.140
 This direction, of course, has some issues.

56:03.140 --> 56:05.820
 You can say, if someone takes a psychedelic drug,

56:05.820 --> 56:08.220
 they might feel that their consciousness expanded,

56:08.220 --> 56:11.100
 even though that drug itself is not conscious.

56:11.100 --> 56:11.780
 Right.

56:11.780 --> 56:15.060
 So you can't fully trust the self report of a person

56:15.060 --> 56:20.540
 saying their consciousness is expanded or not.

56:20.540 --> 56:23.100
 Let me ask you a little bit about psychedelics.

56:23.100 --> 56:24.940
 There have been a lot of excellent research

56:24.940 --> 56:30.660
 on different psychedelics, psilocybin, MDMA, even DMT,

56:30.660 --> 56:34.020
 drugs in general, marijuana too.

56:34.020 --> 56:37.100
 What do you think psychedelics do to the human mind?

56:37.100 --> 56:42.060
 It seems they take the human mind to some interesting places.

56:42.060 --> 56:46.300
 Is that just a little hack, a visual hack,

56:46.300 --> 56:49.420
 or is there some profound expansion of the mind?

56:49.420 --> 56:51.100
 So let's see.

56:51.100 --> 56:52.420
 I don't believe in magic.

56:52.420 --> 56:59.780
 I believe in science, in causality.

56:59.780 --> 57:01.060
 Still, let's say.

57:01.060 --> 57:07.140
 And then as I said, I think that our subjective experience

57:07.140 --> 57:12.860
 of reality is we live in the simulation run by our brain.

57:12.860 --> 57:15.380
 And the simulation that our brain runs,

57:15.380 --> 57:19.220
 they can be very pleasant or very hellish.

57:19.220 --> 57:22.020
 Drugs, they are changing some hyperparameters

57:22.020 --> 57:23.260
 of the simulation.

57:23.260 --> 57:26.660
 It is possible thanks to change of these hyperparameters

57:26.660 --> 57:28.900
 to actually look back on your experience

57:28.900 --> 57:33.180
 and even see that the given things that we took for granted,

57:33.180 --> 57:35.500
 they are changeable.

57:35.500 --> 57:39.340
 So they allow to have an amazing perspective.

57:39.340 --> 57:41.500
 There is also, for instance, the fact

57:41.500 --> 57:46.140
 that after DMT people can see the full movie inside

57:46.140 --> 57:50.620
 of their head, gives me further belief

57:50.620 --> 57:52.820
 that the brain can generate the full movie,

57:52.820 --> 57:56.020
 that the brain is actually learning

57:56.020 --> 57:58.140
 the model of reality to such extent

57:58.140 --> 58:00.580
 that it tries to predict what's going to happen next.

58:00.580 --> 58:02.020
 Yeah, very high resolution.

58:02.020 --> 58:03.900
 So it can replay realities.

58:03.900 --> 58:06.140
 Extremely high resolution.

58:06.140 --> 58:08.460
 Yeah, and it's also kind of interesting to me

58:08.460 --> 58:11.500
 that somehow there seems to be some similarity

58:11.500 --> 58:16.900
 between these drugs and meditation itself.

58:16.900 --> 58:19.100
 And I actually started even these days

58:19.100 --> 58:22.700
 to think about meditation as a psychedelic.

58:22.700 --> 58:24.620
 Do you practice meditation?

58:24.620 --> 58:26.540
 I practice meditation.

58:26.540 --> 58:29.780
 I mean, I went a few times on the retreats

58:29.780 --> 58:37.420
 and it feels like after second or third day of meditation,

58:37.420 --> 58:41.580
 there is almost like a sense of tripping.

58:41.580 --> 58:44.780
 What is a meditation retreat entail?

58:44.780 --> 58:49.260
 So you wake up early in the morning

58:49.260 --> 58:52.540
 and you meditate for an extended period of time.

58:52.540 --> 58:53.180
 Alone?

58:53.180 --> 58:54.420
 Just to say, Jake's been trying.

58:54.420 --> 58:57.580
 Yeah, so it's optimized, even though there are other people,

58:57.580 --> 59:00.100
 it's optimized for isolation.

59:00.100 --> 59:01.500
 So you don't speak with anyone.

59:01.500 --> 59:04.540
 You don't actually look into other people's eyes.

59:04.540 --> 59:09.100
 And you sit on the chair.

59:09.100 --> 59:13.740
 So vipassana meditation tells you to focus on the breath.

59:13.740 --> 59:17.540
 So you try to put all the attention

59:17.540 --> 59:20.980
 into breathing in and breathing out.

59:20.980 --> 59:26.660
 And the crazy thing is that as you focus attention like that,

59:26.660 --> 59:30.380
 after some time, there starts coming back

59:30.380 --> 59:34.540
 like some memories that you completely forgotten.

59:34.540 --> 59:38.380
 It almost feels like you have a mailbox

59:38.380 --> 59:43.660
 and then you are just archiving email one by one.

59:43.660 --> 59:48.620
 And at some point, there is like an amazing feeling

59:48.620 --> 59:51.540
 of getting to mailbox zero, zero emails.

59:51.540 --> 59:53.660
 And it's very pleasant.

59:53.660 --> 1:00:03.700
 It's kind of, it's crazy to me that once you resolve

1:00:03.700 --> 1:00:08.140
 these inner stories or inner traumas,

1:00:08.140 --> 1:00:11.940
 then once there is nothing left,

1:00:11.940 --> 1:00:17.260
 the default state of human mind is extremely peaceful and happy.

1:00:17.260 --> 1:00:23.060
 Like some sense, it feels that the,

1:00:23.060 --> 1:00:28.100
 it feels at least to me in the way how when I was a child

1:00:28.100 --> 1:00:31.420
 that I can look at any object and it's very beautiful.

1:00:31.420 --> 1:00:34.420
 I have a lot of curiosity about the simple things.

1:00:34.420 --> 1:00:37.940
 And that's where the usual in meditation takes me.

1:00:37.940 --> 1:00:40.500
 Are you, what are you experiencing?

1:00:40.500 --> 1:00:44.580
 Are you just taking in simple sensory information

1:00:44.580 --> 1:00:48.660
 and are just enjoying the rawness of that sensory information?

1:00:48.660 --> 1:00:52.500
 So there's no, there's no memories or all that kind of stuff.

1:00:52.500 --> 1:00:55.500
 You're just enjoying being.

1:00:55.500 --> 1:00:56.420
 Yeah, pretty much.

1:00:56.420 --> 1:00:59.460
 I mean, still there is a, that it's,

1:00:59.460 --> 1:01:02.420
 it's thoughts are slowing down sometimes they pop up,

1:01:02.420 --> 1:01:06.180
 but it's also somehow the extended meditation

1:01:06.180 --> 1:01:09.740
 takes you to the space that they are way more friendly.

1:01:09.740 --> 1:01:13.180
 You're way more positive.

1:01:13.180 --> 1:01:18.420
 There is also this, this thing that we've extended.

1:01:18.420 --> 1:01:22.340
 It almost feels that the,

1:01:22.340 --> 1:01:27.700
 it almost feels that we are constantly getting a little bit of a reward function

1:01:27.700 --> 1:01:31.540
 and we are just spreading this reward function on various activities.

1:01:31.540 --> 1:01:35.140
 But if you stay still for extended period of time,

1:01:35.140 --> 1:01:37.580
 it kind of accumulates, accumulates, accumulates.

1:01:37.580 --> 1:01:41.020
 And there is a, there is a sense,

1:01:41.020 --> 1:01:44.180
 there is a sense that some point it passes some threshold

1:01:44.180 --> 1:01:50.660
 and it feels as drop is falling into kind of ocean of love and bliss.

1:01:50.660 --> 1:01:53.020
 And that's like a, this is like a very pleasant.

1:01:53.020 --> 1:01:55.060
 And that's what I'm saying, like a,

1:01:55.060 --> 1:01:58.060
 that corresponds to the subjective experience.

1:01:58.060 --> 1:02:02.780
 Some people, I guess, in spiritual community,

1:02:02.780 --> 1:02:05.620
 they describe it that that's the reality.

1:02:05.620 --> 1:02:09.260
 And I would say I believe that they're like all sorts of subjective experience

1:02:09.260 --> 1:02:10.460
 that one can have.

1:02:10.460 --> 1:02:15.980
 And I believe that, for instance, meditation might take you to the subjective experiences,

1:02:15.980 --> 1:02:17.820
 which are very pleasant, collaborative.

1:02:17.820 --> 1:02:24.620
 And I would like a word to move toward a more collaborative place.

1:02:24.620 --> 1:02:27.980
 Yeah, I would say that's very pleasant and I enjoy doing stuff like that.

1:02:27.980 --> 1:02:33.820
 I wonder how that maps to your mathematical model of love

1:02:33.820 --> 1:02:37.940
 with the reward function combining a bunch of things.

1:02:37.940 --> 1:02:42.580
 It seems like our life then is we're just,

1:02:42.580 --> 1:02:48.980
 we have this reward function and we're accumulating a bunch of stuff in it with weights.

1:02:48.980 --> 1:02:53.020
 It's like a, like multi objective.

1:02:53.020 --> 1:02:57.900
 And what meditation is, is you just remove them, remove them

1:02:57.900 --> 1:03:03.340
 until the weight on one or just a few is very high.

1:03:03.340 --> 1:03:05.140
 And that's where the pleasure comes from.

1:03:05.140 --> 1:03:08.140
 Yeah, so something similar to how I'm thinking about it.

1:03:08.140 --> 1:03:14.060
 So I told you that there is this like, there is a story of who you are.

1:03:14.060 --> 1:03:20.340
 And I think almost about it as a, you know, text prepended to GPT.

1:03:20.340 --> 1:03:20.940
 Yeah.

1:03:20.940 --> 1:03:24.100
 And some people refer to it as ego.

1:03:24.100 --> 1:03:27.540
 Okay, it's like a story who, who, who you are.

1:03:27.540 --> 1:03:27.980
 Okay.

1:03:27.980 --> 1:03:31.300
 So ego is the prompt for GPT three or GPT.

1:03:31.300 --> 1:03:31.740
 Yes, yes.

1:03:31.740 --> 1:03:32.900
 And that's the description of you.

1:03:32.900 --> 1:03:36.380
 And then with meditation, you can get to the point that actually you

1:03:36.380 --> 1:03:39.300
 experience things without the prompt.

1:03:39.300 --> 1:03:43.260
 And you experience things like as they are, you are not biased

1:03:43.260 --> 1:03:46.540
 over the description, how they supposed to be.

1:03:46.540 --> 1:03:47.460
 That's very pleasant.

1:03:47.460 --> 1:03:50.220
 And then we've respected the reward function.

1:03:50.220 --> 1:03:55.460
 It's possible to get to the point that there is the solution of self.

1:03:55.460 --> 1:03:58.820
 And therefore you can say that you're, you're having a, you're,

1:03:58.820 --> 1:04:03.100
 or like a your brain attempts to simulate the reward function of everyone else or

1:04:03.100 --> 1:04:06.340
 like everything that's, that there is this like a love which feels like a

1:04:06.340 --> 1:04:08.740
 oneness with everything.

1:04:08.740 --> 1:04:11.420
 And that's also, you know, very beautiful, very pleasant.

1:04:11.420 --> 1:04:15.780
 At some point you might have a lot of altruistic thoughts during that

1:04:15.780 --> 1:04:19.260
 moment and then the self always comes back.

1:04:19.260 --> 1:04:23.460
 How would you recommend if somebody is interested in meditation, like a big

1:04:23.460 --> 1:04:25.540
 thing to take on as a project?

1:04:25.540 --> 1:04:27.420
 Would you recommend a meditation retreat?

1:04:27.420 --> 1:04:30.180
 How many days, what kind of thing would you recommend?

1:04:30.180 --> 1:04:33.220
 I think that actually retreat is the way to go.

1:04:33.220 --> 1:04:39.260
 It almost feels that, as I said, like a meditation is a psychedelic,

1:04:39.260 --> 1:04:43.540
 but when you take it in the small dose, you might barely feel it.

1:04:43.540 --> 1:04:48.660
 Once you get the high dose, actually you're going to feel it.

1:04:48.660 --> 1:04:52.020
 So even cold turkey, if you haven't really seriously meditated for a long

1:04:52.020 --> 1:04:54.220
 period of time, just go to retreat.

1:04:54.220 --> 1:04:55.300
 Yeah, how many days?

1:04:55.300 --> 1:04:57.420
 How many days? Start weekend one.

1:04:57.420 --> 1:04:59.700
 Weekend, so like two, three days.

1:04:59.700 --> 1:05:04.620
 And it's like, it's interesting that first or second day, it's hard and

1:05:04.620 --> 1:05:07.540
 at some point it becomes easy.

1:05:07.540 --> 1:05:09.500
 There's a lot of seconds in a day.

1:05:09.500 --> 1:05:13.940
 How hard is the meditation retreat, just sitting there in a chair?

1:05:13.940 --> 1:05:22.660
 So the thing is actually, it literally just depends on your, on your own framing.

1:05:22.660 --> 1:05:26.340
 Like if you are in the mindset that you are waiting for it to be over or

1:05:26.340 --> 1:05:30.740
 you are waiting for Nirvana to happen, it will be very unpleasant.

1:05:30.740 --> 1:05:36.260
 And in some sense, even the difficulty, it's not even in the lack

1:05:36.260 --> 1:05:37.740
 of being able to speak with others.

1:05:37.740 --> 1:05:42.660
 Like you're sitting there, your legs will hurt from sitting.

1:05:42.660 --> 1:05:46.740
 In terms of like the practical things, do you experience kind of discomfort,

1:05:46.740 --> 1:05:50.940
 like physical discomfort of just sitting, like your, your butt being numb,

1:05:50.940 --> 1:05:54.220
 your legs being sore, all that kind of stuff?

1:05:54.220 --> 1:05:59.380
 Yes, you experience it and then the, the, they teach you to observe it rather.

1:05:59.380 --> 1:06:03.340
 And it's like a, the crazy thing is, you at first might have a feeling

1:06:03.340 --> 1:06:05.300
 toward trying to escape it.

1:06:05.300 --> 1:06:09.140
 And that becomes very apparent that that's extremely unpleasant.

1:06:09.140 --> 1:06:11.860
 And then you just, just observe it.

1:06:11.860 --> 1:06:17.140
 And at some point it, it just becomes, it just is.

1:06:17.140 --> 1:06:21.660
 It's like a, I remember that we've, Ilya told me some time ago that, you know,

1:06:21.660 --> 1:06:26.060
 he takes a cold shower and his mindset of taking a cold, cold shower was

1:06:26.060 --> 1:06:28.420
 to embrace suffering.

1:06:28.420 --> 1:06:29.500
 Yeah, excellent.

1:06:29.500 --> 1:06:30.420
 I do the same.

1:06:30.420 --> 1:06:31.300
 This is your style?

1:06:31.300 --> 1:06:32.860
 Yeah, it's my style.

1:06:32.860 --> 1:06:34.260
 I like this.

1:06:34.260 --> 1:06:38.980
 So my style is actually, I also sometimes take cold showers.

1:06:38.980 --> 1:06:43.300
 It is purely observing how the water goes through my body, like a purely

1:06:43.300 --> 1:06:46.060
 being present, not trying to escape from there.

1:06:46.060 --> 1:06:46.820
 Yeah.

1:06:46.820 --> 1:06:52.020
 And I would say then it actually becomes pleasant.

1:06:52.020 --> 1:06:56.940
 It's not like, well, that, that's interesting.

1:06:56.940 --> 1:07:00.460
 I, I'm also, that means that's, that's the way to deal with anything really

1:07:00.460 --> 1:07:04.580
 difficult, especially in the physical space is to observe it.

1:07:04.580 --> 1:07:07.700
 To say it's pleasant.

1:07:07.700 --> 1:07:08.500
 Hmm.

1:07:08.500 --> 1:07:11.380
 It's a, I would use a different word.

1:07:11.380 --> 1:07:18.260
 You're, you're accepting of the full beauty of reality, I would say,

1:07:18.260 --> 1:07:22.660
 because it's a pleasant, but yeah, in some sense, it is pleasant.

1:07:22.660 --> 1:07:27.980
 That's the only way to deal with a cold shower is to become an observer

1:07:27.980 --> 1:07:31.660
 and to find joy in it.

1:07:31.660 --> 1:07:36.140
 Same with like really difficult physical exercise or like running for a

1:07:36.140 --> 1:07:40.220
 really long time, endurance events, just anytime you're exhausted, any kind of

1:07:40.220 --> 1:07:43.900
 pain, I think the only way to survive it is not to resist it.

1:07:43.900 --> 1:07:46.100
 It's to observe it.

1:07:46.100 --> 1:07:48.980
 You mentioned Ilya, Ilya Satskever.

1:07:48.980 --> 1:07:53.660
 It's very, he's our chief scientist, but also he's very close friend of mine.

1:07:53.660 --> 1:07:56.340
 You co founded Open AI with you.

1:07:56.340 --> 1:07:58.500
 I've spoken with him a few times.

1:07:58.500 --> 1:07:59.220
 He's brilliant.

1:07:59.220 --> 1:08:03.020
 I really enjoy talking to him.

1:08:03.020 --> 1:08:07.220
 His mind, just like yours, works in fascinating ways.

1:08:07.220 --> 1:08:12.260
 Both of you are not able to define deep learning simply.

1:08:12.260 --> 1:08:17.460
 What's it like having him as somebody you have technical discussions with

1:08:17.460 --> 1:08:23.220
 on in space and machine learning, deep learning AI, but also life?

1:08:23.220 --> 1:08:30.980
 What's it like when these two agents get into a self play situation in a room?

1:08:30.980 --> 1:08:32.980
 What's it like collaborating with him?

1:08:32.980 --> 1:08:37.980
 So I believe that we have extreme respect to each other.

1:08:37.980 --> 1:08:49.420
 So I love Ilya's insight, both like I guess about consciousness, life, AI.

1:08:49.420 --> 1:08:58.100
 But in terms of the, it's interesting to me because you're a brilliant thinker in

1:08:58.100 --> 1:09:04.820
 the space and machine learning, like intuition, like digging deep in what works, what doesn't,

1:09:04.820 --> 1:09:06.580
 why it works, why it doesn't.

1:09:06.580 --> 1:09:07.940
 And so is Ilya.

1:09:07.940 --> 1:09:12.900
 I'm wondering if there's interesting deep discussions you've had with him in the past

1:09:12.900 --> 1:09:15.300
 or disagreements that were very productive.

1:09:15.300 --> 1:09:20.940
 So I can say I also understood over the time where are my strengths.

1:09:20.940 --> 1:09:23.660
 So obviously we have plenty of discussions.

1:09:23.660 --> 1:09:34.140
 And I myself have plenty of ideas, but I consider Ilya one of the most prolific AI scientists

1:09:34.140 --> 1:09:36.020
 in the entire world.

1:09:36.020 --> 1:09:44.620
 And I think that I realize that maybe my super skill is being able to bring people to collaborate

1:09:44.620 --> 1:09:49.620
 together, that I have some level of empathy that is unique in AI world.

1:09:49.620 --> 1:09:54.860
 And that might come from either meditation, psychedelics, or let's say I read just hundreds

1:09:54.860 --> 1:09:56.540
 of books on this topic.

1:09:56.540 --> 1:10:00.940
 And I also went through a journey of, I develop all sorts of algorithms.

1:10:00.940 --> 1:10:09.020
 So I think that maybe I can, that's my super human skill.

1:10:09.020 --> 1:10:15.740
 Ilya is one of the best AI scientists, but then I'm pretty good in assembling teams.

1:10:15.740 --> 1:10:18.660
 And I'm also not holding to people like I'm growing people.

1:10:18.660 --> 1:10:23.820
 And then people become managers at OpenAI, that's room any of them like a research managers.

1:10:23.820 --> 1:10:32.500
 So you find places where you're excellent and he finds like his deep scientific insights

1:10:32.500 --> 1:10:33.500
 is where he is.

1:10:33.500 --> 1:10:36.780
 And you find ways you can, the puzzle pieces fit together.

1:10:36.780 --> 1:10:37.780
 Correct.

1:10:37.780 --> 1:10:42.780
 Like, you know, ultimately, for instance, let's say Ilya, he doesn't manage people.

1:10:42.780 --> 1:10:46.660
 That's not what he likes or so.

1:10:46.660 --> 1:10:48.860
 I like hanging out with people.

1:10:48.860 --> 1:10:51.540
 By default, I'm an extrovert and I care about people.

1:10:51.540 --> 1:10:52.540
 Interesting.

1:10:52.540 --> 1:10:53.540
 Okay.

1:10:53.540 --> 1:10:54.540
 Okay.

1:10:54.540 --> 1:10:55.540
 Cool.

1:10:55.540 --> 1:10:56.540
 So that fits perfectly together.

1:10:56.540 --> 1:11:01.860
 But I mean, I also just like your intuition about various problems in machine learning.

1:11:01.860 --> 1:11:04.700
 He's definitely one I really enjoy.

1:11:04.700 --> 1:11:11.900
 I remember talking to him about something I was struggling with, which is coming up with

1:11:11.900 --> 1:11:17.980
 a good model for pedestrians for human beings that cross the street in the context of autonomous

1:11:17.980 --> 1:11:19.980
 vehicles.

1:11:19.980 --> 1:11:25.460
 And he immediately started to like formulate a framework within which you can evolve a

1:11:25.460 --> 1:11:31.260
 model for pedestrians, like through self play, all that kind of mechanisms.

1:11:31.260 --> 1:11:35.740
 The depth of thought on a particular problem, especially problems he doesn't know anything

1:11:35.740 --> 1:11:38.700
 about is fascinating to watch.

1:11:38.700 --> 1:11:47.620
 And it makes you realize like, yeah, the limits that the human intellect might be limitless.

1:11:47.620 --> 1:11:52.580
 Or it's just impressive to see a descendant of ape come up with clever ideas.

1:11:52.580 --> 1:11:53.580
 Yeah.

1:11:53.580 --> 1:11:57.660
 I mean, so even in the space of deep learning, when you look at various people, there are

1:11:57.660 --> 1:12:05.220
 people now who invented some breakthroughs once, but there are very few people who did

1:12:05.220 --> 1:12:06.220
 it multiple times.

1:12:06.220 --> 1:12:11.980
 And you can think if someone invented it once, that might be just a sure luck.

1:12:11.980 --> 1:12:15.580
 And if someone invented it multiple times, you know, if the probability of inventing

1:12:15.580 --> 1:12:19.740
 it once is one over a million, then probability of inventing it twice or three times would

1:12:19.740 --> 1:12:25.180
 be one over a million square or to the power of three, which would be just impossible.

1:12:25.180 --> 1:12:31.420
 So it literally means that it's given that it's not the luck.

1:12:31.420 --> 1:12:38.780
 And Ilya is one of these few people who have a lot of these inventions in his arsenal.

1:12:38.780 --> 1:12:45.220
 It also feels that, you know, for instance, if you think about folks like Gauss or Euler,

1:12:45.220 --> 1:12:51.660
 you know, at first they read a lot of books, and then they did thinking, and then they

1:12:51.660 --> 1:12:54.100
 figure out math.

1:12:54.100 --> 1:12:55.900
 And that's how it feels with Ilya.

1:12:55.900 --> 1:13:01.300
 You know, at first he read stuff, and then he spent his thinking cycles.

1:13:01.300 --> 1:13:05.860
 And that's a really good way to put it.

1:13:05.860 --> 1:13:12.540
 When I talk to him, I see thinking.

1:13:12.540 --> 1:13:14.580
 He's actually thinking.

1:13:14.580 --> 1:13:19.300
 Like he makes me realize that there's like deep thinking that the human mind can do.

1:13:19.300 --> 1:13:22.620
 Like most of us are not thinking deeply.

1:13:22.620 --> 1:13:25.780
 Like you really have to put in a lot of effort to think deeply.

1:13:25.780 --> 1:13:30.420
 Like I have to really put myself in a place where I think deeply about a problem.

1:13:30.420 --> 1:13:32.060
 It takes a lot of effort.

1:13:32.060 --> 1:13:34.700
 It's like an airplane taking off or something.

1:13:34.700 --> 1:13:36.980
 You have to achieve deep focus.

1:13:36.980 --> 1:13:45.540
 He's just, his brain is like a vertical takeoff in terms of airplane analogy.

1:13:45.540 --> 1:13:47.020
 So it's interesting.

1:13:47.020 --> 1:13:52.100
 But I mean, Cal Newport talks about this, his ideas of deep work.

1:13:52.100 --> 1:13:58.180
 It's, you know, most of us don't work much at all in terms of like deeply think about

1:13:58.180 --> 1:14:03.220
 particular problems, whether it's in math, engineering, all that kind of stuff.

1:14:03.220 --> 1:14:05.220
 You want to go to that place often.

1:14:05.220 --> 1:14:07.100
 And that's real hard work.

1:14:07.100 --> 1:14:09.380
 And some of us are better than others at that.

1:14:09.380 --> 1:14:14.420
 So I think that the big piece has to do with actually even engineering our environment

1:14:14.420 --> 1:14:16.740
 that such that it's conducive to that.

1:14:16.740 --> 1:14:24.180
 So see both Ilya and I on the frequent basis, we kind of disconnect ourselves from the world

1:14:24.180 --> 1:14:28.180
 in order to be able to do extensive amount of thinking.

1:14:28.180 --> 1:14:34.060
 So Ilya usually, he just leaves iPad at hand.

1:14:34.060 --> 1:14:36.460
 He loves his iPad.

1:14:36.460 --> 1:14:42.820
 And for me, I'm even sometimes, you know, just going for a few days to different location

1:14:42.820 --> 1:14:43.820
 to Airbnb.

1:14:43.820 --> 1:14:48.340
 So I'm turning off my phone and there is no access to me.

1:14:48.340 --> 1:14:54.140
 And that's extremely important for me to be able to actually just formulate new thoughts

1:14:54.140 --> 1:14:57.300
 to do deep work rather than to be reactive.

1:14:57.300 --> 1:15:03.740
 And the older I am, the more of this like a random tasks are at hand.

1:15:03.740 --> 1:15:08.580
 Before I go on to that thread, let me return to our friend GPT.

1:15:08.580 --> 1:15:12.540
 Let me ask you another ridiculously big question.

1:15:12.540 --> 1:15:16.060
 Can you give an overview of what GPT three is?

1:15:16.060 --> 1:15:20.500
 Or like you say in your Twitter bio, GPT and plus one.

1:15:20.500 --> 1:15:24.260
 How it works and why it works.

1:15:24.260 --> 1:15:29.540
 So GPT three is a humongous neural network.

1:15:29.540 --> 1:15:33.620
 Let's assume that we know what is neural network by definition.

1:15:33.620 --> 1:15:39.540
 And it is trained on the entire internet just to predict next word.

1:15:39.540 --> 1:15:45.980
 So let's say it sees part of the article and the only task that it has at hand, it is to

1:15:45.980 --> 1:15:48.580
 say what would be the next word?

1:15:48.580 --> 1:15:50.380
 What would be the next word?

1:15:50.380 --> 1:15:56.380
 And it becomes really exceptional at the task of figuring out what's the next word.

1:15:56.380 --> 1:16:01.460
 So you might ask, why would this be an important task?

1:16:01.460 --> 1:16:05.180
 Why would it be important to predict what's the next word?

1:16:05.180 --> 1:16:12.460
 And it turns out that a lot of problems can be formulated as a text completion problem.

1:16:12.460 --> 1:16:16.820
 So GPT is purely learning to complete the text.

1:16:16.820 --> 1:16:21.460
 As you could imagine, for instance, if you are asking a question, who is president of

1:16:21.460 --> 1:16:26.060
 United States, then GPT can give you an answer to it.

1:16:26.060 --> 1:16:29.340
 It turns out that many more things can be formulated this way.

1:16:29.340 --> 1:16:34.700
 You can format text in the way that you have sent us in English.

1:16:34.700 --> 1:16:39.660
 You make it even look like some content of a website elsewhere, which would be teaching

1:16:39.660 --> 1:16:41.980
 people how to translate things between languages.

1:16:41.980 --> 1:16:52.100
 So it would be EN colon, text in English, FR colon, and then you ask model to continue.

1:16:52.100 --> 1:16:57.340
 And it turns out that such a model is predicting translation from English to French.

1:16:57.340 --> 1:17:04.380
 The crazy thing is that this model can be used for way more sophisticated tasks.

1:17:04.380 --> 1:17:09.300
 So you can format text such that it looks like a conversation between two people.

1:17:09.300 --> 1:17:12.940
 And that might be a conversation between you and Elon Musk.

1:17:12.940 --> 1:17:18.460
 And because the model read all the text about Elon Musk, it will be able to predict Elon

1:17:18.460 --> 1:17:20.300
 Musk words as it would be Elon Musk.

1:17:20.300 --> 1:17:26.620
 It will speak about colonization of Mars, about sustainable future and so on.

1:17:26.620 --> 1:17:33.020
 And it's also possible to even give arbitrary personality to the model.

1:17:33.020 --> 1:17:38.260
 You can say, here is a conversation that we've a friendly AI bot.

1:17:38.260 --> 1:17:42.740
 And the model will complete the text as a friendly AI bot.

1:17:42.740 --> 1:17:49.180
 So I mean, how do I express how amazing this is?

1:17:49.180 --> 1:17:56.300
 So just to clarify a conversation, generating a conversation between me and Elon Musk, it

1:17:56.300 --> 1:18:02.100
 wouldn't just generate good examples of what Elon would say.

1:18:02.100 --> 1:18:04.300
 It would get the syntax all correct.

1:18:04.300 --> 1:18:09.380
 So like interview style, it would say like Elon Cohen and Lex Cohen.

1:18:09.380 --> 1:18:17.780
 It's not just like inklings of semantic correctness.

1:18:17.780 --> 1:18:24.740
 It's like the whole thing, grammatical, syntactic, semantic.

1:18:24.740 --> 1:18:28.460
 It's just really, really impressive generalization.

1:18:28.460 --> 1:18:35.500
 Yeah, I mean, I also want to provide some caveat so it can generate few paragraphs of

1:18:35.500 --> 1:18:36.660
 coherent text.

1:18:36.660 --> 1:18:41.540
 But as you go to longer pieces, it actually goes off the rails.

1:18:41.540 --> 1:18:45.860
 If you would try to write a book, it won't work out this way.

1:18:45.860 --> 1:18:47.860
 What way does it go off the rails, by the way?

1:18:47.860 --> 1:18:51.580
 Is there interesting ways in which it goes off the rails?

1:18:51.580 --> 1:18:54.140
 What falls apart first?

1:18:54.140 --> 1:18:59.940
 So the model is trained on all the existing data that is out there, which means that it

1:18:59.940 --> 1:19:02.360
 is not trained on its own mistakes.

1:19:02.360 --> 1:19:08.420
 So for instance, if it would make a mistake, then I kept so to give you an example.

1:19:08.420 --> 1:19:15.420
 So let's say I have a conversation with a model pretending that is Elon Musk, and then

1:19:15.420 --> 1:19:20.700
 I start putting some, I'm start actually making up things which are not factual.

1:19:20.700 --> 1:19:22.700
 Sounds like Twitter.

1:19:22.700 --> 1:19:26.460
 But I got you, sorry.

1:19:26.460 --> 1:19:27.460
 Yeah.

1:19:27.460 --> 1:19:28.460
 Okay.

1:19:28.460 --> 1:19:29.460
 I don't know.

1:19:29.460 --> 1:19:35.580
 I would say that Elon is my wife, and the model will just keep on carrying it on.

1:19:35.580 --> 1:19:37.100
 As if it's true.

1:19:37.100 --> 1:19:38.100
 Yes.

1:19:38.100 --> 1:19:42.420
 And in some sense, if you would have a normal conversation with Elon, he would be, what

1:19:42.420 --> 1:19:43.420
 the fuck?

1:19:43.420 --> 1:19:46.580
 Yeah, there would be some feedback.

1:19:46.580 --> 1:19:52.260
 So the model is trained on things that humans have written, but through the generation process,

1:19:52.260 --> 1:19:54.540
 there's no human in the loop feedback.

1:19:54.540 --> 1:19:55.540
 Correct.

1:19:55.540 --> 1:19:56.540
 That's fascinating.

1:19:56.540 --> 1:19:57.540
 Makes sense.

1:19:57.540 --> 1:19:58.540
 So it's magnified.

1:19:58.540 --> 1:20:01.340
 It's like the errors get magnified and magnified.

1:20:01.340 --> 1:20:06.900
 And it's also interesting, I mean, first of all, humans have the same problem.

1:20:06.900 --> 1:20:14.020
 It's just that we will make fewer errors and magnify the errors slower.

1:20:14.020 --> 1:20:18.820
 I think that actually what happens with humans is if you have a wrong belief about the world

1:20:18.820 --> 1:20:23.940
 as a kid, then very quickly you will learn that it's not correct because they are grounded

1:20:23.940 --> 1:20:27.780
 in reality and they are learning from your new experience.

1:20:27.780 --> 1:20:31.500
 But do you think the model can correct itself too?

1:20:31.500 --> 1:20:38.900
 Won't it, through the power of the representation, and so the absence of Elon Musk being your

1:20:38.900 --> 1:20:44.020
 wife, information on the internet, won't it correct itself?

1:20:44.020 --> 1:20:45.940
 There won't be examples like that.

1:20:45.940 --> 1:20:48.700
 So the errors will be subtle at first.

1:20:48.700 --> 1:20:55.060
 And in some sense, you can also say that the data that is not out there is the data which

1:20:55.060 --> 1:20:58.860
 would represent how the human learns.

1:20:58.860 --> 1:21:03.660
 And maybe the model would be trained on such a data, then it would be better off.

1:21:03.660 --> 1:21:06.740
 How intelligent is GPT3, do you think?

1:21:06.740 --> 1:21:14.700
 When you think about the nature of intelligence, it seems exceptionally impressive.

1:21:14.700 --> 1:21:21.100
 But then if you think about the big AGI problem, is this footsteps along the way to AGI?

1:21:21.100 --> 1:21:26.980
 So it seems that intelligence itself is a multiple axis of it.

1:21:26.980 --> 1:21:34.860
 And I would expect that the systems that we are building, they may end up being superhuman

1:21:34.860 --> 1:21:38.140
 on some axis and subhuman on some other axis.

1:21:38.140 --> 1:21:43.820
 It would be surprising to me on all axis simultaneously, they would become superhuman.

1:21:43.820 --> 1:21:50.260
 Of course, people ask this question, is GPT a spaceship that would take us to Moon?

1:21:50.260 --> 1:21:55.700
 Or are we building a ladder to Heaven that we are just building bigger and bigger ladder?

1:21:55.700 --> 1:22:00.340
 And we don't know in some sense which one of these two works better.

1:22:00.340 --> 1:22:02.340
 Which one is better?

1:22:02.340 --> 1:22:07.740
 I like Stairway to Heaven, it's a good song, so I'm not exactly sure which one is better.

1:22:07.740 --> 1:22:11.340
 But you're saying the spaceship to the Moon is actually effective?

1:22:11.340 --> 1:22:12.340
 Correct.

1:22:12.340 --> 1:22:21.340
 And people who criticize GPT, they say, you are just building a ladder and it will never

1:22:21.340 --> 1:22:23.580
 reach the Moon.

1:22:23.580 --> 1:22:29.780
 And at the moment, I would say the way I'm thinking is like a scientific question.

1:22:29.780 --> 1:22:33.780
 And I'm also in heart, I'm a builder creator.

1:22:33.780 --> 1:22:38.260
 And I'm thinking, let's try out, let's see how far it goes.

1:22:38.260 --> 1:22:42.820
 And so far, we see constantly that there is a progress.

1:22:42.820 --> 1:22:55.100
 So do you think GPT 4, GPT 5, GPT N plus 1, there will be a phase shift, like a transition

1:22:55.100 --> 1:22:58.220
 to a place where we'll be truly surprised.

1:22:58.220 --> 1:23:02.380
 And again, GPT 3 is already very truly surprising.

1:23:02.380 --> 1:23:08.220
 The people that criticize GPT 3 as a Stairway, as a ladder to Heaven, I think two people

1:23:08.220 --> 1:23:13.020
 quickly get accustomed to how impressive it is that the prediction of the next word can

1:23:13.020 --> 1:23:22.020
 achieve such depth of semantics, accuracy of syntax, grammar, and semantics.

1:23:22.020 --> 1:23:28.060
 Do you think GPT 4 and 5 and 6 will continue to surprise us?

1:23:28.060 --> 1:23:31.060
 I mean, definitely, there will be more impressive models.

1:23:31.060 --> 1:23:35.220
 There is a question, of course, if there will be a phase shift.

1:23:35.220 --> 1:23:44.340
 And also even the way I'm thinking about these models is that when we build these models,

1:23:44.340 --> 1:23:48.940
 we see some level of the capabilities, but we don't even fully understand everything

1:23:48.940 --> 1:23:50.380
 that the model can do.

1:23:50.380 --> 1:23:56.140
 And actually one of the best things to do is to allow other people to probe the model

1:23:56.140 --> 1:23:59.100
 to even see what is possible.

1:23:59.100 --> 1:24:05.420
 Since using GPT as an API and opening it up to the world.

1:24:05.420 --> 1:24:06.420
 Yeah.

1:24:06.420 --> 1:24:12.420
 I mean, so when I'm thinking from perspective of obviously various people that have concerns

1:24:12.420 --> 1:24:18.260
 about AGI, including myself, and then when I'm thinking from perspective what's the strategy

1:24:18.260 --> 1:24:23.820
 even to deploy these things to the world, then the one strategy that I have seen many

1:24:23.820 --> 1:24:30.140
 times working is the iterative deployment that you deploy slightly better versions

1:24:30.140 --> 1:24:32.740
 and you allow other people to criticize you.

1:24:32.740 --> 1:24:37.500
 So you actually or try it out, you see where are their fundamental issues.

1:24:37.500 --> 1:24:44.620
 And it's almost you don't want to be in that situation that you are holding into powerful

1:24:44.620 --> 1:24:50.220
 system and there's like a huge overhang, then you deploy it and it might have a random chaotic

1:24:50.220 --> 1:24:51.220
 impact on the world.

1:24:51.220 --> 1:24:56.700
 So you actually want to be in the situation that you are gradually deploying systems.

1:24:56.700 --> 1:25:01.540
 I asked this question of Ilya, let me ask you this question.

1:25:01.540 --> 1:25:10.300
 I've been reading a lot about Stalin and power.

1:25:10.300 --> 1:25:17.380
 If you're in possession of a system that's like AGI, that's exceptionally powerful.

1:25:17.380 --> 1:25:21.940
 Do you think your character and integrity might become corrupted?

1:25:21.940 --> 1:25:25.980
 Like famously power corrupts and absolutely power corrupts, absolutely.

1:25:25.980 --> 1:25:32.980
 So I believe that you want at some point to work toward distributing the power.

1:25:32.980 --> 1:25:39.020
 I think that you want to be in the situation that actually AGI is not controlled by a small

1:25:39.020 --> 1:25:45.220
 number of people, but essentially by a larger collective.

1:25:45.220 --> 1:25:52.580
 So the thing is that requires a George Washington style move in the ascent to power.

1:25:52.580 --> 1:25:59.660
 There's always a moment when somebody gets a lot of power and they have to have the integrity

1:25:59.660 --> 1:26:03.740
 and the moral compass to give away that power.

1:26:03.740 --> 1:26:08.980
 That humans have been good and bad throughout history at this particular step.

1:26:08.980 --> 1:26:17.780
 And I wonder, I wonder we like blind ourselves in, for example, between nations, a race towards

1:26:17.780 --> 1:26:24.980
 the AI race between nations, we might blind ourselves and justify to ourselves the development

1:26:24.980 --> 1:26:30.540
 of AI without distributing the power because we want to defend ourselves against China,

1:26:30.540 --> 1:26:34.500
 against Russia, that kind of logic.

1:26:34.500 --> 1:26:45.300
 And I wonder how we design governance mechanisms that prevent us from becoming power hungry

1:26:45.300 --> 1:26:48.460
 and in the process destroying ourselves.

1:26:48.460 --> 1:26:49.460
 So let's see.

1:26:49.460 --> 1:26:54.700
 I have been thinking about this topic quite a bit, but I also want to admit that once

1:26:54.700 --> 1:27:01.220
 again I actually want to rely way more on some outman, on a hero and excellent block

1:27:01.220 --> 1:27:04.900
 on how even to distribute wealth.

1:27:04.900 --> 1:27:12.700
 And he proposed in his blog to tax equity of the companies rather than profit and to

1:27:12.700 --> 1:27:13.700
 distribute it.

1:27:13.700 --> 1:27:19.060
 And this is an example of Washington move.

1:27:19.060 --> 1:27:24.940
 I guess I personally have insane trust in some.

1:27:24.940 --> 1:27:34.420
 He already spent plenty of money running universal basic income project that gives me, I guess,

1:27:34.420 --> 1:27:41.500
 maybe some level of trust to him, but I also, I guess, love him as a friend.

1:27:41.500 --> 1:27:42.500
 Yeah.

1:27:42.500 --> 1:27:48.380
 I wonder, because we're sort of summoning a new set of technologies, I wonder if we'll

1:27:48.380 --> 1:27:55.660
 be cognizant, like you're describing the process of open AI, but it could also be at other

1:27:55.660 --> 1:27:59.780
 places like in the US government, right?

1:27:59.780 --> 1:28:07.260
 Both China and the US are now full steam ahead on autonomous weapons systems development.

1:28:07.260 --> 1:28:14.020
 And that's really worrying to me because in the framework of something being a national

1:28:14.020 --> 1:28:21.100
 security danger or a military danger, you can do a lot of pretty dark things that blind

1:28:21.100 --> 1:28:23.580
 our moral compass.

1:28:23.580 --> 1:28:26.660
 And I think AI will be one of those things.

1:28:26.660 --> 1:28:32.740
 In some sense, the mission and the work you're doing at open AI is like the counterbalance

1:28:32.740 --> 1:28:33.740
 to that.

1:28:33.740 --> 1:28:38.500
 So you want to have more open AI and less autonomous weapons systems.

1:28:38.500 --> 1:28:40.180
 I like these statements.

1:28:40.180 --> 1:28:45.460
 To be clear, this is interesting, and I'm thinking about it myself, but this is a place

1:28:45.460 --> 1:28:52.540
 that I put my trust actually in some sense, because it's extremely hard for me to reason

1:28:52.540 --> 1:28:53.540
 about it.

1:28:53.540 --> 1:28:54.540
 Yeah.

1:28:54.540 --> 1:28:59.020
 I mean, one important statement to make is it's good to think about this.

1:28:59.020 --> 1:29:00.020
 Yeah.

1:29:00.020 --> 1:29:01.020
 No question about it.

1:29:01.020 --> 1:29:02.020
 Right.

1:29:02.020 --> 1:29:06.380
 So even like low level, quote unquote, engineer.

1:29:06.380 --> 1:29:14.980
 Like there's such a, I remember I programmed a car, our RC car.

1:29:14.980 --> 1:29:18.700
 They went really fast, like 30, 40 miles an hour.

1:29:18.700 --> 1:29:25.500
 And I remember I was like sleep deprived, so I programmed it pretty crappily, and like

1:29:25.500 --> 1:29:26.500
 the code froze.

1:29:26.500 --> 1:29:30.620
 So it's doing some basic computer vision and it's going around on track, but it's going

1:29:30.620 --> 1:29:32.860
 full speed.

1:29:32.860 --> 1:29:40.140
 And there was a bug in the code that the car just went, it didn't turn, it went straight,

1:29:40.140 --> 1:29:42.140
 full speed and smash into the wall.

1:29:42.140 --> 1:29:50.340
 And I remember thinking the seriousness with which you need to approach the design of artificial

1:29:50.340 --> 1:29:56.380
 intelligence systems and the programming of artificial intelligence systems is high because

1:29:56.380 --> 1:29:58.380
 the consequences are high.

1:29:58.380 --> 1:30:03.140
 That little car smashing into the wall, for some reason, I immediately thought of like

1:30:03.140 --> 1:30:07.420
 an algorithm that controls nuclear weapons, having the same kind of bug.

1:30:07.420 --> 1:30:13.740
 And so like the lowest level engineer and the CEO of a company all need to have the seriousness

1:30:13.740 --> 1:30:17.340
 in approaching this problem and thinking about the worst case consequences.

1:30:17.340 --> 1:30:18.780
 So I think that is true.

1:30:18.780 --> 1:30:26.060
 I mean, what I also recognize in myself and others even asking this question is that it

1:30:26.060 --> 1:30:32.420
 evokes a lot of fear and the fear itself ends up being actually quite deabilitating.

1:30:32.420 --> 1:30:42.380
 The place where I arrived at the moment might sound cheesy or so, but it's almost to build

1:30:42.380 --> 1:30:45.860
 things out of love rather than fear.

1:30:45.860 --> 1:30:53.780
 I can focus on how I can maximize the value, how the systems that I'm building might be

1:30:53.780 --> 1:30:57.020
 useful.

1:30:57.020 --> 1:31:02.460
 I'm not saying that the fear doesn't exist out there and it totally makes sense to minimize

1:31:02.460 --> 1:31:03.460
 it.

1:31:03.460 --> 1:31:06.780
 But I don't want to be working because I'm scared.

1:31:06.780 --> 1:31:13.380
 I want to be working out of passion, out of curiosity, out of looking forward for the

1:31:13.380 --> 1:31:15.140
 positive future.

1:31:15.140 --> 1:31:21.900
 With the definition of love arising from rigorous practice of empathy, so not just like your

1:31:21.900 --> 1:31:26.700
 own conception of what is good for the world, but always listening to others.

1:31:26.700 --> 1:31:27.700
 Correct.

1:31:27.700 --> 1:31:32.140
 Like at the love where I'm considering reward functions of others.

1:31:32.140 --> 1:31:38.300
 To limit infinity is like one to N where N is 7 billion or whatever it is.

1:31:38.300 --> 1:31:40.420
 Not projecting my reward functions on others.

1:31:40.420 --> 1:31:41.420
 Yeah, exactly.

1:31:41.420 --> 1:31:42.420
 Okay.

1:31:42.420 --> 1:31:48.660
 Can we just take a step back to something else super cool, which is Open AI Codex?

1:31:48.660 --> 1:31:55.380
 Can you give an overview of what Open AI Codex and GitHub copilot is, how it works and why

1:31:55.380 --> 1:31:58.140
 the hell it works so well?

1:31:58.140 --> 1:32:05.980
 So with GPT3, we noticed that the system train on all the language out there started having

1:32:05.980 --> 1:32:08.460
 some rudimentary coding capabilities.

1:32:08.460 --> 1:32:16.020
 So we're able to implement addition functions between two numbers and indeed it can write

1:32:16.020 --> 1:32:18.260
 Python or JavaScript code for that.

1:32:18.260 --> 1:32:24.260
 And then we thought we might as well just go full steam ahead and try to create a system

1:32:24.260 --> 1:32:30.460
 that is actually good at what we are doing every day ourselves, which is programming.

1:32:30.460 --> 1:32:34.660
 We optimize models for proficiency in coding.

1:32:34.660 --> 1:32:41.940
 We actually even created models that both have a comprehension of language and code.

1:32:41.940 --> 1:32:45.740
 And Codex is API for these models.

1:32:45.740 --> 1:32:52.860
 So it's first pre trained on language and then, I don't know if you can say fine tuned

1:32:52.860 --> 1:32:56.540
 because there's a lot of code, but it's language and code.

1:32:56.540 --> 1:32:58.460
 It's language and code.

1:32:58.460 --> 1:33:03.060
 It's also optimized for various things like let's say low latency and so on.

1:33:03.060 --> 1:33:06.100
 Codex is the API that's similar to GPT3.

1:33:06.100 --> 1:33:10.820
 We expect that there will be proliferation of the potential products that can use coding

1:33:10.820 --> 1:33:15.380
 capabilities and I can speak about it in a second.

1:33:15.380 --> 1:33:18.260
 Copilot is a first product developed by GitHub.

1:33:18.260 --> 1:33:23.860
 So as we're building models, we wanted to make sure that these models are useful and

1:33:23.860 --> 1:33:27.780
 we work together with GitHub on building the first product.

1:33:27.780 --> 1:33:32.380
 Copilot is actually, as you code, it suggests you code completions.

1:33:32.380 --> 1:33:37.500
 And we have seen in the past, there are like various tools that can suggest how to like

1:33:37.500 --> 1:33:41.700
 a few characters of the code or the line of code.

1:33:41.700 --> 1:33:46.460
 The thing about Copilot is it can generate 10 lines of code.

1:33:46.460 --> 1:33:50.740
 It's often the way how it works is you often write in the comment what you want to happen

1:33:50.740 --> 1:33:54.500
 because people in comments, they describe what happens next.

1:33:54.500 --> 1:34:01.540
 So these days when I code, instead of going to Google to search for the appropriate code

1:34:01.540 --> 1:34:07.740
 to solve my problem, I say, oh, for this array, could you smooth it?

1:34:07.740 --> 1:34:13.340
 And then it imports some appropriate libraries and say it uses NumPy convolution or so that

1:34:13.340 --> 1:34:15.300
 I was not even aware that exists.

1:34:15.300 --> 1:34:18.300
 Many does the appropriate thing.

1:34:18.300 --> 1:34:23.060
 So you write a comment, maybe the header of a function and it completes the function.

1:34:23.060 --> 1:34:28.140
 Of course, you don't know what is the space of all the possible small programs it can

1:34:28.140 --> 1:34:29.140
 generate.

1:34:29.140 --> 1:34:30.740
 What are the failure cases?

1:34:30.740 --> 1:34:32.180
 How many edge cases?

1:34:32.180 --> 1:34:34.300
 How many subtle errors there are?

1:34:34.300 --> 1:34:35.980
 How many big errors there are?

1:34:35.980 --> 1:34:41.380
 It's hard to know, but the fact that it works at all in a large number of cases is incredible.

1:34:41.380 --> 1:34:47.660
 It's like a kind of search engine into code that's been written on the Internet.

1:34:47.660 --> 1:34:48.660
 Correct.

1:34:48.660 --> 1:34:55.820
 So for instance, when you search things online, then usually you get to some particular case

1:34:55.820 --> 1:35:02.220
 like if you go to Stack Overflow, people describe that one particular situation and then they

1:35:02.220 --> 1:35:03.380
 seek for a solution.

1:35:03.380 --> 1:35:09.180
 But in case of Copilot, it's aware of your entire context and in context is, oh, these

1:35:09.180 --> 1:35:10.940
 are the libraries that they are using.

1:35:10.940 --> 1:35:14.580
 That's the set of the variables that is initialized.

1:35:14.580 --> 1:35:17.620
 And on the spot, it can actually tell you what to do.

1:35:17.620 --> 1:35:22.460
 So the interesting thing is, and we think that the Copilot is one possible product

1:35:22.460 --> 1:35:25.420
 using Codex, but there is a place for many more.

1:35:25.420 --> 1:35:30.060
 So internally, we tried out to create other fun products.

1:35:30.060 --> 1:35:35.540
 So it turns out that a lot of tools out there, let's say Google Calendar or Microsoft Word

1:35:35.540 --> 1:35:41.580
 or so, they all have internal API to build plugins around them.

1:35:41.580 --> 1:35:47.980
 So there is a way in the sophisticated way to control Calendar or Microsoft Word.

1:35:47.980 --> 1:35:52.780
 Today if you want more complicated behaviors from these programs, you have to add a new

1:35:52.780 --> 1:35:55.460
 button for every behavior.

1:35:55.460 --> 1:36:01.900
 But it is possible to use Codex and tell, for instance, to Calendar, could you schedule

1:36:01.900 --> 1:36:07.900
 an appointment with Lex next week after 2 p.m. and either write corresponding piece

1:36:07.900 --> 1:36:09.900
 of code?

1:36:09.900 --> 1:36:11.540
 And that's the thing that actually you want.

1:36:11.540 --> 1:36:12.540
 So interesting.

1:36:12.540 --> 1:36:16.540
 So what you figure out is there's a lot of programs with which you can interact through

1:36:16.540 --> 1:36:17.540
 code.

1:36:17.540 --> 1:36:23.300
 And so there, you can generate that code from natural language.

1:36:23.300 --> 1:36:24.300
 That's fascinating.

1:36:24.300 --> 1:36:30.020
 That's somewhat like also closest to what was the promise of Siri or Alexa.

1:36:30.020 --> 1:36:33.860
 So previously, all these behaviors, they were hand hard coded.

1:36:33.860 --> 1:36:40.480
 And it seems that Codex on the fly can pick up the API of, let's say, given software.

1:36:40.480 --> 1:36:43.380
 And then it can turn language into use of this API.

1:36:43.380 --> 1:36:47.780
 So without hard coding, it can translate to machine language.

1:36:47.780 --> 1:36:48.780
 Correct.

1:36:48.780 --> 1:36:55.860
 So for example, this would be really exciting for me, like for Adobe products like Photoshop,

1:36:55.860 --> 1:37:00.020
 which I think ActionScript, I think there's a scripting language that communicates with

1:37:00.020 --> 1:37:01.020
 them.

1:37:01.020 --> 1:37:02.020
 Same with Premiere.

1:37:02.020 --> 1:37:07.700
 And do you could imagine that that allows even to do coding by voice on your phone?

1:37:07.700 --> 1:37:14.220
 So for instance, in the past, as of today, I'm not editing Word documents on my phone

1:37:14.220 --> 1:37:16.660
 because it's just the keyboard is too small.

1:37:16.660 --> 1:37:23.340
 But if I would be able to tell to my phone, you know, make the header large, then move

1:37:23.340 --> 1:37:27.020
 the paragraphs around, and it does actually what I want.

1:37:27.020 --> 1:37:31.860
 So I can tell you one more cool thing, or even how I'm thinking about Codex.

1:37:31.860 --> 1:37:39.660
 So if you look actually at the evolution of computers, we started with very primitive

1:37:39.660 --> 1:37:41.460
 interfaces, which is a punch card.

1:37:41.460 --> 1:37:49.340
 And punch card, essentially, you make holes in the plastic card to indicate zeros and ones.

1:37:49.340 --> 1:37:54.380
 And during that time, there was a small number of specialists who were able to use computers.

1:37:54.380 --> 1:37:57.860
 And by the way, people even suspected that there is no need for many more people to use

1:37:57.860 --> 1:38:00.020
 computers.

1:38:00.020 --> 1:38:06.780
 But then we moved from punch cards to, at first, assembly and C. And these programming

1:38:06.780 --> 1:38:09.460
 languages, they were slightly higher level.

1:38:09.460 --> 1:38:11.980
 They allowed many more people to code.

1:38:11.980 --> 1:38:16.540
 And they also led to more of a proliferation of technology.

1:38:16.540 --> 1:38:22.540
 And further on, there was a jump to, say, from C++ to Java and Python.

1:38:22.540 --> 1:38:28.740
 And every time it has happened, more people are able to code, and we build more technology.

1:38:28.740 --> 1:38:34.780
 And it's even hard to imagine now if someone will tell you that you should write code in

1:38:34.780 --> 1:38:39.660
 assembly instead of, let's say, Python or Java or JavaScript.

1:38:39.660 --> 1:38:44.780
 And Codex is yet another step toward kind of bringing computers closer to humans, such

1:38:44.780 --> 1:38:50.660
 that you communicate with a computer with your own language, rather than with a specialized

1:38:50.660 --> 1:38:51.820
 language.

1:38:51.820 --> 1:38:57.500
 And I think that it will lead to increase of number of people who can code.

1:38:57.500 --> 1:38:58.500
 Yeah.

1:38:58.500 --> 1:39:04.380
 And the kind of technologies that those people will create, it's innumerable, it could be

1:39:04.380 --> 1:39:10.140
 a huge number of technologies we're not predicting at all, because that's less and less requirement

1:39:10.140 --> 1:39:14.620
 of having a technical mind, a programming mind.

1:39:14.620 --> 1:39:21.340
 You're not opening it to the world of other kinds of minds, creative minds, artistic minds,

1:39:21.340 --> 1:39:22.340
 all that kind of stuff.

1:39:22.340 --> 1:39:26.860
 I would like, for instance, biologists who work on DNA to be able to program and not

1:39:26.860 --> 1:39:29.620
 to need to spend a lot of time learning it.

1:39:29.620 --> 1:39:32.020
 And I believe that's a good thing to the world.

1:39:32.020 --> 1:39:33.740
 And I would actually add that.

1:39:33.740 --> 1:39:38.540
 So at the moment, I'm a managing Codex team and also language team.

1:39:38.540 --> 1:39:43.380
 And I believe that there is like a plenty of brilliant people out there, and they should

1:39:43.380 --> 1:39:44.380
 apply.

1:39:44.380 --> 1:39:45.380
 Oh, okay.

1:39:45.380 --> 1:39:46.380
 Yeah.

1:39:46.380 --> 1:39:47.380
 Awesome.

1:39:47.380 --> 1:39:48.380
 So what's the language in the Codex?

1:39:48.380 --> 1:39:52.980
 So those are kind of their overlapping teams, it's like GPT, the raw language.

1:39:52.980 --> 1:39:56.740
 And then the Codex is like applied to programming.

1:39:56.740 --> 1:39:57.740
 Correct.

1:39:57.740 --> 1:40:00.140
 And they are quite intertwined.

1:40:00.140 --> 1:40:06.100
 There are many more teams involved making these models extremely efficient and deployable.

1:40:06.100 --> 1:40:12.700
 For instance, there are people who are working to make our data centers amazing, or there

1:40:12.700 --> 1:40:19.060
 are people who work on putting these models into production, or even pushing it at the

1:40:19.060 --> 1:40:21.740
 very limit of the scale.

1:40:21.740 --> 1:40:25.300
 So all aspects from the infrastructure to the actual machine learning?

1:40:25.300 --> 1:40:31.060
 So I'm just saying there are multiple teams, while the team working on Codex and language,

1:40:31.060 --> 1:40:34.140
 I guess, I'm directly managing them.

1:40:34.140 --> 1:40:35.140
 I would love to hire.

1:40:35.140 --> 1:40:36.140
 Yeah.

1:40:36.140 --> 1:40:41.660
 If you're interested in machine learning, this is probably one of the most exciting problems

1:40:41.660 --> 1:40:46.220
 and like systems to be working on, because it's actually, it's pretty cool.

1:40:46.220 --> 1:40:51.740
 Like what the program said, this is like generating a program, it's a very interesting, very interesting

1:40:51.740 --> 1:40:58.340
 problem that has echoes of reasoning and intelligence in it.

1:40:58.340 --> 1:41:05.460
 And I think there's a lot of fundamental questions that you might be able to sneak up to by generating

1:41:05.460 --> 1:41:06.460
 programs.

1:41:06.460 --> 1:41:07.460
 Yeah.

1:41:07.460 --> 1:41:12.300
 One more exciting thing about the programs is that, so I said that the, you know, the

1:41:12.300 --> 1:41:16.060
 in case of language, that one of the troubles is even evaluating language.

1:41:16.060 --> 1:41:23.140
 So when the things are made up, you need somehow either a human to say that this doesn't make

1:41:23.140 --> 1:41:24.140
 sense.

1:41:24.140 --> 1:41:28.260
 Or so in case of program, there is one extra lever that we can actually execute programs

1:41:28.260 --> 1:41:30.580
 and see what they evaluate to.

1:41:30.580 --> 1:41:38.900
 So the process might be somewhat more automated in order to improve the qualities of generations.

1:41:38.900 --> 1:41:40.180
 Oh, that's fascinating.

1:41:40.180 --> 1:41:43.340
 So like the, wow, that's really interesting.

1:41:43.340 --> 1:41:49.220
 So for language, the, you know, the simulation to actually execute it as a human mind.

1:41:49.220 --> 1:41:54.340
 For programs, there is a, there is a computer on which you can evaluate it.

1:41:54.340 --> 1:41:56.060
 Wow.

1:41:56.060 --> 1:42:02.820
 That's a brilliant little insight that the thing compiles and runs.

1:42:02.820 --> 1:42:04.180
 That's first.

1:42:04.180 --> 1:42:09.660
 And second, you can evaluate on a do automated unit testing.

1:42:09.660 --> 1:42:14.020
 And in some sense, it seems to mean that we'll be able to make a tremendous progress.

1:42:14.020 --> 1:42:21.100
 You know, we are in the paradigm that there is way more data and there is like a transcription

1:42:21.100 --> 1:42:24.860
 of millions of, of software engineers.

1:42:24.860 --> 1:42:25.860
 Yeah.

1:42:25.860 --> 1:42:26.860
 Yeah.

1:42:26.860 --> 1:42:32.140
 So, I mean, you just mean because I was going to ask you about reliability, the thing about

1:42:32.140 --> 1:42:38.620
 programs is you don't know if they're going to like a program that's controlling a nuclear

1:42:38.620 --> 1:42:41.580
 power plant has to be very reliable.

1:42:41.580 --> 1:42:46.260
 So I wouldn't start with controlling nuclear power plant, maybe one day, but that's not

1:42:46.260 --> 1:42:48.860
 actually, that's not on the current roadmap.

1:42:48.860 --> 1:42:52.860
 That's not the step one, you know, it's the Russian thing.

1:42:52.860 --> 1:42:57.780
 You just want to go to the most powerful destructive thing right away run by JavaScript.

1:42:57.780 --> 1:42:58.780
 But I got you.

1:42:58.780 --> 1:42:59.780
 So it's a lower impact.

1:42:59.780 --> 1:43:04.380
 But nevertheless, what you make me realize it is possible to achieve some levels of reliability

1:43:04.380 --> 1:43:06.740
 by doing testing.

1:43:06.740 --> 1:43:11.860
 You could imagine that, you know, maybe there are ways for a model to write even code for

1:43:11.860 --> 1:43:14.140
 testing itself and so on.

1:43:14.140 --> 1:43:21.500
 And there exist ways to create the feedback loops that the model could keep on improving.

1:43:21.500 --> 1:43:27.180
 By writing programs that generate tests, for instance, for instance.

1:43:27.180 --> 1:43:30.860
 And that's how we get consciousness because it's meta compression.

1:43:30.860 --> 1:43:31.860
 That's what you're going to write.

1:43:31.860 --> 1:43:32.860
 That's the comment.

1:43:32.860 --> 1:43:36.980
 That's the prompt that generates consciousness, compressor of compressors.

1:43:36.980 --> 1:43:37.980
 You just write that.

1:43:37.980 --> 1:43:42.580
 Do you think the code that generates consciousness would be simple?

1:43:42.580 --> 1:43:48.740
 So let's see, I mean, ultimately, the core idea behind will be simple, but there will

1:43:48.740 --> 1:43:54.180
 be also decent amount of engineering involved.

1:43:54.180 --> 1:44:02.820
 In some sense, it seems that spreading these models on many machines, it's not that trivial.

1:44:02.820 --> 1:44:08.660
 And we find all sorts of innovations that make our models more efficient.

1:44:08.660 --> 1:44:14.980
 I believe that first models that I guess are conscious are truly intelligent.

1:44:14.980 --> 1:44:19.260
 They will have all sorts of tricks.

1:44:19.260 --> 1:44:25.380
 But then again, there's a certain argument that maybe the tricks are temporary things.

1:44:25.380 --> 1:44:26.900
 Yeah, they might be temporary things.

1:44:26.900 --> 1:44:34.260
 And in some sense, it's also even important to know that even the cost of a trick.

1:44:34.260 --> 1:44:39.820
 So sometimes, people are eager to put the trick while forgetting that there is a cost

1:44:39.820 --> 1:44:40.820
 of maintenance.

1:44:40.820 --> 1:44:43.140
 Or like a long term cost.

1:44:43.140 --> 1:44:48.980
 Long term cost or maintenance or maybe even flexibility of code to actually implement

1:44:48.980 --> 1:44:49.980
 new idea.

1:44:49.980 --> 1:44:55.060
 So even if you have something that gives you 2X, but it requires 1,000 lines of code,

1:44:55.060 --> 1:44:57.060
 I'm not sure if it's actually worth it.

1:44:57.060 --> 1:45:03.140
 So in some sense, if it's 5 lines of code and 2X, I would take it.

1:45:03.140 --> 1:45:10.900
 And we see many of this, but also that requires some level of, I guess, lack of attachment

1:45:10.900 --> 1:45:15.900
 to code that we are willing to remove it.

1:45:15.900 --> 1:45:18.940
 So you led the OpenAI robotics team.

1:45:18.940 --> 1:45:22.740
 Can you give an overview of the cool things you're able to accomplish, what are you most

1:45:22.740 --> 1:45:24.140
 proud of?

1:45:24.140 --> 1:45:27.460
 So when we started robotics, we knew that actually reinforcement learning works.

1:45:27.460 --> 1:45:31.620
 And it is possible to solve fairly complicated problems.

1:45:31.620 --> 1:45:37.940
 Like for instance, AlphaGo is an evidence that it is possible to build superhuman and

1:45:37.940 --> 1:45:38.940
 go players.

1:45:38.940 --> 1:45:47.060
 Dota2 is an evidence that it's possible to build superhuman agents playing Dota.

1:45:47.060 --> 1:45:50.740
 So I asked myself a question, you know, what about robots out there?

1:45:50.740 --> 1:45:55.820
 Would we train machines to solve arbitrary tasks in the physical world?

1:45:55.820 --> 1:46:02.340
 Our approach was, I guess, let's pick a complicated problem that if we would solve it, that means

1:46:02.340 --> 1:46:08.420
 that we made some significant progress in the domain, and then we went after the problem.

1:46:08.420 --> 1:46:14.340
 So we noticed that actually the robots out there, they are kind of at the moment optimized

1:46:14.340 --> 1:46:15.340
 per task.

1:46:15.340 --> 1:46:20.380
 So you can have a robot that if you have a robot opening a battle, it's very likely

1:46:20.380 --> 1:46:24.340
 that the end factor is a battle opener.

1:46:24.340 --> 1:46:29.940
 And in some sense, that's a hack to be able to solve a task, which makes any task easier.

1:46:29.940 --> 1:46:35.620
 And I asked myself, so what would be a robot that can actually solve many tasks?

1:46:35.620 --> 1:46:43.180
 And we concluded that human hands have such a quality that indeed they are, you know,

1:46:43.180 --> 1:46:50.820
 you have five kind of tiny arms attached individually, they can manipulate pretty broad spectrum

1:46:50.820 --> 1:46:51.980
 of objects.

1:46:51.980 --> 1:46:57.740
 So we went after a single hand, like a trying to solve Rubik's Cube single handed, we picked

1:46:57.740 --> 1:47:02.220
 this task because we thought that there is no way to hard code it.

1:47:02.220 --> 1:47:06.180
 And also we picked a robot on which it would be hard to hard code it.

1:47:06.180 --> 1:47:11.700
 And we went after the solution such that it could generalize to other problems.

1:47:11.700 --> 1:47:16.660
 And just to clarify, it's one robotic hand solving the Rubik's Cube.

1:47:16.660 --> 1:47:22.340
 The hard part is in the solution to the Rubik's Cube is the manipulation of the, of like having

1:47:22.340 --> 1:47:30.580
 it not fall out of the hand, having it use the five baby arms to what is it like rotate

1:47:30.580 --> 1:47:33.620
 different parts of the Rubik's Cube to achieve the solution.

1:47:33.620 --> 1:47:34.620
 Correct.

1:47:34.620 --> 1:47:35.620
 Yeah.

1:47:35.620 --> 1:47:38.820
 So what, what was the hardest part about that?

1:47:38.820 --> 1:47:40.580
 What was the approach taken there?

1:47:40.580 --> 1:47:42.100
 What are you most proud of?

1:47:42.100 --> 1:47:45.580
 Obviously we have like a strong belief in reinforcement learning.

1:47:45.580 --> 1:47:51.300
 And you know, one path, it is to do reinforcement learning, the real world.

1:47:51.300 --> 1:47:57.740
 Other path is to the simulation, in some sense, the tricky part about the real world is at

1:47:57.740 --> 1:48:02.740
 the moment our models, they require a lot of data, there is essentially no data.

1:48:02.740 --> 1:48:08.260
 And I think we decided to go through the path of the simulation and in simulation, you can

1:48:08.260 --> 1:48:10.140
 have infinite amount of data.

1:48:10.140 --> 1:48:13.180
 The tricky part is the fidelity of the simulation.

1:48:13.180 --> 1:48:19.420
 And also can you in simulation represent everything that you represent otherwise in the real world?

1:48:19.420 --> 1:48:24.420
 And you know, it turned out that, that, you know, because there is lack of fidelity, it

1:48:24.420 --> 1:48:31.820
 is possible to what we, what we arrived at is training a model that doesn't solve one

1:48:31.820 --> 1:48:38.300
 simulation, but it actually solves the entire range of simulations, which vary in terms of

1:48:38.300 --> 1:48:43.860
 like what's exactly the friction of the cube or the weight or so.

1:48:43.860 --> 1:48:49.500
 And the single AI that can solve all of them ends up working well with the reality.

1:48:49.500 --> 1:48:51.700
 How do you generate the different simulations?

1:48:51.700 --> 1:48:56.300
 So you know, there's plenty of parameters out there, we just pick them randomly.

1:48:56.300 --> 1:49:02.860
 And in simulation, model just goes for thousands of years and keeps on solving Rubik's Cube

1:49:02.860 --> 1:49:04.060
 in each of them.

1:49:04.060 --> 1:49:07.180
 And the thing is the neural network that we used.

1:49:07.180 --> 1:49:09.660
 It has a memory.

1:49:09.660 --> 1:49:16.420
 And as it presses, for instance, the side of the, of the cube, it can sense, oh, that's

1:49:16.420 --> 1:49:21.660
 actually this side was difficult to press, I should press it stronger.

1:49:21.660 --> 1:49:27.500
 And throughout this process, kind of learns even how to, how to solve this particular

1:49:27.500 --> 1:49:32.300
 instance of the Rubik's Cube back even mass, it's kind of like a, you know, sometimes when

1:49:32.300 --> 1:49:43.100
 you go to a gym and after, after bench press, you try to lift the glass and you kind of

1:49:43.100 --> 1:49:49.300
 forgot and, and, and your head goes like right away because kind of you got used to maybe

1:49:49.300 --> 1:49:53.300
 different weight and it takes a second to adjust.

1:49:53.300 --> 1:49:57.940
 And this kind of a, of a memory that model gained through the process of interacting

1:49:57.940 --> 1:49:59.980
 with the cube in the simulation.

1:49:59.980 --> 1:50:05.020
 I appreciate you speaking to the audience with a bench press, all the bros in the audience

1:50:05.020 --> 1:50:06.380
 probably working out right now.

1:50:06.380 --> 1:50:10.300
 There's probably somebody listening to this actually doing bench press.

1:50:10.300 --> 1:50:15.980
 So maybe put the bar down and pick up the water bottle and you'll know exactly what,

1:50:15.980 --> 1:50:17.980
 what Jack is talking about.

1:50:17.980 --> 1:50:18.980
 Okay.

1:50:18.980 --> 1:50:24.980
 So what, what was the hardest part of getting the whole thing to work?

1:50:24.980 --> 1:50:31.900
 You know, the hardest part is at the moment when it comes to physical work, when it comes

1:50:31.900 --> 1:50:35.260
 to robots, they require maintenance.

1:50:35.260 --> 1:50:38.340
 It's hard to replicate a million times.

1:50:38.340 --> 1:50:42.220
 It's, it's also, it's hard to replay things exactly.

1:50:42.220 --> 1:50:49.460
 I remember this situation that one guy at our company, he had like a model that performs

1:50:49.460 --> 1:50:53.500
 way better than other models in solving Rubik's Cube.

1:50:53.500 --> 1:50:59.300
 And you know, we kind of didn't know what's going on, why it's that.

1:50:59.300 --> 1:51:05.620
 And it turned out that, you know, he was running it from his laptop that had better

1:51:05.620 --> 1:51:10.700
 CPU or better, maybe local GPU as well.

1:51:10.700 --> 1:51:15.940
 And because of that, there was less of a latency and the model was the same.

1:51:15.940 --> 1:51:19.700
 And that actually made solving Rubik's Cube more reliable.

1:51:19.700 --> 1:51:24.100
 So in some sense, there might be some subtle bugs like that when it comes to running things

1:51:24.100 --> 1:51:27.740
 in the real world, even hinting on that.

1:51:27.740 --> 1:51:31.980
 You could imagine that the initial models, you would like to have models, which are insanely

1:51:31.980 --> 1:51:38.060
 huge neural networks, and you would like to give them even more time for thinking.

1:51:38.060 --> 1:51:44.020
 And when you have these real time systems, then you might be constrained actually by the

1:51:44.020 --> 1:51:46.220
 amount of latency.

1:51:46.220 --> 1:51:52.900
 And ultimately, I would like to build a system that it is worth for you to wait five minutes

1:51:52.900 --> 1:51:57.780
 because it gives you the answer that you're willing to wait for five minutes.

1:51:57.780 --> 1:52:01.380
 So latency is a very unpleasant constraint under which to operate.

1:52:01.380 --> 1:52:02.380
 Correct.

1:52:02.380 --> 1:52:06.740
 And also, there is actually one more thing which is tricky about robots.

1:52:06.740 --> 1:52:10.020
 There is actually no, not much data.

1:52:10.020 --> 1:52:16.340
 So the data that I'm speaking about would be a data of first person experience from the

1:52:16.340 --> 1:52:21.260
 robot, and like gigabytes of data like that, if we would have gigabytes of data like that

1:52:21.260 --> 1:52:26.500
 of robot solving parties problems, it would be very easy to make a progress on robotics.

1:52:26.500 --> 1:52:31.260
 And you can see that in case of text or code, there is a lot of data, like a first person

1:52:31.260 --> 1:52:34.180
 perspective data on writing code.

1:52:34.180 --> 1:52:40.780
 Yeah, so you had this, you mentioned this really interesting idea that if you were to

1:52:40.780 --> 1:52:45.900
 build like a successful robotics company, so OpenAS mission is much bigger than robotics.

1:52:45.900 --> 1:52:49.220
 This is one of the, one of the things you've worked on.

1:52:49.220 --> 1:52:54.820
 But if it was a robotics company, that you wouldn't so quickly dismiss supervised learning.

1:52:54.820 --> 1:53:04.660
 Correct, that you would build a robot that was perhaps like an empty shell like dumb

1:53:04.660 --> 1:53:07.140
 and they would operate under teleoperation.

1:53:07.140 --> 1:53:13.700
 So you would invest, that's just one way to do it, invest in human, like direct human

1:53:13.700 --> 1:53:16.500
 control of the robots as it's learning.

1:53:16.500 --> 1:53:19.580
 And over time, add more and more automation.

1:53:19.580 --> 1:53:20.580
 That's correct.

1:53:20.580 --> 1:53:23.900
 So let's say that's how I would build a robotics company today.

1:53:23.900 --> 1:53:29.820
 If I would be building robotics company, which is spend $10 million or so, recording human

1:53:29.820 --> 1:53:32.500
 trajectories, controlling a robot.

1:53:32.500 --> 1:53:38.700
 After you find a thing that the robot should be doing that there's a market fit for, like

1:53:38.700 --> 1:53:40.660
 that you can make a lot of money with that product.

1:53:40.660 --> 1:53:42.260
 Correct, correct.

1:53:42.260 --> 1:53:48.460
 So I would record data and then I would essentially train supervised learning model on it.

1:53:48.460 --> 1:53:50.660
 That might be the path today.

1:53:50.660 --> 1:53:57.700
 Look term, I think that actually what is needed is to train powerful models over video.

1:53:57.700 --> 1:54:04.180
 So you have seen maybe a models that can generate images like Dali.

1:54:04.180 --> 1:54:06.940
 And people are looking into models generating videos.

1:54:06.940 --> 1:54:10.780
 They're like, are these algorithmic questions, even how to do it?

1:54:10.780 --> 1:54:13.940
 And it's unclear if there is enough compute for this purpose.

1:54:13.940 --> 1:54:22.220
 But I suspect that the models that which would have a level of understanding of video, same

1:54:22.220 --> 1:54:29.420
 as GPT has a level of understanding of text, could be used to train robots to solve tasks.

1:54:29.420 --> 1:54:32.780
 They would have a lot of common sense.

1:54:32.780 --> 1:54:38.580
 If one day, I'm pretty sure one day, there will be a robotics company.

1:54:38.580 --> 1:54:49.020
 I mean, the primary source of income is from robots that is worth over $1 trillion.

1:54:49.020 --> 1:54:50.820
 What do you think that company would do?

1:54:50.820 --> 1:54:53.380
 I think sell driving cars, no.

1:54:53.380 --> 1:54:57.980
 It's interesting because my mind went to personal robotics, robots in the home.

1:54:57.980 --> 1:55:00.980
 It seems like there's much more market opportunity there.

1:55:00.980 --> 1:55:05.100
 I think it's very difficult to achieve.

1:55:05.100 --> 1:55:11.220
 I mean, this might speak to something important, which is I understand self driving much better

1:55:11.220 --> 1:55:13.380
 than I understand robotics in the home.

1:55:13.380 --> 1:55:19.580
 So I understand how difficult it is to actually solve self driving to a level, not just the

1:55:19.580 --> 1:55:24.540
 actual computer vision and the control problem and just the basic problem of self driving,

1:55:24.540 --> 1:55:32.660
 but creating a product that would undeniably be that would cost less money, like it would

1:55:32.660 --> 1:55:33.660
 save you a lot of money.

1:55:33.660 --> 1:55:37.900
 It would order the magnitude less money that could replace Uber drivers, for example.

1:55:37.900 --> 1:55:39.780
 So car sharing is autonomous.

1:55:39.780 --> 1:55:45.900
 It creates a similar or better experience in terms of how quickly you get from A to B

1:55:45.900 --> 1:55:51.020
 or just whatever, the pleasantness of the experience, the efficiency of the experience,

1:55:51.020 --> 1:55:56.780
 the value of the experience, and at the same time, the car itself costs cheaper.

1:55:56.780 --> 1:55:58.540
 I think that's very difficult to achieve.

1:55:58.540 --> 1:56:05.500
 I think there's a lot more low hanging fruit in the home.

1:56:05.500 --> 1:56:06.500
 That could be.

1:56:06.500 --> 1:56:12.580
 I also want to give you a perspective on how challenging it would be at home or maybe kind

1:56:12.580 --> 1:56:16.140
 of depends on the exact problem that you'd be solving.

1:56:16.140 --> 1:56:22.540
 If we're speaking about these robotic arms, and hence, these things, they cost tens of

1:56:22.540 --> 1:56:26.180
 thousands of dollars or maybe 100K.

1:56:26.180 --> 1:56:33.300
 And maybe, obviously, maybe there would be economy of scale, these things would be cheaper.

1:56:33.300 --> 1:56:38.780
 But actually, for any household to buy it, the price would have to go down to maybe 1,000

1:56:38.780 --> 1:56:39.780
 bucks.

1:56:39.780 --> 1:56:40.780
 Yeah.

1:56:40.780 --> 1:56:45.980
 I personally think that so self driving car provides a clear service.

1:56:45.980 --> 1:56:50.300
 I don't think robots in the home, there'll be a trillion dollar company will just be

1:56:50.300 --> 1:56:56.100
 all about service, meaning it will not necessarily be about like a robotic arm.

1:56:56.100 --> 1:57:04.100
 That helps you, I don't know, open a bottle or wash dishes or any of that kind of stuff.

1:57:04.100 --> 1:57:08.460
 It has to be able to take care of that whole, the therapist thing you mentioned.

1:57:08.460 --> 1:57:14.020
 I think that's, of course, there's a line between what is a robot and what is not.

1:57:14.020 --> 1:57:16.060
 Like, does it really need a body?

1:57:16.060 --> 1:57:21.940
 But AI system with some embodiment, I think.

1:57:21.940 --> 1:57:28.060
 So the tricky part when you think actually what's the difficult part is when the robot

1:57:28.060 --> 1:57:32.700
 has, like when there is a diversity of the environment with which the robot has to interact,

1:57:32.700 --> 1:57:33.700
 that becomes hard.

1:57:33.700 --> 1:57:39.340
 So, you know, on one spectrum, you have industrial robots, as they are doing over and over the

1:57:39.340 --> 1:57:44.140
 same thing, it is possible to some extent to prescribe the movements.

1:57:44.140 --> 1:57:50.940
 And with very small amount of intelligence, the movement can be repeated millions of times.

1:57:50.940 --> 1:57:55.220
 And there are also, you know, various pieces of industrial robots where it becomes harder

1:57:55.220 --> 1:57:56.220
 and harder.

1:57:56.220 --> 1:58:03.140
 Like, for instance, in case of Tesla, maybe a matter of putting a rack inside of a car.

1:58:03.140 --> 1:58:08.580
 And you know, because the rack kind of moves their own, it's not that easy, it's not exactly

1:58:08.580 --> 1:58:09.580
 the same every time.

1:58:09.580 --> 1:58:13.740
 It ends up being the case that you need actually humans to do it.

1:58:13.740 --> 1:58:18.540
 While, you know, welding cars together, it's a very repetitive process.

1:58:18.540 --> 1:58:26.860
 And in case of self driving itself, the difficulty has to do with the diversity of the environment.

1:58:26.860 --> 1:58:33.620
 But still the car itself, the problem that you are solving is you try to avoid even interacting

1:58:33.620 --> 1:58:34.620
 with things.

1:58:34.620 --> 1:58:38.140
 You are not touching anything around, because touching itself is hard.

1:58:38.140 --> 1:58:43.180
 And then if you would have in the home robot that, you know, has to touch things and like

1:58:43.180 --> 1:58:47.780
 if these things, they change the shape, if there is a huge variety of things to be touched,

1:58:47.780 --> 1:58:48.780
 then that's difficult.

1:58:48.780 --> 1:58:52.620
 If you are speaking about the robot, which there is, you know, head, that it's smiling

1:58:52.620 --> 1:58:57.500
 in some way with cameras, that it doesn't, you know, touch things, that's relatively

1:58:57.500 --> 1:58:58.500
 simple.

1:58:58.500 --> 1:58:59.620
 Okay.

1:58:59.620 --> 1:59:02.980
 So to both agree and to push back.

1:59:02.980 --> 1:59:09.820
 So you're referring to touch like soft robotics, like the actual touch.

1:59:09.820 --> 1:59:17.780
 But I would argue that you could formulate just basic interaction between like non contact

1:59:17.780 --> 1:59:20.340
 interaction is also a kind of touch.

1:59:20.340 --> 1:59:21.980
 And that might be very difficult to solve.

1:59:21.980 --> 1:59:26.940
 That's the basic this, not disagreement, but that's the basic open question to me with

1:59:26.940 --> 1:59:32.700
 self driving cars and disagreement with Elon, which is how much interaction is required

1:59:32.700 --> 1:59:36.060
 to solve self driving cars, how much touch is required.

1:59:36.060 --> 1:59:42.660
 You said that in your intuition, touch is not required in my intuition to create a product

1:59:42.660 --> 1:59:48.540
 that's compelling to use, you're going to have to interact with pedestrians, not just

1:59:48.540 --> 1:59:51.780
 avoid pedestrians, but interact with them.

1:59:51.780 --> 1:59:56.980
 When we drive around in major cities, we're constantly threatening everybody's life with

1:59:56.980 --> 1:59:59.420
 our movements.

1:59:59.420 --> 2:00:00.820
 And that's how they respect us.

2:00:00.820 --> 2:00:03.900
 There's a game theory going on with pedestrians.

2:00:03.900 --> 2:00:11.900
 And I'm afraid you can't just formulate autonomous driving as a collision avoidance problem.

2:00:11.900 --> 2:00:17.620
 So I think it goes beyond like a collision avoidance is the first order approximation.

2:00:17.620 --> 2:00:23.740
 But then at least in case of Tesla, they are gathering data from people driving their cars.

2:00:23.740 --> 2:00:28.220
 And I believe that's an example of supervised learning data that they can train their models

2:00:28.220 --> 2:00:38.140
 on, and they are doing it, which can give a model this like another level of behavior

2:00:38.140 --> 2:00:41.020
 that is needed to actually interact with the real world.

2:00:41.020 --> 2:00:45.700
 Yeah, it's interesting how much data is required to achieve that.

2:00:45.700 --> 2:00:51.780
 What do you think of the whole Tesla autopilot approach, the computer vision based approach

2:00:51.780 --> 2:00:57.380
 with multiple cameras and as a data engine, it's a multi task, multi headed neural network,

2:00:57.380 --> 2:01:03.580
 and it's this fascinating process of similar to what you're talking about with the robotics

2:01:03.580 --> 2:01:10.340
 approach, which is you deploy a neural network and then there's humans that use it and then

2:01:10.340 --> 2:01:13.780
 it runs into trouble in a bunch of places and that stuff is sent back.

2:01:13.780 --> 2:01:20.020
 So the deployment discovers a bunch of edge cases and those edge cases are sent back for

2:01:20.020 --> 2:01:25.220
 supervised annotation, thereby improving the neural network and that's deployed again.

2:01:25.220 --> 2:01:31.140
 It goes over and over until the network becomes really good at the task of driving, becomes

2:01:31.140 --> 2:01:32.140
 safer and safer.

2:01:32.140 --> 2:01:35.340
 What do you think of that kind of approach to robotics?

2:01:35.340 --> 2:01:36.940
 I believe that's the way to go.

2:01:36.940 --> 2:01:41.780
 So in some sense, even when I was speaking about collecting trajectories from humans,

2:01:41.780 --> 2:01:46.300
 that's like a first step and then you deploy the system and then you have humans revising

2:01:46.300 --> 2:01:52.500
 all the issues and in some sense, like at this approach converges to system that doesn't

2:01:52.500 --> 2:01:56.380
 make mistakes because for the cases where there are mistakes, you gather data how to

2:01:56.380 --> 2:01:59.340
 fix them and the system will keep on improving.

2:01:59.340 --> 2:02:05.500
 So there's a very, to me, difficult question of how long that converging takes, how hard

2:02:05.500 --> 2:02:07.180
 it is.

2:02:07.180 --> 2:02:13.060
 The other aspect of autonomous vehicles probably applies to certain robotics applications

2:02:13.060 --> 2:02:15.020
 is society.

2:02:15.020 --> 2:02:22.620
 As the quality of the system converges, so one, there's a human factors perspective of

2:02:22.620 --> 2:02:28.020
 psychology of humans being able to supervise those, even with teleoperation, those robots.

2:02:28.020 --> 2:02:31.660
 And the other is society willing to accept robots.

2:02:31.660 --> 2:02:35.780
 Currently society is much harsher on self driving cars than it is on human driven cars

2:02:35.780 --> 2:02:37.940
 in terms of the expectation of safety.

2:02:37.940 --> 2:02:41.980
 So the bar is set much higher than for humans.

2:02:41.980 --> 2:02:51.100
 So if there's a death in an autonomous vehicle that's seen as much more dramatic than a death

2:02:51.100 --> 2:02:53.300
 in a human driven vehicle.

2:02:53.300 --> 2:02:57.740
 Part of the success of deployment of robots is figuring out how to make robots part of

2:02:57.740 --> 2:03:04.140
 society, both on the just the human side, on the media journalist side and also on the

2:03:04.140 --> 2:03:05.940
 policy government side.

2:03:05.940 --> 2:03:11.100
 And that seems to be, maybe you can put that into the objective function to optimize.

2:03:11.100 --> 2:03:16.060
 And that is definitely a tricky one.

2:03:16.060 --> 2:03:20.340
 And I wonder if that is actually the trickiest part for self driving cars or any system that's

2:03:20.340 --> 2:03:22.580
 safety critical.

2:03:22.580 --> 2:03:26.500
 It's not the algorithm, it's the society accepting it.

2:03:26.500 --> 2:03:34.380
 Yeah, I would say, I believe that the part of the process of deployment is actually showing

2:03:34.380 --> 2:03:37.780
 people that the given things can be trusted.

2:03:37.780 --> 2:03:46.180
 And trust is also like a glass that is actually really easy to crack it and damage it.

2:03:46.180 --> 2:03:55.300
 And I think that's actually very common with innovation, that there is some resistance

2:03:55.300 --> 2:03:57.220
 toward it.

2:03:57.220 --> 2:03:59.180
 And it's just a natural progression.

2:03:59.180 --> 2:04:04.060
 So in some sense, people will have to keep on proving that indeed these systems are worth

2:04:04.060 --> 2:04:05.340
 being used.

2:04:05.340 --> 2:04:12.860
 And I would say, I also found out that often the best way to convince people is by letting

2:04:12.860 --> 2:04:14.100
 them experience it.

2:04:14.100 --> 2:04:15.100
 Yeah, absolutely.

2:04:15.100 --> 2:04:17.820
 That's the case with Tesla autopilot, for example.

2:04:17.820 --> 2:04:21.900
 That's the case with, yeah, with basically robots in general.

2:04:21.900 --> 2:04:28.140
 It's kind of funny to hear people talk about robots like there's a lot of fear, even like

2:04:28.140 --> 2:04:29.140
 legged robots.

2:04:29.140 --> 2:04:33.900
 But when they actually interact with them, there's joy.

2:04:33.900 --> 2:04:35.340
 I love interacting with them.

2:04:35.340 --> 2:04:37.340
 And the same with the car.

2:04:37.340 --> 2:04:43.220
 With a robot, if it starts being useful, I think people immediately understand.

2:04:43.220 --> 2:04:46.020
 And if the product is designed well, they fall in love.

2:04:46.020 --> 2:04:47.020
 You're right.

2:04:47.020 --> 2:04:51.340
 It's actually even similar when I'm thinking about Copilot, the GitHub Copilot.

2:04:51.340 --> 2:04:54.700
 There was a spectrum of responses that people had.

2:04:54.700 --> 2:05:00.260
 And ultimately, the important piece was to let people try it out.

2:05:00.260 --> 2:05:03.020
 And then many people just loved it.

2:05:03.020 --> 2:05:04.020
 Like programmers.

2:05:04.020 --> 2:05:05.020
 Yeah, programmers.

2:05:05.020 --> 2:05:09.100
 But like some of them, you know, they came with a fear.

2:05:09.100 --> 2:05:11.980
 But then you try it out and you think, actually, that's cool.

2:05:11.980 --> 2:05:16.140
 And you know, you can try to resist the same way as, you know, you could resist moving

2:05:16.140 --> 2:05:21.100
 from punch cards to, let's say, C++ or so.

2:05:21.100 --> 2:05:24.140
 And it's a little bit futile.

2:05:24.140 --> 2:05:31.420
 So we talked about generation of program, generation of language, even self supervised

2:05:31.420 --> 2:05:35.300
 learning in the visual space for robotics and then reinforcement learning.

2:05:35.300 --> 2:05:42.460
 What do you and like this whole beautiful spectrum of AI, do you think is a good benchmark,

2:05:42.460 --> 2:05:48.020
 a good test to strive for, to achieve intelligence?

2:05:48.020 --> 2:05:49.940
 That's a strong test of intelligence.

2:05:49.940 --> 2:05:53.540
 You know, it started with Alan Turing and the Turing test.

2:05:53.540 --> 2:05:57.300
 Maybe you think natural language conversation is a good test.

2:05:57.300 --> 2:06:02.740
 So you know, it would be nice if for instance, machine would be able to solve Riemann hypothesis

2:06:02.740 --> 2:06:04.740
 in math.

2:06:04.740 --> 2:06:07.740
 That would be, I think that would be very impressive.

2:06:07.740 --> 2:06:14.140
 So theorem proving, is that to you, proving theorems is a good, oh, like one thing that

2:06:14.140 --> 2:06:17.020
 the machine did, you would say, damn.

2:06:17.020 --> 2:06:18.020
 Exactly.

2:06:18.020 --> 2:06:19.020
 Okay.

2:06:19.020 --> 2:06:22.100
 That would be quite, quite impressive.

2:06:22.100 --> 2:06:27.820
 I mean, the tricky part about the benchmarks is, you know, as we are getting closer with

2:06:27.820 --> 2:06:29.580
 them, we have to invent new benchmarks.

2:06:29.580 --> 2:06:31.620
 There is actually no ultimate benchmark out there.

2:06:31.620 --> 2:06:32.620
 Yeah.

2:06:32.620 --> 2:06:37.500
 See, my thought with the Riemann hypothesis would be the moment the machine proves it,

2:06:37.500 --> 2:06:41.260
 would say, okay, well, then the problem was easy.

2:06:41.260 --> 2:06:42.380
 That's what happens.

2:06:42.380 --> 2:06:48.300
 And I mean, in some sense, that's actually what happens over there in AI that like we

2:06:48.300 --> 2:06:50.620
 get used to things very quickly.

2:06:50.620 --> 2:06:55.020
 You know something I talked to Rodney Brooks, I don't know if you know that is, he called

2:06:55.020 --> 2:06:59.740
 AlphaZero homework problem, because he was saying like, there's nothing special about

2:06:59.740 --> 2:07:00.740
 it.

2:07:00.740 --> 2:07:01.740
 It's not a big leap.

2:07:01.740 --> 2:07:06.340
 And I didn't, well, he's coming from one of the aspects that we referred to is he was

2:07:06.340 --> 2:07:12.180
 part of the founding of iRobot, which deployed now tens of millions of robot in the home.

2:07:12.180 --> 2:07:20.580
 So if you see robots that are actually in the homes of people as the legitimate instantiation

2:07:20.580 --> 2:07:24.900
 of artificial intelligence, then yes, maybe an AI that plays a silly game like go and

2:07:24.900 --> 2:07:26.660
 chess is not a real accomplishment.

2:07:26.660 --> 2:07:29.460
 But to me, it's a fundamental leap.

2:07:29.460 --> 2:07:34.860
 But I think we as humans then say, okay, well, then that that game of chess or go wasn't

2:07:34.860 --> 2:07:38.420
 that difficult compared to the thing that's currently unsolved.

2:07:38.420 --> 2:07:46.020
 So my intuition is that from perspective of the evolution of these AI systems, we'll

2:07:46.020 --> 2:07:52.060
 at first see the tremendous progress in digital space, and the main thing about digital space

2:07:52.060 --> 2:07:57.300
 is also that you can, everything is, there is a lot of recorded data, plus you can very

2:07:57.300 --> 2:08:00.220
 rapidly deploy things to billions of people.

2:08:00.220 --> 2:08:05.700
 While in case of physical space, the deployment part takes multiple years.

2:08:05.700 --> 2:08:13.780
 You have to manufacture things and, you know, delivering it to actual people is very hard.

2:08:13.780 --> 2:08:21.860
 So I'm expecting that the first and the prices in digital space of goods they would go down

2:08:21.860 --> 2:08:25.300
 to, let's say, marginal costs are two zero.

2:08:25.300 --> 2:08:29.780
 And also the question is how much of our life will be in digital because it seems like we're

2:08:29.780 --> 2:08:33.540
 heading towards more and more of our lives being in the digital space.

2:08:33.540 --> 2:08:38.260
 So like innovation in the physical space might become less and less significant.

2:08:38.260 --> 2:08:43.980
 Like why do you need to drive anywhere if most of your life is spent in virtual reality?

2:08:43.980 --> 2:08:48.620
 I still would like, you know, to, at least at the moment, my impression is that I would

2:08:48.620 --> 2:08:53.140
 like to have a physical contact with other people and that's very important to me.

2:08:53.140 --> 2:08:55.340
 We don't have a way to replicate it in the computer.

2:08:55.340 --> 2:08:58.340
 It might be the case that over the time it will change.

2:08:58.340 --> 2:09:02.940
 Like in 10 years from now, why not have like an arbitrary infinite number of people you

2:09:02.940 --> 2:09:09.660
 can interact with, some of them are real, some are not, with arbitrary characteristics

2:09:09.660 --> 2:09:12.620
 that you can define based on your own preferences.

2:09:12.620 --> 2:09:16.660
 I think that's maybe where we are heading and maybe I'm resisting the future.

2:09:16.660 --> 2:09:17.660
 Yeah.

2:09:17.660 --> 2:09:27.300
 I'm telling you, if I got to choose, if I could live in Elder Scroll Skyrim versus the

2:09:27.300 --> 2:09:31.020
 real world, I'm not so sure I would stay with the real world.

2:09:31.020 --> 2:09:36.900
 Yeah, I mean, the question is, will VR be sufficient to get us there or do you need

2:09:36.900 --> 2:09:42.540
 to, you know, plug electrodes in the brain and it would be nice if these electrodes

2:09:42.540 --> 2:09:44.020
 wouldn't be invasive?

2:09:44.020 --> 2:09:45.020
 Yeah.

2:09:45.020 --> 2:09:49.300
 Or at least like provably nondestructive.

2:09:49.300 --> 2:09:54.860
 But in a digital space, do you think we'll be able to solve the Turing test, the spirit

2:09:54.860 --> 2:10:01.180
 of the Turing test, which is do you think we'll be able to achieve compelling natural

2:10:01.180 --> 2:10:07.140
 language conversation between people, like have friends that are AI systems on the Internet?

2:10:07.140 --> 2:10:08.980
 I totally think it's doable.

2:10:08.980 --> 2:10:12.580
 Do you think the current approach to GPT will take us there?

2:10:12.580 --> 2:10:13.580
 Yes.

2:10:13.580 --> 2:10:18.020
 So there is, you know, the part of at first learning all the content out there and I think

2:10:18.020 --> 2:10:21.500
 that Steel System should keep on learning as it speaks with you.

2:10:21.500 --> 2:10:22.700
 Yeah.

2:10:22.700 --> 2:10:24.180
 And I think that should work.

2:10:24.180 --> 2:10:28.300
 The question is how exactly to do it and, you know, obviously we have people at the

2:10:28.300 --> 2:10:35.220
 open air asking these questions and kind of at first pre training on all existing content

2:10:35.220 --> 2:10:39.540
 is like a backbone and is a decent backbone.

2:10:39.540 --> 2:10:46.660
 Do you think AI needs a body connecting to our robotics question to truly connect with

2:10:46.660 --> 2:10:50.580
 humans or can most of the connection be in the digital space?

2:10:50.580 --> 2:10:52.500
 So let's see.

2:10:52.500 --> 2:10:57.940
 We know that there are people who met each other online and they felt in love.

2:10:57.940 --> 2:10:58.940
 Yeah.

2:10:58.940 --> 2:11:07.540
 So it seems that it's conceivable to establish connection, which is purely through Internet.

2:11:07.540 --> 2:11:12.460
 Of course, it might be more compelling, the more modalities you add.

2:11:12.460 --> 2:11:17.580
 So it would be like you're proposing like a Tinder, but for AI, are you like swipe right

2:11:17.580 --> 2:11:24.700
 left and half the systems are AI and the other is humans and you don't know which is which?

2:11:24.700 --> 2:11:28.180
 That would be our formulation of Turing test.

2:11:28.180 --> 2:11:34.140
 The moment AI is able to achieve more swipe right or left, whatever, the moment it's able

2:11:34.140 --> 2:11:38.380
 to be more attractive than other humans, it passes the Turing test.

2:11:38.380 --> 2:11:40.580
 Then you would pass the Turing test in attractiveness.

2:11:40.580 --> 2:11:43.300
 Well, no, like attractiveness just to clarify.

2:11:43.300 --> 2:11:44.300
 There will be conversation.

2:11:44.300 --> 2:11:45.300
 Not just visual, right?

2:11:45.300 --> 2:11:52.940
 It's also attractiveness with wit and humor and whatever makes conversation pleasant for

2:11:52.940 --> 2:11:53.940
 humans.

2:11:53.940 --> 2:11:54.940
 Okay.

2:11:54.940 --> 2:11:58.940
 All right.

2:11:58.940 --> 2:12:02.740
 So you're saying it's possible to achieve in the digital space.

2:12:02.740 --> 2:12:07.420
 In some sense, I would almost ask the question, why wouldn't that be possible?

2:12:07.420 --> 2:12:08.420
 Right.

2:12:08.420 --> 2:12:11.340
 Well, I have this argument with my dad all the time.

2:12:11.340 --> 2:12:14.580
 He thinks that touch and smell are really important.

2:12:14.580 --> 2:12:16.980
 So they can be very important.

2:12:16.980 --> 2:12:20.860
 And I'm saying the initial systems, they won't have it.

2:12:20.860 --> 2:12:26.860
 Still I wouldn't, like there are people being born without these senses.

2:12:26.860 --> 2:12:31.940
 And you know, I believe that they can still fall in love and have meaningful life.

2:12:31.940 --> 2:12:32.940
 Yeah.

2:12:32.940 --> 2:12:38.940
 I wonder if it's possible to go close to all the way by just training on transcripts

2:12:38.940 --> 2:12:39.940
 of conversations.

2:12:39.940 --> 2:12:42.420
 Like, I wonder how far that takes us.

2:12:42.420 --> 2:12:46.220
 So I think that actually still you want images, I would like.

2:12:46.220 --> 2:12:53.940
 So I don't have kids, but I could imagine having AI tutor, it has to see kids drawing

2:12:53.940 --> 2:12:56.020
 some pictures on the paper.

2:12:56.020 --> 2:12:58.620
 And also facial expressions, all that kind of stuff.

2:12:58.620 --> 2:13:04.180
 We use dogs and humans use their eyes to communicate with each other.

2:13:04.180 --> 2:13:07.860
 I think that's a really powerful mechanism of communication.

2:13:07.860 --> 2:13:12.820
 Body language too, that words are much lower bandwidth.

2:13:12.820 --> 2:13:17.380
 And for body language, we still, you know, we can have a system that displays an image

2:13:17.380 --> 2:13:19.740
 of its artificial expression on the computer.

2:13:19.740 --> 2:13:23.620
 It doesn't have to move, you know, mechanical pieces or so.

2:13:23.620 --> 2:13:27.660
 So I think that, you know, that there is like kind of a progression.

2:13:27.660 --> 2:13:35.500
 You can imagine that text might be the simplest to tackle, but this is not a complete human

2:13:35.500 --> 2:13:36.940
 experience at all.

2:13:36.940 --> 2:13:41.540
 You expand it to let's say images both for input and output.

2:13:41.540 --> 2:13:46.140
 And what you describe is actually the final, I guess, frontier.

2:13:46.140 --> 2:13:50.340
 What makes us human, the fact that we can touch each other or smell or so.

2:13:50.340 --> 2:13:54.460
 And it's the hardest from perspective of data and deployment.

2:13:54.460 --> 2:13:59.900
 And I believe that these things might happen gradually.

2:13:59.900 --> 2:14:01.540
 Are you excited by that possibility?

2:14:01.540 --> 2:14:08.020
 This particular application of human to AI friendship and interaction.

2:14:08.020 --> 2:14:13.340
 So let's see, like would you, do you look forward to a world you said you're living

2:14:13.340 --> 2:14:16.340
 with a few folks and you're very close friends with them?

2:14:16.340 --> 2:14:19.820
 Do you look forward to a day where one or two of those friends are AI systems?

2:14:19.820 --> 2:14:25.580
 So if the system would be truly wishing me well, rather than being in the situation that

2:14:25.580 --> 2:14:29.540
 it optimizes for my time to interact with the system.

2:14:29.540 --> 2:14:34.420
 The line between those is, it's a gray, it's a gray area.

2:14:34.420 --> 2:14:40.580
 I think that's the distinction between love and possession.

2:14:40.580 --> 2:14:47.260
 And these things, they might be often correlated for humans, but you might find that there

2:14:47.260 --> 2:14:51.140
 are some friends with whom you haven't spoke for months.

2:14:51.140 --> 2:14:55.780
 And then, you know, you pick up the phone, it's as the time hasn't passed.

2:14:55.780 --> 2:14:58.020
 They are not holding to you.

2:14:58.020 --> 2:15:04.540
 And I wouldn't like to have AI system that, you know, it's trying to convince me to spend

2:15:04.540 --> 2:15:05.540
 time with it.

2:15:05.540 --> 2:15:12.380
 I would like the system to optimize for what I care about and help me in achieving my own

2:15:12.380 --> 2:15:13.380
 goals.

2:15:13.380 --> 2:15:19.940
 But there's some, I mean, I don't know, there's some manipulation, there's some possessiveness,

2:15:19.940 --> 2:15:25.180
 there's some insecurities, there's fragility, all those things are necessary to form a close

2:15:25.180 --> 2:15:31.380
 friendship over time, to go through some dark shit together, some bliss and happiness together.

2:15:31.380 --> 2:15:36.980
 I feel like there's a lot of greedy self center behavior within that process.

2:15:36.980 --> 2:15:43.100
 My intuition, but I might be wrong, is that human computer interaction doesn't have to

2:15:43.100 --> 2:15:48.060
 go through computer being greedy, possessive and so on.

2:15:48.060 --> 2:15:55.300
 It is possible to train systems, maybe, that they actually, you know, they are, I guess,

2:15:55.300 --> 2:16:00.300
 prompted or fine tuned or so to truly optimize for what you care about.

2:16:00.300 --> 2:16:05.060
 And you could imagine that, you know, the way how the process would look like is at

2:16:05.060 --> 2:16:12.540
 some point, we as humans, we look at the transcript of the conversation or like an entire interaction

2:16:12.540 --> 2:16:17.940
 and we say, actually, here there was more loving way to go about it.

2:16:17.940 --> 2:16:23.900
 Maybe supervise system toward being more loving or maybe we train the system such that it

2:16:23.900 --> 2:16:26.260
 has a reward function toward being more loving.

2:16:26.260 --> 2:16:27.260
 Yeah.

2:16:27.260 --> 2:16:32.900
 Or maybe the possibility of the system being an asshole and manipulative and possessive

2:16:32.900 --> 2:16:36.980
 every once in a while is a feature, not a bug.

2:16:36.980 --> 2:16:43.860
 Because some of the happiness that we experience when two souls meet each other, when two humans

2:16:43.860 --> 2:16:48.580
 meet each other, is a kind of break from the assholes in the world.

2:16:48.580 --> 2:16:54.900
 And so you need assholes in AI as well, because like, it'll be like a breath of fresh air

2:16:54.900 --> 2:17:04.580
 to discover in AI that the three previous AI's you had are too friendly or cruel or whatever.

2:17:04.580 --> 2:17:09.140
 It's like some kind of mix and then this one is just right, but you need to experience

2:17:09.140 --> 2:17:10.140
 the full spectrum.

2:17:10.140 --> 2:17:14.380
 Like, I think you need to be able to engineer assholes.

2:17:14.380 --> 2:17:17.460
 So let's see.

2:17:17.460 --> 2:17:24.260
 Because there's some level to us being appreciated, to appreciate the human experience.

2:17:24.260 --> 2:17:27.340
 We need the dark and the light.

2:17:27.340 --> 2:17:29.940
 So that kind of reminds me.

2:17:29.940 --> 2:17:39.140
 I met a while ago at the meditation retreat, one woman and a beautiful, beautiful woman

2:17:39.140 --> 2:17:45.020
 and she had a crush, she had the trouble of walking on one leg.

2:17:45.020 --> 2:17:53.380
 I asked her what has happened and she said that five years ago she was in Maui, Hawaii

2:17:53.380 --> 2:17:59.020
 and she was eating a salad and some snail fell into the salad and apparently there

2:17:59.020 --> 2:18:06.180
 are neurotoxic snails over there and she got into coma for a year.

2:18:06.180 --> 2:18:11.020
 And apparently there is a high chance of even just dying, but she was in the coma.

2:18:11.020 --> 2:18:15.100
 At some point she regained partially consciousness.

2:18:15.100 --> 2:18:18.740
 She was able to hear people in the room.

2:18:18.740 --> 2:18:21.780
 People behave as she wouldn't be there.

2:18:21.780 --> 2:18:27.980
 At some point she started being able to speak, but she was mumbling like barely able to express

2:18:27.980 --> 2:18:28.980
 herself.

2:18:28.980 --> 2:18:31.580
 Then at some point she got into wheelchair.

2:18:31.580 --> 2:18:38.780
 And at some point she actually noticed that she can move her toe and then she knew that

2:18:38.780 --> 2:18:41.020
 she will be able to walk.

2:18:41.020 --> 2:18:46.180
 And then that's where she was five years after and she said that since then she appreciates

2:18:46.180 --> 2:18:49.220
 the fact that she can move her toe.

2:18:49.220 --> 2:18:54.860
 And I was thinking, do I need to go through such experience to appreciate that I can move

2:18:54.860 --> 2:18:55.860
 my toe?

2:18:55.860 --> 2:19:00.020
 Wow, that's a really good story, a really deep example, yeah.

2:19:00.020 --> 2:19:06.500
 And in some sense it might be the case that we don't see light if we haven't went through

2:19:06.500 --> 2:19:10.140
 the darkness, but I wouldn't say that we shouldn't.

2:19:10.140 --> 2:19:15.660
 We shouldn't assume that that's the case, we may be able to engineer shortcuts.

2:19:15.660 --> 2:19:23.580
 Yeah, Ilya had this belief that maybe one has to go for a week or six months to some

2:19:23.580 --> 2:19:29.980
 challenging camp to just experience a lot of difficulties and then comes back.

2:19:29.980 --> 2:19:33.580
 And actually everything is bright, everything is beautiful.

2:19:33.580 --> 2:19:37.140
 I'm with Ilya, it must be a Russian thing, where are you from originally?

2:19:37.140 --> 2:19:38.140
 I'm Polish.

2:19:38.140 --> 2:19:39.140
 Polish.

2:19:39.140 --> 2:19:40.140
 Okay.

2:19:40.140 --> 2:19:46.420
 I'm tempted to say that explains a lot, but yeah, there's something about the Russian,

2:19:46.420 --> 2:19:47.860
 the necessity of suffering.

2:19:47.860 --> 2:19:52.860
 I believe suffering or rather struggle is necessary.

2:19:52.860 --> 2:19:58.340
 I believe that struggle is necessary, I mean in some sense you even look at the story of

2:19:58.340 --> 2:20:03.180
 any superhero in the movie, it's not that it was like I ever forgot it goes easy, easy,

2:20:03.180 --> 2:20:04.180
 easy.

2:20:04.180 --> 2:20:09.540
 I like how that's your ground truth, is the story of superheroes, okay.

2:20:09.540 --> 2:20:15.220
 You mentioned that you used to do research at night and go to bed at like 6am or 7am,

2:20:15.220 --> 2:20:19.140
 I still do that often.

2:20:19.140 --> 2:20:23.620
 What sleep schedules have you tried to make for a productive and happy life?

2:20:23.620 --> 2:20:30.260
 Is there some interesting wild sleeping patterns that you engaged that you found that works

2:20:30.260 --> 2:20:31.540
 really well for you?

2:20:31.540 --> 2:20:37.260
 I tried at some point decreasing number of hours of sleep, like a gradually, like a half

2:20:37.260 --> 2:20:42.300
 an hour every few days to this, I was hoping to just save time.

2:20:42.300 --> 2:20:47.860
 That clearly didn't work for me, at some point there's like a face shift and I felt tired

2:20:47.860 --> 2:20:50.820
 all the time.

2:20:50.820 --> 2:20:55.700
 There was a time that I used to work during the nights, the nice thing about the nights

2:20:55.700 --> 2:21:03.300
 is that no one disturbs you and even I remember when I was meeting for the first time with

2:21:03.300 --> 2:21:11.140
 Greg Brockman, his CTO and chairman of OpenAI, our meeting was scheduled to 5pm and I overstepped

2:21:11.140 --> 2:21:12.900
 for the meeting.

2:21:12.900 --> 2:21:15.540
 Over slept for the meeting at 5pm, yeah.

2:21:15.540 --> 2:21:19.180
 Now you sound like me, that's hilarious, okay, yeah.

2:21:19.180 --> 2:21:26.180
 At the moment in some sense, my sleeping schedule also has to do with the fact that I'm interacting

2:21:26.180 --> 2:21:29.500
 with people, I sleep without an alarm.

2:21:29.500 --> 2:21:36.500
 So yeah, the team thing, you mentioned the extrovert thing because most humans operate

2:21:36.500 --> 2:21:43.140
 during a certain set of hours, you're forced to then operate at the same set of hours.

2:21:43.140 --> 2:21:49.740
 But I'm not quite there yet, I found a lot of joy just like you said, working through

2:21:49.740 --> 2:21:54.740
 the night because it's quiet, because the world doesn't disturb you and there's some

2:21:54.740 --> 2:22:00.500
 aspect counter to everything you're saying, there's some joyful aspect to sleeping through

2:22:00.500 --> 2:22:07.220
 the mess of the day because people are having meetings and sending emails and there's drama,

2:22:07.220 --> 2:22:08.220
 meetings.

2:22:08.220 --> 2:22:10.220
 I can sleep through all the meetings.

2:22:10.220 --> 2:22:14.140
 You know, I have meetings every day and they prevent me from having sufficient amount of

2:22:14.140 --> 2:22:17.220
 time for focus work.

2:22:17.220 --> 2:22:23.980
 And then I modified my calendar and I said that I'm out of office Wednesday, Thursday

2:22:23.980 --> 2:22:30.780
 and Friday every day and I'm having meetings only Monday and Tuesday and that was the positively

2:22:30.780 --> 2:22:36.460
 influence my mood that I have literally like three days for fully focused work.

2:22:36.460 --> 2:22:40.900
 So there's better solutions to this problem than staying awake all night.

2:22:40.900 --> 2:22:46.340
 Okay, you've been part of development of some of the greatest ideas in artificial intelligence.

2:22:46.340 --> 2:22:50.620
 What would you say is your process for developing good novel ideas?

2:22:50.620 --> 2:22:55.500
 You have to be aware that clearly there are many other brilliant people around.

2:22:55.500 --> 2:23:03.980
 So you have to ask yourself a question, why they give an idea, let's say wasn't tried

2:23:03.980 --> 2:23:11.940
 by someone else and in some sense it has to do with, you know, kind of simple, it might

2:23:11.940 --> 2:23:16.340
 sound simple, but like a thinking outside of the box and what do I mean here?

2:23:16.340 --> 2:23:25.420
 So for instance, for a while, people in academia, they assumed that you have a fixed data set

2:23:25.420 --> 2:23:31.860
 and then you optimize the algorithms in order to get the best performance.

2:23:31.860 --> 2:23:39.940
 And that was so in great assumption that no one thought about training models on anti

2:23:39.940 --> 2:23:49.940
 internet or like that, maybe some people thought about it, but it felt too many as unfair.

2:23:49.940 --> 2:23:54.340
 And in some sense, that's almost like a, it's not my idea or so, but that's an example

2:23:54.340 --> 2:23:57.060
 of breaking a typical assumption.

2:23:57.060 --> 2:24:01.900
 So you want to be in the paradigm that you're breaking a typical assumption.

2:24:01.900 --> 2:24:07.900
 In the context of the AI community, getting to pick your data set is cheating.

2:24:07.900 --> 2:24:08.900
 Correct.

2:24:08.900 --> 2:24:13.660
 And in some sense, so that was a, that was assumption that many people had out there.

2:24:13.660 --> 2:24:21.100
 And then if you free yourself from assumptions, then they are likely to achieve something

2:24:21.100 --> 2:24:22.500
 that others cannot do.

2:24:22.500 --> 2:24:26.900
 And in some sense, if you are trying to do exactly the same things as others, it's very

2:24:26.900 --> 2:24:29.140
 likely that you're going to have the same results.

2:24:29.140 --> 2:24:30.140
 Yeah.

2:24:30.140 --> 2:24:36.060
 I, but there's also that kind of tension, which is asking yourself the question, why

2:24:36.060 --> 2:24:38.860
 haven't others done this?

2:24:38.860 --> 2:24:47.420
 Because I mean, I get a lot of good ideas, but I think probably most of them suck when

2:24:47.420 --> 2:24:49.340
 they meet reality.

2:24:49.340 --> 2:24:56.660
 So actually, I think the other big piece is getting into habit of generating ideas, training

2:24:56.660 --> 2:25:04.220
 your brain towards generating ideas and not even suspending judgment of the ideas.

2:25:04.220 --> 2:25:09.900
 So in some sense, I noticed myself that even if I'm in the process of generating ideas,

2:25:09.900 --> 2:25:16.220
 if I tell myself, oh, that was a bad idea, then that actually interrupts the process

2:25:16.220 --> 2:25:20.500
 and I cannot generate more ideas because I'm actually focused on the negative part, why

2:25:20.500 --> 2:25:22.540
 it won't work.

2:25:22.540 --> 2:25:28.460
 But I created also environment in the way that it's very easy for me to store new ideas.

2:25:28.460 --> 2:25:33.900
 So for instance, next to my bed, I have a voice recorder.

2:25:33.900 --> 2:25:38.620
 And it happens to me often, like I wake up during the night and I have some idea.

2:25:38.620 --> 2:25:44.340
 In the past, I was writing them down on my phone, but that means turning on the screen

2:25:44.340 --> 2:25:51.220
 and that wakes me up or like pulling a paper, which requires turning on the light.

2:25:51.220 --> 2:25:54.060
 These days, I just start recording it.

2:25:54.060 --> 2:25:55.060
 What do you think?

2:25:55.060 --> 2:25:56.420
 I don't know if you know who Jim Keller is.

2:25:56.420 --> 2:25:58.100
 I know Jim Keller.

2:25:58.100 --> 2:26:03.580
 He's a big proponent of thinking hard on a problem right before sleep so that he can

2:26:03.580 --> 2:26:08.700
 sleep through it and solve it in his sleep or like come up with radical stuff in his

2:26:08.700 --> 2:26:09.700
 sleep.

2:26:09.700 --> 2:26:11.340
 He was trying to get me to do this.

2:26:11.340 --> 2:26:16.220
 So it happened from my experience perspective.

2:26:16.220 --> 2:26:22.940
 It happened to me many times during the high school days when I was doing mathematics that

2:26:22.940 --> 2:26:27.540
 I had the solution to my problem as I woke up.

2:26:27.540 --> 2:26:33.540
 At the moment, regarding thinking hard about the given problem is I'm trying to actually

2:26:33.540 --> 2:26:37.540
 devote substantial amount of time to think about important problems, not just before

2:26:37.540 --> 2:26:38.540
 the sleep.

2:26:38.540 --> 2:26:44.860
 I'm organizing the huge chunks of time such that I'm not constantly working on the urgent

2:26:44.860 --> 2:26:48.420
 problems, but I actually have time to think about the important one.

2:26:48.420 --> 2:26:55.340
 So you do it naturally, but his idea is that you prime your brain to make sure that that's

2:26:55.340 --> 2:26:56.340
 the focus.

2:26:56.340 --> 2:27:00.580
 Oftentimes, people have other worries in their life that's not fundamentally deep problems

2:27:00.580 --> 2:27:07.060
 like I don't know, just stupid drama in your life and even at work, all that kind of stuff.

2:27:07.060 --> 2:27:13.020
 He wants to pick the most important problem that you're thinking about and go to bed

2:27:13.020 --> 2:27:14.020
 on that.

2:27:14.020 --> 2:27:18.980
 I think that's why I mean, the other thing that comes to my mind is also I feel the most

2:27:18.980 --> 2:27:20.620
 fresh in the morning.

2:27:20.620 --> 2:27:25.980
 So during the morning, I try to work on the most important things rather than just being

2:27:25.980 --> 2:27:30.060
 pulled by urgent things or checking email or so.

2:27:30.060 --> 2:27:34.260
 What do you do with the because I've been doing the voice recorder thing too, but I end up

2:27:34.260 --> 2:27:37.420
 recording so many messages is hard to organize.

2:27:37.420 --> 2:27:38.740
 I have the same problem.

2:27:38.740 --> 2:27:44.540
 Now I have heard that Google Pixel is really good in transcribing text and I might get

2:27:44.540 --> 2:27:47.620
 Google Pixel just for the sake of transcribing text.

2:27:47.620 --> 2:27:50.940
 People listening to this, if you have a good voice recorder suggestion that transcribes,

2:27:50.940 --> 2:27:53.060
 please let me know.

2:27:53.060 --> 2:27:58.820
 Some of it is this has to do with the open AI codex too.

2:27:58.820 --> 2:28:02.020
 Some of it is simply like the friction.

2:28:02.020 --> 2:28:08.980
 I need apps that remove that friction between voice and the organization of the resulting

2:28:08.980 --> 2:28:12.300
 transcripts and all that kind of stuff.

2:28:12.300 --> 2:28:13.300
 But yes, you're right.

2:28:13.300 --> 2:28:14.300
 Absolutely.

2:28:14.300 --> 2:28:21.540
 Like during for me is walking, sleep too, but walking and running, especially running

2:28:21.540 --> 2:28:26.220
 get a lot of thoughts during running and there's no good mechanism for recording thoughts.

2:28:26.220 --> 2:28:33.980
 So one more thing that I do, I have a separate phone which has no apps.

2:28:33.980 --> 2:28:37.460
 Maybe it's like audible or let's say Kindle.

2:28:37.460 --> 2:28:40.980
 No one has this phone number, this kind of my meditation phone.

2:28:40.980 --> 2:28:47.420
 And I try to expand the amount of time that that's the phone that I'm having.

2:28:47.420 --> 2:28:50.100
 It has also Google Maps if I need to go somewhere.

2:28:50.100 --> 2:28:54.180
 And I also use this phone to write down ideas.

2:28:54.180 --> 2:28:56.020
 That's a really good idea.

2:28:56.020 --> 2:29:01.780
 Often actually what I end up doing is even sending a message from that phone to the other

2:29:01.780 --> 2:29:02.780
 phone.

2:29:02.780 --> 2:29:06.940
 So that's actually my way of recording messages or I just put them into notes.

2:29:06.940 --> 2:29:08.820
 I love it.

2:29:08.820 --> 2:29:17.420
 What advice would you give to a young person, high school, college, about how to be successful?

2:29:17.420 --> 2:29:20.740
 You've done a lot of incredible things in the past decade.

2:29:20.740 --> 2:29:22.260
 So maybe you have some...

2:29:22.260 --> 2:29:24.060
 There might be something.

2:29:24.060 --> 2:29:26.260
 There might be something.

2:29:26.260 --> 2:29:34.740
 I mean, it might sound simplistic or so, but I would say literally just follow your passion

2:29:34.740 --> 2:29:35.740
 double down on it.

2:29:35.740 --> 2:29:40.820
 And if you don't know what's your passion, just figure out what could be a passion.

2:29:40.820 --> 2:29:43.820
 So the step might be an exploration.

2:29:43.820 --> 2:29:48.180
 When I was in elementary school, it was math and chemistry.

2:29:48.180 --> 2:29:53.900
 And I remember for some time I gave up on math because my school teacher, she told me

2:29:53.900 --> 2:29:56.740
 that I'm dumb.

2:29:56.740 --> 2:30:02.300
 And I guess maybe an advice would be just ignore people if they tell you that you're

2:30:02.300 --> 2:30:03.300
 dumb.

2:30:03.300 --> 2:30:08.780
 You mentioned something offline about chemistry and explosives.

2:30:08.780 --> 2:30:09.940
 What was that about?

2:30:09.940 --> 2:30:12.060
 So let's see.

2:30:12.060 --> 2:30:17.140
 So a story goes like that.

2:30:17.140 --> 2:30:18.540
 I got into chemistry.

2:30:18.540 --> 2:30:23.380
 Maybe I was like a second grade of my elementary school, third grade.

2:30:23.380 --> 2:30:28.500
 I started going to chemistry classes.

2:30:28.500 --> 2:30:31.460
 I really love building stuff.

2:30:31.460 --> 2:30:38.020
 And I did all the experiments that they describe in the book, like how to create oxygen with

2:30:38.020 --> 2:30:41.940
 vinegar and baking soda or so.

2:30:41.940 --> 2:30:44.420
 So I did all the experiments.

2:30:44.420 --> 2:30:46.900
 And at some point, I was like, so what's next?

2:30:46.900 --> 2:30:48.740
 What can I do?

2:30:48.740 --> 2:30:56.660
 And the explosives, they also you have a clear reward signal if the thing worked or not.

2:30:56.660 --> 2:31:03.260
 So I remember at first I got interested in producing hydrogen.

2:31:03.260 --> 2:31:05.580
 That was kind of a funny experiment from school.

2:31:05.580 --> 2:31:06.940
 You can just burn it.

2:31:06.940 --> 2:31:09.980
 And then I moved to nitroglycerin.

2:31:09.980 --> 2:31:13.540
 So that's also relatively easy to synthesize.

2:31:13.540 --> 2:31:18.980
 I started producing essentially dynamite and detonating it with a friend.

2:31:18.980 --> 2:31:24.660
 Remember, there was at first like maybe two attempts that I went with a friend to detonate

2:31:24.660 --> 2:31:26.260
 what we built.

2:31:26.260 --> 2:31:27.340
 And it didn't work out.

2:31:27.340 --> 2:31:32.700
 And like a third time, he was like, ah, it won't work, like let's don't waste time.

2:31:32.700 --> 2:31:43.300
 And now we were, I was carrying this tube with dynamite, I don't know, pound or so dynamite

2:31:43.300 --> 2:31:51.540
 in my backpack were like riding on the bike to the edges of the city.

2:31:51.540 --> 2:31:57.900
 And attempt number three, this was be attempt number three, attempt number three.

2:31:57.900 --> 2:32:02.260
 And now we we dig a hole to put it inside.

2:32:02.260 --> 2:32:07.020
 It actually had the, you know, electrical detonator.

2:32:07.020 --> 2:32:10.540
 We draw a cable behind the tree.

2:32:10.540 --> 2:32:15.020
 I even, I never had, I haven't ever seen like a explosion before.

2:32:15.020 --> 2:32:18.260
 So I thought that there will be a lot of sound.

2:32:18.260 --> 2:32:22.740
 But you know, we're like laying down and I'm holding the cable and the battery at some

2:32:22.740 --> 2:32:25.540
 point, you know, we kind of like a three to one.

2:32:25.540 --> 2:32:31.340
 And I just connected it and it felt like at the ground shake, it was like a more like

2:32:31.340 --> 2:32:33.100
 a sound.

2:32:33.100 --> 2:32:38.340
 And then the soil started kind of lifting up and started falling on us.

2:32:38.340 --> 2:32:39.500
 Wow.

2:32:39.500 --> 2:32:45.980
 And then now the friend said, let's, let's make sure next time we have helmets, but it's

2:32:45.980 --> 2:32:49.180
 also, you know, I'm happy that nothing happened to me.

2:32:49.180 --> 2:32:52.580
 It could have been the case that I lost the limb or so.

2:32:52.580 --> 2:32:53.580
 Yeah.

2:32:53.580 --> 2:33:03.780
 But that's childhood of an engineering mind with a strong reward signal of an explosion.

2:33:03.780 --> 2:33:04.780
 I love it.

2:33:04.780 --> 2:33:11.260
 And my, there's some aspect of a chemist, the chemist, I know like my dad with plasma

2:33:11.260 --> 2:33:15.020
 chemistry, plasma physics, he was very much into explosives too.

2:33:15.020 --> 2:33:19.340
 It's a worrying quality of people that work in chemistry that they love.

2:33:19.340 --> 2:33:24.340
 I think it is that exactly is the, the strong signal that the thing worked.

2:33:24.340 --> 2:33:25.900
 There is no doubt.

2:33:25.900 --> 2:33:26.900
 There's no doubt.

2:33:26.900 --> 2:33:28.140
 There's some magic.

2:33:28.140 --> 2:33:32.780
 It's almost like a reminder that physics works, that chemistry works.

2:33:32.780 --> 2:33:33.780
 It's cool.

2:33:33.780 --> 2:33:38.300
 It's almost like a little glimpse at nature that you yourself engineer.

2:33:38.300 --> 2:33:44.560
 That's why I really like artificial intelligence, especially robotics, is you create a little

2:33:44.560 --> 2:33:46.500
 piece of nature.

2:33:46.500 --> 2:33:51.420
 And in some sense, even for me with explosives, the motivation was creation rather than distraction.

2:33:51.420 --> 2:33:53.020
 Yes, exactly.

2:33:53.020 --> 2:33:58.300
 In terms of advice, I forgot to ask about just machine learning and deep learning for

2:33:58.300 --> 2:34:02.380
 people who are specifically interested in machine learning.

2:34:02.380 --> 2:34:04.700
 How would you recommend it get into the field?

2:34:04.700 --> 2:34:07.700
 So I would say re implement everything.

2:34:07.700 --> 2:34:11.580
 And also there is plenty of courses for like from scratch.

2:34:11.580 --> 2:34:15.780
 So on different levels of abstraction in some sense, but I would say re implement something

2:34:15.780 --> 2:34:20.300
 from scratch, re implement something from a paper, re implement something, you know,

2:34:20.300 --> 2:34:24.700
 from podcasts that you have heard about, I would say that's a powerful way to understand

2:34:24.700 --> 2:34:25.700
 things.

2:34:25.700 --> 2:34:31.300
 So it's often the case that you read the description and you think you understand, but you truly

2:34:31.300 --> 2:34:37.460
 understand once you build it, then you actually know what really mattered in the description.

2:34:37.460 --> 2:34:42.580
 Is there a particular topics that you find people just fall in love with?

2:34:42.580 --> 2:34:51.860
 I've seen, I tend to really enjoy reinforcement learning because it's much more, it's much

2:34:51.860 --> 2:34:58.180
 easier to get to a point where you feel like you created something special, like fun games

2:34:58.180 --> 2:34:59.180
 kind of things.

2:34:59.180 --> 2:35:00.180
 Is it rewarding?

2:35:00.180 --> 2:35:07.820
 Yeah, as opposed to like re implementing from scratch, more like supervised learning kind

2:35:07.820 --> 2:35:08.820
 of things.

2:35:08.820 --> 2:35:09.820
 Yeah.

2:35:09.820 --> 2:35:16.180
 So, you know, if someone would optimize for things to be rewarding, then it feels that

2:35:16.180 --> 2:35:19.540
 the things that are somewhat generative, they have such a property.

2:35:19.540 --> 2:35:24.540
 So you have, for instance, adversarial networks, or you have just even generative language

2:35:24.540 --> 2:35:25.860
 models.

2:35:25.860 --> 2:35:31.940
 And you can even see, internally, we have seen this thing with our releases.

2:35:31.940 --> 2:35:35.100
 So we have, we released the recent two models.

2:35:35.100 --> 2:35:38.180
 There is one model called Dali that generates images.

2:35:38.180 --> 2:35:45.020
 And there is other model called Clip that actually you provide various possibilities,

2:35:45.020 --> 2:35:47.500
 what could be the answer to what is on the picture.

2:35:47.500 --> 2:35:50.900
 And it can tell you which one is the most likely.

2:35:50.900 --> 2:35:57.420
 And in some sense, in case of the first one, Dali, it is very easy for you to understand

2:35:57.420 --> 2:36:00.900
 that actually there is magic going on.

2:36:00.900 --> 2:36:05.940
 And in the case of the second one, even though it is insanely powerful, and you know, people

2:36:05.940 --> 2:36:13.780
 from a vision community, they, as they started probing it inside, they actually understood

2:36:13.780 --> 2:36:14.940
 how far it goes.

2:36:14.940 --> 2:36:21.820
 It's difficult for a person at first to see how well it works.

2:36:21.820 --> 2:36:25.460
 And that's the same as you said, that in case of supervised learning models, you might not

2:36:25.460 --> 2:36:31.500
 kind of see, or it's not that easy for you to understand the strength.

2:36:31.500 --> 2:36:37.020
 Even though you don't believe in magic, to see the magic, the generative, that's really

2:36:37.020 --> 2:36:38.020
 brilliant.

2:36:38.020 --> 2:36:43.260
 So anything that's generative, because then you are at the core of the creation, you get

2:36:43.260 --> 2:36:48.780
 to experience creation without much effort, unless you have to do it from scratch.

2:36:48.780 --> 2:36:54.500
 And it feels that, you know, humans are wired, there is some level of reward for creating

2:36:54.500 --> 2:36:56.500
 stuff.

2:36:56.500 --> 2:37:00.660
 Of course, different people have a different weight on this reward.

2:37:00.660 --> 2:37:05.620
 In the big objective function of a person.

2:37:05.620 --> 2:37:12.140
 You wrote that beautiful is what you intensely pay attention to.

2:37:12.140 --> 2:37:16.180
 Even a cockroach is beautiful if you look very closely.

2:37:16.180 --> 2:37:17.500
 Can you expand on this?

2:37:17.500 --> 2:37:19.860
 What is beauty?

2:37:19.860 --> 2:37:27.180
 So what I wrote here actually corresponds to my subjective experience that I had through

2:37:27.180 --> 2:37:30.300
 extended periods of meditation.

2:37:30.300 --> 2:37:35.500
 It's pretty crazy that at some point, the meditation gets you to the place that you

2:37:35.500 --> 2:37:43.460
 have really increased focus, increased attention, and then you look at the very simple objects

2:37:43.460 --> 2:37:49.660
 that were all the time around you, and look at the table or on the pen, or at the nature.

2:37:49.660 --> 2:37:57.100
 And you notice more and more details, and it becomes very pleasant to look at it.

2:37:57.100 --> 2:38:04.980
 And once again, it kind of reminds me my childhood, like a just pure joy of being.

2:38:04.980 --> 2:38:12.740
 It's also, I have seen even the reverse effect that by default, regardless of what we possess,

2:38:12.740 --> 2:38:15.620
 we very quickly get used to it.

2:38:15.620 --> 2:38:19.020
 And you know, you can have a very beautiful house.

2:38:19.020 --> 2:38:24.780
 And if you don't put sufficient effort, you're just going to get used to it.

2:38:24.780 --> 2:38:28.220
 And it doesn't bring any more joy regardless of what you have.

2:38:28.220 --> 2:38:29.220
 Yeah.

2:38:29.220 --> 2:38:39.940
 Well, I actually, I find that material possessions get in the way of that experience of pure joy.

2:38:39.940 --> 2:38:47.220
 So I've always, I've been very fortunate to just find joy in simple things, just like

2:38:47.220 --> 2:38:52.540
 you're saying, just like, I don't know, objects in my life, just stupid objects like this

2:38:52.540 --> 2:38:58.260
 cup, like thing, you know, just objects sounds, okay, I'm not being eloquent, but literally

2:38:58.260 --> 2:39:04.820
 objects in the world, they're just full of joy, because it's like, I can't believe,

2:39:04.820 --> 2:39:10.580
 one I can't believe that I'm fortunate enough to be alive to experience these objects.

2:39:10.580 --> 2:39:16.140
 And then two, I can't believe humans are clever enough to build these objects.

2:39:16.140 --> 2:39:20.540
 The hierarchy of pleasure that that provides is infinite.

2:39:20.540 --> 2:39:24.940
 I mean, even if you look at the cup of water, so you know, you see, first like a level of

2:39:24.940 --> 2:39:29.940
 like a reflection of light, but then you think, no man, there's like a trillions upon trillions

2:39:29.940 --> 2:39:33.220
 of particles bouncing against each other.

2:39:33.220 --> 2:39:39.620
 There is also the tension on the surface that, you know, if the back, back could like a stand

2:39:39.620 --> 2:39:41.300
 on it and move around.

2:39:41.300 --> 2:39:46.700
 And you think it also has this like a magical property that as you decrease temperature,

2:39:46.700 --> 2:39:52.780
 it actually expands in volume, which allows for the, you know, legs to freeze on the,

2:39:52.780 --> 2:39:56.980
 on the surface and then at the bottom to have actually not freeze, which allows for life

2:39:56.980 --> 2:40:04.580
 like a crazy, you look in detail at some object and you think actually, you know, this table,

2:40:04.580 --> 2:40:07.660
 that was just the figment of someone's imagination at some point.

2:40:07.660 --> 2:40:11.580
 And then there was like a thousands of people involved to actually manufacture it and put

2:40:11.580 --> 2:40:12.580
 it here.

2:40:12.580 --> 2:40:16.660
 And by default, no one cares.

2:40:16.660 --> 2:40:21.180
 And then you can start thinking about evolution, how it all started from single cell organisms

2:40:21.180 --> 2:40:22.620
 that led to this table.

2:40:22.620 --> 2:40:28.860
 And these thoughts, they give me life appreciation and even lack of thoughts, just the pure raw

2:40:28.860 --> 2:40:31.780
 signal also gives the life appreciation.

2:40:31.780 --> 2:40:38.700
 See the thing is, and then that's coupled for me with the sadness that the whole ride

2:40:38.700 --> 2:40:44.940
 ends and perhaps is deeply coupled in that the fact that this experience, this moment

2:40:44.940 --> 2:40:51.580
 ends gives it gives it an intensity that I'm not sure I would otherwise have.

2:40:51.580 --> 2:40:56.340
 So in that same way, I tried to meditate on my own death often.

2:40:56.340 --> 2:40:59.460
 Do you think about your mortality?

2:40:59.460 --> 2:41:03.420
 Are you afraid of death?

2:41:03.420 --> 2:41:09.300
 So fear of death is like one of the most fundamental fears that each of us has.

2:41:09.300 --> 2:41:11.140
 We might be not even aware of it.

2:41:11.140 --> 2:41:16.220
 It requires to look inside to even recognize that it's out there.

2:41:16.220 --> 2:41:23.500
 There is still, let's say, this property of nature that if things would last forever,

2:41:23.500 --> 2:41:26.300
 then they would be also boring to us.

2:41:26.300 --> 2:41:31.780
 The fact that the things change in some way gives any meaning to them.

2:41:31.780 --> 2:41:42.900
 I also, you know, found out that it seems to be very healing to people to have these

2:41:42.900 --> 2:41:51.500
 short experiences, like I guess, psychedelic experiences in which they experience death

2:41:51.500 --> 2:42:00.180
 of self, in which they let go of this fear and then maybe can even increase the appreciation

2:42:00.180 --> 2:42:01.780
 of the moment.

2:42:01.780 --> 2:42:12.220
 It seems that many people, they can easily comprehend the fact that their money is finite

2:42:12.220 --> 2:42:15.860
 while they don't see that time is finite.

2:42:15.860 --> 2:42:18.860
 I have this discussion with Ilya from time to time.

2:42:18.860 --> 2:42:23.700
 He's like, you know, man, life will pass very fast.

2:42:23.700 --> 2:42:27.100
 At some point, I will be 40, 50, 60, 70, and then it's over.

2:42:27.100 --> 2:42:36.020
 This is true, which also makes me believe that every single moment, it is so unique

2:42:36.020 --> 2:42:37.900
 that should be appreciated.

2:42:37.900 --> 2:42:44.780
 And it also makes me think that I should be acting on my life, because otherwise it will

2:42:44.780 --> 2:42:46.280
 pass.

2:42:46.280 --> 2:42:53.300
 I also like this framework of thinking from Jeff Bezos on regret minimization, that like

2:42:53.300 --> 2:43:01.580
 I would like if I will be at the death bed to look back on my life and not regret that

2:43:01.580 --> 2:43:03.500
 I haven't done something.

2:43:03.500 --> 2:43:07.820
 It's usually you might regret that you haven't tried.

2:43:07.820 --> 2:43:13.460
 I'm fine with failing, but I haven't tried.

2:43:13.460 --> 2:43:17.980
 What's the need to turn over currents, try to live a life that if you had to live it

2:43:17.980 --> 2:43:24.900
 infinitely many times, that would be the you'd be OK with that kind of life.

2:43:24.900 --> 2:43:27.060
 So try to live it optimally.

2:43:27.060 --> 2:43:36.700
 I can say that it's almost like unavailable to me where I am in my life.

2:43:36.700 --> 2:43:40.660
 I'm extremely grateful for actually people whom I met.

2:43:40.660 --> 2:43:46.500
 I would say I think that I'm decently smart and so on.

2:43:46.500 --> 2:43:52.100
 But I think that actually to a great extent where I am has to do with the people who I

2:43:52.100 --> 2:43:54.580
 met.

2:43:54.580 --> 2:43:58.420
 Would you be OK if after this conversation you died?

2:43:58.420 --> 2:44:03.780
 So if I'm dead, then it kind of I don't have a choice anymore.

2:44:03.780 --> 2:44:07.260
 So in some sense, there's like a plenty of things that I would like to try out in my

2:44:07.260 --> 2:44:09.020
 life.

2:44:09.020 --> 2:44:13.580
 I feel that I'm gradually going one by one and I'm just doing them.

2:44:13.580 --> 2:44:16.180
 I think that the list will be always infinite.

2:44:16.180 --> 2:44:17.420
 Yeah.

2:44:17.420 --> 2:44:19.260
 So might as well go today.

2:44:19.260 --> 2:44:20.260
 Yeah.

2:44:20.260 --> 2:44:23.780
 I mean, to be clear, I'm not looking forward to dying.

2:44:23.780 --> 2:44:27.580
 I would say if there is no choice, I would accept it.

2:44:27.580 --> 2:44:33.820
 But like in some sense, if there would be a choice, if there would be possibility to

2:44:33.820 --> 2:44:36.820
 live, I would fight for a living.

2:44:36.820 --> 2:44:46.100
 I find it's more honest and real to think about dying today at the end of the day.

2:44:46.100 --> 2:44:52.780
 That seems to me, at least to my brain, more honest slap in the face as opposed to I still

2:44:52.780 --> 2:44:59.500
 have 10 years today, then I'm much more about appreciating the cup and the table and so

2:44:59.500 --> 2:45:06.980
 on and less about silly, worldly accomplishments and all those kinds of things.

2:45:06.980 --> 2:45:12.060
 We have in the company, a person who at some point found out that they have cancer.

2:45:12.060 --> 2:45:16.900
 And that also gives huge perspective with respect to what matters now.

2:45:16.900 --> 2:45:20.900
 And often people in situations like that, they conclude that actually what matters is

2:45:20.900 --> 2:45:24.380
 human connection and love.

2:45:24.380 --> 2:45:32.420
 And people conclude also if you have kids, kids, family, you I think tweeted, we don't

2:45:32.420 --> 2:45:36.420
 assign the minus infinity reward to our death.

2:45:36.420 --> 2:45:39.620
 Such a reward would prevent us from taking any risk.

2:45:39.620 --> 2:45:43.460
 We wouldn't be able to cross the road in fear of being hit by a car.

2:45:43.460 --> 2:45:47.180
 So in the objective function, you mentioned fear of death might be fundamental to the

2:45:47.180 --> 2:45:49.380
 human condition.

2:45:49.380 --> 2:45:55.980
 So as I said, let's assume that they're like a reward functions in our brain.

2:45:55.980 --> 2:46:03.380
 And the interesting thing is even realization how different reward functions can play with

2:46:03.380 --> 2:46:05.100
 your behavior.

2:46:05.100 --> 2:46:10.740
 As a matter of fact, I wouldn't say that you should assign infinite negative reward to

2:46:10.740 --> 2:46:14.060
 anything because that messes up the math.

2:46:14.060 --> 2:46:15.300
 The math doesn't work out.

2:46:15.300 --> 2:46:16.300
 It doesn't work out.

2:46:16.300 --> 2:46:23.580
 As you said, even government or some insurance companies, they assign $9 million to human

2:46:23.580 --> 2:46:24.580
 life.

2:46:24.580 --> 2:46:31.100
 And I'm just saying it with respect to, that might be a hard statement to ourselves, but

2:46:31.100 --> 2:46:36.100
 in some sense that there's a finite value of our own life.

2:46:36.100 --> 2:46:44.700
 I'm trying to put it from perspective of being less or of being more ego less and realizing

2:46:44.700 --> 2:46:47.660
 fragility of my own life.

2:46:47.660 --> 2:46:56.180
 And in some sense, the fear of death might prevent you from acting because anything can

2:46:56.180 --> 2:46:57.340
 cause death.

2:46:57.340 --> 2:46:58.340
 Yeah.

2:46:58.340 --> 2:47:03.100
 And I'm sure actually if you were to put death in the objective function, there's probably

2:47:03.100 --> 2:47:10.900
 so many aspects to death and fear of death and realization of death and mortality.

2:47:10.900 --> 2:47:17.180
 There's just whole components of finiteness of not just your life, but every experience

2:47:17.180 --> 2:47:22.020
 and so on that you're going to have to formalize mathematically.

2:47:22.020 --> 2:47:32.620
 And also, that might lead to you spending a lot of compute cycles on this like a deliberating

2:47:32.620 --> 2:47:38.380
 this terrible future instead of experiencing now.

2:47:38.380 --> 2:47:43.420
 But in some sense, it's also kind of unpleasant simulation to run in your head.

2:47:43.420 --> 2:47:44.420
 Yeah.

2:47:44.420 --> 2:47:51.740
 Do you think there's an objective function that describes the entirety of human life?

2:47:51.740 --> 2:47:56.300
 So usually the way you ask that is what is the meaning of life?

2:47:56.300 --> 2:48:02.340
 Is there a universal objective function that captures the why of life?

2:48:02.340 --> 2:48:06.340
 So yeah, I mean, I suspect that they will ask this question, but it's also a question

2:48:06.340 --> 2:48:08.620
 that I ask myself many, many times.

2:48:08.620 --> 2:48:13.380
 See, I can tell you a framework that I have these days to think about this question.

2:48:13.380 --> 2:48:19.420
 So I think that fundamentally meaning of life has to do with some of our reward functions

2:48:19.420 --> 2:48:25.300
 that we have in brain and they might have to do with, let's say, for instance, curiosity

2:48:25.300 --> 2:48:31.540
 or human connection, which might mean understanding others.

2:48:31.540 --> 2:48:36.940
 It's also possible for a person to slightly modify their reward function, usually they

2:48:36.940 --> 2:48:41.980
 mostly stay fixed, but it's possible to modify reward function and you can pretty much choose.

2:48:41.980 --> 2:48:47.820
 So in some sense, reward functions, optimizing reward functions, they will give you life satisfaction.

2:48:47.820 --> 2:48:49.620
 Is there some randomness in the function?

2:48:49.620 --> 2:48:55.340
 I think when you are born, there is some randomness like you can see that some people, for instance,

2:48:55.340 --> 2:48:58.060
 they care more about building stuff.

2:48:58.060 --> 2:49:01.060
 Some people care more about caring for others.

2:49:01.060 --> 2:49:06.020
 Some people, there are all sorts of default reward functions and then in some sense, you

2:49:06.020 --> 2:49:13.860
 can ask yourself, was this the satisfying way for you to go after this reward function

2:49:13.860 --> 2:49:18.500
 and you just go after this reward function and some people also ask, are these reward

2:49:18.500 --> 2:49:19.500
 functions real?

2:49:19.500 --> 2:49:28.740
 I almost think about it as, let's say, if you would have to discover mathematics, in mathematics

2:49:28.740 --> 2:49:35.100
 you are likely to run into various objects, like complex numbers or differentiation, some

2:49:35.100 --> 2:49:40.100
 other objects and these are very natural objects that arise and similarly, the reward functions

2:49:40.100 --> 2:49:45.740
 that we are having in our brain, they are somewhat very natural that there is a reward

2:49:45.740 --> 2:49:53.580
 function for understanding, like a comprehension, curiosity and so on.

2:49:53.580 --> 2:49:59.300
 So in some sense, they are in the same way natural as their natural objects in mathematics.

2:49:59.300 --> 2:50:00.300
 Interesting.

2:50:00.300 --> 2:50:05.860
 So you know there's the old sort of debate, is mathematics invented or discovered?

2:50:05.860 --> 2:50:08.100
 You're saying reward functions are discovered.

2:50:08.100 --> 2:50:09.100
 So nature provides…

2:50:09.100 --> 2:50:13.220
 So nature provided some, you can still, let's say, expand it throughout the life.

2:50:13.220 --> 2:50:16.700
 Some of the reward functions, they might be futile, like for instance, there might be

2:50:16.700 --> 2:50:21.060
 a reward function, maximize amount of wealth.

2:50:21.060 --> 2:50:25.780
 And this is more like a learning reward function.

2:50:25.780 --> 2:50:30.180
 But we know also that some reward functions, if you optimize them, you won't be quite

2:50:30.180 --> 2:50:31.180
 satisfied.

2:50:31.180 --> 2:50:37.100
 Well, I don't know which part of your reward function resulted in you coming today, but

2:50:37.100 --> 2:50:41.180
 I am deeply appreciative that you did spend your valuable time with me.

2:50:41.180 --> 2:50:44.620
 Wojciech is really fun talking to you.

2:50:44.620 --> 2:50:48.300
 You're brilliant, you're a good human being and it's an honor to meet you and an honor

2:50:48.300 --> 2:50:49.300
 to talk to you.

2:50:49.300 --> 2:50:50.300
 Thanks for talking to me, brother.

2:50:50.300 --> 2:50:57.340
 Thank you Lexelad, I appreciated your questions, I had a lot of time being here.

2:50:57.340 --> 2:51:00.740
 Thanks for listening to this conversation with Wojciech Saremba.

2:51:00.740 --> 2:51:05.620
 To support this podcast, please check out our sponsors in the description.

2:51:05.620 --> 2:51:10.860
 And now, let me leave you with some words from Arthur C. Clarke, who is the author of

2:51:10.860 --> 2:51:13.980
 2001 A Space Odyssey.

2:51:13.980 --> 2:51:21.620
 It may be that our role on this planet is not to worship God, but to create Him.

2:51:21.620 --> 2:51:45.980
 Thank you for listening and I hope to see you next time.

