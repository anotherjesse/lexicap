WEBVTT

00:00.000 --> 00:02.760
 The following is a conversation with Gary Marcus.

00:02.760 --> 00:06.480
 He's a professor emeritus at NYU, founder of robust AI

00:06.480 --> 00:08.200
 and geometric intelligence.

00:08.200 --> 00:10.320
 The latter is a machine learning company

00:10.320 --> 00:13.520
 that was acquired by Uber in 2016.

00:13.520 --> 00:16.480
 He's the author of several books on natural

00:16.480 --> 00:18.160
 and artificial intelligence,

00:18.160 --> 00:20.840
 including his new book, Rebooting AI,

00:20.840 --> 00:23.360
 Building Machines We Can Trust.

00:23.360 --> 00:26.480
 Gary has been a critical voice highlighting the limits

00:26.480 --> 00:28.800
 of deep learning and AI in general

00:28.800 --> 00:33.720
 and discussing the challenges before our AI community

00:33.720 --> 00:35.760
 that must be solved in order to achieve

00:35.760 --> 00:38.320
 artificial general intelligence.

00:38.320 --> 00:40.120
 As I'm having these conversations,

00:40.120 --> 00:43.600
 I try to find paths toward insight, towards new ideas.

00:43.600 --> 00:47.640
 I try to have no ego in the process and gets in the way.

00:47.640 --> 00:52.280
 I'll often continuously try on several hats, several roles.

00:52.280 --> 00:54.720
 One, for example, is the role of a three year old

00:54.720 --> 00:57.120
 who understands very little about anything

00:57.120 --> 01:00.360
 and asks big what and why questions.

01:00.360 --> 01:02.920
 The other might be a role of a devil's advocate

01:02.920 --> 01:05.600
 who presents counter ideas with a goal of arriving

01:05.600 --> 01:08.240
 at greater understanding through debate.

01:08.240 --> 01:11.240
 Hopefully both are useful, interesting,

01:11.240 --> 01:13.440
 and even entertaining at times.

01:13.440 --> 01:15.400
 I ask for your patience as I learn

01:15.400 --> 01:17.760
 to have better conversations.

01:17.760 --> 01:20.800
 This is the Artificial Intelligence Podcast.

01:20.800 --> 01:23.120
 If you enjoy it, subscribe on YouTube,

01:23.120 --> 01:26.320
 give it 5,000 iTunes, support it on Patreon,

01:26.320 --> 01:28.560
 or simply connect with me on Twitter

01:28.560 --> 01:32.520
 at Lex Freedman spelled F R I D M A M.

01:32.520 --> 01:36.320
 And now here's my conversation with Gary Marcus.

01:37.200 --> 01:40.400
 Do you think human civilization will one day have

01:40.400 --> 01:42.960
 to face an AI driven technological singularity

01:42.960 --> 01:46.520
 that will in a societal way modify our place

01:46.520 --> 01:49.120
 in the food chain of intelligent living beings

01:49.120 --> 01:50.120
 on this planet?

01:50.120 --> 01:54.880
 I think our place in the food chain has already changed.

01:54.880 --> 01:57.360
 So there are lots of things people used to do by hand

01:57.360 --> 01:59.200
 that they do with machine.

01:59.200 --> 02:01.840
 If you think of a singularity as like one single moment,

02:01.840 --> 02:03.240
 which is I guess what it suggests,

02:03.240 --> 02:04.600
 I don't know if it'll be like that,

02:04.600 --> 02:07.400
 but I think that there's a lot of gradual change

02:07.400 --> 02:09.280
 and AI is getting better and better.

02:09.280 --> 02:11.440
 I mean, I'm here to tell you why I think it's not nearly

02:11.440 --> 02:14.400
 as good as people think, but the overall trend is clear.

02:14.400 --> 02:17.400
 Maybe Rick Hertzwell thinks it's an exponential

02:17.400 --> 02:19.440
 and I think it's linear in some cases,

02:19.440 --> 02:22.440
 it's close to zero right now, but it's all gonna happen.

02:22.440 --> 02:24.840
 We are gonna get to human level intelligence

02:24.840 --> 02:27.440
 or whatever you want, what you will,

02:27.440 --> 02:30.240
 artificial general intelligence at some point.

02:30.240 --> 02:31.840
 And that's certainly gonna change our place

02:31.840 --> 02:32.680
 in the food chain.

02:32.680 --> 02:35.240
 Cause a lot of the tedious things that we do now,

02:35.240 --> 02:36.280
 we're gonna have machines do

02:36.280 --> 02:38.600
 and a lot of the dangerous things that we do now,

02:38.600 --> 02:39.920
 we're gonna have machines do.

02:39.920 --> 02:41.720
 I think our whole lives are gonna change

02:41.720 --> 02:45.040
 from people finding their meaning through their work,

02:45.040 --> 02:46.760
 through people finding their meaning

02:46.760 --> 02:48.720
 through creative expression.

02:48.720 --> 02:53.720
 So the singularity will be a very gradual,

02:53.720 --> 02:56.400
 in fact, removing the meaning of the word singularity.

02:56.400 --> 03:00.320
 It'll be a very gradual transformation in your view.

03:00.320 --> 03:03.240
 I think that it'll be somewhere in between

03:03.240 --> 03:05.480
 and I guess it depends what you mean by gradual and sudden.

03:05.480 --> 03:07.160
 I don't think it's gonna be one day.

03:07.160 --> 03:10.040
 I think it's important to realize that intelligence

03:10.040 --> 03:11.640
 is a multi dimensional variable.

03:11.640 --> 03:16.640
 So people sort of write this stuff as if like IQ was one number

03:17.520 --> 03:20.440
 and the day that you hit 262

03:20.440 --> 03:22.520
 or whatever you displace the human beings.

03:22.520 --> 03:25.280
 And really there's lots of facets to intelligence.

03:25.280 --> 03:26.680
 So there's verbal intelligence

03:26.680 --> 03:28.520
 and there's motor intelligence

03:28.520 --> 03:32.000
 and there's mathematical intelligence and so forth.

03:32.000 --> 03:34.560
 Machines in their mathematical intelligence

03:34.560 --> 03:36.840
 far exceed most people already

03:36.840 --> 03:38.080
 in their ability to play games.

03:38.080 --> 03:40.040
 They far exceed most people already.

03:40.040 --> 03:41.720
 In their ability to understand language,

03:41.720 --> 03:44.680
 they lag behind my five year old, far behind my five year old.

03:44.680 --> 03:46.800
 So there are some facets of intelligence,

03:46.800 --> 03:49.400
 the machines of graphs and some that they haven't.

03:49.400 --> 03:51.760
 And we have a lot of work left to do

03:51.760 --> 03:54.280
 to get them to say understand natural language

03:54.280 --> 03:58.920
 or to understand how to flexibly approach some,

03:58.920 --> 04:03.000
 kind of novel MacGyver problem solving kind of situation.

04:03.000 --> 04:05.640
 And I don't know that all of these things will come once.

04:05.640 --> 04:07.960
 I think there are certain vital prerequisites

04:07.960 --> 04:09.320
 that we're missing now.

04:09.320 --> 04:12.520
 So for example, machines don't really have common sense now.

04:12.520 --> 04:15.560
 So they don't understand that bottles contain water

04:15.560 --> 04:18.160
 and that people drink water to quench their thirst

04:18.160 --> 04:19.360
 and that they don't want to dehydrate.

04:19.360 --> 04:22.080
 They don't know these basic facts about human beings.

04:22.080 --> 04:25.240
 And I think that that's a great limiting step for many things.

04:25.240 --> 04:27.640
 It's a great limiting step for reading, for example,

04:27.640 --> 04:29.680
 because stories depend on things like,

04:29.680 --> 04:31.480
 oh my God, that person's running out of water.

04:31.480 --> 04:33.000
 That's why they did this thing.

04:33.000 --> 04:37.040
 Or if they only had water, they could put out the fire.

04:37.040 --> 04:39.320
 So you watch a movie and your knowledge

04:39.320 --> 04:41.200
 about how things work matter.

04:41.200 --> 04:44.280
 And so a computer can't understand that movie

04:44.280 --> 04:45.760
 if it doesn't have that background knowledge.

04:45.760 --> 04:47.880
 Same thing if you read a book.

04:47.880 --> 04:49.640
 And so there are lots of places where

04:49.640 --> 04:53.720
 if we had a good machine interpretable set of common sense,

04:53.720 --> 04:56.560
 many things would accelerate relatively quickly,

04:56.560 --> 04:59.640
 but I don't think even that is like a single point.

04:59.640 --> 05:02.520
 There's many different aspects of knowledge.

05:02.520 --> 05:05.640
 And we might, for example, find that we make a lot of progress

05:05.640 --> 05:08.440
 on physical reasoning, getting machines to understand,

05:08.440 --> 05:10.920
 for example, how keys fit into the locks

05:10.920 --> 05:15.400
 or that kind of stuff or how this gadget here works

05:15.400 --> 05:17.520
 and so forth and so on.

05:17.520 --> 05:19.480
 Machines might do that long before they do

05:19.480 --> 05:21.720
 really good psychological reasoning,

05:21.720 --> 05:24.360
 because it's easier to get kind of labeled data

05:24.360 --> 05:28.640
 or to do direct experimentation on a microphone stand

05:28.640 --> 05:31.760
 than it is to do direct experimentation on human beings

05:31.760 --> 05:34.800
 to understand the levers that guide them.

05:34.800 --> 05:36.840
 That's a really interesting point actually,

05:36.840 --> 05:39.680
 whether it's easier to gain common sense knowledge

05:39.680 --> 05:41.720
 or psychological knowledge.

05:41.720 --> 05:43.280
 I would say the common sense knowledge

05:43.280 --> 05:46.840
 includes both physical knowledge and psychological knowledge.

05:46.840 --> 05:48.120
 And the argument I was making.

05:48.120 --> 05:49.640
 It's physical versus psychological.

05:49.640 --> 05:51.080
 Yeah, physical versus psychological.

05:51.080 --> 05:53.240
 The argument I was making is physical knowledge

05:53.240 --> 05:54.240
 might be more accessible,

05:54.240 --> 05:56.040
 because you could have a robot, for example,

05:56.040 --> 05:58.400
 lift a bottle, try putting a bottle cap on it,

05:58.400 --> 06:00.400
 see that it falls off if it does this

06:00.400 --> 06:02.000
 and see that it could turn it upside down

06:02.000 --> 06:04.680
 and so the robot could do some experimentation.

06:04.680 --> 06:07.200
 We do some of our psychological reasoning

06:07.200 --> 06:09.240
 by looking at our own minds.

06:09.240 --> 06:11.560
 So I can sort of guess how you might react

06:11.560 --> 06:13.760
 to something based on how I think I would react to it.

06:13.760 --> 06:15.960
 And robots don't have that intuition,

06:15.960 --> 06:18.440
 and they also can't do experiments on people

06:18.440 --> 06:20.480
 in the same way or we'll probably shut them down.

06:20.480 --> 06:24.240
 So if we wanted to have robots figure out

06:24.240 --> 06:27.760
 how I respond to pain by pinching me in different ways,

06:27.760 --> 06:29.040
 like that's probably,

06:29.040 --> 06:31.000
 it's not gonna make it past the human subjects board

06:31.000 --> 06:32.840
 and companies are gonna get sued or whatever.

06:32.840 --> 06:35.800
 So there's certain kinds of practical experience

06:35.800 --> 06:39.640
 that are limited or off limits to robots.

06:39.640 --> 06:41.040
 That's a really interesting point.

06:41.040 --> 06:46.040
 What is more difficult to gain a grounding in?

06:47.560 --> 06:49.960
 Because to play devil's advocate,

06:49.960 --> 06:54.960
 I would say that human behavior is easier expressed

06:55.000 --> 06:56.960
 in data and digital form.

06:56.960 --> 06:59.040
 And so when you look at Facebook algorithms,

06:59.040 --> 07:01.120
 they get to observe human behavior.

07:01.120 --> 07:04.640
 So you get to study and manipulate even a human behavior

07:04.640 --> 07:07.720
 in a way that you perhaps cannot study

07:07.720 --> 07:09.560
 or manipulate the physical world.

07:09.560 --> 07:14.440
 So it's true why you said pain is like physical pain,

07:14.440 --> 07:16.040
 but that's again the physical world.

07:16.040 --> 07:20.120
 Emotional pain might be much easier to experiment with,

07:20.120 --> 07:22.760
 perhaps unethical, but nevertheless,

07:22.760 --> 07:25.400
 some would argue it's already going on.

07:25.400 --> 07:27.360
 I think that you're right, for example,

07:27.360 --> 07:30.840
 that Facebook does a lot of experimentation

07:30.840 --> 07:32.920
 in psychological reasoning.

07:32.920 --> 07:37.080
 In fact, Zuckerberg talked about AI at a talk

07:37.080 --> 07:39.240
 that he gave nips, I wasn't there,

07:39.240 --> 07:41.320
 at the conference that's been renamed Neurups,

07:41.320 --> 07:43.640
 but he used to be called nips when he gave the talk.

07:43.640 --> 07:45.280
 And he talked about Facebook basically

07:45.280 --> 07:47.120
 having a gigantic theory of mind.

07:47.120 --> 07:49.520
 So I think it is certainly possible.

07:49.520 --> 07:51.240
 I mean, Facebook does some of that.

07:51.240 --> 07:52.640
 I think they have a really good idea

07:52.640 --> 07:53.920
 of how to addict people to things.

07:53.920 --> 07:56.440
 They understand what draws people back to things.

07:56.440 --> 07:57.280
 And I think they exploit it

07:57.280 --> 07:59.200
 in ways that I'm not very comfortable with.

07:59.200 --> 08:03.320
 But even so, I think that there are only some slices

08:03.320 --> 08:05.640
 of human experience that they can access

08:05.640 --> 08:07.240
 through the kind of interface they have.

08:07.240 --> 08:08.960
 And of course, they're doing all kinds of VR stuff,

08:08.960 --> 08:11.720
 and maybe that'll change and they'll expand their data.

08:11.720 --> 08:14.920
 And I'm sure that that's part of their goal.

08:14.920 --> 08:16.840
 So it is an interesting question.

08:16.840 --> 08:21.840
 I think love, fear, insecurity, all of the things

08:23.080 --> 08:26.640
 that I would say some of the deepest things

08:26.640 --> 08:28.640
 about human nature and the human mind

08:28.640 --> 08:30.480
 could be explored to digital form.

08:30.480 --> 08:32.240
 It's that you're actually the first person

08:32.240 --> 08:33.680
 just now that brought up.

08:33.680 --> 08:35.840
 I wonder what is more difficult

08:35.840 --> 08:40.240
 because I think folks who are the slow,

08:40.240 --> 08:41.840
 and we'll talk a lot about deep learning,

08:41.840 --> 08:44.840
 but the people who are thinking beyond deep learning

08:44.840 --> 08:46.400
 are thinking about the physical world.

08:46.400 --> 08:48.040
 You're starting to think about robotics

08:48.040 --> 08:49.160
 in the home robotics.

08:49.160 --> 08:52.320
 How do we make robots manipulate objects

08:52.320 --> 08:55.000
 which requires an understanding of the physical world

08:55.000 --> 08:57.280
 and then requires common sense reasoning.

08:57.280 --> 08:59.440
 And that has felt to be like the next step

08:59.440 --> 09:00.440
 for common sense reasoning.

09:00.440 --> 09:02.120
 But you've now brought up the idea

09:02.120 --> 09:03.640
 that there's also the emotional part.

09:03.640 --> 09:06.840
 And it's interesting whether that's hard or easy.

09:06.840 --> 09:08.520
 I think some parts of it are and some aren't.

09:08.520 --> 09:10.960
 So my company that I recently founded

09:10.960 --> 09:13.960
 with Brod Brooks from MIT for many years

09:13.960 --> 09:17.240
 and so forth, we're interested in both.

09:17.240 --> 09:18.600
 We're interested in physical reasoning

09:18.600 --> 09:21.480
 and psychological reasoning among many other things.

09:21.480 --> 09:26.120
 And there are pieces of each of these that are accessible.

09:26.120 --> 09:28.000
 So if you want a robot to figure out

09:28.000 --> 09:29.720
 whether it can fit under a table,

09:29.720 --> 09:33.640
 that's a relatively accessible piece of physical reasoning.

09:33.640 --> 09:34.760
 If you know the height of the table

09:34.760 --> 09:37.000
 and you know the height of the robot, it's not that hard.

09:37.000 --> 09:39.920
 If you wanted to do physical reasoning about Jenga,

09:39.920 --> 09:41.480
 it gets a little bit more complicated

09:41.480 --> 09:43.840
 and you have to have higher resolution data

09:43.840 --> 09:45.240
 in order to do it.

09:45.240 --> 09:46.880
 With psychological reasoning,

09:46.880 --> 09:49.320
 it's not that hard to know, for example,

09:49.320 --> 09:51.680
 that people have goals and they like to act on those goals,

09:51.680 --> 09:54.880
 but it's really hard to know exactly what those goals are.

09:54.880 --> 09:56.800
 My idea is a frustration.

09:56.800 --> 09:58.800
 I mean, you could argue it's extremely difficult

09:58.800 --> 10:01.480
 to understand the sources of human frustration

10:01.480 --> 10:05.760
 as they're playing Jenga with you or not.

10:05.760 --> 10:07.960
 You could argue that it's very accessible.

10:07.960 --> 10:10.440
 There's some things that are gonna be obvious and some not.

10:10.440 --> 10:14.240
 So I don't think anybody really can do this well yet,

10:14.240 --> 10:16.640
 but I think it's not inconceivable

10:16.640 --> 10:20.120
 to imagine machines in the not so distant future

10:20.120 --> 10:24.200
 being able to understand that if people lose in a game

10:24.200 --> 10:26.240
 that they don't like that.

10:26.240 --> 10:27.960
 That's not such a hard thing to program

10:27.960 --> 10:30.000
 and it's pretty consistent across people.

10:30.000 --> 10:31.560
 Most people don't enjoy losing

10:31.560 --> 10:34.640
 and so that makes it relatively easy to code.

10:34.640 --> 10:36.840
 On the other hand, if you wanted to capture everything

10:36.840 --> 10:39.160
 about frustration, well, people get frustrated

10:39.160 --> 10:40.320
 for a lot of different reasons.

10:40.320 --> 10:42.360
 They might get sexually frustrated,

10:42.360 --> 10:43.200
 they might get frustrated,

10:43.200 --> 10:45.160
 they can get their promotion at work,

10:45.160 --> 10:46.880
 all kinds of different things.

10:46.880 --> 10:48.600
 And the more you expand the scope,

10:48.600 --> 10:51.520
 the harder it is for anything like the existing techniques

10:51.520 --> 10:53.000
 to really do that.

10:53.000 --> 10:55.640
 So I'm talking to Gary Kasparov next week

10:55.640 --> 10:57.800
 and he seemed pretty frustrated with this game

10:57.800 --> 10:58.640
 against the blue.

10:58.640 --> 11:00.280
 So yeah, well, I'm frustrated with my game

11:00.280 --> 11:02.640
 against him last year because I played him.

11:02.640 --> 11:04.880
 I had two excuses, I'll give you my excuses up front

11:04.880 --> 11:07.040
 that it won't mitigate the outcome.

11:07.040 --> 11:11.080
 I was jet lagged and I hadn't played in 25 or 30 years,

11:11.080 --> 11:13.000
 but the outcome is he completely destroyed me

11:13.000 --> 11:14.400
 and it wasn't even close.

11:14.400 --> 11:19.400
 Have you ever been beaten in any board game by a machine?

11:19.720 --> 11:24.720
 I have, I actually played the predecessor to deep blue.

11:24.720 --> 11:27.920
 Deep thought, I believe it was called.

11:27.920 --> 11:29.960
 And that too crushed me.

11:31.960 --> 11:35.320
 And after that, you realize it's over for us.

11:35.320 --> 11:36.800
 Well, there's no point in my playing deep blue.

11:36.800 --> 11:40.240
 I mean, it's a waste of deep blues, computation.

11:40.240 --> 11:41.520
 I mean, I played Kasparov

11:41.520 --> 11:44.800
 because we both gave lectures this same event

11:44.800 --> 11:46.040
 and he was playing 30 people.

11:46.040 --> 11:46.880
 I forgot to mention that.

11:46.880 --> 11:47.920
 Not only did he crush me,

11:47.920 --> 11:50.640
 but he crushed 29 other people at the same time.

11:50.640 --> 11:54.880
 I mean, but the actual philosophical and emotional

11:54.880 --> 11:57.880
 experience of being beaten by a machine, I imagine,

11:57.880 --> 12:01.360
 is, I mean, to you who thinks about these things,

12:01.360 --> 12:05.760
 maybe a profound experience or no, it was a simple.

12:05.760 --> 12:06.600
 No, I mean, I think.

12:06.600 --> 12:07.720
 Mathematical experience.

12:07.720 --> 12:10.280
 Yeah, I think a game like chess particularly

12:10.280 --> 12:12.720
 where it's, you know, you have perfect information,

12:12.720 --> 12:14.760
 it's, you know, two player closed end

12:14.760 --> 12:16.920
 and there's more computation for the computer.

12:16.920 --> 12:18.840
 It's no surprise the machine wins.

12:18.840 --> 12:22.000
 I mean, I'm not sad when a computer,

12:22.000 --> 12:23.920
 I'm not sad when a computer calculates

12:23.920 --> 12:25.200
 a cube root faster than me.

12:25.200 --> 12:27.840
 Like, I know I can't win that game.

12:27.840 --> 12:28.880
 I'm not going to try.

12:28.880 --> 12:32.080
 Well, with a system like AlphaGo or AlphaZero,

12:32.080 --> 12:35.080
 do you see a little bit more magic in a system like that,

12:35.080 --> 12:37.240
 even though it's simply playing a board game,

12:37.240 --> 12:39.920
 but because there's a strong learning component?

12:39.920 --> 12:41.320
 You know, I find you should mention that

12:41.320 --> 12:42.600
 in the context of this conversation

12:42.600 --> 12:45.320
 because Kasparov and I are working on an article

12:45.320 --> 12:48.240
 that's going to be called AI is not magic.

12:48.240 --> 12:50.480
 And, you know, neither one of us thinks that it's magic.

12:50.480 --> 12:51.960
 And part of the point of this article

12:51.960 --> 12:55.120
 is that AI is actually a grab bag of different techniques

12:55.120 --> 12:56.040
 and some of them have,

12:56.040 --> 13:00.040
 or they each have their own unique strengths and weaknesses.

13:00.040 --> 13:03.080
 So, you know, you read media accounts and it's like,

13:03.080 --> 13:06.560
 ooh, AI, it must be magical or can solve any problem.

13:06.560 --> 13:09.480
 Well, no, some problems are really accessible

13:09.480 --> 13:11.960
 like chess and Go and other problems like reading

13:11.960 --> 13:14.920
 are completely outside the current technology.

13:14.920 --> 13:17.080
 And it's not like you can take the technology

13:17.080 --> 13:21.320
 that drives AlphaGo and apply it to reading and get anywhere.

13:21.320 --> 13:23.160
 You know, DeepMind has tried that a bit.

13:23.160 --> 13:24.480
 They have all kinds of resources.

13:24.480 --> 13:26.120
 You know, they built AlphaGo and they have,

13:26.120 --> 13:28.400
 you know, they, I wrote a piece recently

13:28.400 --> 13:30.480
 that they lost and you can argue about the word lost,

13:30.480 --> 13:34.840
 but they spent $530 million more than they made last year.

13:34.840 --> 13:36.600
 So, you know, they're making huge investments.

13:36.600 --> 13:37.840
 They have a large budget

13:37.840 --> 13:40.880
 and they have applied the same kinds of techniques

13:40.880 --> 13:43.120
 to reading or to language.

13:43.120 --> 13:45.480
 And it's just much less productive there

13:45.480 --> 13:47.840
 because it's a fundamentally different kind of problem.

13:47.840 --> 13:50.600
 Chess and Go and so forth are closed in problems.

13:50.600 --> 13:52.960
 The rules haven't changed in 2,500 years.

13:52.960 --> 13:54.680
 There's only so many moves you can make.

13:54.680 --> 13:56.400
 You can talk about the exponential

13:56.400 --> 13:58.160
 as you look at the combinations of moves.

13:58.160 --> 14:01.200
 But fundamentally, you know, the Go board has 361 squares.

14:01.200 --> 14:02.040
 That's it.

14:02.040 --> 14:04.040
 That's the only, you know, those intersections

14:04.040 --> 14:07.240
 are the only places that you can place your stone.

14:07.240 --> 14:09.080
 Whereas when you're reading,

14:09.080 --> 14:11.400
 the next sentence could be anything.

14:11.400 --> 14:13.240
 You know, it's completely up to the writer

14:13.240 --> 14:14.400
 what they're gonna do next.

14:14.400 --> 14:16.200
 That's fascinating that you think this way.

14:16.200 --> 14:17.920
 You're clearly a brilliant mind

14:17.920 --> 14:19.680
 who points out the emperor has no clothes,

14:19.680 --> 14:22.280
 but so I'll play the role of a person who says...

14:22.280 --> 14:23.280
 You're gonna put clothes on the emperor?

14:23.280 --> 14:24.120
 Good luck with it.

14:24.120 --> 14:27.600
 Romanticizes the notion of the emperor, period.

14:27.600 --> 14:30.120
 Suggesting that clothes don't even matter.

14:30.120 --> 14:33.560
 Okay, so that's really interesting

14:33.560 --> 14:36.240
 that you're talking about language.

14:36.240 --> 14:37.720
 So there's the physical world

14:37.720 --> 14:39.640
 of being able to move about the world,

14:39.640 --> 14:41.920
 making an omelet and coffee and so on.

14:41.920 --> 14:46.000
 There's language where you first understand

14:46.000 --> 14:47.240
 what's being written

14:47.240 --> 14:48.800
 and then maybe even more complicated

14:48.800 --> 14:51.080
 than that having a natural dialogue.

14:51.080 --> 14:53.560
 And then there's the game of Go and chess.

14:53.560 --> 14:57.480
 I would argue that language is much closer to Go

14:57.480 --> 14:59.680
 than it is to the physical world.

14:59.680 --> 15:01.440
 Like it is still very constrained.

15:01.440 --> 15:03.560
 When you say the possibility

15:03.560 --> 15:05.560
 of the number of sentences that could come,

15:05.560 --> 15:09.240
 it is huge, but it nevertheless is much more constrained.

15:09.240 --> 15:12.680
 It feels maybe I'm wrong than the possibilities

15:12.680 --> 15:14.480
 that the physical world brings us.

15:14.480 --> 15:15.840
 There's something to what you say

15:15.840 --> 15:17.640
 in some ways in which I disagree.

15:17.640 --> 15:20.560
 So one interesting thing about language

15:20.560 --> 15:23.320
 is that it abstracts away.

15:23.320 --> 15:24.960
 This bottle, I don't know if they're gonna be

15:24.960 --> 15:27.200
 in the field of view, is on this table.

15:27.200 --> 15:28.880
 And I use the word on here

15:28.880 --> 15:32.960
 and I can use the word on here, maybe not here,

15:32.960 --> 15:36.960
 but that one word encompasses in analog space

15:36.960 --> 15:39.360
 a sort of infinite number of possibilities.

15:39.360 --> 15:43.080
 So there is a way in which language filters down

15:43.080 --> 15:46.680
 the variation of the world and there's other ways.

15:46.680 --> 15:49.960
 So we have a grammar and more or less,

15:49.960 --> 15:51.760
 you have to follow the rules of that grammar.

15:51.760 --> 15:52.760
 You can break them a little bit,

15:52.760 --> 15:55.480
 but by and large, we follow the rules of grammar

15:55.480 --> 15:57.080
 and so that's a constraint on language.

15:57.080 --> 15:59.480
 So there are ways in which language is a constrained system.

15:59.480 --> 16:02.320
 On the other hand, there are many arguments.

16:02.320 --> 16:04.960
 Let's say there's an infinite number of possible sentences

16:04.960 --> 16:07.720
 and you can establish that by just stacking them up.

16:07.720 --> 16:09.560
 So I think there's water on the table.

16:09.560 --> 16:11.800
 You think that I think there's water on the table.

16:11.800 --> 16:12.960
 Your mother thinks that you think

16:12.960 --> 16:14.640
 that I think the water is on the table.

16:14.640 --> 16:17.020
 Your brother thinks that maybe your mom is wrong

16:17.020 --> 16:18.720
 to think that you think that I think.

16:18.720 --> 16:22.040
 So we can make it in sentences of infinite length

16:22.040 --> 16:23.640
 or we can stack up adjectives.

16:23.640 --> 16:26.480
 This is a very silly example of very, very silly example

16:26.480 --> 16:28.880
 of very, very, very, very, very, very, very silly example

16:28.880 --> 16:29.720
 and so forth.

16:29.720 --> 16:31.040
 So there are good arguments

16:31.040 --> 16:32.520
 that there's an infinite range of sentences.

16:32.520 --> 16:35.840
 In any case, it's vast by any reasonable measure.

16:35.840 --> 16:38.000
 And for example, almost anything in the physical world

16:38.000 --> 16:40.500
 we can talk about in the language world.

16:40.500 --> 16:43.840
 And interestingly, many of the sentences that we understand

16:43.840 --> 16:46.880
 we can only understand if we have a very rich model

16:46.880 --> 16:47.880
 of the physical world.

16:47.880 --> 16:50.660
 So I don't ultimately want to adjudicate the debate

16:50.660 --> 16:53.360
 that I think you just set up, but I find it interesting.

16:54.480 --> 16:57.200
 Maybe the physical world is even more complicated

16:57.200 --> 16:58.040
 than language.

16:58.040 --> 17:00.200
 I think that's fair, but you think

17:00.200 --> 17:03.160
 that language is really, really complicated.

17:03.160 --> 17:04.120
 It's really, really hard.

17:04.120 --> 17:06.120
 Well, it's really, really hard for machines,

17:06.120 --> 17:08.520
 for linguists, people trying to understand it.

17:08.520 --> 17:09.680
 It's not that hard for children

17:09.680 --> 17:12.200
 and that's part of what's driven my whole career.

17:12.200 --> 17:14.400
 I was a student of Stephen Pinkers

17:14.400 --> 17:15.360
 and we were trying to figure out

17:15.360 --> 17:18.740
 why kids could learn language when machines couldn't.

17:18.740 --> 17:20.600
 I think we're gonna get into language.

17:20.600 --> 17:22.480
 We're gonna get into communication intelligence

17:22.480 --> 17:24.240
 and neural networks and so on.

17:24.240 --> 17:29.240
 But let me return to the high level of the futuristic

17:31.080 --> 17:32.520
 for a brief moment.

17:32.520 --> 17:37.320
 So you've written in your book, in your new book,

17:37.320 --> 17:39.960
 it will be arrogant to suppose that we could forecast

17:39.960 --> 17:42.480
 where AI will be, where the impact it will have

17:42.480 --> 17:45.160
 in a thousand years or even 500 years.

17:45.160 --> 17:47.080
 So let me ask you to be arrogant.

17:48.360 --> 17:51.520
 What do AI systems with or without physical bodies

17:51.520 --> 17:53.520
 look like 100 years from now?

17:53.520 --> 17:56.800
 If you would, just a, you can't predict,

17:56.800 --> 18:00.280
 but if you were to philosophize and imagine, do.

18:00.280 --> 18:02.040
 Can I first justify the arrogance

18:02.040 --> 18:04.080
 before you try to push me beyond it?

18:04.080 --> 18:05.920
 Sure.

18:05.920 --> 18:07.720
 I mean, there are examples, like, you know,

18:07.720 --> 18:09.720
 people figured out how electricity worked.

18:09.720 --> 18:12.280
 They had no idea that that was gonna lead to cell phones,

18:12.280 --> 18:13.120
 right?

18:13.120 --> 18:15.600
 I mean, things can move awfully fast

18:15.600 --> 18:17.920
 once new technologies are perfected.

18:17.920 --> 18:19.440
 Even when they made transistors,

18:19.440 --> 18:21.080
 they weren't really thinking that cell phones

18:21.080 --> 18:23.320
 would lead to social networking.

18:23.320 --> 18:25.720
 There are nevertheless predictions of the future,

18:25.720 --> 18:28.800
 which are statistically unlikely to come to be,

18:28.800 --> 18:29.840
 but nevertheless is the best.

18:29.840 --> 18:31.360
 You're asking me to be wrong.

18:31.360 --> 18:32.200
 I'm asking you to be.

18:32.200 --> 18:34.000
 Which way would I like to be wrong?

18:34.000 --> 18:37.480
 Pick the least unlikely to be wrong thing,

18:37.480 --> 18:39.720
 even though it's most very likely to be wrong.

18:39.720 --> 18:40.560
 I mean, here's some things

18:40.560 --> 18:42.720
 that we can safely predict, I suppose.

18:42.720 --> 18:46.240
 We can predict that AI will be faster than it is now.

18:47.240 --> 18:49.480
 It will be cheaper than it is now.

18:49.480 --> 18:52.840
 It will be better in the sense of being more general

18:52.840 --> 18:55.720
 and applicable in more places.

18:56.960 --> 18:58.320
 It will be pervasive.

18:59.280 --> 19:01.560
 You know, I mean, these are easy predictions.

19:01.560 --> 19:03.280
 I'm sort of modeling them in my head

19:03.280 --> 19:05.800
 on Jeff Bezos's famous predictions.

19:05.800 --> 19:07.280
 He says, I can't predict the future.

19:07.280 --> 19:08.120
 Not in every way.

19:08.120 --> 19:10.560
 I'm paraphrasing, but I can predict

19:10.560 --> 19:13.200
 that people will never wanna pay more money for their stuff.

19:13.200 --> 19:15.200
 They're never gonna want it to take longer to get there.

19:15.200 --> 19:17.760
 And you know, so like, you can't predict everything,

19:17.760 --> 19:18.880
 but you can predict some things.

19:18.880 --> 19:20.920
 Sure, of course it's gonna be faster and better.

19:20.920 --> 19:24.520
 And what we can't really predict

19:24.520 --> 19:28.720
 is the full scope of where AI will be in a certain period.

19:28.720 --> 19:31.280
 I mean, I think it's safe to say

19:31.280 --> 19:34.840
 that although I'm very skeptical about current AI,

19:35.720 --> 19:37.720
 that it's possible to do much better.

19:37.720 --> 19:39.760
 You know, there's no in principle at argument

19:39.760 --> 19:42.160
 that says AI is an insolvable problem,

19:42.160 --> 19:43.640
 that there's magic inside our brains

19:43.640 --> 19:45.000
 that will never be captured.

19:45.000 --> 19:46.840
 I mean, I've heard people make those kind of arguments.

19:46.840 --> 19:49.040
 I don't think they're very good.

19:49.040 --> 19:54.040
 So AI is gonna come and probably 500 years of planning

19:54.720 --> 19:55.560
 to get there.

19:55.560 --> 19:59.280
 And then once it's here, it really will change everything.

19:59.280 --> 20:00.720
 So when you say AI is gonna come,

20:00.720 --> 20:03.680
 are you talking about human level intelligence?

20:03.680 --> 20:05.000
 So maybe I...

20:05.000 --> 20:06.680
 I like the term general intelligence.

20:06.680 --> 20:09.560
 So I don't think that the ultimate AI,

20:09.560 --> 20:12.000
 if there is such a thing, is gonna look just like humans.

20:12.000 --> 20:13.640
 I think it's gonna do some things

20:13.640 --> 20:16.600
 that humans do better than current machines,

20:16.600 --> 20:18.600
 like reason flexibly.

20:18.600 --> 20:21.200
 And understand language and so forth.

20:21.200 --> 20:23.480
 But it doesn't mean they have to be identical to humans.

20:23.480 --> 20:26.000
 So for example, humans have terrible memory

20:26.000 --> 20:28.960
 and they suffer from what some people call

20:28.960 --> 20:29.960
 motivated reasoning.

20:29.960 --> 20:32.480
 So they like arguments that seem to support them

20:32.480 --> 20:35.480
 and they dismiss arguments that they don't like.

20:35.480 --> 20:38.720
 There's no reason that a machine should ever do that.

20:38.720 --> 20:42.320
 So you see that those limitations of memory

20:42.320 --> 20:43.960
 as a bug, not a feature?

20:43.960 --> 20:44.880
 Absolutely.

20:44.880 --> 20:46.680
 I'll say two things about that.

20:46.680 --> 20:48.480
 One is I was on a panel with Danny Kahneman,

20:48.480 --> 20:50.320
 the Nobel Prize winner last night,

20:50.320 --> 20:51.800
 and we were talking about this stuff.

20:51.800 --> 20:54.040
 And I think what we converged on is that

20:54.040 --> 20:56.160
 the humans are a low bar to exceed.

20:56.160 --> 20:58.960
 They may be outside of our skill right now,

20:58.960 --> 21:01.160
 but as AI programmers,

21:01.160 --> 21:04.320
 but eventually AI will exceed it.

21:04.320 --> 21:06.120
 So we're not talking about human level AI.

21:06.120 --> 21:07.920
 We're talking about general intelligence

21:07.920 --> 21:09.480
 that can do all kinds of different things

21:09.480 --> 21:12.240
 and do it without some of the flaws that human beings have.

21:12.240 --> 21:14.000
 The other thing I'll say is I wrote a whole book actually

21:14.000 --> 21:15.200
 about the flaws of humans.

21:15.200 --> 21:19.120
 It's actually a nice counterpoint to the current book.

21:19.120 --> 21:21.360
 So I wrote a book called Cluj,

21:21.360 --> 21:24.000
 which was about the limits of the human mind.

21:24.000 --> 21:26.320
 The current book is kind of about those few things

21:26.320 --> 21:28.720
 that humans do a lot better than machines.

21:28.720 --> 21:31.720
 Do you think it's possible that the flaws of the human mind,

21:31.720 --> 21:34.960
 the limits of memory, our mortality,

21:34.960 --> 21:39.960
 our bias is a strength, not a weakness.

21:40.240 --> 21:43.480
 That is the thing that enables

21:43.480 --> 21:47.760
 from which motivation springs and meaning springs.

21:47.760 --> 21:49.480
 I've heard a lot of arguments like this.

21:49.480 --> 21:50.880
 I've never found them that convincing.

21:50.880 --> 21:55.120
 I think that there's a lot of making lemonade out of lemons.

21:55.120 --> 21:58.280
 So we, for example, do a lot of free association

21:58.280 --> 22:00.800
 where one idea just leads to the next

22:00.800 --> 22:02.600
 and they're not really that well connected.

22:02.600 --> 22:04.520
 And we enjoy that and we make poetry out of it

22:04.520 --> 22:07.120
 and we make kind of movies with free associations

22:07.120 --> 22:08.160
 and it's fun and whatever.

22:08.160 --> 22:12.320
 I don't think that's really a virtue of the system.

22:12.320 --> 22:15.360
 I think that the limitations in human reasoning

22:15.360 --> 22:16.600
 actually get us in a lot of trouble.

22:16.600 --> 22:19.320
 Like for example, politically, we can't see eye to eye

22:19.320 --> 22:22.000
 because we have the motivational reasoning I was talking about

22:22.000 --> 22:25.120
 and something related called confirmation bias.

22:25.120 --> 22:26.480
 So we have all of these problems

22:26.480 --> 22:28.640
 that actually make for a rougher society

22:28.640 --> 22:29.960
 because we can't get along

22:29.960 --> 22:32.760
 because we can't interpret the data in shared ways.

22:34.360 --> 22:36.480
 And then we do some nice stuff with that.

22:36.480 --> 22:38.920
 So my free associations are different from yours

22:38.920 --> 22:41.640
 and you're kind of amused by them and that's great.

22:41.640 --> 22:42.680
 And hence poetry.

22:42.680 --> 22:46.200
 So there are lots of ways in which we take a lousy situation

22:46.200 --> 22:47.600
 and make it good.

22:47.600 --> 22:50.640
 Another example would be our memories are terrible.

22:50.640 --> 22:52.360
 So we play games like concentration

22:52.360 --> 22:55.000
 where you flip over the two cards, try to find a pair.

22:55.000 --> 22:56.520
 Can you imagine a computer playing that?

22:56.520 --> 22:58.320
 Computers like this is the dullest game in the world.

22:58.320 --> 22:59.320
 I know where all the cards are.

22:59.320 --> 23:00.160
 I see it once.

23:00.160 --> 23:01.000
 I know where it is.

23:01.000 --> 23:02.600
 What are you even talking about?

23:02.600 --> 23:07.080
 So we make a fun game out of having this terrible memory.

23:07.080 --> 23:12.080
 So we are imperfect in discovering and optimizing

23:12.240 --> 23:13.560
 some kind of utility function.

23:13.560 --> 23:16.280
 But you think in general, there is a utility function.

23:16.280 --> 23:18.840
 There's an objective function that's better than others.

23:18.840 --> 23:19.920
 I didn't say that.

23:21.040 --> 23:24.720
 The presumption, when you say...

23:24.720 --> 23:27.240
 I think you could design a better memory system.

23:27.240 --> 23:29.880
 You could argue about utility functions

23:29.880 --> 23:32.080
 and how you wanna think about that.

23:32.080 --> 23:34.160
 But objectively, it would be really nice

23:34.160 --> 23:36.480
 to do some of the following things.

23:36.480 --> 23:40.840
 To get rid of memories that are no longer useful.

23:40.840 --> 23:42.680
 Like objectively, that would just be good.

23:42.680 --> 23:43.600
 And we're not that good at it.

23:43.600 --> 23:46.520
 So when you park in the same lot every day,

23:46.520 --> 23:47.920
 you confuse where you parked today

23:47.920 --> 23:48.840
 with where you parked yesterday

23:48.840 --> 23:50.720
 with where you parked the day before and so forth.

23:50.720 --> 23:52.600
 So you blur together a series of memories.

23:52.600 --> 23:55.360
 There's just no way that that's optimal.

23:55.360 --> 23:57.040
 I mean, I've heard all kinds of wacky arguments

23:57.040 --> 23:58.120
 of people trying to defend that.

23:58.120 --> 23:58.960
 But in the end of the day,

23:58.960 --> 24:00.400
 I don't think any of them hold water.

24:00.400 --> 24:02.800
 Or trauma memories of traumatic events

24:02.800 --> 24:05.640
 would be possibly a very nice feature to have

24:05.640 --> 24:06.800
 to get rid of those.

24:06.800 --> 24:08.320
 It'd be great if you could just be like,

24:08.320 --> 24:10.600
 I'm gonna wipe this sector.

24:10.600 --> 24:12.040
 I'm done with that.

24:12.040 --> 24:13.320
 I didn't have fun last night.

24:13.320 --> 24:14.760
 I don't wanna think about it anymore.

24:14.760 --> 24:15.880
 Woop, bye bye.

24:15.880 --> 24:17.800
 I'm gone, but we can't.

24:17.800 --> 24:20.400
 Do you think it's possible to build a system?

24:20.400 --> 24:23.400
 So you said human level intelligence is a weird concept,

24:23.400 --> 24:24.240
 but...

24:24.240 --> 24:25.440
 Well, I'm saying I prefer general intelligence.

24:25.440 --> 24:26.280
 General intelligence.

24:26.280 --> 24:28.120
 I mean, human level intelligence is a real thing.

24:28.120 --> 24:29.880
 And you could try to make a machine

24:29.880 --> 24:32.000
 that matches people or something like that.

24:32.000 --> 24:34.280
 I'm saying that per se shouldn't be the objective,

24:34.280 --> 24:37.280
 but rather that we should learn from humans

24:37.280 --> 24:39.720
 the things they do well and incorporate that into our AI

24:39.720 --> 24:42.160
 just as we incorporate the things that machines do well

24:42.160 --> 24:43.320
 that people do terribly.

24:43.320 --> 24:45.840
 So I mean, it's great that AI systems

24:45.840 --> 24:48.520
 can do all this brute force computation that people can't.

24:48.520 --> 24:50.880
 And one of the reasons I work on this stuff

24:50.880 --> 24:53.360
 is because I would like to see machine solve problems

24:53.360 --> 24:56.080
 that people can't that combine the strength

24:56.080 --> 24:59.520
 or that in order to be solved would combine

24:59.520 --> 25:02.280
 the strengths of machines to do all this computation

25:02.280 --> 25:04.280
 with the ability, let's say, of people to read.

25:04.280 --> 25:06.240
 So I'd like machines that can read

25:06.240 --> 25:08.720
 the entire medical literature in a day.

25:08.720 --> 25:11.800
 7,000 new papers or whatever the numbers comes out every day.

25:11.800 --> 25:15.840
 There's no way for any doctor or whatever to read them all.

25:15.840 --> 25:18.040
 Machine that could read would be a brilliant thing.

25:18.040 --> 25:21.160
 And that would be strengths of brute force computation

25:21.160 --> 25:24.360
 combined with kind of subtlety and understanding medicine

25:24.360 --> 25:26.960
 that a good doctor or scientist has.

25:26.960 --> 25:28.120
 So if we can linger a little bit

25:28.120 --> 25:29.720
 on the idea of general intelligence.

25:29.720 --> 25:32.880
 So Yanlacun believes that human intelligence

25:32.880 --> 25:35.600
 is in general at all, it's very narrow.

25:35.600 --> 25:38.160
 How do you think, I don't think that makes sense.

25:38.160 --> 25:40.160
 We have lots of narrow intelligences

25:40.160 --> 25:42.160
 for specific problems.

25:42.160 --> 25:46.000
 But the fact is like anybody can walk into,

25:46.000 --> 25:49.160
 let's say a Hollywood movie and reason about the content

25:49.160 --> 25:51.720
 of almost anything that goes on there.

25:51.720 --> 25:55.200
 So you can reason about what happens in a bank robbery

25:55.200 --> 25:58.640
 or what happens when someone is infertile

25:58.640 --> 26:02.800
 and wants to go to IVF to try to have a child.

26:02.800 --> 26:05.960
 Or you can, the list is essentially endless.

26:05.960 --> 26:09.600
 And not everybody understands every scene in a movie,

26:09.600 --> 26:11.760
 but there's a huge range of things

26:11.760 --> 26:15.080
 that pretty much any ordinary adult can understand.

26:15.080 --> 26:19.400
 His argument is that actually the set of things

26:19.400 --> 26:22.880
 seems large to us humans because we're very limited

26:22.880 --> 26:25.520
 in considering the kind of possibilities

26:25.520 --> 26:27.360
 of experience as they're possible.

26:27.360 --> 26:30.200
 But in fact, the amount of experience that are possible

26:30.200 --> 26:32.520
 is infinitely larger.

26:32.520 --> 26:35.120
 Well, I mean, if you wanna make an argument

26:35.120 --> 26:38.800
 that humans are constrained in what they can understand,

26:38.800 --> 26:41.640
 I have no issue with that, I think that's right.

26:41.640 --> 26:44.440
 But it's still not the same thing at all

26:44.440 --> 26:47.480
 as saying, here's a system that can play go.

26:47.480 --> 26:49.760
 It's been trained on five million games.

26:49.760 --> 26:52.600
 And then I say, can it play on a rectangular board

26:52.600 --> 26:53.680
 rather than a square board?

26:53.680 --> 26:56.560
 And you say, well, if I retrain it from scratch

26:56.560 --> 26:58.320
 on another five million games, I can't.

26:58.320 --> 27:01.120
 That's really, really narrow and that's where we are.

27:01.120 --> 27:05.120
 We don't have even a system that could play go

27:05.120 --> 27:07.080
 and then without further retraining

27:07.080 --> 27:08.680
 play on a rectangular board,

27:08.680 --> 27:12.560
 which any good human could do with very little problem.

27:12.560 --> 27:14.840
 So that's what I mean by narrow.

27:14.840 --> 27:16.840
 And so it's just wordplay to say.

27:16.840 --> 27:19.280
 Then it's semantics, then it's just words.

27:19.280 --> 27:21.120
 Then yeah, you mean general in a sense

27:21.120 --> 27:25.760
 that you can do all kinds of go board shapes flexibly.

27:25.760 --> 27:28.080
 Well, I mean, that would be like a first step

27:28.080 --> 27:28.920
 in the right direction,

27:28.920 --> 27:30.520
 but obviously that's not what it really meaning.

27:30.520 --> 27:31.360
 You're kidding.

27:32.400 --> 27:36.160
 What I mean by a general is that you could transfer

27:36.160 --> 27:38.960
 the knowledge you learn in one domain to another.

27:38.960 --> 27:43.320
 So if you learn about bank robberies in movies

27:43.320 --> 27:44.800
 and there's chase scenes,

27:44.800 --> 27:47.720
 then you can understand that amazing scene in Breaking Bad

27:47.720 --> 27:50.560
 when Walter White has a car chase scene

27:50.560 --> 27:52.640
 with only one person, he's the only one in it.

27:52.640 --> 27:55.520
 And you can reflect on how that car chase scene

27:55.520 --> 27:58.240
 is like all the other car chase scenes you've ever seen

27:58.240 --> 28:01.160
 and totally different and why that's cool.

28:01.160 --> 28:03.120
 And the fact that the number of domains

28:03.120 --> 28:04.560
 you can do that with is finite,

28:04.560 --> 28:05.760
 doesn't make it less general.

28:05.760 --> 28:07.320
 So the idea of general is you can just do it

28:07.320 --> 28:09.400
 on a lot of transfer across a lot of domains.

28:09.400 --> 28:11.760
 Yeah, I mean, I'm not saying humans are infinitely general

28:11.760 --> 28:12.960
 or that humans are perfect.

28:12.960 --> 28:15.360
 I just said a minute ago, it's a low bar,

28:15.360 --> 28:17.440
 but it's just, it's a low bar.

28:17.440 --> 28:20.480
 But right now, like the bar is here and we're there

28:20.480 --> 28:22.640
 and eventually we'll get way past it.

28:22.640 --> 28:25.600
 So speaking of low bars,

28:25.600 --> 28:27.440
 you've highlighted in your new book as well,

28:27.440 --> 28:29.360
 but a couple of years ago wrote a paper

28:29.360 --> 28:31.280
 titled Deep Learning a Critical Appraisal

28:31.280 --> 28:34.040
 that lists 10 challenges faced by

28:34.040 --> 28:36.000
 current deep learning systems.

28:36.000 --> 28:40.160
 So let me summarize them as data efficiency,

28:40.160 --> 28:42.920
 transfer learning, hierarchical knowledge,

28:42.920 --> 28:46.320
 open ended inference, explainability,

28:46.320 --> 28:49.640
 integrating prior knowledge, causal reasoning,

28:49.640 --> 28:53.200
 modeling on a stable world, robustness, adversarial examples

28:53.200 --> 28:54.120
 and so on.

28:54.120 --> 28:56.840
 And then my favorite probably is reliability

28:56.840 --> 28:59.120
 and engineering of real world systems.

28:59.120 --> 29:01.600
 So whatever people can read the paper,

29:01.600 --> 29:02.920
 they should definitely read the paper,

29:02.920 --> 29:04.320
 should definitely read your book.

29:04.320 --> 29:08.120
 But which of these challenges is solved in your view

29:08.120 --> 29:11.040
 has the biggest impact on the AI community?

29:11.040 --> 29:12.600
 It's a very good question.

29:13.920 --> 29:16.320
 And I'm gonna be evasive because I think that

29:16.320 --> 29:17.960
 they go together a lot.

29:17.960 --> 29:21.400
 So some of them might be solved independently of others,

29:21.400 --> 29:24.200
 but I think a good solution to AI starts

29:24.200 --> 29:27.480
 by having real what I would call cognitive models

29:27.480 --> 29:28.440
 of what's going on.

29:28.440 --> 29:31.320
 So right now we have an approach that's dominant

29:31.320 --> 29:33.920
 where you take statistical approximations of things,

29:33.920 --> 29:35.760
 but you don't really understand them.

29:35.760 --> 29:38.520
 So you know that bottles are correlated

29:38.520 --> 29:40.280
 in your data with bottle caps,

29:40.280 --> 29:42.240
 but you don't understand that there's a thread

29:42.240 --> 29:45.280
 on the bottle cap that fits with the thread on the bottle

29:45.280 --> 29:47.800
 and that that's tightens in if I tighten enough

29:47.800 --> 29:49.640
 that there's a seal and the water can come out.

29:49.640 --> 29:51.960
 Like there's no machine that understands that.

29:51.960 --> 29:53.800
 And having a good cognitive model

29:53.800 --> 29:55.480
 of that kind of everyday phenomena

29:55.480 --> 29:56.600
 is what we call common sense.

29:56.600 --> 29:58.880
 And if you had that, then a lot of these other things

29:58.880 --> 30:02.760
 start to fall into at least a little bit better place.

30:02.760 --> 30:04.840
 So right now you're like learning correlations

30:04.840 --> 30:06.520
 between pixels when you play a video game

30:06.520 --> 30:07.640
 or something like that.

30:07.640 --> 30:08.920
 And it doesn't work very well.

30:08.920 --> 30:10.680
 It works when the video game is just the way

30:10.680 --> 30:12.920
 that you studied it and then you alter the video game

30:12.920 --> 30:14.520
 in small ways like you move the paddle

30:14.520 --> 30:17.440
 and break out a few pixels and the system falls apart.

30:17.440 --> 30:19.000
 Because it doesn't understand,

30:19.000 --> 30:20.880
 it doesn't have a representation of a paddle,

30:20.880 --> 30:23.360
 a ball, a wall, a set of bricks and so forth.

30:23.360 --> 30:26.440
 And so it's reasoning at the wrong level.

30:26.440 --> 30:30.200
 So the idea of common sense, it's full of mystery.

30:30.200 --> 30:33.560
 You've worked on it, but it's nevertheless full of mystery,

30:33.560 --> 30:34.720
 full of promise.

30:34.720 --> 30:36.560
 What does common sense mean?

30:36.560 --> 30:38.000
 What does knowledge mean?

30:38.000 --> 30:40.920
 So the way you've been discussing it now is very intuitive.

30:40.920 --> 30:43.160
 It makes a lot of sense that that is something we should have

30:43.160 --> 30:45.600
 and that's something deep learning systems don't have.

30:45.600 --> 30:49.720
 But the argument could be that we're oversimplifying it

30:49.720 --> 30:53.160
 because we're oversimplifying the notion of common sense

30:53.160 --> 30:57.120
 because that's how it feels like we as humans

30:57.120 --> 30:59.320
 at the cognitive level approach problems.

30:59.320 --> 31:00.160
 So maybe...

31:00.160 --> 31:03.320
 A lot of people aren't actually gonna read my book.

31:03.320 --> 31:05.200
 But if they did read the book,

31:05.200 --> 31:07.120
 one of the things that might come as a surprise to them

31:07.120 --> 31:10.640
 is that we actually say a common sense is really hard

31:10.640 --> 31:11.640
 and really complicated.

31:11.640 --> 31:15.160
 So my critics know that I like common sense,

31:15.160 --> 31:18.600
 but that chapter actually starts by us beating up

31:18.600 --> 31:19.880
 not on deep learning,

31:19.880 --> 31:21.960
 but kind of on our own home team as it will.

31:21.960 --> 31:26.040
 So Ernie and I are first and foremost people that believe

31:26.040 --> 31:28.680
 in at least some of what good old fashioned AI tried to do.

31:28.680 --> 31:32.400
 So we believe in symbols and logic and programming.

31:32.400 --> 31:33.760
 Things like that are important.

31:33.760 --> 31:37.040
 And we go through why even those tools

31:37.040 --> 31:39.560
 that we hold fairly dear aren't really enough.

31:39.560 --> 31:42.680
 So we talk about why common sense is actually many things.

31:42.680 --> 31:45.320
 And some of them fit really well with those

31:45.320 --> 31:46.560
 classical sets of tools.

31:46.560 --> 31:48.240
 So things like taxonomy.

31:48.240 --> 31:51.480
 So I know that a bottle is an object

31:51.480 --> 31:52.840
 or it's a vessel, let's say.

31:52.840 --> 31:54.480
 And I know a vessel is an object

31:54.480 --> 31:57.600
 and objects are material things in the physical world.

31:57.600 --> 32:00.520
 So I can make some inferences.

32:00.520 --> 32:05.520
 If I know that vessels need to not have holes in them,

32:07.040 --> 32:09.560
 then I can infer that in order to carry their contents

32:09.560 --> 32:11.560
 that I can infer that a bottle shouldn't have a hole

32:11.560 --> 32:12.880
 in it in order to carry its contents.

32:12.880 --> 32:15.840
 So you can do hierarchical inference and so forth.

32:15.840 --> 32:17.280
 And we say that's great,

32:17.280 --> 32:21.120
 but it's only a tiny piece of what you need for common sense.

32:21.120 --> 32:23.440
 And we give lots of examples that don't fit into that.

32:23.440 --> 32:26.480
 So another one that we talk about is a cheese grater.

32:26.480 --> 32:28.040
 You've got holes in a cheese grater.

32:28.040 --> 32:29.520
 You've got a handle on top.

32:29.520 --> 32:33.400
 You can build a model in the game engine sense of a model

32:33.400 --> 32:35.680
 so that you could have a little cartoon character

32:35.680 --> 32:38.000
 flying around through the holes of the grater.

32:38.000 --> 32:40.000
 But we don't have a system yet.

32:40.000 --> 32:41.640
 Taxonomy doesn't help us that much.

32:41.640 --> 32:43.760
 It really understands why the handle is on top

32:43.760 --> 32:45.240
 and what you do with the handle

32:45.240 --> 32:47.600
 or why all of those circles are sharp

32:47.600 --> 32:50.480
 or how you'd hold the cheese with respect to the grater

32:50.480 --> 32:52.120
 in order to make it actually work.

32:52.120 --> 32:55.020
 Do you think these ideas are just abstractions

32:55.020 --> 32:57.880
 that could emerge on a system like

32:57.880 --> 32:59.920
 a very large deep neural network?

32:59.920 --> 33:03.120
 I'm a skeptic that that kind of emergence per se can work.

33:03.120 --> 33:05.840
 So I think that deep learning might play a role

33:05.840 --> 33:08.760
 in the systems that do what I want systems to do,

33:08.760 --> 33:09.920
 but it won't do it by itself.

33:09.920 --> 33:13.160
 I've never seen a deep learning system

33:13.160 --> 33:15.920
 really extract an abstract concept.

33:15.920 --> 33:18.840
 What they do, principle reasons for that,

33:18.840 --> 33:20.560
 stemming from how back propagation works,

33:20.560 --> 33:22.920
 how the architectures are set up.

33:22.920 --> 33:25.120
 One example is deep learning people

33:25.120 --> 33:29.640
 actually all build in something called convolution

33:29.640 --> 33:33.200
 which Jan Lacoon is famous for, which is an abstraction.

33:33.200 --> 33:34.960
 They don't have their systems learn this.

33:34.960 --> 33:37.760
 So the abstraction is an object that looks the same

33:37.760 --> 33:39.200
 if it appears in different places.

33:39.200 --> 33:41.960
 And what Lacoon figured out and why,

33:41.960 --> 33:44.320
 essentially why he was a co winner of the Turing word

33:44.320 --> 33:47.640
 was that if you program this in innately,

33:47.640 --> 33:50.680
 then your system would be a whole lot more efficient.

33:50.680 --> 33:53.200
 In principle, this should be learnable,

33:53.200 --> 33:56.240
 but people don't have systems that kind of reify things

33:56.240 --> 33:58.000
 and make them more abstract.

33:58.000 --> 34:00.440
 And so what you'd really wind up with,

34:00.440 --> 34:02.720
 if you don't program that in advance as a system,

34:02.720 --> 34:05.460
 the kind of realizes that this is the same thing as this,

34:05.460 --> 34:07.000
 but then I take your little clock there

34:07.000 --> 34:08.400
 and I move it over and it doesn't realize

34:08.400 --> 34:10.480
 that the same thing applies to the clock.

34:10.480 --> 34:12.680
 So the really nice thing, you're right,

34:12.680 --> 34:14.760
 that convolution is just one of the things

34:14.760 --> 34:17.160
 that's like it's an innate feature

34:17.160 --> 34:19.240
 that's programmed by the human expert,

34:19.240 --> 34:21.240
 but we need more of those, not less.

34:21.240 --> 34:23.720
 So the, but the nice feature is,

34:23.720 --> 34:27.240
 it feels like that requires coming up with that brilliant

34:27.240 --> 34:29.800
 idea can get your Turing award,

34:29.800 --> 34:34.760
 but it requires less effort than encoding

34:34.760 --> 34:36.640
 and something we'll talk about the expert system.

34:36.640 --> 34:40.040
 So encoding a lot of knowledge by hand.

34:40.040 --> 34:43.480
 So it feels like one, there's a huge amount of limitations

34:43.480 --> 34:46.480
 which you clearly outline with deep learning,

34:46.480 --> 34:47.800
 but the nice feature of deep learning,

34:47.800 --> 34:49.600
 whatever it is able to accomplish,

34:49.600 --> 34:53.520
 it does it, it does a lot of stuff automatically

34:53.520 --> 34:54.920
 without human intervention.

34:54.920 --> 34:57.120
 Well, and that's part of why people love it, right?

34:57.120 --> 34:59.800
 But I always think of this quote from Bertrand Russell,

34:59.800 --> 35:04.400
 which is it has all the advantages of theft over honest toil.

35:04.400 --> 35:08.120
 It's really hard to program into a machine

35:08.120 --> 35:10.000
 a notion of causality or, you know,

35:10.000 --> 35:12.640
 even how a bottle works or what containers are.

35:12.640 --> 35:14.240
 Ernie Davis and I wrote a, I don't know,

35:14.240 --> 35:18.000
 45 page academic paper trying just to understand

35:18.000 --> 35:19.920
 what a container is, which I don't think anybody

35:19.920 --> 35:24.120
 ever read the paper, but it's a very detailed analysis

35:24.120 --> 35:25.920
 of all the things, not even all,

35:25.920 --> 35:27.120
 some of the things you need to do

35:27.120 --> 35:28.560
 in order to understand a container.

35:28.560 --> 35:30.960
 It would be a whole lot nice and, you know,

35:30.960 --> 35:32.200
 I'm a co author on the paper,

35:32.200 --> 35:33.200
 I made it a little bit better,

35:33.200 --> 35:36.600
 but Ernie did the hard work for that particular paper.

35:36.600 --> 35:38.080
 And it took him like three months

35:38.080 --> 35:40.680
 to get the logical statements correct.

35:40.680 --> 35:42.840
 And maybe that's not the right way to do it.

35:42.840 --> 35:46.120
 It's a way to do it, but on that way of doing it,

35:46.120 --> 35:48.440
 it's really hard work to do something

35:48.440 --> 35:50.280
 as simple as understanding containers.

35:50.280 --> 35:52.840
 And nobody wants to do that hard work.

35:52.840 --> 35:55.600
 Even Ernie didn't want to do that hard work.

35:55.600 --> 35:58.360
 Everybody would rather just like feed their system in

35:58.360 --> 36:00.320
 with a bunch of videos with a bunch of containers

36:00.320 --> 36:03.800
 and have the systems infer how can containers work.

36:03.800 --> 36:05.400
 It would be like so much less effort,

36:05.400 --> 36:06.800
 let the machine do the work.

36:06.800 --> 36:08.200
 And so I understand the impulse,

36:08.200 --> 36:10.200
 I understand why people want to do that.

36:10.200 --> 36:11.840
 I just don't think that it works.

36:11.840 --> 36:14.560
 I've never seen anybody build a system

36:14.560 --> 36:18.680
 that in a robust way can actually watch videos

36:18.680 --> 36:20.160
 and predict exactly, you know,

36:20.160 --> 36:21.280
 which containers would leak

36:21.280 --> 36:23.520
 and which ones wouldn't or something like,

36:23.520 --> 36:25.040
 and I know someone's gonna go out and do that

36:25.040 --> 36:28.080
 since I said it, and I look forward to seeing it,

36:28.080 --> 36:30.520
 but getting these things to work robustly

36:30.520 --> 36:32.880
 is really, really hard.

36:32.880 --> 36:36.120
 So Yann LeCun, who was my colleague at NYU

36:36.120 --> 36:38.800
 for many years, thinks that the hard work

36:38.800 --> 36:43.120
 should go into defining an unsupervised learning algorithm

36:43.120 --> 36:46.640
 that will watch videos, use the next frame basically

36:46.640 --> 36:48.520
 in order to tell it what's going on.

36:48.520 --> 36:49.920
 And he thinks that's the royal road

36:49.920 --> 36:51.240
 and he's willing to put in the work

36:51.240 --> 36:53.280
 in devising that algorithm.

36:53.280 --> 36:55.560
 Then he wants the machine to do the rest.

36:55.560 --> 36:57.800
 And again, I understand the impulse.

36:57.800 --> 37:01.720
 My intuition, based on years of watching this stuff

37:01.720 --> 37:03.960
 and making predictions 20 years ago that still hold,

37:03.960 --> 37:06.480
 even though there's a lot more computation and so forth,

37:06.480 --> 37:08.520
 is that we actually have to do a different kind of hard work,

37:08.520 --> 37:11.320
 which is more like building a design specification

37:11.320 --> 37:13.120
 for what we want the system to do,

37:13.120 --> 37:15.040
 doing hard engineering work to figure out

37:15.040 --> 37:18.440
 how we do things like what Yann did for convolution

37:18.440 --> 37:21.680
 in order to figure out how to encode complex knowledge

37:21.680 --> 37:22.640
 into the systems.

37:22.640 --> 37:25.320
 The current systems don't have that much knowledge

37:25.320 --> 37:26.920
 other than convolution,

37:26.920 --> 37:28.120
 which is again this, you know,

37:28.120 --> 37:30.560
 object experience in different places

37:30.560 --> 37:33.280
 and having the same perception, I guess I'll say.

37:34.480 --> 37:35.320
 Same appearance.

37:36.720 --> 37:38.280
 People don't wanna do that work.

37:38.280 --> 37:41.480
 They don't see how to naturally fit one with the other.

37:41.480 --> 37:43.320
 I think that's, yes, absolutely.

37:43.320 --> 37:45.560
 But also on the expert system side,

37:45.560 --> 37:47.640
 there's a temptation to go too far the other way.

37:47.640 --> 37:49.880
 So it was just having an expert sort of sit down

37:49.880 --> 37:52.720
 and encode the description, the framework

37:52.720 --> 37:54.080
 for what a container is,

37:54.080 --> 37:56.560
 and then having the system reason for the rest.

37:56.560 --> 37:59.280
 For my view, like one really exciting possibility

37:59.280 --> 38:02.200
 is of active learning where it's continuous interaction

38:02.200 --> 38:04.120
 between a human and machine.

38:04.120 --> 38:07.080
 As the machine, there's kind of deep learning type

38:07.080 --> 38:10.160
 extraction of information from data patterns and so on,

38:10.160 --> 38:14.680
 but humans also guiding the learning procedures,

38:14.680 --> 38:19.680
 guiding both the process and the framework

38:19.960 --> 38:22.200
 of how the machine learns, whatever the task is.

38:22.200 --> 38:24.120
 I was with you with almost everything you said,

38:24.120 --> 38:26.520
 except the phrase deep learning.

38:26.520 --> 38:28.240
 What I think you really want there

38:28.240 --> 38:30.520
 is a new form of machine learning.

38:30.520 --> 38:33.000
 So let's remember deep learning is a particular way

38:33.000 --> 38:34.040
 of doing machine learning.

38:34.040 --> 38:37.040
 Most often it's done with supervised data

38:37.040 --> 38:38.840
 for perceptual categories.

38:38.840 --> 38:41.760
 There are other things you can do with deep learning.

38:41.760 --> 38:42.760
 Some of them quite technical,

38:42.760 --> 38:44.640
 but the standard use of deep learning

38:44.640 --> 38:47.640
 is I have a lot of examples and I have labels for them.

38:47.640 --> 38:48.840
 So here are pictures.

38:48.840 --> 38:50.400
 This one's the Eiffel Tower.

38:50.400 --> 38:51.680
 This one's the Sears Tower.

38:51.680 --> 38:53.360
 This one's the Empire State Building.

38:53.360 --> 38:54.200
 This one's a cat.

38:54.200 --> 38:55.040
 This one's a pig and so forth.

38:55.040 --> 38:58.880
 You just get millions of examples, millions of labels.

38:58.880 --> 39:01.240
 And deep learning is extremely good at that.

39:01.240 --> 39:02.680
 It's better than any other solution

39:02.680 --> 39:04.440
 that anybody has devised,

39:04.440 --> 39:07.400
 but it is not good at representing abstract knowledge.

39:07.400 --> 39:10.720
 It's not good at representing things like bottles

39:10.720 --> 39:14.320
 contain liquid and have tops to them and so forth.

39:14.320 --> 39:15.840
 It's not very good at learning

39:15.840 --> 39:17.840
 or representing that kind of knowledge.

39:17.840 --> 39:21.320
 It is an example of having a machine learn something,

39:21.320 --> 39:23.920
 but it's a machine that learns a particular kind of thing,

39:23.920 --> 39:25.520
 which is object classification.

39:25.520 --> 39:27.720
 It's not a particularly good algorithm

39:27.720 --> 39:29.600
 for learning about the abstractions

39:29.600 --> 39:30.760
 that govern our world.

39:30.760 --> 39:33.040
 There may be such a thing,

39:33.040 --> 39:34.280
 part of what we counsel in the book

39:34.280 --> 39:36.960
 is maybe people should be working on devising such things.

39:36.960 --> 39:40.520
 So one possibility, just I wonder what you think about it,

39:40.520 --> 39:45.160
 is deep neural networks do form abstractions,

39:45.160 --> 39:48.480
 but they're not accessible to us humans

39:48.480 --> 39:49.320
 in terms of we can't.

39:49.320 --> 39:50.720
 There's some truth in that.

39:50.720 --> 39:54.760
 So is it possible that either current or future neural networks

39:54.760 --> 39:56.480
 form very high level abstractions,

39:56.480 --> 40:02.360
 which are as powerful as our human abstractions of common sense,

40:02.360 --> 40:04.840
 we just can't get a hold of them.

40:04.840 --> 40:06.560
 And so the problem is essentially

40:06.560 --> 40:09.160
 we need to make them explainable.

40:09.160 --> 40:10.560
 This is an astute question,

40:10.560 --> 40:13.000
 but I think the answer is at least partly no.

40:13.000 --> 40:16.000
 One of the kinds of classical neural network architecture

40:16.000 --> 40:17.560
 is what we call an auto associator.

40:17.560 --> 40:21.440
 It just tries to take an input, goes through a set of hidden layers

40:21.440 --> 40:23.000
 and comes out with an output.

40:23.000 --> 40:25.400
 And it's supposed to learn essentially the identity function,

40:25.400 --> 40:27.200
 that your input is the same as your output.

40:27.200 --> 40:28.400
 So you think of this binary numbers,

40:28.400 --> 40:30.600
 you've got like the one, the two, the four, the eight,

40:30.600 --> 40:32.120
 the 16 and so forth.

40:32.120 --> 40:35.000
 And so if you want to input 24, you turn on the 16,

40:35.000 --> 40:35.840
 you turn on the eight.

40:35.840 --> 40:38.920
 It's like binary one, one and bunch of zeros.

40:38.920 --> 40:41.600
 So I did some experiments in 1998

40:41.600 --> 40:46.720
 with the precursors of contemporary deep learning.

40:46.720 --> 40:50.520
 And what I showed was you could train these networks

40:50.520 --> 40:52.120
 on all the even numbers

40:52.120 --> 40:54.720
 and they would never generalize to the odd number.

40:54.720 --> 40:56.760
 A lot of people thought that I was, I don't know,

40:56.760 --> 41:00.160
 an idiot or faking the experiment or wasn't true or whatever,

41:00.160 --> 41:03.320
 but it is true that with this class of networks

41:03.320 --> 41:04.920
 that we had in that day,

41:04.920 --> 41:07.440
 that they would never, ever make this generalization.

41:07.440 --> 41:09.680
 And it's not that the networks were stupid,

41:09.680 --> 41:13.440
 it's that they see the world in a different way than we do.

41:13.440 --> 41:14.720
 They were basically concerned,

41:14.720 --> 41:18.640
 what is the probability that the right most output node

41:18.640 --> 41:20.000
 is going to be a one?

41:20.000 --> 41:21.240
 And as far as they were concerned,

41:21.240 --> 41:22.840
 in everything that they'd ever been trained on,

41:22.840 --> 41:27.040
 it was a zero, that node had never been turned on.

41:27.040 --> 41:28.960
 And so they figured, why turn it on now?

41:28.960 --> 41:30.720
 Whereas a person would look at the same problem

41:30.720 --> 41:31.720
 and say, well, it's obvious,

41:31.720 --> 41:33.800
 we're just doing the thing that corresponds.

41:33.800 --> 41:35.520
 The Latin for it is mutatus, mutatus,

41:35.520 --> 41:38.200
 we'll change what needs to be changed.

41:38.200 --> 41:40.520
 And we do this, this is what algebra is.

41:40.520 --> 41:43.840
 So I can do f of x equals y plus two

41:43.840 --> 41:45.360
 and I can do it for a couple of values.

41:45.360 --> 41:47.720
 I can tell you if y is three, then x is five

41:47.720 --> 41:49.160
 and if y is four, x is six.

41:49.160 --> 41:50.960
 And now I can do it with some totally different number,

41:50.960 --> 41:52.000
 like a million, then you can say,

41:52.000 --> 41:53.120
 well, obviously it's a million and two

41:53.120 --> 41:55.600
 because you have an algebraic operation

41:55.600 --> 41:57.440
 that you're applying to a variable.

41:57.440 --> 42:00.600
 And deep learning systems kind of emulate that,

42:00.600 --> 42:02.480
 but they don't actually do it.

42:02.480 --> 42:04.120
 The particular example,

42:04.120 --> 42:08.120
 you could fudge a solution to that particular problem.

42:08.120 --> 42:10.480
 The general form of that problem remains

42:10.480 --> 42:12.360
 that what they learn is really correlations

42:12.360 --> 42:14.280
 between different input and output nodes.

42:14.280 --> 42:15.640
 And they're complex correlations

42:15.640 --> 42:18.320
 with multiple nodes involved and so forth,

42:18.320 --> 42:20.200
 but ultimately they're correlative.

42:20.200 --> 42:22.360
 They're not structured over these operations

42:22.360 --> 42:23.200
 over variables.

42:23.200 --> 42:25.920
 Now, someday people may do a new form of deep learning

42:25.920 --> 42:27.280
 that incorporates that stuff

42:27.280 --> 42:28.480
 and I think it will help a lot.

42:28.480 --> 42:30.240
 And there's some tentative work on things

42:30.240 --> 42:32.160
 like differentiable programming right now

42:32.160 --> 42:34.200
 that fall into that category.

42:34.200 --> 42:35.480
 But there's sort of classic stuff

42:35.480 --> 42:38.760
 like people use for ImageNet, doesn't have it.

42:38.760 --> 42:40.480
 And you have people like Hinton going around

42:40.480 --> 42:42.960
 and saying symbol manipulation like what Marcus,

42:42.960 --> 42:45.760
 what I advocate is like the gasoline engine.

42:45.760 --> 42:46.600
 It's obsolete.

42:46.600 --> 42:48.920
 We should just use this cool electric power

42:48.920 --> 42:50.400
 that we've got with the deep learning.

42:50.400 --> 42:52.080
 And that's really destructive

42:52.080 --> 42:56.000
 because we really do need to have the gasoline engine stuff

42:56.000 --> 42:59.680
 that represents, I mean, I don't think it's a good analogy,

42:59.680 --> 43:02.280
 but we really do need to have the stuff

43:02.280 --> 43:03.760
 that represents symbols.

43:03.760 --> 43:06.520
 Yeah, and Hinton as well would say that

43:06.520 --> 43:09.040
 we do need to throw out everything and start over.

43:09.040 --> 43:10.600
 So I mean, there is a question.

43:10.600 --> 43:12.840
 Yeah, Hinton said that to Axios

43:12.840 --> 43:15.520
 and I had a friend who interviewed him

43:15.520 --> 43:17.800
 and tried to pin him down on what exactly we need to throw

43:17.800 --> 43:19.880
 and he was very evasive.

43:19.880 --> 43:21.640
 Well, of course, because we can't,

43:21.640 --> 43:23.880
 if he knew that he'd throw it out himself,

43:23.880 --> 43:25.400
 but I mean, he can't have it both ways.

43:25.400 --> 43:27.520
 He can't be like, I don't know what to throw out,

43:27.520 --> 43:29.960
 but I am gonna throw out the symbols.

43:29.960 --> 43:32.120
 I mean, and not just the symbols,

43:32.120 --> 43:34.080
 but the variables and the operations over variables.

43:34.080 --> 43:36.120
 Don't forget the operations over variables,

43:36.120 --> 43:37.760
 the stuff that I'm endorsing

43:37.760 --> 43:41.520
 and which John McCarthy did when he founded AI.

43:41.520 --> 43:44.200
 That stuff is the stuff that we build most computers out of.

43:44.200 --> 43:45.440
 There are people now who say,

43:45.440 --> 43:48.800
 we don't need computer programmers anymore.

43:48.800 --> 43:50.280
 Not quite looking at the statistics

43:50.280 --> 43:53.000
 of how much computer programmers actually get paid right now.

43:53.000 --> 43:54.440
 We need lots of computer programmers

43:54.440 --> 43:57.800
 and most of them, they do a little bit of machine learning,

43:57.800 --> 43:59.920
 but they still do a lot of code, right?

43:59.920 --> 44:02.640
 Code where it's like, if the value of X is greater

44:02.640 --> 44:04.520
 than the value of Y, then do this kind of thing,

44:04.520 --> 44:08.080
 like conditionals and comparing operations over variables.

44:08.080 --> 44:10.200
 Like there's this fantasy, you can machine learn anything.

44:10.200 --> 44:12.520
 There's some things you would never wanna machine learn.

44:12.520 --> 44:14.960
 I would not use a phone operating system

44:14.960 --> 44:16.080
 that was machine learned.

44:16.080 --> 44:17.760
 Like you made a bunch of phone calls

44:17.760 --> 44:19.720
 and you recorded which packets were transmitted

44:19.720 --> 44:22.480
 and you just machine learned it, it'd be insane.

44:22.480 --> 44:27.440
 Or to build a web browser by taking logs of keystrokes

44:27.440 --> 44:29.440
 and images, screenshots,

44:29.440 --> 44:31.480
 and then trying to learn the relation between them.

44:31.480 --> 44:33.840
 Nobody would ever, no rational person

44:33.840 --> 44:35.920
 would ever try to build a browser that way.

44:35.920 --> 44:37.440
 They would use symbol manipulation,

44:37.440 --> 44:40.080
 the stuff that I think AI needs to avail itself of

44:40.080 --> 44:42.080
 in addition to deep learning.

44:42.080 --> 44:46.480
 Can you describe what your view of symbol manipulation

44:46.480 --> 44:47.880
 in its early days?

44:47.880 --> 44:49.520
 Can you describe expert systems

44:49.520 --> 44:52.520
 and where do you think they hit a wall

44:52.520 --> 44:53.920
 or a set of challenges?

44:53.920 --> 44:56.560
 Sure, so I mean, first I just wanna clarify.

44:56.560 --> 44:58.920
 I'm not endorsing expert systems per se.

44:58.920 --> 45:00.720
 You've been kind of contrasting them.

45:00.720 --> 45:01.560
 There is a contrast,

45:01.560 --> 45:03.240
 but that's not the thing that I'm endorsing.

45:03.240 --> 45:04.200
 Yes.

45:04.200 --> 45:06.480
 So expert systems try to capture things

45:06.480 --> 45:09.440
 like medical knowledge with a large set of rules.

45:09.440 --> 45:12.800
 So if the patient has this symptom and this other symptom,

45:12.800 --> 45:15.680
 then it is likely that they have this disease.

45:15.680 --> 45:16.840
 So there are logical rules

45:16.840 --> 45:18.920
 and they were symbol manipulating rules of just the sort

45:18.920 --> 45:20.920
 that I'm talking about.

45:20.920 --> 45:23.400
 And the problem. They encode a set of knowledge

45:23.400 --> 45:24.960
 that the experts then put in.

45:24.960 --> 45:26.240
 And very explicitly so.

45:26.240 --> 45:28.760
 So you'd have somebody interview an expert

45:28.760 --> 45:31.880
 and then try to turn that stuff into rules.

45:31.880 --> 45:33.920
 And at some level I'm arguing for rules,

45:33.920 --> 45:37.640
 but the difference is those guys did in the 80s

45:37.640 --> 45:39.960
 was almost entirely rules,

45:39.960 --> 45:42.920
 almost entirely handwritten with no machine learning.

45:42.920 --> 45:44.280
 What a lot of people are doing now

45:44.280 --> 45:47.320
 is almost entirely one species of machine learning

45:47.320 --> 45:48.240
 with no rules.

45:48.240 --> 45:50.320
 And what I'm counseling is actually a hybrid.

45:50.320 --> 45:52.880
 I'm saying that both of these things have their advantage.

45:52.880 --> 45:55.280
 So if you're talking about perceptual classification,

45:55.280 --> 45:57.080
 how do I recognize a bottle?

45:57.080 --> 45:59.480
 Deep learning is the best tool we've got right now.

45:59.480 --> 46:00.880
 If you're talking about making inferences

46:00.880 --> 46:02.360
 about what a bottle does,

46:02.360 --> 46:04.080
 something closer to the expert systems

46:04.080 --> 46:07.280
 is probably still the best available alternative.

46:07.280 --> 46:09.800
 And probably we want something that is better able

46:09.800 --> 46:12.560
 to handle quantitative and statistical information

46:12.560 --> 46:14.880
 than those classical systems typically were.

46:14.880 --> 46:16.920
 So we need new technologies

46:16.920 --> 46:18.560
 that are gonna draw some of the strengths

46:18.560 --> 46:21.000
 of both the expert systems and the deep learning,

46:21.000 --> 46:23.200
 but are gonna find new ways to synthesize them.

46:23.200 --> 46:27.680
 How hard do you think it is to add knowledge at the low level?

46:27.680 --> 46:32.120
 So mine human intellects to add extra information

46:32.120 --> 46:36.520
 to symbol manipulating systems.

46:36.520 --> 46:37.840
 In some domains, it's not that hard,

46:37.840 --> 46:40.080
 but it's often really hard.

46:40.080 --> 46:44.120
 Partly because a lot of the things that are important,

46:44.120 --> 46:46.080
 people wouldn't bother to tell you.

46:46.080 --> 46:49.680
 So if you pay someone on Amazon Mechanical Turk

46:49.680 --> 46:52.080
 to tell you stuff about bottles,

46:52.080 --> 46:55.080
 they probably won't even bother to tell you

46:55.080 --> 46:57.040
 some of the basic level stuff

46:57.040 --> 46:59.160
 that's just so obvious to a human being

46:59.160 --> 47:02.160
 and yet so hard to capture in machines.

47:03.840 --> 47:06.560
 You know, they're gonna tell you more exotic things

47:06.560 --> 47:08.960
 and like they're all well and good,

47:08.960 --> 47:12.480
 but they're not getting to the root of the problem.

47:12.480 --> 47:16.520
 So untutored humans aren't very good at knowing

47:16.520 --> 47:18.360
 and why should they be,

47:18.360 --> 47:22.280
 what kind of knowledge the computer system developers

47:22.280 --> 47:23.480
 actually need.

47:23.480 --> 47:26.640
 I don't think that that's an irremediable problem.

47:26.640 --> 47:28.640
 I think it's historically been a problem.

47:28.640 --> 47:31.080
 People have had crowdsourcing efforts

47:31.080 --> 47:32.040
 and they don't work that well.

47:32.040 --> 47:32.960
 There's one at MIT.

47:32.960 --> 47:36.520
 We're recording this at MIT called Virtual Home

47:36.520 --> 47:39.560
 where, and we talk about this in the book.

47:39.560 --> 47:40.720
 Find the exact example there,

47:40.720 --> 47:42.800
 but people were asked to do things

47:42.800 --> 47:44.880
 like describe an exercise routine.

47:44.880 --> 47:47.560
 And the things that the people describe it

47:47.560 --> 47:50.080
 are very low level and don't really capture what's going on.

47:50.080 --> 47:53.120
 So they're like, go to the room with the television

47:53.120 --> 47:56.120
 and the weights, turn on the television,

47:56.120 --> 47:59.040
 press the remote to turn on the television,

47:59.040 --> 48:01.480
 lift weight, put weight down,

48:01.480 --> 48:03.640
 it's like very micro level.

48:03.640 --> 48:06.120
 And it's not telling you what an exercise routine

48:06.120 --> 48:07.960
 is really about, which is like,

48:07.960 --> 48:09.920
 I wanna fit a certain number of exercises

48:09.920 --> 48:11.000
 in a certain time period,

48:11.000 --> 48:12.720
 I wanna emphasize these muscles.

48:12.720 --> 48:15.120
 You want some kind of abstract description.

48:15.120 --> 48:17.280
 The fact that you happen to press the remote control

48:17.280 --> 48:20.040
 in this room when you watch this television

48:20.040 --> 48:23.080
 isn't really the essence of the exercise routine,

48:23.080 --> 48:24.800
 but if you just ask people like, what did they do?

48:24.800 --> 48:27.000
 Then they give you this fine grain.

48:27.000 --> 48:29.800
 And so it takes a little level of expertise

48:29.800 --> 48:33.640
 about how the AI works in order to craft

48:33.640 --> 48:34.480
 the right kind of knowledge.

48:34.480 --> 48:36.200
 So there's this ocean of knowledge

48:36.200 --> 48:37.600
 that we all operate on.

48:37.600 --> 48:39.360
 Some of it may not even be conscious,

48:39.360 --> 48:43.280
 or at least we're not able to communicate it effectively.

48:43.280 --> 48:45.720
 Yeah, most of it we would recognize if somebody said it,

48:45.720 --> 48:47.440
 if it was true or not,

48:47.440 --> 48:49.680
 but we wouldn't think to say that it's true or not.

48:49.680 --> 48:53.080
 It's a really interesting mathematical property.

48:53.080 --> 48:55.480
 This ocean has the property that every piece

48:55.480 --> 48:56.720
 of knowledge in it,

48:56.720 --> 48:59.960
 we will recognize it as true if we're told,

48:59.960 --> 49:04.120
 but we're unlikely to retrieve it in the reverse.

49:04.120 --> 49:07.200
 So that interesting property,

49:07.200 --> 49:10.600
 I would say there's a huge ocean of that knowledge.

49:10.600 --> 49:11.600
 What's your intuition?

49:11.600 --> 49:14.680
 Is it accessible to AI systems somehow?

49:14.680 --> 49:16.680
 Can we, so you said,

49:16.680 --> 49:18.760
 I mean, most of it is not,

49:18.760 --> 49:20.520
 well, I'll give you an asterisk on this in a second,

49:20.520 --> 49:23.280
 but most of it is not ever been encoded

49:23.280 --> 49:25.720
 in machine interpretable form.

49:25.720 --> 49:27.320
 And so, I mean, if you say accessible,

49:27.320 --> 49:28.680
 there's two meanings of that.

49:28.680 --> 49:31.600
 One is like, could you build it into a machine?

49:31.600 --> 49:32.440
 Yes.

49:32.440 --> 49:34.480
 The other is like, is there some database

49:34.480 --> 49:38.440
 that we could go download and stick into our machine?

49:38.440 --> 49:39.520
 But the first thing, no.

49:39.520 --> 49:40.520
 Could we?

49:40.520 --> 49:41.360
 Is what's your intuition?

49:41.360 --> 49:42.200
 I think we could.

49:42.200 --> 49:45.200
 I think it hasn't been done right.

49:45.200 --> 49:47.320
 The closest, and this is the asterisk,

49:47.320 --> 49:51.200
 is the CYC psych system, try to do this.

49:51.200 --> 49:53.080
 A lot of logicians worked for Doug Lennon

49:53.080 --> 49:55.480
 for 30 years on this project.

49:55.480 --> 49:57.920
 I think they stuck too closely to logic,

49:57.920 --> 50:00.240
 didn't represent enough about probabilities,

50:00.240 --> 50:02.200
 tried to hand code it, there are various issues,

50:02.200 --> 50:04.520
 and it hasn't been that successful.

50:04.520 --> 50:08.520
 That is the closest existing system

50:08.520 --> 50:10.640
 to trying to encode this.

50:10.640 --> 50:13.480
 Why do you think there's not more excitement

50:13.480 --> 50:16.440
 slash money behind this idea currently?

50:16.440 --> 50:19.160
 There was, people view that project as a failure.

50:19.160 --> 50:23.160
 I think that they confused the failure of a specific instance

50:23.160 --> 50:26.160
 that was conceived 30 years ago for the failure of an approach,

50:26.160 --> 50:28.120
 which they don't do for deep learning.

50:28.120 --> 50:32.680
 So in 2010, people had the same attitude towards deep learning.

50:32.680 --> 50:35.480
 They're like, this stuff doesn't really work.

50:35.480 --> 50:39.120
 And all these other algorithms work better and so forth.

50:39.120 --> 50:41.840
 And then certain key technical advances were made.

50:41.840 --> 50:45.040
 But mostly, it was the advent of graphics processing units

50:45.040 --> 50:46.400
 that changed that.

50:46.400 --> 50:50.040
 It wasn't even anything foundational in the techniques.

50:50.040 --> 50:51.200
 And there were some new tricks.

50:51.200 --> 50:55.280
 But mostly, it was just more compute and more data,

50:55.280 --> 50:57.880
 things like ImageNet that didn't exist before,

50:57.880 --> 50:59.040
 that allowed deep learning.

50:59.040 --> 51:00.880
 And it could be to work.

51:00.880 --> 51:03.760
 It could be that psych just needs a few more things

51:03.760 --> 51:05.440
 or something like psych.

51:05.440 --> 51:08.840
 But the widespread view is that that just doesn't work.

51:08.840 --> 51:11.760
 And people are reasoning from a single example.

51:11.760 --> 51:13.240
 They don't do that with deep learning.

51:13.240 --> 51:16.600
 They don't say nothing that existed in 2010.

51:16.600 --> 51:18.880
 And there were many, many efforts in deep learning

51:18.880 --> 51:20.600
 was really worth anything.

51:20.600 --> 51:23.840
 I mean, really, there's no model from 2010

51:23.840 --> 51:28.440
 in deep learning that has any commercial value whatsoever

51:28.440 --> 51:29.640
 at this point.

51:29.640 --> 51:31.360
 They're all failures.

51:31.360 --> 51:33.520
 But that doesn't mean that there wasn't anything there.

51:33.520 --> 51:35.960
 I have a friend who I was getting to know him.

51:35.960 --> 51:38.840
 And he said, I had a company, too.

51:38.840 --> 51:40.640
 I was talking about I had a new company.

51:40.640 --> 51:43.400
 And he said, I had a company, too, and it failed.

51:43.400 --> 51:44.320
 And I said, well, what did you do?

51:44.320 --> 51:45.680
 And he said, deep learning.

51:45.680 --> 51:48.680
 And the problem was he did it in 1986 or something like that.

51:48.680 --> 51:51.120
 And we didn't have the tools then or 1990.

51:51.120 --> 51:53.960
 We didn't have the tools then, not the algorithms.

51:53.960 --> 51:56.560
 His algorithms weren't that different from other algorithms.

51:56.560 --> 51:58.480
 But he didn't have the GPUs to run it fast enough.

51:58.480 --> 51:59.720
 He didn't have the data.

51:59.720 --> 52:01.360
 And so it failed.

52:01.360 --> 52:06.920
 It could be that symbol manipulation, per se,

52:06.920 --> 52:09.560
 with modern amounts of data and compute

52:09.560 --> 52:13.720
 and maybe some advance in compute for that kind of compute,

52:13.720 --> 52:14.880
 might be great.

52:14.880 --> 52:18.440
 My perspective on it is not that we

52:18.440 --> 52:20.000
 want to resuscitate that stuff, per se,

52:20.000 --> 52:22.040
 but we want to borrow lessons from it, bring together

52:22.040 --> 52:23.480
 with other things that we've learned.

52:23.480 --> 52:27.120
 And it might have an ImageNet moment where it will spark

52:27.120 --> 52:28.200
 the world's imagination.

52:28.200 --> 52:31.480
 And there will be an explosion of symbol manipulation efforts.

52:31.480 --> 52:35.720
 Yeah, I think that people at AI2, the Paul Allen AI Institute,

52:35.720 --> 52:39.400
 are trying to build data sets that, well,

52:39.400 --> 52:41.120
 they're not doing it for quite the reason that you say,

52:41.120 --> 52:43.600
 but they're trying to build data sets that at least

52:43.600 --> 52:45.400
 spark interest in common sense reasoning.

52:45.400 --> 52:46.800
 To create benchmarks that people are thinking.

52:46.800 --> 52:48.400
 Benchmarks for common sense, that's

52:48.400 --> 52:52.040
 a large part of what the AI2.org is working on right now.

52:52.040 --> 52:54.320
 So speaking of compute, Rich Sutton

52:54.320 --> 52:56.400
 wrote a blog post titled Bitter Lesson.

52:56.400 --> 52:58.800
 I don't know if you've read it, but he said that the biggest

52:58.800 --> 53:01.560
 lesson that can be read from 70 years of AI research

53:01.560 --> 53:04.200
 is that general methods that leverage computation

53:04.200 --> 53:06.400
 are ultimately the most effective.

53:06.400 --> 53:07.000
 Do you think that?

53:07.000 --> 53:08.880
 The most effective of what?

53:08.880 --> 53:13.360
 So they have been most effective for perceptual classification

53:13.360 --> 53:18.040
 problems and for some reinforcement learning problems.

53:18.040 --> 53:19.400
 He works on reinforcement learning.

53:19.400 --> 53:20.720
 Well, no, let me push back on that.

53:20.720 --> 53:22.840
 You're actually absolutely right.

53:22.840 --> 53:28.120
 But I would also say they've been most effective generally

53:28.120 --> 53:31.520
 because everything we've done up to the point.

53:31.520 --> 53:33.560
 Would you argue against that?

53:33.560 --> 53:36.280
 To me, deep learning is the first thing

53:36.280 --> 53:42.200
 that has been successful at anything in AI.

53:42.200 --> 53:46.280
 And you're pointing out that this success is very limited,

53:46.280 --> 53:47.120
 folks.

53:47.120 --> 53:50.280
 But has there been something truly successful

53:50.280 --> 53:51.680
 before deep learning?

53:51.680 --> 53:53.680
 Sure.

53:53.680 --> 53:54.880
 I want to make a larger point.

53:54.880 --> 53:59.640
 But on the narrower point, classical AI

53:59.640 --> 54:04.560
 is used, for example, in doing navigation instructions.

54:04.560 --> 54:06.040
 It's very successful.

54:06.040 --> 54:09.440
 Everybody on the planet uses it now, like multiple times a day.

54:09.440 --> 54:12.240
 That's a measure of success, right?

54:12.240 --> 54:16.080
 So I don't think classical AI was wildly successful.

54:16.080 --> 54:19.160
 But there are cases like that that is used all the time.

54:19.160 --> 54:23.760
 Nobody even notices them because they're so pervasive.

54:23.760 --> 54:26.480
 So there are some successes for classical AI.

54:26.480 --> 54:28.680
 I think deep learning has been more successful.

54:28.680 --> 54:32.040
 But my usual line about this, and I didn't invent it,

54:32.040 --> 54:33.760
 but I like it a lot, is just because you

54:33.760 --> 54:35.560
 can build a better ladder doesn't mean

54:35.560 --> 54:37.200
 you can build a ladder to the moon.

54:37.200 --> 54:41.000
 So the bitter lesson is if you have a perceptual classification

54:41.000 --> 54:43.800
 problem, throwing a lot of data at it

54:43.800 --> 54:45.760
 is better than anything else.

54:45.760 --> 54:50.000
 But that has not given us any material progress

54:50.000 --> 54:51.880
 in natural language understanding,

54:51.880 --> 54:53.960
 common sense reasoning like a robot would

54:53.960 --> 54:56.240
 need to navigate a home.

54:56.240 --> 54:59.440
 Problems like that, there is no actual progress there.

54:59.440 --> 55:02.240
 So flip side of that, if we remove data from the picture,

55:02.240 --> 55:09.120
 another bitter lesson is that you just have a very simple

55:09.120 --> 55:12.240
 algorithm and you wait for compute to scale.

55:12.240 --> 55:13.520
 This doesn't have to be learning.

55:13.520 --> 55:14.840
 It doesn't have to be deep learning.

55:14.840 --> 55:16.360
 It doesn't have to be data driven,

55:16.360 --> 55:18.240
 but just wait for the compute.

55:18.240 --> 55:19.880
 So my question for you, do you think

55:19.880 --> 55:21.640
 compute can unlock some of the things

55:21.640 --> 55:25.440
 with either deep learning or simple manipulation that?

55:25.440 --> 55:29.840
 Sure, but I'll put a proviso on that.

55:29.840 --> 55:32.440
 More compute's always better, like nobody's

55:32.440 --> 55:33.640
 going to argue with more compute.

55:33.640 --> 55:34.720
 It's like having more money.

55:34.720 --> 55:36.080
 I mean, there's the data.

55:36.080 --> 55:37.480
 There's diminishing returns on more money.

55:37.480 --> 55:37.980
 Exactly.

55:37.980 --> 55:39.760
 There's diminishing returns on more money,

55:39.760 --> 55:41.280
 but nobody's going to argue if you

55:41.280 --> 55:42.680
 want to give them more money, right?

55:42.680 --> 55:44.680
 Except maybe the people who signed the giving pledge,

55:44.680 --> 55:46.120
 and some of them have a problem.

55:46.120 --> 55:48.040
 They have problems to give away more money

55:48.040 --> 55:49.720
 than they're able to.

55:49.720 --> 55:52.520
 But the rest of us, if you want to give me more money, fine.

55:52.520 --> 55:54.600
 Say more money, more problems, but OK.

55:54.600 --> 55:55.880
 That's true too.

55:55.880 --> 56:00.120
 What I would say to you is your brain uses like 20 watts,

56:00.120 --> 56:02.720
 and it does a lot of things that deep learning doesn't do,

56:02.720 --> 56:04.720
 or that simple manipulation doesn't do,

56:04.720 --> 56:07.040
 that AI just hasn't figured out how to do.

56:07.040 --> 56:09.440
 So it's an existence proof that you

56:09.440 --> 56:14.240
 don't need server resources that are Google scale in order

56:14.240 --> 56:16.120
 to have an intelligence.

56:16.120 --> 56:18.920
 I built, with a lot of help from my wife,

56:18.920 --> 56:21.680
 two intelligences that are 20 watts each

56:21.680 --> 56:27.320
 and far exceed anything that anybody else has built at a silicon.

56:27.320 --> 56:30.280
 Speaking of those two robots, what

56:30.280 --> 56:33.280
 have you learned about AI from having?

56:33.280 --> 56:35.320
 Well, they're not robots, but.

56:35.320 --> 56:36.800
 Sorry, intelligent agents.

56:36.800 --> 56:38.160
 There's two intelligent agents.

56:38.160 --> 56:42.760
 I've learned a lot by watching my two intelligent agents.

56:42.760 --> 56:45.840
 I think that what's fundamentally interesting,

56:45.840 --> 56:48.000
 well, one of the many things that's fundamentally interesting

56:48.000 --> 56:50.800
 about them is the way that they set their own problems

56:50.800 --> 56:52.040
 to solve.

56:52.040 --> 56:54.560
 So my two kids are a year and a half apart.

56:54.560 --> 56:56.480
 They're both five and six and a half.

56:56.480 --> 56:59.560
 They play together all the time, and they're constantly

56:59.560 --> 57:00.840
 creating new challenges.

57:00.840 --> 57:03.840
 Like that's what they do, is they make up games,

57:03.840 --> 57:05.960
 and they're like, well, what if this, or what if that,

57:05.960 --> 57:07.880
 or what if I had this superpower,

57:07.880 --> 57:10.400
 or what if you could walk through this wall.

57:10.400 --> 57:14.120
 So they're doing these what if scenarios all the time.

57:14.120 --> 57:17.600
 And that's how they learn something about the world

57:17.600 --> 57:22.600
 and grow their minds, and machines don't really do that.

57:22.640 --> 57:23.680
 So that's interesting.

57:23.680 --> 57:25.280
 And you've talked about this, you've written about it,

57:25.280 --> 57:27.600
 you thought about it, nature versus nurture.

57:29.320 --> 57:33.640
 So what innate knowledge do you think we're born with?

57:33.640 --> 57:35.600
 And what do we learn along the way

57:35.600 --> 57:38.320
 in those early months and years?

57:38.320 --> 57:40.520
 Can I just say how much I like that question?

57:41.600 --> 57:45.840
 You phrased it just right, and almost nobody ever does.

57:45.840 --> 57:47.280
 Which is what is the innate knowledge

57:47.280 --> 57:49.280
 in what's learned along the way.

57:49.280 --> 57:51.240
 So many people that catamize it,

57:51.240 --> 57:53.480
 and they think it's nature versus nurture.

57:53.480 --> 57:56.840
 When it is obviously has to be nature and nurture,

57:56.840 --> 57:58.640
 they have to work together.

57:58.640 --> 58:00.560
 You can't learn the stuff along the way

58:00.560 --> 58:02.400
 unless you have some innate stuff.

58:02.400 --> 58:03.960
 But just because you have the innate stuff

58:03.960 --> 58:05.920
 doesn't mean you don't learn anything.

58:05.920 --> 58:09.320
 And so many people get that wrong, including in the field.

58:09.320 --> 58:12.280
 Like people think, if I work in machine learning,

58:12.280 --> 58:15.360
 the learning side, I must not be allowed to work

58:15.360 --> 58:17.360
 on the innate side where that will be cheating.

58:17.360 --> 58:19.680
 Exactly, people have said that to me.

58:19.680 --> 58:21.680
 And it's just absurd.

58:21.680 --> 58:22.520
 So thank you.

58:23.440 --> 58:25.240
 But you could break that apart more.

58:25.240 --> 58:26.640
 I've talked to folks who studied

58:26.640 --> 58:28.320
 the development of the brain.

58:28.320 --> 58:30.760
 And I mean, the growth of the brain

58:30.760 --> 58:35.000
 in the first few days, in the first few months,

58:35.000 --> 58:39.600
 in the womb, all of that, is that innate?

58:39.600 --> 58:42.400
 So that process of development from a stem cell

58:42.400 --> 58:45.480
 to the growth, the central nervous system and so on,

58:45.480 --> 58:49.360
 to the information that's encoded

58:49.360 --> 58:52.360
 through the long arc of evolution.

58:52.360 --> 58:55.400
 So all of that comes into play and it's unclear.

58:55.400 --> 58:57.400
 It's not just whether it's the dichotomy or not.

58:57.400 --> 59:02.160
 It's where most, or where the knowledge is encoded.

59:02.160 --> 59:07.160
 So what's your intuition about the innate knowledge,

59:07.720 --> 59:09.760
 the power of it, what's contained in it?

59:09.760 --> 59:11.440
 What can we learn from it?

59:11.440 --> 59:12.600
 One of my earlier books was actually

59:12.600 --> 59:14.040
 trying to understand the biology of this.

59:14.040 --> 59:15.880
 The book was called The Birth of the Mind.

59:15.880 --> 59:18.920
 Like how is it the genes even build innate knowledge?

59:18.920 --> 59:21.480
 And from the perspective of the conversation

59:21.480 --> 59:23.640
 we're having today, there's actually two questions.

59:23.640 --> 59:26.520
 One is what innate knowledge or mechanisms

59:26.520 --> 59:28.320
 or what have you?

59:28.320 --> 59:30.920
 People or other animals might be endowed with,

59:30.920 --> 59:32.280
 I always like showing this video

59:32.280 --> 59:34.640
 of a baby Ibex climbing down a mountain.

59:34.640 --> 59:37.400
 That baby Ibex a few hours after his birth

59:37.400 --> 59:38.440
 knows how to climb down a mountain.

59:38.440 --> 59:40.960
 That means that it knows, not consciously,

59:40.960 --> 59:43.040
 something about its own body and physics

59:43.040 --> 59:46.440
 and 3D geometry and all of this kind of stuff.

59:47.520 --> 59:49.720
 So there's one question about like what does biology

59:49.720 --> 59:53.240
 give its creatures and what has evolved in our brains?

59:53.240 --> 59:55.000
 How is that represented in our brains?

59:55.000 --> 59:56.200
 The question I thought about in the book,

59:56.200 --> 59:57.360
 The Birth of the Mind.

59:57.360 --> 59:59.320
 And then there's a question of what AI should have.

59:59.320 --> 1:00:01.600
 And they don't have to be the same.

1:00:01.600 --> 1:00:06.600
 But I would say that it's a pretty interesting set

1:00:07.240 --> 1:00:08.720
 of things that we are equipped with

1:00:08.720 --> 1:00:10.520
 that allows us to do a lot of interesting things.

1:00:10.520 --> 1:00:13.760
 So I would argue or guess based on my reading

1:00:13.760 --> 1:00:15.280
 of the developmental psychology literature,

1:00:15.280 --> 1:00:16.800
 which I've also participated in,

1:00:18.040 --> 1:00:22.600
 that children are born with a notion of space, time,

1:00:22.600 --> 1:00:24.480
 other agents, places,

1:00:25.760 --> 1:00:27.680
 and also this kind of mental algebra

1:00:27.680 --> 1:00:30.280
 that I was describing before.

1:00:30.280 --> 1:00:33.120
 No certain of causation if I didn't just say that.

1:00:33.120 --> 1:00:35.680
 So at least those kinds of things.

1:00:35.680 --> 1:00:38.800
 They're like frameworks for learning the other things.

1:00:38.800 --> 1:00:40.400
 So are they disjoint in your view?

1:00:40.400 --> 1:00:42.920
 Or is it just somehow all connected?

1:00:42.920 --> 1:00:44.400
 You've talked a lot about language.

1:00:44.400 --> 1:00:48.000
 Is it all kind of connected in some mesh

1:00:48.000 --> 1:00:52.640
 that's language like if understanding concepts altogether?

1:00:52.640 --> 1:00:54.880
 Or I don't think we know for people

1:00:54.880 --> 1:00:56.280
 how they're represented in machines

1:00:56.280 --> 1:00:58.200
 just don't really do this yet.

1:00:58.200 --> 1:01:00.600
 So I think it's an interesting open question

1:01:00.600 --> 1:01:02.680
 both for science and for engineering.

1:01:03.600 --> 1:01:06.400
 Some of it has to be at least interrelated

1:01:06.400 --> 1:01:10.240
 in the way that the interfaces of a software package

1:01:10.240 --> 1:01:12.200
 have to be able to talk to one another.

1:01:12.200 --> 1:01:16.680
 So the systems that represent space and time

1:01:16.680 --> 1:01:18.320
 can't be totally disjoint

1:01:18.320 --> 1:01:20.760
 because a lot of the things that we reason about

1:01:20.760 --> 1:01:23.040
 are the relations between space and time and cause.

1:01:23.040 --> 1:01:26.480
 So I put this on and I have expectations

1:01:26.480 --> 1:01:28.040
 about what's gonna happen with the bottle cap

1:01:28.040 --> 1:01:29.520
 on top of the bottle.

1:01:29.520 --> 1:01:32.600
 And those span space and time.

1:01:32.600 --> 1:01:35.760
 If the cap is over here, I get a different outcome.

1:01:35.760 --> 1:01:38.560
 If the timing is different, if I put this here

1:01:38.560 --> 1:01:41.920
 after I move that, then I get a different outcome

1:01:41.920 --> 1:01:43.080
 that relates to causality.

1:01:43.080 --> 1:01:47.880
 So obviously these mechanisms, whatever they are,

1:01:47.880 --> 1:01:50.080
 can certainly communicate with each other.

1:01:50.080 --> 1:01:53.200
 So I think evolution had a significant role

1:01:53.200 --> 1:01:57.120
 to play in the development of this whole collage, right?

1:01:57.120 --> 1:01:59.240
 How efficient do you think is evolution?

1:01:59.240 --> 1:02:01.960
 Oh, it's terribly inefficient, except that.

1:02:01.960 --> 1:02:03.080
 Well, can we do better?

1:02:03.080 --> 1:02:05.760
 Well, let's come to that in a second.

1:02:05.760 --> 1:02:09.440
 It's inefficient except that once it gets a good idea,

1:02:09.440 --> 1:02:10.880
 it runs with it.

1:02:10.880 --> 1:02:15.680
 So it took, I guess a billion years,

1:02:15.680 --> 1:02:20.680
 roughly a billion years to evolve to a vertebrate brain plan.

1:02:24.040 --> 1:02:26.920
 And once that vertebrate plan evolved,

1:02:26.920 --> 1:02:28.480
 it spread everywhere.

1:02:28.480 --> 1:02:31.680
 So fish have it and dogs have it and we have it.

1:02:31.680 --> 1:02:34.720
 We have adaptations of it and specializations of it.

1:02:34.720 --> 1:02:37.160
 And the same thing with a primate brain plan.

1:02:37.160 --> 1:02:41.120
 So monkeys have it and apes have it and we have it.

1:02:41.120 --> 1:02:43.760
 So there are additional innovations like color vision

1:02:43.760 --> 1:02:45.880
 and those spread really rapidly.

1:02:45.880 --> 1:02:48.840
 So it takes evolution a long time to get a good idea,

1:02:48.840 --> 1:02:53.280
 but being anthropomorphic and not literal here,

1:02:53.280 --> 1:02:55.600
 but once it has that idea, so to speak,

1:02:55.600 --> 1:02:58.560
 which caches out into one set of genes or in the genome,

1:02:58.560 --> 1:03:00.520
 those genes spread very rapidly

1:03:00.520 --> 1:03:02.640
 and they're like subroutines or libraries,

1:03:02.640 --> 1:03:04.560
 I guess the word people might use nowadays

1:03:04.560 --> 1:03:05.640
 or be more familiar with,

1:03:05.640 --> 1:03:08.800
 they're libraries that can get used over and over again.

1:03:08.800 --> 1:03:11.760
 So once you have the library for building something

1:03:11.760 --> 1:03:13.840
 with multiple digits, you can use it for a hand,

1:03:13.840 --> 1:03:15.520
 but you can also use it for a foot.

1:03:15.520 --> 1:03:17.400
 You just kind of reuse the library

1:03:17.400 --> 1:03:19.080
 with slightly different parameters.

1:03:19.080 --> 1:03:20.640
 Evolution does a lot of that,

1:03:20.640 --> 1:03:23.480
 which means that the speed over time picks up.

1:03:23.480 --> 1:03:25.560
 So evolution can happen faster

1:03:25.560 --> 1:03:28.400
 because you have bigger and bigger libraries.

1:03:28.400 --> 1:03:32.240
 And what I think has happened in attempts

1:03:32.240 --> 1:03:35.760
 at evolutionary computation is that people start

1:03:35.760 --> 1:03:40.360
 with libraries that are very, very minimal,

1:03:40.360 --> 1:03:44.280
 like almost nothing and then progress is slow

1:03:44.280 --> 1:03:46.640
 and it's hard for someone to get a good PhD thesis

1:03:46.640 --> 1:03:48.280
 out of it and they give up.

1:03:48.280 --> 1:03:50.280
 If we had richer libraries to begin with,

1:03:50.280 --> 1:03:52.640
 if you were evolving from systems

1:03:52.640 --> 1:03:55.360
 that hadn't originate structure to begin with,

1:03:55.360 --> 1:03:56.800
 then things might speed up.

1:03:56.800 --> 1:04:00.880
 Or more PhD students, if the evolutionary process is indeed

1:04:00.880 --> 1:04:04.240
 in a meta way, runs away with good ideas,

1:04:04.240 --> 1:04:06.720
 you need to have a lot of ideas,

1:04:06.720 --> 1:04:08.840
 pool of ideas in order for it to discover one

1:04:08.840 --> 1:04:10.240
 that you can run away with.

1:04:10.240 --> 1:04:13.200
 And PhD students representing individual ideas as well.

1:04:13.200 --> 1:04:16.240
 Yeah, I mean, you could throw a billion PhD students at it.

1:04:16.240 --> 1:04:18.960
 Yeah, the monkeys at typewriters with Shakespeare, yep.

1:04:20.160 --> 1:04:22.080
 Well, I mean, those aren't cumulative, right?

1:04:22.080 --> 1:04:23.440
 That's just random.

1:04:23.440 --> 1:04:24.960
 And part of the point that I'm making

1:04:24.960 --> 1:04:26.800
 is that evolution is cumulative.

1:04:26.800 --> 1:04:31.160
 So if you have a billion monkeys independently,

1:04:31.160 --> 1:04:32.440
 you don't really get anywhere.

1:04:32.440 --> 1:04:33.840
 But if you have a billion monkeys,

1:04:33.840 --> 1:04:35.720
 and I think Dawkins made this point originally,

1:04:35.720 --> 1:04:36.560
 or probably other people,

1:04:36.560 --> 1:04:37.600
 Dawkins made it very nice

1:04:37.600 --> 1:04:39.960
 and either a selfish gene or blind watchmaker.

1:04:41.320 --> 1:04:44.080
 If there is some sort of fitness function

1:04:44.080 --> 1:04:45.680
 that can drive you towards something,

1:04:45.680 --> 1:04:47.120
 I guess that's Dawkins point.

1:04:47.120 --> 1:04:49.440
 And my point, which is a little variation on that,

1:04:49.440 --> 1:04:52.120
 is that if the evolution is cumulative,

1:04:52.120 --> 1:04:55.600
 the related points, then you can start going faster.

1:04:55.600 --> 1:04:57.760
 Do you think something like the process of evolution

1:04:57.760 --> 1:05:00.160
 is required to build intelligent systems?

1:05:00.160 --> 1:05:01.000
 So if we...

1:05:01.000 --> 1:05:01.840
 Not logically.

1:05:01.840 --> 1:05:04.040
 So all the stuff that evolution did,

1:05:04.040 --> 1:05:07.040
 a good engineer might be able to do.

1:05:07.040 --> 1:05:10.560
 So for example, evolution made quadrupeds,

1:05:10.560 --> 1:05:14.200
 which distribute the load across a horizontal surface.

1:05:14.200 --> 1:05:17.000
 A good engineer could come up with that idea.

1:05:17.000 --> 1:05:18.720
 I mean, sometimes good engineers come up with ideas

1:05:18.720 --> 1:05:19.760
 by looking at biology.

1:05:19.760 --> 1:05:22.080
 There's lots of ways to get your ideas.

1:05:22.080 --> 1:05:23.680
 And part of what I'm suggesting

1:05:23.680 --> 1:05:26.000
 is we should look at biology a lot more.

1:05:26.000 --> 1:05:29.280
 We should look at the biology of thought

1:05:29.280 --> 1:05:31.720
 and understanding the biology

1:05:31.720 --> 1:05:35.000
 by which creatures intuitively reason about physics

1:05:35.000 --> 1:05:36.240
 or other agents or like,

1:05:36.240 --> 1:05:37.960
 how do dogs reason about people?

1:05:37.960 --> 1:05:39.680
 Like they're actually pretty good at it.

1:05:39.680 --> 1:05:41.840
 If we could understand...

1:05:41.840 --> 1:05:44.040
 At my college, we joked, dognition.

1:05:44.040 --> 1:05:46.320
 If we could understand dognition well,

1:05:46.320 --> 1:05:47.720
 then how it was implemented,

1:05:47.720 --> 1:05:49.800
 that might help us with our AI.

1:05:49.800 --> 1:05:53.800
 So do you think it's possible

1:05:53.800 --> 1:05:57.200
 that the kind of timescale that evolution took

1:05:57.200 --> 1:05:58.960
 is the kind of timescale that will be needed

1:05:58.960 --> 1:06:00.520
 to build intelligent systems?

1:06:00.520 --> 1:06:02.960
 Or can we significantly accelerate that process

1:06:02.960 --> 1:06:04.040
 inside a computer?

1:06:05.440 --> 1:06:07.560
 I mean, I think the way that we accelerate that process

1:06:07.560 --> 1:06:09.720
 is we borrow from biology.

1:06:10.640 --> 1:06:14.280
 Not slavishly, but I think we look at how biology

1:06:14.280 --> 1:06:15.640
 has solved problems and we say,

1:06:15.640 --> 1:06:18.880
 does that inspire any engineering solutions here?

1:06:18.880 --> 1:06:20.720
 And try to mimic biological systems

1:06:20.720 --> 1:06:22.400
 and then therefore have a shortcut?

1:06:22.400 --> 1:06:25.040
 Yeah, I mean, there's a field called biomimicry

1:06:25.040 --> 1:06:29.000
 and people do that for like material science all the time.

1:06:29.000 --> 1:06:32.960
 We should be doing the analog of that for AI.

1:06:32.960 --> 1:06:34.480
 And the analog for that for AI

1:06:34.480 --> 1:06:35.800
 is to look at cognitive science

1:06:35.800 --> 1:06:37.040
 or the cognitive sciences,

1:06:37.040 --> 1:06:40.320
 which is psychology, maybe neuroscience, linguistics

1:06:40.320 --> 1:06:43.480
 and so forth, look to those for insight.

1:06:43.480 --> 1:06:45.360
 What do you think is a good test of intelligence

1:06:45.360 --> 1:06:46.700
 in your view?

1:06:46.700 --> 1:06:48.520
 I don't think there's one good test.

1:06:48.520 --> 1:06:51.840
 In fact, I try to organize a movement

1:06:51.840 --> 1:06:53.400
 towards something called a Turing Olympics.

1:06:53.400 --> 1:06:56.200
 And my hope is that Francois is actually gonna take,

1:06:56.200 --> 1:06:58.280
 Francois Chalet is gonna take over this.

1:06:58.280 --> 1:06:59.960
 I think he's interested in that.

1:06:59.960 --> 1:07:03.480
 I just don't have a place in my busy life at this moment.

1:07:03.480 --> 1:07:06.440
 But the notion is that there'd be many tests

1:07:06.440 --> 1:07:09.520
 and not just one because intelligence is multifaceted.

1:07:09.520 --> 1:07:12.920
 There can't really be a single measure of it

1:07:12.920 --> 1:07:14.640
 because it isn't a single thing.

1:07:15.640 --> 1:07:17.360
 Like just the crudest level,

1:07:17.360 --> 1:07:19.880
 the SAT is a verbal component and a math component

1:07:19.880 --> 1:07:21.360
 because they're not identical.

1:07:21.360 --> 1:07:23.640
 And Howard Gardner has talked about multiple intelligence,

1:07:23.640 --> 1:07:25.440
 like kinesthetic intelligence

1:07:25.440 --> 1:07:27.760
 and verbal intelligence and so forth.

1:07:27.760 --> 1:07:29.960
 There are a lot of things that go into intelligence

1:07:29.960 --> 1:07:32.560
 and people can get good at one or the other.

1:07:32.560 --> 1:07:34.720
 I mean, in some sense, like every expert

1:07:34.720 --> 1:07:37.240
 has developed a very specific kind of intelligence.

1:07:37.240 --> 1:07:39.280
 And then there are people that are generalists.

1:07:39.280 --> 1:07:41.760
 And I think of myself as a generalist

1:07:41.760 --> 1:07:43.400
 with respect to cognitive science,

1:07:43.400 --> 1:07:45.640
 which doesn't mean I know anything about quantum mechanics,

1:07:45.640 --> 1:07:49.240
 but I know a lot about the different facets of the mind.

1:07:49.240 --> 1:07:51.360
 And there's a kind of intelligence

1:07:51.360 --> 1:07:52.680
 to thinking about intelligence.

1:07:52.680 --> 1:07:54.760
 I like to think that I have some of that,

1:07:54.760 --> 1:07:57.480
 but social intelligence, I'm just okay.

1:07:57.480 --> 1:08:00.160
 There are people that are much better at that than I am.

1:08:00.160 --> 1:08:03.040
 Sure, but what would be really impressive to you?

1:08:04.120 --> 1:08:07.080
 I think the idea of a touring Olympics is really interesting,

1:08:07.080 --> 1:08:09.680
 especially if somebody like Francois is running it.

1:08:09.680 --> 1:08:14.400
 But to you in general, not as a benchmark,

1:08:14.400 --> 1:08:18.480
 but if you saw an AI system being able to accomplish something

1:08:18.480 --> 1:08:21.720
 that would impress the heck out of you,

1:08:21.720 --> 1:08:22.720
 what would that thing be?

1:08:22.720 --> 1:08:24.680
 Would it be natural language conversation?

1:08:24.680 --> 1:08:29.520
 For me personally, I would like to see a kind of comprehension

1:08:29.520 --> 1:08:30.680
 that relates to what you just said.

1:08:30.680 --> 1:08:34.960
 So I wrote a piece in the New Yorker in I think 2015,

1:08:34.960 --> 1:08:39.960
 right after Eugene Gustman, which was a software package,

1:08:39.960 --> 1:08:42.880
 won a version of the touring test.

1:08:42.880 --> 1:08:45.160
 And the way that it did this is it'd be,

1:08:45.160 --> 1:08:46.840
 well, the way you win the touring test,

1:08:46.840 --> 1:08:50.680
 so called win it, is the touring test is you fool a person

1:08:50.680 --> 1:08:54.400
 into thinking that a machine is a person.

1:08:54.400 --> 1:08:57.960
 Is you're evasive, you pretend to have limitations

1:08:57.960 --> 1:09:00.560
 so you don't have to answer certain questions and so forth.

1:09:00.560 --> 1:09:02.680
 So this particular system

1:09:02.680 --> 1:09:05.280
 pretended to be a 13 year old boy from Odessa

1:09:05.280 --> 1:09:08.040
 who didn't understand English and was kind of sarcastic

1:09:08.040 --> 1:09:09.680
 and wouldn't answer your questions and so forth.

1:09:09.680 --> 1:09:12.480
 And so judges got fooled into thinking briefly

1:09:12.480 --> 1:09:14.680
 with a very little exposure to the 13 year old boy

1:09:14.680 --> 1:09:17.120
 and it docked all the questions the touring was actually

1:09:17.120 --> 1:09:17.960
 interested in, which is like,

1:09:17.960 --> 1:09:20.440
 how do you make the machine actually intelligent?

1:09:20.440 --> 1:09:22.120
 So that test itself is not that good.

1:09:22.120 --> 1:09:26.080
 And so in New Yorker, I proposed an alternative, I guess.

1:09:26.080 --> 1:09:29.040
 And the one that I proposed there was a comprehension test.

1:09:30.000 --> 1:09:31.080
 And I must like Breaking Bad,

1:09:31.080 --> 1:09:32.920
 because I've already given you one Breaking Bad example

1:09:32.920 --> 1:09:35.640
 and in that article I have one as well,

1:09:35.640 --> 1:09:37.640
 which was something like if Walter,

1:09:37.640 --> 1:09:40.320
 you should be able to watch an episode of Breaking Bad

1:09:40.320 --> 1:09:41.680
 or maybe you have to watch the whole series

1:09:41.680 --> 1:09:43.520
 to be able to answer the question and say,

1:09:43.520 --> 1:09:45.600
 if Walter White took a hit out on Jesse,

1:09:45.600 --> 1:09:47.160
 why did he do that?

1:09:47.160 --> 1:09:49.360
 So if you could answer kind of arbitrary questions

1:09:49.360 --> 1:09:51.240
 about characters motivations,

1:09:51.240 --> 1:09:52.920
 I would be really impressed with that.

1:09:52.920 --> 1:09:55.360
 I mean, he built software to do that.

1:09:55.360 --> 1:09:58.480
 They could watch a film or they're different versions.

1:09:58.480 --> 1:10:01.920
 And so ultimately I wrote this up with Praveen Paratosh

1:10:01.920 --> 1:10:03.600
 in a special issue of AI magazine

1:10:03.600 --> 1:10:05.760
 that basically was about the Turing Olympics.

1:10:05.760 --> 1:10:07.720
 There were like 14 tests proposed.

1:10:07.720 --> 1:10:10.080
 The one that I was pushing was a comprehension challenge

1:10:10.080 --> 1:10:12.360
 and Praveen who's at Google was trying to figure out

1:10:12.360 --> 1:10:13.440
 like how we would actually run it.

1:10:13.440 --> 1:10:15.320
 And so we wrote a paper together.

1:10:15.320 --> 1:10:17.280
 And you could have a text version too,

1:10:17.280 --> 1:10:19.640
 or you could have an auditory podcast version,

1:10:19.640 --> 1:10:20.560
 you could have a written version.

1:10:20.560 --> 1:10:23.720
 But the point is that you win at this test

1:10:23.720 --> 1:10:26.960
 if you can do let's say human level or better than humans

1:10:26.960 --> 1:10:29.520
 at answering kind of arbitrary questions.

1:10:29.520 --> 1:10:31.600
 You know, why did this person pick up the stone?

1:10:31.600 --> 1:10:34.080
 What were they thinking when they picked up the stone?

1:10:34.080 --> 1:10:36.160
 Were they trying to knock down glass?

1:10:36.160 --> 1:10:38.640
 And I mean, ideally these wouldn't be multiple choice either

1:10:38.640 --> 1:10:41.040
 because multiple choice is pretty easily gamed.

1:10:41.040 --> 1:10:44.120
 So if you could have relatively open ended questions

1:10:44.120 --> 1:10:47.440
 and you can answer why people are doing this stuff,

1:10:47.440 --> 1:10:48.280
 I would be very impressed.

1:10:48.280 --> 1:10:50.160
 And of course humans can do this, right?

1:10:50.160 --> 1:10:52.920
 If you watch a well constructed movie

1:10:52.920 --> 1:10:55.600
 and somebody picks up a rock,

1:10:55.600 --> 1:10:57.360
 everybody watching the movie knows

1:10:57.360 --> 1:10:59.520
 why they picked up the rock, right?

1:10:59.520 --> 1:11:03.000
 They all know, oh my gosh, he's gonna hit this character

1:11:03.000 --> 1:11:03.840
 or whatever.

1:11:03.840 --> 1:11:06.280
 We have an example in the book about

1:11:06.280 --> 1:11:08.720
 when a whole bunch of people say, I am Spartacus,

1:11:08.720 --> 1:11:10.040
 you know this famous scene?

1:11:11.800 --> 1:11:14.200
 The viewers understand, first of all,

1:11:14.200 --> 1:11:19.080
 that everybody or everybody minus one has to be lying.

1:11:19.080 --> 1:11:20.400
 They can't all be Spartacus.

1:11:20.400 --> 1:11:21.840
 We have enough common sense knowledge

1:11:21.840 --> 1:11:24.160
 to know they couldn't all have the same name.

1:11:24.160 --> 1:11:25.400
 We know that they're lying

1:11:25.400 --> 1:11:27.160
 and we can infer why they're lying, right?

1:11:27.160 --> 1:11:28.520
 They're lying to protect someone

1:11:28.520 --> 1:11:30.360
 and to protect things they believe in.

1:11:30.360 --> 1:11:32.400
 You get a machine that can do that.

1:11:32.400 --> 1:11:35.160
 They can say, this is why these guys all got up

1:11:35.160 --> 1:11:37.000
 and said, I am Spartacus.

1:11:37.000 --> 1:11:40.560
 I will sit down and say AI has really achieved a lot.

1:11:40.560 --> 1:11:41.400
 Thank you.

1:11:41.400 --> 1:11:43.880
 Without cheating any part of the system.

1:11:43.880 --> 1:11:45.640
 Yeah, I mean, if you do it,

1:11:45.640 --> 1:11:46.600
 there are lots of ways you can cheat.

1:11:46.600 --> 1:11:48.840
 Like you could build a Spartacus machine

1:11:48.840 --> 1:11:50.120
 that works on that film.

1:11:50.120 --> 1:11:51.160
 Like that's not what I'm talking about.

1:11:51.160 --> 1:11:52.880
 I'm talking about, you can do this

1:11:52.880 --> 1:11:55.720
 with essentially arbitrary films from a large size.

1:11:55.720 --> 1:11:57.680
 Even beyond films because it's possible

1:11:57.680 --> 1:11:59.000
 such a system would discover

1:11:59.000 --> 1:12:02.600
 that the number of narrative arcs in film

1:12:02.600 --> 1:12:04.040
 is like limited to like 1930.

1:12:04.040 --> 1:12:06.440
 There's a famous thing about the classic seven plots

1:12:06.440 --> 1:12:07.280
 or whatever.

1:12:07.280 --> 1:12:09.120
 I don't care if you want to build in the system,

1:12:09.120 --> 1:12:11.680
 boy meets girl, boy loses girl, boy finds girl.

1:12:11.680 --> 1:12:12.520
 That's fine.

1:12:12.520 --> 1:12:14.560
 I don't mind having some headstone knowledge.

1:12:14.560 --> 1:12:15.400
 Okay.

1:12:15.400 --> 1:12:16.240
 Good.

1:12:16.240 --> 1:12:18.000
 I mean, you could build it into Nathalie

1:12:18.000 --> 1:12:20.480
 or you could have your system watch a lot of films again.

1:12:20.480 --> 1:12:22.400
 If you can do this at all,

1:12:22.400 --> 1:12:23.760
 but with a wide range of films,

1:12:23.760 --> 1:12:26.240
 not just one film and one genre.

1:12:27.320 --> 1:12:28.880
 But even if you could do it for all Westerns,

1:12:28.880 --> 1:12:30.320
 I'd be reasonably impressed.

1:12:30.320 --> 1:12:31.160
 Yeah.

1:12:31.160 --> 1:12:34.120
 So in terms of being impressed,

1:12:34.120 --> 1:12:35.840
 just for the fun of it,

1:12:35.840 --> 1:12:38.440
 because you've put so many interesting ideas out there

1:12:38.440 --> 1:12:40.240
 in your book,

1:12:40.240 --> 1:12:43.680
 a challenge in the community for further steps,

1:12:43.680 --> 1:12:46.720
 is it possible on the deep learning front

1:12:46.720 --> 1:12:50.240
 that you're wrong about its limitations,

1:12:50.240 --> 1:12:52.280
 that deep learning will unlock,

1:12:52.280 --> 1:12:54.480
 Yanlacou next year will publish a paper

1:12:54.480 --> 1:12:56.880
 that achieves this comprehension.

1:12:56.880 --> 1:13:00.280
 So do you think that way often as a scientist,

1:13:00.280 --> 1:13:03.040
 do you consider that your intuition

1:13:03.040 --> 1:13:06.680
 that deep learning could actually run away with it?

1:13:06.680 --> 1:13:09.720
 I'm more worried about rebranding

1:13:09.720 --> 1:13:11.320
 as a kind of political thing.

1:13:11.320 --> 1:13:14.040
 So I mean, what's gonna happen, I think,

1:13:14.040 --> 1:13:16.400
 is that deep learning is gonna start to encompass

1:13:16.400 --> 1:13:17.360
 simple manipulation.

1:13:17.360 --> 1:13:19.200
 So I think Hinton's just wrong.

1:13:19.200 --> 1:13:20.840
 Hinton says we don't want hybrids.

1:13:20.840 --> 1:13:22.360
 I think people will work towards hybrids

1:13:22.360 --> 1:13:24.680
 and they will relabel their hybrids as deep learning.

1:13:24.680 --> 1:13:25.840
 We've already seen some of that.

1:13:25.840 --> 1:13:29.560
 So AlphaGo is often described as a deep learning system,

1:13:29.560 --> 1:13:31.720
 but it's more correctly described as a system

1:13:31.720 --> 1:13:33.920
 that has deep learning, but also Monte Carlo Tree Search,

1:13:33.920 --> 1:13:35.640
 which is a classical AI technique.

1:13:35.640 --> 1:13:37.560
 And people will start to blur the lines

1:13:37.560 --> 1:13:39.840
 in the way that IBM blurred Watson.

1:13:39.840 --> 1:13:41.600
 First Watson meant this particular system

1:13:41.600 --> 1:13:43.160
 and then it was just anything that IBM built

1:13:43.160 --> 1:13:44.200
 in their cognitive division.

1:13:44.200 --> 1:13:45.800
 But purely, let me ask for sure.

1:13:45.800 --> 1:13:49.520
 That's a branding question and that's a giant mess.

1:13:49.520 --> 1:13:52.000
 I mean purely a single neural network

1:13:52.000 --> 1:13:54.080
 being able to accomplish reasoning and comprehension.

1:13:54.080 --> 1:13:55.360
 I don't stay up at night

1:13:55.360 --> 1:13:57.840
 worrying that that's gonna happen.

1:13:57.840 --> 1:13:59.280
 And I'll just give you two examples.

1:13:59.280 --> 1:14:01.680
 One is a guy at DeepMind

1:14:01.680 --> 1:14:05.560
 thought he had finally outfoxed me at Xergy Lord,

1:14:05.560 --> 1:14:07.160
 I think is his Twitter handle.

1:14:08.040 --> 1:14:10.600
 And he specifically made an example.

1:14:10.600 --> 1:14:14.920
 Marcus said that such and such, he fed it into GP2,

1:14:14.920 --> 1:14:17.680
 which is the AI system that is so smart

1:14:17.680 --> 1:14:19.080
 that OpenAI couldn't release it

1:14:19.080 --> 1:14:21.200
 because it would destroy the world, right?

1:14:21.200 --> 1:14:22.960
 You remember that a few months ago.

1:14:22.960 --> 1:14:27.720
 So he feeds it into GPT2 and my example was something

1:14:27.720 --> 1:14:30.360
 like a rose is a rose, a tulip is a tulip,

1:14:30.360 --> 1:14:31.360
 a lily is a blank.

1:14:31.360 --> 1:14:32.880
 And he got it to actually do that,

1:14:32.880 --> 1:14:34.040
 which was a little bit impressive.

1:14:34.040 --> 1:14:35.400
 And I wrote back and I said, that's impressive,

1:14:35.400 --> 1:14:37.760
 but can I ask you a few questions?

1:14:37.760 --> 1:14:40.080
 I said, was that just one example?

1:14:40.080 --> 1:14:41.680
 Can it do it generally?

1:14:41.680 --> 1:14:43.280
 And can it do it with novel words?

1:14:43.280 --> 1:14:45.360
 Which is part of what I was talking about in 1998

1:14:45.360 --> 1:14:46.760
 when I first raised the example.

1:14:46.760 --> 1:14:49.400
 So a DAX is a DAX, right?

1:14:50.360 --> 1:14:53.080
 And he sheepishly wrote back about 20 minutes later

1:14:53.080 --> 1:14:55.360
 and the answer was, well, it had some problems with those.

1:14:55.360 --> 1:14:58.840
 So I made some predictions 21 years ago

1:14:58.840 --> 1:15:01.960
 that still hold in the world of computer science.

1:15:01.960 --> 1:15:02.800
 That's amazing, right?

1:15:02.800 --> 1:15:06.560
 Because there's a thousand or a million times more memory

1:15:06.560 --> 1:15:10.080
 and computations a million times,

1:15:10.080 --> 1:15:13.240
 do a million times more operations per second,

1:15:13.240 --> 1:15:16.880
 spread across a cluster and there's been advances

1:15:16.880 --> 1:15:21.880
 in replacing sigmoids with other functions and so forth.

1:15:21.880 --> 1:15:25.400
 There's all kinds of advances,

1:15:25.400 --> 1:15:27.120
 but the fundamental architecture hasn't changed

1:15:27.120 --> 1:15:28.600
 and the fundamental limit hasn't changed.

1:15:28.600 --> 1:15:30.880
 And what I said then is kind of still true.

1:15:30.880 --> 1:15:32.240
 And then here's a second example.

1:15:32.240 --> 1:15:35.240
 I recently had a piece in Wired that's adapted from the book

1:15:35.240 --> 1:15:40.120
 and the book didn't, it was when to press before GP2 came out.

1:15:40.120 --> 1:15:42.280
 But we describe this children's story

1:15:42.280 --> 1:15:45.600
 and all the inferences that you make in this story

1:15:45.600 --> 1:15:48.240
 about a boy finding a lost wallet.

1:15:48.240 --> 1:15:52.840
 And for fun in the Wired piece, we ran it through GP2.

1:15:52.840 --> 1:15:55.440
 GP2 at something called TalkToTransformer.com

1:15:55.440 --> 1:15:58.160
 and your viewers can try this experiment themselves,

1:15:58.160 --> 1:16:01.080
 go to the Wired piece that has the link and it has the story.

1:16:01.080 --> 1:16:04.280
 And the system made perfectly fluent text

1:16:04.280 --> 1:16:06.400
 that was totally inconsistent

1:16:06.400 --> 1:16:10.240
 with the conceptual underpinnings of the story, right?

1:16:10.240 --> 1:16:13.200
 And this is what, again, I predicted in 1998

1:16:13.200 --> 1:16:14.680
 and for that matter, Chomsky Miller

1:16:14.680 --> 1:16:16.640
 made the same prediction in 1963.

1:16:16.640 --> 1:16:19.400
 I was just updating their claim for a slightly new text.

1:16:19.400 --> 1:16:22.600
 So those particular architectures

1:16:22.600 --> 1:16:24.800
 that don't have any built in knowledge,

1:16:24.800 --> 1:16:27.000
 they're basically just a bunch of layers

1:16:27.000 --> 1:16:28.920
 doing correlational stuff,

1:16:28.920 --> 1:16:31.240
 they're not gonna solve these problems.

1:16:31.240 --> 1:16:34.520
 So 20 years ago, you said the emperor has no clothes.

1:16:34.520 --> 1:16:36.840
 Today, the emperor still has no clothes.

1:16:36.840 --> 1:16:38.040
 The lighting's better though.

1:16:38.040 --> 1:16:39.040
 The lighting is better.

1:16:39.040 --> 1:16:42.280
 And I think you yourself are also, I mean.

1:16:42.280 --> 1:16:44.360
 And we found out some things to do with Naked Emperors.

1:16:44.360 --> 1:16:46.520
 I mean, it's not like stuff is worthless.

1:16:46.520 --> 1:16:48.360
 I mean, they're not really Naked.

1:16:48.360 --> 1:16:49.680
 It's more like they're in their briefs

1:16:49.680 --> 1:16:50.920
 and everybody thinks that.

1:16:50.920 --> 1:16:54.440
 And so like, I mean, they are great at speech recognition.

1:16:54.440 --> 1:16:56.520
 But the problems that I said were hard.

1:16:56.520 --> 1:16:58.320
 I didn't literally say the emperor has no clothes.

1:16:58.320 --> 1:17:00.200
 I said, this is a set of problems

1:17:00.200 --> 1:17:01.880
 that humans are really good at.

1:17:01.880 --> 1:17:03.200
 And it wasn't couched as AI,

1:17:03.200 --> 1:17:04.400
 it was couched as cognitive science.

1:17:04.400 --> 1:17:07.800
 But I said, if you wanna build a neural model

1:17:07.800 --> 1:17:10.440
 of how humans do certain class of things,

1:17:10.440 --> 1:17:12.040
 you're gonna have to change the architecture.

1:17:12.040 --> 1:17:13.720
 And I stand by those claims.

1:17:13.720 --> 1:17:16.840
 So, and I think people should understand

1:17:16.840 --> 1:17:19.120
 you're quite entertaining in your cynicism,

1:17:19.120 --> 1:17:22.320
 but you're also very optimistic and a dreamer

1:17:22.320 --> 1:17:24.000
 about the future of AI too.

1:17:24.000 --> 1:17:25.440
 So you're both, it's just.

1:17:25.440 --> 1:17:27.920
 There's a famous saying about being,

1:17:27.920 --> 1:17:30.760
 people overselling technology in the short run

1:17:30.760 --> 1:17:34.200
 and underselling it in the long run.

1:17:34.200 --> 1:17:37.240
 And so I actually end the book,

1:17:37.240 --> 1:17:40.600
 Ernie Davis and I end our book with an optimistic chapter,

1:17:40.600 --> 1:17:41.760
 which kind of killed Ernie

1:17:41.760 --> 1:17:44.440
 because he's even more pessimistic than I am.

1:17:44.440 --> 1:17:47.640
 He describes me as a contrarian and him as a pessimist.

1:17:47.640 --> 1:17:49.880
 But I persuaded him that we should end the book

1:17:49.880 --> 1:17:52.680
 with a look at what would happen

1:17:52.680 --> 1:17:55.400
 if AI really did incorporate, for example,

1:17:55.400 --> 1:17:57.320
 the common sense reasoning and the nativism

1:17:57.320 --> 1:17:59.680
 and so forth, the things that we counseled for.

1:17:59.680 --> 1:18:02.160
 And we wrote it and it's an optimistic chapter

1:18:02.160 --> 1:18:05.920
 that AI suitably reconstructed so that we could trust it,

1:18:05.920 --> 1:18:09.520
 which we can't now, could really be world changing.

1:18:09.520 --> 1:18:12.160
 So on that point, if you look at the future

1:18:12.160 --> 1:18:15.400
 trajectories of AI, people have worries

1:18:15.400 --> 1:18:17.160
 about negative effects of AI,

1:18:17.160 --> 1:18:21.040
 whether it's at the large existential scale

1:18:21.040 --> 1:18:25.240
 or smaller short term scale of negative impact on society.

1:18:25.240 --> 1:18:27.160
 So you write about trustworthy AI,

1:18:27.160 --> 1:18:31.480
 how can we build AI systems that align with our values

1:18:31.480 --> 1:18:32.800
 that make for a better world

1:18:32.800 --> 1:18:35.000
 that we can interact with that we can trust?

1:18:35.000 --> 1:18:35.840
 The first thing we have to do

1:18:35.840 --> 1:18:38.240
 is to replace deep learning with deep understanding.

1:18:38.240 --> 1:18:42.440
 So you can't have alignment with a system

1:18:42.440 --> 1:18:44.600
 that traffics only in correlations

1:18:44.600 --> 1:18:47.880
 and doesn't understand concepts like bottles or harm.

1:18:47.880 --> 1:18:51.320
 So you, Asimov talked about these famous laws

1:18:51.320 --> 1:18:54.040
 and the first one was first do no harm.

1:18:54.040 --> 1:18:56.880
 And you can quibble about the details of Asimov's laws,

1:18:56.880 --> 1:18:58.800
 but we have to, if we're gonna build real robots

1:18:58.800 --> 1:19:00.560
 in the real world, have something like that.

1:19:00.560 --> 1:19:02.520
 That means we have to program in a notion

1:19:02.520 --> 1:19:04.240
 that's at least something like harm.

1:19:04.240 --> 1:19:06.600
 That means we have to have these more abstract ideas

1:19:06.600 --> 1:19:08.480
 that deep learning is not particularly good at.

1:19:08.480 --> 1:19:10.600
 They have to be in the mix somewhere.

1:19:10.600 --> 1:19:12.360
 And you could do statistical analysis

1:19:12.360 --> 1:19:14.360
 about probabilities of given harms or whatever,

1:19:14.360 --> 1:19:15.800
 but you have to know what a harm is

1:19:15.800 --> 1:19:17.400
 in the same way that you have to understand

1:19:17.400 --> 1:19:20.640
 that a bottle isn't just a collection of pixels.

1:19:20.640 --> 1:19:24.000
 And also be able to, you're implying

1:19:24.000 --> 1:19:26.840
 that you need to also be able to communicate that to humans.

1:19:26.840 --> 1:19:31.600
 So the AI systems would be able to prove to humans

1:19:31.600 --> 1:19:35.440
 that they understand that they know what harm means.

1:19:35.440 --> 1:19:37.360
 I might run it in the reverse direction,

1:19:37.360 --> 1:19:38.600
 but roughly speaking, I agree with you.

1:19:38.600 --> 1:19:43.360
 So we probably need to have committees of wise people,

1:19:43.360 --> 1:19:46.800
 ethicists and so forth, think about what these rules

1:19:46.800 --> 1:19:48.320
 ought to be and we should just leave it

1:19:48.320 --> 1:19:49.560
 to software engineers.

1:19:49.560 --> 1:19:51.600
 It shouldn't just be software engineers

1:19:51.600 --> 1:19:53.880
 and it shouldn't just be people

1:19:53.880 --> 1:19:58.280
 who own large mega corporations that are good at technology.

1:19:58.280 --> 1:20:00.240
 Ethicists and so forth should be involved,

1:20:00.240 --> 1:20:04.640
 but there should be some assembly of wise people

1:20:04.640 --> 1:20:07.200
 as I was putting it that tries to figure out

1:20:07.200 --> 1:20:08.680
 what the rules ought to be.

1:20:08.680 --> 1:20:11.560
 And those have to get translated into code.

1:20:12.440 --> 1:20:15.440
 You can argue or code or neural networks or something.

1:20:15.440 --> 1:20:18.640
 They have to be translated into something

1:20:18.640 --> 1:20:20.000
 that machines can work with.

1:20:20.000 --> 1:20:21.920
 And that means there has to be a way

1:20:21.920 --> 1:20:23.400
 of working the translation.

1:20:23.400 --> 1:20:24.480
 And right now we don't.

1:20:24.480 --> 1:20:25.360
 We don't have a way.

1:20:25.360 --> 1:20:27.080
 So let's say you and I were the committee

1:20:27.080 --> 1:20:29.880
 and we decide that Asimov's first law is actually right.

1:20:29.880 --> 1:20:31.640
 And let's say it's not just two white guys,

1:20:31.640 --> 1:20:32.880
 which would be kind of unfortunate

1:20:32.880 --> 1:20:34.040
 and then we have a broad.

1:20:34.040 --> 1:20:36.320
 And so we've represented a sample of the world

1:20:36.320 --> 1:20:37.560
 or however we want to do this.

1:20:37.560 --> 1:20:40.480
 And the committee decides eventually,

1:20:40.480 --> 1:20:42.840
 okay, Asimov's first law is actually pretty good.

1:20:42.840 --> 1:20:44.080
 There are these exceptions to it.

1:20:44.080 --> 1:20:46.080
 We want to program in these exceptions,

1:20:46.080 --> 1:20:47.520
 but let's start with just the first one

1:20:47.520 --> 1:20:48.920
 and then we'll get to the exceptions.

1:20:48.920 --> 1:20:50.680
 First one is first do no harm.

1:20:50.680 --> 1:20:53.320
 Well, somebody has to now actually turn that

1:20:53.320 --> 1:20:56.240
 into a computer program or a neural network or something.

1:20:56.240 --> 1:20:58.800
 And one way of taking the whole book,

1:20:58.800 --> 1:21:00.320
 the whole argument that I'm making

1:21:00.320 --> 1:21:02.520
 is that we just don't have to do that yet

1:21:02.520 --> 1:21:04.080
 and we're fooling ourselves if we think

1:21:04.080 --> 1:21:05.880
 that we can build trustworthy AI.

1:21:05.880 --> 1:21:09.560
 If we can't even specify in any kind of,

1:21:09.560 --> 1:21:10.680
 we can't do it in Python

1:21:10.680 --> 1:21:13.160
 and we can't do it in TensorFlow,

1:21:13.160 --> 1:21:14.440
 we're fooling ourselves and thinking

1:21:14.440 --> 1:21:15.840
 that we can make trustworthy AI

1:21:15.840 --> 1:21:18.800
 if we can't translate harm into something

1:21:18.800 --> 1:21:19.960
 that we can execute.

1:21:19.960 --> 1:21:22.880
 And if we can't, then we should be thinking really hard,

1:21:22.880 --> 1:21:24.680
 how could we ever do such a thing?

1:21:24.680 --> 1:21:26.560
 Because if we're going to use AI

1:21:26.560 --> 1:21:29.240
 in the ways that we want to use it to make job interviews

1:21:29.240 --> 1:21:31.120
 or to do surveillance,

1:21:31.120 --> 1:21:32.480
 not that I personally want to do that or whatever,

1:21:32.480 --> 1:21:33.800
 I mean, if we're going to use AI

1:21:33.800 --> 1:21:36.240
 in ways that have practical impact on people's lives

1:21:36.240 --> 1:21:41.240
 or medicine, it's got to be able to understand stuff like that.

1:21:41.240 --> 1:21:42.880
 So one of the things your book highlights

1:21:42.880 --> 1:21:47.440
 is that a lot of people in the deep learning community,

1:21:47.440 --> 1:21:50.240
 but also the general public, politicians,

1:21:50.240 --> 1:21:53.240
 just people in all general groups and walks of life

1:21:53.240 --> 1:21:57.360
 have a different levels of misunderstanding of AI.

1:21:57.360 --> 1:21:59.480
 So when you talk about committees,

1:21:59.480 --> 1:22:04.480
 what's your advice to our society?

1:22:05.640 --> 1:22:06.480
 How do we grow?

1:22:06.480 --> 1:22:09.080
 How do we learn about AI such that

1:22:09.080 --> 1:22:10.840
 such committees could emerge

1:22:10.840 --> 1:22:14.560
 where large groups of people could have a productive discourse

1:22:14.560 --> 1:22:17.840
 about how to build successful AI systems?

1:22:17.840 --> 1:22:19.680
 Part of the reason we wrote the book

1:22:19.680 --> 1:22:22.080
 was to try to inform those committees.

1:22:22.080 --> 1:22:23.560
 So part of the reason we wrote the book

1:22:23.560 --> 1:22:25.680
 was to inspire a future generation of students

1:22:25.680 --> 1:22:27.880
 to solve what we think are the important problems.

1:22:27.880 --> 1:22:29.920
 So a lot of the book is trying to pinpoint

1:22:29.920 --> 1:22:31.280
 what we think are the hard problems

1:22:31.280 --> 1:22:33.920
 where we think effort would most be rewarded.

1:22:33.920 --> 1:22:36.760
 And part of it is to try to train people

1:22:37.840 --> 1:22:41.040
 who talk about AI, but aren't experts in the field

1:22:41.040 --> 1:22:43.560
 to understand what's realistic and what's not.

1:22:43.560 --> 1:22:44.720
 One of my favorite parts in the book

1:22:44.720 --> 1:22:47.040
 is the six questions you should ask.

1:22:47.040 --> 1:22:48.440
 Anytime you read a media account,

1:22:48.440 --> 1:22:51.160
 so number one is if somebody talks about something,

1:22:51.160 --> 1:22:52.000
 look for the demo.

1:22:52.000 --> 1:22:54.200
 If there's no demo, don't believe it.

1:22:54.200 --> 1:22:55.360
 Like the demo that you can try.

1:22:55.360 --> 1:22:56.520
 If you can't try it at home,

1:22:56.520 --> 1:22:58.400
 maybe it doesn't really work that well yet.

1:22:58.400 --> 1:23:00.640
 So if we don't have this example in the book,

1:23:00.640 --> 1:23:04.160
 but if Sundar Pinchai says we have this thing

1:23:04.160 --> 1:23:08.440
 that allows it to sound like human beings in conversation,

1:23:08.440 --> 1:23:10.400
 you should ask, can I try it?

1:23:10.400 --> 1:23:11.880
 And you should ask how general it is.

1:23:11.880 --> 1:23:13.080
 And it turns out at that time,

1:23:13.080 --> 1:23:15.440
 I'm alluding to Google Duplex when it was announced,

1:23:15.440 --> 1:23:18.200
 it only worked on calling hairdressers,

1:23:18.200 --> 1:23:20.000
 restaurants, and finding opening hours.

1:23:20.000 --> 1:23:20.840
 That's not very general.

1:23:20.840 --> 1:23:22.240
 That's narrow AI.

1:23:22.240 --> 1:23:25.400
 And I'm not gonna ask your thoughts about Sophia, but yeah.

1:23:25.400 --> 1:23:28.040
 I understand that's a really good question to ask

1:23:28.040 --> 1:23:30.240
 of any kind of hype top idea.

1:23:30.240 --> 1:23:32.600
 So Sophia has very good material written for her,

1:23:32.600 --> 1:23:35.400
 but she doesn't understand the things that she's saying.

1:23:35.400 --> 1:23:38.240
 So a while ago, you've written a book

1:23:38.240 --> 1:23:40.560
 on the science of learning, which I think is fascinating,

1:23:40.560 --> 1:23:43.520
 but the learning case studies of playing guitar.

1:23:43.520 --> 1:23:44.360
 That's right.

1:23:44.360 --> 1:23:45.200
 All guitar zero.

1:23:45.200 --> 1:23:47.360
 I love guitar myself, I've been playing my whole life.

1:23:47.360 --> 1:23:50.240
 So let me ask a very important question.

1:23:50.240 --> 1:23:54.120
 What is your favorite song, rock song to listen to

1:23:54.120 --> 1:23:56.280
 or try to play?

1:23:56.280 --> 1:23:57.120
 Well, those would be different,

1:23:57.120 --> 1:23:59.640
 but I'll say that my favorite rock song to listen to

1:23:59.640 --> 1:24:01.080
 is probably all along the Watchtower,

1:24:01.080 --> 1:24:02.000
 the Jimi Hendrix version.

1:24:02.000 --> 1:24:03.000
 The Jimi Hendrix version.

1:24:03.000 --> 1:24:04.880
 It just feels magic to me.

1:24:04.880 --> 1:24:07.040
 I've actually recently learned that I love that song.

1:24:07.040 --> 1:24:09.360
 I've been trying to put it on YouTube myself singing.

1:24:09.360 --> 1:24:11.280
 Singing is the scary part.

1:24:11.280 --> 1:24:13.360
 If you could party with a rock star for a weekend,

1:24:13.360 --> 1:24:15.200
 living or dead, who would you choose?

1:24:17.760 --> 1:24:18.640
 And pick their mind,

1:24:18.640 --> 1:24:21.160
 it's not necessarily about the party.

1:24:21.160 --> 1:24:25.640
 Thanks for the clarification. I guess John Lennon

1:24:25.640 --> 1:24:29.640
 is such an intriguing person and I think a troubled person,

1:24:29.640 --> 1:24:31.240
 but an intriguing one.

1:24:31.240 --> 1:24:32.480
 So beautiful.

1:24:32.480 --> 1:24:35.480
 Well, Imagine is one of my favorite songs.

1:24:35.480 --> 1:24:37.120
 Also one of my favorite songs.

1:24:37.120 --> 1:24:38.320
 That's a beautiful way to end it.

1:24:38.320 --> 1:24:39.800
 Gary, thank you so much for talking to me.

1:24:39.800 --> 1:24:51.800
 Thanks so much for having me.

