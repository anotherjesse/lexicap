WEBVTT

00:00.000 --> 00:06.720
 The following is a conversation with Demis Hassabis, CEO and co founder of DeepMind,

00:06.720 --> 00:11.600
 a company that has published and built some of the most incredible artificial intelligence

00:11.600 --> 00:18.320
 systems in the history of computing, including AlphaZero that learned all by itself to play

00:18.320 --> 00:25.760
 the game of go better than any human in the world and AlphaFold 2 that solved protein folding.

00:25.760 --> 00:32.880
 Both tasks considered nearly impossible for a very long time. Demis is widely considered to be

00:32.880 --> 00:38.720
 one of the most brilliant and impactful humans in the history of artificial intelligence and

00:38.720 --> 00:45.360
 science and engineering in general. This was truly an honor and a pleasure for me to finally sit

00:45.360 --> 00:50.240
 down with him for this conversation and I'm sure we will talk many times again in the future.

00:50.240 --> 00:56.560
 This is the Lux Friedman podcast. To support it, please check out our sponsors in the description

00:56.560 --> 01:03.360
 and now, dear friends, here's Demis Hassabis. Let's start with a bit of a personal question.

01:03.920 --> 01:09.680
 Am I an AI program you wrote to interview people until I get good enough to interview you?

01:10.960 --> 01:14.640
 Well, I'd be impressed if you were. I'd be impressed with myself if you were.

01:14.640 --> 01:18.640
 I don't think we're quite up to that yet, but maybe you're from the future, Lux.

01:18.640 --> 01:23.520
 If you did, would you tell me? Is that a good thing to tell a language model that's tasked

01:23.520 --> 01:29.040
 with interviewing that it is, in fact, AI? Maybe we're in a kind of meta chewing test.

01:30.240 --> 01:33.760
 Probably it would be a good idea not to tell you, so it doesn't change your behavior, right?

01:33.760 --> 01:38.080
 This is a kind of... Heisenberg uncertainty principle situation. If I told you, you behaved

01:38.080 --> 01:40.800
 differently. Maybe that's what's happening with us, of course.

01:40.800 --> 01:48.960
 This is a benchmark from the future where they replay 2022 as a year before AIs were good enough

01:48.960 --> 01:52.640
 yet and now we want to see, is it going to pass? Exactly.

01:53.840 --> 01:57.280
 If I was such a program, would you be able to tell, do you think?

01:57.840 --> 02:05.680
 So to the touring test question, you've talked about the benchmark for solving intelligence.

02:05.680 --> 02:09.120
 What would be the impressive thing? You talked about winning a Nobel Prize and

02:09.120 --> 02:14.080
 ass system winning a Nobel Prize, but I still returned to the touring test as a compelling

02:14.080 --> 02:16.560
 test, the spirit of the touring test as a compelling test.

02:17.120 --> 02:20.800
 Yeah, the chewing test, of course, it's been unbelievably influential and chewing is one

02:20.800 --> 02:25.840
 of my all time heroes, but I think if you look back at the 1950 papers, original paper and read

02:25.840 --> 02:31.200
 the original, you'll see I don't think he meant it to be a rigorous formal test. I think it was

02:31.200 --> 02:35.360
 more like a thought experiment, almost a bit of philosophy he was writing if you look at the style

02:35.360 --> 02:40.080
 of the paper. And you can see he didn't specify it very rigorously. So for example, he didn't

02:40.080 --> 02:47.040
 specify the knowledge that the expert or judge would have, not how much time would they have to

02:47.040 --> 02:52.160
 investigate this. So these are important parameters if you were going to make it a true sort of

02:52.160 --> 02:59.600
 formal test. And by some measures, people claim the touring test passed several decades ago.

02:59.600 --> 03:06.240
 I remember someone claiming that with a kind of very bog standard normal logic model, because

03:06.240 --> 03:14.720
 they pretended it was a kid. So the judges thought that the machine was a child. So that would be

03:14.720 --> 03:20.080
 very different from an expert AI person interrogating a machine and knowing how it was built and so

03:20.080 --> 03:27.040
 on. So I think we should probably move away from that as a formal test and move more towards

03:27.040 --> 03:33.840
 a general test where we test the AI capabilities on a range of tasks and see if it reaches human

03:33.840 --> 03:39.040
 level or above performance on maybe thousands, perhaps even millions of tasks eventually,

03:39.040 --> 03:44.640
 and cover the entire sort of cognitive space. So I think for its time, it was an amazing

03:44.640 --> 03:49.280
 thought experiment. And also 1950s, obviously, there's barely the dawn of the computer age.

03:49.280 --> 03:53.840
 So of course, he only thought about text. And now we have a lot more different inputs.

03:53.840 --> 03:59.520
 So yeah, maybe the better thing to test is the generalizability. So across multiple tasks,

03:59.520 --> 04:06.560
 but I think it's also possible as as systems like God will show that eventually that might

04:06.560 --> 04:11.920
 map right back to language. So you might be able to demonstrate your ability to generalize across

04:11.920 --> 04:17.680
 tasks by then communicating your ability to generalize across tasks, which is kind of what

04:17.680 --> 04:22.560
 we do through conversation anyway, when we jump around. Ultimately, what's in there in that

04:22.560 --> 04:29.120
 conversation is not just you moving around knowledge. It's you moving around like these

04:29.120 --> 04:35.760
 entirely different modalities of understanding that ultimately map to your ability to

04:37.680 --> 04:41.840
 operate successfully in all of these domains, which you can think of as tasks.

04:42.480 --> 04:48.480
 Yeah, I think certainly we as humans use language as our main generalization communication tool. So

04:48.480 --> 04:54.800
 I think we end up thinking in language and expressing our solutions in language. So it's

04:54.800 --> 05:01.920
 going to be very powerful mode in which to explain the system to explain what it's doing.

05:02.880 --> 05:09.040
 But I don't think it's the only modality that matters. So I think there's going to be a lot

05:09.040 --> 05:14.480
 of, you know, there's a lot of different ways to express capabilities other than just language.

05:14.480 --> 05:18.480
 Yeah, visual, robotics, body language.

05:21.040 --> 05:24.640
 Yeah, actions, the interactive aspect of all that, that's all part of it.

05:24.640 --> 05:30.240
 But what's interesting with GATO is that it's sort of pushing prediction to the maximum in

05:30.240 --> 05:35.360
 terms of like mapping arbitrary sequences to other sequences and sort of just predicting

05:35.360 --> 05:40.000
 what's going to happen next. So prediction seems to be fundamental to intelligence.

05:40.000 --> 05:44.000
 And what you're predicting doesn't so much matter.

05:44.000 --> 05:48.160
 Yeah, it seems like you can generalize that quite well. So obviously language models predict the

05:48.160 --> 05:55.120
 next word. GATO predicts potentially any action or any token. And it's just the beginning really.

05:55.120 --> 06:00.320
 It's our most general agent one could call it so far. But, you know, that itself can be scaled

06:00.320 --> 06:04.160
 up massively more than we've done so far. And obviously we're in the middle of doing that.

06:04.160 --> 06:10.960
 But the big part of solving AGI is creating benchmarks that help us get closer and closer,

06:10.960 --> 06:16.240
 sort of creating benchmarks that test the generalizability. And it's just still interesting

06:16.240 --> 06:22.640
 that this fella, Alan Turing, was one of the first and probably still one of the only people that

06:22.640 --> 06:27.760
 was trying, maybe philosophically, but was trying to formulate a benchmark that could be followed.

06:27.760 --> 06:33.360
 It is, even though it's fuzzy, it's still sufficiently rigorous to where you can run that

06:33.360 --> 06:37.920
 test. And I still think something like the Turing test will, at the end of the day,

06:38.640 --> 06:44.080
 be the thing that truly impresses other humans so that you can have a close friend

06:44.640 --> 06:49.680
 who's an AI system. And for that friend to be a good friend, they're going to have to be able to

06:51.120 --> 06:57.680
 play StarCrafts. And they're going to have to do all of these tasks, get you a beer,

06:57.680 --> 07:05.120
 so the robotics tasks, play games with you, use language, humor, all those kinds of things. But

07:05.120 --> 07:10.960
 that ultimately can blow down to language. It feels like, not in terms of the AI community,

07:10.960 --> 07:15.600
 but in terms of the actual impact of general intelligence on the world, it feels like language

07:15.600 --> 07:21.200
 will be the place where it truly shines. I think so, because it's such an important kind of input

07:21.200 --> 07:27.280
 output for us. I think you're right. I think the Turing test, the philosophy behind it,

07:27.280 --> 07:35.600
 which is the idea of, can a machine mimic the behaviors of a human? And I would say,

07:35.600 --> 07:41.200
 wider than just language and text, then in terms of actions and everything else,

07:41.200 --> 07:47.280
 creativity, all these things, then if it can match or exceed human cognitive capabilities,

07:47.280 --> 07:52.480
 then I think we have a true intelligence. So from that perspective, you're right. I think he did

07:52.480 --> 07:58.240
 formulate the right kind of setup. I think there'll be a kind of humor

07:59.200 --> 08:05.040
 in the AI systems of the future, looking back to this conversation and thinking about the Turing

08:05.040 --> 08:12.000
 test and also thinking about, by that time, they would know which year they were finally able to

08:12.000 --> 08:18.080
 sort of cross the threshold of human level intelligence and think how funny it is that

08:18.080 --> 08:24.000
 we humans were still confused about this whole problem that's been long ago been solved.

08:24.640 --> 08:30.240
 Well, hello to the AI agents of the future. Anyway, so going back to your journey,

08:30.240 --> 08:36.080
 when did you fall in love with programming first? Well, it was pretty young age, actually. So

08:37.600 --> 08:42.960
 I started off, actually, games was my first love. So starting to play chess when I was around four

08:42.960 --> 08:48.560
 years old. And then it was actually with winnings from a chess competition that I managed to buy

08:48.560 --> 08:52.720
 my first chess computer when I was about eight years old was a ZX Spectrum, which was hugely

08:52.720 --> 08:58.480
 popular in the UK at the time. And it was an amazing machine because I think it trained a

08:58.480 --> 09:03.440
 whole generation of programmers in the UK because it was so accessible. You literally switched it

09:03.440 --> 09:08.640
 on and there was the basic prompt and you could just get going. And my parents didn't really

09:08.640 --> 09:12.480
 know anything about computers. But because it was my money from a chess competition,

09:12.480 --> 09:19.760
 I could say I wanted to buy it. And then I just went to bookstores, got books on programming,

09:19.760 --> 09:26.320
 and started typing in the programming code. And then, of course, once you start doing that,

09:26.320 --> 09:30.720
 you start adjusting it and then making your own games. And that's when I fell in love with computers

09:30.720 --> 09:36.400
 and realized that they were a very magical device. In a way, I don't want to have been able to

09:36.400 --> 09:40.800
 explain this at the time, but I felt that there was almost a magical extension of your mind.

09:40.800 --> 09:45.120
 I always had this feeling, and I've always loved this about computers, that you can set them off

09:45.120 --> 09:50.000
 doing something, some task for you, you can go to sleep, come back the next day and it's solved.

09:51.280 --> 09:56.560
 That feels magical to me. All machines do that to some extent. They all enhance our

09:56.560 --> 10:01.040
 natural capabilities. Obviously, cars make us allow us to move faster than we can run.

10:01.040 --> 10:08.400
 But this was a machine to extend the mind. And then, of course, AI is the ultimate expression

10:08.400 --> 10:14.400
 of what a machine may be able to do or learn. So, very naturally for me, that thought extended

10:14.400 --> 10:20.080
 into AI quite quickly. Do you remember the programming language that was first started?

10:20.080 --> 10:24.720
 Yeah. Was it special to the machine? No, it was just a basic. I think it was just basic

10:24.720 --> 10:28.960
 on the ZX Spectrum. I don't know what specific form it was. And then later on, I got a Commodore

10:28.960 --> 10:34.640
 Amiga, which was a fantastic machine. Now you're just showing off. So, yeah, well,

10:34.640 --> 10:38.800
 lots of my friends had Atari STs and I managed to get Amigas. It was a bit more powerful.

10:38.800 --> 10:46.160
 And that was incredible. And I used to do programming in Assembler and also AmosBasic,

10:46.160 --> 10:50.880
 this specific form of basic. It was incredible, actually. So, all my coding skills.

10:50.880 --> 10:56.720
 And when did you fall in love with AI? So, when did you first start to gain an understanding

10:56.720 --> 11:02.720
 that you can not just write programs that do some mathematical operations for you while you sleep,

11:02.720 --> 11:10.960
 but something that's akin to bringing an entity to life, sort of a thing that can

11:10.960 --> 11:15.760
 figure out something more complicated than a simple mathematical operation.

11:15.760 --> 11:20.320
 Yeah. So, there was a few stages for me all while I was very young. So, first of all,

11:20.320 --> 11:24.160
 as I was trying to improve at playing chess, I was captaining various England junior chess

11:24.160 --> 11:28.800
 teams. And at the time, when I was about maybe 10, 11 years old, I was going to become a professional

11:28.800 --> 11:35.120
 chess player. That was my first thought. So, that dream was there to try to get to the highest

11:35.120 --> 11:40.400
 levels of chess. Yeah. So, when I was about 12 years old, I got to master stand and I was

11:40.400 --> 11:44.080
 second highest rated player in the world to Judith Polgar, who obviously ended up being

11:44.080 --> 11:50.400
 an amazing chess player and a world women's champion. And when I was trying to improve at

11:50.400 --> 11:54.160
 chess, what you do is you obviously, first of all, you're trying to improve your own thinking

11:54.160 --> 11:59.600
 processes. So, that leads you to thinking about thinking. How is your brain coming up with these

11:59.600 --> 12:05.040
 ideas? Why is it making mistakes? How can you improve that thought process? But the second

12:05.040 --> 12:10.400
 thing is that you, it was just the beginning, this was like in the early 80s, mid 80s of

12:10.400 --> 12:13.600
 chess computers. If you remember, they were physical balls like the one we have in front

12:13.600 --> 12:20.960
 of us and you press down the squares. And I think Kasparov had a branded version of it that I got.

12:20.960 --> 12:26.240
 And you were used to, they're not as strong as they are today, but they were pretty strong

12:26.240 --> 12:31.360
 and used to practice against them to try and improve your openings and other things.

12:31.360 --> 12:35.360
 And so, I remember, I think I probably got my first one, I was around 11 or 12. And I remember

12:35.360 --> 12:41.920
 thinking, this is amazing, you know, how someone programmed this chess board to play chess.

12:42.880 --> 12:47.600
 And it was very formative book I bought, which was called The Chess Computer Handbook

12:47.600 --> 12:52.240
 by David Levy. It came out in 1984 or something. So I must have got it when I was about 11, 12.

12:52.240 --> 12:57.600
 And it explained fully how these chess programs were made. And I remember my first AI program

12:57.600 --> 13:03.520
 being programming my Amiga. It wasn't powerful enough to play chess. I couldn't write a whole

13:03.520 --> 13:07.920
 chess program, but I wrote a program for it to play Othello, or reverse it, sometimes called,

13:07.920 --> 13:13.120
 I think, in the US. And so a slightly simpler game than chess. But I used all of the principles

13:13.120 --> 13:17.200
 that chess programs had, alpha, beta, search, all of that. And that was my first AI program.

13:17.200 --> 13:21.520
 I remember that very well. I was around 12 years old. So that brought me into AI.

13:21.520 --> 13:27.200
 And then the second part was later on, around 16, 17, and I was writing games professionally,

13:27.200 --> 13:33.920
 designing games, writing a game called Theme Park, which had AI as a core gameplay component

13:33.920 --> 13:38.320
 as part of the simulation. And it sold, you know, millions of copies around the world.

13:38.320 --> 13:43.440
 And people loved the way that the AI, even though it was relatively simple by today's AI standards,

13:43.440 --> 13:49.040
 was reacting to the way you, as the player, played it. So it was called a sandbox game.

13:49.040 --> 13:53.440
 So it was one of the first types of games like that, along with SimCity. And it meant that

13:53.440 --> 13:58.160
 every game you played was unique. Is there something you could say, just on a small tangent,

13:58.800 --> 14:05.760
 about really impressive AI from a game design, human enjoyment perspective,

14:06.560 --> 14:12.320
 really impressive AI that you've seen in games? And maybe what does it take to create AI system?

14:12.320 --> 14:17.520
 And how hard of a problem is that? So a million questions, just as a brief tangent.

14:18.240 --> 14:24.320
 Well, look, I think games have been significant in my life for three reasons. So first of all,

14:25.200 --> 14:30.320
 I was playing them and training myself on games when I was a kid. Then I went through a phase of

14:30.320 --> 14:34.800
 designing games and writing AI for games. So all the games I professionally wrote

14:35.840 --> 14:42.080
 had AI as a core component. And that was mostly in the 90s. And the reason I was doing that in

14:42.080 --> 14:47.040
 games industry was at the time, the games industry, I think, was the cutting edge of technology.

14:47.040 --> 14:51.600
 So whether it was graphics with people like John Carmack and Quake and those kind of things,

14:51.600 --> 14:58.080
 or AI, I think actually all the action was going on in games. And we're still reaping the benefits

14:58.080 --> 15:02.720
 of that, even with things like GPUs, which I find ironic was obviously invented for graphics,

15:02.720 --> 15:06.800
 computer graphics, but then turns out to be amazingly useful for AI. It just turns out

15:06.800 --> 15:13.920
 everything's a matrix multiplication. It appears in the whole world. So I think games at the time

15:13.920 --> 15:20.000
 had the most cutting edge AI. And a lot of the games, I was involved in writing. So there was

15:20.000 --> 15:23.840
 a game called Black and White, which was one game I was involved with in the early stages of,

15:23.840 --> 15:30.480
 which I still think is the most impressive example of reinforcement learning in a computer game.

15:30.480 --> 15:35.200
 So in that game, you trained a little pet animal. It's a brilliant game.

15:35.200 --> 15:39.040
 Yeah. And it sort of learned from how you were treating it. So if you treated it badly,

15:39.040 --> 15:43.920
 then it became mean. And then it would be mean to your villagers and your population,

15:43.920 --> 15:49.200
 the little tribe that you were running. But if you were kind to it, then it would be kind.

15:49.200 --> 15:52.880
 And people were fascinated by how that worked. And so as I had to be honest with the way it

15:52.880 --> 15:58.880
 kind of developed. Especially the mapping to good and evil. It made you realize,

15:58.880 --> 16:06.560
 made me realize that you can sort of in the way in the choices you make can define where you

16:06.560 --> 16:14.800
 end up. And that means all of us are capable of the good evil. It all matters in the different

16:14.800 --> 16:19.520
 choices along the trajectory to those places that you make. It's fascinating. I mean,

16:19.520 --> 16:22.400
 games can do that philosophically to you. And it's rare. It seems rare.

16:22.400 --> 16:26.960
 Yeah. Well, games I think are unique medium because you as the player, you're not just

16:26.960 --> 16:34.160
 passively consuming the entertainment, right? You're actually actively involved as an agent.

16:34.160 --> 16:38.720
 So I think that's what makes it in some ways can be more visceral than other mediums like

16:38.720 --> 16:43.440
 films and books. So the second, so that was designing AI in games. And then the third use

16:45.360 --> 16:51.520
 we've used of AI is indeed mind from the beginning, which is using games as a testing ground for

16:51.520 --> 16:58.320
 proving out AI algorithms and developing AI algorithms. And that was a sort of a core component

16:58.320 --> 17:04.240
 of our vision at the start of DeepMind was that we would use games very heavily as our main testing

17:04.240 --> 17:09.920
 ground, certainly to begin with, because it's super efficient to use games. And also, it's very

17:09.920 --> 17:15.360
 easy to have metrics to see how well your systems are improving and what direction your ideas are

17:15.360 --> 17:19.920
 going in and whether you're making incremental improvements. And because those games are often

17:19.920 --> 17:26.000
 rooted in something that humans did for a long time beforehand, there's already a strong set of

17:26.000 --> 17:30.400
 rules like it's already a damn good benchmark. Yes, it's really good for so many reasons because

17:30.400 --> 17:36.720
 you've got clear measures of how good humans can be at these things. And in some cases like Go,

17:36.720 --> 17:43.200
 we've been playing it for thousands of years. And often they have scores or at least win conditions.

17:43.200 --> 17:47.440
 So it's very easy for reward learning systems to get a reward. It's very easy to specify what

17:47.440 --> 17:55.920
 that reward is. And also at the end, it's easy to test externally how strong is your system

17:55.920 --> 18:01.600
 by, of course, playing against the world's strongest players at those games. So it's so good

18:01.600 --> 18:06.480
 for so many reasons. And it's also very efficient to run potentially millions of simulations

18:06.480 --> 18:13.360
 in parallel on the cloud. So I think there's a huge reason why we were so successful back in

18:13.360 --> 18:18.800
 the starting out 2010, how come we were able to progress so quickly because we'd utilize games.

18:18.800 --> 18:25.280
 And at the beginning of DeepMind, we also hired some amazing game engineers who I knew from my

18:25.280 --> 18:30.800
 previous lives in the games industry. And that helped to bootstrap us very quickly.

18:30.800 --> 18:37.120
 And plus it's somehow super compelling, almost at a philosophical level of man versus machine

18:37.120 --> 18:43.520
 over over a chessboard or a Go board. And especially given that the entire history of AI

18:43.520 --> 18:49.200
 is defined by people saying it's going to be impossible to make a machine that beats a human

18:49.200 --> 18:55.760
 being in chess. And then once that happened, people were certain when I was coming up in AI,

18:55.760 --> 19:01.680
 that Go is not a game that can be solved because of the combinatorial complexity. It's just too,

19:01.680 --> 19:08.400
 it's, you know, no matter how much Moore's law you have, compute is just never going to be able

19:08.400 --> 19:15.360
 to crack the game of Go. And so then there's something compelling about facing sort of

19:15.360 --> 19:22.320
 taking on the impossibility of that task from the AI researcher perspective,

19:22.320 --> 19:25.600
 engineer perspective. And then as a human being just observing this whole thing,

19:25.600 --> 19:36.960
 your beliefs about what you thought was impossible being broken apart. It's humbling

19:37.520 --> 19:42.720
 to realize we're not as smart as we thought. It's humbling to realize that the things we

19:42.720 --> 19:49.360
 think are impossible now perhaps will be done in the future. There's something really powerful

19:49.360 --> 19:55.920
 about a game, AI system being a human being in a game that drives that message home for like

19:55.920 --> 20:01.520
 millions, billions of people, especially in the case of Go. Sure. Well, look, I think it's,

20:01.520 --> 20:07.360
 I mean, it has been a fascinating journey. And especially as I think about it from, I can

20:07.360 --> 20:14.640
 understand it from both sides, both as the AI creators of the AI, but also as a games player

20:14.640 --> 20:20.960
 originally. So it was a really interesting, I mean, it was a fantastic, but also somewhat

20:20.960 --> 20:27.520
 bittersweet moment, the AlphaGo match for me seeing that and being obviously heavily

20:27.520 --> 20:34.240
 involved in that. But as you say, Chess has been the, I mean, Kasparov, I think rightly called it

20:34.240 --> 20:40.160
 the Drosophila of intelligence, right? So it's sort of, I love that phrase. And I think he's

20:40.160 --> 20:47.360
 right because Chess has been hand in hand with AI from the beginning of the whole field, right?

20:47.360 --> 20:52.320
 So I think every AI practitioner starting with Turing and Claude Shannon and all those,

20:52.320 --> 20:58.960
 the sort of forefathers of the field, tried their hand at writing a chess program. I've got

20:58.960 --> 21:04.560
 an original edition of Claude Shannon's first chess program. I think it was 1949, the original

21:04.560 --> 21:11.360
 sort of paper. And they all did that and Turing famously wrote a chess program that all the

21:11.360 --> 21:15.040
 computers around them were obviously too slow to run it. So he had to run, he had to be the

21:15.040 --> 21:20.320
 computer, right? So he literally, I think spent two or three days running his own program by hand

21:20.320 --> 21:26.240
 with pencil and paper and playing a friend of his with his chess program. So of course,

21:26.240 --> 21:32.560
 Deep Blue was a huge moment beating Kasparov. But actually, when that happened, I remember that very,

21:32.560 --> 21:37.520
 very vividly, of course, because it was Chess and computers and AI, all the things I loved. And I

21:37.520 --> 21:42.320
 was at college at the time. But I remember coming away from that, being more impressed by Kasparov's

21:42.320 --> 21:48.160
 mind than I was by Deep Blue. Because here was Kasparov with his human mind, not only could he

21:48.160 --> 21:53.680
 play chess more or less to the same level as this brute of a calculation machine. But of course,

21:53.680 --> 21:58.000
 Kasparov can do everything else humans can do, ride a bike, talk many languages, do politics,

21:58.000 --> 22:03.760
 all the rest of the amazing things that Kasparov does. And so with the same brain. And yet Deep

22:03.760 --> 22:12.480
 Blue, brilliant as it was at chess, it'd been hand coded for chess and actually had distilled

22:12.480 --> 22:17.600
 the knowledge of chess grandmasters into a cool program. But it couldn't do anything else.

22:17.600 --> 22:23.440
 Like it couldn't even play a strictly simpler game like Tic Tac Toe. So something to me was missing

22:23.440 --> 22:29.200
 from intelligence from that system that we would regard as intelligence. And I think it was this

22:29.200 --> 22:36.000
 idea of generality and also learning. So that's what we tried to do with AlphaGo.

22:36.000 --> 22:42.560
 Yeah, with AlphaGo and AlphaZero, MuZero, and then God on all the things that we'll get into some

22:42.560 --> 22:47.920
 parts of there's just a fascinating trajectory here. But let's just stick on chess briefly.

22:47.920 --> 22:53.760
 On the human side of chess, you've proposed that from a game design perspective, the thing that

22:53.760 --> 23:02.160
 makes chess compelling as a game is that there's a creative tension between a bishop and the knight.

23:02.800 --> 23:07.360
 Can you explain this? First off, it's really interesting to think about what makes a game

23:07.360 --> 23:13.440
 compelling. It makes it stick across centuries. Yeah, I was sort of thinking about this. And

23:13.440 --> 23:17.360
 actually a lot of even amazing chess players don't think about it necessarily from a game's

23:17.360 --> 23:21.120
 designer point of view. So it's with my game design hat on that I was thinking about this.

23:21.120 --> 23:28.240
 Why is chess so compelling? And I think a critical reason is the dynamicness of the

23:28.240 --> 23:32.080
 different kind of chess positions you can have, whether they're closed or open and other things

23:32.080 --> 23:38.160
 comes from the bishop and the knight. So if you think about how different the capabilities of

23:38.160 --> 23:42.960
 the bishop and knight are in terms of the way they move, and then somehow chess has evolved

23:42.960 --> 23:47.520
 to balance those two capabilities more or less equally. So they're both roughly worth three

23:47.520 --> 23:51.520
 points each. So you think that dynamics is always there, and then the rest of the rules

23:51.520 --> 23:55.360
 are kind of trying to stabilize the game? Well, maybe. I mean, it's sort of, I don't know if

23:55.360 --> 23:59.120
 chicken and egg situation probably both came together. But the fact that it's got to this

23:59.120 --> 24:03.520
 beautiful equilibrium where you can have the bishop and knight, they're so different in power,

24:04.320 --> 24:08.400
 but so equal in value across the set of the universe of all positions,

24:08.400 --> 24:14.800
 right? Somehow they've been balanced by humanity over hundreds of years. I think gives the game

24:14.800 --> 24:20.000
 the creative tension that you can swap the bishop and knights for a bishop for a knight,

24:20.000 --> 24:23.840
 and they're more or less the worth the same. But now you aim for a different type of position.

24:23.840 --> 24:27.920
 If you have the knight, you want a closed position. If you have the bishop, you want an open position.

24:27.920 --> 24:30.800
 So I think that creates a lot of the creative tension in chess.

24:30.800 --> 24:37.040
 So some kind of controlled creative tension. From an AI perspective, do you think AI systems

24:37.040 --> 24:40.480
 convention design games that are optimally compelling to humans?

24:41.360 --> 24:46.960
 Well, that's an interesting question. Sometimes I get asked about AI and creativity, and the way

24:46.960 --> 24:51.040
 I answered that is relevant to that question, which is that I think they're different levels

24:51.040 --> 24:56.080
 of creativity, one could say. So I think if we define creativity as coming up with something

24:56.080 --> 25:02.080
 original that's useful for a purpose, then I think the kind of lowest level of creativity

25:02.080 --> 25:07.440
 is like an interpolation. So an averaging of all the examples you see. So maybe a very basic AI

25:07.440 --> 25:11.920
 system could say you could have that. So you show it millions of pictures of cats, and then you say,

25:11.920 --> 25:17.040
 give me an average looking cat, generate me an average looking cat. I would call that interpolation.

25:17.040 --> 25:22.720
 Then there's extrapolation, which something like AlphaGo showed. So AlphaGo played millions of games

25:22.720 --> 25:28.080
 of go against itself. And then it came up with brilliant new ideas like move 37 in game two,

25:28.080 --> 25:33.840
 brilliant motif strategies in go that no humans had ever thought of, even though we've played it

25:33.840 --> 25:38.720
 for thousands of years and professionally for hundreds of years. So that I call that extrapolation.

25:38.720 --> 25:44.400
 But then there's still a level above that, which is you could call out of the box thinking or true

25:44.400 --> 25:49.280
 innovation, which is could you invent go? Could you invent chess and not just come up with a

25:49.280 --> 25:54.400
 brilliant chess move or brilliant go move, but can you actually invent chess or something as good

25:54.400 --> 26:01.440
 as chess or go? And I think one day AI could, but what's missing is how would you even specify

26:01.440 --> 26:07.440
 that task to a program right now? And the way I would do it, if I was telling a human to do it,

26:07.440 --> 26:11.760
 or a game designer, a human game designer to do it, is I would say something like go, I would say,

26:13.200 --> 26:17.280
 come up with a game that only takes five minutes to learn, which go does because it's got simple

26:17.280 --> 26:22.160
 rules, but many lifetimes to master, right, or impossible to master in one lifetime because

26:22.160 --> 26:28.960
 it's so deep and so complex. And then it's aesthetically beautiful. And also, it can be

26:28.960 --> 26:35.840
 completed in three or four hours of gameplay time, which is useful for us in a human day.

26:35.840 --> 26:41.200
 And so you might specify these sort of high level concepts like that. And then with that,

26:41.200 --> 26:47.440
 and maybe a few other things, one could imagine that go satisfies those constraints.

26:47.440 --> 26:52.960
 But the problem is, is that we're not able to specify abstract notions like that,

26:52.960 --> 26:57.840
 high level abstract notions like that yet, to our AI systems. And I think there's still

26:57.840 --> 27:02.880
 something missing there in terms of high level concepts or abstractions that they truly understand

27:02.880 --> 27:09.760
 and they're combinable and compositional. So for the moment, I think AI is capable of doing

27:09.760 --> 27:13.040
 interpolation and extrapolation, but not true invention.

27:13.040 --> 27:20.320
 So coming up with rule sets and optimizing with complicated objectives around those rule sets,

27:20.320 --> 27:26.800
 we can't currently do. But you could take a specific rule set, and then run a kind of

27:26.800 --> 27:33.520
 self play experiment to see how long, just observe how an AI system from scratch learns,

27:33.520 --> 27:39.120
 how long is that journey of learning. And maybe if it satisfies some of those other things you

27:39.120 --> 27:44.960
 mentioned, in terms of quickness to learn and so on, and you could see a long journey to master for

27:44.960 --> 27:50.880
 even an AI system, then you could say that this is a promising game. But it would be nice to do

27:50.880 --> 27:58.800
 almost like alpha codes or programming rules. So generating rules that automate even that part

27:58.800 --> 28:03.280
 of the generation of rules. So I have thought about systems actually that I think would be

28:03.280 --> 28:09.840
 amazing for a games designer. If you could have a system that takes your game, plays it tens of

28:09.840 --> 28:15.840
 millions of times, maybe overnight, and then self balances the rules better. So it tweaks the rules

28:15.840 --> 28:23.680
 and maybe the equations and the parameters so that the game is more balanced, the units in the game,

28:23.680 --> 28:29.520
 or some of the rules could be tweaked. So it's a bit of like giving a base set and then allowing

28:29.520 --> 28:34.560
 Monte Carlo tree search or something like that to sort of explore it. And I think that would be

28:34.560 --> 28:41.520
 super, super powerful tool actually for balancing, auto balancing a game, which usually takes thousands

28:41.520 --> 28:47.520
 of hours from hundreds of human games testers normally to balance one game like StarCraft,

28:47.520 --> 28:51.360
 which is, you know, Blizzard are amazing at balancing their games, but it takes them years

28:51.360 --> 28:56.640
 and years and years. So one could imagine at some point when this stuff becomes efficient

28:56.640 --> 29:02.640
 enough to, you know, you might better do that overnight. Do you think a game that is optimal,

29:02.640 --> 29:07.920
 designed by an AI system, would look very much like Planet Earth?

29:09.520 --> 29:15.040
 Maybe, maybe it's only the sort of game I would love to make is, and I've tried, you know, in my

29:15.040 --> 29:20.160
 games career, the games design career, you know, my first big game was designing a theme park,

29:20.160 --> 29:25.600
 an amusement park. Then with games like Republic, I tried to, you know, have games where we designed

29:25.600 --> 29:30.880
 whole cities and allowed you to play in. So, and of course, people like Will Wright have written

29:30.880 --> 29:36.000
 games like SimEarth, trying to simulate the whole of Earth, pretty tricky. But I think...

29:36.000 --> 29:40.160
 SimEarth, I haven't actually played that one. So what is it? Does it incorporate an evolution?

29:40.160 --> 29:45.200
 Yeah, it has evolution. And it sort of, it tries to, it sort of treats it as an entire biosphere,

29:45.200 --> 29:47.440
 but from quite high level. So...

29:47.440 --> 29:51.200
 It'd be nice to be able to sort of zoom in, zoom out and zoom in.

29:51.200 --> 29:54.800
 Exactly. So obviously it couldn't do that. That was in the night. I think he wrote that in the 90s.

29:54.800 --> 29:59.120
 So it couldn't, you know, it wasn't able to do that. But that would be, obviously,

29:59.120 --> 30:00.800
 the ultimate sandbox game, of course.

30:01.360 --> 30:03.600
 On that topic, do you think we're living in a simulation?

30:04.720 --> 30:06.320
 Yes. Well, so, okay, so...

30:06.320 --> 30:09.760
 We're going to jump around from the absurdly philosophical to the technical.

30:09.760 --> 30:14.800
 Sure, sure. Very, very happy to. So I think my answer to that question is a little bit complex,

30:14.800 --> 30:20.320
 because there is simulation theory, which obviously Nick Bostrom, I think, famously first proposed.

30:20.320 --> 30:28.880
 And I don't quite believe it in that sense. So in the sense that are we in some sort of computer

30:28.880 --> 30:36.960
 game, or have our descendants somehow recreated Earth in the 21st century, and for some kind

30:36.960 --> 30:44.080
 of experimental reason? I think that, but I do think that we might be, that the best way to

30:44.080 --> 30:50.480
 understand physics and the universe is from a computational perspective. So understanding it

30:50.480 --> 30:58.240
 as an information universe and actually information being the most fundamental unit of reality,

30:58.240 --> 31:02.560
 rather than matter or energy. So physicists would say, you know, matter or energy, you know,

31:02.560 --> 31:07.200
 E equals MC squared, these are the things that are the fundamentals of the universe.

31:07.200 --> 31:13.200
 I'd actually say information, which of course itself can be, can specify energy or matter,

31:13.200 --> 31:17.760
 right? Matter is actually just, you know, we're just out the way our bodies and the molecules

31:17.760 --> 31:23.120
 in our body arrange is information. So I think information may be the most fundamental way to

31:23.120 --> 31:28.800
 describe the universe. And therefore, you could say we're in some sort of simulation because of that.

31:29.680 --> 31:34.720
 But I don't, I do, I'm not really subscribed, but to the idea that, you know, these are sort

31:34.720 --> 31:40.080
 of throw away billions of simulations around. I think this is actually very critical and possibly

31:40.080 --> 31:46.320
 unique this simulation. This particular one. Yes. But and you just mean treating the universe

31:46.960 --> 31:53.840
 as a computer that's processing and modifying information is a good way to solve the problems

31:53.840 --> 32:00.640
 of physics, of chemistry, of biology, and perhaps of humanity and so on. Yes. I think

32:00.640 --> 32:07.760
 understanding physics in terms of information theory might be the best way to really understand

32:07.760 --> 32:14.480
 what's going on here. From our understanding of a universal Turing machine, from our understanding

32:14.480 --> 32:19.520
 of a computer, do you think there's something outside of the capabilities of a computer that

32:19.520 --> 32:25.440
 is present in our universe? You have a disagreement with Roger Penrose about the nature of consciousness.

32:25.440 --> 32:31.200
 He thinks that consciousness is more than just a computation. Do you think all of it,

32:31.200 --> 32:35.680
 the whole shebangs can be a computation? Yeah, I've had many fascinating debates

32:35.680 --> 32:43.840
 with Roger Penrose. And obviously, he's famously, and I read Emperor's New Mind and his books,

32:43.840 --> 32:49.360
 his classical books, and they were pretty influential in the 90s. And he believes that

32:49.360 --> 32:54.640
 there's something more, something quantum that is needed to explain consciousness in the brain.

32:55.680 --> 32:59.680
 I think about what we're doing actually at DeepMind and what my career is being,

32:59.680 --> 33:05.120
 we're almost like Turing's champion. So we are pushing Turing machines or classical computation

33:05.120 --> 33:08.800
 to the limits. What are the limits of what classical computing can do?

33:10.320 --> 33:15.680
 And at the same time, I've also studied neuroscience to see, and that's why I did my PhD in, was to

33:15.680 --> 33:20.560
 see also to look at, is there anything quantum in the brain from a neuroscience or biological

33:20.560 --> 33:26.240
 perspective? And so far, I think most neuroscientists and most mainstream biologists and neuroscientists

33:26.240 --> 33:31.120
 would say there's no evidence of any quantum systems or effects in the brain. As far as we

33:31.120 --> 33:39.120
 can see, it can be mostly explained by classical theories. And then so there's sort of the search

33:39.120 --> 33:44.960
 from the biology side. And then at the same time, there's the raising of the water at the bar from

33:44.960 --> 33:53.600
 what classical Turing machines can do and including our new AI systems. And as you alluded to earlier,

33:53.600 --> 34:00.480
 I think AI, especially in the last decade plus, has been a continual story now of surprising

34:01.200 --> 34:06.160
 events and surprising successes, knocking over one theory after another of what was

34:06.160 --> 34:13.120
 thought to be impossible from go to protein folding and so on. And so I think I would

34:13.120 --> 34:21.600
 be very hesitant to bet against how far the universal Turing machine and classical computation

34:21.600 --> 34:28.240
 paradigm can go. And my betting would be that all of certainly what's going on in our brain

34:28.960 --> 34:35.920
 can probably be mimicked or approximated on a classical machine, not requiring

34:35.920 --> 34:41.040
 something metaphysical or quantum. And we'll get there with some of the work with AlphaFold,

34:41.680 --> 34:47.280
 which I think begins the journey of modeling this beautiful and complex world of biology.

34:47.280 --> 34:53.040
 So you think all the magic of the human mind comes from this, just a few pounds of mush,

34:54.080 --> 35:00.080
 a biological computational mush that's akin to some of the neural networks,

35:01.440 --> 35:05.520
 not directly but in spirit that DeepMind has been working with.

35:05.520 --> 35:09.840
 Well, look, I think it's, you say it's a few, you know, of course, this is the, I think,

35:09.840 --> 35:15.360
 the biggest miracle of the universe is that it's just a few pounds of mush in our skulls. And yet

35:15.360 --> 35:20.160
 it's also our brains are the most complex objects that we know of in the universe.

35:20.160 --> 35:26.800
 So there's something profoundly beautiful and amazing about our brains. And I think that it's

35:26.800 --> 35:35.440
 an incredibly, incredible efficient machine. And it's, you know, a phenomenon basically.

35:35.440 --> 35:40.320
 And I think that building AI, one of the reasons I want to build AI, and I've always wanted to, is

35:40.320 --> 35:45.280
 I think by building an intelligent artifact like AI, and then comparing it to the human

35:45.280 --> 35:51.520
 mind, that will help us unlock the uniqueness and the true secrets of the mind that we've always

35:51.520 --> 35:56.400
 wondered about since the dawn of history, like consciousness, dreaming, creativity,

35:58.160 --> 36:04.080
 emotions, what are all these things, right? We've wondered about them since the dawn of humanity.

36:04.080 --> 36:08.720
 And I think one of the reasons, and you know, I love philosophy and philosophy of mind is

36:08.720 --> 36:13.360
 we found it difficult is there haven't been the tools for us to really other than introspection

36:13.360 --> 36:18.320
 to from very clever people in history, very clever philosophers to really investigate this

36:18.320 --> 36:22.320
 scientifically. But now, suddenly, we have a plethora of tools. Firstly, we have all the

36:22.320 --> 36:26.800
 neuroscience tools, fMRI machines, single cell recording, all of this stuff. But we also have

36:26.800 --> 36:35.120
 the ability computers and AI to build intelligent systems. So I think that, you know, I think it

36:35.120 --> 36:42.800
 is amazing what the human mind does. And I'm kind of in awe of it really. And I think it's amazing

36:42.800 --> 36:48.160
 that with our human minds, we're able to build things like computers and actually even, you know,

36:48.160 --> 36:52.720
 think and investigate about these questions. I think that's also a testament to the human mind.

36:52.720 --> 36:59.520
 Yeah, the universe built the human mind that now is building computers that help us understand

36:59.520 --> 37:03.520
 both the universe and our own human mind. That's right. It's exactly it. I mean, I think that's one,

37:03.520 --> 37:08.640
 you know, one could say we are maybe we're the mechanism by which the universe is going to try

37:08.640 --> 37:16.080
 to understand itself. Yeah, it's beautiful. So let's let's go to the basic building blocks of

37:16.080 --> 37:21.280
 biology that I think is another angle at which you can start to understand the human mind,

37:21.280 --> 37:26.560
 the human body, which is quite fascinating, which is from the basic building blocks,

37:26.560 --> 37:31.440
 start to simulate, start to model how from those building blocks, you can construct bigger and

37:31.440 --> 37:37.920
 bigger, more complex systems, maybe one day, the entirety of the human biology. So here's another

37:37.920 --> 37:44.000
 problem that thought to be impossible to solve, which is protein folding and alpha fold or

37:45.200 --> 37:51.200
 specific alpha fold to did just that. It's solved protein folding. I think it's one of the biggest

37:51.200 --> 37:57.680
 breakthroughs, certainly in the history of structural biology, but in general, in science,

38:00.080 --> 38:06.800
 maybe from a high level, what is it and how does it work? And then we can ask some fascinating

38:06.800 --> 38:13.840
 questions after. Sure. So maybe to explain it to people not familiar with protein folding is,

38:13.840 --> 38:18.720
 you know, first of all, explain proteins, which is, you know, proteins are essential to all life.

38:18.720 --> 38:23.760
 Every function in your body depends on proteins. Sometimes they're called the workhorses of biology.

38:23.760 --> 38:27.120
 And if you look into them, and I've, you know, obviously, as part of alpha fold, I've been

38:27.120 --> 38:33.040
 researching proteins and structural biology for the last few years, you know, they're amazing little

38:33.040 --> 38:37.040
 bio nanomachines proteins. They're incredible if you actually watch little videos of how they work,

38:37.040 --> 38:42.800
 animations of how they work. And proteins are specified by their genetic sequence called the

38:42.800 --> 38:48.080
 amino acid sequence. So you can think of it as their genetic makeup. And then in the body,

38:48.960 --> 38:54.080
 in nature, they, when they, when they fold up into a 3d structure, so you can think of it as a

38:54.080 --> 38:59.280
 string of beads, and then they fold up into a ball. Now the key thing is you want to know what that

38:59.280 --> 39:05.920
 3d structure is, because the structure, the 3d structure of a protein is what helps to determine

39:05.920 --> 39:11.280
 what does it do, the function it does in your body. And also, if you're interested in drug drugs or

39:11.280 --> 39:15.920
 disease, you need to understand that 3d structure, because if you want to target something with a

39:15.920 --> 39:21.280
 drug compound, about to block something the proteins doing, you need to understand where it's

39:21.280 --> 39:25.360
 going to bind on the surface of the protein. So obviously, in order to do that, you need to

39:25.360 --> 39:29.200
 understand the 3d structure. So the structure is mapped to the function? The structure is mapped to

39:29.200 --> 39:34.720
 the function. And the structure is obviously somehow specified by the, by the amino acid sequence.

39:34.720 --> 39:39.040
 And that's the, in essence, the protein folding problem is, can you just from the amino acid

39:39.040 --> 39:45.440
 sequence, the one dimensional string of letters, can you immediately computationally predict

39:45.440 --> 39:51.360
 the 3d structure? Right. And this has been a grand challenge in biology for over 50 years.

39:51.360 --> 39:56.320
 So I think it was first articulated by Christian Anfinsen, a Nobel Prize winner in 1972,

39:56.880 --> 40:01.760
 as part of his Nobel Prize winning lecture. And he just speculated, this should be possible

40:01.760 --> 40:06.160
 to go from the amino acid sequence to the 3d structure. But he didn't say how. So it was,

40:07.040 --> 40:11.920
 you know, it's been described to me as equivalent to Fermat's last theorem, but for biology.

40:11.920 --> 40:16.400
 Right. You should, as somebody that very well might win the Nobel Prize in the future,

40:16.400 --> 40:20.480
 but outside of that, you should do more of that kind of thing. In the margin,

40:20.480 --> 40:24.320
 just put random things. That will take like 200 years to solve.

40:24.320 --> 40:25.840
 Set people off for 200 years.

40:25.840 --> 40:26.880
 It should be possible.

40:26.880 --> 40:27.600
 Exactly.

40:27.600 --> 40:28.800
 And just don't give any details.

40:28.800 --> 40:33.360
 Exactly. I think everyone's, exactly. It should be, I'll have to remember that for future.

40:33.360 --> 40:37.200
 So yeah. So he set off, you know, with this one throwaway remark, just like Fermat, you know,

40:37.200 --> 40:44.240
 he, he set off this whole 50 years field, really, of computation of biology.

40:44.240 --> 40:48.400
 And, and they had, you know, they got stuck. They hadn't really got very far with doing this.

40:48.400 --> 40:54.240
 And, and until now, until Alpha fold came along, this has done experimentally, right,

40:54.240 --> 40:58.480
 very painstakingly. So the rule of thumb is, and you have to like crystallize the protein,

40:58.480 --> 41:02.880
 which is really difficult. Some proteins can't be crystallized like membrane proteins.

41:02.880 --> 41:08.160
 And then you have to use very expensive electron microscopes or x ray crystallography machines,

41:08.160 --> 41:12.240
 really painstaking work to get the 3D structure and visualize the 3D structure.

41:12.240 --> 41:16.720
 So the rule of thumb in, in, in experimental biology is that it takes one PhD student,

41:16.720 --> 41:23.760
 their entire PhD, to do one protein. And without for full two, we were able to predict the 3D

41:23.760 --> 41:29.520
 structure in a matter of seconds. And so we were, you know, over Christmas, we did the whole human

41:29.520 --> 41:34.240
 proteome or every protein in the human body will 20,000 proteins. So the human proteomes like the

41:34.240 --> 41:40.400
 equivalent of the human genome, but on protein space, and, and sort of revolutionize really what

41:40.400 --> 41:46.880
 a structural biologist can do. Because now they don't have to worry about these painstaking

41:46.880 --> 41:50.400
 experimental, you know, should they put all of that effort in or not, they can almost just look

41:50.400 --> 41:55.440
 up the structure of their proteins like a Google search. And so there's a data set on which it's

41:56.080 --> 42:00.560
 trained and how to map this amino acid sequence. First of all, it's incredible that approaching

42:00.560 --> 42:05.600
 this little chemical computer is able to do that computation itself in some kind of distributed way

42:05.600 --> 42:10.480
 and do it very quickly. That's a weird thing. And they evolved that way. Because, you know, in the

42:10.480 --> 42:16.160
 beginning, I mean, that's a great invention, just the protein itself. Yes. I mean, and then there's,

42:16.160 --> 42:23.440
 I think, probably a history of like, they evolved to have many of these proteins. And those proteins

42:23.440 --> 42:28.480
 figure out how to be computers themselves, in such a way that you can create structures that

42:28.480 --> 42:32.720
 can interact in complexes with each other in order to form high level functions. I mean,

42:32.720 --> 42:37.200
 it's a weird system that they've figured it out. Well, for sure. I mean, we, you know, maybe we

42:37.200 --> 42:41.440
 should talk about the origins of life too. But proteins themselves, I think, are magical and

42:41.440 --> 42:48.880
 incredible, as I said, little, little bio nanomachines. And, and, and actually, Leventhal,

42:48.880 --> 42:55.360
 who was another scientist, a contemporary of Anfinson, he coined this Leventhal, what became

42:55.360 --> 43:00.400
 known as Leventhal's paradox, which is exactly what you're saying. He calculated roughly an

43:00.400 --> 43:08.400
 average protein, which is maybe 2000 amino acids base as long, is, is, is can fold in maybe 10 to

43:08.400 --> 43:13.760
 the power 300 different conformations. So there's 10 to the power 300 different ways that protein

43:13.760 --> 43:20.400
 could fold up. And yet somehow, in nature, physics solves this, solves this in a matter of milliseconds.

43:20.400 --> 43:26.880
 So proteins fold up in your body in, you know, sometimes in fractions of a second. So physics

43:26.880 --> 43:31.440
 is somehow solving that search problem. And just to be clear, in many of these cases, maybe you

43:31.440 --> 43:38.560
 correct me if I'm wrong, there's often a unique way for that sequence to form itself. So among

43:38.560 --> 43:46.080
 that huge number of possibilities, it figures out a way how to stably, in some cases, there's

43:46.080 --> 43:50.640
 might be a misfunction, so on, which leads to a lot of the disorders and stuff like that. But

43:50.640 --> 43:54.800
 most of the time it's a unique mapping. And that unique mapping is not obvious.

43:54.800 --> 43:59.600
 No, exactly. Which is what the problem is. Exactly. So there's a unique mapping, usually,

43:59.600 --> 44:06.000
 in a healthy, if it's healthy. And as you say, in disease, so for example, Alzheimer's, one, one,

44:06.000 --> 44:10.640
 one conjecture is that it's because of misfolder protein, protein that folds in the wrong way,

44:10.640 --> 44:15.600
 amyloid beta protein. So, and then because it folds in the wrong way, it gets tangled up,

44:15.600 --> 44:22.000
 right, in your, in your neurons. So it's super important to understand both healthy functioning

44:22.000 --> 44:27.440
 and also disease is to understand what these things are doing and how they're structuring.

44:27.440 --> 44:32.080
 Of course, the next step is sometimes proteins change shape when they interact with something.

44:32.080 --> 44:35.680
 So they're not just static necessarily in biology.

44:37.120 --> 44:43.120
 Maybe you can give some interesting, sort of beautiful things to you about these early days

44:43.120 --> 44:51.920
 of alpha fold of solving this problem because unlike games, this is real physical systems that are

44:51.920 --> 44:58.640
 less amenable to self play type of mechanisms. The size of the data set is smaller that you

44:58.640 --> 45:02.240
 might otherwise like. So you have to be very clever about certain things. Is there something you

45:02.240 --> 45:09.840
 could speak to what was very hard to solve and what are some beautiful aspects about the solution?

45:09.840 --> 45:14.640
 Yeah, I would say alpha fold is the most complex and also probably most meaningful system we've

45:14.640 --> 45:19.680
 built so far. So it's been amazing time actually in the last, you know, two, three years to see

45:19.680 --> 45:24.400
 that come through because as we talked about earlier, you know, games is what we started on

45:25.360 --> 45:31.040
 building things like AlphaGo and AlphaZero. But really the ultimate goal was to not just to crack

45:31.040 --> 45:36.240
 games, it was just to build, use them to bootstrap general learning systems we could then apply to

45:36.240 --> 45:41.840
 real world challenges. Specifically, my passion is scientific challenges like protein folding.

45:41.840 --> 45:47.600
 And then alpha fold, of course, is our first big proof point of that. And so, you know, in terms of

45:47.600 --> 45:52.240
 the data and the amount of innovations that had to go into it, we, you know, it was like

45:52.240 --> 45:57.040
 more than 30 different component algorithms needed to be put together to crack the protein folding.

45:57.840 --> 46:04.160
 I think some of the big innovations were that kind of building in some hard coded constraints

46:04.160 --> 46:10.160
 around physics and evolutionary biology to constrain sort of things like the bond angles

46:11.600 --> 46:18.640
 in the protein and things like that, a lot, but not to impact the learning system. So still

46:18.640 --> 46:25.360
 allowing the system to be able to learn the physics itself from the examples that we had.

46:25.360 --> 46:29.840
 And the examples, as you say, there are only about 150,000 proteins, even after 40 years of

46:29.840 --> 46:35.760
 experimental biology, only around 150,000 proteins have been the structures have been found out about.

46:35.760 --> 46:41.440
 So that was our training set, which is much less than normally we would like to use. But using

46:41.440 --> 46:47.440
 various tricks, things like self distillation, so actually using alpha fold predictions,

46:48.160 --> 46:52.080
 some of the best predictions that it thought was highly confident in, we put them back into the

46:52.080 --> 46:58.240
 training set, right, to make the training set bigger. That was critical to alpha fold working.

46:58.240 --> 47:04.320
 So there was actually a huge number of different innovations like that that were required to

47:04.320 --> 47:11.440
 ultimately crack the problem. Alpha fold one, what it produced was a histogram. So a kind of a

47:11.440 --> 47:18.400
 matrix of the pairwise distances between all of the molecules in the protein. And then there had

47:18.400 --> 47:24.800
 to be a separate optimization process to create the 3D structure. And what we did for alpha fold

47:24.800 --> 47:32.640
 two is make it truly end to end. So we went straight from the amino acid sequence of bases to the

47:32.640 --> 47:37.200
 3D structure directly without going through this intermediate step. And in machine learning,

47:37.200 --> 47:42.000
 what we've always found is that the more end to end you can make it, the better the system.

47:42.000 --> 47:48.400
 And it's probably because in the end, the system is better at learning what the constraints are

47:48.400 --> 47:53.840
 than we are as the human designers of specifying it. So anytime you can let it flow end to end

47:53.840 --> 47:58.240
 and actually just generate what it is you're really looking for, in this case, the 3D structure,

47:58.240 --> 48:02.320
 you're better off than having this intermediate step, which you then have to handcraft the next

48:02.320 --> 48:08.000
 step for. So it's better to let the gradients and the learning flow all the way through the system

48:08.000 --> 48:10.640
 from the endpoint, the end output you want to the inputs.

48:10.640 --> 48:15.360
 So that's a good way to start on a new problem. Handcraft a bunch of stuff, add a bunch of manual

48:15.360 --> 48:22.560
 constraints with a small learning piece and grow that learning piece until it consumes the whole

48:22.560 --> 48:26.880
 thing. That's right. And so you can also see, you know, this is a bit of a method we've developed

48:26.880 --> 48:32.880
 over doing many sort of successful alphas, we call them alpha X projects, right? And the easiest

48:32.880 --> 48:39.440
 way to see that is the evolution of AlphaGo to AlphaZero. So AlphaGo was a learning system,

48:39.440 --> 48:44.320
 but it was specifically trained to only play Go, right? So and what we wanted to do with

48:44.320 --> 48:48.800
 the first version of AlphaGo is just get to world champion performance, no matter how we did it,

48:48.800 --> 48:53.600
 right? And then, and then of course, AlphaGo zero, we, we, we remove the need to use human

48:53.600 --> 48:59.040
 games as a starting point, right? So it could just play against itself from random starting

48:59.040 --> 49:04.320
 point from the beginning. So that removed the need for human knowledge about Go. And then finally,

49:04.320 --> 49:09.520
 AlphaZero then generalized it so that any things we had in there, the system, including things like

49:09.520 --> 49:15.200
 symmetry of the Go board, were removed. So the AlphaZero could play from scratch any two player

49:15.200 --> 49:19.920
 game. And then MuZero, which is the final, our latest version of that set of things, was then

49:19.920 --> 49:24.400
 extending it so that you didn't even have to give it the rules of the game. It would learn that for

49:24.400 --> 49:28.720
 itself. So it could also deal with computer games as well as board games. So that line of AlphaGo,

49:28.720 --> 49:34.320
 AlphaGo zero, AlphaZero, MuZero, that's the full trajectory of what you can take from

49:35.440 --> 49:42.640
 imitation learning to full self supervised learning. Yeah, exactly. And learning, learning

49:42.640 --> 49:48.640
 the entire structure of the environment you put in from scratch, right? And, and, and bootstrapping

49:48.640 --> 49:53.360
 it through self play yourself. But the thing is, it would have been impossible, I think, or very

49:53.360 --> 49:59.360
 hard for us to build AlphaZero or MuZero first out of the box. Even psychologically, because you

49:59.360 --> 50:04.160
 have to believe in yourself for a very long time, you're constantly dealing with doubt because a

50:04.160 --> 50:08.320
 lot of people say that it's impossible. Exactly. So it's hard enough just to do Go, as you were

50:08.320 --> 50:13.280
 saying, everyone thought that was impossible, or at least a decade away from when we, when we

50:13.280 --> 50:20.560
 did it back in 2015, 2014, you know, 2016. And, and so, yes, it would have been psychologically,

50:20.560 --> 50:25.200
 probably very difficult, as well as the fact that of course, we learn a lot by building AlphaGo

50:25.200 --> 50:30.000
 first. Right. So it's, I think this is why I call AI an engineering science. It's one of the most

50:30.000 --> 50:34.560
 fascinating science disciplines. But it's also an engineering science in the sense that, unlike

50:34.560 --> 50:39.440
 unlike natural sciences, the phenomenon you're studying, it doesn't exist out in nature. You

50:39.440 --> 50:44.560
 have to build it first. So you have to build the artifact first, and then you can study how, how,

50:44.560 --> 50:50.400
 and pull it apart and how it works. This is tough to ask you this question, because you probably

50:50.400 --> 50:54.720
 will say it's everything. But let's, let's try, let's try to think through this, because you're

50:54.720 --> 51:00.320
 in a very interesting position where deep mind is a place of some of the most brilliant ideas in

51:00.320 --> 51:07.200
 the history of AI, but it's also a place of brilliant engineering. So how much of solving

51:07.200 --> 51:13.200
 intelligence, this big goal for deep mind, how much of it is science? How much is engineering?

51:13.200 --> 51:19.680
 So how much is the algorithms? How much is the data? How much is the hardware compute infrastructure?

51:19.680 --> 51:25.680
 How much is the software compute infrastructure? Yeah. What else is there? How much is the human

51:25.680 --> 51:31.120
 infrastructure? And like just the humans interacting certain kinds of ways in all the space of all

51:31.120 --> 51:39.600
 those ideas? And how much is maybe like philosophy? How much, what's the key? If you were to sort of

51:39.600 --> 51:45.040
 look back, like if we go forward 200 years and look back, what was the key thing that solved

51:45.040 --> 51:49.360
 intelligence? Is it the ideas or the engineering? I think it's a combination. I first of all,

51:49.360 --> 51:54.000
 of course, it's a combination of all those things, but the ratios of them changed over time.

51:54.000 --> 52:00.560
 Right. So even in the last 12 years, we started deep mind in 2010, which is hard to imagine now

52:00.560 --> 52:06.000
 because 2010, it's only 12 short years ago, but nobody was talking about AI. I don't even remember

52:06.000 --> 52:10.960
 back to your MIT days. No one was talking about it. I did a postdoc at MIT back around then,

52:10.960 --> 52:14.960
 and it was sort of thought of as, well, look, we know AI doesn't work. We tried this hard in the

52:14.960 --> 52:20.400
 90s at places like MIT, mostly using logic systems and old fashioned sort of good old

52:20.400 --> 52:25.600
 fashioned AI, we would call it now. People like Minsky and Patrick Winston, and you know all

52:25.600 --> 52:29.360
 these characters, right? And used to debate a few of them. And they used to think I was mad

52:29.360 --> 52:34.160
 thinking about that some new advance could be done with learning systems. I was actually pleased

52:34.160 --> 52:38.560
 to hear that because at least you know, you're on a unique track at that point, right? Even if

52:38.560 --> 52:44.880
 all of your professors are telling you you're mad. And of course, in industry, we couldn't get as

52:44.880 --> 52:49.280
 difficult to get two cents together, which is hard to imagine now as well, given that it's the

52:49.280 --> 52:54.640
 biggest sort of buzzword in VCs and fundraising is easy and all these kind of things today.

52:54.640 --> 53:00.400
 So back in 2010, it was very difficult. And the reason we started then and Shane and I used to

53:00.400 --> 53:06.480
 discuss what were the sort of founding tenants of DeepMind. And it was various things. One was

53:07.120 --> 53:12.000
 algorithmic advances. So deep learning, you know, Jeff Hinton and Co had just sort of invented

53:12.000 --> 53:16.640
 that in academia, but no one in industry knew about it. We love reinforcement learning. We

53:16.640 --> 53:20.960
 thought that could be scaled up. But also understanding about the human brain had advanced

53:20.960 --> 53:26.640
 quite a lot in the decade prior with fMRI machines and other things. So we could get some

53:26.640 --> 53:33.280
 good hints about architectures and algorithms and sort of representations maybe that the brain uses.

53:33.280 --> 53:39.760
 So at a systems level, not at a implementation level. And then the other big things were compute

53:39.760 --> 53:45.120
 and GPUs, right? So we could see a compute was going to be really useful and it got to a place

53:45.120 --> 53:50.240
 where it become commoditized mostly through the games industry. And that could be taken advantage

53:50.240 --> 53:54.800
 of. And then the final thing was also mathematical and theoretical definitions of intelligence.

53:54.800 --> 54:00.480
 So things like AIXI, AIXE, which Shane worked on with his supervisor Marcus Hutto, which is

54:00.480 --> 54:05.760
 this sort of theoretical proof, really, of universal intelligence, which is actually a

54:05.760 --> 54:10.240
 reinforcement learning system in the limit. I mean, it assumes infinite compute and infinite

54:10.240 --> 54:14.240
 memory in the way, you know, like a Turing machine proves. But I was also waiting to see

54:14.240 --> 54:19.760
 something like that to, you know, like Turing machines and computation theory that people

54:19.760 --> 54:25.360
 like Turing and Shannon came up with underpins modern computer science. You know, I was waiting

54:25.360 --> 54:30.560
 for a theory like that to sort of underpin AGI research. So when I met Shane and saw he was

54:30.560 --> 54:34.480
 working on something like that, you know, that to me was a sort of final piece of the jigsaw.

54:34.480 --> 54:40.720
 So in the early days, I would say that ideas were the most important. You know, for us,

54:40.720 --> 54:46.160
 it was deep reinforcement learning, scaling up deep learning. Of course, we've seen transformers.

54:46.160 --> 54:51.840
 So huge leaps, I would say, like three or four from, if you think from 2010 till now, huge

54:51.840 --> 55:00.000
 evolutions, things like AlphaGo. And maybe there's a few more still needed. But as we get closer to

55:00.000 --> 55:07.200
 AI, AGI, I think engineering becomes more and more important and data. Because scale and of

55:07.200 --> 55:12.160
 course the recent results of GPT3 and all the big language models and large models, including our

55:12.160 --> 55:19.040
 ones, has shown that scale is and large models are clearly going to be unnecessary, but perhaps

55:19.040 --> 55:25.920
 not sufficient part of an AGI solution. And throughout that, like you said, and I'd like

55:25.920 --> 55:31.440
 to give you a big thank you, you're one of the pioneers in this is sticking by ideas like

55:31.440 --> 55:38.080
 reinforcement learning, that this can actually work, given actually limited success in the past.

55:38.880 --> 55:47.120
 And also, which we still don't know, but proudly, having the best researchers in the world

55:47.120 --> 55:52.000
 and talking about solving intelligence. So talking about whatever you call it, AGI or

55:52.000 --> 55:57.600
 something like this, that speaking of MIT, that's just something you wouldn't bring up.

55:57.600 --> 56:09.760
 No, not maybe you did in like 40, 50 years ago. But that was AI was a place where you do tinkering,

56:09.760 --> 56:16.720
 very small scale, not very ambitious projects. And maybe the biggest ambitious projects were in

56:16.720 --> 56:21.600
 the space of robotics and doing like the DARPA challenge. But the task of solving intelligence

56:21.600 --> 56:27.840
 and believing you can, that's really, really powerful. So in order for engineering to do its work,

56:27.840 --> 56:33.440
 to have great engineers build great systems, you have to have that belief, that threads

56:33.440 --> 56:36.880
 throughout the whole thing that you can actually solve some of these impossible challenges.

56:36.880 --> 56:42.080
 Yeah, that's right. And back in 2010, our mission statement, and still is today,

56:42.080 --> 56:47.200
 is it was used to be solving step one, solve intelligence, step two, use it to solve everything

56:47.200 --> 56:53.440
 else. So if you can imagine pitching that to a VC in 2010, the kind of looks we got, we managed to

56:53.440 --> 56:59.600
 find a few kooky people to back us. But it was tricky. And it got to the point where we wouldn't

56:59.600 --> 57:04.720
 mention it to any of our professors, because they would just eye roll and think we committed

57:04.720 --> 57:11.440
 career suicide. And so it was a lot of things that we had to do. But we always believed it.

57:11.440 --> 57:17.680
 And one reason, by the way, one reason I've always believed in reinforcement learning is that, if

57:17.680 --> 57:23.680
 you look at neuroscience, that is the way that the primate brain learns. One of the main mechanisms

57:23.680 --> 57:27.840
 is the dopamine system implements some form of TD learning. It's a very famous result in the late

57:27.840 --> 57:36.080
 90s, where they saw this in monkeys, and as a propagating prediction error. So again, in the

57:36.080 --> 57:42.240
 limit, this is what I think you can use neuroscience for is, in any mathematics, when you're doing

57:42.240 --> 57:46.640
 something as ambitious as trying to solve intelligence, and it's blue sky research, no one

57:46.640 --> 57:53.200
 knows how to do it, you need to use any evidence or any source of information you can to help guide

57:53.200 --> 57:57.760
 you in the right direction or give you confidence you're going in the right direction. So that was

57:57.760 --> 58:02.080
 one reason we pushed so hard on that. And it's just going back to your earlier question about

58:02.080 --> 58:08.240
 organization. The other big thing that I think we innovated with at DeepMind to encourage invention

58:08.240 --> 58:14.080
 and innovation was the multidisciplinary organization we built, and we still have today.

58:14.080 --> 58:19.360
 So DeepMind originally was a confluence of the most cutting edge knowledge in neuroscience

58:19.360 --> 58:25.040
 with machine learning, engineering, and mathematics, and gaming. And then since then,

58:25.040 --> 58:30.720
 we've built that out even further. So we have philosophers here and by ethicists, but also

58:30.720 --> 58:35.840
 other types of scientists, physicists, and so on. And that's what brings together, I tried to build a

58:35.840 --> 58:44.160
 sort of new type of Bell Labs, but in its golden era, right? And a new expression of that, to try

58:44.160 --> 58:50.480
 and foster this incredible sort of innovation machine. So talking about the humans in the machine,

58:50.480 --> 58:56.400
 DeepMind itself is a learning machine with a lot of amazing human minds in it, coming together to

58:56.400 --> 59:04.800
 try and build these learning systems. If we return to the big ambitious dream of AlphaFold,

59:04.800 --> 59:10.320
 that may be the early steps on a very long journey in biology,

59:12.480 --> 59:16.560
 do you think the same kind of approach can use to predict the structure and function of more

59:16.560 --> 59:23.440
 complex biological systems, so multi protein interaction? And then, I mean, you can go out

59:23.440 --> 59:28.560
 from there, just simulating bigger and bigger systems that eventually simulate something like

59:28.560 --> 59:35.600
 the human brain or the human body, just the big mush, the mess of the beautiful resilient mess

59:35.600 --> 59:42.720
 of biology. Do you see that as a long term vision? I do. And I think, if you think about what are

59:42.720 --> 59:48.960
 the top things I wanted to apply AI to once we had powerful enough systems, biology and curing

59:48.960 --> 59:54.800
 diseases and understanding biology was right up there, top of my list. That's one of the reasons

59:54.800 --> 1:00:01.040
 I personally pushed that myself and with AlphaFold. But I think AlphaFold, amazing as it is,

1:00:01.040 --> 1:00:08.720
 is just the beginning. And I hope it's evidence of what could be done with computational methods.

1:00:08.720 --> 1:00:15.120
 So AlphaFold solved this huge problem of the structure of proteins, but biology is dynamic.

1:00:15.120 --> 1:00:19.360
 So really, what I imagined from here, and we're working on all these things now, is protein

1:00:19.360 --> 1:00:25.760
 protein interaction, protein ligand binding, so reacting with molecules, then you want to

1:00:25.760 --> 1:00:31.840
 get built up to pathways, and then eventually a virtual cell. That's my dream, maybe in the next

1:00:31.840 --> 1:00:35.360
 10 years. And I've been talking actually to a lot of biologists, friends of mine, Paul Nurse,

1:00:35.360 --> 1:00:39.680
 who runs the Crick Institute, amazing biologists, Nobel Prize winning biologists, we've been discussing

1:00:39.680 --> 1:00:44.960
 for 20 years now, virtual cells. Could you build a virtual simulation of a cell? And if you

1:00:44.960 --> 1:00:48.880
 could, that would be incredible for biology and disease discovery, because you could do loads of

1:00:48.880 --> 1:00:53.760
 experiments on the virtual cell, and then only at the last stage validate it in the wet lab.

1:00:53.760 --> 1:00:59.600
 So in terms of the search space of discovering new drugs, it takes 10 years roughly to go from

1:01:01.920 --> 1:01:08.400
 identifying a target to having a drug candidate. Maybe that could be shortened by an order of

1:01:08.400 --> 1:01:15.760
 magnitude, if you could do most of that work in silico. So in order to get to a virtual cell, we

1:01:15.760 --> 1:01:22.720
 have to build up understanding of different parts of biology and the interactions. And so every

1:01:22.720 --> 1:01:26.960
 few years we talk about this, I talked about this with Paul, and then finally, last year,

1:01:26.960 --> 1:01:31.680
 after AlphaFold, I said, now's the time, we can finally go for it. And AlphaFold's the first

1:01:31.680 --> 1:01:35.760
 proof point that this might be possible. And he's very exciting, we have some collaborations

1:01:35.760 --> 1:01:40.800
 with his lab, they're just across the road actually from us as wonderful being here in Kings Cross

1:01:40.800 --> 1:01:46.240
 with the Crick Institute across the road. And I think the next steps, I think there's going

1:01:46.240 --> 1:01:51.440
 to be some amazing advances in biology built on top of things like AlphaFold. We're already seeing

1:01:51.440 --> 1:01:58.080
 that with the community doing that after we've open sourced it and released it. And I often say

1:01:58.080 --> 1:02:04.960
 that I think, if you think of mathematics, is the perfect description language for physics.

1:02:04.960 --> 1:02:09.520
 I think AI might end up being the perfect description language for biology, because

1:02:10.160 --> 1:02:16.400
 biology is so messy, it's so emergent, so dynamic and complex. I find it very hard to

1:02:16.400 --> 1:02:21.280
 believe we'll ever get to something as elegant as Newton's Laws of Motions to describe a cell,

1:02:21.280 --> 1:02:25.920
 right? It's just too complicated. So I think AI is the right tool for this.

1:02:25.920 --> 1:02:31.440
 You have to start at the basic building blocks and use AI to run the simulation

1:02:31.440 --> 1:02:36.960
 for all those building blocks. So have a very strong way to do prediction of what given these

1:02:36.960 --> 1:02:42.480
 building blocks, what kind of biology, how the function and the evolution of that biological

1:02:42.480 --> 1:02:47.440
 system. It's almost like a cellular automata. You have to run it. You can't analyze it from a high

1:02:47.440 --> 1:02:52.240
 level. You have to take the basic ingredients, figure out the rules and let it run. But in this

1:02:52.240 --> 1:02:57.280
 case, the rules are very difficult to figure out. You have to learn them. That's exactly it. So the

1:02:57.280 --> 1:03:04.000
 biology is too complicated to figure out the rules. It's too emergent, too dynamic, say, compared

1:03:04.000 --> 1:03:09.360
 to a physics system, like the motion of a planet. And so you have to learn the rules. And that's

1:03:09.360 --> 1:03:14.560
 exactly the type of systems that we're building. So you mentioned you've open sourced Alpha Fold

1:03:14.560 --> 1:03:21.200
 and even the data involved. To me, personally, also really happy and a big thank you for open

1:03:21.200 --> 1:03:28.880
 sourcing Majoko, the physics simulation engine that's often used for robotics research and so on.

1:03:28.880 --> 1:03:38.880
 So I think that's a pretty gangster move. So very few companies or people do that kind of thing.

1:03:38.880 --> 1:03:44.400
 What's the philosophy behind that? It's a case by case basis. And in both those cases, we felt

1:03:44.400 --> 1:03:49.280
 that was the maximum benefit to humanity to do that. And the scientific community,

1:03:49.280 --> 1:03:55.520
 in one case, the robotics physics community with Majoko. We purchased it for open sourcing.

1:03:55.520 --> 1:04:02.080
 Yes, we purchased it for the express principle to open sourcing. So I hope people appreciate

1:04:02.080 --> 1:04:06.800
 that. It's great to hear that you do. And then the second thing was, and mostly we did it because

1:04:06.800 --> 1:04:13.280
 the person building it was not able to cope with supporting it anymore because it got too big for

1:04:13.280 --> 1:04:18.000
 him. He's an amazing professor who built it in the first place. So we helped him out with that.

1:04:18.000 --> 1:04:21.840
 And then with Alpha Fold is even bigger, I would say. And I think in that case,

1:04:21.840 --> 1:04:28.080
 we decided that there were so many downstream applications of Alpha Fold that we couldn't

1:04:28.080 --> 1:04:34.880
 possibly even imagine what they all were. So the best way to accelerate drug discovery and also

1:04:34.880 --> 1:04:42.080
 fundamental research would be to give all that data away and the system itself.

1:04:43.760 --> 1:04:46.880
 It's been so gratifying to see what people have done that within just one year,

1:04:46.880 --> 1:04:53.440
 which is a short amount of time in science. And it's been used by over 500,000 researchers have

1:04:53.440 --> 1:04:57.840
 used it. We think that's almost every biologist in the world. I think there's roughly 500,000

1:04:57.840 --> 1:05:02.960
 biologists in the world, professional biologists, have used it to look at their proteins of interest.

1:05:04.320 --> 1:05:09.280
 We've seen amazing fundamental research done. So a couple of weeks ago, there was a whole

1:05:09.280 --> 1:05:13.840
 special issue of science, including the front cover, which had the nuclear pore complex on it,

1:05:13.840 --> 1:05:17.360
 which is one of the biggest proteins in the body. The nuclear pore complex is

1:05:17.360 --> 1:05:21.520
 a protein that governs all the nutrients going in and out of your cell nucleus.

1:05:21.520 --> 1:05:26.400
 So they're like little gateways that open and close to let things go in and out of your cell

1:05:26.400 --> 1:05:30.880
 nucleus. So they're really important. But they're huge because they're massive doughnut ring shaped

1:05:30.880 --> 1:05:35.440
 things. And they've been looking to try and figure out that structure for decades. And they have

1:05:35.440 --> 1:05:39.840
 lots of experimental data, but it's too low resolution. There's bits missing. And they were

1:05:39.840 --> 1:05:46.080
 able to, like a giant Lego jigsaw puzzle, use alpha fold predictions plus experimental data

1:05:46.080 --> 1:05:50.720
 and combined those two independent sources of information, actually four different groups

1:05:50.720 --> 1:05:55.360
 around the world were able to put it together more or less simultaneously using alpha fold

1:05:55.360 --> 1:05:59.760
 predictions. So that's been amazing to see. And pretty much every pharma company, every drug

1:05:59.760 --> 1:06:04.720
 company executive I've spoken to has said that their teams are using alpha fold to accelerate

1:06:04.720 --> 1:06:11.360
 whatever drugs they're trying to discover. So I think the knock on effect has been enormous

1:06:11.360 --> 1:06:16.320
 in terms of the impact that alpha fold has made. And it's probably bringing in,

1:06:16.320 --> 1:06:21.920
 it's creating biologists, it's bringing more people into the field, both on the excitement and

1:06:21.920 --> 1:06:28.640
 both on the technical skills involved. And it's almost like a gateway drug to biology.

1:06:28.640 --> 1:06:33.760
 Yes, it is. And more computational people involved too, hopefully. And I think for us,

1:06:33.760 --> 1:06:37.840
 you know, the next stage, as I said, you know, in future, we have to have other considerations too.

1:06:37.840 --> 1:06:41.680
 We're building on top of alpha fold and these other ideas I discussed with you about protein,

1:06:41.680 --> 1:06:46.080
 protein interactions and genomics and other things. And not everything will be open source.

1:06:46.080 --> 1:06:49.840
 Some of it will do commercially because that will be the best way to actually get the most

1:06:49.840 --> 1:06:54.720
 resources and impact behind it. In other ways, some other projects will do non profit style.

1:06:55.600 --> 1:06:59.600
 And also we have to consider for future things as well, safety and ethics as well,

1:06:59.600 --> 1:07:05.440
 like synthetic biology, there is dual use. And we have to think about that as well. With alpha

1:07:05.440 --> 1:07:10.400
 fold, we consulted with 30 different bioethicists and other people expert in this field to make

1:07:10.400 --> 1:07:15.520
 sure it was safe before we released it. So there'll be other considerations in future. But for

1:07:15.520 --> 1:07:20.720
 right now, I think alpha fold is a kind of a gift from us to the scientific community.

1:07:20.720 --> 1:07:28.240
 So I'm pretty sure that something like alpha fold would be part of Nobel prizes in the future.

1:07:28.240 --> 1:07:32.880
 But us humans, of course, are horrible with credit assignment. So we'll of course give it to the

1:07:32.880 --> 1:07:43.360
 humans. Do you think there will be a day when AI system can't be denied that it earned that

1:07:43.360 --> 1:07:46.560
 Nobel prize? Do you think we will see that in 21st century?

1:07:46.560 --> 1:07:50.080
 It depends what type of AI as we end up building, right? Whether they're

1:07:50.080 --> 1:07:58.800
 goal seeking agents who specifies the goals, who comes up with the hypotheses, who determines

1:07:58.800 --> 1:08:02.240
 which problems to tackle, right? So I think it's about announcement.

1:08:02.240 --> 1:08:10.160
 Yes, it's about results exactly as part of it. So I think right now, of course, it's amazing human

1:08:10.160 --> 1:08:14.960
 ingenuity that's behind these systems. And then the system, in my opinion, is just a tool. It would

1:08:14.960 --> 1:08:20.240
 be a bit like saying with Galileo and his telescope, you know, the ingenuity that the credit should go

1:08:20.240 --> 1:08:25.840
 to the telescope. I mean, it's clearly Galileo building the tool which he then uses. So I still

1:08:25.840 --> 1:08:31.840
 see that in the same way today, even though these tools learn for themselves. I think of

1:08:31.840 --> 1:08:36.640
 things like alpha fold and the things we're building as the ultimate tools for science

1:08:36.640 --> 1:08:41.040
 and for acquiring new knowledge to help us as scientists acquire new knowledge.

1:08:41.040 --> 1:08:47.360
 I think one day there will come a point where an AI system may solve or come up with something like

1:08:47.360 --> 1:08:53.120
 general relativity of its own bat, not just by averaging everything on the internet or averaging

1:08:53.120 --> 1:08:57.280
 everything on PubMed. Although that would be interesting to see what that would come up with.

1:08:58.400 --> 1:09:03.200
 So that to me is a bit like our earlier debate about creativity, you know, inventing go,

1:09:03.200 --> 1:09:10.240
 rather than just coming up with a good go move. And so I think solving, I think to, you know,

1:09:10.240 --> 1:09:14.800
 if we wanted to give it the credit of like a Nobel type of thing, then it would need to invent go

1:09:15.600 --> 1:09:21.760
 and sort of invent that new conjecture out of the blue, rather than being specified by the

1:09:21.760 --> 1:09:26.160
 human scientists or the human creators. So I think right now that's, it's definitely just a tool.

1:09:26.160 --> 1:09:29.600
 Although it is interesting how far you get by averaging everything on the internet, like you

1:09:29.600 --> 1:09:34.560
 said, because, you know, a lot of people do see science as you're always standing on the shoulders

1:09:34.560 --> 1:09:41.840
 of giants. And the question is how much are you really reaching up above the shoulders of giants?

1:09:41.840 --> 1:09:48.640
 Maybe it's just assimilating different kinds of results of the past with ultimately this new

1:09:48.640 --> 1:09:54.800
 perspective that gives you this breakthrough idea. But that idea may not be novel in the way that

1:09:54.800 --> 1:09:59.920
 it can't be already discovered on the internet. Maybe the Nobel prizes of the next hundred years

1:09:59.920 --> 1:10:04.960
 are already all there on the internet to be discovered. They could be. They could be. I mean,

1:10:04.960 --> 1:10:12.560
 I think this is one of the big mysteries, I think, is that I, first of all, I believe a lot of the

1:10:12.560 --> 1:10:16.640
 big new breakthroughs that are going to come in the next few decades. And even in the last decade

1:10:16.640 --> 1:10:21.520
 are going to come at the intersection between different subject areas, where there'll be some

1:10:21.520 --> 1:10:27.200
 new connection that's found between what seemingly were disparate areas. And one can even think of

1:10:27.200 --> 1:10:32.320
 deep mind, as I said earlier, as a sort of interdiscipline between neuroscience ideas and AI

1:10:32.320 --> 1:10:39.280
 engineering ideas originally. And so I think there's that. And then one of the things we can't

1:10:39.280 --> 1:10:43.600
 imagine today is, and one of the reasons I think people, we were so surprised by how well large

1:10:43.600 --> 1:10:48.800
 models worked is that actually, it's very hard for our human minds, our limited human minds to

1:10:48.800 --> 1:10:52.800
 understand what it would be like to read the whole internet, right? I think we can do a thought

1:10:52.800 --> 1:10:57.760
 experiment. And I used to do this of like, well, what if I read the whole of Wikipedia? What would

1:10:57.760 --> 1:11:02.080
 I know? And I think our minds can just about comprehend maybe what that would be like, but

1:11:02.080 --> 1:11:06.640
 the whole internet is beyond comprehension. So I think we just don't understand what it would be

1:11:06.640 --> 1:11:12.800
 like to be able to hold all of that in mind, potentially, right? And then active at once.

1:11:12.800 --> 1:11:17.120
 And then maybe what are the connections that are available there? So I think no doubt there are huge

1:11:17.120 --> 1:11:22.320
 things to be discovered just like that. But I do think there is this other type of creativity of

1:11:22.320 --> 1:11:28.000
 true spark of new knowledge, new idea never thought before about can't be averaged from things that

1:11:28.000 --> 1:11:33.600
 are known, that really, of course, everything come, you know, nobody creates in a vacuum. So

1:11:33.600 --> 1:11:38.720
 there must be clues somewhere. But just a unique way of putting those things together, I think

1:11:38.720 --> 1:11:42.640
 some of the greatest scientists in history have displayed that I would say, although it's very

1:11:42.640 --> 1:11:47.920
 hard to know, going back to their time, what was exactly known when they came up with those things.

1:11:47.920 --> 1:11:53.120
 Although you're making me really think because just the thought experiment

1:11:53.120 --> 1:12:01.280
 of deeply knowing 100 Wikipedia pages, I don't think I can. I've been really impressed by Wikipedia

1:12:01.280 --> 1:12:07.680
 for technical topics. So if you know 100 pages or 1000 pages, I don't think we can

1:12:08.320 --> 1:12:15.440
 truly comprehend what kind of intelligence that is. It's a pretty powerful. If you know how to

1:12:15.440 --> 1:12:20.480
 use that and integrate that information correctly, I think you can go really far. You can probably

1:12:20.480 --> 1:12:27.120
 construct thought experiments based on that, like simulate different ideas. So if this is true,

1:12:27.120 --> 1:12:31.440
 let me run this thought experiment that maybe this is true. It's not really invention. It's

1:12:31.440 --> 1:12:37.360
 like just taking literally the knowledge and using it to construct a very basic simulation of the

1:12:37.360 --> 1:12:42.320
 world. I mean, some argue it's romantic in part, but Einstein would do the same kind of things

1:12:42.320 --> 1:12:47.200
 with thought experiments. Yeah, one could imagine doing that systematically across millions of

1:12:47.200 --> 1:12:53.520
 Wikipedia pages, plus PubMed, all these things. I think there are many, many things to be discovered

1:12:53.520 --> 1:12:57.520
 like that that are hugely useful. You could imagine, and I want us to do some of these things in

1:12:57.520 --> 1:13:01.600
 material science, like room temperature superconductors or something on my list one day that I'd

1:13:01.600 --> 1:13:07.920
 like to have an AI system to help build better optimized batteries. All of these sort of mechanical

1:13:07.920 --> 1:13:16.000
 things, I think a systematic sort of search could be guided by a model, could be extremely

1:13:16.000 --> 1:13:22.080
 powerful. So speaking of which, you have a paper on nuclear fusion, magnetic control of

1:13:22.080 --> 1:13:28.080
 tachymic plasmus through deeper enforcement learning. So you're seeking to solve nuclear fusion

1:13:28.080 --> 1:13:32.720
 with deep RL, so it's doing control of high temperature plasmas. Can you explain this work

1:13:32.720 --> 1:13:40.000
 and can AI eventually solve nuclear fusion? It's been very fun last year or two and very productive

1:13:40.000 --> 1:13:44.800
 because we've been ticking off a lot of my dream projects, if you like, of things that I've collected

1:13:44.800 --> 1:13:49.520
 over the years of areas of science that I would like to, I think could be very transformative

1:13:49.520 --> 1:13:55.120
 if we helped accelerate and are really interesting problems, scientific challenges in of themselves.

1:13:55.680 --> 1:14:01.040
 This is energy. So energy, yes, exactly. So energy and climate. So we talked about disease and

1:14:01.040 --> 1:14:06.160
 biology as being one of the biggest places I think AI can help with. I think energy and climate

1:14:06.160 --> 1:14:12.320
 is another one. So maybe they would be my top two. And fusion is one area I think AI can help with.

1:14:12.320 --> 1:14:18.080
 Now, fusion has many challenges, mostly physics and material science and engineering challenges

1:14:18.080 --> 1:14:22.480
 as well to build these massive fusion reactors and contain the plasma. And what we try to do,

1:14:22.480 --> 1:14:28.960
 and whenever we go into a new field to apply our systems is we look for, we talk to domain experts,

1:14:28.960 --> 1:14:33.920
 we try and find the best people in the world to collaborate with. In this case, in fusion,

1:14:33.920 --> 1:14:38.000
 we collaborate with EPFL in Switzerland, the Swiss Technical Institute, who are amazing.

1:14:38.000 --> 1:14:43.200
 They have a test reactor. They were willing to let us use, which I double checked with the team,

1:14:43.200 --> 1:14:48.080
 we were going to use carefully and safely. I was impressed. They managed to persuade them to let us

1:14:48.080 --> 1:14:55.520
 use it. And it's an amazing test reactor they have there. And they try all sorts of pretty crazy

1:14:55.520 --> 1:15:01.600
 experiments on it. And what we tend to look at is if we go into a new domain like fusion,

1:15:01.600 --> 1:15:06.160
 what are all the bottleneck problems? Thinking from first principles, what are all the

1:15:06.160 --> 1:15:11.280
 bottleneck problems that are still stopping fusion working today? And then we get a fusion expert to

1:15:11.280 --> 1:15:16.000
 tell us. And then we look at those bottlenecks and we look at the ones which ones are amenable

1:15:16.000 --> 1:15:22.720
 to our AI methods today. And we'd be interesting from a research perspective, from our point of

1:15:22.720 --> 1:15:27.040
 view, from an AI point of view. And that would address one of their bottlenecks. And in this

1:15:27.040 --> 1:15:32.800
 case, plasma control was perfect. So the plasma, it's a million degrees Celsius, something like

1:15:32.800 --> 1:15:37.920
 that's hotter than the sun. And there's obviously no material that can contain it. So they have to

1:15:37.920 --> 1:15:42.880
 be containing these magnetic, very powerful superconducting magnetic fields. But the problem

1:15:42.880 --> 1:15:48.080
 is plasma is pretty unstable, as you imagine. You're kind of holding a mini sun, mini star

1:15:48.080 --> 1:15:53.840
 in a reactor. So you kind of want to predict ahead of time what the plasma is going to do,

1:15:53.840 --> 1:15:59.840
 so you can move the magnetic field within a few milliseconds to basically contain what it's going

1:15:59.840 --> 1:16:04.640
 to do next. So it seems like a perfect problem if you think of it for a reinforcement learning

1:16:04.640 --> 1:16:10.320
 prediction problem. So you've got a controller, you've got to move the magnetic field. And until

1:16:10.320 --> 1:16:16.640
 we came along, they were doing it with traditional operational research type of controllers,

1:16:16.640 --> 1:16:20.480
 which are kind of handcrafted. And the problem is, of course, they can't react in the moment to

1:16:20.480 --> 1:16:24.560
 something the plasma is doing. They have to be hard coded. And again, knowing that that's

1:16:24.560 --> 1:16:29.200
 normally our go to solution is we would like to learn that instead. And they also had a simulator

1:16:29.200 --> 1:16:33.920
 of these plasma. So there were lots of criteria that matched what we like to use.

1:16:34.720 --> 1:16:38.320
 So can AI eventually solve nuclear fusion?

1:16:38.320 --> 1:16:42.960
 Well, so with this problem, and we published it in Nature paper last year, we held the

1:16:42.960 --> 1:16:47.840
 fusion that we held the plasma in a specific shapes. So actually, it's almost like carving the

1:16:47.840 --> 1:16:54.480
 plasma into different shapes and hold it there for a record amount of time. So that's one of the

1:16:54.480 --> 1:17:00.560
 problems of fusion sort of solved. So have a controller that's able to, no matter the shape,

1:17:01.280 --> 1:17:05.360
 contain it. Yeah, contain it and hold it in structure. And there's different shapes that are

1:17:05.360 --> 1:17:12.560
 better for the energy productions called droplets and so on. So that was huge. And now we're looking,

1:17:12.560 --> 1:17:18.000
 we're talking to lots of fusion startups to see what's the next problem we can tackle in the fusion

1:17:18.000 --> 1:17:24.960
 area. So another fascinating place in a paper title, pushing the frontiers of density functionals

1:17:24.960 --> 1:17:31.040
 by solving the fractional electron problem. So you're taking on modeling and simulating the

1:17:31.040 --> 1:17:39.760
 quantum mechanical behavior of electrons. Can you explain this work and can AI model and simulate

1:17:39.760 --> 1:17:44.160
 arbitrary quantum mechanical systems in the future? Yeah, so this is another problem I've had my eye on

1:17:44.160 --> 1:17:52.320
 for decade or more, which is sort of simulating the properties of electrons. If you can do that,

1:17:52.320 --> 1:17:58.880
 you can basically describe how elements and materials and substances work. So it's kind

1:17:58.880 --> 1:18:04.480
 of like fundamental if you want to advance material science. And we have Schrodinger's

1:18:04.480 --> 1:18:09.520
 equation and then we have approximations to that density functional theory. These things are

1:18:09.520 --> 1:18:16.800
 famous. And people try and write approximations to these to these functionals and kind of come

1:18:16.800 --> 1:18:21.440
 up with descriptions of the electron clouds, where they're going to go, how they're going to

1:18:21.440 --> 1:18:26.400
 interact when you put two elements together. And what we try to do is learn a simulation,

1:18:27.600 --> 1:18:33.440
 learn a functional that will describe more chemistry types of chemistry. So until now,

1:18:33.440 --> 1:18:38.640
 you know, you can run expensive simulations, but then you can only simulate very small molecules,

1:18:38.640 --> 1:18:44.880
 very simple molecules. We would like to simulate large materials. And so today there's no way of

1:18:44.880 --> 1:18:51.120
 doing that. And we're building up towards building functionals that approximate Schrodinger's equation

1:18:51.120 --> 1:18:57.440
 and then allow you to describe what the electrons are doing. And all material sort of science and

1:18:57.440 --> 1:19:04.240
 material properties are governed by the electrons and how they interact. So have a good summarization

1:19:04.240 --> 1:19:12.480
 of the simulation through the functional, but one that is still close to what the actual simulation

1:19:12.480 --> 1:19:17.920
 will come out with. So what, how difficult is that task? What's involved in that task? Is it

1:19:17.920 --> 1:19:23.840
 running those those complicated simulations and learning the task of mapping from the initial

1:19:23.840 --> 1:19:28.240
 conditions and the parameters of the simulation, learning what the functional would be? Yeah.

1:19:28.240 --> 1:19:33.600
 So it's pretty tricky. And we've done it with, you know, the nice thing is we there are we can run a

1:19:33.600 --> 1:19:39.600
 lot of the simulations, the molecular dynamic simulations on our compute clusters. And so that

1:19:39.600 --> 1:19:44.720
 generates a lot of data. So in this case, the data is generated. So we like those sort of systems,

1:19:44.720 --> 1:19:49.840
 and that's why we use games, it's simulator generator data. And we can kind of create as much of it as

1:19:49.840 --> 1:19:55.120
 we want really. And just let's leave some, you know, if any computers are free in the cloud,

1:19:55.120 --> 1:19:59.280
 we just run, we run some of these calculations, right compute cluster calculation.

1:19:59.280 --> 1:20:03.440
 The free compute time is used up on quantum mechanics. Yeah, quantum mechanics, exactly,

1:20:03.440 --> 1:20:08.560
 simulations and protein simulations and other things. And so, and so, you know, when you're

1:20:08.560 --> 1:20:13.040
 not searching on YouTube for video, cat videos, we're using those computers usefully and quantum

1:20:13.040 --> 1:20:18.480
 chemistry, the idea. Fine. And, and putting them to good use. And then, yeah, and then all of that

1:20:18.480 --> 1:20:23.280
 computational data that's generated, we can then try and learn the functionals from that,

1:20:23.280 --> 1:20:28.560
 which of course are way more efficient. Once we learn the functional, then running those

1:20:28.560 --> 1:20:35.840
 simulations would be. Do you think one day AI may allow us to do something like basically crack open

1:20:35.840 --> 1:20:40.640
 physics, so do something like travel faster than the speed of light? My ultimate aim has always

1:20:40.640 --> 1:20:48.080
 been with AI is the reason I am personally working on AI for my whole life, it was to build a tool

1:20:48.080 --> 1:20:54.240
 to help us understand the universe. So I wanted to, and that means physics, really, and the nature

1:20:54.240 --> 1:21:00.000
 of reality. So I don't think we have systems that are capable of doing that yet. But when we get

1:21:00.000 --> 1:21:05.440
 towards AGI, I think that's one of the first things I think we should apply AGI to. I would

1:21:05.440 --> 1:21:09.280
 like to test the limits of physics and our knowledge of physics. There's so many things we

1:21:09.280 --> 1:21:13.920
 don't know. This is one thing I find fascinating about science. And, you know, as a huge proponent

1:21:13.920 --> 1:21:18.240
 of the scientific method is being one of the greatest ideas humanities ever had and allowed

1:21:18.240 --> 1:21:22.960
 us to progress with our knowledge. What I think is a true scientist, I think what you find is

1:21:22.960 --> 1:21:29.760
 the more you find out, the more you realize we don't know. And I always think that it's surprising

1:21:29.760 --> 1:21:34.400
 that more people aren't troubled. You know, every night I think about all these things we interact

1:21:34.400 --> 1:21:40.400
 with all the time, that we have no idea how they work, time, consciousness, gravity, life.

1:21:41.600 --> 1:21:46.400
 These are all the fundamental things of nature. We don't really know what they are.

1:21:46.400 --> 1:21:53.680
 To live life, we pin certain assumptions on them and kind of treat our assumptions as if

1:21:53.680 --> 1:21:58.800
 they're a fact that allows us to sort of box them off somehow. Yeah, box them off somehow.

1:21:58.800 --> 1:22:03.760
 But the reality is when you think of time, you should remind yourself, you should

1:22:04.400 --> 1:22:10.480
 take it off the shelf and realize like, no, we have a bunch of assumptions. There's even

1:22:10.480 --> 1:22:14.560
 not a lot of debate. There's a lot of uncertainty about exactly what is time.

1:22:14.560 --> 1:22:19.120
 Is there an error of time? You know, there's a lot of fundamental questions that you can't

1:22:19.120 --> 1:22:26.320
 just make assumptions about. And maybe AI allows you to not put anything on the shelf.

1:22:26.320 --> 1:22:27.120
 Yeah.

1:22:27.120 --> 1:22:31.280
 Not make any hard assumptions and really open it up and see what's going on.

1:22:31.280 --> 1:22:35.600
 Exactly. I think we should be truly open minded about that. And exactly that,

1:22:35.600 --> 1:22:42.000
 not be dogmatic to a particular theory. It'll also allow us to build better tools,

1:22:42.000 --> 1:22:48.320
 experimental tools eventually that can then test certain theories that may not be testable today.

1:22:48.320 --> 1:22:52.880
 As things about what we spoke about at the beginning, about the computational nature

1:22:52.880 --> 1:23:00.240
 of the universe, if that was true, how one might go about testing that. And there are people who've

1:23:00.240 --> 1:23:06.240
 conjectured people like Scott Aronson and others about how much information can a specific

1:23:06.240 --> 1:23:12.240
 specific Planck unit of space and time contain. So one might be able to think about testing those

1:23:12.240 --> 1:23:20.880
 ideas if you had AI helping you build some new exquisite experimental tools. This is what I

1:23:20.880 --> 1:23:23.520
 imagine many decades from now will be able to do.

1:23:24.080 --> 1:23:30.400
 And what kind of questions can be answered through running a simulation of them? There's

1:23:30.400 --> 1:23:36.560
 a bunch of physics simulations you can imagine that could be run in some kind of efficient way,

1:23:36.560 --> 1:23:39.200
 much like you're doing in the quantum simulation work.

1:23:41.120 --> 1:23:46.800
 And perhaps even the origin of life. So figuring out how going even back before

1:23:46.800 --> 1:23:53.280
 the work of AlphaFold begins of how this whole thing emerges from a rock.

1:23:53.280 --> 1:23:53.920
 Yes.

1:23:53.920 --> 1:23:59.600
 From a static thing. Do you think AI will allow us to, is that something you have your eye on?

1:23:59.600 --> 1:24:04.240
 Is trying to understand the origin of life? First of all, yourself, what do you think?

1:24:06.240 --> 1:24:08.080
 How the heck did life originate on Earth?

1:24:08.640 --> 1:24:15.120
 Yeah. Well, maybe I'll come to that in a second. But I think the ultimate use of AI is to kind

1:24:15.120 --> 1:24:21.680
 of use it to accelerate science to the maximum. So I think of it a little bit like the tree of

1:24:21.680 --> 1:24:25.760
 all knowledge. If you imagine that's all the knowledge there is in the universe to attain.

1:24:25.760 --> 1:24:31.600
 And we sort of barely scratch the surface of that so far. And even though we've done pretty

1:24:31.600 --> 1:24:36.720
 well since the Enlightenment as humanity. And I think AI will turbocharge all of that,

1:24:36.720 --> 1:24:41.440
 like we've seen with AlphaFold. And I want to explore as much of that tree of knowledge as

1:24:41.440 --> 1:24:48.720
 it's possible to do. And I think that involves AI helping us with understanding or finding patterns,

1:24:49.440 --> 1:24:53.440
 but also potentially designing and building new tools, experimental tools.

1:24:53.440 --> 1:24:58.800
 So I think that's all and also running simulations and learning simulations.

1:24:58.800 --> 1:25:06.080
 All of that, we're sort of doing at a baby steps level here. But I can imagine that

1:25:06.960 --> 1:25:12.960
 in the decades to come as what's the full flourishing of that line of thinking? It's

1:25:12.960 --> 1:25:17.200
 going to be truly incredible, I would say. If I visualize this tree of knowledge,

1:25:17.200 --> 1:25:23.600
 something tells me that that tree of knowledge for humans is much smaller. In a set of all

1:25:23.600 --> 1:25:29.680
 possible trees of knowledge, it's actually quite small, given our cognitive limitations,

1:25:31.600 --> 1:25:36.480
 limited cognitive capabilities, that even with the tools we build, we still won't be able to

1:25:36.480 --> 1:25:42.000
 understand a lot of things. And that's perhaps what non human systems might be able to reach

1:25:42.000 --> 1:25:47.840
 further, not just as tools, but in themselves, understanding something that they can bring

1:25:47.840 --> 1:25:53.920
 back. Yeah, it could well be. So I mean, there's so many things that are sort of encapsulated

1:25:53.920 --> 1:25:58.480
 in what you just said there. I think first of all, there's two different things that's like,

1:25:58.480 --> 1:26:04.160
 what do we understand today? What could the human mind understand? And what is the totality of what

1:26:04.160 --> 1:26:10.000
 is there to be understood? And so there's three concentric, you can think of them as three larger

1:26:10.000 --> 1:26:14.400
 and larger trees or exploring more branches of that tree. And I think with AI, we're going to

1:26:14.400 --> 1:26:20.640
 explore that whole lot. Now, the question is, if you think about what is the totality of what

1:26:20.640 --> 1:26:25.760
 could be understood, there may be some fundamental physics reasons why certain things can't be

1:26:25.760 --> 1:26:30.560
 understood, like what's outside a simulation or outside the universe. Maybe it's not understandable

1:26:30.560 --> 1:26:35.440
 from within the universe. So there may be some hard constraints like that. It could be smaller

1:26:35.440 --> 1:26:42.880
 constraints. We think of space time as fundamental. Our human brains are really used to this idea of

1:26:42.880 --> 1:26:47.680
 a three dimensional world with time. Right. Maybe. But our tools could go beyond that.

1:26:47.680 --> 1:26:51.680
 They wouldn't have that limitation necessary. They could think in 11 dimensions, 12 dimensions,

1:26:51.680 --> 1:26:56.640
 whatever is needed. But we could still maybe understand that in several different ways.

1:26:56.640 --> 1:27:02.240
 The example I always give is when I play Gary Kasparov at Speed Chess or we've talked about

1:27:02.240 --> 1:27:09.680
 chess and these kind of things, if you're reasonably good at chess, you can't come up with

1:27:09.680 --> 1:27:13.920
 the move Gary comes up with in his move, but he can explain it to you. And you can understand.

1:27:13.920 --> 1:27:19.280
 And you can understand post hoc the reasoning. So I think there's an even further level of like,

1:27:19.280 --> 1:27:24.160
 well, maybe you couldn't have invented that thing, but going back to using language again,

1:27:24.160 --> 1:27:30.080
 perhaps you can understand and appreciate that. Same way, you can appreciate Vivaldi or Mozart

1:27:30.080 --> 1:27:35.040
 or something without, you can appreciate the beauty of that without being able to construct it

1:27:35.040 --> 1:27:39.200
 yourself, right? Invent the music yourself. So I think we see this in all forms of life.

1:27:39.200 --> 1:27:44.800
 So it'll be that times, you know, a million. But it would you can imagine also one sign of

1:27:44.800 --> 1:27:49.680
 intelligence is the ability to explain things clearly and simply, right? You know, people like

1:27:49.680 --> 1:27:52.960
 Richard Feynman, another one of my all time heroes used to say that, right? If you can't,

1:27:52.960 --> 1:27:57.680
 you know, if you can explain it something simply, then that's the best sign, a complex

1:27:57.680 --> 1:28:02.160
 topic simply, then that's one of the best signs of you understanding it. So I can see myself

1:28:02.160 --> 1:28:08.880
 talking trash in the AI system in that way. It gets frustrated how dumb I am in trying to explain

1:28:08.880 --> 1:28:12.560
 something to me. I was like, well, that means you're not intelligent, because if you were intelligent,

1:28:12.560 --> 1:28:16.320
 you'd be able to explain it simply. Yeah, of course, as you know, there's also the other

1:28:16.320 --> 1:28:21.040
 option, of course, we could enhance ourselves and with our devices, we are already sort of

1:28:21.040 --> 1:28:25.200
 symbiotic with our compute devices, right? With our phones and other things. And, you know,

1:28:25.200 --> 1:28:29.120
 there's stuff like neural link and accepture that could be could could advance that further.

1:28:29.920 --> 1:28:35.200
 So I think there's lots of lots of really amazing possibilities that I could foresee from here.

1:28:35.200 --> 1:28:39.120
 Well, let me ask you some wild questions. So out there, looking for friends,

1:28:39.840 --> 1:28:42.320
 do you think there's a lot of alien civilizations out there?

1:28:43.040 --> 1:28:47.760
 So I guess this also goes back to your origin of life question too, because I think that that's key.

1:28:49.280 --> 1:28:53.680
 My personal opinion, looking at all this and, you know, it's one of my hobbies, physics, I guess.

1:28:53.680 --> 1:28:59.760
 So I, you know, it's something I think about a lot and talk to a lot of experts on and read a

1:28:59.760 --> 1:29:06.240
 lot of books on. And I think my feeling currently is that we are alone. I think that's the most

1:29:06.240 --> 1:29:13.120
 likely scenario given what evidence we have. So, and the reasoning is, I think that, you know,

1:29:13.120 --> 1:29:19.040
 we've tried since things like SETI program, and I guess since the dawning of the space age,

1:29:19.040 --> 1:29:24.640
 we've, you know, had telescopes, open radio telescopes and other things. And if you think about

1:29:25.760 --> 1:29:29.680
 and try to detect signals, now, if you think about the evolution of humans on Earth,

1:29:30.240 --> 1:29:36.240
 we could have easily been a million years ahead of our time now, or million years behind,

1:29:36.240 --> 1:29:41.280
 right easily, with just some slightly different quirk thing happening hundreds of thousands years

1:29:41.280 --> 1:29:45.360
 ago, you know, things could have been slightly different. If the meteor would hit the dinosaurs

1:29:45.360 --> 1:29:50.080
 a million years earlier, maybe things would have evolved, we'd be a million years ahead of where

1:29:50.080 --> 1:29:55.440
 we are now. So what that means is, if you imagine where humanity will be in a few hundred years,

1:29:55.440 --> 1:30:00.800
 let alone a million years, especially if we hopefully, you know, solve things like climate

1:30:00.800 --> 1:30:05.840
 change and other things, and we continue to flourish, and we build things like AI, and we

1:30:05.840 --> 1:30:11.360
 do space traveling, and all of the stuff that humans have dreamed of forever, right, and sci fi

1:30:11.360 --> 1:30:18.160
 has talked about forever. We will be spreading across the stars, right, and von Neumann famously

1:30:18.160 --> 1:30:22.240
 calculated, you know, it would only take about a million years if you send out von Neumann probes

1:30:22.240 --> 1:30:27.840
 to the nearest, you know, the nearest other solar systems, and then they built, all they did was

1:30:27.840 --> 1:30:31.440
 built two more versions of themselves and set those two out to the next nearest systems.

1:30:32.080 --> 1:30:35.760
 You, you know, within a million years, I think you would have one of these probes in every system

1:30:35.760 --> 1:30:41.120
 in the galaxy. So it's not actually in cosmological time, that's actually a very short amount of

1:30:41.120 --> 1:30:46.640
 time. So, and, you know, we people like Dyson have thought about constructing Dyson spheres around

1:30:46.640 --> 1:30:51.360
 stars to collect all the energy coming out of the star, you know, that there would be constructions

1:30:51.360 --> 1:30:57.280
 like that would be visible across space, probably even across a galaxy. So, and then, you know, if

1:30:57.280 --> 1:31:02.400
 you think about all of our radio, television, emissions that have gone out since, since the,

1:31:02.400 --> 1:31:09.360
 you know, 30s and 40s, imagine a million years of that, and now hundreds of civilizations doing

1:31:09.360 --> 1:31:14.880
 that. When we opened our ears, at the point we got technologically sophisticated enough in the

1:31:14.880 --> 1:31:20.400
 space age, we should have heard a cacophony of voices. We should have joined that cacophony of

1:31:20.400 --> 1:31:25.920
 voices. And what, what we did, we open our ears and we heard nothing. And many people who argue

1:31:25.920 --> 1:31:30.560
 that there are aliens would say, well, we haven't really done exhaustive search yet. And maybe we're

1:31:30.560 --> 1:31:35.280
 looking in the wrong bands and, and we've got the wrong devices and we wouldn't notice what an alien

1:31:35.280 --> 1:31:40.480
 form was like to be so different to what we're used to. But, you know, I don't really buy that,

1:31:40.480 --> 1:31:44.080
 that it shouldn't be as difficult as that. Like we, I think we've searched enough.

1:31:44.080 --> 1:31:45.520
 There should be everywhere.

1:31:45.520 --> 1:31:49.120
 If it was, it should be everywhere. We should see Dyson spheres being put up,

1:31:49.120 --> 1:31:52.800
 suns blinking in and out. You know, there should be a lot of evidence for those things.

1:31:52.800 --> 1:31:56.560
 And then there are other people who argue, well, the sort of safari view of like, well, we're a

1:31:56.560 --> 1:32:00.400
 primitive species still because we're not space faring yet. And, and, and we're, you know, there's

1:32:00.400 --> 1:32:05.440
 some kind of globe, like universal rule not to interfere your Star Trek rule. But like, look,

1:32:05.440 --> 1:32:10.240
 we can't even coordinate humans to deal with climate change. And we're one species. What,

1:32:10.240 --> 1:32:13.680
 what is the chance that of all of these different human civilization, you know,

1:32:13.680 --> 1:32:18.560
 alien civilizations, they would have the same priorities and, and, and agree or cross the,

1:32:18.560 --> 1:32:23.600
 you know, these kind of matters. And even if that was true, and we were in some sort of safari

1:32:23.600 --> 1:32:28.000
 for our own good, to me, that's not much different from the simulation hypothesis. Because what does

1:32:28.000 --> 1:32:32.000
 it mean, the simulation hypothesis? I think in its most fundamental level, it means what we're

1:32:32.000 --> 1:32:37.600
 seeing is not quite reality, right? It's something, there's something more deeper underlying it,

1:32:37.600 --> 1:32:43.120
 maybe computational. Now, if we were in a, if we were in a sort of safari park, and everything we

1:32:43.120 --> 1:32:47.280
 were seeing was a hologram, and it was projected by the aliens or whatever, that to me is not much

1:32:47.280 --> 1:32:52.560
 different than thinking we're inside of another universe, because we still can't see true reality,

1:32:52.560 --> 1:32:55.840
 right? I mean, there's, there's other explanations. It could be that

1:32:55.840 --> 1:33:00.160
 that the way they're communicating is just fundamentally different, that we're too dumb to

1:33:00.160 --> 1:33:05.760
 understand the much better methods of communication they have. It could be, I mean, I mean, it's

1:33:05.760 --> 1:33:11.760
 silly to say, but our own thoughts could be the methods by which they're communicating. Like,

1:33:11.760 --> 1:33:15.360
 the place from which our ideas, writers talk about this, like the muse. Yeah.

1:33:16.960 --> 1:33:23.840
 I mean, it sounds like very kind of wild, but it could be thoughts, it could be some interactions

1:33:23.840 --> 1:33:31.840
 with our mind that we think are originating from us is actually something that is coming from other

1:33:31.840 --> 1:33:36.880
 life forms elsewhere. Consciousness itself might be that. It could be, but I don't see any sensible

1:33:36.880 --> 1:33:42.080
 argument to the why, why would all of the alien species behave in this way? Yeah, some of them

1:33:42.080 --> 1:33:46.000
 will be more primitive, they will be close to our level. You know, there would, there should be a

1:33:46.000 --> 1:33:50.080
 whole sort of normal distribution of these things, right? Some would be aggressive, some would be,

1:33:50.080 --> 1:33:55.840
 but, you know, curious, others would be very stoical and philosophical, because, you know,

1:33:55.840 --> 1:34:00.960
 maybe they're a million years older than us. But it's not, it shouldn't be like, I mean, one,

1:34:00.960 --> 1:34:04.480
 one alien civilization might be like that, communicating thoughts and others, but I don't

1:34:04.480 --> 1:34:09.840
 see why, you know, potentially the hundreds there should be would be uniform in this way, right?

1:34:09.840 --> 1:34:14.320
 It could be a violent dictatorship that the people, the alien civilizations that

1:34:14.320 --> 1:34:24.080
 that become successful, become gain the ability to be destructive in order of magnitude more

1:34:24.080 --> 1:34:34.000
 destructive. But of course, the sad thought, well, either humans are very special. We took a lot of

1:34:34.000 --> 1:34:40.960
 leaps that arrived at what it means to be human. There's a question there, which was the hardest,

1:34:40.960 --> 1:34:46.080
 which was the most special. But also, if others have reached this level, and maybe many others

1:34:46.080 --> 1:34:53.440
 have reached this level, the great filter that prevented them from going farther to becoming

1:34:53.440 --> 1:34:59.600
 a multiplayer species are reaching out into the stars. And those are really important questions

1:34:59.600 --> 1:35:05.440
 for us, whether, whether there's other alien civilizations out there or not, this is very

1:35:05.440 --> 1:35:12.160
 useful for us to think about. If we destroy ourselves, how will we do it? And how easy is it to do?

1:35:12.160 --> 1:35:16.240
 Yeah. Well, you know, these are big questions. And I've thought about these a lot. But the

1:35:16.240 --> 1:35:21.600
 interesting thing is that if we're, if we're alone, that's somewhat comforting from the great filter

1:35:21.600 --> 1:35:26.240
 perspective, because it probably means the great filters were passed us. And I'm pretty sure they

1:35:26.240 --> 1:35:31.120
 are. So going back to your origin of life question, there are some incredible things that no one

1:35:31.120 --> 1:35:36.960
 knows how happened. Like obviously, the first life form from chemical soup, that seems pretty hard.

1:35:36.960 --> 1:35:42.560
 But I would guess the multicellular, I wouldn't be that surprised if we saw single cell sort of

1:35:42.560 --> 1:35:48.160
 life forms elsewhere, bacteria type things. But multicellular life seems incredibly hard,

1:35:48.160 --> 1:35:52.960
 that step of, you know, capturing mitochondria and then sort of using that as part of yourself,

1:35:52.960 --> 1:35:56.960
 you know, when you've just eaten it. Would you say that's the biggest, the most,

1:35:56.960 --> 1:36:04.960
 like, if you had to choose one, sort of Hitchhiker's Galaxy, one sentence summary of like, oh,

1:36:04.960 --> 1:36:08.400
 those clever creatures did this, there would be the multicellular.

1:36:08.400 --> 1:36:11.760
 I think that was probably the one that's the biggest. I mean, there's a great book called

1:36:11.760 --> 1:36:17.200
 The 10 Great Inventions of Evolution by Nick Lane, and he speculates on 10, 10 of these,

1:36:17.200 --> 1:36:22.560
 you know, what could be great filters. I think that's one, I think the, the advent of, of, of

1:36:22.560 --> 1:36:27.360
 intelligence and, and conscious intelligence and in order, you know, to us to be able to do science

1:36:27.360 --> 1:36:32.080
 and things like that is huge as well. I mean, there's only evolved once as far as, you know,

1:36:32.720 --> 1:36:38.240
 in, in, in Earth history. So that would be a later candidate. But there's certainly for the

1:36:38.240 --> 1:36:41.200
 early candidates, I think multicellular life forms is huge.

1:36:41.200 --> 1:36:46.720
 By the way, what it's interesting to ask you, if you can hypothesize about what is the origin of

1:36:46.720 --> 1:36:55.520
 intelligence? Is it that we started cooking meat over fire? Is it that we somehow figured out that

1:36:55.520 --> 1:37:02.880
 we could be very powerful when we started collaborating? So cooperation between our ancestors

1:37:03.440 --> 1:37:08.320
 so that we can overthrow the alpha male? What is it, Richard? I talked to Richard

1:37:08.320 --> 1:37:12.800
 Randham, who thinks we're all just beta males who figured out how to collaborate to defeat

1:37:12.800 --> 1:37:18.240
 the one, the dictator, the authoritarian alpha male that controlled the tribe.

1:37:19.040 --> 1:37:25.360
 Is there other explanation? Was there a 2001 space obviously type of monolith that came down to

1:37:25.360 --> 1:37:30.080
 Earth? Well, I think, I think all of those things you suggested are good candidates, fire and, and,

1:37:30.080 --> 1:37:35.520
 and cooking, right? So that's clearly important for energy, you know, energy efficiency,

1:37:36.080 --> 1:37:41.040
 cooking our meat and then, and then being able to, to, to be more efficient about eating it and

1:37:41.040 --> 1:37:46.480
 getting, consuming the energy. I think that's huge. And then utilizing fire and tools. I think

1:37:46.480 --> 1:37:50.560
 you're right about the tribal cooperation aspects and probably language as part of that.

1:37:51.440 --> 1:37:55.680
 Because probably that's what allowed us to outcompete Neanderthals and perhaps less cooperative

1:37:55.680 --> 1:38:03.120
 species. So, so that may be the case. Toolmaking, spears, axes, I think that let us, I mean,

1:38:03.120 --> 1:38:06.720
 I think it's pretty clear now that humans were responsible for a lot of the extinctions of

1:38:06.720 --> 1:38:13.600
 megafauna, especially in, in, in the Americas when humans arrived. So you can imagine, once you

1:38:13.600 --> 1:38:18.640
 discover tool usage, how powerful that would have been and how scary for animals. So I think all of

1:38:18.640 --> 1:38:23.120
 those could have been explanations for it. You know, the interesting thing is that it's a bit

1:38:23.120 --> 1:38:28.960
 like general intelligence too, is it's very costly to begin with, to have a brain, and especially

1:38:28.960 --> 1:38:32.480
 a general purpose brain rather than a special purpose one, because you have energy our brains

1:38:32.480 --> 1:38:36.720
 use. I think it's like 20% of the body's energy. And it's, it's massive. And when you're thinking

1:38:36.720 --> 1:38:41.920
 chess, one of the funny things that, that we used to say is it's as much as a racing driver uses

1:38:41.920 --> 1:38:46.000
 for a whole, you know, Formula One race, just playing a game of, you know, serious high level

1:38:46.000 --> 1:38:50.480
 chess, which we know you wouldn't think just sitting there, because the brain's using so much

1:38:51.200 --> 1:38:57.680
 energy. So in order for an animal and organism to justify that, there has to be a huge payoff.

1:38:57.680 --> 1:39:03.920
 And the problem with, with half a brain, or half, you know, intelligence, say an IQs of, you know,

1:39:05.200 --> 1:39:10.880
 of like a monkey brain, it's, it's not clear you can justify that evolutionary until you get to

1:39:10.880 --> 1:39:15.200
 the human level brain. And so, but how do you, how do you do that jump? It's very difficult,

1:39:15.200 --> 1:39:18.880
 which is why I think it's only been done once from the sort of specialized brains that you see

1:39:18.880 --> 1:39:24.880
 in animals, to this sort of general purpose, chewing powerful brains that humans have.

1:39:24.880 --> 1:39:31.440
 And, which allows us to invent the modern, modern world. And, you know, it takes a lot to, to cross

1:39:31.440 --> 1:39:36.480
 that barrier. And I think we've seen the same with AI systems, which is that maybe until very

1:39:36.480 --> 1:39:41.280
 recently, it's always been easier to craft a specific solution to a problem like chess,

1:39:41.280 --> 1:39:45.120
 than it has been to build a general learning system that could potentially do many things.

1:39:45.120 --> 1:39:51.120
 Because initially, that system will be way worse than less efficient than the specialized system.

1:39:51.120 --> 1:39:59.040
 So one of the interesting quirks of the human mind of this evolved system is that it appears to be

1:39:59.040 --> 1:40:05.520
 conscious. This thing that we don't quite understand, but it seems very, very special,

1:40:05.520 --> 1:40:11.520
 its ability to have a subjective experience that it feels like something to eat a cookie,

1:40:11.520 --> 1:40:16.480
 the deliciousness of it, or see a color and that kind of stuff. Do you think in order to solve

1:40:16.480 --> 1:40:22.160
 intelligence, we also need to solve consciousness along the way? Do you think AI systems need to

1:40:22.160 --> 1:40:26.960
 have consciousness in order to be truly intelligent?

1:40:26.960 --> 1:40:32.880
 Yeah, we thought about this a lot actually. And I think that my guess is that consciousness and

1:40:32.880 --> 1:40:38.240
 intelligence are double dissociable. So you can have one without the other both ways. And I think

1:40:38.240 --> 1:40:44.240
 you can see that with consciousness in that, I think some animals, pets, if you have a pet dog,

1:40:44.240 --> 1:40:48.960
 or something like that, you can see some of the higher animals and dolphins, things like that,

1:40:50.080 --> 1:40:58.880
 have self awareness and are very sociable, seem to dream. A lot of the traits one would regard

1:40:58.880 --> 1:41:05.280
 as being kind of conscious and self aware. But yet they're not that smart, right? So they're

1:41:05.280 --> 1:41:08.800
 not that intelligent by, say, IQ standards or something like that.

1:41:08.800 --> 1:41:12.160
 Yeah, it's also possible that our understanding of intelligence is flawed,

1:41:12.160 --> 1:41:19.440
 like putting an IQ to it. Maybe the thing that a dog can do is actually go on a very far along

1:41:19.440 --> 1:41:24.640
 the path of intelligence and we humans are just able to play chess and maybe write poems.

1:41:24.640 --> 1:41:29.200
 Right. But if we go back to the idea of AGI and general intelligence, dogs are very specialized,

1:41:29.200 --> 1:41:32.240
 right? Most animals are pretty specialized. They can be amazing at what they do,

1:41:32.240 --> 1:41:36.640
 but they're like kind of elite sports people or something, right? So they do one thing

1:41:37.200 --> 1:41:39.840
 extremely well because their entire brain is optimized.

1:41:39.840 --> 1:41:44.400
 They have somehow convinced the entirety of the human population to feed them and service them.

1:41:44.400 --> 1:41:48.800
 So in some way, they're controlling. Yes, exactly. Well, we co evolved to some crazy

1:41:48.800 --> 1:41:54.320
 degree, right? Including the way the dogs, you know, even wag their tails and twitch their

1:41:54.320 --> 1:42:01.440
 noses, right? We find it inexorably cute. But I think you can also see intelligence on the other

1:42:01.440 --> 1:42:07.520
 side. So systems like artificial systems that are amazingly smart at certain things like maybe

1:42:07.520 --> 1:42:13.360
 playing go in chess and other things. But they don't feel at all in any shape or form conscious

1:42:13.360 --> 1:42:22.880
 in the way that you do to me or I do to you. And I think actually building AI is these intelligent

1:42:22.880 --> 1:42:27.840
 constructs is one of the best ways to explore the mystery of consciousness to break it down.

1:42:27.840 --> 1:42:34.960
 Because we're going to have devices that are pretty smart at certain things or capable at

1:42:34.960 --> 1:42:40.640
 certain things, but potentially won't have any semblance of self awareness or other things.

1:42:40.640 --> 1:42:45.520
 And in fact, I would advocate if there's a choice, building systems in the first place,

1:42:45.520 --> 1:42:51.680
 AI systems that are not conscious to begin with are just tools until we understand them better

1:42:52.320 --> 1:42:57.920
 and the capabilities better. So on that topic, just not as the CEO of DeepMind,

1:42:59.200 --> 1:43:04.000
 just as a human being, let me ask you about this one particular anecdotal evidence of the Google

1:43:04.000 --> 1:43:10.880
 engineer who made a comment or believed that there's some aspect of a language model,

1:43:11.680 --> 1:43:17.600
 the Lambda language model that exhibited sentience. So you said you believe there might be a

1:43:17.600 --> 1:43:22.800
 responsibility to build systems that are not sentient. And this experience of a particular

1:43:22.800 --> 1:43:27.280
 engineer, I think I'd love to get your general opinion on this kind of thing, but I think it

1:43:27.280 --> 1:43:32.640
 will happen more and more and more, which not one engineers, but when people out there that

1:43:32.640 --> 1:43:37.520
 don't have an engineer background start interacting with increasingly intelligent systems, we

1:43:37.520 --> 1:43:45.280
 anthropomorphize them, they start to have deep impactful interactions with us in a way that

1:43:45.280 --> 1:43:51.920
 we miss them when they're gone. And we sure as heck feel like they're living entities,

1:43:51.920 --> 1:43:57.440
 self aware entities, and maybe even we project sentience onto them. So what's your thought about

1:43:57.440 --> 1:44:03.760
 this particular system? Have you ever met a language model that's sentient?

1:44:04.480 --> 1:44:11.760
 No. And what do you make of the case of when you feel that there's some elements of sentience to

1:44:11.760 --> 1:44:17.600
 this system? Yeah, so this is an interesting question and obviously a very fundamental one.

1:44:17.600 --> 1:44:22.640
 So the first thing to say is I think that none of the systems we have today, I would say even have

1:44:22.640 --> 1:44:28.640
 one iota of semblance of consciousness or sentience, that's my personal feeling interacting with them

1:44:28.640 --> 1:44:34.000
 every day. So I think this way premature to be discussing what that engineer talked about.

1:44:34.000 --> 1:44:38.720
 I think at the moment, it's more of a projection of other way our own minds work, which is to see

1:44:41.120 --> 1:44:46.080
 sort of purpose and direction in almost anything that we, our brains are trained to interpret

1:44:46.080 --> 1:44:54.720
 agency basically in things, even inanimate things sometimes. And of course, with a language system

1:44:54.720 --> 1:44:58.960
 because language is so fundamental to intelligence that's going to be easy for us to anthropomorphize

1:44:58.960 --> 1:45:05.680
 that. I mean, back in the day, even the first, you know, the dumbest sort of template chatbots ever,

1:45:05.680 --> 1:45:11.440
 Eliza and the ilk of the original chatbots back in the 60s fooled some people under certain

1:45:11.440 --> 1:45:16.080
 circumstances, right, it pretended to be a psychologist. So we just basically wrap it back to you the

1:45:16.080 --> 1:45:22.560
 same question you asked it back to you. And some people believe that. So I don't think we can,

1:45:22.560 --> 1:45:26.240
 this is why I think the truing test is a little bit flawed as a formal test because it depends on

1:45:26.240 --> 1:45:33.120
 the sophistication of the of the judge, whether or not they are qualified to make that distinction.

1:45:33.120 --> 1:45:39.360
 So I think we should talk to, you know, the top philosophers about this people like Daniel Dennett

1:45:39.360 --> 1:45:43.920
 and David Chalmers and others who've obviously thought deeply about consciousness. Of course,

1:45:43.920 --> 1:45:49.600
 consciousness itself hasn't been well, there's no agreed definition. If I was to, you know,

1:45:49.600 --> 1:45:54.960
 speculate about that, you know, I kind of the definite the working definition I like is,

1:45:54.960 --> 1:45:59.120
 it's the way information feels when, you know, it gets processed, I think maybe Max Tegmark

1:45:59.120 --> 1:46:03.200
 came up with that. I like that idea. I don't know if it helps us get towards any more operational

1:46:03.200 --> 1:46:09.120
 thing. But it's, I think it's a nice way of viewing it. I think we can obviously see from

1:46:09.120 --> 1:46:14.240
 neuroscience certain prerequisites that require like self awareness, I think is necessary,

1:46:14.240 --> 1:46:20.320
 but not sufficient component, this idea of a self and other and set of coherent preferences

1:46:20.320 --> 1:46:25.760
 that are coherent over time. You know, these things are maybe memory. These things are probably

1:46:25.760 --> 1:46:31.520
 needed for a sentient or conscious being. But the reason that the difficult thing I think for

1:46:31.520 --> 1:46:35.680
 us when we get, and I think this is a really interesting philosophical debate, is when we get

1:46:35.680 --> 1:46:43.040
 closer to AGI and, you know, and much more powerful systems than we have today, how are we going to

1:46:43.040 --> 1:46:48.560
 make this judgment? And one way, which is the Turing test is sort of a behavioral judgment,

1:46:48.560 --> 1:46:56.080
 is the system exhibiting all the behaviors that a human sentient or sentient being would exhibit?

1:46:56.720 --> 1:47:00.400
 Is it answering the right questions? Is it saying the right things? Is it indistinguishable from a

1:47:00.400 --> 1:47:08.320
 human? And so on. But I think there's a second thing that makes us as humans regard each other

1:47:08.320 --> 1:47:12.560
 as sentient, right? Why do we, why do we think this? And I debated this with Daniel Dennett.

1:47:12.560 --> 1:47:16.640
 And I think there's a second reason that's often overlooked, which is that we're running on the

1:47:16.640 --> 1:47:22.480
 same substrate, right? So if we're exhibiting the same behavior, more or less as humans,

1:47:22.480 --> 1:47:26.960
 and we're running on the same, you know, carbon based biological substrate, the squishy, you know,

1:47:26.960 --> 1:47:33.040
 a few pounds of flesh in our skulls, then the most parsimonious, I think, explanation is that

1:47:33.040 --> 1:47:37.680
 you're feeling the same thing as I'm feeling, right? But we will never have that second part,

1:47:37.680 --> 1:47:43.840
 the substrate equivalence with a machine, right? So we will have to only judge based on the behavior.

1:47:43.840 --> 1:47:48.240
 And I think the substrate equivalence is a critical part of why we make assumptions that

1:47:48.240 --> 1:47:52.560
 we're conscious. And in fact, even with animals, high level animals, why we think they might be,

1:47:52.560 --> 1:47:56.160
 because they're exhibiting some of the behaviors we would expect from a sentient animal. And we

1:47:56.160 --> 1:47:59.920
 know they're made of the same things, biological neurons. So we're going to have to come up with

1:48:01.120 --> 1:48:07.440
 explanations or models of the gap between substrate differences between machines and humans

1:48:08.560 --> 1:48:12.960
 to get anywhere beyond the behavioral. But to me, sort of the practical question is

1:48:13.840 --> 1:48:19.040
 very interesting and very important. When you have millions, perhaps billions of people believing

1:48:19.040 --> 1:48:25.280
 that you have a sentient AI, believing what that Google engineer believed, which I just see as an

1:48:25.280 --> 1:48:32.400
 obvious, very near term future thing, certainly on the path to AGI, how does that change the world?

1:48:32.960 --> 1:48:36.720
 What's the responsibility of the AI system to help those millions of people?

1:48:38.000 --> 1:48:46.800
 And also, what's the ethical thing? Because you can make a lot of people happy by creating a meaningful,

1:48:46.800 --> 1:48:58.320
 deep experience with a system that's faking it before it makes it. Who is to say what's the

1:48:58.320 --> 1:49:06.480
 right thing to do? Should AI always be tools? Why are we constraining AI to always be tools as opposed

1:49:06.480 --> 1:49:14.640
 to friends? I think, well, these are fantastic questions and also critical ones. And we've

1:49:14.640 --> 1:49:19.680
 been thinking about this since the start of DeepMind and before that, because we planned for success

1:49:19.680 --> 1:49:26.240
 and have a remote that looked like back in 2010. And we've always had sort of these ethical

1:49:26.240 --> 1:49:32.560
 considerations as fundamental at DeepMind. And my current thinking on the language models and

1:49:32.560 --> 1:49:37.760
 large models is they're not ready. We don't understand them well enough yet. And in terms

1:49:37.760 --> 1:49:43.920
 of analysis tools and guardrails, what they can and can't do and so on to deploy them at scale,

1:49:43.920 --> 1:49:48.400
 because I think there are big, still ethical questions like, should an AI system always

1:49:48.400 --> 1:49:54.400
 announce that it is an AI system to begin with? Probably yes. What do you do about answering

1:49:54.400 --> 1:49:58.960
 those philosophical questions about the feelings people may have about AI systems,

1:49:58.960 --> 1:50:03.840
 perhaps incorrectly attributed? So I think there's a whole bunch of research that needs to be done

1:50:03.840 --> 1:50:10.320
 first. You can responsibly deploy these systems at scale. That will at least be my

1:50:10.320 --> 1:50:16.320
 current position. Over time, I'm very confident we'll have those tools, like interpretability

1:50:16.320 --> 1:50:24.080
 questions and analysis questions. And then with the ethical quandary, I think there,

1:50:24.960 --> 1:50:31.600
 it's important to look beyond just science. That's why I think philosophy, social sciences,

1:50:31.600 --> 1:50:38.320
 even theology, other things like that come into it, where arts and humanities, what does it mean

1:50:38.320 --> 1:50:44.000
 to be human and the spirit of being human and to enhance that and the human condition and allow

1:50:44.000 --> 1:50:48.880
 us to experience things we could never experience before and improve the overall human condition

1:50:48.880 --> 1:50:53.920
 and humanity overall, get radical abundance, solve many scientific problems, solve disease.

1:50:53.920 --> 1:50:58.240
 So this is the era I think, this is the amazing era I think we're heading into if we do it right.

1:50:59.280 --> 1:51:03.440
 But we've got to be careful. We've already seen with things like social media how dual use

1:51:03.440 --> 1:51:12.480
 technologies can be misused by firstly by bad actors or naive actors or crazy actors. So that's

1:51:12.480 --> 1:51:18.640
 that set of just the common or garden misuse of existing dual use technology. And then of course,

1:51:18.640 --> 1:51:23.600
 there's an additional thing that has to be overcome with AI that eventually it may have its own

1:51:23.600 --> 1:51:31.440
 agency. So it could be good or bad in itself. So I think these questions have to be approached

1:51:31.440 --> 1:51:37.040
 very carefully using the scientific method, I would say, in terms of hypothesis generation,

1:51:37.040 --> 1:51:42.320
 careful control testing, not live AB testing out in the world. Because with powerful dual

1:51:42.320 --> 1:51:49.040
 technologies like AI, if something goes wrong, it may cause a lot of harm before you can fix it.

1:51:49.040 --> 1:51:55.760
 It's not like an imaging app or game app where if something goes wrong, it's relatively easy to

1:51:55.760 --> 1:52:03.120
 fix and the harm is relatively small. So I think it comes with the usual cliche of like with a lot

1:52:03.120 --> 1:52:08.240
 of power comes a lot of responsibility. And I think that's the case here with things like AI given

1:52:08.240 --> 1:52:15.120
 the enormous opportunity in front of us. And I think we need a lot of voices and as many inputs

1:52:15.120 --> 1:52:20.560
 into things like the design of the systems and the values they should have and what goals should

1:52:20.560 --> 1:52:26.640
 they be put to, I think as wide a group of voices as possible beyond just the technologist is needed

1:52:26.640 --> 1:52:30.880
 to input into that and to have a say in that, especially when it comes to deployment of these

1:52:30.880 --> 1:52:34.880
 systems, which is when the rubber really hits the road, it really affects the general person

1:52:34.880 --> 1:52:40.160
 in the street rather than fundamental research. And that's why I say, I think as a first step,

1:52:40.160 --> 1:52:45.920
 it would be better if we have the choice to build these systems as tools to give and I'm not saying

1:52:45.920 --> 1:52:50.640
 that they should never go beyond tools because of course the potential is there for it to go

1:52:50.640 --> 1:52:57.840
 way beyond just tools. But I think that would be a good first step in order for us to allow us to

1:52:57.840 --> 1:53:02.800
 carefully experiment and understand what these things can do. So the leap between tool,

1:53:02.800 --> 1:53:09.760
 the sentient entity being as well should take very careful. Yes. Let me ask a dark personal

1:53:09.760 --> 1:53:14.720
 question. So you're one of the most brilliant people in the AI community. You're also one of the

1:53:14.720 --> 1:53:25.120
 most kind and if I may say sort of loved people in the community, that said, creation of a super

1:53:25.120 --> 1:53:33.840
 intelligent AI system would be one of the most powerful things in the world, tools or otherwise.

1:53:34.720 --> 1:53:40.320
 And again, as the old saying goes, power corrupts and absolute power corrupts, absolutely.

1:53:40.320 --> 1:53:50.800
 You are likely to be one of the people, I would say probably the most likely person to be in the

1:53:50.800 --> 1:53:57.920
 control of such a system. Do you think about the corrupting nature of power when you talk about

1:53:57.920 --> 1:54:05.360
 these kinds of systems that as all dictators and people have caused atrocities in the past,

1:54:05.360 --> 1:54:11.520
 always think they're doing good, but they don't do good because the power has polluted their mind

1:54:11.520 --> 1:54:15.920
 about what is good and what is evil. Do you think about this stuff or we just focus on language

1:54:15.920 --> 1:54:22.800
 model? No, I think about them all the time. And I think what are the defenses against that? I think

1:54:22.800 --> 1:54:28.640
 one thing is to remain very grounded and sort of humble no matter what you do or achieve.

1:54:28.640 --> 1:54:33.680
 And I try to do that. My best friends are still my set of friends from my undergraduate Cambridge

1:54:33.680 --> 1:54:41.440
 days. My families and friends are very important. I've always, I think trying to be a multidisciplinary

1:54:41.440 --> 1:54:45.760
 person, it helps to keep you humble because no matter how good you are at one topic,

1:54:45.760 --> 1:54:50.880
 someone will be better than you at that. And always relearning a new topic again from scratch

1:54:50.880 --> 1:54:55.680
 is or new field is very humbling. So for me, that's been biology over the last five years.

1:54:56.320 --> 1:55:02.480
 Huge area topic and I just love doing that, but it helps to keep you grounded and keeps you open

1:55:02.480 --> 1:55:08.560
 minded. And then the other important thing is to have a really group, amazing set of

1:55:09.120 --> 1:55:13.760
 people around you at your company or your organization who are also very ethical and

1:55:13.760 --> 1:55:18.400
 grounded themselves and help to keep you that way. And then ultimately, just to answer your

1:55:18.400 --> 1:55:23.760
 question, I hope we're going to be a big part of birthing AI and that being the greatest benefit

1:55:23.760 --> 1:55:29.920
 to humanity of any tool or technology ever and getting us into a world of radical abundance and

1:55:29.920 --> 1:55:35.120
 curing diseases and solving many of the big challenges we have in front of us and then

1:55:35.120 --> 1:55:39.920
 ultimately help the ultimate flourishing of humanity to travel the stars and find those

1:55:39.920 --> 1:55:43.920
 aliens if they are there. And if they're not there, find out why they're not there, what is

1:55:43.920 --> 1:55:49.120
 going on here in the universe. This is all to come and that's what I've always dreamed about.

1:55:50.560 --> 1:55:56.880
 But I think AI is too big an idea. There'll be a certain set of pioneers who get there first.

1:55:56.880 --> 1:56:03.920
 I hope we're in the vanguard so we can influence how that goes. And I think it matters which cultures

1:56:03.920 --> 1:56:08.240
 they come from and what values they have, the builders of AI systems. Because I think even

1:56:08.240 --> 1:56:12.960
 though the AI system is going to learn for itself, most of its knowledge, there'll be a residue in

1:56:12.960 --> 1:56:18.160
 the system of the culture and the values of the creators of that system. And there's interesting

1:56:18.160 --> 1:56:23.440
 questions to discuss about that geopolitically, different cultures as we're in a more fragmented

1:56:23.440 --> 1:56:28.480
 world than ever. Unfortunately, I think in terms of global cooperation, we see that in things

1:56:28.480 --> 1:56:33.280
 like climate where we can't seem to get our act together globally to cooperate on these pressing

1:56:33.280 --> 1:56:38.480
 matters. I hope that will change over time. Perhaps if we get to an era of radical abundance,

1:56:38.480 --> 1:56:42.560
 we don't have to be so competitive anymore. Maybe we can be more correct cooperative

1:56:42.560 --> 1:56:48.880
 if resources aren't so scarce. It's true that in terms of power corrupting and leading to

1:56:48.880 --> 1:56:53.680
 destructive things, it seems that some of the atrocities of the past happen when there's a

1:56:53.680 --> 1:56:58.240
 significant constraint on resources. I think that's the first thing. I don't think that's enough.

1:56:58.240 --> 1:57:04.080
 I think scarcity is one thing that's led to competition, zero sum game thinking. I would

1:57:04.080 --> 1:57:08.320
 like us to all be in a positive sum world. And I think for that, you have to remove scarcity.

1:57:08.320 --> 1:57:11.920
 I don't think that's enough, unfortunately, to get well peace because there's also other

1:57:11.920 --> 1:57:16.000
 corrupting things like wanting power over people and this kind of stuff, which is not

1:57:16.000 --> 1:57:23.200
 necessarily satisfied by just abundance. But I think it will help. But I think ultimately,

1:57:23.200 --> 1:57:27.360
 AI is not going to be run by any one person, one organization. I think it should belong

1:57:27.360 --> 1:57:33.120
 to the world, belong to humanity. And I think there'll be many ways this will happen. And

1:57:33.120 --> 1:57:36.480
 ultimately, everybody should have a say in that.

1:57:37.520 --> 1:57:45.120
 Do you have advice for young people in high school and college? Maybe if they're interested in

1:57:45.120 --> 1:57:52.320
 AI or interested in having a big impact on the world, what they should do to have a career

1:57:52.320 --> 1:57:54.800
 they can be proud of or to have a life they can be proud of?

1:57:54.800 --> 1:57:59.760
 So I love giving talks to the next generation. What I say to them is actually two things. I think

1:57:59.760 --> 1:58:04.960
 the most important things to learn about and to find out about when you're young is what are

1:58:04.960 --> 1:58:10.320
 your true passions is, first of all, as two things. One is find your true passions. And I think

1:58:10.320 --> 1:58:15.280
 you can do that by the way to do that is to explore as many things as possible when you're young and

1:58:15.280 --> 1:58:20.960
 you have the time and you can take those risks. I would also encourage people to look at the

1:58:20.960 --> 1:58:25.760
 finding the connections between things in a unique way. I think that's a really great way

1:58:25.760 --> 1:58:32.080
 to find a passion. Second thing I would say advice is know yourself. So spend a lot of time

1:58:33.040 --> 1:58:38.160
 understanding how you work best, like what are the optimal times to work? What are the optimal

1:58:38.160 --> 1:58:45.040
 ways that you study? How do you deal with pressure? Sort of test yourself in various scenarios and

1:58:45.040 --> 1:58:50.640
 try and improve your weaknesses, but also find out what your unique skills and strengths are

1:58:50.640 --> 1:58:56.080
 and then hone those. So then that's what will be your super value in the world later on. And if you

1:58:56.080 --> 1:59:02.320
 can then combine those two things and find passions that you're genuinely excited about that intersect

1:59:02.320 --> 1:59:08.720
 with what your unique strong skills are, then you're onto something incredible and I think

1:59:08.720 --> 1:59:13.440
 you can make a huge difference in the world. So let me ask about know yourself. This is fun.

1:59:13.440 --> 1:59:19.360
 This is fun. Quick questions about day in the life, the perfect day, the perfect productive day in

1:59:19.360 --> 1:59:27.680
 the life of Demesis Hub. Maybe these days, there's a lot involved. So maybe a slightly younger

1:59:27.680 --> 1:59:34.560
 Demesis Hub where you could focus on a single project maybe. How early do you wake up? Are you

1:59:34.560 --> 1:59:39.600
 night owl? Do you wake up early in the morning? What are some interesting habits? How many

1:59:39.600 --> 1:59:46.880
 dozens of cups of coffees do you drink a day? What's the computer that you use? What's the setup?

1:59:46.880 --> 1:59:52.160
 How many screens? What kind of keyboard are we talking? Emax Vim or we're talking something

1:59:52.160 --> 1:59:58.320
 more modern. There's a bunch of those questions. So maybe day in the life, what's the perfect day

1:59:58.320 --> 2:00:03.760
 involved? Well, these days, it's quite different from say 10, 20 years ago. Back 10, 20 years ago,

2:00:03.760 --> 2:00:11.120
 it would have been a whole day of research, individual research or programming, doing some

2:00:11.120 --> 2:00:15.600
 experiment, neuroscience, computer science experiment, reading lots of research papers,

2:00:15.600 --> 2:00:24.960
 and then perhaps at night time, reading science fiction books or playing some games.

2:00:24.960 --> 2:00:31.920
 But lots of focus, deep focused work on whether it's programming or reading research papers.

2:00:31.920 --> 2:00:37.360
 Yes. So that would be lots of deep, focused work. These days, for the last sort of, I guess,

2:00:38.000 --> 2:00:42.160
 five to 10 years, I've actually got quite a structure that works very well for me now,

2:00:42.160 --> 2:00:49.600
 which is that I'm a complete night owl, always have been. So I optimize for that. So I basically

2:00:49.600 --> 2:00:54.160
 do a normal day's work, get into work about 11 o clock and sort of do work to about seven

2:00:54.800 --> 2:01:00.720
 in the office. And I will arrange back to back meetings for the entire time of that.

2:01:00.720 --> 2:01:05.680
 And with as many, me as many people as possible. So that's my collaboration, management part of the

2:01:05.680 --> 2:01:13.440
 day. Then I go home, spend time with the family and friends, have dinner, relax a little bit.

2:01:13.440 --> 2:01:18.320
 And then I start a second day of work, I call it my second day of work around 10pm, 11pm.

2:01:18.320 --> 2:01:22.400
 And that's the time till about the small hours of the morning, four, five in the morning,

2:01:22.400 --> 2:01:29.200
 where I will do my thinking and reading research, writing research papers. Sadly,

2:01:29.200 --> 2:01:34.720
 don't have time to code anymore. But it's not efficient to do that these days,

2:01:34.720 --> 2:01:41.040
 given the amount of time I have. But that's when I do, maybe do the long kind of stretches of

2:01:41.040 --> 2:01:46.560
 thinking and planning. And then probably using email or other things, I would fire off a lot

2:01:46.560 --> 2:01:51.440
 of things to my team to deal with the next morning. But actually, thinking about this overnight,

2:01:51.440 --> 2:01:54.640
 we should go for this project or arrange this meeting the next day.

2:01:54.640 --> 2:01:58.560
 When you think it through a problem, like talking about sheet of paper, is there some

2:01:59.120 --> 2:02:00.880
 structured process?

2:02:00.880 --> 2:02:06.160
 I still like pencil and paper best for working out things. But these days, it's just so

2:02:06.160 --> 2:02:09.760
 efficient to read research papers just on the screen. I still often print them out,

2:02:09.760 --> 2:02:14.720
 actually. I still prefer to mark out things. And I find it goes into the brain quick better

2:02:14.720 --> 2:02:19.280
 and sticks in the brain better when you're still using physical pen and pencil and paper.

2:02:19.280 --> 2:02:20.640
 So you take notes?

2:02:20.640 --> 2:02:26.880
 I have lots of notes, electronic ones and also whole stacks of notebooks that I use at home.

2:02:26.880 --> 2:02:32.640
 On some of these most challenging next steps, for example, stuff none of us know about that

2:02:32.640 --> 2:02:38.480
 you're working on, you're thinking, there's some deep thinking required there. What is the

2:02:38.480 --> 2:02:43.040
 right problem? What is the right approach? Because you're going to have to invest a huge

2:02:43.040 --> 2:02:46.800
 amount of time for the whole team. They're going to have to pursue this thing. What's

2:02:46.800 --> 2:02:49.920
 the right way to do it? Is RL going to work here or not?

2:02:49.920 --> 2:02:50.080
 Yes.

2:02:51.760 --> 2:02:55.760
 What's the right thing to try? What's the right benchmark to you? Do we need to construct a

2:02:55.760 --> 2:02:58.080
 benchmark from scratch? All those kinds of things.

2:02:58.080 --> 2:03:02.640
 Yes. So I think of all those kind of things in the night time phase, but also much more.

2:03:03.280 --> 2:03:09.840
 I find I've always found the quiet hours of the morning when everyone's asleep. It's super quiet

2:03:09.840 --> 2:03:15.360
 outside. I love that time. It's the golden hours like between like one and three in the morning.

2:03:16.320 --> 2:03:21.920
 Put some music on, some inspiring music on and then think these deep thoughts. So that's when

2:03:21.920 --> 2:03:30.240
 I would read my philosophy books and Spinoza's, my recent favorite can all these things. I read

2:03:30.240 --> 2:03:36.080
 about a great scientist of history, how they did things, how they thought things. So that's

2:03:36.080 --> 2:03:43.520
 when I do all my creative thinking. It's good. I think people recommend you do your sort of

2:03:43.520 --> 2:03:48.560
 creative thinking in one block. The way I organize the day, that way I don't get interrupted because

2:03:48.560 --> 2:03:57.360
 obviously no one else is up at those times. So I can sort of get super deep and super into flow.

2:03:57.360 --> 2:04:03.520
 The other nice thing about doing it night time wise is if I'm really onto something or I've got

2:04:03.520 --> 2:04:08.240
 really deep into something, I can choose to extend it and I'll go into six in the morning,

2:04:08.240 --> 2:04:12.000
 whatever, and then I'll just pay for it the next day because I'll be a bit tired and I won't be

2:04:12.000 --> 2:04:18.000
 my best. But that's fine. I can decide looking at my schedule the next day and given where I'm at

2:04:18.000 --> 2:04:22.400
 with this particular thought or creative idea that I'm going to pay that cost the next day.

2:04:23.360 --> 2:04:28.320
 So I think that's more flexible than morning people who do that. They get up at four in the

2:04:28.320 --> 2:04:32.320
 morning. They can also do those golden hours then. But then their start of their scheduled

2:04:32.320 --> 2:04:36.560
 day starts at breakfast, AAM, whatever they have their first meeting. And then it's hard,

2:04:36.560 --> 2:04:40.160
 you have to reschedule a day if you're in flow. So I don't have to do that.

2:04:40.160 --> 2:04:44.960
 Yeah, that could be a truly special thread of thoughts that you're too passionate about.

2:04:44.960 --> 2:04:48.480
 This is where some of the greatest ideas could potentially come is when you just

2:04:48.480 --> 2:04:54.240
 lose yourself late into the night. And for the meetings, I mean, you're loading in really hard

2:04:54.240 --> 2:04:58.640
 problems in a very short amount of time. So you have to do some kind of first principles thinking

2:04:58.640 --> 2:05:02.320
 here. It's like, what's the problem? What's the state of things? What's the right next step?

2:05:02.320 --> 2:05:06.800
 Yes, you have to get really good at context switching, which is one of the hardest things

2:05:06.800 --> 2:05:10.720
 because especially as we do so many things, if you include all the scientific things we do,

2:05:10.720 --> 2:05:16.000
 scientific fields we're working in, these are entire complex fields in themselves. And you

2:05:16.000 --> 2:05:23.280
 have to sort of keep up to a rest of that. But I enjoy it. I've always been a sort of generalist

2:05:23.280 --> 2:05:28.320
 in a way. And that's actually what happened with my games career after chess. One of the reasons

2:05:28.320 --> 2:05:31.600
 I stopped playing chess was because I got into computers, but also I started realizing there

2:05:31.600 --> 2:05:35.840
 were many other great games out there to play too. So I've always been that way inclined,

2:05:35.840 --> 2:05:40.240
 multidisciplinary. And there's too many interesting things in the world to spend all your time just

2:05:40.240 --> 2:05:46.960
 on one thing. So you mentioned Spinoza got asked the big, ridiculously big question about life.

2:05:47.520 --> 2:05:53.040
 What do you think is the meaning of this whole thing? Why are we humans here? You've already

2:05:53.040 --> 2:05:59.440
 mentioned that perhaps the universe created us. Is that why you think we're here to understand

2:05:59.440 --> 2:06:03.920
 how the universe? Yeah, I think my answer to that would be, and at least the life I'm living,

2:06:03.920 --> 2:06:10.480
 is to gain and understand the knowledge, to gain knowledge and understand the universe.

2:06:10.480 --> 2:06:14.560
 That's what I think. I can't see any higher purpose than that. If you think back to the

2:06:14.560 --> 2:06:20.800
 classical Greeks, the virtue of gaining knowledge, I think it's one of the few true virtues is to

2:06:20.800 --> 2:06:27.680
 understand the world around us and the context and humanity better. And I think if you do that,

2:06:27.680 --> 2:06:32.480
 you become more compassionate and more understanding yourself and more tolerant. And all these,

2:06:32.480 --> 2:06:37.520
 I think all these other things may flow from that. And to me, understanding the nature of reality,

2:06:37.520 --> 2:06:41.280
 that is the biggest question. What is going on here is sometimes the colloquial way I say.

2:06:41.280 --> 2:06:48.400
 What is really going on here? It's so mysterious. I feel like we're in some huge puzzle. But the

2:06:48.400 --> 2:06:53.840
 world also seems to be, the universe seems to be structured in a way. Why is it structured

2:06:53.840 --> 2:06:58.960
 in a way that science is even possible? The scientific method works, things are repeatable.

2:06:58.960 --> 2:07:04.480
 It feels like it's almost structured in a way to be conducive to gaining knowledge.

2:07:06.400 --> 2:07:11.200
 Why should computers be even possible? Isn't that amazing that computational or electronic

2:07:11.200 --> 2:07:17.280
 devices can be possible? And they're made of sand, our most common element that we have,

2:07:17.280 --> 2:07:21.280
 silicon on the Earth's crust, that could be made of diamond or something,

2:07:21.280 --> 2:07:26.320
 that we would have only had one computer. So a lot of things are slightly suspicious to me.

2:07:26.320 --> 2:07:30.720
 It sure as heck sounds, this puzzle sure as heck sounds like something we talked about earlier,

2:07:30.720 --> 2:07:36.240
 what it takes to design a game that's really fun to play for prolonged periods of time.

2:07:37.600 --> 2:07:42.160
 And it does seem like this puzzle, like you mentioned, the more you learn about it,

2:07:42.160 --> 2:07:47.600
 the more you realize how little you know. So it humbles you, but excites you by the possibility

2:07:47.600 --> 2:07:54.720
 of learning more. It's one heck of a, one heck of a puzzle we got going on here. So like I mentioned,

2:07:54.720 --> 2:08:00.960
 of all the people in the world, you're very likely to be the one who creates the AGI system

2:08:02.560 --> 2:08:08.240
 that achieves human level intelligence and goes beyond it. So if you got a chance and very well,

2:08:08.240 --> 2:08:12.000
 you could be the person that goes into the room with the system and have a conversation.

2:08:12.960 --> 2:08:17.760
 Maybe you only get to ask one question. If you do, what question would you ask her?

2:08:19.280 --> 2:08:24.640
 I would probably ask, what is the true nature of reality? I think that's the question. I don't

2:08:24.640 --> 2:08:29.840
 know if I'd understand the answer because maybe it would be 42 or something like that. But that's

2:08:29.840 --> 2:08:35.120
 the question I would ask. And then there'll be a deep sigh from the systems like, all right,

2:08:35.120 --> 2:08:42.000
 how do I explain to this human? Exactly. All right, let me, I don't have time to explain. Maybe

2:08:42.000 --> 2:08:49.440
 I'll draw you a picture. It is, I mean, how do you even begin to answer that question?

2:08:49.440 --> 2:08:55.600
 Well, I think it would... What would you think the answer could possibly look like?

2:08:55.600 --> 2:09:02.320
 I think it could start looking like more fundamental explanations of physics would be the

2:09:02.320 --> 2:09:07.920
 beginning, more careful specification of that, taking you, walking us through by the hand as to

2:09:07.920 --> 2:09:13.840
 what one would do to maybe prove those things out. Maybe giving you glimpses of what things you

2:09:13.840 --> 2:09:20.240
 totally missed in the physics of today. Exactly. Here's glimpses of, no, like there's a much

2:09:22.400 --> 2:09:25.040
 more elaborate world or a much simpler world or something.

2:09:26.640 --> 2:09:31.760
 A much deeper, maybe simpler explanation of things, right, than the standard model of physics,

2:09:31.760 --> 2:09:37.760
 which we know doesn't work, but we still keep adding to. And that's how I think the beginning

2:09:37.760 --> 2:09:41.680
 of an explanation would look. And it would start encompassing many of the mysteries that we have

2:09:41.680 --> 2:09:47.840
 wondered about for thousands of years, like consciousness, dreaming, life, and gravity,

2:09:47.840 --> 2:09:52.800
 all of these things. Yeah, giving us glimpses of explanations for those things. Yeah. Well,

2:09:53.920 --> 2:09:59.600
 Dennis, you're one of the special human beings in this giant puzzle of ours. And it's a huge

2:09:59.600 --> 2:10:03.280
 honor that you would take a pause from the bigger puzzle to solve this small puzzle of a

2:10:03.280 --> 2:10:06.560
 conversation with me today. It's truly an honor and a pleasure. Thank you so much.

2:10:06.560 --> 2:10:11.680
 Thank you for having me. I really enjoyed it. Thanks, Lex. Thanks for listening to this conversation

2:10:11.680 --> 2:10:16.480
 with Dennis Lasabas. To support this podcast, please check out our sponsors in the description.

2:10:16.480 --> 2:10:23.280
 And now, let me leave you with some words from Ezker Dijkstra. Computer science is no more about

2:10:23.280 --> 2:10:38.560
 than astronomy is about telescopes. Thank you for listening and hope to see you next time.

