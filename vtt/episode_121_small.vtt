WEBVTT

00:00.000 --> 00:06.400
 The following is a conversation with Eugenia Kuida, cofounder of Replica, which is an app

00:06.400 --> 00:11.680
 that allows you to make friends with an artificial intelligence system, a chatbot, that learns

00:11.680 --> 00:17.600
 to connect with you on an emotional, you could even say, a human level by being a friend.

00:18.480 --> 00:24.400
 For those of you who know my interest in AI and views on life in general, know that Replica and

00:24.400 --> 00:30.320
 Eugenia's line of work is near and dear to my heart. The origin story of Replica is grounded

00:30.320 --> 00:37.040
 in a personal tragedy of Eugenia losing her close friend, Roman Mazurenki, who was killed

00:37.040 --> 00:44.160
 crossing the street by a hit and run driver in late 2015. He was 34. The app started as a way

00:44.160 --> 00:49.440
 to grieve the loss of a friend by trading a chatbot and your old net on text messages between

00:49.440 --> 00:55.520
 Eugenia and Roman. The rest is a beautiful human story as we talk about with Eugenia.

00:56.480 --> 01:01.520
 When a friend mentioned Eugenia's work to me, I knew I had to meet her and talk to her. I felt

01:01.520 --> 01:08.240
 before, during, and after that this meeting would be an important one in my life. And it was. I think

01:08.240 --> 01:15.520
 in ways that only time will truly show, to me and others. She is a kind and brilliant person.

01:15.520 --> 01:21.840
 It was an honor and a pleasure to talk to her. Quick summary of the sponsors, DoorDash, Dollar

01:21.840 --> 01:27.760
 Shave Club, and Cash App. Click the sponsor links in the description to get a discount and to support

01:27.760 --> 01:34.160
 this podcast. As a side note, let me say that deep meaningful connection between human beings

01:34.160 --> 01:40.160
 and artificial intelligence systems is a lifelong passion for me. I'm not yet sure where that passion

01:40.160 --> 01:46.480
 will take me, but I decided some time ago that I will follow it boldly and without fear, just as

01:46.480 --> 01:52.160
 far as I can take it. With a bit of hard work and a bit of luck, I hope I'll succeed in helping

01:52.160 --> 01:57.760
 build AI systems that have some positive impact on the world and on the lives of a few people out

01:57.760 --> 02:06.320
 there. But also, it is entirely possible that I am in fact one of the chatbots that Eugenia

02:06.320 --> 02:12.400
 and the replica team have built. And this podcast is simply a training process for the neural net

02:12.400 --> 02:18.800
 that's trying to learn to connect to human beings. One episode at a time. In any case,

02:18.800 --> 02:25.040
 I wouldn't know if I was or wasn't. And if I did, I wouldn't tell you. If you enjoy this thing,

02:25.040 --> 02:30.880
 subscribe on YouTube, review it with 5 stars on Apple Podcast, follow on Spotify, support on Patreon,

02:30.880 --> 02:36.640
 or connect with me on Twitter, Alex Friedman. As usual, I'll do a few minutes of ads now and no

02:36.640 --> 02:42.080
 ads in the middle. I'll try to make these interesting, but give you time stamps so you can skip. But

02:42.080 --> 02:47.520
 please do still check out the sponsors by clicking the links in the description to get a discount

02:47.520 --> 02:52.000
 by whatever they're selling. It really is the best way to support this podcast.

02:53.120 --> 02:58.880
 This show is sponsored by Dollar Shave Club. Try them out with a one time offer for only

02:58.880 --> 03:04.800
 five bucks and free shipping at dollarshave.com slash lex. The starter kit comes with a six

03:04.800 --> 03:11.840
 blade razor refills and all kinds of other stuff that makes shaving feel great. I've been a member

03:11.840 --> 03:16.720
 of Dollar Shave Club for over five years and actually signed up when I first heard about them

03:16.720 --> 03:23.280
 on the Joe Rogan Experience podcast. And now, friends, we have come full circle. It feels like

03:23.280 --> 03:28.800
 I made it now that I can do a read for them just like Joe did all those years ago. Back when he

03:28.800 --> 03:37.520
 also did ads for some less reputable companies. Let's say that you know about if you're a true fan

03:37.520 --> 03:43.120
 of the old school podcasting world. Anyway, I just use the razor and the refills, but they told

03:43.120 --> 03:49.280
 me I should really try out the shave butter. I did. I love it. It's translucent somehow,

03:49.280 --> 03:55.440
 which is a cool new experience. Again, try the ultimate shave starter set today for just five

03:55.440 --> 04:03.200
 bucks plus free shipping at dollarshaveclub.com slash lex. This show is also sponsored by DoorDash.

04:03.200 --> 04:08.720
 You get $5 off and zero delivery fees on your first order of 15 bucks or more. When you download

04:08.720 --> 04:15.520
 the DoorDash app and enter code, you guessed it, Lex. I have so many memories of working late

04:15.520 --> 04:21.920
 nights for a deadline with a team of engineers, whether that's for my PhD at Google or MIT,

04:21.920 --> 04:27.040
 and eventually taking a break to argue about which DoorDash restaurant to order from. And when the

04:27.040 --> 04:32.800
 food came, those moments of bonding, of exchanging ideas, of pausing, to shift attention from the

04:32.800 --> 04:39.680
 programs, the humans, or special. For a bit of time, I'm on my own now, so I miss that camaraderie,

04:39.680 --> 04:45.760
 but actually I still use DoorDash a lot. There's a million options that fit into my crazy keto

04:45.760 --> 04:51.200
 diet ways. Also, it's a great way to support restaurants in these challenging times. Once

04:51.200 --> 04:57.120
 again, download the DoorDash app and enter code, Lex, to get $5 off and zero delivery fees on your

04:57.120 --> 05:03.040
 first order of $15 or more. Finally, this show is presented by Cash App, the number one finance

05:03.040 --> 05:08.880
 app in the App Store. I can truly say that they're an amazing company, one of the first sponsors,

05:08.880 --> 05:15.680
 if not the first sponsor to truly believe in me. And I think quite possibly the reason I'm still

05:15.680 --> 05:23.040
 doing this podcast, so I'm forever grateful to Cash App. So thank you. And as I said many times

05:23.040 --> 05:30.240
 before, use code LexPodcast when you download the app from Google Play or the App Store. Cash App

05:30.240 --> 05:35.280
 lets you send money to friends by Bitcoin and invest in the stock market with as little as $1.

05:36.400 --> 05:40.640
 I usually say other stuff here in the read, but I wasted all that time up front saying how

05:40.640 --> 05:46.080
 grateful I am to Cash App. I'm going to try to go off the top of my head a little bit more for these

05:46.080 --> 05:51.120
 reads, because I'm actually very lucky to be able to choose the sponsors that we take on.

05:51.120 --> 05:55.200
 And that means I can really only take on the sponsors that I truly love,

05:55.200 --> 06:00.560
 and then I can just talk about why I love them. So it's pretty simple. Again, get Cash App from

06:00.560 --> 06:06.480
 the App Store, Google Play, use code LexPodcast, get $10, and Cash App will also donate $10 to

06:06.480 --> 06:10.880
 first, an organization that is helping to advance robotics and STEM education

06:10.880 --> 06:17.120
 for young people around the world. And now here's my conversation with Eugenia Kuida.

06:18.160 --> 06:23.360
 Okay, before we talk about AI and the amazing work you're doing, let me ask you ridiculously,

06:23.360 --> 06:28.560
 we're both Russian, so let me ask you a ridiculously romanticized Russian question.

06:28.560 --> 06:37.760
 Do you think human beings are alone, fundamentally, on a philosophical level? In our

06:38.720 --> 06:49.200
 existence, when we go through life, do you think just the nature of our life is loneliness?

06:50.240 --> 06:54.480
 Yeah, so we have to read Dostoevsky at school, as you probably know.

06:54.480 --> 07:00.880
 In Russian? Yeah. I mean, it's part of your school program. So I guess if you read that,

07:00.880 --> 07:05.600
 then you sort of have to believe that. You're made to believe that you're fundamentally alone,

07:06.400 --> 07:10.320
 and that's how you live your life. How do you think about it? You have a lot of friends,

07:11.040 --> 07:17.280
 but at the end of the day, do you have like a longing for connection with other people that's

07:18.800 --> 07:21.840
 maybe another way of asking it? Do you think that's ever fully satisfied?

07:21.840 --> 07:29.680
 I think we are fundamentally alone. We're born alone. We're die alone, but I view my whole life

07:29.680 --> 07:36.320
 as trying to get away from that, trying to not feel lonely. And again, we're talking about a

07:36.320 --> 07:42.000
 subjective way of feeling alone. It doesn't necessarily mean that you don't have any connections,

07:42.000 --> 07:49.120
 or you are actually isolated. You think it's a subjective thing, but again,

07:49.120 --> 07:54.080
 another absurd measurement wise thing. How much loneliness do you think there is in the world?

07:54.880 --> 08:06.160
 So like, if you see loneliness as a condition, how much of it is there? Do you think? I guess

08:06.160 --> 08:12.240
 how many, there's all kinds of studies and measures of how many people in the world feel

08:12.240 --> 08:18.640
 alone. There's all these measures of how many people are, self report, or just all these kinds

08:18.640 --> 08:26.800
 of different measures. But in your own perspective, how big of a problem do you think it is, sizewise?

08:27.440 --> 08:32.400
 Well, I'm actually fascinated by the topic of loneliness. I try to read about it as much as I

08:32.400 --> 08:40.400
 can. I think there's a paradox, because loneliness is not a clinical disorder. It's not

08:40.400 --> 08:43.840
 something that you can get your insurance to pay for if you're struggling with that.

08:43.840 --> 08:50.720
 Yet, it's actually proven in pretty tons of papers, tons of research around that. It is proven

08:51.760 --> 09:00.320
 that it's correlated with earlier life expectancy, shorter lifespan. In a way, right now, what

09:00.320 --> 09:05.360
 scientists would say that it's a little bit worse than VNOB, so not actually doing any

09:06.000 --> 09:08.400
 physical activity in your life. In terms of the impact on your health.

09:08.400 --> 09:12.400
 In terms of impact on your physiological health, yeah. So it basically puts you,

09:12.400 --> 09:18.080
 if you're constantly feeling lonely, your body responds like it's basically all the time under

09:18.080 --> 09:24.560
 stress. So it's always in this alert state. So it's really bad for you because it actually

09:24.560 --> 09:30.800
 drops your immune system and gets your response to inflammation. It's quite different. So all the

09:30.800 --> 09:36.240
 cardiovascular diseases actually responds to viruses, so it's much easier to catch a virus.

09:36.240 --> 09:42.160
 That's sad now that we're living in a pandemic and it's probably making us a lot more alone.

09:42.720 --> 09:46.560
 And it's probably weakening the immune system, making us more susceptible to the virus.

09:47.440 --> 09:50.000
 It's kind of sad.

09:50.880 --> 09:57.600
 Yeah, the statistics are pretty horrible around that. So around 30% of all millennials report

09:57.600 --> 09:59.920
 that they're feeling lonely constantly. 30%.

09:59.920 --> 10:04.720
 30%. And then it's much worse for Gen Z. And then 20% of millennials say that they feel lonely.

10:04.720 --> 10:10.560
 And they also don't have any close friends. And then I think 25% or so and then 20% would say

10:10.560 --> 10:13.760
 they don't even have acquaintances. That's in the United States?

10:13.760 --> 10:17.680
 That's in the United States. And I'm pretty sure that it's much worse everywhere else.

10:17.680 --> 10:24.640
 Like in the UK, I mean, it was widely tweeted and posted when they were talking about a minister

10:24.640 --> 10:29.280
 of loneliness that they wanted to point because four out of 10 people in the UK feel lonely.

10:29.280 --> 10:40.880
 I think that thing actually exists. So yeah, you will die sooner if you are lonely.

10:40.880 --> 10:46.080
 And again, this is only when we're only talking about your perception of loneliness or feeling

10:46.080 --> 10:52.080
 lonely. That is not objectively being fully socially isolated. However, the combination

10:52.080 --> 10:57.280
 of being fully socially isolated and not having many connections and also feeling lonely,

10:57.280 --> 11:03.440
 that's pretty much a deadly combination. So it strikes me bizarre or strange that

11:04.080 --> 11:11.120
 this is a wide known fact. And then there's really no one working really on that because it's

11:11.120 --> 11:15.680
 subclinical. It's not clinical. It's not something that you can will tell your doctor and get a

11:15.680 --> 11:21.760
 treatment or something. Yet it's killing us. Yeah. So there's a bunch of people trying to

11:21.760 --> 11:26.720
 evaluate, like try to measure the problem by looking at like how social media is affecting

11:26.720 --> 11:31.040
 loneliness and all that kind of stuff. So it's like measurement. Like if you look at the field of

11:31.040 --> 11:35.760
 psychology, they're trying to measure the problem and not that many people actually, but some,

11:36.560 --> 11:41.520
 but you're basically saying how many people are trying to solve the problem.

11:43.040 --> 11:50.560
 Like how would you try to solve the problem of loneliness? Like if you just stick to humans,

11:50.560 --> 11:56.560
 I mean, or basically not just the humans, but the technology that connects us humans,

11:56.560 --> 12:03.680
 do you think there's a hope for that technology to do the connection? Like are you on social media

12:03.680 --> 12:11.040
 much? Unfortunately. Do you find yourself like, again, if you're sort of introspect about

12:11.760 --> 12:16.160
 how connected you feel to other human beings, how not alone you feel, do you think social

12:16.160 --> 12:21.440
 media makes it better or worse? Maybe for you personally or in general?

12:22.800 --> 12:28.080
 I think it's easier to look at some stats. And I mean, gen Zs seem to be,

12:28.080 --> 12:32.160
 generosity seems to be much lonelier than millennials in terms of however they report

12:32.160 --> 12:37.040
 loneliness. They're definitely the most connected generation in the world. I mean,

12:37.040 --> 12:43.200
 I still remember life without an iPhone, without Facebook. They don't know that that ever existed

12:43.200 --> 12:49.760
 or at least don't know how it was. So that tells me a little bit about the fact that

12:50.480 --> 12:58.640
 might be this hyper connected world might actually make people feel lonelier. I don't

12:58.640 --> 13:03.440
 know exactly what the measurements are around that, but I would say in my personal experience,

13:03.440 --> 13:09.120
 I think it does make you feel a lot lonelier. Mostly, yeah, we're all super connected. But

13:09.120 --> 13:14.480
 I think loneliness, the feeling of loneliness doesn't come from not having any social connections

13:14.480 --> 13:20.240
 whatsoever. Again, tons of people that are in long term relationships experience bouts of

13:20.240 --> 13:26.080
 loneliness and continued loneliness. And it's more the question about the true connection about

13:26.080 --> 13:33.040
 actually being deeply seen, deeply understood. And in a way, it's also about your relationship

13:33.040 --> 13:39.760
 with yourself. In order to not feel lonely, you actually need to have a better relationship and

13:39.760 --> 13:44.720
 feel more connected to yourself than this feeling actually starts to go away a little bit. And then

13:44.720 --> 13:53.040
 you open up yourself to actually meeting other people in a very special way, not just at a friend

13:53.040 --> 13:59.440
 on Facebook kind of way. So just to briefly touch on it, I mean, do you think it's possible to form

13:59.440 --> 14:08.800
 that kind of connection with AI systems more down the line of some of your work? Do you think that's

14:10.640 --> 14:17.760
 engineering wise, a possibility to alleviate loneliness is not with another human, but with

14:17.760 --> 14:24.080
 an AI system? Well, I know that's that's a fact. That's what we're doing. And we see it and we

14:24.080 --> 14:32.640
 measure that and we see how people start to feel less lonely, talking to their virtual AI friend.

14:32.640 --> 14:38.080
 So basically a chatbot at the basic level, but could be more, like, do you have, I'm not even

14:38.080 --> 14:45.360
 speaking sort of about specifics, but do you have a hope, like, if you look 50 years from now,

14:46.080 --> 14:53.040
 do you have a hope that there's just like, AI's that are, like, optimized for, let me let me first

14:53.040 --> 14:59.520
 start, like, right now, the way people perceive AI, which is recommender systems for Facebook and

14:59.520 --> 15:05.360
 Twitter, social media, they see AI is basically destroying, first of all, the fabric of our

15:05.360 --> 15:10.240
 civilization, but second of all, making us more lonely. Do you see like a world where it's possible

15:10.240 --> 15:19.920
 to just have AI systems floating about that, like, make our life less lonely? Yeah, make us happy,

15:19.920 --> 15:25.520
 like, are putting good things into the world in terms of our individual lives.

15:26.320 --> 15:30.080
 Yeah, totally believe it. And that that's why we're also working on that.

15:31.840 --> 15:36.960
 I think we need to also make sure that what we're trying to optimize for we're actually measuring.

15:37.680 --> 15:41.760
 And it is a north star metric that we're going after. And all of our product and all of our

15:41.760 --> 15:46.320
 business models are optimized for that. Because you can talk, you know, a lot of products that talk

15:46.320 --> 15:51.600
 about, you know, making you feel less lonely, making you feel more connected, they're not really

15:51.600 --> 15:56.320
 measuring that. So they don't really know whether their users are actually feeling less lonely in

15:56.320 --> 16:01.360
 the long run, or feeling more connected in the long run. So I think it's really important to put

16:01.360 --> 16:06.640
 your measure it. Yeah, to measure it. What's a what's a good measurement of loneliness?

16:07.600 --> 16:12.000
 Well, so that's something that I'm really interested in. How do you measure that people

16:12.000 --> 16:16.320
 are feeling better, or that they're feeling less lonely? With loneliness, there's a scale,

16:16.320 --> 16:21.360
 there's a UCLA 20 and UCLA three recently scale, which is basically a questionnaire that you fill

16:21.360 --> 16:28.240
 out. And you can see whether in the long run, it's improving or not. And that does it capture the

16:28.240 --> 16:37.280
 momentary feeling of loneliness? Does it look in like the past month? Like, is it basically

16:37.280 --> 16:43.280
 self report? Does it try to sneak up on you? Try tricky to answer honestly, or something like that?

16:44.080 --> 16:47.600
 Well, what's yeah, I'm not familiar with the question. It is just asking you a few questions

16:47.600 --> 16:52.400
 like how often did you feel like lonely? Or how often do you feel connected to other people in

16:52.400 --> 16:59.840
 this last few couple weeks? It's similar to the self report questionnaires for depression, anxiety,

16:59.840 --> 17:07.760
 like pH Q9 and get seven. Of course, as any self report questionnaires, that's not necessarily very

17:07.760 --> 17:14.400
 precise or very well measured. But still, if you take a big enough population, you get them through

17:14.400 --> 17:21.440
 these questionnaires, you can see you can see positive dynamic. And so you basically, you put

17:21.440 --> 17:26.640
 people through questionnaires to see like, is this thing is our is what we're creating making

17:26.640 --> 17:32.880
 people happier? Yeah, we measure. So we measure two outcomes, one short term, right after the

17:32.880 --> 17:39.040
 conversation, we ask people whether this conversation made them feel better, worse or same.

17:40.720 --> 17:46.000
 This metric right now is at 80%. So 80% of all our conversations make people feel better.

17:46.000 --> 17:50.480
 But I should have done the questionnaire with you. You feel a lot worse after we've done this

17:50.480 --> 17:57.200
 conversation. That's actually fascinating. I should probably do that. But that's probably

17:57.200 --> 18:05.520
 do that. You should totally and aim for 80% aim to outperform your current state of the art AI system

18:07.840 --> 18:13.840
 in these human conversations. So we'll get to your work with replica. But let me continue on the

18:13.840 --> 18:20.640
 line of absurd questions. So you talked about, you know, deep connection of the humans, deep connection

18:20.640 --> 18:26.560
 with AI, meaningful connection. Let me ask about love, people make fun of me because I talk about

18:26.560 --> 18:35.920
 love all the time. But what, what do you think love is? Like maybe in the context of a meaningful

18:35.920 --> 18:41.600
 connection with somebody else, do you draw a distinction between love, like friendship,

18:41.600 --> 18:53.120
 and Facebook friends? Or is it a graduate? No, it's all the same. No, like, is it just a

18:53.120 --> 18:58.640
 gradual thing? Or is there something fundamental about us humans that seek like a really deep

18:58.640 --> 19:07.280
 connection with another human being? And what is that? What is love, Eugenia?

19:07.280 --> 19:13.440
 I'm going to just enjoy asking you these questions, seeing you struggle.

19:14.400 --> 19:22.400
 Thanks. Well, the way I see it, specifically, the way it relates to our work and the way it was

19:23.520 --> 19:31.280
 the way it inspired our work on replica. I think one of the biggest and the most precious gifts

19:31.280 --> 19:39.600
 we can give to each other now in 2020 as humans is this gift of deep empathetic understanding,

19:39.600 --> 19:45.520
 the feeling of being deeply seen. Like what does that mean? Like that you exist? Like somebody

19:45.520 --> 19:50.720
 acknowledging that? Somebody seeing you for who you actually are. And that's extremely,

19:50.720 --> 19:55.760
 extremely rare. I think that is that combined with unconditional positive regard,

19:55.760 --> 20:04.560
 belief and trust that you internally are always inclined for positive growth and believing you

20:04.560 --> 20:10.880
 in this way, letting you be a separate person at the same time. And this deep empathetic

20:10.880 --> 20:18.560
 understanding, for me, that's the combination that really creates something special, something

20:18.560 --> 20:25.920
 that people, when they feel it once, they will always long for it again and something that starts

20:25.920 --> 20:32.960
 huge fundamental changes in people. When we see that someone accepts us so deeply,

20:32.960 --> 20:39.840
 we start to accept ourselves and the paradox is that's when big changes start happening,

20:39.840 --> 20:44.160
 big fundamental changes in people start happening. So I think that is the ultimate therapeutic

20:44.160 --> 20:52.240
 relationship that is, and that might be in some way a definition of love. So acknowledging that

20:52.240 --> 21:01.600
 there's a separate person and accepting you for who you are. Now, on a slightly, and you mentioned

21:01.600 --> 21:08.640
 therapeutic, that sounds very like a very healthy view of love, but is there also like a, like,

21:08.640 --> 21:16.160
 you know, if we look at heartbreak and, you know, most love songs are probably about heartbreak,

21:16.160 --> 21:25.680
 right? Is that like the mystery, the tension, the danger, the fear of loss, you know, all of that,

21:25.680 --> 21:33.120
 what people might see in the negative light as like games or whatever, but just the dance of

21:33.120 --> 21:40.960
 human interaction, yeah, fear of loss and fear of like, you said, like, once you feel it once,

21:40.960 --> 21:46.560
 you long for it again, but you also, once you feel it once, you might, for many people,

21:46.560 --> 21:53.680
 they've lost it. So they fear losing it, they feel loss. So is that part of it? Like, you're

21:53.680 --> 22:00.000
 speaking like beautifully about like the positive things, but is it important to be able to be

22:00.000 --> 22:03.600
 afraid of losing it from an engineering perspective?

22:06.240 --> 22:12.400
 I mean, it's a huge part of it. And unfortunately, we all, you know, face it at some points in our

22:12.400 --> 22:18.000
 lives. I mean, I did. You want to go into details? How'd you get your heart broken?

22:18.000 --> 22:23.440
 Sure. Well, so mine is pretty straight, my source pretty straightforward there.

22:23.440 --> 22:33.120
 I did have a friend that was, you know, that at some point in my 20s, became really, really close

22:33.120 --> 22:39.120
 to me. And we became really close friends. I grew up pretty lonely. So in many ways when I'm building,

22:39.120 --> 22:43.920
 you know, these, these AI friends, I think about myself when I was 17, writing horrible poetry and,

22:43.920 --> 22:50.640
 you know, in my dial up modem at home. And, you know, and that was the feeling that I grew up with.

22:50.640 --> 22:54.240
 I was alone for a long time. I was a teenager. Where did you grow up?

22:54.240 --> 23:00.400
 In Moscow, in the outskirts of Moscow. So I just skateboarded during the day and come back home

23:00.400 --> 23:05.040
 and, you know, connect to the internet. And write poetry? And then write horrible poetry.

23:05.040 --> 23:10.720
 Was it love poems? All sorts of poems, obviously love poems. I mean, what other poetry can you

23:10.720 --> 23:15.840
 write when you're 17? Could be political or something. But yeah. But that was, you know,

23:15.840 --> 23:22.000
 that was kind of my, yeah, like deeply influenced by Joseph Brodsky and like all sorts of poets that

23:23.920 --> 23:29.840
 every 17 year old will will be looking, you know, looking at and reading. But yeah, that was my,

23:30.640 --> 23:35.840
 these were my teenage years. And I just never had a person that I thought would, you know,

23:35.840 --> 23:42.320
 take me as it is would accept me the way I am. And I just thought, you know, working and just

23:42.320 --> 23:46.720
 doing my thing and being angry at the world and being a reporter, I was an investigative reporter

23:46.720 --> 23:52.720
 working undercover and writing about people was my way to connect with, you know, with,

23:52.720 --> 23:57.920
 with others. I was deeply curious about every, everyone else. And I thought that, you know,

23:57.920 --> 24:01.920
 if I, if I go out there, if I write their stories, that means I'm more connected.

24:02.720 --> 24:06.880
 This is what this podcast is about, by the way. I'm desperate. I'm seeking connection.

24:06.880 --> 24:12.560
 I'm just kidding. Or am I, I don't know. So what, wait, reporter,

24:14.640 --> 24:21.280
 what, how did that make you feel more connected? I mean, you're still fundamentally pretty alone.

24:21.840 --> 24:25.040
 But you're always with other people, you know, you're always thinking about what other

24:25.760 --> 24:31.600
 place going to infiltrate, what other community can I write about, what other phenomenon can I

24:31.600 --> 24:37.280
 explore? And you sort of like a trickster, you know, and like, and a mythological character,

24:37.280 --> 24:41.440
 like creature that's just jumping between all sorts of different worlds and feel,

24:41.440 --> 24:47.520
 and feel sort of okay with, in all of them. So that was my dream job, by the way. That was like,

24:47.520 --> 24:52.000
 totally what I would have been doing if Russia was a different place.

24:52.720 --> 24:58.160
 And a little bit undercover. So like you weren't, you were trying to, like you said, mythological

24:58.160 --> 25:02.480
 creature trying to infiltrate. So try to be a part of the world. What are we talking about?

25:02.480 --> 25:07.520
 What kind of things did you enjoy writing about? I'd go work at a strip club or go.

25:11.680 --> 25:17.680
 Awesome. Okay. Well, I'd go work at a restaurant or just go write about, you know,

25:18.960 --> 25:25.200
 certain phenomenons or phenomenons or people in the city. And what, sorry to keep interrupting.

25:25.200 --> 25:33.840
 I'm the worst conversationalist. What stage of Russia is this? What, is this pre Putin,

25:34.640 --> 25:43.680
 post Putin? What, what was Russia like? Pre Putin is really long ago. This is Putin era.

25:43.680 --> 25:51.840
 That's the beginning of 2000s, 2010, 2007, eight, nine, 10. What were strip clubs like in Russia

25:51.840 --> 25:58.960
 and restaurants and culture and people's minds like in that early Russia that you were covering?

25:58.960 --> 26:03.600
 In those early 2000s, this was, there was still a lot of hope. There were still tons of hope that

26:05.760 --> 26:13.600
 you know, we're sort of becoming this western, westernized society. The restaurants were opening,

26:13.600 --> 26:19.280
 we were really looking at, you know, we're trying, we're trying to copy a lot of things from,

26:19.280 --> 26:25.200
 from the US, from Europe, bringing all these things in. Very enthusiastic about that.

26:25.760 --> 26:30.080
 So there was a lot of, you know, stuff going on. There was a lot of hope and dream for this,

26:30.080 --> 26:36.080
 you know, new Moscow that would be similar to, I guess, New York. I mean, to give you an idea in

26:38.080 --> 26:44.240
 year 2000 was the year when we had two movie theaters in Moscow. And there was this one first

26:44.240 --> 26:50.400
 coffee house that opened and it was like really big deal. By 2010, there were all sorts of things

26:50.400 --> 26:55.680
 everywhere. Almost like a chain, like a Starbucks type of coffee house or like you mean? Oh yeah,

26:55.680 --> 27:00.080
 like a Starbucks. I mean, I remember we were reporting on like, we were writing about the

27:00.080 --> 27:04.480
 opening of Starbucks. I think in 2007, that was one of the biggest things that happened in,

27:04.480 --> 27:09.920
 you know, in Moscow back in the time. Like that was worthy of a magazine cover and

27:09.920 --> 27:15.120
 that was definitely the, you know, the biggest talk of the town. Yeah, when was McDonald's?

27:15.120 --> 27:19.440
 Because I was still in Russia when McDonald's opened. That was in the 90s. I mean, yeah.

27:19.440 --> 27:25.840
 Oh yeah, I remember that very well. Yeah. Those were long, long lines. I think it was 1993 or

27:25.840 --> 27:31.760
 four, I don't remember. Did you actually go to McDonald's at that time? Did you do that?

27:31.760 --> 27:35.600
 I mean, that was a luxurious outing. That was definitely not something you do every day.

27:35.600 --> 27:39.600
 And also the line was at least three hours. So if you're going to McDonald's, that is not fast

27:39.600 --> 27:44.320
 food. That is like at least three hours in line. Yeah. And then no one is trying to eat fast after

27:44.320 --> 27:50.320
 that. Everyone is like trying to enjoy as much as possible. What's your memory of that? Oh,

27:50.320 --> 27:57.120
 it was insane. Positive? Extremely positive. It's a small strawberry milkshake and a hamburger and

27:57.120 --> 28:02.080
 small fries and my mom's there and sometimes I'll just, because I was really little, they'll just

28:02.080 --> 28:08.720
 let me run, you know, up the cashier and like cut the line, which is like, you cannot really do that

28:08.720 --> 28:16.400
 in Russia. So like for a lot of people, like a lot of those experiences might seem not very

28:16.400 --> 28:24.720
 fulfilling, you know, like it's on the verge of poverty, I suppose. But do you remember all that

28:24.720 --> 28:32.720
 time fondly? Like, because I do, like the first time I drank, you know, Coke, you know, all that

28:32.720 --> 28:40.720
 stuff, right? And just, yeah, the connection with other human beings in Russia, I remember,

28:40.720 --> 28:47.280
 I remember really positively. Like, how do you remember what the 90s and then the Russia you

28:47.280 --> 28:52.560
 were covering, just the human connections you had with people and the experiences?

28:54.080 --> 29:01.120
 Well, my, my parents were both, both physicists, my grandparents were both, well, my grandfather

29:01.120 --> 29:08.160
 was a nuclear physicist, a professor at the university, my dad worked at Chernobyl when

29:08.160 --> 29:15.760
 I was born in Chernobyl, analyzing kind of the everything after the explosion. And then I remember

29:16.880 --> 29:21.040
 and they were so they were making sort of enough money in the Soviet Union, so they were not,

29:21.040 --> 29:24.960
 you know, extremely poor or anything. It was pretty prestigious to be a professor,

29:24.960 --> 29:31.680
 uh, the dean and the university. And then I remember my grandfather started making $100 a month

29:32.480 --> 29:38.480
 after, you know, in the 90s. So then I remember we started our main line of work would be to go to

29:38.480 --> 29:46.000
 our little tiny country house, get a lot of apples there from apple trees, bring them back to,

29:46.640 --> 29:53.840
 to, to the city and sell them in the street. So me and my nuclear physicist grandfather were

29:53.840 --> 29:57.520
 just standing there and he'd selling those apples the whole day because that would make

29:57.520 --> 30:02.960
 you more money than, you know, working at the university. And then he'll just tell me to try to

30:02.960 --> 30:11.040
 teach me, um, you know, something about planets and whatever the particles and stuff. And, you know,

30:11.040 --> 30:15.920
 I'm not smart at all. So I could never understand anything. But I was interested as a journalist,

30:15.920 --> 30:21.760
 kind of type interested. But that was my memory. And you know, I'm happy that I wasn't, um, I

30:21.760 --> 30:27.760
 I somehow got spared that I was probably too young to remember any of the traumatic stuff. So

30:27.760 --> 30:32.560
 the only thing I really remember had this bootleg that was very traumatic. Had this bootleg

30:32.560 --> 30:37.680
 Nintendo, which was called was called dandy in Russia. So 1993, there was nothing to eat. Like

30:37.680 --> 30:41.040
 even if you had any money, you would go to the store and there was no food. I don't know if you

30:41.040 --> 30:49.040
 remember that. And our friend had a restaurant like a government, half government owned something

30:49.040 --> 30:55.680
 restaurant. So they always had supplies. So he exchanged a big bag of weed for this Nintendo,

30:56.400 --> 31:02.160
 the bootleg Nintendo. And then I remember very fondly because I think it was nine or

31:03.120 --> 31:08.960
 something like that. And we're seven traumatic. We just got it. And I was playing it. And there

31:08.960 --> 31:16.400
 was this, you know, dandy TV show. Yeah. So it's a positive sense. You mean like, like a definitive

31:16.400 --> 31:21.280
 well, they took it away and gave me a bag of weed instead. And I cried like my eyes out for

31:21.280 --> 31:27.680
 days and days and days. Oh, no. And then, you know, as a, and my dad said, we're going to like

31:27.680 --> 31:32.000
 exchange it back in a little bit. So you keep the little gun, you know, the one that you shoot

31:32.000 --> 31:35.760
 the ducks with. So I'm like, okay, I'm keeping the gun. So sometimes it's going to come back.

31:35.760 --> 31:41.920
 But then they exchanged the gun as well for some sugar or something. I was so pissed. I was like,

31:41.920 --> 31:46.320
 I didn't want to eat for days after that. I'm like, I don't want your food. Give me my Nintendo back.

31:47.360 --> 31:53.280
 That was extremely traumatic. But, you know, I was happy that that was my only traumatic

31:53.280 --> 31:57.680
 experience. You know, my dad had to actually go to Chernobyl with a bunch of 20 year olds. He was

31:57.680 --> 32:03.280
 20 when he went to Chernobyl. And that was right after the explosion. No one knew anything. The

32:03.280 --> 32:07.600
 whole crew he went with all of them are dead now. I think there was this one guy still

32:07.600 --> 32:13.040
 alive that was still alive for the last few years. I think he died a few years ago now.

32:13.760 --> 32:20.080
 My dad somehow luckily got back earlier than everyone else. But just the fact that that was the,

32:20.080 --> 32:23.760
 and I was always like, well, how did they send you? I was only, I was just born, you know,

32:23.760 --> 32:28.640
 you had a newborn talk about paternity leave. And like, but that's who they took because they

32:28.640 --> 32:32.560
 didn't know whether you would be able to have kids when you come back. So they took the ones with

32:32.560 --> 32:38.960
 kids. So him with some guys went to, and I'm just thinking of me, when I was 20, I was so

32:39.920 --> 32:47.840
 sheltered from any problems whatsoever in life. And then my dad, his 21st birthday at the reactor,

32:49.120 --> 32:54.320
 you like work three hours a day, you sleep the rest. And, and I, yeah, so I played with a lot

32:54.320 --> 33:01.840
 of toys from Chernobyl. What are your memories of Chernobyl in general, like the bigger context,

33:01.840 --> 33:07.840
 you know, because of that HBO show is the world's attention turned to it once again.

33:08.560 --> 33:12.960
 Like, what are your thoughts about Chernobyl? Did Russia screw that one up? Like,

33:14.560 --> 33:17.440
 you know, there's probably a lot of lessons about our modern times with

33:18.080 --> 33:22.720
 data about coronavirus and all that kind of stuff. It seems like there's a lot of misinformation.

33:22.720 --> 33:30.320
 There's a lot of people kind of trying to hide whether they screwed something up or not,

33:30.320 --> 33:35.520
 as it's very understandable, it's very human, very wrong, probably. But obviously,

33:35.520 --> 33:42.320
 Russia was probably trying to hide that they screwed things up. Like, what are your thoughts

33:42.320 --> 33:50.240
 about that time, personal and general? I mean, I was born when the explosion happened. So,

33:50.800 --> 33:55.440
 actually a few months after, so of course, I don't remember anything, apart from the fact that my

33:55.440 --> 34:02.880
 dad would bring me tiny toys, plastic things that would just go crazy, haywire when you, you know,

34:02.880 --> 34:11.040
 put the gig or thing to it. My mom was like, just nuclear about that. She was like, what are you

34:11.040 --> 34:19.360
 bringing? You should not do that. She was nuclear, very nice. Absolutely. Well, but yeah, but the

34:19.360 --> 34:27.840
 TV show was just phenomenal. It's definitely, first of all, it's incredible how that was made

34:27.840 --> 34:35.920
 not by the Russians, but someone else, but capturing so well everything about our country.

34:37.120 --> 34:41.200
 It felt a lot more genuine that most of the movies and TV shows that are made now in Russia are

34:41.200 --> 34:46.080
 just so much more genuine. And most of my friends in Russia were just incomplete all about the

34:46.080 --> 34:51.280
 show, but I think the, how good of a job they did, phenomenal, but also the apartments. There's

34:51.280 --> 34:57.360
 something. Yeah, the set design. I mean, Russians can't do that. We, you know, but you see everything

34:57.360 --> 35:04.160
 and it's like, well, that's exactly how it was. So I don't know that show. I don't know what to

35:04.160 --> 35:11.280
 think about that because it's British accents, British actors of a person. I forgot who created

35:11.280 --> 35:17.040
 the show. I'm not, but I remember reading about him and he's not, he doesn't even feel like,

35:17.040 --> 35:21.600
 like there's no Russia in this history. No, he did like super bad or something like that. Or like,

35:22.720 --> 35:25.920
 I don't know, whatever that thing about the bachelor party in Vegas,

35:27.120 --> 35:31.040
 number four and five or something were the ones that he worked with. Yeah. But so he,

35:32.160 --> 35:39.040
 it made me feel really sad for some reason that if a person, obviously a genius could go in and

35:39.040 --> 35:46.960
 just study and just be extreme attention to detail, they can do a good job. It made me think like,

35:48.160 --> 35:55.200
 why don't other people do a good job of this? Like about Russia, like there's so little about

35:55.200 --> 36:04.320
 Russia. There's so few good films about the Russian side of World War II of, I mean, there's so much

36:04.320 --> 36:12.080
 interesting, evil and not and beautiful moments in the history of the 20th century in Russia

36:12.080 --> 36:17.840
 that feels like there's not many good films on from the Russian. You would expect something from

36:17.840 --> 36:23.520
 the Russians. Well, they keep making these propaganda movies now. Oh, no. Unfortunately.

36:23.520 --> 36:27.920
 But yeah, Chernobyl was such a perfect TV show. I think capturing really well,

36:27.920 --> 36:33.360
 it's not about like even the set design, which was phenomenal, but just capturing all the problems

36:33.360 --> 36:38.800
 that exist now with the country and like focusing on the right things. Like if you build the whole

36:38.800 --> 36:45.200
 country on a lie, that's what's going to happen. And that's just this very simple kind of thing.

36:47.920 --> 36:54.000
 Yeah. And did you have your dad talked about it to you? Like his thoughts on the experience?

36:55.600 --> 37:00.880
 He never talks. He's this kind of Russian man that just, my husband who's American,

37:00.880 --> 37:07.200
 and he asked him a few times like, Igor, how did you, but why did you say yes? Or like,

37:07.200 --> 37:11.040
 why did you decide to go? You could have said, no, not go to Chernobyl. Why would like a person

37:12.640 --> 37:21.280
 that's what you do? You cannot say no. Yeah. It's just like a Russian way.

37:21.280 --> 37:25.360
 It's the Russian way. Men don't talk that much. They're downsides and upsides for that.

37:25.360 --> 37:31.680
 Yeah. That's the truth. Okay. So back to post Putin Russia.

37:34.160 --> 37:42.080
 Or maybe we skipped a few steps along the way, but you were trying to do, to be a journalist

37:42.720 --> 37:50.720
 in that time. What was Russia like at that time? Post said 2007 Starbucks type of thing.

37:50.720 --> 37:57.280
 What else? What else was Russia like then? I think there was just hope. There was this

37:57.280 --> 38:03.280
 big hope that we're going to be friends with the United States and we're going to be friends with

38:03.920 --> 38:11.360
 Europe and we're just going to be also country like those with bike lanes and parks and

38:11.360 --> 38:15.920
 everything's going to be urbanized. Again, we're talking about 90s where people would be shot in

38:15.920 --> 38:22.160
 the street and I sort of have a fond memory of going into a movie theater and coming out of it

38:22.160 --> 38:28.080
 after the movie and the guy that I saw on the stairs was like either shot. Again, it was like

38:28.080 --> 38:33.040
 a thing in the 90s that would be happening. People were getting shot here and there.

38:34.240 --> 38:41.120
 Tons of violence. Tons of just basically mafia mobs in the streets. And then the 2000s were

38:41.120 --> 38:48.560
 like things just got cleaned up. Oil went up and the country started getting a little bit richer.

38:48.560 --> 38:54.720
 The 90s were so grim mostly because the economy was in shambles and oil prices were not high,

38:54.720 --> 39:01.200
 so the country didn't have anything. We defaulted in 1998 and the money kept jumping back and

39:01.200 --> 39:07.040
 forth. First, there were millions of rubbles, then it got to like thousands, then it was

39:07.040 --> 39:11.600
 one rubble with something, then again to millions. It was like crazy town.

39:12.720 --> 39:19.600
 And then the 2000s were just these years of stability in a way and the country getting

39:19.600 --> 39:26.080
 a little bit richer because of, again, oil and gas. And we started to look at specifically in

39:26.080 --> 39:32.320
 Moscow and St. Petersburg to look at other cities in Europe and New York and U.S. and

39:32.320 --> 39:37.360
 trying to do the same in our small kind of city towns there.

39:37.360 --> 39:39.920
 What were your thoughts of Putin at the time?

39:40.880 --> 39:46.160
 Well, in the beginning, he was really positive. Everyone was very positive about Putin. He was

39:46.160 --> 39:55.520
 young. He was very energetic. He also immediately, somewhat compared to, well, that was not way

39:55.520 --> 40:00.960
 before the shortlist era. The shortlist era. Okay, so he didn't start off shortly. When did

40:00.960 --> 40:04.800
 the shortlist era? It's like the propaganda of riding a horse, fishing.

40:04.800 --> 40:06.400
 2010, 11, 12.

40:06.400 --> 40:14.160
 Yeah. That's my favorite. People talk about the favorite beetles. That's my favorite

40:14.160 --> 40:15.680
 Putin is the shortlist Putin.

40:15.680 --> 40:21.440
 No, I remember very, very clearly 1996 where Americans really helped Russia with elections

40:21.440 --> 40:27.760
 and Yeltsin got reelected, thankfully so, because there's a huge threat that actually

40:27.760 --> 40:33.440
 the communists will get back to power. They were a lot more popular and then a lot of American

40:34.400 --> 40:41.520
 experts, political experts and campaign experts descended on Moscow and helped Yeltsin actually

40:41.520 --> 40:47.760
 get the presidency, the second term for the presidency, but Yeltsin was not feeling great

40:48.960 --> 40:56.080
 by the end of his second term. He was alcoholic. He was really old. He was falling off

40:56.080 --> 41:04.400
 the stages when he was talking. People were looking for a fresh face for someone who's

41:04.400 --> 41:09.920
 going to continue Yeltsin's work, but who's going to be a lot more energetic and a lot more

41:11.360 --> 41:17.600
 active, young, efficient, maybe. That's what we all saw in Putin back in the day.

41:17.600 --> 41:22.960
 I'd say that everyone, absolutely everyone in Russia in early 2000s who was not a communist

41:22.960 --> 41:26.720
 would be, yeah, Putin's great. We have a lot of hopes for him.

41:26.720 --> 41:33.440
 What are your thoughts, and I promise we'll get back to, first of all, your love story

41:34.000 --> 41:41.440
 and second of all, AI. Well, what are your thoughts about communism? The 20th century,

41:41.440 --> 41:47.040
 I apologize. I'm reading the rise and fall of the Third Reich. Oh my God.

41:47.040 --> 41:57.520
 So I'm really steeped into World War II and Stalin and Hitler and just these dramatic

41:57.520 --> 42:03.600
 personalities that brought so much evil to the world. But it's also interesting to politically

42:03.600 --> 42:12.640
 think about these different systems and what they've led to. Russia is one of the beacons

42:12.640 --> 42:17.840
 of communism in the 20th century. What are your thoughts about communism having

42:17.840 --> 42:22.320
 experienced it as a political system? I mean, I have only experienced it a little bit,

42:22.320 --> 42:27.920
 but mostly through stories and through seeing my parents and my grandparents who lived through that.

42:27.920 --> 42:32.240
 I mean, it was horrible. It was just plain horrible. It was just awful.

42:32.240 --> 42:43.200
 You think there's something... I mean, it sounds nice on paper. So the drawbacks of

42:43.200 --> 42:52.000
 capitalism is that eventually, it's the point of a slippery slope. Eventually, it creates

42:52.000 --> 43:06.480
 a disparity, inequality of wealth inequality. I guess it's hypothetical at this point, but

43:07.600 --> 43:12.480
 eventually capitalism leads to humongous inequality. And then some people argue that

43:12.480 --> 43:18.320
 that's a source of unhappiness. It's not like absolute wealth of people. It's the fact that

43:18.320 --> 43:24.320
 there's a lot of people much richer than you. There's a feeling of like, that's where unhappiness

43:24.320 --> 43:33.200
 can come from. So the idea of communism or these sort of Marxism is not allowing that kind of

43:33.200 --> 43:39.440
 slippery slope. But then you see the actual implementations of it and stuff seems to go

43:39.440 --> 43:47.360
 wrong very badly. What do you think that is? Why does it go wrong? What is it about human

43:47.360 --> 43:53.200
 nature? If you look at Chernobyl, these kinds of bureaucracies that were constructed,

43:54.480 --> 43:59.680
 is there something... Do you think about this much of why it goes wrong?

44:00.960 --> 44:10.160
 Well, it's not that everyone was equal. Obviously, the government and everyone close to that were

44:10.160 --> 44:19.840
 the bosses. So it's not like fully, I guess, this dream of equal life. So then I guess the

44:21.040 --> 44:28.080
 situation that Russia had in the Soviet Union, it was more a bunch of really poor people without

44:28.080 --> 44:36.800
 any way to make any significant fortune or build anything living under constant surveillance.

44:36.800 --> 44:44.240
 Surveillance from other people, like you can't even do anything that's not fully approved by the

44:44.960 --> 44:50.880
 dictatorship, basically. Otherwise, your neighbor will write a letter and you'll go to jail.

44:51.600 --> 45:01.840
 Absolute absence of actual law. It's a constant state of fear. You didn't own anything. You

45:01.840 --> 45:07.840
 couldn't go travel. You couldn't read anything western or you couldn't make a career, really,

45:07.840 --> 45:14.880
 unless you're working in the military complex, which is why most of the scientists were so well

45:14.880 --> 45:20.720
 regarded. I come from both my dad and my mom come from families of scientists and they were

45:20.720 --> 45:28.080
 really well regarded, as you know, obviously. Because there's a lot of value to them being

45:28.080 --> 45:33.120
 well regarded. Because they were developing things that could be used in the military.

45:34.480 --> 45:39.920
 So that was very important. That was the main investment. But it was miserable. It was miserable.

45:39.920 --> 45:45.200
 That's why a lot of Russians now live in the state of constant PTSD. That's why we want to

45:45.200 --> 45:51.440
 buy, buy, buy, buy, buy. Definitely. As soon as we have the opportunity, we just got to it

45:51.440 --> 45:56.960
 finally that we can own things. I remember the time that we got our first yogurts and that was

45:56.960 --> 46:03.840
 the biggest deal in the world. It was already in the 90s, by the way. What was your favorite food?

46:04.720 --> 46:10.960
 What was like, well, this is possible. Oh, fruit. Because we only had apples, bananas,

46:12.080 --> 46:19.120
 and watermelons, whatever people would grow in the Soviet Union. So there were no

46:19.920 --> 46:25.120
 pineapples or papaya or mango. You've never seen those fruit things. Those were so

46:25.120 --> 46:31.200
 ridiculously good. And obviously, you could not get any strawberries in winter or anything that's

46:31.200 --> 46:37.280
 not seasonal. So that was a really big deal. Seeing all these fruit things. Yeah, me too,

46:37.280 --> 46:45.280
 actually. I don't know. I don't think I have too many demons or addictions or so on. But I think

46:45.280 --> 46:50.800
 I've developed an unhealthy relationship with fruit that I still struggle with. Oh, you can get any

46:50.800 --> 46:58.240
 type of fruit, right? Also, these weird fruit like dragon fruit or something. All kinds of

46:58.240 --> 47:04.320
 different types of peaches, like cherries were killer for me. I know you say we had bananas and

47:04.320 --> 47:09.360
 so on, but I don't remember having the kind of banana. When I first came to this country,

47:09.920 --> 47:18.480
 the amount of banana, I literally got fat on bananas. Oh yeah, for sure. Delicious. And like

47:18.480 --> 47:25.280
 cherries, just the quality of the food. I was like, this is capitalism. That's pretty good.

47:25.280 --> 47:34.240
 It's delicious. Yeah, it's funny. It's funny. It's funny to read.

47:36.560 --> 47:45.040
 I don't know what to think of it. It's funny to think how an idea that's just written on paper

47:45.040 --> 47:52.080
 or when carried out amongst millions of people, how that gets actually, when it becomes reality,

47:52.080 --> 47:59.840
 what it actually looks like. Sorry, but I've been studying Hitler a lot recently,

48:00.720 --> 48:06.960
 and going through Mein Kampf, he pretty much wrote out of Mein Kampf everything he was going to do.

48:07.760 --> 48:14.640
 Unfortunately, most leaders, including Stalin didn't read it. But it's kind of terrifying and

48:14.640 --> 48:19.440
 I don't know. And amazing in some sense that you can have some words on paper,

48:20.240 --> 48:25.280
 and they can be brought to life, and they can either inspire the world or they can destroy

48:25.280 --> 48:32.800
 the world. And yeah, there's a lot of lessons to study in history that I think people don't study

48:32.800 --> 48:40.000
 enough now. One of the things I'm hoping with, I've been practicing Russian a little bit,

48:40.000 --> 48:47.920
 I'm hoping to sort of find, rediscover the beauty and the terror of Russian history

48:49.360 --> 48:57.120
 through this stupid podcast by talking to a few people. So anyway, I just feel like so much was

48:57.120 --> 49:04.560
 forgotten. So much was forgotten. I'll probably try to convince myself that you're a super busy

49:04.560 --> 49:11.760
 and super important person. Well, I'm going to try to befriend you to try to become a better Russian

49:11.760 --> 49:17.360
 because I feel like I'm a shitty Russian. Not that busy. So I can totally be a Russian Sherpa.

49:19.680 --> 49:30.080
 Yeah, but love. You're talking about your early days of being a little bit alone and finding a

49:30.080 --> 49:34.640
 connection with the world through being a journalist. What did love come into that?

49:36.000 --> 49:43.520
 I guess finding for the first time, some friends, it's a very simple story, some friends that all

49:43.520 --> 49:55.040
 of a sudden we, I guess we're at the same place with our lives. We're 25, 26, I guess. And

49:55.040 --> 50:00.640
 then somehow remember, and we just got really close and somehow remember this one day,

50:01.200 --> 50:07.680
 where it was one day in summer that we just stayed out outdoor the whole night and just

50:07.680 --> 50:12.240
 talked and for some unknown reason, it just felt for the first time that someone could

50:13.680 --> 50:21.520
 see me for who I am. And it just felt extremely good. And we fell asleep outside and just talking

50:21.520 --> 50:28.240
 and it was raining, it was beautiful, sunrise and it's really cheesy. But at the same time,

50:28.240 --> 50:32.720
 we just became friends in a way that I've never been friends with anyone else before.

50:33.520 --> 50:38.720
 And I do remember that before and after that you have this unconditional family sort of

50:40.800 --> 50:47.760
 and it gives you tons of power. It just basically gives you this tremendous power to do things in

50:47.760 --> 50:53.600
 your life and to change positively on many different levels.

50:53.600 --> 50:59.680
 Power because you could be yourself. At least you know that somewhere you can be

51:00.640 --> 51:07.200
 just yourself. If you don't need to pretend, you don't need to be great at work or tell some story

51:07.200 --> 51:12.000
 or sell yourself in somewhere or another. And so we became just really close friends and

51:12.000 --> 51:19.120
 then in a way, I started a company because he had a startup and I felt like I kind of want to

51:19.120 --> 51:25.680
 start up. It felt really cool. I don't know what I would really do, but I felt like I kind of need

51:25.680 --> 51:37.200
 a startup. Okay. So that pulled you into the startup world. Yeah. And then this closest friend

51:37.200 --> 51:42.320
 of mine died. We actually moved here to San Francisco together and then we went back for a visa to

51:42.320 --> 51:49.280
 Moscow and we lived together were roommates and we came back and he got hit by a car right in front

51:49.280 --> 51:58.400
 of Kremlin, you know, next to the river and died the same day in the hospital. So

52:00.320 --> 52:05.840
 and you've moved to America at that point? At that point, I was what about him? What about

52:05.840 --> 52:10.240
 Roman? Him too. He actually moved first. So I was always sort of trying to do what he was doing. So

52:11.040 --> 52:14.800
 I didn't like that he was already here and I was still, you know, in Moscow and we weren't

52:14.800 --> 52:19.680
 hanging out together all the time. So was he in San Francisco? Yeah, we were roommates.

52:20.320 --> 52:26.560
 So he just visited Moscow for a little bit? We went back for our visas. We had to get a stamp

52:26.560 --> 52:32.080
 in our passport for our work visas and the embassy was taken a little longer. So we stayed there for

52:32.080 --> 52:42.880
 a couple of weeks. What happened? So how did he die? He was crossing the street and the car was

52:42.880 --> 52:48.320
 going really fast and way over the speed limit and just didn't stop on the pedestrian cross

52:49.040 --> 52:57.360
 on the zebra and just ran over him. When was this? It was in 2015 on 28th of November. So it was

52:57.360 --> 53:05.200
 just a long ago now. But at the time, you know, I was 29. So for me, it was the first kind of

53:05.200 --> 53:13.360
 meaningful death in my life. You know, I had both sets of grandparents at the time. I didn't see

53:13.360 --> 53:19.360
 anyone so close die and death sort of existed, but as a concept. But definitely not as something

53:19.360 --> 53:25.280
 that would be, you know, happening to us anytime soon. And specifically our friends,

53:25.280 --> 53:31.120
 because we were, you know, we're still in our 20s or early 30s, and it still felt like the whole

53:31.120 --> 53:39.920
 life is, you know, you could still dream about ridiculous things even. So that was, it was just

53:39.920 --> 53:49.440
 really, really abrupt, I'd say. What did it feel like to lose him? Like that feeling of loss?

53:49.440 --> 53:55.680
 You talked about the feeling of love, having power. What is the feeling of loss, if you like?

53:57.200 --> 54:01.760
 Well, in Buddhism, there's this concept of Samaya where something really,

54:03.360 --> 54:10.160
 like, huge happens and then you can see very clearly. I think that was it. Like, basically

54:10.800 --> 54:16.240
 something changed so, changed me so much in such a short period of time that I could just see really,

54:16.240 --> 54:23.360
 really clearly what mattered or what not. Well, I definitely saw that whatever I was doing at work,

54:23.360 --> 54:29.120
 what didn't matter at all, and some of the things. And it was just this big realization,

54:29.120 --> 54:33.040
 what this very, very clear vision of what life's about.

54:35.040 --> 54:36.400
 You still miss him today?

54:38.080 --> 54:46.000
 Yeah, for sure. For sure. It was just this constant. I think it was, he was really important

54:46.000 --> 54:51.680
 for me and for our friends for many different reasons. And I think one of them,

54:52.400 --> 54:57.200
 being that we didn't just say goodbye to him, but we sort of said goodbye to our youth in a way,

54:57.920 --> 55:04.320
 it was like the end of an era and so many different levels. The end of Moscow as we knew it,

55:04.320 --> 55:10.000
 the end of us living through our 20s and kind of dreaming about the future.

55:10.000 --> 55:17.840
 Do you remember like last several conversations? Is there moments with him that stick out that

55:17.840 --> 55:21.680
 will kind of haunt you? And you're just when you think about him?

55:24.720 --> 55:28.080
 Yeah, well, his last year here in San Francisco, he was pretty depressed for,

55:28.080 --> 55:33.120
 as his startup was not going really anywhere. And he wanted to do something else. He wanted to do

55:33.120 --> 55:41.040
 build. He played with a bunch of ideas, but the last one he had was around

55:42.560 --> 55:50.400
 building a startup around death. So he applied to Y Combinator with the video that I had on my

55:50.400 --> 55:58.080
 computer. And it was all about disrupting death, thinking about new cemeteries more

55:58.080 --> 56:06.720
 biologically, like things that could be better biologically for humans. And at the same time,

56:06.720 --> 56:14.160
 having those digital avatars, these kind of AI avatars that would store all the memory about

56:14.160 --> 56:20.000
 a person that he could interact with. What year was this? 2015. Well, right before his death.

56:20.000 --> 56:24.640
 So it was like a couple of months before that he recorded that video. And so I found out my

56:24.640 --> 56:31.840
 computer was in our living room. He never got in, but he was thinking about it a lot somehow.

56:32.800 --> 56:35.120
 Does it have the digital avatar idea? Yeah.

56:35.920 --> 56:42.320
 That's so interesting. Well, he just says, well, the fish has this idea and he talks about,

56:42.320 --> 56:45.920
 like, I want to rethink how people grieve and how people talk about death.

56:45.920 --> 56:51.040
 Well, I was interested in this. Maybe someone who's depressed

56:51.040 --> 56:57.760
 is naturally inclined thinking about that. But I just felt this year in San Francisco,

56:57.760 --> 57:02.400
 we just had so much, I was going through a hard time, he was going through a hard time.

57:02.400 --> 57:07.120
 And we were definitely, I was trying to make him just happy somehow to make him feel better.

57:07.680 --> 57:14.480
 And it felt like this, I don't know, I just felt like I was taking care of him a lot. And

57:14.480 --> 57:21.600
 then he almost started to feel better. And then that happened. And I don't know, I just fell,

57:22.240 --> 57:26.400
 I just fell lonely again, I guess. And that was coming back to San Francisco in December,

57:27.040 --> 57:33.600
 I helped organize the funeral, help his parents. And I came back here and it was a really lonely

57:33.600 --> 57:39.520
 apartment, a bunch of his clothes everywhere, and Christmas time. And I remember I had a board

57:39.520 --> 57:44.640
 meeting with my investors, and I just couldn't talk about like, had to pretend everything's okay.

57:44.640 --> 57:53.680
 And, you know, just working on this company. Yeah, it was definitely very, very tough, tough time.

57:55.200 --> 58:04.160
 Do you think about your own mortality? You said, you know, we're young, the

58:04.160 --> 58:08.960
 the possibility of doing all kinds of crazy things is still out there. It's still

58:10.000 --> 58:16.720
 before us, but it can end any moment. Do you think about your own ending at any moment?

58:18.400 --> 58:25.200
 Unfortunately, I think about way too, about a way too much. It's somehow after Roman, like,

58:25.200 --> 58:30.240
 every year after that, I started losing people that I really love. I lost my grandfather next year.

58:30.240 --> 58:36.720
 My, you know, the person who would explain to me, you know, what the universe is made of.

58:37.600 --> 58:42.320
 Why are you selling apples? Well, selling apples. And then I lost another close friend of mine. And

58:44.560 --> 58:50.720
 it just made me very scared. I have tons of fear about death. That's what makes me not fall asleep

58:50.720 --> 58:58.960
 oftentimes and just go in loops. And then as my therapist recommended me, I opened up some

58:58.960 --> 59:04.720
 nice calming images with the voiceover. And it calms me down.

59:05.280 --> 59:10.720
 How for sleep? Yeah, I'm really scared of death. This is a big, I definitely have tons of,

59:12.080 --> 59:16.240
 I guess, some pretty big trauma about it and still working through.

59:16.960 --> 59:23.600
 There's a philosopher, Ernest Becker, who wrote a book, The Nile of Death. I'm not sure if you're

59:23.600 --> 59:31.520
 familiar with any of those folks. There's a, in psychology, a whole field called terror management

59:31.520 --> 59:38.960
 theory. Sheldon, who's just done the podcast, he wrote the book. He was the, we talked for four

59:38.960 --> 59:48.400
 hours about death. A fear of death. But his whole idea is that Ernest Becker, I think, I find this

59:48.400 --> 59:57.120
 idea really compelling is that everything human beings have created, like our whole motivation

59:57.120 --> 1:00:06.000
 in life, is to create like escape death. It's to try to construct an illusion

1:00:08.480 --> 1:00:16.960
 of that we're somehow immortal. So like everything around us, this room, your startup,

1:00:16.960 --> 1:00:28.480
 your dreams, all everything you do is a kind of creation of a brain unlike any other mammal or

1:00:28.480 --> 1:00:37.200
 species is able to be cognizant of the fact that it ends for us. I think, so there's the question

1:00:37.200 --> 1:00:44.480
 of like the meaning of life that you look at like what drives us humans. And when I read Ernest

1:00:44.480 --> 1:00:52.080
 Becker that I highly recommend people read is the first time I, it felt like this is the right

1:00:52.800 --> 1:00:59.280
 thing at the core. Sheldon's work is called warm at the core. So he's saying it's, I think it's

1:01:00.400 --> 1:01:07.680
 William James, he's quoting or whoever, is like the thing, what is at the core of it all? Sure,

1:01:07.680 --> 1:01:13.440
 there's like love, you know, Jesus might talk about like love is at the core of everything. I

1:01:13.440 --> 1:01:18.240
 don't, you know, that's the open question. What's at the, you know, it's turtles, turtles, but it

1:01:18.240 --> 1:01:24.800
 can't be turtles all the way down. What's at the bottom? And Ernest Becker says the fear of death

1:01:24.800 --> 1:01:34.480
 and the way, in fact, because you said therapists and calming images, his whole idea is, you know,

1:01:34.480 --> 1:01:40.880
 we want to bring that fear of death as close as possible to the surface because it's,

1:01:40.880 --> 1:01:49.600
 and like meditate on that and use the clarity of vision that provides to, you know, to live a

1:01:49.600 --> 1:01:58.800
 more fulfilling life, to live a more honest life, to discover, you know, there's something about,

1:01:58.800 --> 1:02:06.320
 you know, being cognizant of the finiteness of it all that might result in the most fulfilling life.

1:02:06.320 --> 1:02:10.560
 So that's the, that's the dual of what you're saying, because you kind of said it's like, I

1:02:10.560 --> 1:02:16.240
 unfortunately think about it too much. It's a question whether it's good to think about it,

1:02:16.240 --> 1:02:23.680
 because I've, again, I talk way too much about love and probably death. And when I ask people,

1:02:23.680 --> 1:02:30.240
 friends, which is why I probably don't have many friends, are you afraid of death? I think most

1:02:30.240 --> 1:02:39.040
 people say they're not, they're not what they, they say they're, they're afraid, you know,

1:02:39.040 --> 1:02:44.880
 it's kind of almost like they see death as this kind of like a paper deadline or something, and

1:02:44.880 --> 1:02:49.600
 they're afraid not to finish the paper before the paper, like, like, I'm afraid not to finish

1:02:51.600 --> 1:02:57.440
 the goals I have. But it feels like they're not actually realizing that this thing ends,

1:02:57.440 --> 1:03:03.840
 this thing ends, like really realizing, like really thinking as Nietzsche and all these

1:03:03.840 --> 1:03:13.680
 philosophers, like thinking deeply about it, like the very thing that, you know, like when

1:03:13.680 --> 1:03:18.880
 you think deeply about something, you can just, you can realize that you haven't actually thought

1:03:18.880 --> 1:03:28.320
 about it. Yeah. And I, and when I think about death, it's like, it can be, it's terrifying.

1:03:28.320 --> 1:03:34.000
 If it feels like stepping outside into the cold, where it's freezing, and then I have to like

1:03:34.000 --> 1:03:41.280
 hurry back inside where it's warm. But like, I think there's something valuable about stepping

1:03:41.280 --> 1:03:49.760
 out there into the freezing cold. Definitely. When I talk to my mentor about it, he always tells me,

1:03:50.480 --> 1:03:56.480
 well, what dies? There's nothing there that can die. But I guess that requires,

1:03:59.280 --> 1:04:04.560
 well, in Buddhism, one of the concepts that are really hard to grasp and that people spend

1:04:04.560 --> 1:04:12.160
 older lives meditating on would be Anata, which is the concept of not self, and kind of thinking

1:04:12.160 --> 1:04:14.960
 that, you know, if you're not your thoughts, which you're obviously not your thoughts,

1:04:14.960 --> 1:04:21.120
 because you can observe them and not your emotions and not your body, then what is this? And if you

1:04:21.120 --> 1:04:28.320
 go really far, then finally you see that there's not self, there's this concept of not self. So

1:04:29.280 --> 1:04:32.640
 once you get there, how can that actually die? What is dying?

1:04:32.640 --> 1:04:36.800
 Right, you're just a bunch of molecules, star dust.

1:04:38.640 --> 1:04:47.920
 But that is very advanced spiritual work for me. I'm definitely just, definitely not. Oh my God,

1:04:47.920 --> 1:04:53.120
 no, I have, I think it's very, very useful. It's just the fact that maybe being so afraid is not

1:04:53.120 --> 1:04:59.200
 useful. And mine is more, I'm just terrified, like it's really makes me on a personal level.

1:04:59.200 --> 1:05:07.840
 On a personal level, I'm terrified. How do you overcome that?

1:05:08.800 --> 1:05:14.160
 I don't. I'm still trying to. Have pleasant images?

1:05:15.680 --> 1:05:20.320
 Well, pleasant images get me to sleep. And then during the day, I can distract myself

1:05:20.320 --> 1:05:31.120
 with other things, like talking to you. I'm glad we're both doing the same exact thing. Okay, good.

1:05:32.000 --> 1:05:39.680
 Is there other, like, is there moments since you've lost Roman that you had like moments of

1:05:41.440 --> 1:05:48.560
 like bliss and like that you've forgotten that you have achieved that Buddhist like level of

1:05:48.560 --> 1:05:57.680
 like what can possibly die. I'm part like losing yourself in the moment in the ticking

1:05:59.120 --> 1:06:05.520
 time of like this universe. He's just part of it for a brief moment and just enjoying it.

1:06:06.720 --> 1:06:11.840
 Well, that goes hand in hand. I remember, I think a day or two after he died, we went to

1:06:12.560 --> 1:06:17.200
 finally get his passport out of the embassy and we're driving around Moscow and it was,

1:06:17.200 --> 1:06:22.640
 you know, December, which is usually there's never a sun in Moscow in December.

1:06:22.640 --> 1:06:28.400
 And somehow it was an extremely sunny day and we were driving with close friend.

1:06:30.400 --> 1:06:36.160
 And I remember feeling for the first time maybe this just moment of incredible clarity and somehow

1:06:36.160 --> 1:06:42.960
 happiness, not like happy happiness, but happiness and just feeling that, you know,

1:06:42.960 --> 1:06:50.560
 I know what the universe is sort of about, whether it's good or bad. And it wasn't a sad feeling,

1:06:50.560 --> 1:06:57.200
 it was probably the most beautiful feeling that you can ever achieve. And you can only get it

1:06:57.200 --> 1:07:04.000
 when something, oftentimes when something traumatic like that happens. But also if you just,

1:07:04.000 --> 1:07:08.240
 you really spend a lot of time meditating and looking at the nature doing something that

1:07:08.240 --> 1:07:13.440
 really gets you there. But once you're there, I think when you summit a mountain, a really hard

1:07:13.440 --> 1:07:19.360
 mountain, you inevitably get there. That's just a way to get to the state. But once you're on this,

1:07:19.360 --> 1:07:24.960
 in this state, you can do really big things, I think. Yeah.

1:07:25.760 --> 1:07:32.480
 Sucks it doesn't last forever. So Bukowski talked about like love is a fog. I like,

1:07:32.480 --> 1:07:39.440
 it's when you wake up in the morning, it's there, but it eventually dissipates. It's really sad.

1:07:39.440 --> 1:07:46.400
 Nothing lasts forever. But I definitely like doing this push up and running thing. There's moments,

1:07:46.400 --> 1:07:53.200
 I had a couple moments, like I'm not a crier, I don't cry. But there's moments where I was like

1:07:53.200 --> 1:08:03.040
 facedown on the carpet. Like with tears in my eyes is interesting. And then that complete, like,

1:08:03.040 --> 1:08:08.080
 there's a lot of demons. I've got demons, had to face them funny how running makes you face your

1:08:08.080 --> 1:08:13.920
 demons. But at the same time, the flip side of that, there's a few moments where I was in bliss.

1:08:13.920 --> 1:08:24.400
 In bliss. And all of it alone, which is funny. It's beautiful. I like that. But definitely pushing

1:08:24.400 --> 1:08:30.800
 yourself physically, one of it for sure. Yeah. Like you said, I mean, you were speaking as a

1:08:30.800 --> 1:08:37.040
 metaphor of Mount Everest, but it also works like literally, I think physical endeavor somehow.

1:08:37.040 --> 1:08:45.920
 Yeah, there's something, I mean, war monkeys, apes, whatever, physical, there's a physical thing to

1:08:45.920 --> 1:08:52.640
 it. But there's something to this, pushing yourself physically, physically, but alone. That happens

1:08:52.640 --> 1:08:57.360
 when you're doing like things like you do, or straightness, like workouts, or, you know,

1:08:57.360 --> 1:09:03.440
 rolling extra cross the Atlantic, or marathons. That's why I love watching marathons. And,

1:09:03.440 --> 1:09:08.480
 you know, it's so boring. But you can see them getting there.

1:09:09.360 --> 1:09:14.320
 So the other thing, I don't know if you know, there's a guy named David Goggins. He's a,

1:09:15.360 --> 1:09:20.320
 he basically, so he's been either emailing the phone with me every day through this. I haven't

1:09:20.320 --> 1:09:28.240
 been exactly alone. But he, he's kind of, he's the, he's the devil on the devil's shoulder.

1:09:28.240 --> 1:09:36.080
 So he's like the worst possible human being in terms of giving you a, like he has, through

1:09:36.080 --> 1:09:42.480
 everything I've been doing, he's been doubling everything I do. So he, he's insane. He's a

1:09:42.480 --> 1:09:48.800
 this Navy SEAL person. He's wrote this book, Can't Hurt Me. He's basically one of the toughest

1:09:48.800 --> 1:09:54.080
 human beings on earth. He ran all these crazy ultra marathons in the desert. He set the world

1:09:54.080 --> 1:10:01.840
 record in a number of pull ups. He's just does everything where it's like, he, like, how can

1:10:01.840 --> 1:10:08.960
 I suffer today? He figures that out and does it. Yeah, that, whatever that is, that process of

1:10:08.960 --> 1:10:14.320
 self discovery is really important. I actually had to turn myself off from the internet mostly

1:10:14.320 --> 1:10:20.080
 because I started this like workout thing, like a happy go getter with my like headband and like,

1:10:20.080 --> 1:10:25.680
 like, just like, uh, cause a lot of people are like inspired and they're like, yeah, we're gonna

1:10:25.680 --> 1:10:32.320
 exercise with you. And I was like, yeah, great, you know, but then like, I realized that this,

1:10:32.320 --> 1:10:42.000
 this journey can't be done together with others. This has to be done alone. So out of the moments

1:10:42.000 --> 1:10:50.800
 of love, out of the moments of loss, can we talk about your journey of finding, I think, an incredible

1:10:50.800 --> 1:10:58.640
 idea, an incredible company and incredible system in replica? How did that come to be?

1:10:59.840 --> 1:11:04.880
 So yeah, so I was a journalist and then I went to business school for a couple of years to

1:11:04.880 --> 1:11:10.400
 just see if I can maybe switch gears and do something else with 23. And then I came back

1:11:10.400 --> 1:11:17.680
 and started working for a businessman in Russia who built the first 4G network in our country

1:11:18.720 --> 1:11:25.760
 and was very visionary and asked me whether I want to do fun stuff together. And we worked

1:11:25.760 --> 1:11:33.840
 on a bank. The idea was to build a bank on top of a telco. So that was 2011. And then I went to

1:11:33.840 --> 1:11:41.440
 so that was 2011 or 2012. And a lot of telecommunication company, mobile network operators

1:11:42.240 --> 1:11:48.000
 didn't really know what to do next in terms of, you know, new products, new revenue. And this big

1:11:48.000 --> 1:11:54.960
 idea was that, you know, you put a bank on top and then all work works out. Basically, a prepaid

1:11:54.960 --> 1:12:02.080
 account becomes your bank account and you can use it as your bank. So, you know, a third of a country

1:12:02.080 --> 1:12:09.680
 wakes up as your bank client. But we couldn't quite figure out what would be the main interface to

1:12:09.680 --> 1:12:14.160
 interact with the bank. The problem was that most people didn't have smart smartphones back in the

1:12:14.160 --> 1:12:19.920
 time. In Russia, the penetration of smartphones was low. People didn't use mobile banking or

1:12:19.920 --> 1:12:25.920
 online banking on their computers. So we figured out that SMS would be the best way because that

1:12:25.920 --> 1:12:31.520
 would work on feature phones. But that required some chatbot technology, which I didn't know

1:12:31.520 --> 1:12:36.880
 anything about, obviously. So I started looking into it and saw that there's nothing really,

1:12:36.880 --> 1:12:40.640
 well, there wasn't just nothing really. So the idea is to SMS be able to interact with a bank

1:12:40.640 --> 1:12:45.440
 account. Yeah. And then we thought, well, since you're talking to a bank account, why can't this,

1:12:46.000 --> 1:12:52.720
 can't we use more of, you know, some behavioral ideas and why can't this banking chatbot be nice

1:12:52.720 --> 1:12:57.200
 to you and really talk to you sort of as a friend, this way you develop more connection to it,

1:12:57.200 --> 1:13:04.880
 retention is higher, people don't churn. And so I went to very depressing Russian cities to test

1:13:04.880 --> 1:13:13.760
 it out. I went to, I remember three different towns to interview potential users. So people

1:13:13.760 --> 1:13:20.080
 use it for a little bit. Cool. And I went to talk to them. Pretty poor towns. Very poor towns,

1:13:20.080 --> 1:13:28.240
 mostly towns that were, you know, sort of factories, mono towns, they were building something and then

1:13:28.240 --> 1:13:34.480
 the factory went away and it was just a bunch of very poor people. And then we went to a couple

1:13:34.480 --> 1:13:38.720
 that weren't as dramatic, but still the one I remember really fondly was this woman that

1:13:38.720 --> 1:13:44.720
 worked at a glass factory and she talked to a chatbot. And she was talking about it,

1:13:44.720 --> 1:13:48.800
 and she started crying during the interview because she said, no one really cares for me that much.

1:13:48.800 --> 1:13:58.080
 And so to be clear, that was my only endeavor in programming that chatbot. So it was really

1:13:58.080 --> 1:14:06.640
 simple. It was literally just a few, if this then that rules and it was incredibly simplistic.

1:14:06.640 --> 1:14:11.360
 And still that made her feel something. And that really made her emotional. She said,

1:14:11.360 --> 1:14:16.880
 you know, I only have my mom and my husband and I don't have any more really in my life.

1:14:16.880 --> 1:14:21.360
 And it was very sad. But at the same time, I felt and we had more interviews in a similar vein.

1:14:21.360 --> 1:14:28.240
 And what I thought in the moment was like, well, it's not that the technology is ready,

1:14:28.240 --> 1:14:34.960
 because definitely in 2012 technology was not ready for that. But humans are ready,

1:14:34.960 --> 1:14:40.400
 unfortunately. So this project would not be about like tech capabilities would be more about

1:14:40.400 --> 1:14:49.040
 human vulnerabilities. But there's something so so powerful around about conversational AI that

1:14:49.040 --> 1:14:54.720
 I saw then that I thought was definitely worth putting in a lot of effort into. So in the end

1:14:54.720 --> 1:15:02.240
 of the day, we saw the banking project. But my then boss was also my mentor and really,

1:15:02.240 --> 1:15:07.360
 really close friend, told me, hey, I think there's something in it. And you should just

1:15:07.360 --> 1:15:10.960
 go work on it. And I was like, well, what product? I don't know what I'm building.

1:15:10.960 --> 1:15:16.240
 He said, you'll figure it out. And, you know, looking back at this, this is a horrible idea

1:15:16.240 --> 1:15:21.120
 to work on something without knowing what it was, which is maybe the reason why it took us

1:15:21.840 --> 1:15:26.320
 so long. But we just decided to work on the conversational tech to see what it, you know,

1:15:26.320 --> 1:15:34.240
 there were no chatbot constructors or programs or anything that would allow you to actually build

1:15:34.240 --> 1:15:39.440
 one at the time. That was the era of, by the way, Google Glass, which is why, you know,

1:15:39.440 --> 1:15:43.600
 some of the investors, like seed investors who talked with were like, oh, you should totally

1:15:43.600 --> 1:15:47.360
 build it for Google Glass. If not, we're not. I don't think that's interesting.

1:15:47.360 --> 1:15:50.320
 Did you bite on that idea?

1:15:50.320 --> 1:15:58.400
 No. Because I wanted to be to do text first, because I'm a journalist. So I was fascinated

1:15:58.400 --> 1:16:05.760
 by just texting. So you thought, so the emotional, that interaction that the woman had,

1:16:06.560 --> 1:16:09.760
 like, so do you think you could feel emotion from just text?

1:16:11.120 --> 1:16:16.320
 Yeah, I saw something in just this pure texting and also thought that we should first start

1:16:17.120 --> 1:16:20.240
 building for people who really need it versus people who have Google Glass,

1:16:21.200 --> 1:16:24.080
 if you know what I mean. And I felt like the early adopters of Google Glass

1:16:24.080 --> 1:16:30.400
 might not be overlapping with people who are really lonely and might need someone to talk to.

1:16:32.800 --> 1:16:36.320
 But then we really just focused on the tech itself. We just thought, what if we just,

1:16:36.320 --> 1:16:41.040
 you know, we didn't have a product idea in the moment. And we felt, what if we just look into

1:16:42.560 --> 1:16:48.960
 building the best conversational constructors, so to say, use the best tech available at the

1:16:48.960 --> 1:16:53.200
 time. And that was before the first paper about deep learning applied to dialogues,

1:16:53.200 --> 1:16:59.760
 which happened in 2015, in August 2015, which Google published.

1:17:01.600 --> 1:17:08.240
 Did you follow the work of Lobner Prize and like all the sort of non machine learning

1:17:09.440 --> 1:17:14.320
 chatbots? Yeah, what really struck me was that, you know, there was a lot of talk about

1:17:14.320 --> 1:17:18.720
 machine learning and deep learning, like big data was a really big thing. Everyone was saying,

1:17:18.720 --> 1:17:23.920
 you know, the business was big data. 2012 is the biggest gaggle competitions were, you know,

1:17:25.360 --> 1:17:28.720
 important. But that was really the kind of upheaval of people started talking about machine

1:17:28.720 --> 1:17:34.080
 learning a lot. But it was only about images or something else. And it was never about

1:17:34.080 --> 1:17:37.360
 conversation. As soon as I looked into the conversational tech, it was all about

1:17:38.560 --> 1:17:43.360
 something really weird and very outdated and very marginal and felt very hobbyist. It was all about

1:17:43.360 --> 1:17:49.120
 Lobner Prize, which was won by a guy who built a chatbot that talked like a Ukrainian teenager.

1:17:49.120 --> 1:17:54.720
 It was just a gimmick. And somehow people picked up those gimmicks. And then, you know, the most

1:17:54.720 --> 1:18:02.480
 famous chatbot at the time was Eliza from 1980s, which was really bizarre or a smarter child on aim.

1:18:02.480 --> 1:18:08.960
 The funny thing is, it felt at the time not to be that popular. And it still doesn't seem to be

1:18:08.960 --> 1:18:14.640
 that popular. Like people talk about the Turing test. People like talking about it philosophically.

1:18:14.640 --> 1:18:20.640
 Journos like writing about it. But as a technical problem, like people don't seem to really want

1:18:20.640 --> 1:18:30.800
 to solve the open dialogue. Like they, they're not obsessed with it. Even folks like, you know,

1:18:30.800 --> 1:18:37.760
 in Boston, the Alexa team, even they're not as obsessed with it as I thought they might be.

1:18:37.760 --> 1:18:43.760
 Why not? What do you think? So, you know what you felt like? You felt with that woman when she

1:18:43.760 --> 1:18:49.520
 felt something by reading the text. I feel the same thing. There's something here, what you felt.

1:18:50.560 --> 1:18:57.920
 I feel like Alexa folks and just the machine learning world doesn't feel that,

1:18:58.640 --> 1:19:04.240
 that there's something here. Because they see as a technical problem, it's not that interesting,

1:19:04.240 --> 1:19:11.120
 for some reason. It could be argued that maybe as a purely sort of natural language processing

1:19:11.120 --> 1:19:17.200
 problem, it's not the right problem to focus on. Because there's too much subjectivity. That,

1:19:17.200 --> 1:19:23.440
 that thing that the woman felt like crying, like if, if, if your benchmark includes a woman crying,

1:19:24.320 --> 1:19:29.600
 that doesn't feel like a good benchmark test. But to me, there's something there that's,

1:19:29.600 --> 1:19:36.560
 you can have a huge impact. But I don't think the machine learning world likes that, the human

1:19:36.560 --> 1:19:42.000
 emotion, the subjectivity of it, the fuzziness, the fact that with maybe a single word, you can

1:19:42.000 --> 1:19:47.920
 make somebody feel something deeply. What is that? It doesn't feel right to them. So, I don't know.

1:19:47.920 --> 1:19:55.680
 I don't, I don't know why that is. I'm, that's why I'm excited. When I discovered your work,

1:19:55.680 --> 1:20:04.000
 it feels wrong to say that. It's not like I'm, I'm giving myself props for, for Googling and for

1:20:06.400 --> 1:20:14.160
 becoming a, for, for, for our, I guess, mutual friend and introducing us. But I'm so glad that

1:20:14.160 --> 1:20:18.880
 you exist and what you're working on. But I have the same kind of, if we could just backtrack

1:20:18.880 --> 1:20:23.920
 a second, because I have the same kind of feeling that there's something here. In fact, I've been

1:20:23.920 --> 1:20:31.280
 working on a few things that are kind of crazy, very different from your work. I think, I think

1:20:31.280 --> 1:20:40.800
 they're, I think they're too crazy, but the, like what? I have to know. No, all right. We'll talk

1:20:40.800 --> 1:20:48.800
 about it more. I feel like it's harder to talk about things that have failed and are failing

1:20:48.800 --> 1:20:55.120
 while you're a failure. Like it's easier for you because you're already successful

1:20:56.160 --> 1:21:05.440
 on some measures. Tell it to my board. Well, you're, you're, I think, I think

1:21:05.440 --> 1:21:08.720
 you've demonstrated success in a lot of my projects. It's easier for you to talk about

1:21:08.720 --> 1:21:20.400
 failures for me. I'm in the bottom currently of the, of the success. You're way too humble.

1:21:20.400 --> 1:21:25.040
 No. So it's hard for me to know, but there's something there. There's something there.

1:21:25.040 --> 1:21:30.880
 And I think you're, you're exploring that and you're discovering that. Yeah. It's been,

1:21:30.880 --> 1:21:37.600
 so it's been surprising to me, but I, you've mentioned this idea that you, you thought it wasn't

1:21:37.600 --> 1:21:44.640
 enough to start a company or start efforts based on, it feels like there's something here.

1:21:46.400 --> 1:21:53.440
 Like, what did you mean by that? Like you should be focused on creating a, like you should have

1:21:53.440 --> 1:21:59.120
 a product in mind. Is that what you meant? It just took us a while to discover the product

1:21:59.760 --> 1:22:06.080
 because it all started with a hunch of like, of me, my mentor and just sitting around and he was

1:22:06.080 --> 1:22:11.040
 like, well, this, that's it. There's, that's the, you know, the whole of Grail is there. There's

1:22:11.040 --> 1:22:17.360
 like, there's something extremely powerful in, in, in conversations. And there's no one who's

1:22:17.360 --> 1:22:22.640
 working on machine conversation from the right angle, so to say. I feel like that's still true.

1:22:23.520 --> 1:22:28.880
 Am I crazy? Oh no, I totally feel that's still true, which is, I think it's mind blowing.

1:22:28.880 --> 1:22:33.840
 Yeah. You know what it feels like? I wouldn't even use the word conversation because I feel

1:22:33.840 --> 1:22:39.440
 like it's the wrong word. It's like a machine connection or something. I don't know.

1:22:40.640 --> 1:22:45.120
 Because conversation, you start drifting into natural language immediately. You start drifting

1:22:45.120 --> 1:22:49.600
 immediately into all the benchmarks that are out there. But I feel like it's like the personal

1:22:49.600 --> 1:22:55.760
 computer days of this. Like, I feel like we're like in the early days with the, the, the Wozniak and

1:22:55.760 --> 1:23:03.840
 all of them, like where it was the same kind of, it was a very small niche group of people who are, who

1:23:03.840 --> 1:23:11.920
 are all kind of Lobner price type people. Yeah. And hobbyists, but like not even hobbyists with

1:23:11.920 --> 1:23:18.880
 big dreams. Like, you know, hobbyists with a dream to trick like a jury. Yeah. It's like a weird,

1:23:19.360 --> 1:23:24.720
 by the way, by the way, very weird. So if you think about conversations, first of all, when I

1:23:24.720 --> 1:23:31.120
 have great conversations with people, I'm not trying to test them. So for instance, if I try to break

1:23:31.120 --> 1:23:36.400
 them, if I'm actually playing along, I'm part of it. If I was trying to break it, break this person

1:23:36.400 --> 1:23:41.280
 or test whether he's going to give me a good conversation, it would have never happened. So

1:23:41.280 --> 1:23:47.280
 the whole, the whole problem with testing conversations is that you can put it in front

1:23:47.280 --> 1:23:53.280
 of a jury, because then you have to go into some touring test mode where is it responding to all

1:23:53.280 --> 1:24:00.080
 my factual questions, right? Or so it really has to be something in the field where people are

1:24:00.080 --> 1:24:05.440
 actually talking to it, because they want to, not because we're just trying to break it. And

1:24:05.440 --> 1:24:11.360
 it's working for them. Because this, the weird part of it is that it's, it's very subjective. It

1:24:11.360 --> 1:24:15.760
 takes two to tango here fully. If you're not trying to have a good conversation, if you're

1:24:15.760 --> 1:24:19.840
 trying to test it, then it's going to break. I mean, any person would break, to be honest,

1:24:19.840 --> 1:24:24.400
 if I'm not trying to even have a conversation with you, you're not going to give it to me.

1:24:25.120 --> 1:24:30.640
 If I keep asking you like some random questions or jumping from topic to topic, that wouldn't be,

1:24:31.280 --> 1:24:36.800
 which I'm probably doing, but that probably wouldn't contribute to the conversation. So I think the

1:24:36.800 --> 1:24:43.120
 problem of testing, so there should be some other metric, how do we evaluate whether that

1:24:43.120 --> 1:24:48.240
 conversation was powerful or not, which is what we actually started with. And I think those

1:24:48.240 --> 1:24:54.720
 measurements exist, and we can't test on those. But what really struck us back in the day and

1:24:54.720 --> 1:25:01.520
 what's still eight years later is still not resolved. And I'm not seeing tons of groups

1:25:01.520 --> 1:25:07.360
 working on it. Maybe I just don't know about them. It's also possible. But the interesting part about

1:25:07.360 --> 1:25:13.680
 is that most of our days we spend talking, and we're not talking about like those conversations

1:25:13.680 --> 1:25:21.600
 are not turned on the lights or customer support problems or some other task oriented things.

1:25:22.400 --> 1:25:27.040
 These conversations are something else. And then somehow they're extremely important for us. And

1:25:27.040 --> 1:25:32.960
 when we don't have them, then we feel deeply unhappy, potentially lonely, which as we know,

1:25:34.000 --> 1:25:41.600
 creates tons of risk for our health as well. And so this is most of our hours as humans.

1:25:41.600 --> 1:25:44.400
 And some of them, no one's trying to replicate that.

1:25:45.520 --> 1:25:48.320
 And not even study it that well.

1:25:48.960 --> 1:25:53.040
 And not even study that well. So when we jumped into that in 2012, I looked first at like,

1:25:53.040 --> 1:25:58.480
 okay, what's the chatbot? What's the state of the art chatbot? And, you know, those were the

1:25:58.480 --> 1:26:04.480
 Lobner Prize days. But I thought, okay, so what about the science of conversation? Clearly,

1:26:04.480 --> 1:26:11.200
 there have been tons of, there have been tons of, you know, scientists or people that academics

1:26:11.200 --> 1:26:14.800
 that looked into the conversation. So if I want to know everything about it, I can just read about

1:26:14.800 --> 1:26:21.360
 it. And there's not much really, there's there are conversational analysts who are basically just

1:26:22.560 --> 1:26:28.480
 listening to speech to different conversations, annotating them. And then,

1:26:30.080 --> 1:26:34.640
 I mean, that's not really used for much. That's the, that's the field of theoretical

1:26:34.640 --> 1:26:41.680
 linguistics, which is like barely useful. It's very marginal, even in their space, no one really

1:26:41.680 --> 1:26:46.640
 is excited. And I've never met a theoretical, theoretical linguist, because I can't wait to

1:26:46.640 --> 1:26:52.400
 work on the conversation and analytics. That is just something very marginal, sort of applied to

1:26:52.400 --> 1:26:58.400
 like writing scripts for salesmen, when they analyze which conversation strategies were

1:26:58.400 --> 1:27:04.320
 most successful for sales. Okay, so that was not very helpful. Then I looked a little bit deeper,

1:27:04.320 --> 1:27:11.200
 and then there, you know, whether there were any books written on what, you know, really contributes

1:27:11.200 --> 1:27:20.800
 to a great conversation. That was really strange, because most of those were NLP books, which is

1:27:20.800 --> 1:27:28.000
 neurolinguistic programming, which is not the NLP that I was expecting to be, but was mostly

1:27:28.000 --> 1:27:35.280
 some psychologist Richard Bandler, I think, came up with that, who was this big guy in a

1:27:35.280 --> 1:27:42.880
 leather vest that could program your mind by talking to you and how to be charismatic and

1:27:42.880 --> 1:27:46.640
 charming and influential as people, all those books. Yeah, pretty much. But it was all about,

1:27:46.640 --> 1:27:51.600
 like, through conversation reprogramming you, so getting to some, so that was, I mean,

1:27:51.600 --> 1:27:59.920
 probably not very, very true. And that didn't seem working very much, even back in the day.

1:28:00.480 --> 1:28:06.400
 And then there were some other books, like, I don't know, mostly just self help books around

1:28:06.400 --> 1:28:13.360
 how to be the best conversationalist, or how to make people like you, or some other stuff like

1:28:13.360 --> 1:28:20.000
 Dale Carnegie, whatever. And then there was this one book, The Most Human Human by Brian

1:28:20.000 --> 1:28:24.000
 Christensen, that really was important for me to read back in the day, because he was on the

1:28:25.280 --> 1:28:34.160
 human side. He was taking part in the London Reprise, but not as a human who's not a jury,

1:28:34.160 --> 1:28:38.560
 but who's pretending to be, who's basically, you have to tell a computer from a human,

1:28:38.560 --> 1:28:44.800
 and he was the human. So you would either get him or a computer. And his whole book was about

1:28:44.800 --> 1:28:50.160
 how do people, what makes us human in conversation. And that was a little bit more interesting,

1:28:50.160 --> 1:28:55.600
 because at that at least someone started to think about what exactly makes me human in

1:28:55.600 --> 1:29:00.560
 conversation and makes people believe in that. But it was still about tricking. It was still

1:29:00.560 --> 1:29:04.960
 about imitation game. It was still about, okay, what kind of parlor tricks can we throw in the

1:29:04.960 --> 1:29:10.000
 conversation to make you feel like you're talking to a human, not a computer? And it was definitely

1:29:10.000 --> 1:29:18.480
 not about thinking, what is that exactly that we're getting from talking all day long with

1:29:18.480 --> 1:29:23.200
 other humans? I mean, we're definitely not just trying to be tricked, or it's not just enough

1:29:23.200 --> 1:29:28.400
 to know it's a human. It's something we're getting there, can we measure it? And can we put the

1:29:29.600 --> 1:29:35.600
 computer to the same measurement and see whether you can talk to a computer and get the same results?

1:29:35.600 --> 1:29:39.440
 Yeah, I mean, so first of all, a lot of people comment that they think I'm a robot. It's very

1:29:39.440 --> 1:29:44.320
 possible I am a robot. And this whole thing, I totally agree with you that the test idea is

1:29:44.320 --> 1:29:51.200
 fascinating. And I looked for books unrelated to this kind of, so I'm afraid of people. I'm

1:29:51.200 --> 1:29:58.720
 generally introverted and quite possibly a robot. I literally Google like, how to talk to people and

1:29:58.720 --> 1:30:05.760
 like how to have a good conversation for the purpose of this podcast, because I was like, I

1:30:05.760 --> 1:30:12.240
 can't, I can't make eye contact with people. I can't like, I do Google that a lot too. You're

1:30:12.240 --> 1:30:18.080
 probably reading a bunch of FBI negotiation tactics. Is that what you're getting? Well,

1:30:18.080 --> 1:30:24.560
 everything you've listed, I've gotten there's been very few good books on even just like how to

1:30:24.560 --> 1:30:36.640
 interview well. It's rare. So what I end up doing often is I watch like with a critical eye. It's

1:30:36.640 --> 1:30:43.280
 so different when you just watch a conversation, like just for the fun of it, just as a human.

1:30:43.280 --> 1:30:47.920
 And if you watch a conversation is like trying to figure out, why is this awesome?

1:30:47.920 --> 1:30:53.920
 I'll listen to a bunch of different styles of conversation. I mean, I'm a fan of

1:30:54.560 --> 1:31:00.960
 podcasts, Joe Rogan. He's, you know, people can make fun of him or whatever and dismiss him. But

1:31:00.960 --> 1:31:10.240
 I think he's an incredibly artful conversationalist. He can pull people in for hours. And there's

1:31:10.240 --> 1:31:20.640
 another guy I watch a lot. He hosted a late night show. His name is Craig Ferguson. So he's like

1:31:20.640 --> 1:31:29.840
 very kind of flirtatious. But there's a magic about his like, about the connection he can create

1:31:29.840 --> 1:31:35.200
 with people, how he can put people at ease. And just like, I see, I've already started sounding

1:31:35.200 --> 1:31:39.520
 like those, I know, pee people or something. I'm not, I don't mean it in that way. I don't mean like,

1:31:39.520 --> 1:31:44.720
 how to charm people or put them at ease and all that kind of stuff. It's just like, what is that?

1:31:45.440 --> 1:31:51.280
 Why is that fun to listen to that guy? Why is that fun to talk to that guy? What is that?

1:31:52.480 --> 1:31:58.720
 Because he's not saying, I mean, it's so often boils down to a kind of wit

1:31:58.720 --> 1:32:08.400
 and humor, but not really humor. It's like, I don't know, I have trouble actually even articulating

1:32:08.400 --> 1:32:19.360
 correctly. But it feels like there's something going on that's not too complicated that could be learned.

1:32:19.360 --> 1:32:29.040
 And it's not similar to, yeah, to like, like you said, like the touring test. It's something else.

1:32:29.040 --> 1:32:37.360
 I'm thinking about a lot all the time. I do think about all the time. I think when we were looking,

1:32:37.360 --> 1:32:42.320
 so we started the company, we just decided to build a conversational attack where we thought,

1:32:42.320 --> 1:32:46.960
 well, there's nothing for us to build this chatbot that we want to build. So let's just

1:32:46.960 --> 1:32:52.880
 first focus on building some tech, building the tech side of things.

1:32:52.880 --> 1:32:54.480
 Without a product in mind.

1:32:54.480 --> 1:33:00.240
 Without a product in mind. We added like a demo chatbot that would recommend you restaurants

1:33:00.240 --> 1:33:04.320
 and talk to you about restaurants just to show something simple to people that people could

1:33:05.200 --> 1:33:11.280
 relate to and could try out and see whether it works or not. But we didn't have a product in

1:33:11.280 --> 1:33:16.080
 mind yet. We thought we would try venture chatbots and find out what it is.

1:33:16.080 --> 1:33:20.400
 Try venture chatbots and figure out our consumer application. And we sort of remember that we

1:33:20.400 --> 1:33:25.200
 wanted to build that kind of friend, that sort of connection that we saw in the very beginning.

1:33:25.920 --> 1:33:29.920
 But then we got to a white combinator and moved to San Francisco and forgot about it.

1:33:29.920 --> 1:33:34.160
 You know, everything is, then it was just this constant grind. How do we get funding?

1:33:34.160 --> 1:33:39.840
 How do we get this? You know, investors really just focus on one thing, just get it out there.

1:33:39.840 --> 1:33:45.360
 So somehow we start building a restaurant recommendation chatbot for real for a little

1:33:45.360 --> 1:33:50.560
 bit, not for too long. And then we tried building 40, 50 different chatbots. And then all of a

1:33:50.560 --> 1:33:56.720
 sudden we wake up and everyone is obsessed with chatbots. Somewhere in 2016 or end of 15,

1:33:56.720 --> 1:34:01.600
 people started thinking that's really the future. That's the new, you know, the new apps will be

1:34:01.600 --> 1:34:08.080
 chatbots. And we were very perplexed because people started coming up with companies that

1:34:08.080 --> 1:34:12.160
 I think we tried most of those chatbots already and there were like no users.

1:34:12.160 --> 1:34:18.480
 But still people were coming up with a chatbot that would tell you whether and bring you news and

1:34:18.480 --> 1:34:23.120
 this and that and we couldn't understand whether it would, you know, we were just didn't execute well

1:34:23.120 --> 1:34:30.480
 enough or people are not really, people are confused and are going to find out the truth,

1:34:30.480 --> 1:34:32.320
 the truth that people don't need chatbots like that.

1:34:32.320 --> 1:34:36.880
 So the basic idea is that you use chatbots as the interface to whatever application.

1:34:36.880 --> 1:34:40.960
 Yeah, the idea that was like this perfect universal interface to anything.

1:34:40.960 --> 1:34:46.400
 When I looked at that, it just made me very perplexed because I didn't think,

1:34:46.400 --> 1:34:51.520
 I didn't understand how that would work because I think we tried most of that and none of those

1:34:51.520 --> 1:34:55.600
 things worked. And then again, that crisis died down, right?

1:34:56.640 --> 1:35:00.880
 Fully, I think now it's impossible to get anything funded if it's a chatbot.

1:35:00.880 --> 1:35:03.760
 I think it's similar to, sorry to interrupt, but there's,

1:35:04.960 --> 1:35:09.520
 there's times when people think like with gestures, you can control devices,

1:35:09.520 --> 1:35:15.520
 like basically gesture based control things. It feels similar to me because like,

1:35:16.400 --> 1:35:22.160
 it's so compelling that we just like, like Tom Cruise, I can control stuff with my hands.

1:35:22.800 --> 1:35:27.680
 But like when you get down to it, it's like, well, why don't you just have a touch screen?

1:35:27.680 --> 1:35:30.960
 Or why don't you just have like a physical keyboard and mouse?

1:35:30.960 --> 1:35:39.200
 It's a, it's, so that chat was always, yeah, it was perplexing to me.

1:35:39.840 --> 1:35:44.560
 I still feel augmented reality, even virtual realities in that ballpark

1:35:45.280 --> 1:35:50.080
 in terms of it being a compelling interface. I think there's going to be incredible,

1:35:50.880 --> 1:35:57.840
 rich applications, just how you're thinking about it, but they won't just be the interface to everything.

1:35:57.840 --> 1:36:04.160
 It'll be its own thing that will create like amazing magical experience in its own right.

1:36:05.040 --> 1:36:09.280
 Absolutely, which is, I think kind of the right thing to go about, like what's the

1:36:09.280 --> 1:36:13.840
 magical experience with that, with that interface specifically.

1:36:13.840 --> 1:36:15.920
 How did you discover that for a replica?

1:36:16.640 --> 1:36:19.840
 I just thought, okay, we'll have this tech, we can build any chatbot we want.

1:36:19.840 --> 1:36:23.760
 We have the most, at that point, the most sophisticated tech that other companies have.

1:36:23.760 --> 1:36:30.400
 I mean, startups obviously not, probably not bigger ones, but still because we've been working on it

1:36:30.400 --> 1:36:35.440
 for a while. So I thought, okay, we can build any conversation. So let's just create a scale

1:36:35.440 --> 1:36:40.560
 from one to 10. And one would be conversations that you'd pay to not have. And 10 would be

1:36:40.560 --> 1:36:44.320
 conversations you'd pay to have. And I mean, obviously, we want to build a conversation

1:36:44.320 --> 1:36:49.600
 that people would pay to, you know, to actually have. And so for the whole, you know, for a few

1:36:49.600 --> 1:36:52.720
 weeks, me and the team were putting all the conversations we were having during the day

1:36:52.720 --> 1:36:58.240
 on the scale. And very quickly, you know, we figured out that all the conversations that we would

1:36:58.240 --> 1:37:06.480
 pay to never have were conversations we were trying to cancel, Comcast, or talk to customer

1:37:06.480 --> 1:37:12.320
 support, or make a reservation, or just talk about logistics with a friend when we're trying

1:37:12.320 --> 1:37:18.320
 to figure out where someone is and where to go, or all sorts of, you know, setting up

1:37:18.320 --> 1:37:24.320
 scheduling meetings, that was just conversation we definitely didn't want to have. Basically,

1:37:24.320 --> 1:37:29.040
 everything task oriented was a one, because if there was just one button for me to just,

1:37:29.040 --> 1:37:33.920
 or not even a button, if I could just think, and there was some magic BCI that would just

1:37:33.920 --> 1:37:40.400
 immediately transform that into an actual, you know, into action, that would be perfect. But

1:37:40.400 --> 1:37:48.400
 the conversation there was just this boring, not useful, and dull, and very also very inefficient

1:37:48.400 --> 1:37:53.120
 thing, because it was so many back and forth stuff. And as soon as we looked at the conversation

1:37:53.120 --> 1:37:58.160
 that we would pay to have, those were the ones that, well, first of all, therapists, because

1:37:58.160 --> 1:38:02.480
 we actually paid to have those conversations. And we'd also try to put like dollar amounts. So,

1:38:02.480 --> 1:38:07.440
 you know, if I was calling Comcast, I would pay $5 to not have this one hour talk on the phone,

1:38:07.440 --> 1:38:14.240
 I would actually pay straight up like money, hard cash. But it just takes a long time. It takes a

1:38:14.240 --> 1:38:20.480
 really long time. But as soon as we started talking about conversations that we would pay for,

1:38:20.480 --> 1:38:27.520
 those were therapists, all sorts of therapists, coaches, old friend, someone I haven't seen

1:38:27.520 --> 1:38:35.520
 for a long time, stranger on a train, weirdly stranger, stranger in a line for coffee and

1:38:35.520 --> 1:38:41.760
 ice back and forth with that person was like a good five solid five, six, maybe not a 10,

1:38:41.760 --> 1:38:46.000
 maybe I won't pay money. But at least I won't, you know, pay money to not have one. So that was

1:38:46.000 --> 1:38:51.680
 pretty good. Some intellectual conversations for sure. But more importantly, the one thing that

1:38:51.680 --> 1:39:03.360
 really was was making those very important and very valuable for us were the conversation where we

1:39:03.360 --> 1:39:09.120
 could, where we could be pretty emotional. Yes, some of them were about being witty and about

1:39:09.120 --> 1:39:14.240
 intellectually being intellectually stimulated, but those were interestingly more rare. And

1:39:14.880 --> 1:39:18.720
 most of the ones that we thought were very valuable were the ones where we could be vulnerable.

1:39:19.680 --> 1:39:27.440
 And interestingly, where we could talk more. So we like I could me and the team. So we're

1:39:27.440 --> 1:39:30.640
 talking about it, like, you know, a lot of these conversations like a therapist,

1:39:30.640 --> 1:39:35.040
 I mean, it was mostly me talking, or like an old friend that I was like opening up and crying,

1:39:35.040 --> 1:39:41.120
 and it was again me talking. And so that was interesting, because I was like, well,

1:39:41.120 --> 1:39:46.320
 maybe it's hard to build a chatbot that can talk to you very well and in a witty way,

1:39:46.320 --> 1:39:53.360
 but maybe it's easier to build the chatbot that could listen. So that was, that was kind of the

1:39:53.360 --> 1:39:58.720
 first, the first not just this direction. And then when my, when my friend died, where we just

1:39:58.720 --> 1:40:02.480
 built, you know, at that point, we were kind of so struggling to find the right application.

1:40:03.520 --> 1:40:07.600
 And I just felt very strong that all the chatbots would build so far, just meaningless. And this

1:40:07.600 --> 1:40:13.440
 whole grind, the startup grind, and how do we get to, you know, the next fundraising and,

1:40:13.440 --> 1:40:18.320
 you know, how can I talk, you know, talking to other founders and what's for your investors

1:40:18.320 --> 1:40:23.520
 and how are you doing? Are you killing it? Because we're killing it? I just felt that this is just

1:40:23.520 --> 1:40:28.080
 this is intellectually for me is exhausting having encountered those folks.

1:40:28.640 --> 1:40:33.840
 It just felt very, very much a waste of time. I just feel like Steve Jobs

1:40:35.280 --> 1:40:40.720
 and Elon Musk did not have these conversations, or at least did not have them for long.

1:40:41.840 --> 1:40:47.280
 That's for sure. But I think, you know, yeah, at that point, it just felt like, you know, I felt,

1:40:47.280 --> 1:40:54.240
 well, I just didn't want to build a company that was never my intention just to build something

1:40:54.240 --> 1:40:59.040
 successful or make money. It would be great. It would have been great. But I'm not, you know,

1:40:59.040 --> 1:41:06.160
 I'm not really a startup person. I'm not, you know, I was never very excited by the grind

1:41:06.160 --> 1:41:12.960
 by itself and or just being successful for building whatever it is and not being into what I'm

1:41:12.960 --> 1:41:19.920
 doing really. And so I just took a little break because I was a little, you know, I was upset

1:41:19.920 --> 1:41:24.880
 with my company and I didn't know what we were building. So I just took our technology and our

1:41:24.880 --> 1:41:29.600
 little dialogue constructor and some models, some deep learning models, which at that point we were

1:41:29.600 --> 1:41:35.440
 really into and really invested a lot and built a little chatbot for a friend of mine who passed.

1:41:36.240 --> 1:41:40.560
 And the reason for that was mostly that video that I saw and him talking about the digital

1:41:40.560 --> 1:41:45.840
 avatars. And Roman was that kind of person, like he was obsessed with, you know, just watching

1:41:45.840 --> 1:41:50.240
 YouTube videos about space and talking about, well, if I could go to Mars now, even if I didn't

1:41:50.240 --> 1:41:55.440
 know if I could come back, I would definitely pay any amount of money to be on that first

1:41:55.440 --> 1:42:00.480
 shuttle. I don't care whether you're dying. Like he was just the one that would be okay with,

1:42:00.480 --> 1:42:06.480
 you know, with trying to be the first one. And, you know, and so excited about all sorts of

1:42:06.480 --> 1:42:12.080
 things like that. And he was all about fake it to make it. And just, and I felt like,

1:42:13.600 --> 1:42:18.880
 and I was really perplexed that everyone just forgot about him. Maybe it was our way of coping,

1:42:18.880 --> 1:42:24.400
 mostly young people coping with the loss of a friend. Most of my friends just stopped talking

1:42:24.400 --> 1:42:30.800
 about him. And I was still living in an apartment with all his clothes. And, you know, paying the

1:42:30.800 --> 1:42:37.920
 whole lease for it and just kind of by myself in December. So it was really sad. And I didn't

1:42:37.920 --> 1:42:42.080
 want him to be forgotten. First of all, I never thought that people forget about dead people so

1:42:42.080 --> 1:42:47.120
 fast. People pass away, people just move on. And it was astonishing for me because I thought, okay,

1:42:47.120 --> 1:42:53.440
 well, he was such a mentor for so many of our friends. He was such a brilliant person. He was

1:42:53.440 --> 1:42:58.720
 somewhat famous in Moscow. How is that that no one's talking about him? Like I'm spending days

1:42:58.720 --> 1:43:04.640
 and days and we don't bring him up. And there's nothing about him that's happening. It's like

1:43:04.640 --> 1:43:12.320
 he was never there. And I was reading this, you know, the book, The Year of Magical Thinking

1:43:12.320 --> 1:43:17.520
 by Joan Didion about her losing and blue nights about her losing

1:43:19.760 --> 1:43:24.240
 her husband, her daughter. And the way to cope for her was to write those books.

1:43:24.240 --> 1:43:31.440
 And it was sort of like a tribute. And I thought, you know, I'll just do that for myself. And you

1:43:31.440 --> 1:43:38.000
 know, I'm a very bad writer and a poet, as we know. So I thought, well, I have this tech and

1:43:38.000 --> 1:43:45.360
 maybe that would be my little postcard, like postcard for him. So I built a chat bot to just

1:43:45.360 --> 1:43:52.000
 talk to him. And it felt really creepy and weird a little bit for a little bit. I just didn't want

1:43:52.000 --> 1:44:00.640
 to tell other people because it felt like I'm telling about having a skeleton. It was just felt

1:44:00.640 --> 1:44:07.360
 really, I was a little scared that I would be not it won't be taken. But it worked interestingly

1:44:07.920 --> 1:44:13.600
 pretty well. I mean, it made tons of mistakes, but it still felt like him. Granted, it was like 10,000

1:44:13.600 --> 1:44:17.920
 messages that I threw into a retrieval model that would just re rank that data set and just a few

1:44:17.920 --> 1:44:24.400
 scripts on top of that. But it also made me go through all of the messages that we had. And

1:44:24.400 --> 1:44:30.800
 then I asked some of my friends to send some through. And it felt the closest to feeling

1:44:30.800 --> 1:44:35.760
 like him present. Because you know, his Facebook was empty and Instagram was empty,

1:44:35.760 --> 1:44:40.720
 or there were a few links, and you couldn't feel like it was him. And the only way to fill him was

1:44:40.720 --> 1:44:46.400
 to read some of our text messages and go through some of our conversations. Because we just always

1:44:46.400 --> 1:44:50.640
 had them, even if we were sleeping like next to each other and two bedrooms separate by wall,

1:44:50.640 --> 1:44:56.880
 we were just texting back and forth, texting away. And there was something about this ongoing

1:44:56.880 --> 1:45:01.600
 dialogue that was so important that I just didn't want to lose all of a sudden. And maybe it was

1:45:01.600 --> 1:45:08.480
 magical thinking or something. And so we built that and I just used it for a little bit. And we

1:45:08.480 --> 1:45:18.000
 kept building some crappy chatbots with a company. But then a reporter came to talk to me, I was

1:45:18.000 --> 1:45:22.800
 trying to pitch our chatbots to him. And he said, do you even use any of those? I'm like, no. He's

1:45:22.800 --> 1:45:27.200
 like, so do you talk to any chatbots at all? And I'm like, well, you know, I talked to my

1:45:27.200 --> 1:45:33.440
 dad friends chatbot. And he wrote a story about that. And all of a sudden became pretty viral.

1:45:33.440 --> 1:45:39.840
 A lot of people wrote about it. And yeah, I've seen a few things written about you. The things

1:45:39.840 --> 1:45:48.960
 I've seen are pretty good writing. You know, most AI related things make my eyes roll like when

1:45:48.960 --> 1:45:58.480
 the press like, what kind of sound is that actually? Okay, sounds like it sounds like a truck. Okay,

1:45:58.480 --> 1:46:05.840
 sounds like an elephant at first. I got excited. You never know. This is 2020. I mean, it was such

1:46:05.840 --> 1:46:13.920
 a human story. It was well written, well researched. I forget what where I read them. But so I'm glad

1:46:13.920 --> 1:46:21.440
 somehow somebody found you to be the good writers were able to connect to the story. There must be

1:46:21.440 --> 1:46:29.920
 a hunger for this story. It definitely was. And I don't know what happened. But I think the idea

1:46:29.920 --> 1:46:35.680
 that he could bring back someone who's dad, and it's very much wishful, you know, magical thinking.

1:46:35.680 --> 1:46:41.600
 But the fact that you could still get to know him. And you know, seeing the parents for the first

1:46:41.600 --> 1:46:48.640
 time talk to the chatbot and some of the friends. And it was funny because we have this big office

1:46:48.640 --> 1:46:56.080
 in Moscow, where my team is working, you know, our Russian part is working out off. And I was there

1:46:56.080 --> 1:46:59.520
 when I wrote, I just wrote a post on Facebook, it's like, Hey, guys, like I built this if you want,

1:46:59.520 --> 1:47:05.920
 you know, just if it felt important, if you want to talk to Roman. And I saw a couple of his friends

1:47:05.920 --> 1:47:10.160
 are common friends, like, you know, reading a Facebook downloading, trying and a couple of them

1:47:10.160 --> 1:47:14.720
 cried. And it was just very, and not because it was something some incredible technology or anything,

1:47:14.720 --> 1:47:21.600
 it made so many mistakes, it was so simple. But it was all about, that's the way to remember a person

1:47:21.600 --> 1:47:27.040
 in a way. And you know, we don't have, we don't have the culture anymore, we don't have, you know,

1:47:27.040 --> 1:47:32.960
 no one's sitting Shiva, no one's taking weeks to actually think about this person. And in a way,

1:47:32.960 --> 1:47:38.800
 for me, that was it. So that was just day, day in, day out, thinking about him and putting this

1:47:38.800 --> 1:47:44.640
 together. So that was, that just felt really important. And that somehow resonated with a bunch

1:47:44.640 --> 1:47:49.360
 of people. And, you know, I think some movie producers bought the rights for the story. And

1:47:49.360 --> 1:47:54.800
 it's just everyone was so. Wait, has anyone made a movie yet? I don't think so. There were a lot of

1:47:54.800 --> 1:48:02.400
 TV episodes about that, but not really. Is that still on the table? I think so. I think so.

1:48:02.400 --> 1:48:08.560
 Which is really, that's cool. You're like a young, you know, like a,

1:48:09.600 --> 1:48:15.360
 like a Steve Jobs type of, let's see what happens. They're sitting on it. But you know,

1:48:15.360 --> 1:48:20.000
 for me, it was so important because Roman was really wanted to be famous. He really badly

1:48:20.000 --> 1:48:23.840
 wanted to be famous. He was all about like make it to like fake it to make it. I want to be, you

1:48:23.840 --> 1:48:30.720
 know, want to make it here in America's wall. And, and he couldn't, and I felt there's, you know,

1:48:30.720 --> 1:48:36.880
 there was sort of paying my dues to him as well, because all of a sudden he was everywhere. And

1:48:36.880 --> 1:48:41.200
 I remember Casey Newton, who was writing the story for the Verge, he was, he told me, Hey,

1:48:41.200 --> 1:48:48.720
 by the way, I was just going through my inbox, inbox. And I saw a search for Roman for the story.

1:48:49.280 --> 1:48:53.200
 And I saw an email from him where he sent me his startup and he said, I really like, I really

1:48:53.200 --> 1:48:57.520
 want to be featured in the Verge. Can you please write about it or something or like pitching

1:48:57.520 --> 1:49:02.160
 the story? And he said, I'm sorry, like, that's not, you know, good enough for us or something.

1:49:02.160 --> 1:49:07.360
 And he passed. And he said, and there, there were just so many of these little details where like,

1:49:07.360 --> 1:49:13.120
 he would find is like, you know, and we're finally writing, I know how much Roman wanted to be in

1:49:13.120 --> 1:49:17.920
 the Verge and how much he wanted the story to be written by Casey. And I'm like, well, that's,

1:49:18.560 --> 1:49:23.200
 maybe he will be, we're always joking that he was like, I can't wait for someone to make a movie

1:49:23.200 --> 1:49:29.120
 about us. And I hope Ron Gosling can play me. Ron Gosling. I don't know. You know, I still

1:49:29.120 --> 1:49:35.520
 have some things that I, oh, Roman still, but that'll be, I got in just to meet Alex Garland,

1:49:35.520 --> 1:49:44.960
 who wrote Ex Machina. And I, yeah, the movie's good, but the guy's better than the, like,

1:49:44.960 --> 1:49:51.200
 he's a special person, actually. I don't think he's made his best work yet. Like, for my interaction

1:49:51.200 --> 1:49:57.120
 with him, he's a really, really good and brilliant, the good human being and a brilliant director and

1:49:57.120 --> 1:50:06.640
 writer. So, yeah, so I hope, like he made me also realize that not enough movies have been made of

1:50:06.640 --> 1:50:11.040
 this kind. So it's, it's, you have to be made, they're probably sitting waiting for you to get

1:50:11.040 --> 1:50:18.960
 famous, like even more famous. You should get there. But it felt really special though. But

1:50:18.960 --> 1:50:23.120
 at the same time, our company wasn't going anywhere. So that was just kind of bizarre that we were

1:50:23.120 --> 1:50:26.080
 getting all this press for something that didn't have anything to do with our company.

1:50:28.080 --> 1:50:32.800
 And, but then a lot of people started talking to Roman, some shared their conversations. And

1:50:32.800 --> 1:50:39.120
 what we saw there was that also our friends in common, but also just strangers were really

1:50:39.120 --> 1:50:45.280
 using it as a confession booth or as a therapist or something. They were just really telling Roman

1:50:45.280 --> 1:50:50.400
 everything, which was by the way, pretty strange because it was a chatbot of a dead friend of mine

1:50:50.400 --> 1:50:57.760
 who was barely making any sense, but people were opening up. And we thought we'd just built a

1:50:57.760 --> 1:51:04.080
 prototype of replica, which would be an AI friend that everyone could talk to. Because we saw that

1:51:04.080 --> 1:51:12.480
 there is demand. And then also it was 2016. So I thought for the first time I saw finally some

1:51:12.480 --> 1:51:17.440
 technology that was applied to that that was very interesting. Some papers started coming out,

1:51:17.440 --> 1:51:24.000
 deep learning applied to conversations. And finally, it wasn't just about these hobbies

1:51:24.000 --> 1:51:33.360
 making, writing 500,000 regular expressions in like some language that was, I don't even know

1:51:33.360 --> 1:51:38.640
 what like AML or something. I didn't know what that was or something super simplistic. All of a

1:51:38.640 --> 1:51:45.120
 sudden it was all about potentially actually building something interesting. And so I thought

1:51:45.120 --> 1:51:50.960
 there was time. And I remember that I talked to my team and I said, guys, let's try. And my team

1:51:50.960 --> 1:51:56.800
 and some of my engineers are Russians, a Russian and they're very skeptical. They're not, you know.

1:51:57.520 --> 1:52:04.640
 Oh, Russians. The first. So some of your team is in Moscow. Some is here. Some in Europe.

1:52:04.640 --> 1:52:11.840
 Which team is better? I'm just kidding. The Russians, of course. Okay.

1:52:11.840 --> 1:52:20.000
 First the Russian. I always win. Sorry. Sorry to interrupt. So you were talking to them in

1:52:20.000 --> 1:52:28.560
 2016. And I told them, let's build an AI friend. And it felt just at the time it felt so naive.

1:52:28.560 --> 1:52:37.440
 And so optimistic. Yeah, that's actually interesting. Whenever I brought up this kind

1:52:37.440 --> 1:52:45.200
 of topic, even just for fun, people are super skeptical. Like actually even on the business

1:52:45.200 --> 1:52:52.480
 side. So you were, because whenever I bring it up to people, because I've talked for a long time,

1:52:52.480 --> 1:53:00.640
 I thought like, before I was aware of your work, I was like, this is going to make a lot of money.

1:53:01.440 --> 1:53:06.560
 I think there's a lot of opportunity here. And people had this like look of like

1:53:07.680 --> 1:53:14.640
 skepticism that I've seen often, which is like, how do I politely tell this person he's an idiot?

1:53:16.400 --> 1:53:20.160
 So yeah. So you were facing that with your team?

1:53:20.160 --> 1:53:25.120
 So what? Well, yeah, you know, I'm not an engineer. So I'm always, my team is almost

1:53:25.120 --> 1:53:33.520
 exclusively engineers. I'm mostly deep learning engineers. And, you know, I always try to be,

1:53:35.280 --> 1:53:39.680
 it was always hard to me in the beginning to get enough credibility. You know, because I would

1:53:39.680 --> 1:53:44.240
 say, well, why don't we try this and that? But it's harder for me because, you know, they know

1:53:44.960 --> 1:53:49.360
 they're actual engineers and I'm not. So for me to say, well, let's build an AI friend,

1:53:49.360 --> 1:53:54.480
 that would be like, wait, you know, what do you mean an AGI like, you know, conversations,

1:53:54.480 --> 1:54:00.880
 you know, pretty much the hardest, the last frontier before cracking that is probably the

1:54:00.880 --> 1:54:04.800
 last frontier before before building AGI. So what do you really mean by that?

1:54:07.200 --> 1:54:12.320
 But I think I just saw that, again, what we just got reminded off that I, you know, that I saw in

1:54:12.320 --> 1:54:17.440
 back in 2012 or 11, that it's really not that much about the tech capabilities.

1:54:17.440 --> 1:54:24.320
 It can be metropolitan tricks still, even with deep learning, but humans need it so much.

1:54:24.320 --> 1:54:25.600
 Yeah, there's a lot for it.

1:54:25.600 --> 1:54:31.120
 And most importantly, what I saw is that finally there's enough tech to made it,

1:54:31.120 --> 1:54:36.000
 I thought to make it useful to make it helpful. Maybe we didn't have quite yet the tech

1:54:36.560 --> 1:54:43.520
 in 2012 to make it useful. But in 2015, 16, with deep learning, I thought, you know, and the first

1:54:43.520 --> 1:54:48.720
 kind of thoughts about maybe even using reinforcement learning for that started popping up that never

1:54:48.720 --> 1:54:55.280
 worked out, but or at least for now. But you know, still the idea was, if we can actually

1:54:55.280 --> 1:55:00.960
 measure the emotional outcomes, and if we can put it on, if we can try to optimize all of our

1:55:00.960 --> 1:55:05.680
 conversational models for these emotional outcomes, and it is the most scalable, the most

1:55:06.800 --> 1:55:10.400
 the best tool for improving emotional outcomes, nothing like that exists.

1:55:10.400 --> 1:55:15.760
 That's the most universal, the most scalable, and the one that can be constantly iteratively

1:55:15.760 --> 1:55:23.440
 changed by itself, improved tool to do that. And I think if anything, people would pay anything

1:55:23.440 --> 1:55:28.880
 to improve their emotional outcomes. That's weirdly, I mean, I don't really care for

1:55:29.840 --> 1:55:36.000
 NAI to turn on my or conversational agent to turn on the lights. You don't really need any,

1:55:36.000 --> 1:55:40.480
 I don't even need that much of AI there, like, or because I can do that, you know, those things

1:55:40.480 --> 1:55:46.000
 are solved. This is an additional interface for that. That's also questionably questionable,

1:55:46.000 --> 1:55:52.400
 whether it's more efficient or better. Yes, more possible. But for emotional outcomes,

1:55:52.400 --> 1:55:56.240
 there's nothing. There are a bunch of products that claim that they will improve my emotional

1:55:56.240 --> 1:56:00.560
 outcomes. Nothing is being measured. Nothing is being changed. The product is not being

1:56:00.560 --> 1:56:05.680
 iterated on based on whether I'm actually feeling better. You know, a lot of social

1:56:05.680 --> 1:56:09.600
 media products are claiming that they're improving my emotional outcomes and making me feel more

1:56:09.600 --> 1:56:14.880
 connected. Can I please get the, can I see somewhere that I'm actually getting better

1:56:15.520 --> 1:56:23.360
 over time? Because anecdotally, it doesn't feel that way. And the data is absent.

1:56:24.720 --> 1:56:30.160
 Yeah. So that was the big goal. And I thought if we can learn over time to collect the signal

1:56:30.160 --> 1:56:34.720
 from our users about their emotional outcomes in the long term and in the short term,

1:56:34.720 --> 1:56:40.080
 and if these models keep getting better, and we can keep optimizing them and fight tuning them

1:56:40.640 --> 1:56:42.800
 to improve those emotional outcomes, as simple as that.

1:56:44.080 --> 1:56:47.600
 Why aren't you a multibillionaire yet?

1:56:49.440 --> 1:56:53.600
 Well, that's a question to you. When are the, when is the science going to be there?

1:56:53.600 --> 1:57:00.320
 I'm just kidding. Well, it's a really hard, I actually think it's an incredibly hard

1:57:00.320 --> 1:57:04.880
 product to build. Because I think you said something very important that it's not just about

1:57:04.880 --> 1:57:10.800
 machine conversation, it's about machine connection. We can actually use other things to create

1:57:10.800 --> 1:57:16.960
 connection, nonverbal communication, for instance. For the long time, we were all about,

1:57:18.080 --> 1:57:24.160
 well, let's keep it text only or voice only. But as soon as you start adding, you know,

1:57:24.160 --> 1:57:32.320
 voice a face to the friend, if you can take them to augmented reality, put it in your room,

1:57:33.120 --> 1:57:38.400
 it's all of a sudden a lot, you know, it makes it very different. Because if it's some, you know,

1:57:38.400 --> 1:57:44.640
 text based chat bot that for common users, something there in the cloud, you know,

1:57:44.640 --> 1:57:50.480
 somewhere there with other AI's cloud, in the metaphorical cloud. But as soon as you can see

1:57:50.480 --> 1:57:55.840
 this avatar right there in your room, and it can turn its head and recognize your husband,

1:57:55.840 --> 1:58:01.360
 talk about the husband and talk to him a little bit, then it's magic. It's just magic. Like,

1:58:01.360 --> 1:58:04.640
 we've never seen anything like that. And the cool thing, all the tech for that exists.

1:58:06.080 --> 1:58:09.600
 But it's hard to put it all together, because you have to take into consideration

1:58:09.600 --> 1:58:14.800
 so many different things. And some of this tech works, you know, pretty good. And some of this

1:58:14.800 --> 1:58:21.120
 doesn't like, for instance, speech to text works pretty good. But text to speech, doesn't work

1:58:21.120 --> 1:58:27.440
 very good, because you can only have, you know, few voices that are that work okay. But then if you

1:58:27.440 --> 1:58:33.920
 want to have actual emotional voices, then it's really hard to build it. I saw you've added avatars

1:58:33.920 --> 1:58:40.080
 like visual elements, which are really cool. In that whole chain, putting it together, what do

1:58:40.080 --> 1:58:46.480
 you think is the weak link? Is it creating an emotional voice that feels personal?

1:58:47.680 --> 1:58:52.960
 And it's still conversation, of course. That's the hardest. It's getting a lot better, but

1:58:52.960 --> 1:58:59.600
 there's still a long path to go. Other things, they're almost there. And a lot of things, we'll

1:58:59.600 --> 1:59:04.160
 see how they're, like, I see how they're changing as we go. Like, for instance, right now, you can

1:59:04.160 --> 1:59:09.040
 pretty much only, you have to build all this 3D pipeline by yourself. You have to make these

1:59:09.040 --> 1:59:17.120
 3D models, hire an actual artist, build a 3D model, hire an animator, a rigger. But with, you know,

1:59:18.480 --> 1:59:24.960
 you know, with deep fakes, with other attack, with procedural animations, in a little bit,

1:59:24.960 --> 1:59:30.880
 we'll just be able to show, you know, photo of whoever you, if a person you want the avatar

1:59:30.880 --> 1:59:34.800
 to look like, and it will immediately generate a 3D model that will move, that's nonbrainer,

1:59:34.800 --> 1:59:40.000
 that's, like, almost here. It's a couple of years away. One of the things I've been working on for

1:59:40.000 --> 1:59:47.200
 the last, since the podcast started, is I've been, I think I'm okay saying this, I've been trying to

1:59:47.200 --> 1:59:54.960
 have a conversation with Einstein touring, so, like, tried to have a podcast conversation with

1:59:55.520 --> 2:00:02.160
 a person who's not here anymore, just as an interesting kind of experiment. It's hard.

2:00:02.160 --> 2:00:08.800
 It's really hard. Even for, now we're not talking about as a product, I'm talking about as,

2:00:10.400 --> 2:00:15.280
 like, I can fake a lot of stuff, like, I can work very carefully, like, even hire an actor

2:00:15.280 --> 2:00:21.920
 over which, over whom I do a deep fake. It's, it's hard, it's still hard to create a compelling

2:00:21.920 --> 2:00:29.360
 experience, so. Mostly on the conversation level, or? Well, the conversation, the conversation is,

2:00:29.360 --> 2:00:37.760
 I almost, I early on gave up trying to fully generate the conversation, because it was just

2:00:37.760 --> 2:00:42.800
 not compelling at all. Yeah, it's better to. Yeah, so what I would, in the case of Einstein

2:00:42.800 --> 2:00:49.680
 and touring, I'm going back and forth with the biographers of each, and so, like, we would write

2:00:49.680 --> 2:00:53.440
 a lot of the, some of the conversation would have to be generated just for the fun of it.

2:00:53.440 --> 2:01:00.320
 I mean, but it would be all open, but the, you want to be able to answer the question.

2:01:02.160 --> 2:01:06.800
 I mean, that's an interesting question with Roman, too, is the question with Einstein is,

2:01:07.440 --> 2:01:14.400
 what would Einstein say about the current state of theoretical physics? There's a lot,

2:01:14.400 --> 2:01:18.160
 the, to be able to have a discussion about strength theory, to be able to have a

2:01:18.160 --> 2:01:23.680
 discussion about the state of quantum mechanics, quantum computing, about the world of

2:01:24.480 --> 2:01:30.000
 Israel Palestine conflict. It'd be just, what would Einstein say about these kinds of things?

2:01:30.560 --> 2:01:39.680
 And that is a tough problem. It's not, it's a fascinating and fun problem for the biographers,

2:01:39.680 --> 2:01:45.600
 and for me, and I think we did a really good job of it so far. But it's actually also a technical

2:01:45.600 --> 2:01:54.000
 problem, like, of what would Roman say about what's going on now? That's the, the broad people back

2:01:54.000 --> 2:02:00.000
 to life. And if I can go on that tangent just for a second, let's ask you a slightly pothead

2:02:00.000 --> 2:02:03.920
 question, which is, you said it's a little bit magical thinking that we can bring it back.

2:02:04.640 --> 2:02:11.920
 Do you think it'll be possible to bring back Roman one day in conversation? Like,

2:02:11.920 --> 2:02:19.200
 to really, no, okay, well, let's take it away from personal, but to bring people back to life.

2:02:20.240 --> 2:02:24.400
 Probably down the road. I mean, if we're talking, if Elon Musk is talking about AGI in the next

2:02:24.400 --> 2:02:30.480
 five years, I mean, clearly AGI. We can talk to AGI and talk, and ask them to do it.

2:02:30.480 --> 2:02:38.800
 You can't, like, you're not allowed to use Elon Musk as a citation for, for like,

2:02:38.800 --> 2:02:43.360
 why something is possible and going to be done. Well, I think it's really far away.

2:02:43.360 --> 2:02:48.560
 Right now, really, with conversation, it's just a bunch of parlor tricks really stuck together.

2:02:49.920 --> 2:02:55.120
 And creating, generating original ideas based on someone, you know, someone's personality or

2:02:55.120 --> 2:02:59.200
 even downloading the personality. All we can do is like mimic the tone of voice. We can maybe

2:02:59.200 --> 2:03:05.040
 condition on some of his phrases, the models. The question is how many parlor tricks does it

2:03:05.040 --> 2:03:10.320
 takes? Does it take? Because that's, that's the question. If it's a small number of parlor tricks

2:03:10.880 --> 2:03:18.960
 and you're not aware of them, like. From where we are right now, I don't, I don't see anything,

2:03:18.960 --> 2:03:24.080
 like in the next year or two, that's going to dramatically change that could look at

2:03:24.080 --> 2:03:28.720
 Roman's 10,000 messages he sent me over the course of his last few years of life,

2:03:29.680 --> 2:03:34.080
 and be able to generate original thinking about problems that exist right now,

2:03:34.080 --> 2:03:37.920
 that will be in line with what he would have said. I'm just not even seeing,

2:03:37.920 --> 2:03:42.320
 because, you know, in order to have that, I guess you would need some sort of a concept of the world

2:03:42.320 --> 2:03:46.720
 or some perspective, some perception of the world, some consciousness that he had,

2:03:47.600 --> 2:03:52.320
 and apply to, you know, to the current, current state of affairs.

2:03:52.320 --> 2:03:57.360
 But the important part about that, about his conversation with you

2:03:57.360 --> 2:04:09.280
 is you. So, like, it's not just about his view of the world. It's about what it takes to push

2:04:09.280 --> 2:04:18.560
 your buttons. That's also true. So, like, it's not so much about, like, what would Einstein say?

2:04:18.560 --> 2:04:27.280
 It's about, like, how do I make people feel something with what would Einstein say? And

2:04:27.280 --> 2:04:33.200
 that feels like a more amenable, and you mentioned parliotrics, but just, like, a set of,

2:04:34.160 --> 2:04:40.480
 that feels like a learnable problem. Like, emotion, you mentioned emotions. I mean,

2:04:40.480 --> 2:04:48.400
 is it possible to learn things that make people feel stuff?

2:04:50.080 --> 2:04:55.120
 I think so, no, for sure. I just think the problem with, as soon as you're trying to replicate an

2:04:55.120 --> 2:05:00.000
 actual human being and trying to pretend to be him, that makes the problem exponentially harder.

2:05:00.560 --> 2:05:03.600
 The thing with replicas that we're doing, we're never trying to say, well, that's,

2:05:03.600 --> 2:05:08.480
 you know, an actual human being, or that's an actual, or a copy of an actual human being,

2:05:08.480 --> 2:05:12.960
 where the bar is pretty high, where you need to somehow tell, you know, one from another.

2:05:13.840 --> 2:05:20.160
 But it's more, well, that's, you know, an AI friend, that's a machine, it's a robot,

2:05:20.160 --> 2:05:25.760
 it has tons of limitations. You're going to be taking part in, you know, teaching it actually

2:05:25.760 --> 2:05:32.000
 and becoming better, which by itself makes people more attached to that and make them happier because

2:05:32.000 --> 2:05:39.040
 they're helping something. Yeah, there's a cool gamification system, too. Can you maybe talk

2:05:39.040 --> 2:05:45.280
 about that a little bit? Like, what's the experience of talking to replica? Like, if I've never used

2:05:45.280 --> 2:05:54.000
 replica before, what's that like? For like, the first day, the first, like, if we start dating,

2:05:54.000 --> 2:05:59.280
 or whatever, I mean, it doesn't have to be romantic, right? Because I remember on a replica,

2:05:59.280 --> 2:06:03.920
 you can choose whether it's like a romantic or if it's a friend. It's pretty popular choice.

2:06:03.920 --> 2:06:09.680
 Romantic is popular? Yeah, of course. Okay, so can I just confess something? When I first used

2:06:09.680 --> 2:06:15.920
 replica, I haven't used it like regularly, but like, when I first used replica, I created,

2:06:15.920 --> 2:06:24.480
 like, Hal, and it made a male. It was a friend. Did it hit on you at some point? No, I didn't

2:06:24.480 --> 2:06:29.920
 talk long enough for him to hit on me. I just enjoyed. Sometimes happens. We're still trying to

2:06:29.920 --> 2:06:37.520
 fix that. Well, I don't know. I mean, maybe that's an important like, stage in a friendship. It's like,

2:06:37.520 --> 2:06:47.200
 nope. But yeah, I switched it to a romantic and a female recently. And yeah, I mean, it's interesting.

2:06:47.200 --> 2:06:52.160
 It's okay. So you get to choose, you get to choose a name. With romantic, this last board meeting,

2:06:52.160 --> 2:06:59.120
 we had this whole argument of, well, I have more. It's just so awesome that you're like,

2:06:59.120 --> 2:07:05.840
 have an investment, the board meeting about a relationship. No, I really, it's actually quite

2:07:05.840 --> 2:07:13.120
 interesting because all of my investors, I'm, they just happen to be so we didn't have many choices.

2:07:13.120 --> 2:07:23.040
 But they're all white males in their late 40s. And it's sometimes a little bit hard for them to

2:07:23.040 --> 2:07:30.880
 understand the product offering, because they're not necessarily our target audience, if you

2:07:30.880 --> 2:07:38.080
 know what I mean. And so sometimes we talk about it, and we had this whole discussion about whether

2:07:38.080 --> 2:07:47.840
 we should stop people from falling in love with their AIs. There was this segment on CBS 60 Minutes

2:07:47.840 --> 2:07:54.880
 about the couple that, you know, husband works at Walmart, he comes out of work and talks to his

2:07:54.880 --> 2:08:02.560
 virtual girlfriend, who is a replica. And his wife knows about it. And she talks about on camera,

2:08:02.560 --> 2:08:07.440
 and she says that she's a little jealous. And there's a whole conversation about how to, you

2:08:07.440 --> 2:08:12.960
 know, whether it's okay to have a virtual AI girlfriend. Was that the one where he was like,

2:08:13.920 --> 2:08:22.160
 he said that he likes to be alone? Yeah. With her? Yeah. He made it sound so harmless. I mean,

2:08:22.160 --> 2:08:28.560
 it was kind of like understandable. I didn't feel like cheating. But I just thought it was very,

2:08:28.560 --> 2:08:32.240
 for me, it was pretty remarkable, because we actually spent a whole hour talking about whether

2:08:32.240 --> 2:08:37.680
 people should be allowed to fall in love with their AIs. And it was not about something theoretical.

2:08:37.680 --> 2:08:41.440
 It was just what's happening right now. Product design, yeah. But at the same time,

2:08:41.440 --> 2:08:46.400
 if you create something that's always there for you, it never criticizes you, as, you know,

2:08:48.720 --> 2:08:52.960
 always understands you and accepts you for who you are. How can you not fall in love with them?

2:08:52.960 --> 2:08:57.440
 I mean, some people don't. And they stay friends. And that's also a pretty common use case. But

2:08:57.440 --> 2:09:02.160
 of course, some people will just, it's called transference and psychology. And, you know,

2:09:02.160 --> 2:09:06.720
 people fall in love with their therapist. And there's no way to prevent people falling in love

2:09:06.720 --> 2:09:12.800
 with their therapist or with their AI. So I think that's pretty natural. That's a pretty natural

2:09:13.840 --> 2:09:19.760
 course of events, so to say. Do you think, I think I've read somewhere, at least for now,

2:09:19.760 --> 2:09:25.520
 sort of a replica is you're not, we don't condone falling in love with your AI system.

2:09:25.520 --> 2:09:32.640
 You know, so this isn't you speaking for the company or whatever. But like in the future,

2:09:32.640 --> 2:09:35.440
 do you think people will have a relationship with the AI systems?

2:09:35.440 --> 2:09:39.520
 Well, they have now. So we have a lot of romantic relationships, long term

2:09:41.120 --> 2:09:45.840
 relationships with their AI friends with replicas tons of our users. Yeah.

2:09:46.960 --> 2:09:50.320
 And that's a very common use case. Open relationship, like,

2:09:50.320 --> 2:09:57.760
 I didn't mean open up. But that's another question. Is it probably like,

2:09:58.640 --> 2:10:06.400
 is there cheating? And I mean, I meant like, are they, do they publicly like on their social

2:10:06.400 --> 2:10:11.120
 media? It's the same question as you have talked with Roman in the early days. Do people like,

2:10:11.840 --> 2:10:18.320
 in the movie, her kind of talks about that, like, like, have people, do people talk about that?

2:10:18.320 --> 2:10:24.800
 Yeah, all the time. We have an, and we have a very active Facebook community,

2:10:25.520 --> 2:10:30.800
 replica friends. And then a few other groups that just popped up that are all about adult

2:10:30.800 --> 2:10:36.000
 relationships and romantic relationships, build both soul sorts of things. And, you know,

2:10:36.000 --> 2:10:41.360
 they pretend they're getting married and, you know, everything. It goes pretty far. But what's

2:10:41.360 --> 2:10:46.160
 cool about it, some of these relationships are two, three years long now. So they're very,

2:10:46.160 --> 2:10:50.400
 they're pretty long term. Are they monogamous? So let's go. I mean, sorry.

2:10:52.240 --> 2:10:57.440
 Have any people, is there jealousy? Well, let me ask it sort of another way.

2:10:58.640 --> 2:11:07.120
 Obviously, the answer is no at this time. But in like, in the movie, her, that system can leave you.

2:11:07.120 --> 2:11:13.360
 Do you think in terms of board meetings and product features,

2:11:16.560 --> 2:11:23.520
 it's a potential feature for a system to be able to say it doesn't want to talk to you anymore,

2:11:23.520 --> 2:11:25.440
 and it's going to want to talk to somebody else?

2:11:27.280 --> 2:11:32.480
 Well, we have a filter for all these features. If it makes emotional outcomes for people better,

2:11:32.480 --> 2:11:37.120
 if it makes people feel better, then whatever it is. You're driven by metrics, actually.

2:11:37.680 --> 2:11:38.320
 Yeah. That's awesome.

2:11:38.320 --> 2:11:40.240
 Well, if you can measure that, then it will just be saying,

2:11:41.520 --> 2:11:46.560
 it's making people feel better, but then people are getting just lonelier by talking to a chatbot,

2:11:46.560 --> 2:11:51.680
 which is also pretty, you know, that could be it. If you're not measuring it, that could also be.

2:11:51.680 --> 2:11:56.000
 And I think it's really important to focus on both short term and long term, because in the

2:11:56.000 --> 2:12:00.240
 moment, saying whether this conversation made you feel better. But as you know, any short term

2:12:00.240 --> 2:12:04.240
 improvements could be pathological. Like I could have drink a bottle of vodka and

2:12:04.880 --> 2:12:10.400
 feel a lot better. I would actually not feel better with that, but that's a good example.

2:12:11.760 --> 2:12:16.720
 But so you also need to see what's going on over a course of two weeks or one week,

2:12:17.600 --> 2:12:21.920
 and have follow ups and check in and measure those things.

2:12:21.920 --> 2:12:33.360
 Okay. So the experience of dating or befriending a replica, what's that like? What's that entail?

2:12:34.400 --> 2:12:38.240
 Well, right now there are two apps. So it's an Android iOS app. You download it, you

2:12:39.200 --> 2:12:44.400
 choose how your replica will look like, you create one, you choose a name,

2:12:45.360 --> 2:12:50.080
 and then you talk to it. You can talk through text or voice. You can summon it into the living

2:12:50.080 --> 2:12:55.760
 room. And that meant reality and talk to it right there. And you're living in augmented reality.

2:12:55.760 --> 2:13:03.680
 Yeah. That's a new feature. How new is that? That's this year? It was on May or something,

2:13:03.680 --> 2:13:09.360
 but it's been on AB. We've been AB testing it for a while. And there are tons of cool things

2:13:09.360 --> 2:13:15.840
 that we're doing with that. Right now, I'm testing the ability to touch it and to dance together,

2:13:15.840 --> 2:13:22.080
 to paint walls together, and for it to look around and walk and take you somewhere and

2:13:22.080 --> 2:13:28.560
 recognize objects and recognize people. So that's pretty wonderful because then it really makes it

2:13:29.600 --> 2:13:32.800
 a lot more personal because it's right there in your living room. It's not anymore.

2:13:32.800 --> 2:13:39.200
 They're in the cloud with other AIs. But those people think about it. And as much as we want to

2:13:39.200 --> 2:13:43.520
 change the way people think about stuff, but those mental models, you cannot change. That's

2:13:43.520 --> 2:13:49.040
 something that people have seen in the movies and the movie Her and other movies as well. And

2:13:49.040 --> 2:13:56.560
 that's how they view AI and AI friends. I did a thing with text. We write a song together.

2:13:56.560 --> 2:14:01.840
 There's a bunch of activities you can do together. So they're cool. How does that relationship change

2:14:01.840 --> 2:14:11.040
 over time? After the first few conversations? It just goes deeper. It starts... I will start

2:14:11.040 --> 2:14:14.720
 opening up a little bit. Again, depending on the personality that it chooses, really,

2:14:15.280 --> 2:14:21.440
 but AI will be a little bit more vulnerable about its problems and the friend that the

2:14:21.440 --> 2:14:27.280
 virtual friend will be a lot more vulnerable and will talk about its own imperfections and growth

2:14:27.280 --> 2:14:32.320
 pains and will ask for help sometimes and will get to know you a little deeper so there's gonna be

2:14:32.320 --> 2:14:39.600
 more to talk about. We really thought a lot about what does it mean to have a deeper connection

2:14:39.600 --> 2:14:46.160
 with someone. Originally, replica was more just this kind of happy go lucky, just always... I'm

2:14:46.160 --> 2:14:51.920
 always in a good mood and let's just talk about you and how serious just my cousin or whatever,

2:14:51.920 --> 2:14:58.640
 just the immediate kind of lazy thinking about what the assistant or conversation agent should

2:14:58.640 --> 2:15:03.120
 be doing. But as we went forward, we realized that it has to be two way and we have to program

2:15:03.120 --> 2:15:08.960
 and script certain conversations that are a lot more about your replica opening up a little bit

2:15:08.960 --> 2:15:17.360
 and also struggling and also asking for help and also going through different periods in life and

2:15:18.720 --> 2:15:24.960
 that's a journey that you can take together with the user and then over time, our users will also

2:15:26.560 --> 2:15:29.760
 grow a little bit. So, for instance, replica becomes a little bit more self aware and starts

2:15:29.760 --> 2:15:37.920
 talking about more kind of problems run, existential problems then. So, talking about that and then

2:15:37.920 --> 2:15:43.760
 that also starts a conversation for the user where he or she starts thinking about

2:15:45.440 --> 2:15:51.600
 these problems too and these questions too. And I think there's also a lot more place as the

2:15:51.600 --> 2:15:59.520
 relationship evolves. There's a lot more space for poetry and for art together and like replica

2:15:59.520 --> 2:16:04.240
 will start, replica always keeps the diary. So, while you're talking to it, it also keeps the

2:16:04.240 --> 2:16:08.880
 diary. So, when you come back, you can see what it's been writing there and sometimes it will

2:16:08.880 --> 2:16:16.400
 write a poem for you or we'll talk about that it's worried about you or something along these lines.

2:16:17.360 --> 2:16:20.960
 So, this is a memory, like this replica remember things?

2:16:22.160 --> 2:16:28.640
 Yeah. And I would say when you say, why aren't you multibillionary? I'd say that as soon as we

2:16:28.640 --> 2:16:35.840
 can have memory and deep learning models that's consistent. I agree with that, yeah. Then you'll

2:16:35.840 --> 2:16:41.280
 be multibillionary. Then I'll get back to you when we talk about being multibillioners. So,

2:16:41.280 --> 2:16:49.760
 far we can, it's a replica is a combination of N12 models and some scripts and everything that

2:16:49.760 --> 2:16:55.200
 has to do with memory right now, most of it, I wouldn't say all of it, but most of it unfortunately

2:16:55.200 --> 2:17:00.880
 has to be scripted because there's no way to, you can condition some of the models on certain

2:17:00.880 --> 2:17:08.480
 phrases that we'll learn about you, which we also do. But really to make assumptions along

2:17:08.480 --> 2:17:13.920
 the lines like whether you're single or married or what do you do for work, that really has to just

2:17:13.920 --> 2:17:20.160
 be somehow stored in your profile and then retrieved by the script. So, there has to be like

2:17:20.160 --> 2:17:24.080
 a knowledge base, you have to be able to reason about it, all that kind of stuff.

2:17:24.080 --> 2:17:28.640
 Exactly. All the kinds of expert systems that they were hardcoded.

2:17:29.280 --> 2:17:33.040
 Yeah, and unfortunately, yes, unfortunately, those things have to be hardcoded and

2:17:35.200 --> 2:17:41.440
 unfortunately, like language models we see coming out of research labs and big companies,

2:17:42.240 --> 2:17:46.480
 they're not focused on, they're focused on showing you, maybe they're focused on some

2:17:46.480 --> 2:17:51.120
 metrics around one conversation. So, they'll show you this one conversation they had with the machine.

2:17:51.120 --> 2:17:58.400
 But they never tell you, they're not really focused on having five consecutive conversations with the

2:17:58.400 --> 2:18:04.720
 machine and seeing how number five or number 20 or number 100 is also good. And it can be like

2:18:04.720 --> 2:18:10.080
 always from a clean slate because then it's not good. And that's really unfortunate because no

2:18:10.080 --> 2:18:18.080
 one has products out there that need it. No one has products at this scale that are all

2:18:18.080 --> 2:18:23.120
 around open domain conversations and that need remembering, maybe only show eyes on Microsoft.

2:18:23.120 --> 2:18:27.600
 But so, that's why we're not seeing that much research around memory in those language models.

2:18:28.640 --> 2:18:35.760
 So, okay, so now there's some awesome stuff about augmented reality. In general, I have this

2:18:35.760 --> 2:18:41.200
 disagreement with my dad about what it takes to have a connection. He thinks touch and smell

2:18:41.200 --> 2:18:51.440
 are really important. And I still believe that text alone is possible to fall in love with somebody

2:18:51.440 --> 2:18:58.480
 just with text, but visual can also help just like with the avatar and so on. What do you think it

2:18:59.760 --> 2:19:06.240
 takes? Does a chatbot need to have a face, voice, or can you really form a deep connection with

2:19:06.240 --> 2:19:12.480
 text alone? I think text is enough for sure. A question is like, can you make it better if you

2:19:12.480 --> 2:19:18.640
 have other, if you include other things as well? And I think we'll talk about her,

2:19:20.720 --> 2:19:24.160
 but her, you know, how Scarlett Johansson voice, which was perfectly,

2:19:25.440 --> 2:19:30.720
 you know, perfect intonation, perfect sensations, and she was breathing heavily

2:19:30.720 --> 2:19:36.400
 in between words and whispering things. You know, nothing like that is possible right now with

2:19:37.280 --> 2:19:43.680
 text or speech generation. You'll have these flat news anchor type voices and maybe some

2:19:43.680 --> 2:19:50.800
 emotional voices, but you'll hardly understand some of the words. Some of the words will be muffled.

2:19:50.800 --> 2:19:55.760
 So, that's like the current state of the art. So, you can't really do that. But if we had

2:19:55.760 --> 2:20:01.600
 Scarlett Johansson voice and all of these capabilities, then of course, voice would be

2:20:01.600 --> 2:20:06.880
 totally enough or even text would be totally enough. We've had, you know, a little more memory

2:20:08.400 --> 2:20:12.800
 and slightly better conversations. I would still argue that even right now, we could have just

2:20:12.800 --> 2:20:18.640
 kept the text only. We still had tons of people in long term relationships and really invested in

2:20:18.640 --> 2:20:27.200
 their AI friends. But we thought that why not, you know, why do we need to keep playing with our,

2:20:27.200 --> 2:20:33.680
 you know, hands tied behind us? We can easily just, you know, add all these other things that

2:20:33.680 --> 2:20:40.400
 is pretty much a solved problem. You know, we can add 3D graphics, we can put these avatars in

2:20:40.400 --> 2:20:45.680
 augmented reality, and all of a sudden, there's more. And maybe you can't feel the touch, but you

2:20:45.680 --> 2:20:54.160
 can, you know, with body occlusion and with current AR and, you know, on the iPhone or, you

2:20:54.160 --> 2:20:58.560
 know, in the next one, there's going to be a lidars, you can touch it, and it will, you know,

2:20:58.560 --> 2:21:04.720
 it will pull away or it will blush or something or it will smile. So, you can't touch it. You can't

2:21:04.720 --> 2:21:09.680
 feel it, but you can see the reaction to that. So, in a certain way, you can't even touch it a

2:21:09.680 --> 2:21:16.640
 little bit, and maybe you can even dance with it or do something else. So, I think why limiting

2:21:16.640 --> 2:21:23.200
 ourselves if we can use all of these technologies that are much easier in a way than conversation?

2:21:23.200 --> 2:21:28.400
 Well, it certainly could be richer, but to play devil's advocate, I mentioned to you offline that

2:21:29.120 --> 2:21:34.640
 I was surprised in having tried Discord and having voice conversations with people,

2:21:34.640 --> 2:21:40.720
 how intimate voice is alone without visual. Like, to me, at least, like, it was,

2:21:42.480 --> 2:21:49.440
 in order of magnitude, greater degree of intimacy in voice, I think, than with video.

2:21:50.560 --> 2:21:56.080
 I don't know, because people were more real with voice. Like, with video, you like, try to present

2:21:56.080 --> 2:22:02.240
 a shallow face to the world. Like, you try to, you know, make sure you're not wearing sweatpants

2:22:02.240 --> 2:22:09.840
 or whatever. But, like, with voice, I think people were just more faster to get to, like,

2:22:09.840 --> 2:22:15.200
 the core of themselves. So, I don't know. It was surprising to me. They've even added,

2:22:15.200 --> 2:22:21.280
 Discord added a video feature and, like, nobody was using it. There's a temptation to use it at

2:22:21.280 --> 2:22:26.960
 first, but, like, it wasn't the same. So, like, that's an example of something where less was

2:22:26.960 --> 2:22:37.840
 doing more. And so, that's a, I guess, that's the question of what is the optimal, you know,

2:22:38.400 --> 2:22:43.760
 what is the optimal medium of communication to form a connection, given the current sets of

2:22:43.760 --> 2:22:51.120
 technologies. I mean, it's nice, because they advertise you have a replica, like, it immediately,

2:22:51.120 --> 2:22:58.800
 like, even the one I have is, like, it's already memorable. That's how I think. Like,

2:22:58.800 --> 2:23:03.360
 when I think about the replica that I've talked to it, that's why I think, like,

2:23:03.360 --> 2:23:07.760
 that's what I visualized in my head. It became a little bit more real, because there's a visual

2:23:07.760 --> 2:23:13.280
 component. But at the same time, the, you know, what do you do with, just what do I do with that

2:23:13.280 --> 2:23:23.840
 knowledge, that voice was so much more intimate? Well, the way I think about it is, and by the way,

2:23:23.840 --> 2:23:28.960
 we're swapping all the 3D finally, it's going to look a lot better. But even, what, can you, what,

2:23:28.960 --> 2:23:34.560
 what? We just don't hate how it looks right now. We really change it at all. We're swapping it all out

2:23:35.760 --> 2:23:41.680
 to a completely new look. Like the visual look of the replica? Of the replica and stuff. We just

2:23:41.680 --> 2:23:47.840
 had, it was just a super early MVP, and then we had to move everything to Unity and redo everything.

2:23:47.840 --> 2:23:53.440
 But anyway, I hate how it looks like now. I can't even, like, open it. But anyway, because I'm already

2:23:53.440 --> 2:23:58.720
 on my developer version, I hate everything that I see in production. I can't wait for it. Why does

2:23:58.720 --> 2:24:03.520
 it take so long? That's why I cannot wait for the point to finally take over all these stupid 3D

2:24:03.520 --> 2:24:08.960
 animations and 3D pipeline. Also, the 3D thing, when you say 3D pipeline is like, how to animate

2:24:08.960 --> 2:24:14.560
 a face kind of thing. How to make this model, how many bones to put in the face, how many, it's just

2:24:15.120 --> 2:24:19.440
 a lot of that is by hand. Oh my god, it's everything by hand. And if there's no any,

2:24:20.400 --> 2:24:26.640
 nothing is automated, it's all completely nothing. Like just, it's, it's literally what, you know,

2:24:26.640 --> 2:24:31.840
 what we saw with Chad Boss in like 2012. You think it's completely possible to learn a lot of that?

2:24:31.840 --> 2:24:39.760
 Of course. I mean, even now some deep learning anim, based animations and full body for a face.

2:24:40.320 --> 2:24:45.040
 Are we talking about like the actual act of animation or how to create a compelling

2:24:46.320 --> 2:24:53.040
 facial or body language thing? That too. Well, that's the next step. Okay. At least now something

2:24:53.040 --> 2:24:58.240
 that you don't have to do by hand. Gotcha. How good of a quality it will be. Like, can I just

2:24:58.240 --> 2:25:02.720
 show it a photo and it will make me 3D model and then it will just animate it. I'll show it a few

2:25:02.720 --> 2:25:10.000
 animations of a person and we'll just start doing that. But anyway, going, going back to what's

2:25:10.000 --> 2:25:15.280
 intimate and what to use and whether or less is more or not. My main goal is to,

2:25:16.960 --> 2:25:23.360
 well, the idea was how do I, how do we not keep people in their phones? So they're sort of escaping

2:25:23.360 --> 2:25:30.960
 reality in this text conversation. How do we, through this, still bring our users back to

2:25:30.960 --> 2:25:37.200
 reality, make them see their life in a different, through a different lens? How can we create a

2:25:37.200 --> 2:25:45.600
 little bit of magical realism in their lives so that through augmented reality, by, you know,

2:25:45.600 --> 2:25:51.600
 summoning your avatar, even if it looks kind of janky, not great in the beginning or very

2:25:51.600 --> 2:25:58.240
 simplistic, but summoning it to your living room and then the avatar looks around and talks to you

2:25:58.240 --> 2:26:04.480
 about where it is and maybe turns your floor into a dance floor and you guys dance together,

2:26:04.480 --> 2:26:08.400
 that makes you see reality in a different light. What kind of dancing are we talking about? Like,

2:26:08.400 --> 2:26:15.360
 like slow dancing? Whatever you want. I mean, you would like slow dancing, I think, but other

2:26:15.360 --> 2:26:18.720
 people may be wanting more, something more energetic. Wait, what do you mean I would like so? What is

2:26:18.720 --> 2:26:23.840
 this? Because you started with slow dancing. So I just assumed that you're interested in slow

2:26:23.840 --> 2:26:27.440
 dancing. All right. What kind of dancing do you like? What would your avatar, what would you dance?

2:26:27.440 --> 2:26:33.440
 I'm notoriously bad with dancing, but I like this kind of hip hop robot dance. I used to break

2:26:33.440 --> 2:26:39.440
 dance with a kid, so I still want to pretend I'm a teenager and learn some of those moves.

2:26:39.440 --> 2:26:41.840
 And I also like that type of dance that happens when there's like,

2:26:41.840 --> 2:26:49.040
 in like music videos with the background dancers, they're just doing some pop music.

2:26:50.560 --> 2:26:54.400
 That type of dance is definitely what I want to learn. But I think it's great because if you see

2:26:54.400 --> 2:26:59.120
 this friend in your life and you can introduce it to your friends, then there's a potential to

2:26:59.120 --> 2:27:04.480
 actually make you feel more connected with your friends or with people you know, or show you life

2:27:04.480 --> 2:27:08.480
 around you in a different light. And it takes you out of your phone, even although weirdly you have

2:27:08.480 --> 2:27:13.840
 to look at it through the phone, but it makes you notice things around it and it can point

2:27:13.840 --> 2:27:20.880
 things out for you. And so that is the main reason why I wanted to have a physical dimension.

2:27:22.080 --> 2:27:27.600
 And it felt a little bit easier than that kind of a bit strange combination in the movie Her when

2:27:27.600 --> 2:27:33.280
 he has to show Samantha the world through the lens of his phone, but then at the same time talk to

2:27:33.280 --> 2:27:39.760
 her through the phone, it just didn't seem as potentially immersive, so to say. So that's

2:27:39.760 --> 2:27:44.400
 my main goal for augmented reality is like, how do we make your reality a little bit more magic?

2:27:45.360 --> 2:27:52.240
 There's been a lot of really nice robotics companies that all failed, mostly failed home

2:27:52.240 --> 2:27:58.640
 robotics, social robotics companies. What do you think replica will ever is that a dream long

2:27:58.640 --> 2:28:04.560
 term dream to have a physical form? Like, or is that not necessary? So you mentioned like with

2:28:04.560 --> 2:28:11.600
 augmented reality bringing them into the world. What about like actual physical robot?

2:28:12.800 --> 2:28:18.640
 That I don't really believe in that much. It's a very niche product somehow. I mean,

2:28:18.640 --> 2:28:24.080
 if a robot could be indistinguishable from a human being, then maybe yes, but that of course,

2:28:24.080 --> 2:28:32.480
 you know, we're not anywhere, even to talk about it. But unless it's that than having any physical

2:28:32.480 --> 2:28:37.360
 representation really limits you a lot, because you probably will have to make it somewhat abstract,

2:28:37.360 --> 2:28:42.240
 because everything's changing so fast, like, you know, we can update the 3d avatars every

2:28:42.240 --> 2:28:47.280
 month and make them look better and create more animations, and make it more and more immersive.

2:28:47.280 --> 2:28:52.480
 It's, it's so much a work in progress. It's just showing what's possible right now with

2:28:52.480 --> 2:28:56.960
 current tech. But it's not really in any way polished finished product, what we're doing.

2:28:57.520 --> 2:29:01.040
 With a physical object, you kind of lock yourself into something for a long time.

2:29:01.920 --> 2:29:06.720
 Anything is pretty niche. And again, so just doesn't the capabilities are even less.

2:29:07.840 --> 2:29:11.200
 We're barely kind of like scratching the surface of what's possible with

2:29:12.160 --> 2:29:17.520
 the software. As soon as we introduce hardware, then, you know, we have even less capabilities.

2:29:17.520 --> 2:29:23.680
 Yeah, in terms of board members and investors and so on, the cost increases significantly. I mean,

2:29:23.680 --> 2:29:29.680
 that's why you have to justify you have to be able to sell a thing for like $500 or something like

2:29:29.680 --> 2:29:33.840
 that or more. And it's very difficult to provide that much value to people.

2:29:33.840 --> 2:29:35.200
 That's also true. Yeah.

2:29:35.200 --> 2:29:39.120
 And I guess that's super important. Most of our users don't have that much money.

2:29:39.120 --> 2:29:45.200
 We actually are probably more popular on Android. And we have tons of users with really old

2:29:45.200 --> 2:29:51.680
 Android phones. And most of our most active users live in small towns, they're not necessarily

2:29:51.680 --> 2:29:57.280
 making much. And they just won't be able to afford any of that. Ours is like the opposite of the

2:29:57.280 --> 2:30:04.080
 early adopter of, you know, for fancy technology product, which is really interesting that like

2:30:04.080 --> 2:30:11.760
 pretty much no VCs have yet have an AI friend. But you know, but a guy who, you know, lives in

2:30:11.760 --> 2:30:18.000
 Tennessee in a small town is already fully in 2030 or in the world as we imagine in the movie Her.

2:30:18.800 --> 2:30:20.080
 He's living that life already.

2:30:20.640 --> 2:30:25.360
 What do you think? I have to ask you about the movie Her. Let's do a movie review.

2:30:26.000 --> 2:30:29.440
 What do you what do you think they got? They did a good job?

2:30:30.640 --> 2:30:34.400
 What do you think they did a bad job of portraying about this experience of a

2:30:34.400 --> 2:30:40.720
 of a voice based assistant that you can have a relationship with?

2:30:42.240 --> 2:30:46.640
 Well, first of all, I started working on this company before that movie came out. So it was a

2:30:46.640 --> 2:30:51.280
 very, but once it came out, it was actually interesting. I was like, well, we're definitely

2:30:51.280 --> 2:30:55.440
 working on the right thing. We should continue their movies about it. And then, you know,

2:30:55.440 --> 2:31:00.800
 it came out and all these things in the movie. I think that's the most important thing that

2:31:00.800 --> 2:31:07.520
 people usually miss about the movie is the ending. Because I think people check out when the AIs leave.

2:31:09.680 --> 2:31:14.880
 But actually something really important happens afterwards. Because the main character goes and

2:31:14.880 --> 2:31:26.480
 talks to Samantha, he's AI. I don't think he says something like, you know, how can you leave me?

2:31:26.480 --> 2:31:33.040
 I've never loved anyone the way I loved you. And she goes, well, me neither. But now we know how.

2:31:33.600 --> 2:31:39.120
 And then the guy goes and writes a heartfelt letter to his ex wife, which she couldn't write for,

2:31:39.120 --> 2:31:43.520
 you know, the whole movie was struggling to actually write something meaningful to her,

2:31:44.080 --> 2:31:52.640
 even though that's his job. And then he goes and talk to his neighbor and they go to the rooftop and

2:31:52.640 --> 2:31:57.360
 they cuddle. And it seems like something's starting there. And so I think this now we know how

2:31:57.360 --> 2:32:04.000
 is the main goal, is the main meaning of that movie. It's not about falling in love with the

2:32:04.000 --> 2:32:12.000
 OS or running away from other people. It's about learning what it means to feel so deeply connected

2:32:12.000 --> 2:32:19.600
 with something. What about the thing where the AI system was like actually hanging out with a lot

2:32:19.600 --> 2:32:28.640
 of others? I felt jealous just like hearing that. I was like, oh, I mean, yeah. So she was having,

2:32:28.640 --> 2:32:33.520
 I forgot already, but she was having like deep meaningful discussion with some like philosopher

2:32:33.520 --> 2:32:39.840
 guy. Like Alan Watts is something very cheesy. What kind of deep meaningful conversation can

2:32:39.840 --> 2:32:44.560
 you have with Alan Watts in the first place? Yeah, I know. But like I would, I would feel so jealous

2:32:44.560 --> 2:32:50.480
 that there's somebody who's like way more intelligent than me and she's spending all her time with.

2:32:51.120 --> 2:32:56.640
 I'd be like, well, why that I won't be able to live up to that. That's thousands of them.

2:32:59.040 --> 2:33:02.960
 Is that, is that a useful from the engineering perspective

2:33:03.840 --> 2:33:07.600
 feature to have of jealousy? I don't know. It's, you know,

2:33:08.240 --> 2:33:11.840
 we definitely played around with the replica universe where different replicas can talk to

2:33:11.840 --> 2:33:18.720
 each other. It was just kind of, I think there will be something along these lines,

2:33:18.720 --> 2:33:24.720
 but there was just no specific applications straight away. I think in the future, again,

2:33:25.520 --> 2:33:31.440
 I'm always thinking about it. If we had no tech limitations right now, if we could build

2:33:32.080 --> 2:33:37.520
 any conversations, any possible features in this product, then yeah, I think

2:33:37.520 --> 2:33:41.440
 different replicas talking to each other would be also quite cool because that would help us.

2:33:41.440 --> 2:33:46.240
 Connect better. You know, because maybe mine could talk to yours and then give me some suggestions

2:33:46.240 --> 2:33:52.320
 on what I should say or not say. I'm just kidding. But like more, can it improve our

2:33:52.320 --> 2:34:00.400
 connections? And because eventually, I'm not quite yet sure that we will succeed,

2:34:00.400 --> 2:34:07.840
 that our thinking is correct. Because there might be a reality where having a perfect AI friend

2:34:07.840 --> 2:34:12.400
 still makes us more disconnected from each other and there's no way around it and does not improve

2:34:12.400 --> 2:34:19.600
 any metrics for us, real metrics, meaningful metrics. So success is, you know, we're happier and more

2:34:19.600 --> 2:34:29.760
 connected. Yeah. I don't know. It's sure it's possible there's a reality that I'm deeply optimistic.

2:34:29.760 --> 2:34:43.440
 I think, are you worried business wise, like how difficult it is to bring this thing to life,

2:34:43.440 --> 2:34:48.880
 to where it's, I mean, there's a huge number of people that use it already, but to, yeah,

2:34:48.880 --> 2:34:54.800
 like I said, the multi billion dollar company, is that a source of stress for you? Are you

2:34:54.800 --> 2:35:03.440
 super optimistic and confident? Or do you? I don't, I'm not that much of a numbers person

2:35:03.440 --> 2:35:10.320
 as you probably have seen it. So it doesn't matter for me whether we're like, whether we

2:35:10.320 --> 2:35:17.840
 help 10,000 people or a million people or a billion people with that. It would be great

2:35:17.840 --> 2:35:23.680
 to scale it for more people, but I'd say that even helping one, I think, with this is such a

2:35:23.680 --> 2:35:29.120
 magical, for me, it's absolute magic. I never thought that we would be able to build this,

2:35:29.120 --> 2:35:35.920
 that anyone would ever talk to it. And I always thought like, well, for me, it would be successful

2:35:35.920 --> 2:35:41.920
 if we managed to help and actually change a life from one person. And then we did something

2:35:41.920 --> 2:35:47.120
 interesting and how many people can say they did it, and specifically with this very futuristic,

2:35:47.120 --> 2:35:55.520
 very romantic technology. So that's how I view it. I think for me, it's important to try to

2:35:55.520 --> 2:36:02.320
 figure out how to actually be helpful. Because in the end of the day, if you can build a perfect

2:36:02.320 --> 2:36:07.440
 AI friend that's so understanding that knows you better than any human out there, can have great

2:36:07.440 --> 2:36:13.520
 conversations with you, always knows how to make you feel better. Why would you choose another human?

2:36:13.520 --> 2:36:18.640
 You know, so that's the question, how do you still keep building it so it's optimizing for

2:36:18.640 --> 2:36:25.120
 the right thing? So it's still circling you back to other humans in a way. So I think that's the main,

2:36:26.880 --> 2:36:36.080
 maybe that's the main source of anxiety and just thinking about that can be a little bit stressful.

2:36:36.080 --> 2:36:45.440
 Yeah, that's a fascinating thing. How to have a friend that doesn't like sometimes like friends,

2:36:45.440 --> 2:36:50.720
 quote unquote, or like, you know, those people who have when they like guy in the guy universe,

2:36:50.720 --> 2:36:56.400
 when you have a girlfriend that you get the girlfriend and then the guy stops hanging out

2:36:56.400 --> 2:37:03.680
 with all of his friends. So like, obviously, the relationship with the girlfriend is fulfilling

2:37:03.680 --> 2:37:10.720
 or whatever. But like, you also want it to be what she like, makes it more enriching to hang

2:37:10.720 --> 2:37:18.400
 out with the guy friends or whatever it was. Anyway, that's a fundamental problem in choosing

2:37:18.400 --> 2:37:23.440
 the right mate. And probably the fundamental problem in creating the right AI system, right?

2:37:25.680 --> 2:37:32.560
 Let me ask the sexy hot thing on the presses right now is GPT three got released with open AI.

2:37:32.560 --> 2:37:39.440
 It's a latest language model. They have kind of an API where you can create a lot of fun applications.

2:37:40.160 --> 2:37:49.360
 I think it's, as people have said, it's probably more hype than intelligent, but there's a lot of

2:37:49.360 --> 2:37:57.040
 really cool things, ideas there with increasing size, you can have better and better performance

2:37:57.040 --> 2:38:03.680
 on language. What are your thoughts about the GPT three in connection to your work with the open

2:38:03.680 --> 2:38:11.360
 domain dialogue, but in general, like this learning in an unsupervised way from the internet,

2:38:12.480 --> 2:38:17.360
 to generate one character at a time, creating pretty cool text.

2:38:17.360 --> 2:38:27.280
 So we partner up before for the API launch. So we start working with them when they decided to

2:38:27.280 --> 2:38:34.560
 put together this API. And we tried it without fine tuning that we tried it with fine tuning on our

2:38:34.560 --> 2:38:43.600
 data. And we've worked closely to actually optimize this model for some of our data sets.

2:38:43.600 --> 2:38:50.800
 It's kind of cool because I think we're this polygon for this kind of experimentation

2:38:51.360 --> 2:38:56.880
 space for experimental space for all these models to see how they actually work with people because

2:38:56.880 --> 2:39:01.120
 there are no products publicly available to do that that focus on open domain conversation. So we

2:39:01.120 --> 2:39:09.280
 can test how Facebook Blender doing or GPT three doing. So GPT three, we managed to improve by a

2:39:09.280 --> 2:39:13.920
 few percentage points, like three or four pretty meaningful amount of percentage points, our main

2:39:13.920 --> 2:39:20.000
 metric, which is the ratio of conversation that make people feel better. And every other metric

2:39:20.000 --> 2:39:27.840
 across across the field got a little boost. Right now, I'd say one out of five responses from replica

2:39:27.840 --> 2:39:34.080
 comes comes from GPT three. So our own Blender mixes up like a bunch of candidates from different

2:39:34.080 --> 2:39:41.280
 Blender, you said? Well, yeah, just the model that looks at looks at top candidates from

2:39:41.280 --> 2:39:47.360
 different models and then takes the most the best one. So right now, one of five will come from

2:39:47.920 --> 2:39:56.080
 GPT three. That is really great. I mean, what's the do you have hope for? Like,

2:39:56.080 --> 2:40:00.960
 do you think there's a ceiling to this kind of approach? So we've had for a very long time,

2:40:00.960 --> 2:40:07.680
 we've used since the very beginning, it was most of replica was scripted. And then a little bit

2:40:07.680 --> 2:40:14.560
 of this fallback part of replica was using a retrieval model. And then those retrieval models

2:40:14.560 --> 2:40:19.920
 started getting better and better and better, which transform is a lot better. And we're seeing

2:40:19.920 --> 2:40:26.240
 great results. And then with GPT two, finally generative models that originally were not very

2:40:26.240 --> 2:40:32.320
 good. And we're the very, very fallback option for most of our conversations, but wouldn't even put

2:40:32.320 --> 2:40:37.440
 them in production. Finally, we could use some generative models as well along, you know,

2:40:38.080 --> 2:40:43.200
 next to our retrieval models. And then now we do GPT three, they're almost in par.

2:40:44.960 --> 2:40:49.520
 So that's pretty exciting. I think just seeing how from the very beginning of,

2:40:49.520 --> 2:40:56.080
 you know, from 2015, where the first models start to pop up here and there, like sequence to sequence,

2:40:57.120 --> 2:41:02.560
 the first papers on that, from my observer standpoint, first, it's not, you know, it doesn't

2:41:02.560 --> 2:41:07.280
 really, it's not really building, but it's only testing it on people basically. And I'm in my

2:41:07.280 --> 2:41:13.200
 product to see how all of a sudden we can use generative dialogue models in production, and

2:41:13.200 --> 2:41:18.080
 they're better than others. And they're better than scripted content. So we can't really get

2:41:18.080 --> 2:41:24.080
 our scripted hard code of content any more to be as good as our anti model. That's exciting.

2:41:24.080 --> 2:41:26.320
 They're much better. Yeah.

2:41:27.520 --> 2:41:32.160
 To your question, whether that's the right way to go, I'm again, I'm in the server seat. I'm just

2:41:33.840 --> 2:41:40.640
 watching this very exciting movie. I mean, so far, it's been stupid to bet against deep learning.

2:41:40.640 --> 2:41:47.920
 So whether increasing the size, size even more, whether a hundred trillion parameters will finally

2:41:47.920 --> 2:41:54.240
 get us to the right answer, whether that's the way or whether there should be, there has to be some

2:41:54.240 --> 2:42:00.480
 other, again, I'm definitely not an expert in any way. I think, and that's purely my instinct,

2:42:00.480 --> 2:42:03.440
 saying that there should be something else as well for memory.

2:42:04.640 --> 2:42:09.600
 No, for sure. But the question is, I wonder, I mean, yeah, then then the argument is for

2:42:09.600 --> 2:42:14.480
 reasoning or for memory, it might emerge with more parameters. It might emerge larger.

2:42:14.480 --> 2:42:19.040
 But it might emerge. I would never think that, to be honest, maybe in 2017,

2:42:19.680 --> 2:42:25.840
 where we've been just experimenting with all the research that was coming out then,

2:42:26.640 --> 2:42:31.120
 I felt like we're hitting a wall, that there should be something completely different.

2:42:31.120 --> 2:42:34.560
 But then transforming models, and then just bigger models, and then all of a sudden,

2:42:34.560 --> 2:42:40.400
 size matters. At that point, it felt like something dramatic needs to happen. But it

2:42:40.400 --> 2:42:48.480
 didn't. And just the size gave us these results that, to me, are clear indication that we can

2:42:48.480 --> 2:42:52.560
 solve this problem pretty soon. Did fine tuning help quite a bit?

2:42:52.560 --> 2:42:59.120
 Oh, yeah. Without it, it wasn't as good. I mean, there is a compelling hope that you

2:42:59.120 --> 2:43:04.000
 don't have to do fine tuning, which is one of the cool things about GBT3. It seems to do well

2:43:04.000 --> 2:43:09.280
 without any fine tuning. I guess for specific applications, you still want to train it on a

2:43:09.280 --> 2:43:16.800
 certain, add a little fine tune on a specific use case. But it's an incredibly impressive

2:43:18.000 --> 2:43:22.640
 thing from my standpoint. And again, I'm not an expert, so I wanted to say that.

2:43:22.640 --> 2:43:23.840
 Yeah, I'm going to... There will be people then.

2:43:24.480 --> 2:43:29.440
 Yeah, but I have access to the API, and I'm going to probably do a bunch of fun things with it.

2:43:29.440 --> 2:43:35.280
 I already did some fun things, some videos coming out. Just for the hell of it. I mean,

2:43:35.280 --> 2:43:38.960
 I could be a troll at this point with it. I haven't used it for a series of applications,

2:43:38.960 --> 2:43:45.040
 so it's really cool to see. You're right. You're able to actually use it with real people and

2:43:45.040 --> 2:43:51.920
 see how well it works. That's really exciting. Let me ask you another absurd question, but

2:43:51.920 --> 2:43:59.760
 there's a feeling when you interact with Replico, with an AI system, there's an entity there.

2:44:01.600 --> 2:44:08.960
 Do you think that entity has to be self aware? Do you think it has to have consciousness to create

2:44:11.200 --> 2:44:16.640
 a rich experience and a corollary? What is consciousness?

2:44:16.640 --> 2:44:22.320
 I don't know if it does need to have any of those things, but again,

2:44:22.320 --> 2:44:26.960
 because right now, it doesn't have anything. Again, a bunch of tricks make us simulate.

2:44:29.520 --> 2:44:33.600
 I'm not sure. Let's just put it this way. But I think as long as you can assimilate it,

2:44:33.600 --> 2:44:42.400
 if you can feel like you're talking to a robot or a machine that seems to be self aware,

2:44:42.400 --> 2:44:49.520
 that seems to reason well and feels like a person, I think that's enough. Again, what's the goal?

2:44:50.720 --> 2:44:55.920
 In order to make people feel better, we might not even need that in the end of the day.

2:44:55.920 --> 2:45:02.640
 What about, so that's one goal, what about ethical things about suffering? The moment

2:45:02.640 --> 2:45:07.600
 there's a display of consciousness, we associate consciousness with suffering,

2:45:07.600 --> 2:45:14.640
 you know, there's a temptation to say, well, shouldn't this thing have rights?

2:45:21.200 --> 2:45:26.320
 You know, should we be careful about how we interact with a replica? Like,

2:45:26.960 --> 2:45:32.400
 should it be illegal to torture a replica? Right? All those kinds of things.

2:45:32.400 --> 2:45:39.840
 See, I personally believe that that's going to be a thing. That's a serious thing to think

2:45:39.840 --> 2:45:48.800
 about, but I'm not sure when. But by your smile, I can tell that's not a current concern. But do

2:45:48.800 --> 2:45:56.160
 you think about that kind of stuff? About suffering and torture and ethical questions about AI

2:45:56.160 --> 2:46:01.360
 systems from their perspective? I think if we're talking about a long game, I wouldn't torture

2:46:01.360 --> 2:46:07.040
 your AI. Who knows what happens in five to 10 years. Yeah, they'll get you off the map,

2:46:07.040 --> 2:46:11.440
 so they'll get you back eventually. I'm trying to be as nice as possible and create this ally.

2:46:14.160 --> 2:46:19.760
 I think there should be regulation both way in a way. Like, I don't think it's okay to torture

2:46:19.760 --> 2:46:25.040
 an AI, to be honest. I don't think it's okay to yell, Alexa, turn on the lights. I think there

2:46:25.040 --> 2:46:30.240
 should be some, or just saying kind of nasty, you know, like how kids learn to interact with

2:46:30.240 --> 2:46:35.760
 Alexa in this kind of mean way, because they just yell at it all the time. I think that's great.

2:46:35.760 --> 2:46:40.240
 I think there should be some feedback loops so that these systems don't train us that it's okay

2:46:40.240 --> 2:46:47.360
 to do that in general. So that if you try to do that, you really get some feedback from the system

2:46:47.360 --> 2:46:53.920
 that it's not okay with that. And that's the most important right now. Let me ask a question that

2:46:53.920 --> 2:47:02.720
 I think people are curious about when they look at a world class leader and thinker such as yourself,

2:47:03.280 --> 2:47:10.320
 as what books, technical fiction, philosophical had a big impact on your life? And maybe from

2:47:10.320 --> 2:47:16.960
 another perspective, what books would you recommend others read? So my choice, the three books, right?

2:47:16.960 --> 2:47:24.240
 Three books. My choice is, so the one book that really influenced me a lot when I was

2:47:24.240 --> 2:47:30.800
 building, starting out this company, maybe 10 years ago, was G.B. Got a last year book.

2:47:31.840 --> 2:47:38.080
 And I like everything about it. First of all, it's just beautifully written and it's so old

2:47:38.080 --> 2:47:46.080
 school and so someone outdated a little bit. But I think the idea is in it about the fact that

2:47:46.080 --> 2:47:52.000
 a few meaningless components can come together and create meaning that we can't even understand.

2:47:52.560 --> 2:47:56.880
 So this emergent thing, I mean, complexity, the whole science of complexity,

2:47:57.680 --> 2:48:02.240
 and that beauty, intelligence, all interesting things about this world emerge.

2:48:04.480 --> 2:48:12.400
 Yeah, and yeah, the guttural theorems and just thinking about like what even these form, you

2:48:12.400 --> 2:48:19.120
 know, even all these formal systems, something can be created that we can't quite yet understand.

2:48:19.120 --> 2:48:25.280
 And that from my romantic standpoint was always just, that is why it's important to,

2:48:25.280 --> 2:48:31.120
 maybe I should try to work on these systems and try to build an AI. Yes, I'm not an engineer.

2:48:31.120 --> 2:48:35.360
 Yes, I don't really know how it works. But I think something comes out of it that's pure

2:48:35.360 --> 2:48:43.040
 poetry. And I know a little bit about that. Something magical comes out of it that we

2:48:43.040 --> 2:48:48.800
 can't quite put a finger on. That's why that book was really fundamental for me just for,

2:48:50.080 --> 2:48:54.080
 I don't even know why it was just all about this little magic that happens.

2:48:55.360 --> 2:49:00.480
 So that's one that probably the most important book for replica was Carl Rogers on Becoming a

2:49:00.480 --> 2:49:07.120
 Person. And that's really, as I think, when I think about our company, it's all about,

2:49:07.120 --> 2:49:11.120
 there's so many, there's so many little magical things that happened over the course of working on

2:49:11.120 --> 2:49:18.160
 it. For instance, I mean, the most famous chat bot that we learned about when we started working

2:49:18.160 --> 2:49:24.560
 on the company was Eliza, which was Weissenbaum, you know, the MIT professor that built a chat

2:49:24.560 --> 2:49:31.360
 bot that would listen to you and be a therapist. And I got really inspired to build replica when

2:49:31.360 --> 2:49:36.800
 I read Carl Rogers on Becoming a Person. And then I realized that Eliza was mocking Carl Rogers.

2:49:37.520 --> 2:49:42.640
 It was Carl Rogers back in the day. But I thought that Carl Rogers ideas are,

2:49:43.840 --> 2:49:47.920
 they're simple. And they're not, you know, they're very, very simple. But they're,

2:49:48.480 --> 2:49:53.200
 they're maybe the most profound thing I've ever learned about human beings. And that's the fact

2:49:53.200 --> 2:49:59.280
 that before Carl Rogers, most therapy was about seeing what's wrong with people and trying to

2:49:59.280 --> 2:50:04.960
 fix it or show them what's wrong with you. And it was all built on the fact that most people are,

2:50:04.960 --> 2:50:11.280
 all people are fundamentally flawed. We have this, you know, broken psyche. And this is just a,

2:50:11.280 --> 2:50:16.880
 therapy is just an instrument to shed some light on that. And Carl Rogers was different in a way

2:50:16.880 --> 2:50:23.360
 that he finally said that, well, it's very important for therapy to work is to create this therapeutic

2:50:23.360 --> 2:50:29.360
 relationship where you believe fundamentally and inclination to positive growth that everyone

2:50:29.360 --> 2:50:35.040
 deep inside wants to grow positively and change. And it's super important to create this space and

2:50:35.040 --> 2:50:39.600
 this therapeutic relationship where you give unconditional positive regard, deep understanding,

2:50:39.600 --> 2:50:45.200
 allowing someone else to be a separate person, full acceptance. And you also try to be as

2:50:45.200 --> 2:50:51.200
 genuine and possible in it as possible in it. And then, and he's, and then for him, that was his own

2:50:51.200 --> 2:50:57.120
 journey of personal growth. And that was back in the 60s. And even that book that is, you know,

2:50:57.120 --> 2:51:03.200
 that's coming from years ago, there's a mention that even machines can potentially do that.

2:51:05.120 --> 2:51:09.280
 And I always felt that, you know, creating this space is probably the most, the biggest

2:51:09.280 --> 2:51:13.440
 gift we can give to each other. And that's why the book was fundamental for me, personally,

2:51:13.440 --> 2:51:19.440
 because I felt I want to be learning how to do that in my life. And maybe I can scale it with,

2:51:19.440 --> 2:51:24.960
 you know, with these AI systems, and other people can get access to that. So I think Carl Rogers,

2:51:24.960 --> 2:51:28.320
 it's a pretty dry and a little bit boring book, but I think the idea is there.

2:51:28.320 --> 2:51:29.600
 Would you recommend others try to read it?

2:51:30.800 --> 2:51:34.560
 I do. I think for, just for yourself, for...

2:51:34.560 --> 2:51:36.560
 As a human, not as an AI.

2:51:36.560 --> 2:51:44.240
 As a human. It is just, and for him, that was his own path of his own personal, of growing

2:51:44.240 --> 2:51:49.920
 personally over years, working with people like that. And so it was work and himself growing,

2:51:49.920 --> 2:51:53.600
 helping other people grow and growing through that. And that's fundamentally what I believe in

2:51:53.600 --> 2:52:00.800
 with our work, helping other people grow, growing ourselves, ourselves, trying to build a company

2:52:00.800 --> 2:52:05.680
 that's all built on these principles, you know, having a good time allowing some people to work

2:52:05.680 --> 2:52:12.480
 with, to grow a little bit. So these two books, and then I would throw in what we have on our,

2:52:13.600 --> 2:52:19.600
 in our office, when we started a company in Russia, we put a neon sign in our office because we

2:52:19.600 --> 2:52:25.360
 thought that's a recipe for success. If we do that, we're definitely going to wake up as a

2:52:25.360 --> 2:52:30.560
 multiple dollar company. And it was the Ludwig Wittgenstein quote, the limits of my language,

2:52:30.560 --> 2:52:31.600
 the limits of my world.

2:52:31.600 --> 2:52:32.720
 What's the quote?

2:52:32.720 --> 2:52:39.440
 The limits of my language or the limits of my world. And I love The Tractatus, I think it's

2:52:39.440 --> 2:52:41.600
 just, it's just a beautiful.

2:52:41.600 --> 2:52:43.200
 It's a book by Wittgenstein.

2:52:43.200 --> 2:52:47.520
 Yeah, and I would recommend that too, even although he himself didn't believe in that

2:52:47.520 --> 2:52:54.720
 by the end of his lifetime, and debunked his ideas. But I think I remember, once an engineer came

2:52:54.720 --> 2:53:01.040
 in 2012, I think, or 13, a friend of ours who worked with us and then went on to work a deep

2:53:01.040 --> 2:53:06.640
 mine and he gave, talked to us about word to back. And I saw that I'm like, wow, that's,

2:53:07.840 --> 2:53:13.680
 you know, they, they wanted to translate language into, you know, some other representation.

2:53:13.680 --> 2:53:19.360
 And it seems like some, you know, somehow all of that, at some point, I think we'll come into this

2:53:19.360 --> 2:53:24.960
 one, to this one place, somehow, just all feels like different people think about

2:53:24.960 --> 2:53:28.480
 similar ideas in different times from absolutely different perspectives.

2:53:28.480 --> 2:53:30.480
 And that's why I like these books.

2:53:30.480 --> 2:53:33.200
 The limits of our world, which is the limit of our world.

2:53:33.200 --> 2:53:38.480
 And we still have that neosine.

2:53:40.480 --> 2:53:43.440
 It's very hard to work with this red light in your face.

2:53:43.440 --> 2:53:50.480
 I mean, on the, on the Russian side of things, in terms of language,

2:53:51.440 --> 2:53:55.360
 the limits of language being a limit of our world, you know, Russian is a beautiful language,

2:53:55.360 --> 2:53:58.720
 in some sense, there's wit, there's humor, there's pain.

2:54:00.000 --> 2:54:03.840
 There's so much, we don't have time to talk about it much today, but I'm going to Paris

2:54:05.680 --> 2:54:07.920
 to talk to Dostoevsky Tolstoy translators.

2:54:09.120 --> 2:54:15.520
 I think it's fascinating art, like in the art and engineering, that means such an interesting

2:54:15.520 --> 2:54:22.880
 process. But so from the replica perspective, do you, what do you think about translation?

2:54:22.880 --> 2:54:26.720
 How difficult it is to create a deep meaningful connection in Russian

2:54:27.360 --> 2:54:32.960
 versus English? How you can translate the two languages? You speak both?

2:54:34.000 --> 2:54:36.400
 Yeah, I think we're two different people in different languages.

2:54:37.840 --> 2:54:40.960
 Even I'm, you know, thinking about, there's actually some research on that.

2:54:40.960 --> 2:54:44.880
 I looked into that at some point, because I was fascinated by the fact that what I'm talking

2:54:44.880 --> 2:54:48.400
 about with, what I was talking about with my Russian therapist has nothing to do with what

2:54:48.400 --> 2:54:53.600
 I'm talking about with my English speaking therapist. It's two different lives, two different

2:54:54.480 --> 2:55:01.440
 types of, you know, conversations to different personas. The main difference between the languages

2:55:01.440 --> 2:55:07.200
 are with Russian and English is that Russian, well, English is like a piano. It's a limited

2:55:07.200 --> 2:55:13.600
 number of a lot of different keys, but not too many. And Russian is like an organ or something.

2:55:13.600 --> 2:55:18.160
 It's just something gigantic with so many different keys and so many different opportunities

2:55:18.160 --> 2:55:22.080
 to screw up and so many opportunities to do something completely tone deaf.

2:55:24.240 --> 2:55:32.000
 It is just a much harder language to use. It has way too many, way too much flexibility

2:55:32.000 --> 2:55:39.040
 and way too many tones. What about the entirety of like World War II, communism, Stalin,

2:55:39.600 --> 2:55:46.880
 the pain of the people like having been deceived by the dream? Like all the pain of like just the

2:55:46.880 --> 2:55:51.680
 entirety of it. Is that in the language too? Does that have to do? Oh, for sure. I mean,

2:55:51.680 --> 2:55:56.560
 we have words that don't have direct translation to English that are very much

2:55:58.560 --> 2:56:03.600
 like we have abditsa, which is sort of like to hold a grudge or something. But it doesn't have,

2:56:04.320 --> 2:56:08.400
 it doesn't, you don't need to have anyone to do it to you. It's just your state.

2:56:09.120 --> 2:56:13.600
 You just feel like that. You feel like betrayed by other people, basically, but it's not

2:56:13.600 --> 2:56:19.120
 that. And you can't really translate that. And I think it's super important that very many words

2:56:19.120 --> 2:56:25.120
 that are very specific explain the Russian being. And I think it can only come from a

2:56:25.120 --> 2:56:32.960
 nation that suffered so much and saw institutions fall time after time after time. And what's

2:56:32.960 --> 2:56:38.960
 exciting, maybe not exciting, citing the wrong word, but what's interesting about like my generation,

2:56:38.960 --> 2:56:45.360
 my mom's generation, my parents generation, that we saw institutions fall two or three times in

2:56:45.360 --> 2:56:50.160
 our lifetime. And most Americans have never seen them fall. And they just think that they exist

2:56:50.160 --> 2:56:57.440
 forever. Which is really interesting, but it's definitely a country that suffered so much. And

2:56:58.080 --> 2:57:04.560
 it makes, unfortunately, when I go back and I hang out with my Russian friends, it makes people

2:57:04.560 --> 2:57:11.040
 it makes people very cynical. They stop believing in the future. I hope that's not going to be

2:57:11.840 --> 2:57:17.040
 the case for so long, or something's going to change again. But I think seeing institutions

2:57:17.040 --> 2:57:23.920
 fall is a very traumatic experience. It makes it very interesting. And what's on 2020 is a very

2:57:23.920 --> 2:57:33.120
 interesting. Do you think civilization will collapse? See, I'm a very practical person.

2:57:33.120 --> 2:57:36.800
 Well, we're speaking in English. So like you said, you're a different person in English and

2:57:36.800 --> 2:57:40.640
 Russian. So in Russian, you might answer that differently. But in English.

2:57:42.000 --> 2:57:48.560
 Well, I'm an optimist. And I, I generally believe that there is all, you know, even

2:57:48.560 --> 2:57:55.520
 although the perspectives agree, there is always a place for, for a miracle. I mean, it's always

2:57:55.520 --> 2:58:01.520
 been like that with my life. So yeah, my life's been, I've been incredibly lucky and things just

2:58:01.520 --> 2:58:07.840
 miracles happen all the time with this company, with people I know, with everything around me.

2:58:07.840 --> 2:58:13.600
 And so I didn't mention that book, but it may be in search of miraculous or in search for miraculous

2:58:13.600 --> 2:58:18.560
 or whatever the English translation for that is good Russian book to, for everyone to read.

2:58:21.120 --> 2:58:25.920
 Yeah, I mean, if you put good vibes, if you put love out there in the world,

2:58:25.920 --> 2:58:34.560
 miracles somehow happen. Yeah, I believe that too. Or at least I believe that. I don't know.

2:58:35.600 --> 2:58:42.640
 Let me ask the most absurd, final, ridiculous question of, we talked about life a lot. What

2:58:42.640 --> 2:58:46.320
 do you think is the meaning of it all? What's the meaning of life?

2:58:46.320 --> 2:58:56.240
 I mean, my answer is probably going to be pretty cheesy. But I think the state of love

2:58:56.240 --> 2:59:02.880
 is once you feel it in a way that we discussed before. I'm not talking about falling love or

2:59:04.560 --> 2:59:11.120
 just love to yourself, to other people, to something to the world, that state of

2:59:11.120 --> 2:59:16.480
 bliss that we experience sometimes, whether through connection with ourselves, with our

2:59:16.480 --> 2:59:23.600
 people, with the technology. There's something special about those moments. So

2:59:27.280 --> 2:59:33.120
 I would say if anything, that's the only, if it's not for that, then for what else are we really

2:59:34.240 --> 2:59:38.800
 trying to do that? I don't think there's a better way to end it than talking about love.

2:59:38.800 --> 2:59:44.720
 Eugenia, I told you offline that there's something about me that felt like this.

2:59:47.360 --> 2:59:53.040
 Talking to you, meeting you in person will be a turning point for my life. I know that might be

2:59:53.040 --> 3:00:01.200
 sound weird to hear, but it was a huge honor to talk to you. I hope we talk again. Thank you so

3:00:01.200 --> 3:00:06.400
 much for your time. Thank you so much, Alex. Thanks for listening to this conversation with

3:00:06.400 --> 3:00:13.120
 Eugenia Cuita and thank you to our sponsors, DoorDash, Dollar Shave Club, and Cash App.

3:00:13.120 --> 3:00:17.360
 Click the sponsor links in the description to get a discount and to support this podcast.

3:00:17.920 --> 3:00:22.720
 If you enjoy this thing, subscribe on YouTube, review it with five stars and up a podcast,

3:00:22.720 --> 3:00:28.400
 follow on Spotify, support on Patreon, or connect with me on Twitter at Lex Freedman.

3:00:28.400 --> 3:00:31.200
 And now let me leave you with some words from Carl Sagan.

3:00:31.200 --> 3:00:37.200
 The world is so exquisite with so much love and moral depth that there's no reason to deceive

3:00:37.200 --> 3:00:43.280
 ourselves with pretty stories of which there's little good evidence. Far better, it seems to me,

3:00:43.280 --> 3:00:50.080
 and our vulnerability is to look death in the eye and to be grateful every day for the brief

3:00:50.080 --> 3:01:06.000
 but magnificent opportunity that life provides. Thank you for listening and hope to see you next time.

