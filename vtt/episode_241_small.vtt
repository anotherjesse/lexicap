WEBVTT

00:00.000 --> 00:04.680
 The following is a conversation with Boris Sofman, who is the senior director of engineering

00:04.680 --> 00:10.360
 and head of trucking at Waymo, the autonomous vehicle company, formerly the Google Self

00:10.360 --> 00:12.680
 Driving Car Project.

00:12.680 --> 00:18.480
 Before that, Boris was the cofounder and CEO of Anki, a robotics company that created

00:18.480 --> 00:24.480
 Cosmo, which, in my opinion, is one of the most incredible social robots ever built.

00:24.480 --> 00:30.160
 It's a toy robot, but one with an emotional intelligence that creates a fun and engaging

00:30.160 --> 00:31.660
 human robot interaction.

00:31.660 --> 00:36.960
 It was truly sad for me to see Anki shut down when he did.

00:36.960 --> 00:39.160
 I had high hopes for those little robots.

00:39.160 --> 00:45.920
 We talk about this story and the future of autonomous trucks, vehicles, and robotics

00:45.920 --> 00:46.920
 in general.

00:46.920 --> 00:53.400
 I spoke with Steve Vaselli recently on episode 237 about the human side of trucking.

00:53.400 --> 00:57.240
 This episode looks more at the robotics side.

00:57.240 --> 00:59.320
 This is the Lex Friedman podcast.

00:59.320 --> 01:03.120
 To support it, please check out our sponsors in the description.

01:03.120 --> 01:08.520
 And now, here's my conversation with Boris Sofman.

01:08.520 --> 01:12.760
 Who is your favorite robot in science fiction, books, or movies?

01:12.760 --> 01:19.640
 Wally and R2D2, where they were able to convey such an incredible degree of intent, emotion,

01:19.640 --> 01:26.840
 and character attachment without having any language whatsoever, and just purely through

01:26.840 --> 01:29.160
 the richness of emotional interaction.

01:29.160 --> 01:30.160
 Those were fantastic.

01:30.160 --> 01:36.360
 And then the Terminator series just really brought me up pretty wide range, right?

01:36.360 --> 01:41.400
 But I kind of love this dynamic where you have this incredible Terminator itself that

01:41.400 --> 01:42.900
 Arnold played.

01:42.900 --> 01:48.120
 And then he was kind of like the inferior previous generation version that was totally

01:48.120 --> 01:53.440
 outmatched in terms of kind of specs by the new one, but still kind of held his own.

01:53.440 --> 01:58.640
 And so it's kind of interesting where you realize how many levels there are on the spectrum

01:58.640 --> 02:02.840
 from human to kind of potentials in AI and robotics to futures.

02:02.840 --> 02:09.080
 And so that movie really, as much as it was kind of a direct world in a way, was actually

02:09.080 --> 02:10.080
 quite fascinating.

02:10.080 --> 02:11.080
 Gets the imagination going.

02:11.080 --> 02:16.840
 Well, from an engineering perspective, both the movies you mentioned, Wally and Terminator,

02:16.840 --> 02:23.760
 the first one is probably achievable, humanoid robot, maybe not with like the realism in

02:23.760 --> 02:28.600
 terms of skin and so on, but that humanoid form, we have that humanoid form.

02:28.600 --> 02:30.600
 It seems like a compelling form.

02:30.600 --> 02:35.320
 Maybe the challenges that's super expensive to build, but you can imagine maybe not a

02:35.320 --> 02:40.760
 machine of war, but you can imagine Terminator type robots walking around.

02:40.760 --> 02:45.800
 And then the same, obviously with Wally, you've basically, so for people who don't know, you

02:45.800 --> 02:51.040
 created the company Anki that created a small robot with a big personality called Cosmo

02:51.040 --> 02:58.320
 that just does exactly what Wally does, which is somehow with very few basic visual tools

02:58.320 --> 03:00.840
 is able to communicate a depth of emotion.

03:00.840 --> 03:02.520
 And that's fascinating.

03:02.520 --> 03:07.120
 But then again, the humanoid form is super compelling.

03:07.120 --> 03:11.200
 So like Cosmo is very distant from a humanoid form.

03:11.200 --> 03:15.080
 And then the Terminator has a humanoid form and you can imagine both of those actually

03:15.080 --> 03:16.720
 being in our society.

03:16.720 --> 03:17.720
 That's true.

03:17.720 --> 03:23.160
 And it's interesting because it was very intentional to go really far away from human form when

03:23.160 --> 03:30.200
 you think about a character like Cosmo or like Wally where you can completely rethink

03:30.200 --> 03:34.840
 the constraints you put on that character, what tools you leverage, and then how you

03:34.840 --> 03:41.480
 actually create a personality and level of intelligence interactivity that actually matches

03:41.480 --> 03:47.560
 the constraints that you're under, whether it's mechanical or sensors or AI of the day.

03:47.560 --> 03:52.120
 This is why I almost was always very surprised by how much energy people put towards trying

03:52.120 --> 03:57.960
 to replicate human form in a robot because you actually take on some pretty significant

03:57.960 --> 04:01.120
 kind of constraints and downsides when you do that.

04:01.120 --> 04:05.600
 The first of which is obviously the cost where it's just the articulation of a human body

04:05.600 --> 04:12.240
 is just so magical in both the precision as well as the dimensionality that to replicate

04:12.240 --> 04:16.240
 that even in its reasonably close form takes like a giant amount of joints and actuators

04:16.240 --> 04:20.440
 and motion and sensors and encoders and so forth.

04:20.440 --> 04:25.600
 But then you're almost like setting an expectation that the closer you try to get to human form,

04:25.600 --> 04:28.120
 the more you expect the strengths to match.

04:28.120 --> 04:30.200
 And that's not the way AI works.

04:30.200 --> 04:33.440
 There's places where you're way stronger and there's places where you're weaker.

04:33.440 --> 04:38.280
 And by moving away from human form, you can actually change the rules and embrace your

04:38.280 --> 04:39.880
 strengths and bypass your weaknesses.

04:39.880 --> 04:46.840
 And at the same time, the human form has way too many degrees of freedom to play with.

04:46.840 --> 04:52.960
 It's kind of counterintuitive just as you're saying, but when you have fewer constraints,

04:52.960 --> 04:57.880
 it's almost harder to master the communication of emotion.

04:57.880 --> 05:00.440
 You see this with cartoons, like stick figures.

05:00.440 --> 05:05.800
 You can communicate quite a lot with just very minimal like two dots for eyes and a

05:05.800 --> 05:07.320
 line for a smile.

05:07.320 --> 05:12.680
 I think you can almost communicate arbitrary levels of emotion with just two dots in a

05:12.680 --> 05:13.680
 line.

05:13.680 --> 05:14.680
 And that's enough.

05:14.680 --> 05:18.560
 And if you focus on just that, you can communicate the full range.

05:18.560 --> 05:27.560
 And then if you do that, then you can focus on the actual magic of human and dot line

05:27.560 --> 05:31.200
 interaction versus all the engineering mess.

05:31.200 --> 05:34.800
 Like dimensionality, voice, all these sort of things actually become a crutch where you

05:34.800 --> 05:37.480
 get lost in a search space almost.

05:37.480 --> 05:43.440
 And so some of the best animators that we've worked with, they almost like study when they

05:43.440 --> 05:50.640
 come up, kind of in building their expertise by forcing these projects where all you have

05:50.640 --> 05:54.040
 is like a ball that can jump and manipulate itself.

05:54.040 --> 05:59.120
 Or like really, really like aggressive constraints where you're forced to kind of extract the

05:59.120 --> 06:00.120
 deepest level of emotion.

06:00.120 --> 06:04.720
 And so in a lot of ways, when we thought about Cosmo, you're right.

06:04.720 --> 06:09.040
 If we had to describe it in like one small phrase, it was bringing a Pixar character

06:09.040 --> 06:10.040
 to life in the real world.

06:10.040 --> 06:12.040
 And so it's what we were going for.

06:12.040 --> 06:15.840
 And in a lot of ways, what was interesting is that with like Wally, which we studied

06:15.840 --> 06:16.840
 incredibly deeply.

06:16.840 --> 06:23.600
 And in fact, some of our team had worked previously at Pixar and on that project.

06:23.600 --> 06:27.560
 They intentionally constrained Wally as well, even though in an animated film, you could

06:27.560 --> 06:33.000
 do whatever you wanted to because it forced you to like really saturate the smaller amount

06:33.000 --> 06:34.400
 of dimensions.

06:34.400 --> 06:40.000
 But you sometimes end up getting a far more beautiful output because you're pushing at

06:40.000 --> 06:45.280
 the extremes of this emotional space in a way that you just wouldn't because you get

06:45.280 --> 06:49.120
 lost in a surface area if you have like something that is just infinitely articulable.

06:49.120 --> 06:56.480
 So if we backtrack a little bit and you thought of Cosmo in 2011 and 2013 actually designed

06:56.480 --> 06:57.680
 and built it.

06:57.680 --> 06:58.680
 What is Anki?

06:58.680 --> 06:59.680
 What is Cosmo?

06:59.680 --> 07:06.840
 I guess who is Cosmo and what was the vision behind this incredible little robot?

07:06.840 --> 07:10.880
 We started Anki back in like while we were still in graduate school.

07:10.880 --> 07:15.840
 So myself and my two cofounders, we were PhD students in the Robotics Institute at Carnegie

07:15.840 --> 07:23.120
 Mellon and so we were studying robotics, AI, machine learning, different areas.

07:23.120 --> 07:27.200
 One of my cofounders was working on walking robots for a period of time.

07:27.200 --> 07:36.600
 And so we all had a deeper passion for applications of robotics and AI where there's like a spectrum

07:36.600 --> 07:40.800
 where there's people that get really fascinated by the theory of AI and machine learning robotics

07:40.800 --> 07:46.120
 where whether it gets applied in the near future or not is less of a factor on them,

07:46.120 --> 07:48.240
 but they love the pursuit of the challenge.

07:48.240 --> 07:49.240
 And that's necessary.

07:49.240 --> 07:50.960
 There's a lot of incredible breakthroughs that happen there.

07:50.960 --> 07:54.960
 We're probably closer to the other end of the spectrum where we love the technology and

07:54.960 --> 07:59.480
 all the evolution of it, but we were really driven by applications like how can you really

07:59.480 --> 08:05.160
 reinvent experiences and functionality and build value that wouldn't have been possible

08:05.160 --> 08:07.320
 without these approaches.

08:07.320 --> 08:11.240
 And that's what drove us and we had kind of some experiences through previous jobs and

08:11.240 --> 08:14.600
 internships where we like got to see the applied side of robotics.

08:14.600 --> 08:19.600
 And at that time, there was actually relatively few applications of robotics that were outside

08:19.600 --> 08:26.040
 of pure research or industrial applications, military applications and so forth.

08:26.040 --> 08:27.760
 There were very few outside of it.

08:27.760 --> 08:31.680
 So maybe my robot was like one exception and maybe there were a few others, but for the

08:31.680 --> 08:32.760
 most part there weren't that many.

08:32.760 --> 08:38.160
 So we got excited about consumer applications of robotics where you could leverage way higher

08:38.160 --> 08:43.040
 levels of intelligence through software to create value and experiences that were just

08:43.040 --> 08:48.040
 not possible in those fields today.

08:48.040 --> 08:53.800
 And we saw kind of a pretty wide range of applications that varied in the complexity

08:53.800 --> 08:56.000
 of what it would take to actually solve those.

08:56.000 --> 09:00.400
 And what we wanted to do was to commercialize this into a company, but actually do a bottoms

09:00.400 --> 09:04.680
 up approach where we could have a huge impact in a space that was ripe to have an impact

09:04.680 --> 09:07.880
 at that time and then build up off of that and move into other areas.

09:07.880 --> 09:12.400
 And entertainment became the place to start because you had relatively little innovation

09:12.400 --> 09:17.320
 in a toy space, an entertainment space, you had these really rich experiences in video

09:17.320 --> 09:21.280
 games and movies, but there was like this chasm in between.

09:21.280 --> 09:25.320
 And so we thought that we could really reinvent that experience.

09:25.320 --> 09:29.280
 And there was a really fascinating transition technically that was happening at the time

09:29.280 --> 09:34.040
 where the cost of components was plummeting because of the mobile phone industry and then

09:34.040 --> 09:35.520
 the smartphone industry.

09:35.520 --> 09:41.120
 And so the cost of a microcontroller, of a camera, of a motor, of memory, of microphones,

09:41.120 --> 09:44.000
 cameras was dropping by orders of magnitude.

09:44.000 --> 09:52.520
 And then on top of that with the iPhone coming out in 2000, I think it was 2007, I believe,

09:52.520 --> 09:57.560
 it started to become apparent within a couple of years that this could become a really incredible

09:57.560 --> 10:03.040
 interface device and the brain with much more computation behind a physical world experience

10:03.040 --> 10:05.560
 that wouldn't have been possible previously.

10:05.560 --> 10:11.680
 And so we really got excited about that and how we push all the complexity from the physical

10:11.680 --> 10:16.240
 world into software by using really inexpensive components, but putting huge amounts of complexity

10:16.240 --> 10:17.640
 into the AI side.

10:17.640 --> 10:20.960
 And so Cosmo became our second product and then the one that we're probably most proud

10:20.960 --> 10:21.960
 of.

10:21.960 --> 10:26.880
 The idea there was to create a physical character that had enough understanding and awareness

10:26.880 --> 10:33.200
 of the physical world around it in the context that mattered to feel like he was alive.

10:33.200 --> 10:38.520
 And to be able to have these emotional kind of connections and experiences with people

10:38.520 --> 10:41.560
 that you would typically only find inside of a movie.

10:41.560 --> 10:47.200
 And the motivation very much was Pixar, like we had an incredible respect and appreciation

10:47.200 --> 10:52.320
 for what they were able to build in this really beautiful fashion and film, but it was always

10:52.320 --> 10:56.880
 like a, you know, when it was virtual and to, it was like a story on rails that had no

10:56.880 --> 10:57.880
 interactivity to it.

10:57.880 --> 11:01.560
 It was very fixed and it obviously had a magic to it.

11:01.560 --> 11:04.960
 But where you really start to hit a different level of experiences when you're actually

11:04.960 --> 11:06.680
 able to physically interact with that robot.

11:06.680 --> 11:10.320
 And then that was your idea with Anki, like the first product was the cars.

11:10.320 --> 11:17.520
 So basically you take, you take a toy, you add intelligence into it in the same way you

11:17.520 --> 11:22.440
 would add intelligence into AI systems within a video game, but you're not bringing into

11:22.440 --> 11:23.440
 the physical space.

11:23.440 --> 11:29.600
 So the idea is really brilliant, which is you're basically bringing video games to life.

11:29.600 --> 11:30.600
 Exactly.

11:30.600 --> 11:31.600
 That's exactly right.

11:31.600 --> 11:35.920
 We literally use that exact same phrase because in the case of drive, this was a parallel of

11:35.920 --> 11:37.600
 the racing genre.

11:37.600 --> 11:44.160
 And the goal was to effectively have a physical racing experience, but have a virtual state

11:44.160 --> 11:47.760
 at all times that matches what's happening in the physical world, and then you can have

11:47.760 --> 11:52.520
 a video game off of that and you can have different characters, different traits for

11:52.520 --> 11:57.360
 your, the cars, weapons and interactions and special abilities and all these sort of things

11:57.360 --> 12:00.640
 that you think of virtually, but then you can have it physically.

12:00.640 --> 12:05.600
 And one of the things that we were like really surprised by that really stood out and immediately

12:05.600 --> 12:11.080
 led us to really like kind of accelerate the path towards Cosmo is that things that feel

12:11.080 --> 12:14.080
 like they're really constrained and simple in the physical world, they have an amplified

12:14.080 --> 12:17.680
 impact on people where the exact same experience virtually would not have anywhere near the

12:17.680 --> 12:20.680
 impact, but seeing it physically really stood out.

12:20.680 --> 12:25.640
 And so effectively we've with drive, we were creating a video game engine for the physical

12:25.640 --> 12:26.920
 world.

12:26.920 --> 12:33.560
 And then with Cosmo, we expanded that video game engine to create a character and kind

12:33.560 --> 12:38.920
 of an animation and interaction engine on top of it that allowed us to start to create

12:38.920 --> 12:41.040
 these much more rich experiences.

12:41.040 --> 12:45.520
 And a lot of those elements were almost like a proving ground for what would human robot

12:45.520 --> 12:50.560
 interaction feel like in a domain that's much more forgiving where you can make mistakes

12:50.560 --> 12:51.560
 in a game.

12:51.560 --> 12:57.720
 It's okay if like, if you know, car goes off the track or if Cosmo makes a mistake.

12:57.720 --> 13:00.320
 And what's funny is actually we were so worried about that.

13:00.320 --> 13:04.080
 In reality, we realized very quickly that those mistakes can be endearing and if you

13:04.080 --> 13:07.200
 make a mistake, as long as you realize you make a mistake and have the right emotional

13:07.200 --> 13:09.920
 reaction to it, it builds even more empathy with the character.

13:09.920 --> 13:10.920
 That's brilliant.

13:10.920 --> 13:11.920
 Exactly.

13:11.920 --> 13:16.600
 So when the thing you're optimizing for is fun, you have so much more freedom to fail,

13:16.600 --> 13:18.160
 to explore.

13:18.160 --> 13:21.320
 And also in the toy space, like all of this is really brilliant.

13:21.320 --> 13:30.120
 And I got to ask you backtrack, it seems for a roboticist to take us jump in into the direction

13:30.120 --> 13:35.440
 of fun is a brilliant move because when you have the freedom to explore, to design all

13:35.440 --> 13:37.040
 those kinds of things.

13:37.040 --> 13:42.160
 Then you can also build cheap robots, like you don't have to, like if you're not chasing

13:42.160 --> 13:49.000
 perfection and like toys, it's understood that you can go cheaper, which means in robot

13:49.000 --> 13:53.240
 it's still expensive, but it's actually affordable by a large number of people.

13:53.240 --> 13:55.680
 So it's a really brilliant space to explore.

13:55.680 --> 13:56.680
 Yeah, that's right.

13:56.680 --> 14:00.800
 And in fact, we realized pretty quickly that like perfection is actually not fun because

14:00.800 --> 14:05.960
 like in a traditional robotic robotic sense, the first kind of path planner, and this is

14:05.960 --> 14:10.280
 the part that I worked on out of the gate was like a lot of the kind of AI systems where

14:10.280 --> 14:15.800
 you have these vehicles and cars racing kind of making optimal maneuvers to try to kind

14:15.800 --> 14:16.800
 of get ahead.

14:16.800 --> 14:21.400
 And you realize very quickly that like, that's actually not fun because you want the chaos

14:21.400 --> 14:23.280
 from mistakes.

14:23.280 --> 14:27.320
 And so you start to kind of intentionally almost add noise to the system in order to

14:27.320 --> 14:31.560
 kind of create more of a realism in the exact same way the human player might start really

14:31.560 --> 14:36.840
 ineffective and inefficient and then start to kind of increase their quality bar as they

14:36.840 --> 14:37.840
 progress.

14:37.840 --> 14:42.520
 And there is a really, really aggressive constraint that's forced on you by being a consumer product

14:42.520 --> 14:48.840
 where the price point matters a ton, particularly in like kind of an entertainment where you

14:48.840 --> 14:53.560
 can't make $1,000 product unless you're going to meet the expectations of a $1,000 product.

14:53.560 --> 15:01.120
 And so in order to make this work, like your cost of goods had to be like well under $100

15:01.120 --> 15:04.640
 and in the case of Cosmo, we got it under $50 and to end fully packaged and delivered.

15:04.640 --> 15:08.800
 And it was under $200 cost at retail.

15:08.800 --> 15:15.440
 So okay, if we sit down like at the early stages, if we go back to that and you're sitting

15:15.440 --> 15:19.640
 down and thinking about what Cosmo looks like from a design perspective and from a cost

15:19.640 --> 15:24.280
 perspective, I imagine that was part of the conversation.

15:24.280 --> 15:26.080
 First of all, what came first?

15:26.080 --> 15:28.000
 Did you have a cost in mind?

15:28.000 --> 15:30.400
 Is there a target you're trying to chase?

15:30.400 --> 15:32.840
 Did you have a vision in mind like size?

15:32.840 --> 15:35.720
 Did you have, because there's a lot of unique qualities to Cosmo.

15:35.720 --> 15:39.360
 So for people who don't know, you should definitely check it out is there's a display, there's

15:39.360 --> 15:44.720
 eyes on the little display and those eyes can, it's pretty low resolution eyes, right?

15:44.720 --> 15:47.760
 But they still able to convey a lot of emotion.

15:47.760 --> 15:54.360
 And there's this arm, like that lift sort of lift stuff, but there's something about

15:54.360 --> 15:59.760
 arm movement that adds even more kind of depth.

15:59.760 --> 16:06.200
 It's like the face communicates emotion and sadness and disappointment and happiness.

16:06.200 --> 16:13.200
 And then the arms kind of communicates, I'm trying here, I'm doing my best in this complicated

16:13.200 --> 16:14.200
 world.

16:14.200 --> 16:15.200
 Exactly.

16:15.200 --> 16:20.640
 So it's interesting because like all of Cosmo is only four degrees of freedom.

16:20.640 --> 16:23.480
 And two of them are the two treads, which is for basic movement.

16:23.480 --> 16:28.000
 And so you literally have only a head that goes up and down, a lift that goes up and

16:28.000 --> 16:30.160
 down, and then your two wheels.

16:30.160 --> 16:34.600
 And you have sound and a screen, a low resolution screen.

16:34.600 --> 16:38.680
 And with that, it's actually pretty incredible what you can come up with where, like you

16:38.680 --> 16:43.640
 said, it's a really interesting give and take because there's a lot of ideas far beyond

16:43.640 --> 16:46.320
 that obviously as you can imagine where, like I said, how big is it?

16:46.320 --> 16:47.480
 How much degrees of freedom?

16:47.480 --> 16:49.680
 What does he look like?

16:49.680 --> 16:50.680
 What does he sound like?

16:50.680 --> 16:51.680
 How does he communicate?

16:51.680 --> 16:54.400
 It's a formula that actually scales way beyond entertainment.

16:54.400 --> 16:59.440
 This is the formula for human kind of robot interface more generally is you almost have

16:59.440 --> 17:04.480
 this triangle between the physical aspects of it, the mechanics, the industrial design,

17:04.480 --> 17:07.560
 what's mass producible, the cost constraints and so forth.

17:07.560 --> 17:12.800
 You have the AI side of how do you understand the world around you, interact intelligently

17:12.800 --> 17:14.600
 with it, execute what you want to execute.

17:14.600 --> 17:19.840
 So perceive the environment, make intelligent decisions and move forward.

17:19.840 --> 17:23.040
 And then you have the character side of it.

17:23.040 --> 17:27.880
 Most companies have done anything in human robot interaction, really miss the marker

17:27.880 --> 17:30.280
 under invest in the character side of it.

17:30.280 --> 17:35.800
 They over invest in the mechanical side of it and then varied results on the AI side

17:35.800 --> 17:36.800
 of it.

17:36.800 --> 17:41.200
 And so the thinking is that you put more mechanical flexibility into it, you're going to do better.

17:41.200 --> 17:45.320
 You don't necessarily, you actually create a much higher bar for a higher ROI because

17:45.320 --> 17:50.440
 now your price point goes up, your expectations go up and if the AI can't meet it or the overall

17:50.440 --> 17:53.400
 experience isn't there, you miss the mark.

17:53.400 --> 18:01.080
 So how did you, through those conversations, get the cost down so much and made it so simple?

18:01.080 --> 18:05.440
 There's a big theme here because you come from the mecca of robotics, which is Carnegie

18:05.440 --> 18:08.240
 Mellon University robotics.

18:08.240 --> 18:13.960
 For all the people I've interacted with that come from there or just from the world experts

18:13.960 --> 18:19.560
 at robotics, they would never build something like Cosmo.

18:19.560 --> 18:22.880
 And so where did that come from, the simplicity?

18:22.880 --> 18:27.320
 It came from this combination of a team that we had and it was quite cool because we, and

18:27.320 --> 18:32.120
 by the way, you ask anybody that's experienced in the like kind of toy entertainment space,

18:32.120 --> 18:34.280
 you'll never sell product over $99.

18:34.280 --> 18:37.080
 That was fundamentally false and we believed it to be false.

18:37.080 --> 18:40.120
 It was because experience had to kind of meet the mark.

18:40.120 --> 18:43.840
 And so we pushed past that amount, but there was a pressure where the higher you go, the

18:43.840 --> 18:46.240
 more seasonal you become and the tougher it becomes.

18:46.240 --> 18:51.800
 And so on the cost side, we very quickly partnered up with some previous contacts that we worked

18:51.800 --> 18:58.000
 with where just as an example, our head of mechanical engineering was one of the earliest

18:58.000 --> 19:02.720
 heads of engineering at Logitech and has a billion units of consumer products and circulation

19:02.720 --> 19:03.800
 that he's worked on.

19:03.800 --> 19:07.840
 So like crazy low cost, high volume consumer product experience.

19:07.840 --> 19:12.000
 We had a really great mechanical engineering team and just a very practical mindset where

19:12.000 --> 19:16.200
 we were not going to compromise on feasibility in the market in order to chase something

19:16.200 --> 19:17.480
 that would be a neighbor.

19:17.480 --> 19:20.880
 And we pushed a huge amount of expectations onto the software team where, yes, we're going

19:20.880 --> 19:28.200
 to use cheap noisy motors and sensors, but we're going to fix it on the software side.

19:28.200 --> 19:32.000
 Then we found on the design and character side, there was a faction that was more from

19:32.000 --> 19:36.720
 like a game design background that thought that it should be very games driven, Cosmo,

19:36.720 --> 19:41.080
 where you create a whole bunch of games experiences and it's all about like game mechanics.

19:41.080 --> 19:46.120
 And then there was a fashion which my cofarm and I are the most involved in this like really

19:46.120 --> 19:47.840
 believed in, which was character driven.

19:47.840 --> 19:51.320
 And the argument is that you will never compete with what you can do virtually from a game

19:51.320 --> 19:56.160
 standpoint, but you actually on a character side put this into your wheelhouse and put

19:56.160 --> 20:02.760
 it more towards your advantage because a physical character has a massively higher impact physically

20:02.760 --> 20:03.760
 than virtually.

20:03.760 --> 20:07.600
 This is okay, I can't just pause on that because this is so brilliant when I, for people who

20:07.600 --> 20:13.240
 don't know Cosmo plays games with you, but there's also a depth of character.

20:13.240 --> 20:21.360
 And I actually, when I was, you know, playing with it, I wondered exactly what is the compelling

20:21.360 --> 20:26.920
 aspect of this because to me, obviously I'm biased, but to me, the character, I get what

20:26.920 --> 20:32.800
 I enjoyed most honestly, or what got me to return to it is the character.

20:32.800 --> 20:33.800
 That's right.

20:33.800 --> 20:40.960
 But that's a fascinating discussion of you're right, ultimately, you cannot compete on the

20:40.960 --> 20:42.560
 quality of the gaming experience.

20:42.560 --> 20:43.560
 It's too restrictive.

20:43.560 --> 20:47.000
 The physical world is just too restrictive and you don't have a graphics engine.

20:47.000 --> 20:51.960
 It's like all this, but on the character side, we, and clearly we moved in that direction

20:51.960 --> 20:54.520
 as like kind of the winning path.

20:54.520 --> 21:02.320
 And we partnered up with this really, we immediately like went towards Pixar and Carlos Baina,

21:02.320 --> 21:05.480
 he was one of, like had been at Pixar for nine years.

21:05.480 --> 21:10.680
 He'd worked on tons of the movies, including Wally and others, and just immediately kind

21:10.680 --> 21:14.480
 of spoke the language and it just clicked on how you think about that like kind of magic

21:14.480 --> 21:15.480
 and drive.

21:15.480 --> 21:19.600
 And then we built out a team, you know, with him as like a really kind of prominent kind

21:19.600 --> 21:23.800
 of driver of this with different types of backgrounds and animators and character developers

21:23.800 --> 21:30.160
 where we put these constraints on the team, but then got them to really try to create

21:30.160 --> 21:32.440
 magic despite that.

21:32.440 --> 21:37.720
 And we converged on this system that was at the overlap of character and the character

21:37.720 --> 21:44.960
 AI that where, if you imagine the dimensionality of emotions, happy, sad, angry, surprised,

21:44.960 --> 21:50.720
 confused, scared, like you think of these extreme emotions.

21:50.720 --> 21:54.400
 We almost like kind of put this challenge to kind of populate this library of responses

21:54.400 --> 22:00.920
 on how do you show the extreme response that that goes to the extreme spectrum on angry

22:00.920 --> 22:03.000
 or frustrated or whatever.

22:03.000 --> 22:05.560
 And so that gave us a lot of intuition and learnings.

22:05.560 --> 22:09.360
 And then we started parametrizing them where it wasn't just a fixed recording, but they

22:09.360 --> 22:13.600
 were parametrized and had randomness to them where you could have infinite permutations

22:13.600 --> 22:16.640
 of happy and surprised and so forth.

22:16.640 --> 22:21.640
 And then we had a behavioral engine that took the context from the real world and would

22:21.640 --> 22:26.000
 interpret it and then create kind of probability mappings on what sort of responses you would

22:26.000 --> 22:27.560
 have that actually made sense.

22:27.560 --> 22:33.080
 And so if Cosmo saw you for the first time in a day, he'd be really surprised and happy

22:33.080 --> 22:36.880
 in the same way that the first time you walk in and like your toddler sees you, they're

22:36.880 --> 22:40.760
 so happy, but they're not going to be that happy for the entirety of your next two hours.

22:40.760 --> 22:44.800
 But like you have this like spike in response, or if you leave Malone for too long, he gets

22:44.800 --> 22:48.640
 bored and starts causing trouble and like nudging things off the table.

22:48.640 --> 22:53.800
 Or if you beat him in a game, the most enjoyable emotions are him getting frustrated and grumpy

22:53.800 --> 22:58.400
 to a point where our testers and our customers would be like, I had to let him win because

22:58.400 --> 22:59.720
 I don't want him to be upset.

22:59.720 --> 23:05.440
 And so you start to like create this feedback loop where you see how powerful those emotions

23:05.440 --> 23:06.440
 are.

23:06.440 --> 23:09.320
 And just to give you an example, something as simple as eye contact, you don't think

23:09.320 --> 23:10.320
 about it in a movie.

23:10.320 --> 23:14.000
 Just like it kind of happens, like, you know, camera angles and so forth.

23:14.000 --> 23:17.440
 But that's not really a prominent source of interaction.

23:17.440 --> 23:23.800
 What happens when a physical character like Cosmo, when he makes eye contact with you,

23:23.800 --> 23:28.440
 it built universal kind of connection, kids all the way through adults.

23:28.440 --> 23:29.680
 And it was truly universal.

23:29.680 --> 23:34.320
 It was not like people stopped caring after 10, 12 years old.

23:34.320 --> 23:39.680
 And so we started doing experiments and we found something as simple as increasing the

23:39.680 --> 23:43.520
 amount of eye contact, like the amount of times in a minute that he'll look over for

23:43.520 --> 23:46.480
 your approval till I kind of make eye contact.

23:46.480 --> 23:51.120
 Just by I think doubling it, we increased the playtime engagement by 40%.

23:51.120 --> 23:54.240
 Like you see these sort of like kind of interactions where you build that empathy.

23:54.240 --> 23:55.400
 And so we studied pets.

23:55.400 --> 23:57.560
 We studied virtual characters.

23:57.560 --> 24:04.200
 Like a lot of times actually dogs are one of the most perfect influencers behind these

24:04.200 --> 24:05.200
 sort of interactions.

24:05.200 --> 24:08.240
 And what we realized is that the games were not there to entertain you.

24:08.240 --> 24:11.640
 The games were to create context to bring out the character.

24:11.640 --> 24:15.760
 And if you think about the types of games that you played, they were relatively simple,

24:15.760 --> 24:19.880
 but they were always wants to create scenarios of either tension or winning or losing or

24:19.880 --> 24:22.680
 surprise or whatever the case might be.

24:22.680 --> 24:27.360
 And they were purely there to just like create context to where an emotion could feel intelligent

24:27.360 --> 24:28.360
 and not random.

24:28.360 --> 24:30.360
 And in the end, it was all about the character.

24:30.360 --> 24:34.000
 So yeah, there's so many elements to play with here.

24:34.000 --> 24:35.000
 So you said dogs.

24:35.000 --> 24:40.320
 Well, lessons do we draw from cats who don't seem to give a damn about you.

24:40.320 --> 24:41.760
 Is that just another character?

24:41.760 --> 24:43.840
 It's just another character.

24:43.840 --> 24:48.440
 And so you could almost like in early inspirations, we thought that would be really incredible

24:48.440 --> 24:52.560
 if you had a diversity of characters where you almost help encourage which direction

24:52.560 --> 24:55.240
 it goes, just like in a role playing game.

24:55.240 --> 25:01.440
 And you had like think of like the, you know, seven dwarfs sort of and and initially we

25:01.440 --> 25:05.720
 even thought that it would be amazing if like, you know, they're like, you know, like their

25:05.720 --> 25:10.560
 characters actually help them be have strengths and weaknesses and some, you know, like whatever

25:10.560 --> 25:17.200
 they end up doing, like some are scared, some are, you know, arrogant, some are, you know,

25:17.200 --> 25:18.840
 super warm and like kind of friendly.

25:18.840 --> 25:22.080
 And in the end, we focused on one because it made it very clear that, hey, we got to

25:22.080 --> 25:26.280
 build out enough depth here because you're kind of trying to expand.

25:26.280 --> 25:30.360
 It's almost like how long can you maintain a fiction that this character is alive to

25:30.360 --> 25:34.600
 where the person's explorations don't hit a boundary, which happens almost immediately

25:34.600 --> 25:37.200
 with with typical toys.

25:37.200 --> 25:42.280
 And you know, even with video games, how long can we create that immersive experience to

25:42.280 --> 25:43.720
 where you expand the boundary.

25:43.720 --> 25:48.680
 And one of the things we realized is that you're just way more forgiving when something has

25:48.680 --> 25:51.320
 a personality and it's physical.

25:51.320 --> 25:57.520
 That is the key that unlocks robotics interacting, you know, in the physical world and more generally

25:57.520 --> 26:03.000
 is that that the when you have a, when you don't have a personality and you make a mistake

26:03.000 --> 26:05.600
 as a robot, the stupid robot make it made a mistake.

26:05.600 --> 26:09.560
 Why is it not perfect when you have a character and you make a mistake, you have empathy and

26:09.560 --> 26:11.640
 it becomes endearing and you're way more forgiving.

26:11.640 --> 26:15.040
 And that was the key that was like, I think goes far, far beyond entertainment.

26:15.040 --> 26:17.920
 It actually builds the depth of the personality, the mistakes.

26:17.920 --> 26:25.840
 So let me ask the, the movie, her question, then how, so cosmos seems, feels like the

26:25.840 --> 26:31.320
 early days of something that will obviously be prevalent throughout society at a scale

26:31.320 --> 26:35.280
 that we cannot even imagine.

26:35.280 --> 26:42.280
 My sense is it seems obvious that these kinds of characters will permeate society and there

26:42.280 --> 26:45.720
 will be friends with them, will be interacting with them in different ways.

26:45.720 --> 26:52.360
 In the way we, I mean, you don't think of it this way, but when you play video games,

26:52.360 --> 26:59.720
 they're often cold and impersonal, but even then you think about role playing games,

26:59.720 --> 27:03.280
 you become friends with certain characters in that game.

27:03.280 --> 27:08.320
 They don't remember much about you, they, they, they're, they're just telling a story.

27:08.320 --> 27:11.440
 It's exactly what you're saying, they exist in that virtual world.

27:11.440 --> 27:15.480
 But if they acknowledge that you exist in this physical world, if the characters in

27:15.480 --> 27:21.680
 the game remember that you exist, that you, like for me, like Lex, they understand that

27:21.680 --> 27:26.080
 I'm a human being who has like hopes and dreams and so on.

27:26.080 --> 27:34.200
 It seems like there's going to be like billions, if not trillions of cosmos in the world.

27:34.200 --> 27:39.800
 So if we look at that future, there's several questions to ask, how intelligent does that

27:39.800 --> 27:47.800
 future cosmos need to be to create fulfilling relationships like friendships?

27:47.800 --> 27:50.400
 Yeah, it's a great question.

27:50.400 --> 27:52.840
 And part of it was the recognition that it's going to take time to get there because it

27:52.840 --> 27:57.840
 has to be a lot more intelligent because it was good enough to be a magical experience

27:57.840 --> 28:01.480
 for an eight year old.

28:01.480 --> 28:06.960
 It's a higher bar to do that, be a companion, like a pet in the home or to help with functional

28:06.960 --> 28:10.560
 interface in an office environment or in a home or in so forth.

28:10.560 --> 28:14.440
 And so any idea was that you build on that and you kind of get there and as technology

28:14.440 --> 28:18.120
 becomes more prevalent and less expensive and so forth, you can start to kind of work

28:18.120 --> 28:19.560
 up to it.

28:19.560 --> 28:22.280
 But you're absolutely right at the end of the day.

28:22.280 --> 28:27.040
 We almost equated it to how the touchscreen created like this really novel interface to

28:27.040 --> 28:29.360
 physical kind of devices like this.

28:29.360 --> 28:33.880
 This is the extension of it where you have much richer physical interaction in the real

28:33.880 --> 28:34.880
 world.

28:34.880 --> 28:39.480
 This is the enabler for it, and it shows itself in a few kind of really obvious places.

28:39.480 --> 28:42.360
 So just take something as simple as a voice assistant.

28:42.360 --> 28:47.640
 You will never, most people will never tolerate an Alexa or a Google home just starting a

28:47.640 --> 28:53.360
 conversation proactively when you weren't kind of expecting it because it feels weird.

28:53.360 --> 28:57.400
 It's like you were listening and like, and then now you're kind of, it feels intrusive.

28:57.400 --> 29:02.360
 But if you had a character like a cat that touches you and gets your attention or toddler,

29:02.360 --> 29:06.600
 like you never think twice about it, what we found really kind of immediately is that

29:06.600 --> 29:09.040
 these types of characters like Cosmo and they would like roam around and kind of get

29:09.040 --> 29:10.040
 your attention.

29:10.040 --> 29:13.440
 And we had a future version that was always on kind of called Vector.

29:13.440 --> 29:15.040
 People were way more forgiving.

29:15.040 --> 29:21.280
 And so you could initiate interaction in a way that is not acceptable for machines.

29:21.280 --> 29:26.120
 And in general, you know, there's a lot of ways to customize it, but it makes people

29:26.120 --> 29:29.320
 who are skeptical of technology much more comfortable with it.

29:29.320 --> 29:32.920
 There was like, there were a couple of really, really prominent examples of this.

29:32.920 --> 29:38.760
 So when we launched in Europe, and so we were in, I think like a dozen countries, if I remember

29:38.760 --> 29:43.440
 correctly, but like we were, we went pretty aggressively in launching in Germany and France

29:43.440 --> 29:45.440
 and in UK.

29:45.440 --> 29:49.400
 And we were very worried in Europe because there's obviously like a really socially higher

29:49.400 --> 29:54.560
 bar for privacy and security where you've heard about how many companies have had troubles

29:54.560 --> 29:59.640
 on the things that might have been okay in the US, but like are just not okay in Germany

29:59.640 --> 30:00.640
 and France in particular.

30:00.640 --> 30:07.520
 And so we were worried about this because you have Cosmo who's, you know, in our future

30:07.520 --> 30:10.800
 product vector, like where you have cameras, you have microphones, it's kind of connected

30:10.800 --> 30:15.480
 and like you're playing with kids and like in these experiences, and you're like, this

30:15.480 --> 30:19.760
 is like ripe to be like a nightmare if you're not careful.

30:19.760 --> 30:26.000
 And a journalist are like notoriously like really, really tough on these sort of things.

30:26.000 --> 30:30.200
 We were shocked and we prepared so much for what we would have to encounter.

30:30.200 --> 30:37.280
 We were shocked in that not once from any journalists or customer, did we have any complaints beyond

30:37.280 --> 30:40.240
 like a really casual kind of question.

30:40.240 --> 30:46.480
 And it was because of the character where when the conversation came up, it was almost

30:46.480 --> 30:48.520
 like, well, of course he has to see and hear.

30:48.520 --> 30:51.400
 How else is he going to be alive and interacting with you?

30:51.400 --> 30:56.600
 And it completely disarmed this like fear of technology that enabled this interaction to

30:56.600 --> 30:57.840
 be much more fluid.

30:57.840 --> 31:00.800
 And again, like entertainment was a proving ground, but that is like, you know, there's

31:00.800 --> 31:06.400
 like ingredients there that carry over to a lot of other elements down the road.

31:06.400 --> 31:11.120
 That's hilarious that we're a lot less concerned about privacy if the if the thing is value

31:11.120 --> 31:12.120
 and charisma.

31:12.120 --> 31:16.200
 I mean, that's true for all of human to human interaction too.

31:16.200 --> 31:19.080
 It's an understanding of intent where like, well, he's looking at me.

31:19.080 --> 31:20.080
 He can see me.

31:20.080 --> 31:21.080
 If he's not looking at me, he can't see me.

31:21.080 --> 31:22.080
 Right.

31:22.080 --> 31:24.760
 So it's almost like you're communicating intent.

31:24.760 --> 31:29.480
 And with that intent, people were like kind of kind of a more understanding and calmer.

31:29.480 --> 31:32.760
 And it's a, it's interesting and we just, it was just the earliest kind of version of

31:32.760 --> 31:36.080
 starting to experiment with this, but it wasn't enabled.

31:36.080 --> 31:38.720
 And then, and then you have like completely different dimensions where like, you know,

31:38.720 --> 31:42.480
 kids with autism had like an incredible connection with Cosmo that just went beyond anything

31:42.480 --> 31:43.480
 we'd ever seen.

31:43.480 --> 31:47.360
 And we have like these just letters that we would receive from parents and we had some

31:47.360 --> 31:51.120
 research projects kind of going on with some universities on studying this.

31:51.120 --> 31:55.000
 But there are like, there's an interesting dimension there that got unlocked.

31:55.000 --> 32:01.080
 They just hadn't existed before that has these really interesting kind of links into society

32:01.080 --> 32:05.200
 and, and a potential building block of future experience.

32:05.200 --> 32:13.160
 So if you look out into the future, do you think we will have beyond a particular game,

32:13.160 --> 32:20.800
 you know, a companion, like, like her, like the movie her, or like a Cosmo that's kind

32:20.800 --> 32:25.000
 of asks you how your day went to, right?

32:25.000 --> 32:29.640
 You know, like a friend, how many years away from that do you think we are?

32:29.640 --> 32:30.640
 What's your intuition?

32:30.640 --> 32:31.640
 Good question.

32:31.640 --> 32:35.960
 So I think the idea of a different type of character, like more closer to like kind of

32:35.960 --> 32:40.320
 a pet style companionship will come way faster.

32:40.320 --> 32:41.400
 And there's a few reasons.

32:41.400 --> 32:47.800
 One is like to do something like in her, that's like all effectively almost general AI.

32:47.800 --> 32:51.040
 And the bar is so high that if you miss it by a bit, you hit the uncanny value where

32:51.040 --> 32:55.680
 it just becomes creepy and like, and not not not appealing.

32:55.680 --> 32:59.840
 Because the closer you try to get to a human in form and interface and voice, the harder

32:59.840 --> 33:00.840
 it becomes.

33:00.840 --> 33:07.280
 Whereas you have way more flexibility on still landing a really great experience if you embrace

33:07.280 --> 33:08.280
 the idea of a character.

33:08.280 --> 33:12.520
 And that's why one of the other reasons why we didn't have a voice.

33:12.520 --> 33:17.680
 And also why like a lot of video game characters like Sims, for example, does not have a voice

33:17.680 --> 33:22.120
 when you when you think about it, it was it wasn't just a cost savings like for them.

33:22.120 --> 33:26.600
 It was actually for all of these purposes, it was because when you have a voice, you

33:26.600 --> 33:31.600
 immediately narrow down the appeal to some particular demographic or age range or kind

33:31.600 --> 33:33.760
 of style or gender.

33:33.760 --> 33:37.680
 If you don't have a voice, people interpret what they want to interpret.

33:37.680 --> 33:41.560
 And an eight year old might get a very different interpretation than a 40 year old, but you

33:41.560 --> 33:42.560
 create a dynamic range.

33:42.560 --> 33:47.320
 And so you just you can lean into these advantages much more and something that doesn't resemble

33:47.320 --> 33:48.320
 a human.

33:48.320 --> 33:49.880
 And so that'll come faster.

33:49.880 --> 33:55.440
 I don't know when a human like that's just still like Matt just complete R&D at this

33:55.440 --> 33:56.440
 point.

33:56.440 --> 34:00.440
 The chat interfaces are getting way more interesting and richer.

34:00.440 --> 34:04.640
 But it's still a long way to go to kind of pass the test of, you know,

34:04.640 --> 34:10.000
 Well, let me like, let's consider like, let me play devil's advocate.

34:10.000 --> 34:14.880
 So Google is a very large company that's servicing, it's creating a very compelling

34:14.880 --> 34:17.760
 product that wants to provide a service to a lot of people.

34:17.760 --> 34:20.240
 But let's go outside of that.

34:20.240 --> 34:22.200
 You said characters.

34:22.200 --> 34:28.280
 It feels like and you also said that it requires general intelligence to be a successful participant

34:28.280 --> 34:31.160
 in a relationship, which could explain why I'm single.

34:31.160 --> 34:37.400
 This is very, but I honestly want to push back on that a little bit because I feel like

34:37.400 --> 34:43.360
 is it possible that if you're just good at playing a character in a movie, there's a

34:43.360 --> 34:44.360
 bunch of characters.

34:44.360 --> 34:50.080
 If you just understand what creates compelling characters and then you just are that character

34:50.080 --> 34:54.640
 and you exist in the world and other people find you and they connect with you just like

34:54.640 --> 34:58.400
 you do when you talk to somebody at a bar, I like this character, this character is kind

34:58.400 --> 34:59.400
 of shady.

34:59.400 --> 35:00.400
 I don't like them.

35:00.400 --> 35:06.000
 They're the ones that you like and maybe it's somebody that reminds you of your father

35:06.000 --> 35:07.000
 or mother.

35:07.000 --> 35:10.520
 I don't know what it is, but the Freudian thing, but there's some kind of connection

35:10.520 --> 35:14.440
 that happens and that's the Cosmo you connect to.

35:14.440 --> 35:15.960
 That's the future Cosmo you connect.

35:15.960 --> 35:21.080
 And that's, so I guess the statement I'm trying to make, is it possible to achieve a depth

35:21.080 --> 35:24.520
 of friendship without solving general intelligence?

35:24.520 --> 35:25.520
 I think so.

35:25.520 --> 35:27.400
 And it's about intelligent kind of constraints, right?

35:27.400 --> 35:32.760
 And just you set expectations and constraints such that in the space that's left, you can

35:32.760 --> 35:33.760
 be successful.

35:33.760 --> 35:37.880
 And so you can do that by having a very focused domain that you can operate in.

35:37.880 --> 35:41.360
 For example, you're a customer support agent for a particular product and you create intelligence

35:41.360 --> 35:43.240
 and a good interface around that.

35:43.240 --> 35:50.520
 Or kind of in the personal companionship side, you can't be everything across the board.

35:50.520 --> 35:51.720
 You kind of solve those constraints.

35:51.720 --> 35:54.040
 And I think it's possible.

35:54.040 --> 36:01.440
 And my worry is right now, I don't see anybody that has picked up on where kind of Cosmo

36:01.440 --> 36:04.280
 left off and is pushing on it in the same way.

36:04.280 --> 36:08.440
 And so I don't know if it's a sort of thing where similar to like how in dot com, there

36:08.440 --> 36:12.960
 were all these concepts that we considered like, that didn't work out or like failed

36:12.960 --> 36:14.600
 or like we're too early or whatnot.

36:14.600 --> 36:18.640
 And then 20 years later, you have these like incredible successes on almost the same concept.

36:18.640 --> 36:22.120
 Like it might be that sort of thing where like there's another pass at it that happens

36:22.120 --> 36:24.160
 in five years or in 10 years.

36:24.160 --> 36:31.040
 But it does feel like that appreciation of that like the three like it's do if you will

36:31.040 --> 36:35.360
 between like, you know, the hardware, the AI and the character that balance.

36:35.360 --> 36:41.160
 It's hard to, I'm not aware of any anywhere right now where like that same kind of aggressive

36:41.160 --> 36:44.400
 drive with the value on the character is happening.

36:44.400 --> 36:50.080
 And so to me, just a prediction, exactly as you said, something that looks awfully a lot

36:50.080 --> 36:55.120
 like Cosmo, not in the actual physical form, but in the three legged stool, something like

36:55.120 --> 36:58.440
 that in some number of years will be a trillion dollar company.

36:58.440 --> 37:07.120
 I don't understand, like it's obvious to me that like character, not just as robotic companions,

37:07.120 --> 37:10.360
 but in all our computers, they'll be there.

37:10.360 --> 37:17.040
 It's like Clippy was like two legs of that stool or something like that.

37:17.040 --> 37:23.640
 I mean, those are all different attempts as what's really confusing to me is they, they're

37:23.640 --> 37:29.200
 born these attempts and they, they, everybody gets excited and for some reason they die

37:29.200 --> 37:34.360
 and then nobody else tries to pick it up and then maybe a few years later, a crazy guy

37:34.360 --> 37:40.640
 like you comes, comes around with just enough brilliance and vision to create this thing

37:40.640 --> 37:45.840
 and is born a lot of people love it, a lot of people get excited, but maybe the timing

37:45.840 --> 37:46.840
 is not right yet.

37:46.840 --> 37:52.120
 And then, and then when the timing is right, it just blows up, it just keeps blowing up

37:52.120 --> 37:57.480
 more and more until it just blows up and I guess everything in the full span of human

37:57.480 --> 37:59.040
 civilization collapses eventually.

37:59.040 --> 38:03.040
 And that wouldn't surprise me at all and like what's going to be different in another five

38:03.040 --> 38:08.600
 years or 10 years or whatnot, physical component costs will continue to come down in price

38:08.600 --> 38:12.560
 and you know, mobile devices and computations are going to become more and more prevalent

38:12.560 --> 38:16.960
 as well as cloud as a, as a big tool to offload cost.

38:16.960 --> 38:22.120
 AI is going to be a massive transformation compared to what we dealt with where everything

38:22.120 --> 38:29.720
 from voice understanding to, to just, you know, kind of a broader contextual understanding

38:29.720 --> 38:35.520
 and mapping of, of semantics and understanding scenes and so forth.

38:35.520 --> 38:38.560
 And then the character side will continue to kind of, you know, progress as well because

38:38.560 --> 38:39.560
 that magic does exist.

38:39.560 --> 38:44.960
 It just exists in different forms and you have just the brilliance of the tapping and

38:44.960 --> 38:50.680
 animation and, you know, these other areas where that is, that was a big unlock in,

38:50.680 --> 38:52.520
 you know, in film obviously.

38:52.520 --> 38:55.440
 And so I think, yeah, the pieces can reconnect and the building blocks are actually going

38:55.440 --> 38:57.840
 to be way more impressive than they were five years ago.

38:57.840 --> 39:08.640
 So 2019, Anki, the company that created Cosmo, the company that you started had to shut down.

39:08.640 --> 39:11.000
 How did you feel at that time?

39:11.000 --> 39:12.920
 Yeah, it was tough.

39:12.920 --> 39:17.920
 That was a really emotional stretch and it was really tough year.

39:17.920 --> 39:24.120
 Like about a year ahead of that was actually a pretty brutal stretch because we were kind

39:24.120 --> 39:30.480
 of life or death on many, many moments, just navigating these insane kind of just ups and

39:30.480 --> 39:32.360
 downs and barriers.

39:32.360 --> 39:38.600
 And the thing that made it like, just reminding a tiny bit like what, you know, what ended

39:38.600 --> 39:44.320
 up being really challenging about it as a business where is from a commercial standpoint

39:44.320 --> 39:47.360
 and customer reception standpoint, there's a lot of things you could point to that were

39:47.360 --> 39:52.640
 like, you know, pretty big successes, so millions of units, like, you know, got to like pretty

39:52.640 --> 39:57.880
 serious revenue, like kind of cost 100 million annual revenue, number one kind of product

39:57.880 --> 40:03.040
 in kind of various categories, but it was pretty expensive end up being very seasonal

40:03.040 --> 40:08.960
 where something like 85% of our volume was in Q4 because it was a, you know, a present

40:08.960 --> 40:13.400
 and it was expensive to market it and explain it and so forth.

40:13.400 --> 40:19.240
 And even though the volume was like really sizable and like reviews are really fantastic,

40:19.240 --> 40:24.160
 forecasting and planning for it and managing the cash operations was just brutal.

40:24.160 --> 40:25.920
 Like it was absolutely brutal.

40:25.920 --> 40:29.280
 You don't think about this when you're starting a company or when you have a few million and

40:29.280 --> 40:33.560
 you know, and revenue because it's just your biggest costs are kind of just your headcount

40:33.560 --> 40:36.040
 and operations and everything's ahead of you.

40:36.040 --> 40:42.320
 But we got to a point where, you know, you, if you look at the entire year, you have to

40:42.320 --> 40:45.840
 operate your company, pay all, you know, the people and so forth, you have to pay for

40:45.840 --> 40:50.080
 the manufacturing, the marketing and everything else to do your sales and mostly November,

40:50.080 --> 40:53.680
 December and then get paid in December, January by retailers.

40:53.680 --> 40:59.320
 And those swings were pretty, were really rough and just made it like so difficult because

40:59.320 --> 41:03.000
 the more it successfully became, the more wild those swings became because you'd have

41:03.000 --> 41:07.280
 to like spend, you know, tens of millions of dollars on inventory, tens of millions

41:07.280 --> 41:11.600
 of dollars on marketing and tens of millions of dollars on payroll and everything else.

41:11.600 --> 41:15.800
 And then the bigger dip and then you're waiting for the Q4 and yeah, and it's not a business

41:15.800 --> 41:19.400
 that like is recurring kind of month to month of predictable and it's just, and then you're

41:19.400 --> 41:25.720
 walking in your forecast in July, you know, maybe August, if you're lucky.

41:25.720 --> 41:30.160
 And it's also like very hit driven and seasonal where like you don't have this sort of continued

41:30.160 --> 41:34.280
 kind of slow growth like you do in some other consumer electronics industries.

41:34.280 --> 41:37.640
 And so before then like hardware kind of like went out of favor too.

41:37.640 --> 41:41.760
 And so you had Fitbit and GoPro drop from 10 billion revenue to 1 billion revenue and hardware

41:41.760 --> 41:46.600
 companies are getting valued at like 1x revenue oftentimes, which is tough, right?

41:46.600 --> 41:51.960
 And so we effectively kind of got caught in the middle where we were trying to quickly

41:51.960 --> 41:56.280
 evolve out of entertainment and move into some other categories, but you can't let go

41:56.280 --> 41:59.080
 of that business because like that's what you're valued on, that's what you're raising

41:59.080 --> 42:00.080
 money on.

42:00.080 --> 42:05.080
 But there was no path to kind of pure profitability just there because it was, you know, such,

42:05.080 --> 42:07.920
 you know, specific type of price points and so forth.

42:07.920 --> 42:14.960
 And so we tried really hard to make that transition and yeah, we had a financing round that fell

42:14.960 --> 42:19.760
 apart at the last second and effectively there was just no path to kind of get through that

42:19.760 --> 42:22.240
 and get to the next kind of high holiday season.

42:22.240 --> 42:26.440
 And so we ended up selling some of the assets and kind of winding down the company.

42:26.440 --> 42:27.440
 It was brutal.

42:27.440 --> 42:32.360
 Like we, I was very transparent with the company like in the team while we were going through

42:32.360 --> 42:36.840
 it where actually despite how challenging that period was, very few people left.

42:36.840 --> 42:40.840
 I mean, like people loved the vision, the team, the culture of the like kind of chemistry

42:40.840 --> 42:41.840
 and kind of what we were doing.

42:41.840 --> 42:43.640
 There was just a huge amount of pride there.

42:43.640 --> 42:47.040
 And then we wanted to see it through and we felt like we had a shot to kind of get through

42:47.040 --> 42:49.720
 these checkpoints.

42:49.720 --> 42:54.760
 We ended up, I mean, by brutal, I mean like literally like days of cash like three, four

42:54.760 --> 43:01.040
 different times runway like in the year, you know, kind of before it where you're like

43:01.040 --> 43:08.440
 playing games of chicken on negotiating credit line timelines and like repayment terms and

43:08.440 --> 43:10.440
 how to get like a bridge loan from an investor.

43:10.440 --> 43:14.200
 It's just like level of stress that like as, as hard as things might be anywhere else,

43:14.200 --> 43:18.000
 like you'll never come, you know, come close to that where you feel that like responsibility

43:18.000 --> 43:21.200
 for, you know, 200 plus people, right?

43:21.200 --> 43:25.320
 And so we were very transparent during our fundraise on who we're talking to, the challenges

43:25.320 --> 43:30.280
 that we have, how it's going and when things are going well, when things were tough.

43:30.280 --> 43:34.120
 And so it wasn't a complete shock when it happened, but it was just very emotional where

43:34.120 --> 43:39.320
 like I, you know, like, you know, when we announced it finally that like, you know, we've, you

43:39.320 --> 43:42.280
 know, basically we're just like watching kind of like, you know, the runway and trying to

43:42.280 --> 43:45.440
 kind of time it and when we realized that like we didn't have any more outs, we wanted

43:45.440 --> 43:49.560
 to like kind of wind it down, make sure that it was like clean and, you know, we could

43:49.560 --> 43:53.080
 like kind of take care of people the best we could, but they're like broke down crying

43:53.080 --> 43:56.920
 at all, you know, hands and so we all had to step in for a bit and like, it was just

43:56.920 --> 44:00.520
 very, very emotional, but the beautiful part is like afterwards, like everybody stayed

44:00.520 --> 44:04.880
 at the office to like two, three in the morning, just like drinking and hanging out and telling

44:04.880 --> 44:09.800
 stories and celebrating and it was just like one of the best for many people was like the

44:09.800 --> 44:14.200
 best kind of work experience that they had and there was a lot of pride in what we did.

44:14.200 --> 44:17.040
 And it wasn't anything obviously we could point to that like, hey, if only we had done

44:17.040 --> 44:19.040
 that different things would have been completely different.

44:19.040 --> 44:27.840
 It was just like the physics didn't line up and but the experience was pretty incredible,

44:27.840 --> 44:28.840
 but it was hard.

44:28.840 --> 44:33.600
 Like it was, it had this feeling that there was just like incredible beauty in both the

44:33.600 --> 44:40.200
 technology and products and the team that, you know, there's, there's a lot there that

44:40.200 --> 44:46.080
 like in the, you know, right context could have been pretty incredible, but it was emotional

44:46.080 --> 44:47.080
 just

44:47.080 --> 44:48.080
 Yeah.

44:48.080 --> 44:52.960
 Just thinking, I mean, just looking at this company, like you said, the product and technology,

44:52.960 --> 44:59.760
 but the vision, the implementation, you got the cost down very low and the compelling,

44:59.760 --> 45:06.440
 the nature of the product was great, so many robotics companies failed at this at the robot

45:06.440 --> 45:07.520
 was too expensive.

45:07.520 --> 45:09.640
 It didn't have the personality.

45:09.640 --> 45:14.400
 It didn't really provide any value, like a sufficient value to justify the price.

45:14.400 --> 45:20.280
 So like you succeeded where basically every single other robotics company or most of them

45:20.280 --> 45:25.720
 that are like going to category of social robotics have kind of failed.

45:25.720 --> 45:29.280
 And I mean, it's, it's, it's quite tragic.

45:29.280 --> 45:34.840
 I remember reading that, I'm not sure if I talked to you before that happened or not,

45:34.840 --> 45:37.680
 but I remember, you know, I'm distant from this.

45:37.680 --> 45:47.640
 I remember being heartbroken reading that because like if, if Cosmo is not going to succeed,

45:47.640 --> 45:49.080
 what is going to succeed?

45:49.080 --> 45:51.680
 Because that to me was incredible.

45:51.680 --> 45:55.280
 Like it was an incredible idea.

45:55.280 --> 45:56.280
 Cost is down.

45:56.280 --> 46:03.440
 It's just like the most minimal design in physical form that you could do.

46:03.440 --> 46:06.440
 It's really compelling, the balance of games.

46:06.440 --> 46:08.160
 So it's a fun toy.

46:08.160 --> 46:12.320
 It's a great gift for all kinds of age groups, right?

46:12.320 --> 46:15.200
 It's just, it's compelling in every single way.

46:15.200 --> 46:22.400
 And it seemed like it was a huge success and it, it failing was, I don't know, there was

46:22.400 --> 46:28.880
 heartbreak on many levels for me just as an external observer is I was thinking, how hard

46:28.880 --> 46:31.440
 is it to run a business?

46:31.440 --> 46:32.440
 That's what I was thinking.

46:32.440 --> 46:38.120
 Like if this failed, this must have failed because it's obviously not like, yeah, it's

46:38.120 --> 46:39.120
 business.

46:39.120 --> 46:40.120
 Yeah.

46:40.120 --> 46:43.240
 Maybe it's some aspect of the manufacturing and so on, but I'm now realizing it's also

46:43.240 --> 46:47.520
 not just that it's sales, marketing, all those.

46:47.520 --> 46:48.520
 Oh, it's everything, right?

46:48.520 --> 46:51.480
 Like, how do you explain something that's like a new category to people that like how

46:51.480 --> 46:56.360
 all these previous positions and so like, you know, it had some of the hardest elements

46:56.360 --> 47:01.840
 of if you were to pick a business, it had some of the hardest customer dynamics because

47:01.840 --> 47:07.960
 like to sell a $150 product, you got to convince both the child wanted and the parents to agree

47:07.960 --> 47:08.960
 that it's valuable.

47:08.960 --> 47:11.440
 So you're having like this dual prong marketing challenge.

47:11.440 --> 47:12.440
 You have manufacturing.

47:12.440 --> 47:16.200
 You have like really high precision on the components that you need to have the AI challenges.

47:16.200 --> 47:20.880
 So there were a lot of tough elements, but is this feeling where like just really great

47:20.880 --> 47:24.880
 alignment of unique strength across kind of like all these different areas, just like

47:24.880 --> 47:29.360
 incredible like, you know, kind of character and animation team between this like Carlos

47:29.360 --> 47:32.720
 and there's like a character director day that came on board and like really great people

47:32.720 --> 47:33.720
 there.

47:33.720 --> 47:41.160
 The AI side, the manufacturing, the, you know, where like never missing a launch, right?

47:41.160 --> 47:45.560
 And actually, you know, he kind of hidden that quality was, yeah, it was heartbreaking,

47:45.560 --> 47:51.480
 but here's one neat thing is like we had so much like fan mail from kind of kids and parents

47:51.480 --> 47:56.760
 like, I actually like there was a bunch that collected in the end that I actually saved

47:56.760 --> 48:00.320
 and like I never, it was too emotional to open it and I still haven't opened it.

48:00.320 --> 48:03.960
 And so I actually have this giant envelope of like a stack this much of like letters

48:03.960 --> 48:09.160
 from, you know, kids and families, just like every, you know, presentation you can imagine.

48:09.160 --> 48:12.760
 And so planning to kind of, I don't know, maybe like a five year, you know, five year

48:12.760 --> 48:16.560
 to eight some year reunion, just inviting everybody over and we'll just like kind of

48:16.560 --> 48:19.000
 dig into it and kind of bring back some memories.

48:19.000 --> 48:21.240
 But you know, good impact.

48:21.240 --> 48:28.160
 And well, I think there will be companies, maybe Waymo and Google will be somehow involved

48:28.160 --> 48:33.880
 that will carry this flag forward and will will make you proud whether you're involved

48:33.880 --> 48:34.880
 or not.

48:34.880 --> 48:39.800
 I think this is one of the greatest robotics companies in the history of robotics.

48:39.800 --> 48:45.880
 So you should be proud, it's still tragic to know that, you know, because you read all

48:45.880 --> 48:55.080
 the stories of Apple and, let's see, SpaceX and like companies that were just on the verge

48:55.080 --> 48:59.400
 of failure several times through that story and they just, it's almost like a role of

48:59.400 --> 49:01.080
 the dice they succeeded.

49:01.080 --> 49:03.920
 And here's a role of the dice that just happened to go.

49:03.920 --> 49:08.360
 And that's the appreciation that like when you really like talk to a lot of the founders,

49:08.360 --> 49:12.880
 like everybody goes through those moments and sometimes it really is a matter of like,

49:12.880 --> 49:16.720
 you know, timing a little bit of luck, like some things are just out of your control.

49:16.720 --> 49:24.280
 And you get a much deeper appreciation for just the dimensionality of that challenge.

49:24.280 --> 49:27.920
 But the great thing is, is that like a lot of the team actually like stayed together.

49:27.920 --> 49:32.680
 And so there were actually, you know, a couple of companies that we kind of kept big chunks

49:32.680 --> 49:37.120
 of the team together and we actually kind of helped align this, you know, to help people

49:37.120 --> 49:38.440
 out as well.

49:38.440 --> 49:44.880
 And one of them was Waymo where a majority of the AI and robotics team actually had the

49:44.880 --> 49:49.080
 exact background that you would look for in like kind of a V space and it was a space

49:49.080 --> 49:53.800
 that a lot of us like, you know, worked on in grad school were always passionate about

49:53.800 --> 49:59.160
 and ended up, you know, maybe the time, you know, serendipitous timings from another perspective

49:59.160 --> 50:04.640
 where like kind of landed in a really unique circumstance that's actually been quite exciting

50:04.640 --> 50:05.640
 too.

50:05.640 --> 50:13.800
 So it's interesting to ask you just your thoughts, Cosmo still lives on under Dream Labs, I think.

50:13.800 --> 50:19.520
 Is that, are you tracking the progress there or is it too much pain?

50:19.520 --> 50:24.160
 Is it, are you, is that something that you're excited to see where that goes?

50:24.160 --> 50:27.880
 So keeping an eye on it, of course, just out of your curiosity and obviously just kind

50:27.880 --> 50:33.600
 of care for product line, I think it's deceptive how complex it is to manufacture and evolve

50:33.600 --> 50:42.160
 that product line and the amount of experiences that are required to complete the picture

50:42.160 --> 50:43.440
 and be able to move that forward.

50:43.440 --> 50:48.160
 And I think that's going to make it pretty hard to do something really substantial with

50:48.160 --> 50:49.160
 it.

50:49.160 --> 50:51.880
 It would be cool if like even the product in the way it was was able to be manufactured.

50:51.880 --> 50:52.880
 Yes.

50:52.880 --> 50:53.880
 You know, again, that would.

50:53.880 --> 50:54.880
 Which is the current goal, I suppose.

50:54.880 --> 50:55.880
 Yeah.

50:55.880 --> 50:56.880
 Which would be neat.

50:56.880 --> 51:01.880
 But it's, I think it was deceptive how tricky that is on like everything from the quality

51:01.880 --> 51:07.880
 control, the details and, and then like technology changes that forces you to re reinvent and

51:07.880 --> 51:09.760
 update certain things.

51:09.760 --> 51:13.560
 So I haven't been super close to it, but just kind of keeping an eye on it.

51:13.560 --> 51:14.560
 Yeah.

51:14.560 --> 51:20.480
 It's really interesting how it's deceptively difficult, just as you're saying, for example,

51:20.480 --> 51:28.640
 those same folks, and I spoke with them there, they partnered up with Rick and Morty creators

51:28.640 --> 51:30.360
 to do the Butter Robot.

51:30.360 --> 51:31.360
 Yeah.

51:31.360 --> 51:32.360
 That was the idea.

51:32.360 --> 51:37.600
 I just recently, I kind of half fast watched Rick and Morty previously, but now I just

51:37.600 --> 51:39.440
 watched like the first season.

51:39.440 --> 51:41.360
 It's such a brilliant show.

51:41.360 --> 51:45.400
 I'd like, I did not understand how brilliant that show is.

51:45.400 --> 51:50.560
 And obviously I think in season one is where the Butter Robot comes along for just a few

51:50.560 --> 51:53.960
 minutes or whatever, but I just fell in love with the Butter Robot.

51:53.960 --> 51:58.360
 The sort of the, that particular character, just like you said, there's characters you

51:58.360 --> 52:04.400
 can create personality, you can create in that particular robot who's doing a particular

52:04.400 --> 52:12.760
 task realizes, you know, this, like realize, ask the existential question, the myth of

52:12.760 --> 52:17.880
 Sisyphus question that Camus writes about is like, is this all there is?

52:17.880 --> 52:19.800
 Is he moves butter?

52:19.800 --> 52:25.960
 But you know, that realization, that's a beautiful little realization for a robot that on my

52:25.960 --> 52:29.560
 purpose is very limited to this particular task.

52:29.560 --> 52:33.320
 It's a beautiful, it's humor, of course, it's darkness, it's a beautiful mix.

52:33.320 --> 52:41.880
 But so they want to release that Butter Robot, but something tells me that to do the same

52:41.880 --> 52:48.040
 depth of personality as Cosmo had the same richness, it would be on the manufacturing,

52:48.040 --> 52:53.400
 on the AI, on the storytelling, on the design, it's going to be very, very difficult.

52:53.400 --> 53:00.840
 It could be a cool sort of a toy for Rick and Morty fans, but to create the same depth

53:00.840 --> 53:09.680
 of existential angst that the Butter Robot symbolizes is really, that's the brave effort

53:09.680 --> 53:13.400
 you've succeeded at with Cosmo, but it's not easy.

53:13.400 --> 53:15.120
 It's really difficult.

53:15.120 --> 53:19.960
 You can fail on almost any one of the kind of dimensions and like, and yeah, it takes,

53:19.960 --> 53:24.400
 you know, yeah, unique convergence of a lot of different skill sets to try to pull that

53:24.400 --> 53:25.400
 off.

53:25.400 --> 53:26.400
 Yeah.

53:26.400 --> 53:31.640
 On this topic, let me ask you for some advice, because as I've been watching Rick and Morty,

53:31.640 --> 53:36.360
 as I told myself, I have to build the Butter Robot, just as a hobby project.

53:36.360 --> 53:40.840
 And so I got a nice platform for it with treads and there's a camera that moves up and down

53:40.840 --> 53:43.000
 and so on.

53:43.000 --> 53:49.400
 I'll probably paint it, but the question I'd like to ask, there's obvious technical

53:49.400 --> 53:53.600
 questions I'm fine with, communication, the personality, storytelling, all those kinds

53:53.600 --> 53:55.480
 of things.

53:55.480 --> 54:02.840
 I think I understand the process of that, but how do you know when you got it right?

54:02.840 --> 54:10.080
 So with Cosmo, how did you know this is great or something is off?

54:10.080 --> 54:12.440
 Is this brainstorming with the team?

54:12.440 --> 54:13.680
 Do you know it when you see it?

54:13.680 --> 54:17.440
 Is it like, love at first sight, it's like, this is right?

54:17.440 --> 54:23.560
 Or like, I guess if we think of it as an optimization space, is there Uncanny Valley?

54:23.560 --> 54:28.240
 We're like, that's not right, or this is right, or are a lot of characters right?

54:28.240 --> 54:29.240
 Yeah.

54:29.240 --> 54:34.400
 We stayed away from Uncanny Valley just by having such a different mapping where it didn't

54:34.400 --> 54:37.240
 try to look like a dog or a human or anything like that.

54:37.240 --> 54:44.280
 And so you avoided having like a weird pseudo similarity, but not quite hitting the mark.

54:44.280 --> 54:48.240
 But you could like just fall flat where just like a personality or a character emotion

54:48.240 --> 54:49.560
 just didn't feel right.

54:49.560 --> 54:52.400
 And so it actually mirrored very closely to kind of the iterations that a character

54:52.400 --> 54:57.880
 director at Pixar would have, where you're running through it and you can virtually kind

54:57.880 --> 55:00.160
 of like see what it'll look like.

55:00.160 --> 55:05.640
 We created a plugin to where we actually used like Maya, the animation tools, and then we

55:05.640 --> 55:10.200
 created a plugin that perfectly matched it to the physical one.

55:10.200 --> 55:14.360
 So you could like test it out virtually and then push a button and see it physically play

55:14.360 --> 55:15.360
 out.

55:15.360 --> 55:16.360
 And there's like subtle differences.

55:16.360 --> 55:19.440
 And so you want to like make sure that that feedback loop is super easy to be able to

55:19.440 --> 55:21.240
 test it live.

55:21.240 --> 55:26.880
 And then sometimes like, you would just feel it that it's right and intuitively know.

55:26.880 --> 55:29.480
 And then you'd also do, we did user testing.

55:29.480 --> 55:35.080
 But it was very, very often that like the into, like if we found it magical, it would

55:35.080 --> 55:37.520
 scale and be magical more broadly.

55:37.520 --> 55:42.000
 There was not too many cases where like, like we were pretty decent about not like getting

55:42.000 --> 55:45.760
 to it, you know, geeking out or getting too attached to something that was super unique

55:45.760 --> 55:51.160
 to us, but trying to kind of like, you know, put a customer hat on and does it truly kind

55:51.160 --> 55:52.160
 of feel magical?

55:52.160 --> 55:57.760
 And so in a lot of ways, we just give a lot of autonomy to the character team to really

55:57.760 --> 56:02.720
 think about the, you know, character board and mood boards and storyboards and like, what's

56:02.720 --> 56:05.840
 the background of this character and how would they react?

56:05.840 --> 56:09.240
 And they went through a process that's actually pretty familiar, but now had to operate under

56:09.240 --> 56:11.400
 these unique constraints.

56:11.400 --> 56:17.120
 But the moment where it felt right, kind of took a fairly similar journey than like as

56:17.120 --> 56:19.440
 a character in an animated film, actually, it's quite cool.

56:19.440 --> 56:23.360
 Well, the thing that's really important to me, and I wonder if it's possible, well,

56:23.360 --> 56:29.680
 I hope it's possible, pretty sure it's possible is for me, even though I know how it works,

56:29.680 --> 56:35.760
 to make sure there's sufficient randomness in the process, probably because it would

56:35.760 --> 56:42.720
 be machine learning based that I'm surprised that I don't, I'm surprised by certain reactions,

56:42.720 --> 56:44.320
 surprised by certain communication.

56:44.320 --> 56:50.400
 Maybe that's in a form of a question, were you surprised by certain things Cosmo did,

56:50.400 --> 56:51.880
 like certain interactions?

56:51.880 --> 56:58.720
 Yeah, we made it intentionally, like, so that there would be some surprise than like a decent

56:58.720 --> 57:02.280
 amount of variability and how he'd respond in certain circumstances.

57:02.280 --> 57:10.280
 And so in the end, like, this isn't generally I, this is a giant like spectrum and library

57:10.280 --> 57:14.160
 of like parameterized kind of emotional responses and an emotional engine that would like kind

57:14.160 --> 57:19.680
 of map your current state of the game, your emotions, the world that people were playing

57:19.680 --> 57:22.720
 with you all so forth to what's happening.

57:22.720 --> 57:29.360
 But we could make it feel spontaneous by creating enough diversity and randomness, but still

57:29.360 --> 57:33.520
 within the bounds of what felt felt like very realistic to make that work.

57:33.520 --> 57:36.760
 And then what was really neat is that we could get statistics on how much of that space we

57:36.760 --> 57:40.920
 were saturating, and then add more animations and more diversity in the places that would

57:40.920 --> 57:46.640
 get hit more often, so that you stay ahead of the, you know, the curve and maximize the

57:46.640 --> 57:49.200
 chance that it stays feeling alive.

57:49.200 --> 57:54.440
 And so, but then when you like combine it like the permutations and kind of like the

57:54.440 --> 57:58.320
 combinations of emotions stitched together, sometimes surprised us because you see them

57:58.320 --> 57:59.320
 in isolation.

57:59.320 --> 58:03.080
 But when you actually see them and you see them live, you know, relative to some event

58:03.080 --> 58:06.240
 that happened in the game or whatnot, like, it was kind of cool to see the combination

58:06.240 --> 58:07.240
 of the two.

58:07.240 --> 58:11.440
 And it's not too different in other robotics applications where like you get so used to

58:11.440 --> 58:16.160
 thinking about like the modules of a system and how things progress through a tech stack

58:16.160 --> 58:20.840
 that the real magic is when all the pieces come together and you start getting the right

58:20.840 --> 58:25.120
 emergent behavior in a way that's easy to lose when you just kind of go too deep into

58:25.120 --> 58:26.120
 any one piece of it.

58:26.120 --> 58:29.560
 Yeah, when the system is sufficiently complex, there is something like emergent behavior

58:29.560 --> 58:30.560
 and that's where the magic is.

58:30.560 --> 58:34.720
 As a human being, you can still appreciate the beauty of that magic of the final at the

58:34.720 --> 58:35.720
 system level.

58:35.720 --> 58:38.240
 First of all, thank you for humoring me on this.

58:38.240 --> 58:40.440
 It's really, really fascinating.

58:40.440 --> 58:41.960
 I think a lot of people would love this.

58:41.960 --> 58:46.280
 I love to just, one last thing on the butter robot, I promise.

58:46.280 --> 58:55.640
 In terms of speech, Cosmo is able to communicate so much with just movement and face.

58:55.640 --> 59:01.880
 Do you think speech is too much of a degree of freedom?

59:01.880 --> 59:09.840
 Like a speech, a feature or a bug of deep interaction or emotional interaction?

59:09.840 --> 59:11.080
 Yeah.

59:11.080 --> 59:13.560
 For a product, it's too deep right now.

59:13.560 --> 59:15.080
 It's just not real.

59:15.080 --> 59:19.720
 You would immediately break the fiction because the state of the art is just not good enough.

59:19.720 --> 59:24.280
 And that's on top of just narrowing down the demographic where like the way you speak to

59:24.280 --> 59:29.040
 an adult versus a way you speak to a child is very different, yet a dog is able to appeal

59:29.040 --> 59:30.760
 to everybody.

59:30.760 --> 59:38.920
 And so right now, there is no speech system that is rich enough and subtly realistic enough

59:38.920 --> 59:40.960
 to feel appropriate.

59:40.960 --> 59:43.320
 And so we very, very quickly moved away from it.

59:43.320 --> 59:48.080
 Now, speech understanding is a different matter where understanding intent, that's a really

59:48.080 --> 59:55.920
 valuable input, but giving it back requires like a way, way higher bar given kind of where

59:55.920 --> 59:57.640
 today's world is.

59:57.640 --> 1:00:04.520
 And so that realization that you can do surprisingly much with either no speech or kind of tonal

1:00:04.520 --> 1:00:10.800
 like the way Wally R2D2 and kind of other characters are able to, it's quite powerful

1:00:10.800 --> 1:00:15.040
 and it generalizes across cultures and across ages really, really well.

1:00:15.040 --> 1:00:19.520
 I think we're going to be in that world for a little while where it's still very much

1:00:19.520 --> 1:00:22.480
 an unsolved problem on how to like make something.

1:00:22.480 --> 1:00:23.880
 It touches on any value thing.

1:00:23.880 --> 1:00:28.080
 So if you have legs and you're a big humanoid looking thing, you have very different expectations

1:00:28.080 --> 1:00:32.080
 and a much narrower degree of what's going to be acceptable by society.

1:00:32.080 --> 1:00:38.480
 Then if you're a robot like Cosmore Wally or some other form where you can kind of reinvent

1:00:38.480 --> 1:00:44.160
 the character, speech has that same property where speech is so well understood in terms

1:00:44.160 --> 1:00:48.960
 of expectations by humans that you have far less flexibility on how to deviate from that

1:00:48.960 --> 1:00:52.120
 and lean into your strengths and avoid weaknesses.

1:00:52.120 --> 1:00:58.720
 But I wonder if there is, obviously there is certain kinds of speech that activates

1:00:58.720 --> 1:01:01.880
 the uncanny valley and breaks the illusion faster.

1:01:01.880 --> 1:01:10.300
 So I guess my intuition is we will solve certain, we would be able to create some speech based

1:01:10.300 --> 1:01:13.440
 personalities sooner than others.

1:01:13.440 --> 1:01:21.360
 So for example, I could think of a robot that doesn't know English and is learning English.

1:01:21.360 --> 1:01:22.360
 Those kinds of personalities.

1:01:22.360 --> 1:01:27.760
 A fiction where you're intentionally kind of like getting a toddler level of speech.

1:01:27.760 --> 1:01:28.760
 So that's exactly right.

1:01:28.760 --> 1:01:34.640
 So you can't have like tied into the experience where it is a more limited character or you

1:01:34.640 --> 1:01:40.640
 embrace the lack of emotions or the lack of dynamic range in the speech kind of capabilities

1:01:40.640 --> 1:01:43.920
 and emotions as like part of the character itself and you've seen that in like kind of

1:01:43.920 --> 1:01:46.320
 fictional characters as well.

1:01:46.320 --> 1:01:48.520
 But that's why this podcast works.

1:01:48.520 --> 1:01:53.960
 And you kind of had that with like, I don't know, I guess like data and some of the other

1:01:53.960 --> 1:01:54.960
 ones.

1:01:54.960 --> 1:01:55.960
 Yeah, exactly.

1:01:55.960 --> 1:02:01.080
 But yeah, so you have to, and that becomes a constraint that lets you meet the bar.

1:02:01.080 --> 1:02:12.120
 See, I honestly think like also if you add drunk and angry, that gives you more constraints

1:02:12.120 --> 1:02:17.320
 that allow you to be done more from an NLP perspective, like there's certain aspects.

1:02:17.320 --> 1:02:22.320
 So if you modify human behavior, like, so forget the sort of artificial thing where

1:02:22.320 --> 1:02:28.880
 you don't know English toddler thing, we, if you just look at the full range of humans,

1:02:28.880 --> 1:02:38.480
 I think we, there's certain situations where we put up with like lower level of intelligence

1:02:38.480 --> 1:02:40.000
 in our communication.

1:02:40.000 --> 1:02:43.400
 Like if somebody's drunk, we understand the situation that they're probably under the

1:02:43.400 --> 1:02:44.400
 influence.

1:02:44.400 --> 1:02:48.040
 Like we understand that they're not going to be making any sense, anger is another one

1:02:48.040 --> 1:02:49.040
 like that.

1:02:49.040 --> 1:02:54.400
 I'm sure there's a lot of other kind of situations like this, maybe, yeah, again, language, loss

1:02:54.400 --> 1:03:01.840
 in translation, that kind of stuff that I think if you play with that, what is it, the

1:03:01.840 --> 1:03:05.160
 Ukrainian boy that passed the touring test, you know, I'll play with those ideas.

1:03:05.160 --> 1:03:08.640
 I think that's really interesting that you can create compelling characters, but you're

1:03:08.640 --> 1:03:13.200
 right, that's a dangerous sort of road to walk because you're adding degrees of freedom

1:03:13.200 --> 1:03:14.200
 that can get you in trouble.

1:03:14.200 --> 1:03:19.440
 Yeah, and that's why like you have these big pushes that like for most of the last decade

1:03:19.440 --> 1:03:25.040
 plus like where you'd have like full like human replicas of robots really being down

1:03:25.040 --> 1:03:33.280
 to like skin and like kind of in some places, my personal feeling is like, man, like, that's

1:03:33.280 --> 1:03:37.480
 not the direction that's most fruitful right now.

1:03:37.480 --> 1:03:38.480
 Beautiful art.

1:03:38.480 --> 1:03:39.480
 Yeah.

1:03:39.480 --> 1:03:43.000
 It's not in terms of a rich, deep, fulfilling experience.

1:03:43.000 --> 1:03:44.000
 Yeah, you're right.

1:03:44.000 --> 1:03:45.000
 Yeah.

1:03:45.000 --> 1:03:51.480
 You're creating a minefield of potential places to feel off and then your side stepping where

1:03:51.480 --> 1:03:55.800
 like the biggest kind of functional AI challenges are to actually have, you know, kind of like

1:03:55.800 --> 1:03:59.760
 really rich productivity that actually kind of justifies a, you know, kind of the higher

1:03:59.760 --> 1:04:00.760
 price points.

1:04:00.760 --> 1:04:03.400
 And that's part of the challenge is like, yeah, like robots are going to get to like

1:04:03.400 --> 1:04:06.840
 thousands of dollars, tens of thousands of dollars and so forth, but you can imagine

1:04:06.840 --> 1:04:10.160
 what sort of expectation of value that comes with it.

1:04:10.160 --> 1:04:15.840
 And so that's where you want to be able to invest the time and depth.

1:04:15.840 --> 1:04:25.480
 And so going down the full human replica route creates a gigantic distraction and really,

1:04:25.480 --> 1:04:30.960
 really high bar that can end up sucking up so much of your resources.

1:04:30.960 --> 1:04:36.880
 So it's weird to say, but you happen to be one of the greatest at this point roboticist

1:04:36.880 --> 1:04:44.120
 ever because you created this little guy, your part obviously of a great team that created

1:04:44.120 --> 1:04:52.040
 the little guy with a deep personality and are now switching to an entirely, well, maybe

1:04:52.040 --> 1:04:58.880
 not entirely, but a different, fascinating, impactful robotics problem, which is autonomous

1:04:58.880 --> 1:05:03.680
 driving and more specifically the biggest version of autonomous driving, which is autonomous

1:05:03.680 --> 1:05:04.920
 trucking.

1:05:04.920 --> 1:05:07.420
 So you are at Waymo now.

1:05:07.420 --> 1:05:10.800
 Can you give us a big picture overview?

1:05:10.800 --> 1:05:12.160
 What is Waymo?

1:05:12.160 --> 1:05:13.640
 What is Waymo Driver?

1:05:13.640 --> 1:05:14.640
 What is Waymo One?

1:05:14.640 --> 1:05:17.160
 What is Waymo Via?

1:05:17.160 --> 1:05:20.600
 Can you give an overview of the company and the vision behind the company?

1:05:20.600 --> 1:05:21.600
 For sure.

1:05:21.600 --> 1:05:25.680
 Waymo, by the way, it's just, it's been eyeopening on just how incredible that the people and

1:05:25.680 --> 1:05:30.800
 the talent is and how in one company you almost have to create, I don't know, 30 companies

1:05:30.800 --> 1:05:34.760
 worth of like technology and capability to like kind of solve the full spectrum of it.

1:05:34.760 --> 1:05:40.760
 So yeah, so I've been at Waymo since 2019, it's about two and a half years.

1:05:40.760 --> 1:05:47.560
 So Waymo is focused on building what we call a driver, which is creating the ability to

1:05:47.560 --> 1:05:54.200
 have autonomous driving across different environments, vehicle platforms, domains and use cases.

1:05:54.200 --> 1:06:00.580
 You know, as you know, it got started in 2009, it was almost like an immediate successor

1:06:00.580 --> 1:06:05.800
 to the grand challenge and urban challenges that were like incredible kind of catalysts

1:06:05.800 --> 1:06:07.720
 for this whole space.

1:06:07.720 --> 1:06:10.640
 And so Google started this project and then eventually Waymo spun out.

1:06:10.640 --> 1:06:18.320
 And so what Waymo is doing is creating the systems, both hardware, software, infrastructure,

1:06:18.320 --> 1:06:22.680
 everything that goes into it to enable and to commercialize autonomous driving.

1:06:22.680 --> 1:06:28.920
 This hits on consumer transportation and ride sharing and kind of vehicles and urban environments.

1:06:28.920 --> 1:06:34.560
 And as you mentioned, it hits on autonomous trucking to transport goods in a lot of ways

1:06:34.560 --> 1:06:37.200
 it's transporting people and transporting goods.

1:06:37.200 --> 1:06:42.160
 But at the end of the day, the underlying capabilities are required to do that are surprisingly

1:06:42.160 --> 1:06:48.680
 better aligned than one might expect, where it's the fundamentals of being able to understand

1:06:48.680 --> 1:06:53.760
 the world around you, process it, make intelligent decisions and prove that we are at a level

1:06:53.760 --> 1:06:56.720
 of safety that enables large scale autonomy.

1:06:56.720 --> 1:07:02.840
 So from a branding perspective, sort of Waymo driver is the system that's irrespective

1:07:02.840 --> 1:07:07.920
 of a particular vehicle it's operating into.

1:07:07.920 --> 1:07:13.200
 You have a set of sensors that perceive the world can act in that world and move this

1:07:13.200 --> 1:07:15.200
 whatever the vehicle is through the world.

1:07:15.200 --> 1:07:16.200
 That's right.

1:07:16.200 --> 1:07:19.200
 And so in the same way that you have a driver's license and like your ability to drive is

1:07:19.200 --> 1:07:21.720
 tied to a particular make a model of a car.

1:07:21.720 --> 1:07:25.120
 And of course there's special licenses for other types of vehicles, but the fundamentals

1:07:25.120 --> 1:07:29.880
 of a human driver very, very large to carry over and then there's uniqueness is related

1:07:29.880 --> 1:07:35.400
 to a particular environment or domain or a particular vehicle type that kind of add

1:07:35.400 --> 1:07:37.480
 some extra additive challenges.

1:07:37.480 --> 1:07:38.680
 But that's exactly right.

1:07:38.680 --> 1:07:47.360
 It's the underlying systems and enable a physical vehicle without a human driver to very successfully

1:07:47.360 --> 1:07:54.840
 accomplish a task that previously wasn't possible without 100% human driving.

1:07:54.840 --> 1:08:01.680
 And then there's Waymo one, which is the transporting people from a brand perspective.

1:08:01.680 --> 1:08:04.320
 And just in case we refer to it, so people know.

1:08:04.320 --> 1:08:07.840
 And then there's Waymo via, which is the trucking component.

1:08:07.840 --> 1:08:08.840
 Why via by the way?

1:08:08.840 --> 1:08:09.840
 What is that?

1:08:09.840 --> 1:08:10.840
 What is that?

1:08:10.840 --> 1:08:11.840
 What is it?

1:08:11.840 --> 1:08:17.480
 Just like a cool sounding name that just, is there an interesting story there?

1:08:17.480 --> 1:08:18.680
 It is a pretty cool sounding name.

1:08:18.680 --> 1:08:19.680
 It's a cool sounding name.

1:08:19.680 --> 1:08:22.640
 I mean, when you think about it, it's just like, well, we're going to transport it via

1:08:22.640 --> 1:08:23.640
 this and that.

1:08:23.640 --> 1:08:24.640
 Oh cool.

1:08:24.640 --> 1:08:28.520
 It's kind of like an allusion to the mechanics of transporting something.

1:08:28.520 --> 1:08:29.520
 Yes.

1:08:29.520 --> 1:08:30.520
 Cool.

1:08:30.520 --> 1:08:31.520
 And it is a pretty good grouping.

1:08:31.520 --> 1:08:34.520
 And the interesting thing is that even the groupings kind of bore where Waymo one is

1:08:34.520 --> 1:08:39.120
 like human transportation and there's a fully autonomous service and the Phoenix area that

1:08:39.120 --> 1:08:41.000
 like every day is transporting people.

1:08:41.000 --> 1:08:45.560
 And it's pretty incredible to like just see that operate at reasonably large scale and

1:08:45.560 --> 1:08:46.560
 just kind of happen.

1:08:46.560 --> 1:08:53.160
 And then on the via side, it doesn't even have to be like long haul trucking is a major

1:08:53.160 --> 1:08:58.680
 focus of ours, but down the road, you can stitch together the vehicle transportation

1:08:58.680 --> 1:09:00.880
 as well for local delivery.

1:09:00.880 --> 1:09:05.120
 Also in a lot of the requirements for local delivery overlap very heavily with consumer

1:09:05.120 --> 1:09:09.600
 transportation, obviously, you know, given that you're operating on a lot of the same

1:09:09.600 --> 1:09:14.320
 roads and navigating the same safety challenges.

1:09:14.320 --> 1:09:23.000
 And so, yeah, and Waymo very much is a multi product company that has ambitions in both.

1:09:23.000 --> 1:09:26.360
 They have different challenges and both are tremendous opportunities.

1:09:26.360 --> 1:09:31.160
 But the cool thing is, is that there's a huge amount of leverage and this kind of core technology

1:09:31.160 --> 1:09:34.520
 stack now gets pushed on by both sides.

1:09:34.520 --> 1:09:36.640
 And that adds its own unique challenges.

1:09:36.640 --> 1:09:42.240
 But the success case is that the challenges that you push on, they get leveraged across

1:09:42.240 --> 1:09:43.800
 all platforms and all domains.

1:09:43.800 --> 1:09:47.240
 So from an engineer perspective, the teams are integrated.

1:09:47.240 --> 1:09:48.240
 It's a mix.

1:09:48.240 --> 1:09:52.280
 So there's a huge amount of centralized kind of core teams that support all applications.

1:09:52.280 --> 1:09:55.960
 So you think of something like the hardware team that develops the lasers, the compute

1:09:55.960 --> 1:09:57.880
 integrates into vehicle platforms.

1:09:57.880 --> 1:10:01.800
 This is an experience that carries over across, you know, any application that we have in

1:10:01.800 --> 1:10:03.680
 a ebb and flow with both.

1:10:03.680 --> 1:10:08.920
 Then there's like really unique perception challenges, planning challenges, like other

1:10:08.920 --> 1:10:12.600
 you know, types of challenges where there's a huge amount of leverage on a core tech stack.

1:10:12.600 --> 1:10:16.080
 But then there's like dedicated teams that think of how do you deal with a unique challenge?

1:10:16.080 --> 1:10:21.360
 For example, an articulated trailer with varying loads that completely changes the physical

1:10:21.360 --> 1:10:26.200
 dynamics of a vehicle that doesn't exist on a car, but becomes one of the most important

1:10:26.200 --> 1:10:28.480
 kind of unique new challenges on a truck.

1:10:28.480 --> 1:10:36.280
 So what's the long term dream of Waymo via the autonomous trucking effort that Waymo

1:10:36.280 --> 1:10:37.280
 is doing?

1:10:37.280 --> 1:10:38.280
 Yeah.

1:10:38.280 --> 1:10:43.480
 So we're starting with developing L4 autonomy for classic trucks.

1:10:43.480 --> 1:10:48.360
 These are 53 foot trailers that capture like a big, a pretty sizable percentage of the

1:10:48.360 --> 1:10:51.040
 goose transportation in the country.

1:10:51.040 --> 1:10:56.520
 Long term, the opportunity is obviously to expand a much more diverse types of vehicles,

1:10:56.520 --> 1:11:01.560
 types of good transportation, and start to really expand in both the volume and the route

1:11:01.560 --> 1:11:03.400
 feasibility that's possible.

1:11:03.400 --> 1:11:09.400
 And so just like we did on the car side, you start with a single route with a very specific

1:11:09.400 --> 1:11:14.160
 operating kind of domain and constraints that allow you to solve the problem.

1:11:14.160 --> 1:11:19.560
 But then over time, you start to really try to push against those boundaries and open

1:11:19.560 --> 1:11:25.840
 up deeper feasibility across routes, across surface streets, across environmental conditions,

1:11:25.840 --> 1:11:30.000
 across the type of goods that you carry, the versatility of those goods, and how little

1:11:30.000 --> 1:11:33.840
 supervision is necessary to just start to scale this network.

1:11:33.840 --> 1:11:40.840
 And long term, there's actually, it's a pretty incredible enabler where today you have already

1:11:40.840 --> 1:11:42.760
 a giant shortage of truck drivers.

1:11:42.760 --> 1:11:47.560
 It's over 80,000 truck driver shortage that's expected to grow to hundreds of thousands in

1:11:47.560 --> 1:11:48.840
 the years ahead.

1:11:48.840 --> 1:11:54.440
 You have really, really quickly increasing demand from eCommerce and just distribution

1:11:54.440 --> 1:11:57.800
 of where people are located.

1:11:57.800 --> 1:12:04.640
 You have one of the deepest safety challenges of any profession in the US where there's

1:12:04.640 --> 1:12:10.560
 a huge, huge, huge kind of challenge around fatigue and around kind of the long routes

1:12:10.560 --> 1:12:11.560
 that are driven.

1:12:11.560 --> 1:12:16.040
 And even beyond kind of the cost and necessity of it, there are fundamental constraints built

1:12:16.040 --> 1:12:22.520
 into our logistics network that are tied to the type of human constraints and regulatory

1:12:22.520 --> 1:12:24.960
 constraints that are tied to trucking today.

1:12:24.960 --> 1:12:31.560
 For example, our limits on how long a driver can be driving in a single day before they're

1:12:31.560 --> 1:12:35.520
 not allowed to drive anymore, which is a very important safety constraint.

1:12:35.520 --> 1:12:40.040
 What that does is it enforces limitations on how far jumps with a single driver it could

1:12:40.040 --> 1:12:45.480
 be and makes you very subject to availability of drivers, which influences where warehouses

1:12:45.480 --> 1:12:49.200
 are built, which influences how goods are transported, which influences costs.

1:12:49.200 --> 1:12:55.240
 And so you start to have an opportunity on everything from plugging into existing fleets

1:12:55.240 --> 1:13:00.160
 and brokerages and the existing logistics network and just immediately start to have

1:13:00.160 --> 1:13:09.080
 a huge opportunity to add value from a cost and driving fuel insurance and safety standpoint

1:13:09.080 --> 1:13:14.040
 all the way to completely reinventing the logistics network across the United States

1:13:14.040 --> 1:13:16.480
 and enabling something completely different than what it looks like today.

1:13:16.480 --> 1:13:20.880
 Yeah, I had a republished before this had a great conversation with Steve Vichelli,

1:13:20.880 --> 1:13:23.400
 who we talked about the manual driving.

1:13:23.400 --> 1:13:27.280
 He echoed many of the same things that you were talking about, but we talked about much

1:13:27.280 --> 1:13:31.560
 of the fascinating human stories of truck drivers.

1:13:31.560 --> 1:13:36.680
 He was also was a truck driver for a bit as a grass thin to try to understand the depth

1:13:36.680 --> 1:13:37.680
 of the problem.

1:13:37.680 --> 1:13:38.680
 Fascinating lives.

1:13:38.680 --> 1:13:42.480
 We have some drivers that have four million miles of lifetime driving experience.

1:13:42.480 --> 1:13:48.480
 It's pretty incredible and yeah, it's learning from them like some of them are on the road

1:13:48.480 --> 1:13:49.880
 for 300 days a year.

1:13:49.880 --> 1:13:51.760
 It's a very unique type of lifestyle.

1:13:51.760 --> 1:13:53.360
 So there's fascinating stuff there.

1:13:53.360 --> 1:13:59.800
 Just like you said, there's a shortage of actually people, truck drivers taking the job counter

1:13:59.800 --> 1:14:04.320
 to what I think is publicly believed.

1:14:04.320 --> 1:14:08.760
 So there's an excess of jobs and a shortage of people to take up those jobs.

1:14:08.760 --> 1:14:14.160
 And just like you said, it's such a difficult problem and these are experts at driving it

1:14:14.160 --> 1:14:19.880
 solving this particular problem and it's fascinating to learn from them to understand how hard

1:14:19.880 --> 1:14:20.880
 is this problem.

1:14:20.880 --> 1:14:25.880
 And that's the question I want to ask you from a perception from a robotics perspective.

1:14:25.880 --> 1:14:29.840
 What's your sense of how difficult is autonomous trucking?

1:14:29.840 --> 1:14:35.720
 Maybe you can comment on which scenarios are super difficult, which are more manageable?

1:14:35.720 --> 1:14:40.920
 Is there a way to kind of convert into words how difficult the problem is?

1:14:40.920 --> 1:14:44.120
 Yeah, that's a good question.

1:14:44.120 --> 1:14:45.840
 And as you can expect, it's a mix.

1:14:45.840 --> 1:14:52.840
 Some things become a lot easier or at least more flexible.

1:14:52.840 --> 1:14:54.120
 Some things are harder.

1:14:54.120 --> 1:15:02.000
 And so on the things that are like the tailwinds, the benefits, a big focus of automating trucking,

1:15:02.000 --> 1:15:05.920
 especially initially, is really focusing on the long haul freeway stretch of it, where

1:15:05.920 --> 1:15:08.080
 that's where a majority of the value is captured.

1:15:08.080 --> 1:15:12.160
 On a freeway, you have a lot more structure and a lot more consistency across freeways

1:15:12.160 --> 1:15:18.360
 across the US compared to surface streets where you have a way higher dimensionality

1:15:18.360 --> 1:15:23.240
 of what can happen, lack of structure, lack of consistency and variability across cities.

1:15:23.240 --> 1:15:29.560
 So you can leverage that consistency to tackle, at least in that respect, a more constrained

1:15:29.560 --> 1:15:32.760
 AI problem, which has some benefits to it.

1:15:32.760 --> 1:15:35.480
 You can itemize much more of the sort of things you might encounter and so forth.

1:15:35.480 --> 1:15:37.080
 And so those are benefits.

1:15:37.080 --> 1:15:42.880
 Is there a canonical freeway and city we should be thinking about?

1:15:42.880 --> 1:15:46.720
 Is there a standard thing that's brought up in conversation often?

1:15:46.720 --> 1:15:50.040
 Here's a stretch of road.

1:15:50.040 --> 1:15:51.040
 What is it?

1:15:51.040 --> 1:15:57.720
 When people talk about traveling across country, they'll talk about New York to San Francisco.

1:15:57.720 --> 1:15:59.240
 Is that the route?

1:15:59.240 --> 1:16:05.720
 Is there a stretch of road that's nice and clean, and then there's cities with difficulties

1:16:05.720 --> 1:16:10.360
 in them that you kind of think of as the canonical problems that solve here?

1:16:10.360 --> 1:16:16.280
 So starting with the car side, Waymo very intentionally picked the Phoenix area and

1:16:16.280 --> 1:16:21.520
 the San Francisco area as a follow once we had driverless, where when you think of consumer

1:16:21.520 --> 1:16:26.720
 transportation and ride sharing kind of economy, a big percentage of that market is captured

1:16:26.720 --> 1:16:28.560
 in the densest cities in the United States.

1:16:28.560 --> 1:16:32.960
 And so really pushing out and solving San Francisco becomes a really huge opportunity

1:16:32.960 --> 1:16:40.400
 and importance and places one dot on kind of like the spectrum of like kind of complexity.

1:16:40.400 --> 1:16:43.560
 The Phoenix area, starting with Chandler and then like kind of expanding more broadly in

1:16:43.560 --> 1:16:48.680
 the Phoenix metropolitan area, it's I believe the fastest growing city in the U.S.

1:16:48.680 --> 1:16:54.600
 It's a kind of a higher medium sized city, but growing quickly and still captures a really

1:16:54.600 --> 1:16:56.520
 wide range of kind of like complexities.

1:16:56.520 --> 1:17:00.160
 And so getting to driverless there actually exposes you to a lot of the building blocks

1:17:00.160 --> 1:17:03.200
 you need for the more complicated environments.

1:17:03.200 --> 1:17:06.960
 And so in a lot of ways, there's a thesis that if you start to kind of place a few of

1:17:06.960 --> 1:17:11.640
 these kind of dots where San Francisco has these types of unique challenges, dense pedestrians,

1:17:11.640 --> 1:17:15.240
 all this like complexity, especially when you get into the downtown areas and so forth.

1:17:15.240 --> 1:17:19.120
 And Phoenix has like a really interesting kind of spectrum of challenges.

1:17:19.120 --> 1:17:23.360
 Maybe you know, other ones like LA kind of add freeway focus and so forth.

1:17:23.360 --> 1:17:27.640
 You start to kind of cover the full set of features that you might expect and it becomes

1:17:27.640 --> 1:17:32.520
 faster and faster if you have the right systems and the right organization to then open up

1:17:32.520 --> 1:17:35.000
 the fifth city and the tenth city and the twentieth city.

1:17:35.000 --> 1:17:40.480
 On trucking, there is a similar properties where obviously there's uniqueness is in freeways

1:17:40.480 --> 1:17:45.240
 when you get into really dense environments and then the real opportunity to then, you

1:17:45.240 --> 1:17:49.680
 know, get even more value is to think about how you expand with like some of the surface

1:17:49.680 --> 1:17:54.120
 street challenges, but for example, right now, we're looking, we have a big facility that

1:17:54.120 --> 1:17:58.600
 we're finishing building in Q1 in Dallas area.

1:17:58.600 --> 1:18:02.640
 That'll allow us to do testing from the Dallas area on routes like Dallas to Houston, Dallas

1:18:02.640 --> 1:18:10.800
 to Phoenix, going out east and Austin, so that triangle Waymo should come to Austin.

1:18:10.800 --> 1:18:13.960
 Well, Waymo, the car side wasn't Austin for a while.

1:18:13.960 --> 1:18:14.960
 Yes, I know.

1:18:14.960 --> 1:18:15.960
 Come back.

1:18:15.960 --> 1:18:16.960
 Yeah.

1:18:16.960 --> 1:18:21.080
 Actually, Texas is one of the best places to start because of both volume, regulatory

1:18:21.080 --> 1:18:23.440
 weather, there's a lot of benefits.

1:18:23.440 --> 1:18:27.280
 On trucking, a huge opportunity is Port of LA going east.

1:18:27.280 --> 1:18:32.160
 So in a lot of ways, a lot of the work is to start this dish together and network and

1:18:32.160 --> 1:18:37.800
 converge to Port of LA where you have the biggest port in the United States.

1:18:37.800 --> 1:18:41.040
 And the amount of goods going east from there is pretty tremendous.

1:18:41.040 --> 1:18:44.800
 And then obviously there's, you know, kind of channels everywhere and then you have

1:18:44.800 --> 1:18:48.520
 extra complexities as you get into like snow and inclement weather and so forth.

1:18:48.520 --> 1:18:53.240
 But what's interesting about trucking is every single route segment that you add increases

1:18:53.240 --> 1:18:54.440
 the value of the whole network.

1:18:54.440 --> 1:18:57.800
 And so it has this kind of network effect and cumulative effect that's very unique.

1:18:57.800 --> 1:19:00.480
 And so there's all these dimensions that we think about.

1:19:00.480 --> 1:19:04.080
 And so in a lot of ways, Dallas is a really unique hub that opens up a lot of options

1:19:04.080 --> 1:19:06.080
 has become a really valuable lever.

1:19:06.080 --> 1:19:11.720
 So the million questions I could ask you, first of all, you mentioned level four.

1:19:11.720 --> 1:19:17.760
 For people who totally don't know, there's these levels of automation that level four

1:19:17.760 --> 1:19:24.600
 refers to kind of the first step that you could recognize as fully autonomous driving.

1:19:24.600 --> 1:19:29.400
 Level five is really fully autonomous driving and level four is kind of fully autonomous

1:19:29.400 --> 1:19:30.400
 driving.

1:19:30.400 --> 1:19:34.800
 And then there are specific definitions depending on who you ask what that actually means.

1:19:34.800 --> 1:19:38.280
 But for you, what does the level four mean?

1:19:38.280 --> 1:19:43.320
 And you mentioned freeway, let's say like there's three parts of long haul trucking.

1:19:43.320 --> 1:19:46.960
 Maybe I'm wrong in this, but there's freeway driving.

1:19:46.960 --> 1:19:49.920
 There's like truck stop.

1:19:49.920 --> 1:19:54.040
 And then there's more urban type of area.

1:19:54.040 --> 1:19:57.760
 So which of those do you want to tackle?

1:19:57.760 --> 1:20:00.600
 Which of them do you include under level four?

1:20:00.600 --> 1:20:01.800
 How do you think about this problem?

1:20:01.800 --> 1:20:02.800
 What do you focus on?

1:20:02.800 --> 1:20:05.960
 Where's the biggest impact to be had in the short term?

1:20:05.960 --> 1:20:09.840
 So the goal is to, we got to get to market as fast as we can because the moment you get

1:20:09.840 --> 1:20:13.680
 to market, you just learn so much and it influences everything that you do.

1:20:13.680 --> 1:20:20.400
 And it is, one of the experiences I carried over from before is that you add constraints,

1:20:20.400 --> 1:20:24.280
 you figure out the right compromises, you do whatever it takes because getting to market

1:20:24.280 --> 1:20:25.280
 is so critical.

1:20:25.280 --> 1:20:29.680
 But here with autonomous driving, you can get to market in so many different ways.

1:20:29.680 --> 1:20:34.480
 And so one of the simplifications that we intentionally have put on is using what we

1:20:34.480 --> 1:20:41.600
 call transfer hubs, where you can imagine depots that are at the entry points to metropolitan

1:20:41.600 --> 1:20:46.800
 areas, like let's say Dallas, like the hub that we're building, which does a few things

1:20:46.800 --> 1:20:47.800
 that are very valuable.

1:20:47.800 --> 1:20:52.600
 So from a first product standpoint, you can automate transfer hub to transfer hub.

1:20:52.600 --> 1:20:59.360
 And that path from the transfer hub to the full freeway route can be a very intentional

1:20:59.360 --> 1:21:03.400
 single route that you can select for the features that you feel you want to handle at that point

1:21:03.400 --> 1:21:04.400
 in time.

1:21:04.400 --> 1:21:08.600
 And you build a hub specifically designed for autonomous trucking.

1:21:08.600 --> 1:21:11.720
 And that's what's going to happen actually, like you need to come out in January and check

1:21:11.720 --> 1:21:13.040
 it out because it's going to be really cool.

1:21:13.040 --> 1:21:19.160
 It's the, not only is it our main operating headquarters for our fleet there, but it will

1:21:19.160 --> 1:21:24.240
 be the first fully ground up design driverless hub for autonomous, driverless autonomous

1:21:24.240 --> 1:21:27.920
 trucks in terms of where do they enter, where do they depart, how do you think about the

1:21:27.920 --> 1:21:31.520
 flow of people, goods, everything, it's like, it's quite cool and it's really beautiful

1:21:31.520 --> 1:21:32.920
 on how it's thought through.

1:21:32.920 --> 1:21:38.840
 And so early on, it is totally reasonable to do the last five miles manually to get

1:21:38.840 --> 1:21:42.640
 to the final kind of depot to avoid having to solve the general surface street problem,

1:21:42.640 --> 1:21:44.040
 which is obviously very complex.

1:21:44.040 --> 1:21:49.600
 Now when the time comes, and we are increasingly, already we're pushing on some of this, but

1:21:49.600 --> 1:21:53.520
 we will increasingly be pushing on surface street capabilities to build out the value

1:21:53.520 --> 1:21:57.520
 chain to go all the way depot to depot instead of transfer hub to transfer hub.

1:21:57.520 --> 1:22:00.680
 And we have probably the best advantages in the world because of all the Waymo experience

1:22:00.680 --> 1:22:02.000
 on surface streets.

1:22:02.000 --> 1:22:06.040
 But that's not the highest ROI right now, where the highest ROI is hub to hub and get

1:22:06.040 --> 1:22:07.160
 the routes going.

1:22:07.160 --> 1:22:13.360
 And so when you ask what's L4, L4 can be applied to any domain operating domain or scope, but

1:22:13.360 --> 1:22:17.760
 it's effectively for the places where we say we're ready for autonomous operation.

1:22:17.760 --> 1:22:27.600
 We are 100% operating through the as a self driving truck with no human behind the wheel.

1:22:27.600 --> 1:22:28.760
 That is L4 autonomy.

1:22:28.760 --> 1:22:30.760
 And it doesn't mean that you operate in every condition.

1:22:30.760 --> 1:22:32.720
 It doesn't mean you operate on every road.

1:22:32.720 --> 1:22:39.600
 But for a particularly well defined area, operating conditions, routes kind of domain,

1:22:39.600 --> 1:22:40.600
 you are fully autonomous.

1:22:40.600 --> 1:22:42.200
 And that's the difference between L4 and L5.

1:22:42.200 --> 1:22:45.800
 And most people would agree that at least anytime in the foreseeable future, L5 is just

1:22:45.800 --> 1:22:50.640
 not even really worth thinking about because there's always going to be these extremes.

1:22:50.640 --> 1:22:55.880
 And so it's a race and almost like a game where you think of what is the sequence of

1:22:55.880 --> 1:23:00.960
 expanded capabilities that create the most value and teach us the most and create this

1:23:00.960 --> 1:23:05.760
 feedback loop where we're building out and unlocking more and more capability over time.

1:23:05.760 --> 1:23:07.960
 I got to ask you just curious.

1:23:07.960 --> 1:23:12.480
 So first of all, I have to, when I'm allowed to visit the Dallas facility, because it's

1:23:12.480 --> 1:23:13.480
 super cool.

1:23:13.480 --> 1:23:20.120
 It's like robot on the giving and the receiving end is the truck is a robot and the hub is

1:23:20.120 --> 1:23:21.120
 a robot.

1:23:21.120 --> 1:23:22.520
 Yeah, it's got to be very robot friendly.

1:23:22.520 --> 1:23:27.040
 That's great, I will feel at home.

1:23:27.040 --> 1:23:28.840
 What's the sensor suite like on the hub?

1:23:28.840 --> 1:23:35.280
 If you can just high level mention it, does the hub have like lidars?

1:23:35.280 --> 1:23:39.920
 Is the truck doing most of the intelligence or is the hub also intelligent?

1:23:39.920 --> 1:23:44.120
 Yeah, so most of it will be the truck and everything is like connected.

1:23:44.120 --> 1:23:48.960
 So we have our servers where we know exactly where every truck is, we know exactly what's

1:23:48.960 --> 1:23:49.960
 happening at a hub.

1:23:49.960 --> 1:23:55.560
 And so you can imagine like a large backend system that over time starts to manage timings,

1:23:55.560 --> 1:23:58.640
 goods, delivery, windows, all these sort of things.

1:23:58.640 --> 1:24:04.720
 And so you don't actually need to, there might be special cases where that is valuable to

1:24:04.720 --> 1:24:08.040
 equip some sensors in the hub, but a majority of the intelligence is going to be on the

1:24:08.040 --> 1:24:13.800
 truck because whatever is relevant to the truck, relevant should be seen by the truck

1:24:13.800 --> 1:24:19.480
 and can be relayed remotely for any sort of kind of cognizance or decision making.

1:24:19.480 --> 1:24:24.400
 But there's a distinct type of workflow where, where do you check trucks, where do you want

1:24:24.400 --> 1:24:28.840
 them to enter, what if there's many operating at once, where's the staging area to depart,

1:24:28.840 --> 1:24:34.680
 how do you set up the flow of humans and human cars and traffic so that you minimize the

1:24:34.680 --> 1:24:38.680
 interaction between humans and kind of self driving trucks.

1:24:38.680 --> 1:24:42.400
 And then how do you even intelligently select the locations of these transfer hubs that are

1:24:42.400 --> 1:24:46.640
 both really great service locations for a metropolitan area, and there could be over

1:24:46.640 --> 1:24:53.040
 time, many of them for a metropolitan area, while at the same time leaning into the path

1:24:53.040 --> 1:24:57.440
 of least resistance to lean into your current capabilities and strengths so that you minimize

1:24:57.440 --> 1:25:00.960
 the amount of work that's necessary to unlock the next kind of big bar.

1:25:00.960 --> 1:25:02.360
 I have a million questions.

1:25:02.360 --> 1:25:06.200
 So first, is the goal to have no human in the truck?

1:25:06.200 --> 1:25:08.360
 The goal is to have no human in the truck.

1:25:08.360 --> 1:25:12.840
 Now, of course, right now we're testing with expert operators and so forth, but the goal

1:25:12.840 --> 1:25:18.880
 is to, now there might be circumstances where it makes sense to have a human or, and obviously

1:25:18.880 --> 1:25:20.560
 these trucks can also be manually driven.

1:25:20.560 --> 1:25:27.400
 So sometimes like our, we talk with our fleet partners about how you can buy way mill equipped

1:25:27.400 --> 1:25:31.720
 die more truck down the road and on the routes that are autonomous, it's autonomous on the

1:25:31.720 --> 1:25:34.520
 routes that are not, it's human driven.

1:25:34.520 --> 1:25:37.880
 Maybe there's all two functionality that add safety systems and so forth.

1:25:37.880 --> 1:25:42.880
 But as soon as they become, as soon as we expand in software, the availability of driverless

1:25:42.880 --> 1:25:47.840
 routes, the hardware is forward compatible to just now start using them in real time.

1:25:47.840 --> 1:25:54.120
 And so you can imagine this mixed use, but at the end of the day, the largest value proposition

1:25:54.120 --> 1:25:59.080
 is where you're able to have no constraints on how you can operate this truck and it's

1:25:59.080 --> 1:26:01.640
 100% autonomous with nobody inside.

1:26:01.640 --> 1:26:02.640
 That's amazing.

1:26:02.640 --> 1:26:09.000
 So let me ask on the logistics front, because you mentioned that also opportunity to revamp

1:26:09.000 --> 1:26:12.840
 or for built from scratch, some of the ideas around logistics.

1:26:12.840 --> 1:26:17.360
 I don't want to throw too much shade, but from talking to Steve, my understanding is

1:26:17.360 --> 1:26:23.640
 logistics is not perhaps as great as it could be in the current trucking environment.

1:26:23.640 --> 1:26:28.600
 I'm not, maybe you can break down why, but there's probably competing companies.

1:26:28.600 --> 1:26:29.680
 There's just the mess.

1:26:29.680 --> 1:26:32.800
 Maybe some of it is literally just, it's old school.

1:26:32.800 --> 1:26:40.200
 Like it's just like, it's not computerized, like truckers are almost like contractors.

1:26:40.200 --> 1:26:43.880
 There's an independence and there's not a nice interface where they can communicate

1:26:43.880 --> 1:26:47.200
 where they're going, where they're at, all those kinds of things.

1:26:47.200 --> 1:26:51.840
 And so it just feels like there's so much opportunity to digitize everything to where

1:26:51.840 --> 1:26:58.400
 you could optimize the use of human time, optimize the use of all kinds of resources.

1:26:58.400 --> 1:27:00.000
 How much are you thinking about that problem?

1:27:00.000 --> 1:27:02.640
 How fascinating is that problem?

1:27:02.640 --> 1:27:07.400
 How difficult does it, how much opportunity is there to revolutionize the space of logistics

1:27:07.400 --> 1:27:09.840
 in autonomous trucking, in trucking period?

1:27:09.840 --> 1:27:10.840
 It's pretty fascinating.

1:27:10.840 --> 1:27:14.840
 It's, this is one of the most motivating aspects of all this where like, yes, there's

1:27:14.840 --> 1:27:18.120
 like a mountain of problems that are like, you want to, you have to solve to get to like

1:27:18.120 --> 1:27:20.920
 the first checkpoints and first driver list and so forth.

1:27:20.920 --> 1:27:25.360
 And inevitably, like in a space like this, you plug in initially into the existing kind

1:27:25.360 --> 1:27:29.200
 of system and start to kind of, you know, learn and iterate, but that opportunity is

1:27:29.200 --> 1:27:30.200
 massive.

1:27:30.200 --> 1:27:32.120
 And so, you know, a couple of the factors that play into it.

1:27:32.120 --> 1:27:37.680
 So first of all, there's obviously just the physical constraints of driving time, driver

1:27:37.680 --> 1:27:39.080
 availability.

1:27:39.080 --> 1:27:44.560
 Some fleets have a 95% attrition rate, you know, right now because of just this demands

1:27:44.560 --> 1:27:48.160
 and like, you know, kind of gaps in competition and so forth.

1:27:48.160 --> 1:27:52.600
 And then it's also incredibly fragmented where you would be shocked at like, when you, when

1:27:52.600 --> 1:27:56.080
 you look at industries, like, and you think of the top 10 players, like the biggest fleets,

1:27:56.080 --> 1:28:00.680
 like the Walmart's and FedEx's and so forth, the percentage of the overall trucking market

1:28:00.680 --> 1:28:04.880
 that's captured by the top 10 or 50 fleets is surprisingly small.

1:28:04.880 --> 1:28:11.520
 The average kind of truck operation is like a one to five truck, you know, family business.

1:28:11.520 --> 1:28:17.280
 And so, and so there's just like a huge amount of like fragmentation, which makes for really

1:28:17.280 --> 1:28:22.640
 interesting challenges in kind of stitching together through like bolt and boards and brokerages

1:28:22.640 --> 1:28:27.920
 and some people run their own fleets and, and this world's kind of like evolving.

1:28:27.920 --> 1:28:34.840
 But it is one of the less digitized and optimized worlds that there is.

1:28:34.840 --> 1:28:38.960
 And the part that is optimized is optimized to the constraints of today.

1:28:38.960 --> 1:28:43.440
 And even within the constraints of today, this is the $900 billion industry in the US

1:28:43.440 --> 1:28:44.440
 and it's continuing to grow.

1:28:44.440 --> 1:28:50.160
 It feels like from a business perspective, if I were to predict that while trying to

1:28:50.160 --> 1:28:55.760
 solve the autonomous trucking problem, Waymo might solve first the logistics problem, like

1:28:55.760 --> 1:28:58.560
 because that, that would already be a huge impact.

1:28:58.560 --> 1:28:59.560
 Yeah.

1:28:59.560 --> 1:29:05.240
 So on the way to solving autonomous trucking, the human driven, like there's so much opportunity

1:29:05.240 --> 1:29:12.480
 to significantly improve the human driven trucking, the timing, the logistics.

1:29:12.480 --> 1:29:13.880
 So you use humans optimal.

1:29:13.880 --> 1:29:19.120
 The handoffs, the like, you know, well, even you get really ambitious, you start to expand

1:29:19.120 --> 1:29:23.200
 as beyond like, how does the fulfillment center work and like, how does the transfer

1:29:23.200 --> 1:29:24.200
 hub work?

1:29:24.200 --> 1:29:27.560
 How does the warehouse work to, I mean, there's a lot of opportunities to start to automate

1:29:27.560 --> 1:29:28.760
 these chains.

1:29:28.760 --> 1:29:34.480
 And a lot of the inefficiency today is because like you have a delay, like Port of LA has

1:29:34.480 --> 1:29:38.400
 a bunch of ships right now waiting outside of it because they can't dock because there's

1:29:38.400 --> 1:29:41.720
 not enough labor inside of the Port of LA.

1:29:41.720 --> 1:29:45.040
 That means there's a big backlog of trucks, which means there's a big backlog of deliveries,

1:29:45.040 --> 1:29:46.920
 which means the drivers aren't where they need to be.

1:29:46.920 --> 1:29:51.560
 And so you have this like huge chain reaction and your feasibility of readjusting in this

1:29:51.560 --> 1:29:58.120
 network is low because everything's tied to humans and manual kind of processes or distributed

1:29:58.120 --> 1:30:00.680
 processes across a whole bunch of players.

1:30:00.680 --> 1:30:05.480
 And so one of the biggest enablers is, yes, we have to solve autonomous trucking first.

1:30:05.480 --> 1:30:07.520
 And that, by the way, that's not like an overnight thing.

1:30:07.520 --> 1:30:13.200
 It's decades of continued kind of expansion and work, but the first checkpoint in the

1:30:13.200 --> 1:30:16.200
 first route is like, is not that far off.

1:30:16.200 --> 1:30:21.200
 But once you start enabling and you start to learn about how the constraints of autonomous

1:30:21.200 --> 1:30:25.600
 trucking, which are very, very different than the constraints of human trucking and again,

1:30:25.600 --> 1:30:32.480
 strengths and weaknesses, how do you then start to leverage that and rethink a flow

1:30:32.480 --> 1:30:34.880
 of goods more broadly?

1:30:34.880 --> 1:30:39.080
 And this is where the learnings of really partnering with some of the largest fleets

1:30:39.080 --> 1:30:43.880
 in the US and the sort of learnings that they have about the industry and the sort of needs

1:30:43.880 --> 1:30:44.880
 that they have.

1:30:44.880 --> 1:30:49.880
 And what would change if you just really broke this one constraint that holds up the whole

1:30:49.880 --> 1:30:50.880
 network?

1:30:50.880 --> 1:30:53.200
 Or what if you enabled this other constraint?

1:30:53.200 --> 1:30:56.960
 That actually drives the roadmap in a lot of ways because this is not like an all or

1:30:56.960 --> 1:30:58.720
 nothing problem.

1:30:58.720 --> 1:31:02.880
 You start to kind of unlock more and more functionality over time, which functionality

1:31:02.880 --> 1:31:07.120
 most enables this optimization ends up being kind of part of the discussion.

1:31:07.120 --> 1:31:08.320
 But you're totally right.

1:31:08.320 --> 1:31:16.680
 You fast forward to like five years, 10 years, 15 years, and you think about very generalized

1:31:16.680 --> 1:31:21.960
 capability of automation and logistics as well as the ability to poke into how those

1:31:21.960 --> 1:31:23.520
 handoffs work.

1:31:23.520 --> 1:31:28.520
 The efficiency goes far beyond just direct cost of today's like unit economics of a truck.

1:31:28.520 --> 1:31:34.920
 They go toward reinventing the entire system in the same way that you see these other industries

1:31:34.920 --> 1:31:39.840
 that like when you get to enough scale, you can really rethink how you build around your

1:31:39.840 --> 1:31:43.080
 new set of capabilities, not the old set of capabilities.

1:31:43.080 --> 1:31:44.080
 Yeah.

1:31:44.080 --> 1:31:48.840
 Use the analogy metaphor or whatever that autonomous trucking is like email versus mail.

1:31:48.840 --> 1:31:54.080
 And then with email, you're still doing the communication, but it opens up all kinds of

1:31:54.080 --> 1:31:57.440
 varieties of communication that you didn't anticipate.

1:31:57.440 --> 1:31:58.440
 That's right.

1:31:58.440 --> 1:31:59.440
 Constraints are just completely different.

1:31:59.440 --> 1:32:03.080
 And yeah, there's definitely a property of that here.

1:32:03.080 --> 1:32:08.720
 And we're also still learning about it because there is a lot of really fascinating and sometimes

1:32:08.720 --> 1:32:11.680
 really elegant things that the industry has done where there's companies whose entire

1:32:11.680 --> 1:32:16.520
 existence is around despite the constraints, optimizing as much as they can out of it.

1:32:16.520 --> 1:32:20.640
 And those lessons do carry over, but it's an interesting kind of merger of worlds to

1:32:20.640 --> 1:32:24.080
 think about like, well, what if this was completely different?

1:32:24.080 --> 1:32:25.080
 How would we approach it?

1:32:25.080 --> 1:32:30.760
 And the interesting thing is that for a really, really, really long time, it's actually going

1:32:30.760 --> 1:32:34.920
 to be the merger between how to use autonomy and how to use humans that leans into each

1:32:34.920 --> 1:32:35.920
 of their strengths.

1:32:35.920 --> 1:32:36.920
 Yeah.

1:32:36.920 --> 1:32:40.520
 And then we're back to Cosmo, human robot interaction.

1:32:40.520 --> 1:32:45.760
 So the interesting thing about Waymo is because there's the passenger vehicle, the transportation

1:32:45.760 --> 1:32:51.360
 of humans and transportation of goods, you could see over time they might kind of meld

1:32:51.360 --> 1:32:56.920
 together more because you'll probably have like zero occupancy vehicles moving around.

1:32:56.920 --> 1:33:02.400
 You have transportation of goods for short distances and then for slightly longer distances

1:33:02.400 --> 1:33:05.880
 and then slightly longer and then there'll be this, then you just see the difference

1:33:05.880 --> 1:33:10.640
 between a passenger vehicle and a truck is just size and you could have different sizes

1:33:10.640 --> 1:33:14.560
 and all that kind of stuff and at the core, you can have a Waymo driver that doesn't,

1:33:14.560 --> 1:33:17.560
 as long as you have the same sense of suite, you can just think of it as one problem.

1:33:17.560 --> 1:33:21.760
 And that's why over time, these do kind of converge where in a lot of ways, a lot of

1:33:21.760 --> 1:33:25.120
 the challenges we're solving are freeway driving, which are going to carry over very

1:33:25.120 --> 1:33:28.560
 well to the vehicles, to the car side.

1:33:28.560 --> 1:33:33.160
 But there are like then unique challenges like you have a very different dynamics in

1:33:33.160 --> 1:33:37.800
 your vehicle where you have to see much further out in order to have the proper response time

1:33:37.800 --> 1:33:41.320
 because you have an 80,000 pound fully loaded truck.

1:33:41.320 --> 1:33:44.840
 That's a very, very different type of braking profile than a car.

1:33:44.840 --> 1:33:51.320
 You have really interesting kind of dynamic limits because of the trailer where you actually,

1:33:51.320 --> 1:33:55.920
 it's very, very hard to like physically like flip a car or do something like physically,

1:33:55.920 --> 1:33:59.200
 like most risk in a car is from just collisions.

1:33:59.200 --> 1:34:03.320
 It's very hard to like in any normal operation to do something other than like, unless you

1:34:03.320 --> 1:34:07.320
 hit something to actually kind of like roll over something on a truck, you actually have

1:34:07.320 --> 1:34:12.000
 to drive much closer to the physical bounds of the safety limits, but you actually have

1:34:12.000 --> 1:34:18.680
 like real constraints because you could have really interesting interactions between the

1:34:18.680 --> 1:34:20.200
 cabin and the trailer.

1:34:20.200 --> 1:34:24.960
 There's something called jackknifeing if you turn too quickly, you have roll risks and

1:34:24.960 --> 1:34:25.960
 so forth.

1:34:25.960 --> 1:34:28.200
 And so we spend a huge amount of time understanding those boundaries.

1:34:28.200 --> 1:34:32.080
 And those boundaries change based on the load that you have, which is also an interesting

1:34:32.080 --> 1:34:36.080
 difference and you have to propagate that through the algorithm so that you're leveraging

1:34:36.080 --> 1:34:40.560
 your dynamic range, but always staying within a safety balance, but understanding what those

1:34:40.560 --> 1:34:42.200
 safety bounds are.

1:34:42.200 --> 1:34:46.480
 We have this like really cool test facility where we like take it to the max and actually

1:34:46.480 --> 1:34:50.760
 imagine a truck with these giant training wheels on the back of the trailer and you're

1:34:50.760 --> 1:34:55.960
 pushing it past the safety limits in order to like try to actually see where it rolls.

1:34:55.960 --> 1:35:00.720
 And so you define this high dimensional boundary, which then gets captured in software to stay

1:35:00.720 --> 1:35:02.160
 safe and actually do the right thing.

1:35:02.160 --> 1:35:07.080
 But it's kind of fascinating, the sort of challenges you have there, but then all of

1:35:07.080 --> 1:35:11.960
 these things drive really interesting challenges from perception to unique behavior prediction

1:35:11.960 --> 1:35:16.920
 challenges and obviously in planar where you have to think about merging and creating

1:35:16.920 --> 1:35:20.080
 gaps with a 53 foot trailer and so forth.

1:35:20.080 --> 1:35:22.120
 And then obviously the platform itself is very different.

1:35:22.120 --> 1:35:26.000
 We have different numbers of sensors, sometimes types of sensors, and you also have unique

1:35:26.000 --> 1:35:28.760
 blind spots that you have because of the trailer, which you have to think about.

1:35:28.760 --> 1:35:30.880
 And so it's a really interesting spectrum.

1:35:30.880 --> 1:35:37.680
 And in the end, you try to capture these special cases in a way that is cleanly augmentations

1:35:37.680 --> 1:35:43.840
 of the existing tech stack because a majority of what we're solving is actually generalizable,

1:35:43.840 --> 1:35:47.000
 the freeway driving and different platforms.

1:35:47.000 --> 1:35:52.200
 And over time, they all start to kind of merge ideally where the things that are unique are

1:35:52.200 --> 1:35:54.880
 as minimal as possible.

1:35:54.880 --> 1:35:56.320
 And that's where you get the most leverage.

1:35:56.320 --> 1:36:04.520
 And that's why Waymo can take on $2 trillion opportunities and be nowhere near 2x the cost

1:36:04.520 --> 1:36:05.520
 or investment or size.

1:36:05.520 --> 1:36:10.480
 In fact, it's much, much smaller than that because of the high degree of leverage.

1:36:10.480 --> 1:36:17.800
 So what kind of sensors they can speak to that a long haul truck needs to have, lidar,

1:36:17.800 --> 1:36:21.200
 vision, how many, what are we talking about here?

1:36:21.200 --> 1:36:22.200
 Yeah.

1:36:22.200 --> 1:36:23.200
 So it's more than a car.

1:36:23.200 --> 1:36:27.880
 So very loose, you can think of as like 2x, but it varies depending on the sensor.

1:36:27.880 --> 1:36:33.240
 And so we have dozens of cameras, radar, and then multiple lidar as well.

1:36:33.240 --> 1:36:38.200
 You'll see one difference where the cars have a central main sensor pod on the roof in the

1:36:38.200 --> 1:36:42.120
 middle and then some kind of hood sensors for blind spots.

1:36:42.120 --> 1:36:46.040
 The truck moves to two main sensor pods on the outsides where you would typically have

1:36:46.040 --> 1:36:49.040
 the mirrors next to the driver.

1:36:49.040 --> 1:36:54.680
 The effect of he goes far out as possible, kind of up to the drivers.

1:36:54.680 --> 1:36:58.240
 Kind of on the cabin, not all the way in the front, but like kind of where the mirrors

1:36:58.240 --> 1:36:59.600
 for the driver would be.

1:36:59.600 --> 1:37:02.760
 And so those are the main sensor pods and the reason they're there is because if you

1:37:02.760 --> 1:37:06.400
 had one in the middle, the trailer is higher than the cabin and you would be occluded

1:37:06.400 --> 1:37:08.160
 with this like awkward wedge.

1:37:08.160 --> 1:37:09.160
 Too much occlusion.

1:37:09.160 --> 1:37:10.160
 Too much occlusion.

1:37:10.160 --> 1:37:13.960
 And so then you would add a lot of complexity to the software to make up for that and just

1:37:13.960 --> 1:37:14.960
 unnecessary complexity.

1:37:14.960 --> 1:37:17.880
 There's so many probably fascinating design churches.

1:37:17.880 --> 1:37:18.880
 Really cool.

1:37:18.880 --> 1:37:21.520
 You could probably bring up a ladder higher and have it in the center or something.

1:37:21.520 --> 1:37:26.120
 You could have all kinds of choices to make the decisions here that ultimately probably

1:37:26.120 --> 1:37:27.560
 will define the industry.

1:37:27.560 --> 1:37:28.560
 Right.

1:37:28.560 --> 1:37:30.560
 But by having two on the side, there's actually multiple benefits.

1:37:30.560 --> 1:37:34.240
 So one is like you're just beyond the trailer.

1:37:34.240 --> 1:37:37.840
 So you can see fully flush with the trailer and so you eliminate most of your blind spot

1:37:37.840 --> 1:37:42.120
 except for right behind the trailer, which is great because now the software carries

1:37:42.120 --> 1:37:43.360
 over really well.

1:37:43.360 --> 1:37:46.840
 And the same perception system you use on the car side, largely that architecture can

1:37:46.840 --> 1:37:51.240
 carry over and you can retrain some models and so forth that you leverage it a lot.

1:37:51.240 --> 1:37:55.760
 It also actually helps with redundancy where there's a really nice built in redundancy

1:37:55.760 --> 1:38:00.200
 for all the LiDAR cameras and radar where you can afford to have any one of them fail

1:38:00.200 --> 1:38:01.920
 and you're still okay.

1:38:01.920 --> 1:38:03.920
 And at scale, every one of them will fail.

1:38:03.920 --> 1:38:04.920
 Right.

1:38:04.920 --> 1:38:09.000
 And you will be able to detect when one of them fails because they don't, because the

1:38:09.000 --> 1:38:13.560
 redundancy, they're giving you the data that's inconsistent with the rest of the situation.

1:38:13.560 --> 1:38:14.560
 That's right.

1:38:14.560 --> 1:38:15.840
 And it's not just like they no longer give data.

1:38:15.840 --> 1:38:20.680
 It could be like they're fouled or they stop giving data where some electrical thing gets

1:38:20.680 --> 1:38:23.840
 cut or part of your compute goes down.

1:38:23.840 --> 1:38:27.800
 So what's neat is that you have way more sensors, part of his field of view and occlusions, part

1:38:27.800 --> 1:38:30.120
 of his redundancy, and part of it is new use cases.

1:38:30.120 --> 1:38:38.040
 So there's new types of sensors to optimize for long range and kind of the sensing horizon

1:38:38.040 --> 1:38:43.440
 that we look for on our vehicles that is unique to trucks because it actually is like kind

1:38:43.440 --> 1:38:47.160
 of much like further out than a car.

1:38:47.160 --> 1:38:50.080
 But a majority are actually we use to cross both cars and trucks and so we use the same

1:38:50.080 --> 1:38:57.240
 compute, the same fundamental baseline sensors, cameras, radar, IMUs.

1:38:57.240 --> 1:39:01.040
 And so you get a great leverage from all of the infrastructure and the hardware development

1:39:01.040 --> 1:39:02.040
 as a result.

1:39:02.040 --> 1:39:03.440
 So what about cameras?

1:39:03.440 --> 1:39:10.960
 What role does, so LiDAR is this rich set of information as its strengths, has some weaknesses,

1:39:10.960 --> 1:39:24.640
 what role does LiDAR play, what role does vision cameras play in this beautiful problem

1:39:24.640 --> 1:39:25.640
 of autonomous trucking?

1:39:25.640 --> 1:39:26.640
 It is beautiful.

1:39:26.640 --> 1:39:28.920
 It's like so much that comes together.

1:39:28.920 --> 1:39:32.840
 And at which point do they come together?

1:39:32.840 --> 1:39:34.080
 So let's start with LiDAR.

1:39:34.080 --> 1:39:40.280
 So LiDAR has been like one of Waymo's big strengths and advantages where we developed

1:39:40.280 --> 1:39:47.440
 our own LiDAR in house, where many generations in both in cost and functionality, it is the

1:39:47.440 --> 1:39:49.840
 best in this space.

1:39:49.840 --> 1:39:50.840
 Which generation?

1:39:50.840 --> 1:39:56.320
 Because I know there's this cool, I love versions that are increasing.

1:39:56.320 --> 1:39:59.320
 Which version of the hardware stack is it currently?

1:39:59.320 --> 1:40:02.760
 Officially, publicly.

1:40:02.760 --> 1:40:04.320
 So some parts iterate more than others.

1:40:04.320 --> 1:40:05.840
 I'm trying to remember on the sensor side.

1:40:05.840 --> 1:40:11.080
 So the entire self driving system, which includes sensors and compute, is fifth generation.

1:40:11.080 --> 1:40:18.600
 I can't wait until there's iPhone style announcements for new versions of the Waymo hardware stack.

1:40:18.600 --> 1:40:22.080
 Well, we try to be careful because man, when you change the hardware, it takes a lot to

1:40:22.080 --> 1:40:24.240
 retrain the models and everything.

1:40:24.240 --> 1:40:27.000
 So we just went through that and going from the Pacificus to the Jaguars.

1:40:27.000 --> 1:40:31.760
 And so the Jaguars and the trucks have the same generation now.

1:40:31.760 --> 1:40:33.920
 But yeah, the LiDAR is incredible.

1:40:33.920 --> 1:40:36.800
 So Waymo has leaned into that as a strength.

1:40:36.800 --> 1:40:41.840
 And so a lot of the near range perception system that obviously kind of carries over

1:40:41.840 --> 1:40:46.720
 a lot from the car side, uses LiDAR as a very prominent kind of like primary sensor.

1:40:46.720 --> 1:40:49.400
 But then obviously everything has its strengths and weaknesses.

1:40:49.400 --> 1:40:54.080
 And so in the near range, LiDAR is a gigantic advantage.

1:40:54.080 --> 1:40:59.120
 And it has its weaknesses on, you know, when it comes to occlusions in certain areas, rain

1:40:59.120 --> 1:41:01.480
 and weather, like, you know, things like that.

1:41:01.480 --> 1:41:06.360
 But it's an incredible sensor and it gives you incredible density, perfect location precision

1:41:06.360 --> 1:41:13.080
 and consistency, which is a very valuable property to be able to kind of apply a mel

1:41:13.080 --> 1:41:14.080
 approach.

1:41:14.080 --> 1:41:15.080
 Can you elaborate consistency?

1:41:15.080 --> 1:41:16.080
 Yeah.

1:41:16.080 --> 1:41:22.040
 When you have a camera, the position of the sun, the time of the day, various of the properties

1:41:22.040 --> 1:41:27.000
 can have a big impact, whether there's glare, the field of view, things like that.

1:41:27.000 --> 1:41:34.280
 So consistent with, in the face of a changing external environment, the signal.

1:41:34.280 --> 1:41:35.280
 Yeah.

1:41:35.280 --> 1:41:42.040
 Daytime, nighttime, it's about 3D physical existence in a fact, like you're seeing beams

1:41:42.040 --> 1:41:45.040
 of light that bounce, physically bounce off of something and come back.

1:41:45.040 --> 1:41:51.520
 And so whatever the conditional conditions are, like the shape of a human sensor reading

1:41:51.520 --> 1:41:56.980
 from a human or from a car or from an animal, like you have a reliability there, which ends

1:41:56.980 --> 1:41:59.840
 up being valuable for kind of like the long tail of challenges.

1:41:59.840 --> 1:42:03.520
 Now, LiDAR is the first sensor to drop off in terms of range and ours has a really good

1:42:03.520 --> 1:42:05.880
 range, but at the end of the day, it drops off.

1:42:05.880 --> 1:42:10.600
 And so particularly for trucks, on top of the general redundancy that you want for near

1:42:10.600 --> 1:42:15.720
 range and compliments through cameras and radar for occlusions and for complementary information

1:42:15.720 --> 1:42:20.600
 and so forth, when you get to long range, you have to be radar and camera primary because

1:42:20.600 --> 1:42:24.880
 your LiDAR data will fundamentally drop off after a period of time and you have to be

1:42:24.880 --> 1:42:27.440
 able to see kind of objects further out.

1:42:27.440 --> 1:42:35.280
 Now, cameras have the incredible range where you get a high density, high resolution camera.

1:42:35.280 --> 1:42:40.520
 You can get data well past a kilometer and it's like really potentially a huge value.

1:42:40.520 --> 1:42:46.120
 Now the signal drops off, the noise is higher, detecting is harder, classifying is harder.

1:42:46.120 --> 1:42:51.440
 And one that you might not think about localizing is harder because you can be off by like two

1:42:51.440 --> 1:42:55.600
 meters and where something's located a kilometer away and that's the difference between being

1:42:55.600 --> 1:42:57.080
 on the shoulder and being in your lane.

1:42:57.080 --> 1:42:59.920
 And so you have like interesting challenges there that you have to solve which have a bunch

1:42:59.920 --> 1:43:01.960
 of approaches that come into it.

1:43:01.960 --> 1:43:11.960
 Radar is interesting because it also has longer range than LiDAR and it gives you speed information

1:43:11.960 --> 1:43:18.640
 so it becomes very, very useful for dynamic information of traffic flow, vehicle motions,

1:43:18.640 --> 1:43:25.160
 signals, pedestrians, like just things that might be useful signals.

1:43:25.160 --> 1:43:28.920
 And it helps with weather conditions where radar actually penetrates weather conditions

1:43:28.920 --> 1:43:30.720
 in a better way than other sensors.

1:43:30.720 --> 1:43:35.680
 And so it's kind of interesting where we've kind of started to converge towards not thinking

1:43:35.680 --> 1:43:39.840
 about a problem as a LiDAR problem or a camera problem or radar problem, but it's a fusion

1:43:39.840 --> 1:43:46.640
 problem where these are all like large scale ML problems where you put data into the system.

1:43:46.640 --> 1:43:51.520
 And in many cases, you just look for the signals that might be present in the union of all

1:43:51.520 --> 1:43:57.600
 of these and leave it to the system as much as possible to start to really identify how

1:43:57.600 --> 1:43:58.600
 to extract that.

1:43:58.600 --> 1:44:03.200
 And then there's places we have to intervene and actually include more, but no single sensor

1:44:03.200 --> 1:44:06.920
 is in a great position to like really solve this problem and end without a huge extra

1:44:06.920 --> 1:44:07.920
 challenge.

1:44:07.920 --> 1:44:10.440
 That's fascinating.

1:44:10.440 --> 1:44:14.640
 There's a question that's probably still an open question is at which point you fuse

1:44:14.640 --> 1:44:15.640
 them.

1:44:15.640 --> 1:44:23.400
 Do you solve the perception problem for each sensor suite individually, the LiDAR suite

1:44:23.400 --> 1:44:28.960
 and the camera suite, or do you do some kind of heterogeneous fusion, or do you fuse at

1:44:28.960 --> 1:44:32.280
 the very beginning?

1:44:32.280 --> 1:44:35.520
 Is there a good answer or at least an inkling of intuitions you can come up with?

1:44:35.520 --> 1:44:36.520
 Yeah.

1:44:36.520 --> 1:44:38.960
 So people refer to this as like early fusion or late fusion.

1:44:38.960 --> 1:44:43.680
 So late fusion might be that you have like the camera pipeline, the LiDAR pipeline, and

1:44:43.680 --> 1:44:49.160
 then you like fuse them and like when it gets to like final semantics and classification

1:44:49.160 --> 1:44:54.040
 and tracking, you like kind of fuse them together and figure out which one's best.

1:44:54.040 --> 1:44:59.360
 There's more and more evidence that early fusion is important.

1:44:59.360 --> 1:45:06.000
 And that is because late fusion does not allow you to pick up on the complementary strengths

1:45:06.000 --> 1:45:08.280
 and weaknesses of the sensors.

1:45:08.280 --> 1:45:12.480
 Weather's a great example where if you do early fusion, you have an incredibly hard

1:45:12.480 --> 1:45:18.320
 problem for any single sensor in rain to solve that problem because you have reflections

1:45:18.320 --> 1:45:19.760
 from the LiDAR.

1:45:19.760 --> 1:45:24.680
 You have weird kind of noise from the camera, blah, blah, blah, right?

1:45:24.680 --> 1:45:29.000
 But the combination of all of them can help you filter and help you get to the real signal

1:45:29.000 --> 1:45:34.160
 that then gets you as close as possible to the original stack and be much more fluid

1:45:34.160 --> 1:45:39.280
 about the strengths and weaknesses where your camera is much more susceptible to like kind

1:45:39.280 --> 1:45:46.320
 of fouling on the actual lens from like rain or random stuff, whereas like you might be

1:45:46.320 --> 1:45:47.720
 a little bit more resilient in other sensors.

1:45:47.720 --> 1:45:51.960
 And so there's an element of logic that always happens late in the game.

1:45:51.960 --> 1:45:56.600
 But that fusion early on, actually, especially as you move towards ML and large scale data

1:45:56.600 --> 1:46:00.640
 driven approaches, just maximizes your ability to pull out the best signal you can out of

1:46:00.640 --> 1:46:05.240
 each modality before you start making constraining decisions that end up being hard to unwind

1:46:05.240 --> 1:46:06.240
 late in the stack.

1:46:06.240 --> 1:46:09.560
 So how much of this is a machine learning problem?

1:46:09.560 --> 1:46:15.040
 What role does ML machine learning play in this whole problem of autonomous driving,

1:46:15.040 --> 1:46:16.840
 autonomous trucking?

1:46:16.840 --> 1:46:20.600
 It's massive and it's increasing over time.

1:46:20.600 --> 1:46:27.880
 If you go back to the grand challenge days in the early days of kind of AV development,

1:46:27.880 --> 1:46:32.040
 there was ML, but it was not in like kind of the mass scale data style of ML.

1:46:32.040 --> 1:46:36.200
 It was like learning models, but in a more structured kind of way.

1:46:36.200 --> 1:46:40.360
 And it was a lot of heuristic and search based approaches and planning and so forth.

1:46:40.360 --> 1:46:44.920
 You can make a lot of progress with these types of approaches kind of across the board

1:46:44.920 --> 1:46:46.480
 and almost deceptive amount of progress.

1:46:46.480 --> 1:46:50.800
 We can get pretty far, but then you start to really grind the further you get in some

1:46:50.800 --> 1:46:55.800
 parts of the stack if you don't have an ability to absorb a massive amount of experience in

1:46:55.800 --> 1:46:59.880
 a way that scales very sublinearly in terms of human labor and human attention.

1:46:59.880 --> 1:47:04.400
 So when you look at the stack, the perception side is probably the first to get really revolutionized

1:47:04.400 --> 1:47:10.200
 by ML and it goes back many years because ML for like computer vision and these types

1:47:10.200 --> 1:47:17.000
 of approaches kind of took off was a lot of the early kind of push and deep learning.

1:47:17.000 --> 1:47:23.480
 And so there's always a debate on the spectrum between kind of like end to end ML, which

1:47:23.480 --> 1:47:27.680
 is a little bit kind of like too far to how you architect it to where you have modules,

1:47:27.680 --> 1:47:31.120
 good enough ability to think about long tail problems and so forth.

1:47:31.120 --> 1:47:36.480
 But at the end of the day, you have big parts of system that are very ML and data driven

1:47:36.480 --> 1:47:41.720
 and we're increasingly moving that direction all the way across the board, including behavior

1:47:41.720 --> 1:47:48.600
 where even when it's not like a gigantic ML problem that covers like a giant swath end

1:47:48.600 --> 1:47:52.400
 to end, more and more parts of system have this property where you want to be able to

1:47:52.400 --> 1:47:55.600
 put more data into it and it gets better.

1:47:55.600 --> 1:47:59.440
 And that has been one of the realizations is you drive tens of millions of miles and

1:47:59.440 --> 1:48:04.760
 try to like solve new expansions of domains without regressing in your old ones.

1:48:04.760 --> 1:48:09.880
 It becomes intractable for a human to approach that in the way that traditionally robotics

1:48:09.880 --> 1:48:12.480
 has kind of approached some elements of the tech stack.

1:48:12.480 --> 1:48:20.720
 So are you trying to create a data pipeline specifically for the trucking problem?

1:48:20.720 --> 1:48:25.280
 How much leveraging of the autonomous driving is there in terms of data collection?

1:48:25.280 --> 1:48:30.520
 And how unique is the data required for the trucking problem?

1:48:30.520 --> 1:48:33.520
 So we use all the same infrastructure.

1:48:33.520 --> 1:48:38.560
 So labeling workflows, ML workflows, everything, so that actually carries over quite well.

1:48:38.560 --> 1:48:44.120
 We heavily reuse the data even where almost every model that we have on a truck, we started

1:48:44.120 --> 1:48:46.360
 with the latest car model.

1:48:46.360 --> 1:48:49.080
 So it's almost like a good background model.

1:48:49.080 --> 1:48:50.080
 Yeah.

1:48:50.080 --> 1:48:53.080
 It's like you can think of like, despite the different domain and different numbers of

1:48:53.080 --> 1:48:57.200
 sensors and position of sensors, there's a lot of signals that carry over across driving.

1:48:57.200 --> 1:49:00.640
 And so it's almost like pre training and getting a big boost out of the gate where you can

1:49:00.640 --> 1:49:02.600
 reduce the amount of data you need by a lot.

1:49:02.600 --> 1:49:04.080
 And it goes both ways, actually.

1:49:04.080 --> 1:49:09.240
 And so we're increasingly thinking about our data strategy on how we leverage both of these.

1:49:09.240 --> 1:49:12.880
 So you think about how other agents react to a truck.

1:49:12.880 --> 1:49:16.120
 Yeah, it's a little bit different, but the fundamentals are actually like, what will

1:49:16.120 --> 1:49:18.320
 other vehicles in the road do?

1:49:18.320 --> 1:49:19.640
 There's a lot of carryover that's possible.

1:49:19.640 --> 1:49:24.560
 But in fact, just to give you an example, we're constantly kind of like adding more

1:49:24.560 --> 1:49:26.000
 data from the trucking side.

1:49:26.000 --> 1:49:30.440
 But as of right now, when we think of our, like one of our models, behavior prediction

1:49:30.440 --> 1:49:38.840
 for other agents on the road, like vehicles, 85% of that data comes from cars.

1:49:38.840 --> 1:49:43.440
 And a lot of that 85% comes from surface streets because we just had so much of it and it was

1:49:43.440 --> 1:49:44.680
 really valuable.

1:49:44.680 --> 1:49:49.000
 And so we're adding in more and more, particularly in the areas where we need more data.

1:49:49.000 --> 1:49:50.840
 But you get a huge boost out of the gate.

1:49:50.840 --> 1:49:55.240
 Just all different visual characteristics of roads, lane markings, pedestrians, all

1:49:55.240 --> 1:49:56.760
 that, that's still relevant.

1:49:56.760 --> 1:49:57.760
 It's all still relevant.

1:49:57.760 --> 1:50:01.480
 And then just the fundamentals of how you detect the car.

1:50:01.480 --> 1:50:05.240
 Does it really change that much, whether you're detecting it from a car or a truck?

1:50:05.240 --> 1:50:09.840
 The fundamentals of how a person will walk around your vehicle is it'll change a little

1:50:09.840 --> 1:50:10.840
 bit.

1:50:10.840 --> 1:50:14.640
 But the basics, like there's a lot of signal in there that as a starting point to a network

1:50:14.640 --> 1:50:16.320
 can actually be very valuable.

1:50:16.320 --> 1:50:20.660
 Now we do have some very unique challenges where there's a sparsity of events on a freeway.

1:50:20.660 --> 1:50:26.640
 The frequency of events happening on a freeway, whether it's interesting objects in the road

1:50:26.640 --> 1:50:32.000
 or incidents or even like from a human benchmark, like how often does a human have an accident

1:50:32.000 --> 1:50:35.680
 on a freeway is far more sparse than on a surface street.

1:50:35.680 --> 1:50:40.560
 And so that leads to really interesting data problems where you can't just drive infinitely

1:50:40.560 --> 1:50:43.880
 to encounter all the different permutations of things you might encounter.

1:50:43.880 --> 1:50:48.840
 And so there you get into interesting tools like structure testing and data collection,

1:50:48.840 --> 1:50:50.840
 data augmentation and so forth.

1:50:50.840 --> 1:50:55.880
 And so there's really interesting kind of technical challenges that push some of the

1:50:55.880 --> 1:51:00.080
 research that enables these new suites of approaches.

1:51:00.080 --> 1:51:02.360
 What role does simulation play?

1:51:02.360 --> 1:51:03.360
 Really good question.

1:51:03.360 --> 1:51:07.080
 So Waymo simulates about a thousand miles for every mile in drives.

1:51:07.080 --> 1:51:08.360
 So you think of...

1:51:08.360 --> 1:51:09.360
 In both.

1:51:09.360 --> 1:51:10.360
 So across the board.

1:51:10.360 --> 1:51:11.360
 Across the board, yeah.

1:51:11.360 --> 1:51:16.680
 So you think of, for example, well, if we've driven over 20 million miles, that's over

1:51:16.680 --> 1:51:18.320
 20 billion miles in simulation.

1:51:18.320 --> 1:51:20.800
 Now, how do you use simulation?

1:51:20.800 --> 1:51:22.360
 It's a multi purpose.

1:51:22.360 --> 1:51:25.800
 So you use it for basic development.

1:51:25.800 --> 1:51:26.800
 So you want to do...

1:51:26.800 --> 1:51:30.920
 Make sure you have regression prevention and protection of everything you're doing, right?

1:51:30.920 --> 1:51:32.920
 That's an easy one.

1:51:32.920 --> 1:51:35.560
 When you encounter something interesting in the world, let's say there was an issue with

1:51:35.560 --> 1:51:38.680
 how the vehicle behaved versus an ideal human.

1:51:38.680 --> 1:51:43.400
 You can play that back in simulation and start augmenting your system and seeing how you would

1:51:43.400 --> 1:51:46.960
 have reacted to that scenario with this improvement or this new area.

1:51:46.960 --> 1:51:51.520
 You can create scenarios that become part of your regression set after that point, right?

1:51:51.520 --> 1:51:57.120
 Then you start getting into really, really hill climbing where you say, hey, I need to

1:51:57.120 --> 1:51:58.120
 improve this system.

1:51:58.120 --> 1:52:01.040
 I have these metrics that are really correlated with final performance.

1:52:01.040 --> 1:52:04.080
 How do I know how well I'm doing?

1:52:04.080 --> 1:52:07.080
 The actual physical driving is the least efficient form of testing.

1:52:07.080 --> 1:52:09.240
 It is the expense of its time consuming.

1:52:09.240 --> 1:52:16.760
 So grabbing a large scale batch of historical data and simulating it to get a signal of

1:52:16.760 --> 1:52:22.000
 over these last or just random sample of 100,000 miles, how has this metric changed versus

1:52:22.000 --> 1:52:23.080
 where we are today?

1:52:23.080 --> 1:52:26.640
 You can do that far more efficiently in simulation than just driving with that new system on

1:52:26.640 --> 1:52:28.280
 board, right?

1:52:28.280 --> 1:52:33.400
 And then you go all the way to the validation phase where to actually see your human relative

1:52:33.400 --> 1:52:37.760
 safety of like how well you're performing on the car side or the trucking side relative

1:52:37.760 --> 1:52:39.800
 to a human.

1:52:39.800 --> 1:52:45.800
 A lot of that safety case is actually driven by taking all of the physical operational

1:52:45.800 --> 1:52:52.320
 driving, which probably includes a lot of interventions where the driver took over just

1:52:52.320 --> 1:52:53.320
 in case.

1:52:53.320 --> 1:52:58.040
 And then you simulate those forward and see if what anything have happened.

1:52:58.040 --> 1:53:01.520
 And in most cases, the answer is no, but you can simulate it forward.

1:53:01.520 --> 1:53:06.440
 And you can even start to do really interesting things where you add virtual agents to create

1:53:06.440 --> 1:53:07.440
 harder environments.

1:53:07.440 --> 1:53:10.320
 You can fuzz the locations of physical agents.

1:53:10.320 --> 1:53:14.160
 You can muck with the scene and stress test the scenario from a whole bunch of different

1:53:14.160 --> 1:53:15.160
 dimensions.

1:53:15.160 --> 1:53:19.440
 And effectively you're trying to like more efficiently sample this like infinite dimensional

1:53:19.440 --> 1:53:24.160
 space, but try to encounter the problems as fast as possible because what most people

1:53:24.160 --> 1:53:29.280
 don't realize is the hardest problem in autonomous driving is actually the evaluation problem

1:53:29.280 --> 1:53:31.840
 in many ways, not the actual autonomy problem.

1:53:31.840 --> 1:53:36.960
 And so if you couldn't really evaluate perfectly and instantaneously, you can solve that problem

1:53:36.960 --> 1:53:40.120
 in a really fast feedback loop quite well.

1:53:40.120 --> 1:53:44.200
 But the hardest part is being really smart about this suite of approaches on how can

1:53:44.200 --> 1:53:49.800
 you get an accurate signal on how well you're doing as quickly as possible in a way that

1:53:49.800 --> 1:53:51.320
 correlates to physical driving.

1:53:51.320 --> 1:53:53.760
 Can you explain the evaluation problem?

1:53:53.760 --> 1:53:58.480
 Which metric are you evaluating towards over talking about safety and some, what are the

1:53:58.480 --> 1:54:00.360
 performance metrics that we're talking about?

1:54:00.360 --> 1:54:02.640
 So in the end, you care about end safety.

1:54:02.640 --> 1:54:08.080
 That's in the end what keeps you, that's what's deceptive where there's a lot of companies

1:54:08.080 --> 1:54:10.560
 that have a great demo.

1:54:10.560 --> 1:54:16.240
 The path from a really great demo to being able to go driverless can be deceptively long

1:54:16.240 --> 1:54:18.480
 even when that demo looks like it's driverless quality.

1:54:18.480 --> 1:54:22.840
 The difference is that the thing that keeps you from going driverless is not the stuff

1:54:22.840 --> 1:54:23.840
 you encounter on a demo.

1:54:23.840 --> 1:54:27.880
 It's the stuff that you encounter once at 100,000 miles or 500,000 miles.

1:54:27.880 --> 1:54:34.040
 And so that is at the root of what is most challenging about going driverless because

1:54:34.040 --> 1:54:37.280
 any issue you encounter, you can go and fix it, but how do you know you didn't create

1:54:37.280 --> 1:54:40.200
 five other issues that you haven't encountered yet?

1:54:40.200 --> 1:54:44.800
 So those learnings, like those were painful learnings in Waymo's history that Waymo went

1:54:44.800 --> 1:54:49.840
 through and led to us then finally being able to go driverless in Phoenix and now are at

1:54:49.840 --> 1:54:53.080
 the heart of how we develop.

1:54:53.080 --> 1:54:58.720
 Humanization is simultaneously evaluating final kind of end safety of how ready are

1:54:58.720 --> 1:55:08.000
 you to go driverless, which may be as direct as what is your human relative kind of collision

1:55:08.000 --> 1:55:13.960
 rate for all these types of scenarios and severities to make sure that you're better

1:55:13.960 --> 1:55:17.480
 than a human bar by a good amount.

1:55:17.480 --> 1:55:19.600
 But that's not actually the most useful for development.

1:55:19.600 --> 1:55:28.800
 For development, it's much more analog metrics that are part of the art of finding what are

1:55:28.800 --> 1:55:32.880
 the properties of driving that give you a way quicker signal that's more sensitive than

1:55:32.880 --> 1:55:38.640
 a collision that can correlate to the quality you care about and push the feedback loop

1:55:38.640 --> 1:55:40.040
 to all of your development.

1:55:40.040 --> 1:55:43.960
 A lot of these are, for example, comparisons to human drivers, like manual drivers, on

1:55:43.960 --> 1:55:48.720
 how do you do relative to a human driver in various dimensions or various circumstances.

1:55:48.720 --> 1:55:51.240
 Can I ask a tricky question?

1:55:51.240 --> 1:55:54.960
 So if I brought you a truck, how would you test it?

1:55:54.960 --> 1:55:55.960
 Okay.

1:55:55.960 --> 1:55:58.320
 Alan Turing came along and you said...

1:55:58.320 --> 1:56:01.800
 This one can't tell if it's a human driver or a autonomous driver.

1:56:01.800 --> 1:56:06.160
 But not the human because humans are flawed.

1:56:06.160 --> 1:56:07.160
 It's different, but yeah.

1:56:07.160 --> 1:56:11.360
 How do you actually know you're ready, basically, and how do you know it's good enough?

1:56:11.360 --> 1:56:15.440
 And by the way, this is the reason why Waymo released a safety framework for the car side

1:56:15.440 --> 1:56:20.200
 because, like, one, it sets the bar so nobody cuts below it and does something bad for the

1:56:20.200 --> 1:56:25.800
 field that causes an accident, and two, it's to start the conversation on framing what

1:56:25.800 --> 1:56:27.000
 does this need to look like.

1:56:27.000 --> 1:56:30.440
 Same thing we'll end up doing for the trucking side.

1:56:30.440 --> 1:56:35.360
 It ends up being different portfolio of approaches.

1:56:35.360 --> 1:56:39.800
 There's easy things like, are you compliant with all these fundamental rules of the road?

1:56:39.800 --> 1:56:41.280
 You never drive above the speed limit.

1:56:41.280 --> 1:56:42.720
 That's actually pretty easy.

1:56:42.720 --> 1:56:47.160
 You can fundamentally prove that it's either impossible to violate that rule or that in

1:56:47.160 --> 1:56:52.320
 these, like, you can itemize the scenarios where that comes up and you can do a test

1:56:52.320 --> 1:56:58.040
 and show that you pass that test and, therefore, you can handle that scenario.

1:56:58.040 --> 1:57:02.880
 And so those are, like, traditional structure testing kind of system engineering approaches

1:57:02.880 --> 1:57:04.560
 where you can just quant...

1:57:04.560 --> 1:57:08.960
 Like, fault rates is another example where when something fails, how do you deal with

1:57:08.960 --> 1:57:09.960
 it?

1:57:09.960 --> 1:57:11.280
 You're not going to drive and randomly wait for it to fail.

1:57:11.280 --> 1:57:14.960
 You're going to force a failure and make sure that you can handle it and close courses and

1:57:14.960 --> 1:57:20.680
 simulation on the road and run through all the permutations of failures, which you can

1:57:20.680 --> 1:57:25.100
 oftentimes for some parts of system itemize, like hardware.

1:57:25.100 --> 1:57:31.720
 The hardest part is behavioral, where you have just infinite situations that could,

1:57:31.720 --> 1:57:33.640
 in theory, happen.

1:57:33.640 --> 1:57:39.120
 And you want to figure out the combinations of approaches that can work there.

1:57:39.120 --> 1:57:42.920
 You can probably pass the Turing test pretty quickly, even if you're not, like, completely

1:57:42.920 --> 1:57:48.440
 ready for driverless, because the events that are really kind of, like, hard will not happen

1:57:48.440 --> 1:57:49.440
 that often.

1:57:49.440 --> 1:57:54.720
 Just to give you a perspective, a human has a serious accident on a freeway.

1:57:54.720 --> 1:57:56.640
 Like a truck driver on a freeway has...

1:57:56.640 --> 1:58:01.480
 There's a serious event happens once every 1.3 million miles, and something that actually

1:58:01.480 --> 1:58:03.880
 has, like, a really serious injury is 28 million miles.

1:58:03.880 --> 1:58:07.840
 And so those are really rare, and so you could have a driver that looks like it's ready to

1:58:07.840 --> 1:58:11.240
 go, but you have no signal on what happens there.

1:58:11.240 --> 1:58:16.720
 And so that's where you start to get creative on combinations of sampling and statistical

1:58:16.720 --> 1:58:22.880
 arguments, focused, structured arguments, where you can kind of simulate those scenarios

1:58:22.880 --> 1:58:28.440
 and show that you can handle them, and metrics that are correlated with what you care about,

1:58:28.440 --> 1:58:32.200
 but you can measure much more quickly and get to a right answer.

1:58:32.200 --> 1:58:33.800
 And that's what makes it pretty hard.

1:58:33.800 --> 1:58:39.920
 And in the end, you end up borrowing a lot of properties from aerospace and, like, space

1:58:39.920 --> 1:58:43.480
 shuttles and so forth, where you don't get the chance to launch it a million times just

1:58:43.480 --> 1:58:46.840
 to say you're ready because it's too expensive to fail.

1:58:46.840 --> 1:58:52.800
 And so you go through a huge amount of kind of structured approaches in order to validate

1:58:52.800 --> 1:58:58.480
 it, and then by thoroughness, you can make a strong argument that you're ready to go.

1:58:58.480 --> 1:59:01.080
 This is actually a harder problem in a lot of ways, though, because you can think of

1:59:01.080 --> 1:59:05.360
 a space shuttle as getting to a fixed point, and then you kind of like, or an airplane,

1:59:05.360 --> 1:59:08.920
 and you freeze the software, and then you prove it, and you're good to go.

1:59:08.920 --> 1:59:13.080
 Here you have to get to a driverless quality bar, but then continue to aggressively change

1:59:13.080 --> 1:59:15.320
 the software even while you're driverless.

1:59:15.320 --> 1:59:21.280
 And also the full range of environment that there's an external environment with a shuttle.

1:59:21.280 --> 1:59:27.400
 You're basically testing the systems, the internal stuff, and you have a lot of control

1:59:27.400 --> 1:59:28.760
 on the external stuff.

1:59:28.760 --> 1:59:32.240
 Yeah, and the hard part is how do you know you didn't get worse in something that you

1:59:32.240 --> 1:59:33.240
 just changed?

1:59:33.240 --> 1:59:34.240
 Yes, sure.

1:59:34.240 --> 1:59:39.400
 And so in a lot of ways, the Turing test starts to fail pretty quickly because you start

1:59:39.400 --> 1:59:43.320
 to feel driverless quality pretty early in that curve.

1:59:43.320 --> 1:59:51.400
 If you think about it, in most really good AV demos, maybe you'll sit there for 30 minutes.

1:59:51.400 --> 1:59:56.040
 So you've driven 15 miles or something like that.

1:59:56.040 --> 2:00:00.560
 To go driverless, what's the sort of rate of issues that you need to have you won't even

2:00:00.560 --> 2:00:01.560
 encounter?

2:00:01.560 --> 2:00:03.000
 So let's try something different then.

2:00:03.000 --> 2:00:08.040
 Let's try a different version of the Turing test, which is like an IQ test.

2:00:08.040 --> 2:00:11.800
 So there's these difficult questions of increasing difficulty.

2:00:11.800 --> 2:00:16.400
 They're designed, you don't know them ahead of time.

2:00:16.400 --> 2:00:18.680
 Nobody knows the answer to them.

2:00:18.680 --> 2:00:23.240
 And so is it possible to, in the future, orchestrate basically really...

2:00:23.240 --> 2:00:30.520
 Off the course, almost, of that maybe change every year and that represent, if you can

2:00:30.520 --> 2:00:34.240
 pass these, they don't necessarily represent the full spectrum.

2:00:34.240 --> 2:00:35.240
 That's it, yeah.

2:00:35.240 --> 2:00:38.560
 They won't be conclusive, but you can at least get a really quick read and filter.

2:00:38.560 --> 2:00:39.560
 You're able to...

2:00:39.560 --> 2:00:40.560
 Yeah.

2:00:40.560 --> 2:00:41.560
 Because you didn't know them ahead of time.

2:00:41.560 --> 2:00:44.120
 Like, I don't know, probably...

2:00:44.120 --> 2:00:46.480
 Like construction zones, failures.

2:00:46.480 --> 2:00:48.720
 Or driving anywhere in Russia.

2:00:48.720 --> 2:00:56.600
 Snow, weather, cut ins, dense traffic, kind of merging, lane closures, animal, foreign

2:00:56.600 --> 2:01:02.360
 objects on a road that pop out on short notice, mechanical failures, sensor, braking, tire

2:01:02.360 --> 2:01:07.200
 popped, weird behaviors by other vehicles, like a heartbreak, something reckless that

2:01:07.200 --> 2:01:12.840
 they've done, fouling of sensors like bugs or birds, poop or something.

2:01:12.840 --> 2:01:18.240
 But yeah, you have these kind of extreme conditions where you have a nasty construction

2:01:18.240 --> 2:01:22.680
 zone where everything shuts down and you have to get pulled to the other side of the freeway

2:01:22.680 --> 2:01:25.120
 with a temporary lane like that.

2:01:25.120 --> 2:01:28.080
 Those are sort of conditions where we do that to ourselves.

2:01:28.080 --> 2:01:31.640
 We itemize everything that could possibly happen to give you a starting point to how

2:01:31.640 --> 2:01:35.240
 to think about what you need to develop and at the end of the day, there's no substitute

2:01:35.240 --> 2:01:36.240
 for real miles.

2:01:36.240 --> 2:01:40.040
 If you think of traditional ML, you know how there's a validation set where you hold out

2:01:40.040 --> 2:01:44.400
 some data and real world driving is the ultimate validation set.

2:01:44.400 --> 2:01:47.680
 That's in the end, like the queenest signal.

2:01:47.680 --> 2:01:50.960
 You can do a really good job on creating an obstacle course and you're absolutely right.

2:01:50.960 --> 2:01:58.000
 At the end, if there was such a thing as automating and kind of a readiness, it would be these

2:01:58.000 --> 2:02:04.080
 extreme conditions like a red light runner, a really reckless pedestrian that's jaywalking,

2:02:04.080 --> 2:02:07.720
 a cyclist that makes a really awkward maneuver.

2:02:07.720 --> 2:02:09.440
 That's actually what keeps you from going driverless.

2:02:09.440 --> 2:02:10.920
 In the end, that is the long tail.

2:02:10.920 --> 2:02:11.920
 Yeah.

2:02:11.920 --> 2:02:13.560
 And it's interesting to think about that.

2:02:13.560 --> 2:02:14.560
 That to me is the touring test.

2:02:14.560 --> 2:02:19.680
 Steward test means a lot of things, but to me, in driving, the touring test is exactly

2:02:19.680 --> 2:02:23.160
 this validation set that is handcrafted.

2:02:23.160 --> 2:02:25.400
 I don't know if you know him.

2:02:25.400 --> 2:02:30.160
 There's a guy named François Chouin.

2:02:30.160 --> 2:02:32.960
 He thinks about how to design a test for general intelligence.

2:02:32.960 --> 2:02:35.880
 He designs these IQ tests for machines.

2:02:35.880 --> 2:02:39.880
 The validation set for him is handcrafted.

2:02:39.880 --> 2:02:47.400
 It requires human genius or ingenuity to create a really good test and you truly hold it out.

2:02:47.400 --> 2:02:54.880
 It's an interesting perspective on the validation set, which is make that as hard as possible.

2:02:54.880 --> 2:02:59.560
 Not a generic representation of the data, but this is the hardest thing.

2:02:59.560 --> 2:03:00.560
 The hardest stuff.

2:03:00.560 --> 2:03:01.560
 Yeah.

2:03:01.560 --> 2:03:02.560
 It's like go.

2:03:02.560 --> 2:03:05.960
 You'll never out fully itemize all the world states that you'll expand and so you have to

2:03:05.960 --> 2:03:07.480
 come up with different approaches.

2:03:07.480 --> 2:03:11.640
 This is where you start hitting the struggles of ML, where ML is fantastic at optimizing

2:03:11.640 --> 2:03:13.160
 the average case.

2:03:13.160 --> 2:03:17.200
 It's a really unique craft to think about how you deal with the worst case, which is what

2:03:17.200 --> 2:03:24.720
 we care about in AV space when using an ML system on something that occurs super infrequently.

2:03:24.720 --> 2:03:28.360
 You don't care about the worst case really on ads because if you miss a few, it's not

2:03:28.360 --> 2:03:33.120
 a big deal, but you do care about it on the driving side.

2:03:33.120 --> 2:03:36.520
 Typically you'll never fully enumerate the world.

2:03:36.520 --> 2:03:38.480
 You have to take a step back and abstract away.

2:03:38.480 --> 2:03:43.920
 What are the signals that you care about and the properties of a driver that correlate

2:03:43.920 --> 2:03:52.080
 to defensive driving and avoiding nasty situations that even though you'll always be surprised

2:03:52.080 --> 2:03:56.320
 by things you'll encounter, you feel good about your ability to generalize from what

2:03:56.320 --> 2:03:57.320
 you've learned.

2:03:57.320 --> 2:03:58.820
 All right.

2:03:58.820 --> 2:04:01.720
 Let me ask you a tricky question.

2:04:01.720 --> 2:04:10.120
 To me, the two companies that are building at scale some of the most incredible robots

2:04:10.120 --> 2:04:16.600
 ever built is Waymo and Tesla.

2:04:16.600 --> 2:04:23.640
 There's very distinct approaches technically, philosophically in these two systems.

2:04:23.640 --> 2:04:29.760
 Let me ask you to play sort of devil's advocate and then the devil's advocate to the devil's

2:04:29.760 --> 2:04:31.700
 advocate.

2:04:31.700 --> 2:04:39.560
 It's a bit of a race, of course, everyone can win, but if Waymo wins this race to level

2:04:39.560 --> 2:04:43.920
 four, why would they win?

2:04:43.920 --> 2:04:47.920
 What aspect of the approach do you think would be the winning aspect?

2:04:47.920 --> 2:04:56.320
 If Tesla wins, why would they win and which aspect of their approach would be the reason?

2:04:56.320 --> 2:05:00.320
 Just building some intuition, almost not from a business perspective, from any of that just

2:05:00.320 --> 2:05:01.320
 technically.

2:05:01.320 --> 2:05:02.320
 Yeah.

2:05:02.320 --> 2:05:03.320
 Yeah.

2:05:03.320 --> 2:05:09.360
 We could summarize, I think, maybe you can correct me, one of the more distinct aspects

2:05:09.360 --> 2:05:16.160
 is Waymo has a richer suite of sensors as lidar and vision.

2:05:16.160 --> 2:05:20.800
 Tesla now removed radar, they do vision only.

2:05:20.800 --> 2:05:25.640
 Tesla has a larger fleet of vehicles operated by humans, so it's already deployed on the

2:05:25.640 --> 2:05:33.560
 field and it's larger, what do you call it, the operational domain, and then Waymo is

2:05:33.560 --> 2:05:39.080
 more focused on a specific domain and growing it with fewer vehicles.

2:05:39.080 --> 2:05:43.080
 That's the both of fascinating approaches, both of, I think, there's a lot of brilliant

2:05:43.080 --> 2:05:48.320
 ideas, nobody knows the answer, so I'd love to get your comments on the sleigh of the

2:05:48.320 --> 2:05:49.320
 land.

2:05:49.320 --> 2:05:50.320
 Yeah, for sure.

2:05:50.320 --> 2:05:54.760
 Maybe I'll start with Waymo and you're right, both incredible companies and just a gigantic

2:05:54.760 --> 2:06:00.160
 respect to everything Tesla has accomplished and how they pushed the field forward as well.

2:06:00.160 --> 2:06:06.240
 On the Waymo side, there is a fundamental advantage in the fact that it is focused and

2:06:06.240 --> 2:06:08.800
 geared towards L4 from the very beginning.

2:06:08.800 --> 2:06:13.800
 We've customized the sensor suite for it, the hardware, the compute, the infrastructure,

2:06:13.800 --> 2:06:17.720
 the tech stack, and all of the investment inside the company.

2:06:17.720 --> 2:06:21.840
 That's deceptively important because there's a giant spectrum of problems you have to solve

2:06:21.840 --> 2:06:27.440
 in order to really do this from infrastructure to hardware to autonomy stack to the safety

2:06:27.440 --> 2:06:28.960
 framework.

2:06:28.960 --> 2:06:32.960
 That's an advantage because there's a reason why it's the fifth generation hardware and

2:06:32.960 --> 2:06:38.960
 why all of those learnings went into the Dymor program, it becomes such an advantage because

2:06:38.960 --> 2:06:41.280
 you learn a lot as you drive.

2:06:41.280 --> 2:06:45.600
 You optimize for the best information you have, but fundamentally, there's a big, big

2:06:45.600 --> 2:06:52.200
 jump, like every order of magnitude that you drive in numbers of miles and what you learn,

2:06:52.200 --> 2:06:57.040
 and the gap from really decent progress for L2 and so forth to what it takes to actually

2:06:57.040 --> 2:06:58.040
 go L4.

2:06:58.040 --> 2:07:04.840
 At the end of the day, there's a feeling that Waymo has, there's a long way to go.

2:07:04.840 --> 2:07:12.480
 Nobody's won, but there's a lot of advantages in all of these buckets where it's the only

2:07:12.480 --> 2:07:15.640
 company that has shipped a fully driverless service where you can go and you can use it.

2:07:15.640 --> 2:07:22.680
 It's at a decently sizeable scale, and those learnings can feed forward how to solve the

2:07:22.680 --> 2:07:23.680
 more general problems.

2:07:23.680 --> 2:07:26.800
 You see this process, you've deployed it in Chandler.

2:07:26.800 --> 2:07:33.320
 You don't know the timeline exactly, but you could see the steps, they seem almost incremental.

2:07:33.320 --> 2:07:34.320
 The steps don't...

2:07:34.320 --> 2:07:36.640
 It's become more engineering than totally blind R&Ds.

2:07:36.640 --> 2:07:40.520
 It works in one place and then you move to another place and you grow it this way.

2:07:40.520 --> 2:07:45.040
 Just to give you an example, we fundamentally changed our hardware and our software stack

2:07:45.040 --> 2:07:50.080
 almost entirely from what went driverless in Phoenix to what is the current generation

2:07:50.080 --> 2:07:55.560
 of the system on both sides because the things that got us to driverless, even though it

2:07:55.560 --> 2:08:02.960
 got to driverless way beyond human relative safety, it is fundamentally not well set up

2:08:02.960 --> 2:08:09.160
 to scale in an exponential fashion without getting into huge scaling pains.

2:08:09.160 --> 2:08:10.920
 Those learnings, you just can't shortcut.

2:08:10.920 --> 2:08:11.920
 That's an advantage.

2:08:11.920 --> 2:08:14.960
 There's a lot of open challenges to get through.

2:08:14.960 --> 2:08:18.960
 Technical organizational, how do you solve problems that are increasingly broad and complex

2:08:18.960 --> 2:08:22.600
 like this, work on multiple products, but there's a few in that, okay, like balls in

2:08:22.600 --> 2:08:25.400
 our court, there's a head start there.

2:08:25.400 --> 2:08:26.400
 Now we got to go and solve it.

2:08:26.400 --> 2:08:28.960
 I think that focus on L4, it's a fundamentally different problem.

2:08:28.960 --> 2:08:33.640
 If you think about it, like, let's say we were designing an L2 truck that was meant to

2:08:33.640 --> 2:08:35.600
 be safer and help a human.

2:08:35.600 --> 2:08:41.520
 You could do that with far less sensors, far less complexity, and provide value very quickly,

2:08:41.520 --> 2:08:46.480
 arguably what we already have today just packaged up in a good product, but you would take a

2:08:46.480 --> 2:08:52.960
 huge risk in having a gap from even the compute and sensors, not to mention the software,

2:08:52.960 --> 2:08:54.920
 to then jump from that system to an L4 system.

2:08:54.920 --> 2:08:56.280
 So it's a huge risk, basically.

2:08:56.280 --> 2:09:01.200
 So again, let me allow me to be the person that plays a devil's advocate and let's argue

2:09:01.200 --> 2:09:07.760
 for the Tesla approach, so what you just laid out makes perfect sense and is exactly right.

2:09:07.760 --> 2:09:15.680
 There's some open questions here, which is, it's possible that investing more in faster

2:09:15.680 --> 2:09:21.960
 data collection, which is essentially what Tesla's doing, will get us there faster if

2:09:21.960 --> 2:09:29.160
 the sensor suite doesn't matter as much, and machine learning can do a lot of the work.

2:09:29.160 --> 2:09:34.840
 Just the open question is, how much is the thing you mentioned before, how much of driving

2:09:34.840 --> 2:09:37.720
 can be end to end learned?

2:09:37.720 --> 2:09:39.240
 That's the open question.

2:09:39.240 --> 2:09:46.960
 Obviously, the Waymo and the vision only machine learning approach will solve driving eventually,

2:09:46.960 --> 2:09:47.960
 both.

2:09:47.960 --> 2:09:48.960
 Yeah.

2:09:48.960 --> 2:09:50.280
 The question is of timeline, what's faster?

2:09:50.280 --> 2:09:51.280
 That's right.

2:09:51.280 --> 2:09:54.000
 And what you mentioned, like, if I were to make the opposite argument, like, what puts

2:09:54.000 --> 2:09:57.320
 Tesla in the strongest position, it's data.

2:09:57.320 --> 2:10:03.080
 What is their super power, where they have an access to real world data, effectively

2:10:03.080 --> 2:10:10.720
 with a safety driver, and they've found a way to get paid by safety drivers versus

2:10:10.720 --> 2:10:12.280
 safety drivers.

2:10:12.280 --> 2:10:14.880
 It's brilliant, right?

2:10:14.880 --> 2:10:19.400
 But all joking aside, one, it is incredible that they've built a business that's incredibly

2:10:19.400 --> 2:10:23.760
 successful that can now be a foundation and bootstrap really aggressive investment in

2:10:23.760 --> 2:10:25.600
 autonomy space.

2:10:25.600 --> 2:10:28.560
 If you can do it, that's always like an incredible kind of advantage.

2:10:28.560 --> 2:10:32.040
 And in the data aspect of it, it is a giant amount of data.

2:10:32.040 --> 2:10:34.080
 If you can use it the right way to then solve the problem.

2:10:34.080 --> 2:10:40.120
 But the ability to collect and filter through the things that matter at real world scale

2:10:40.120 --> 2:10:43.560
 at a large distribution, that is huge.

2:10:43.560 --> 2:10:45.560
 It's a big advantage.

2:10:45.560 --> 2:10:49.160
 And so then the question becomes, can you use it in our right way, and do you have the

2:10:49.160 --> 2:10:53.160
 right software systems and hardware systems in order to solve the problem?

2:10:53.160 --> 2:10:58.680
 And you're right that in the long term, there's no reason to believe that pure camera systems

2:10:58.680 --> 2:11:03.600
 can't solve the problem that humans obviously are solving with vision systems.

2:11:03.600 --> 2:11:06.880
 But it's a risk.

2:11:06.880 --> 2:11:09.800
 So there's no argument that it's not a risk, right?

2:11:09.800 --> 2:11:12.320
 And it's already such a hard problem.

2:11:12.320 --> 2:11:17.240
 And so much of that problem, by the way, is even beyond the perception side, some of

2:11:17.240 --> 2:11:20.960
 the hardest elements of the problem are on behavioral side and decision making and the

2:11:20.960 --> 2:11:22.800
 long tail safety case.

2:11:22.800 --> 2:11:28.520
 If you are adding risk and complexity on the input side from perception, you're now making

2:11:28.520 --> 2:11:34.840
 a really, really hard problem, which on its own is still almost insurmountably even harder.

2:11:34.840 --> 2:11:36.920
 And so the question is just how much?

2:11:36.920 --> 2:11:43.640
 And this is where you can easily get into a little bit of a trap where, similar to how

2:11:43.640 --> 2:11:48.760
 do you evaluate how good an AV company's product is, you go and you do a trial, kind

2:11:48.760 --> 2:11:52.240
 of a test run with them, a demo run, which they've kind of optimized like crazy and so

2:11:52.240 --> 2:11:53.240
 forth.

2:11:53.240 --> 2:11:54.240
 And it feels good.

2:11:54.240 --> 2:11:55.840
 Do you put any weight in that, right?

2:11:55.840 --> 2:11:59.720
 You know that that gap is kind of pretty large still.

2:11:59.720 --> 2:12:05.280
 Same thing on the perception case, the long tail of computer vision is really, really hard.

2:12:05.280 --> 2:12:08.120
 And there's a lot of ways that that can come up.

2:12:08.120 --> 2:12:12.040
 And even if it doesn't happen that often at all, when you think about the safety bar

2:12:12.040 --> 2:12:16.480
 and what it takes to actually go full driverless, not like incredible assistance driverless,

2:12:16.480 --> 2:12:20.920
 but full driverless, that bar gets crazy high.

2:12:20.920 --> 2:12:25.560
 And not only do you have to solve it on the behavioral side, but now you have to push

2:12:25.560 --> 2:12:28.840
 computer vision beyond arguably where it's ever been pushed.

2:12:28.840 --> 2:12:32.480
 And so you now on top of the broader AV challenge, you have a really hard perception challenge

2:12:32.480 --> 2:12:33.480
 as well.

2:12:33.480 --> 2:12:36.360
 So there's perception, there's planning, there's human robot interaction.

2:12:36.360 --> 2:12:43.440
 To me, what's fascinating about what Tesla is doing is in this March towards level four,

2:12:43.440 --> 2:12:48.320
 because it's in the hands of so many humans, you get to see video, you get to see humans.

2:12:48.320 --> 2:12:54.880
 I mean, forget companies, forget businesses, it's fascinating for humans to be interacting

2:12:54.880 --> 2:12:55.880
 with robots.

2:12:55.880 --> 2:12:56.880
 That's incredible.

2:12:56.880 --> 2:12:58.600
 And they're actually helping kind of push it forward.

2:12:58.600 --> 2:13:02.800
 And that is valuable, by the way, where even for us, a decent percentage of our data is

2:13:02.800 --> 2:13:04.440
 human driving.

2:13:04.440 --> 2:13:08.560
 We intentionally have humans drive higher percentage than you might expect because that

2:13:08.560 --> 2:13:11.360
 creates some of the best signals to train the autonomy.

2:13:11.360 --> 2:13:14.560
 And so that is on its own value.

2:13:14.560 --> 2:13:19.160
 So together, we're kind of learning about this problem in an applied sense, just like

2:13:19.160 --> 2:13:22.000
 you had with Cosmo.

2:13:22.000 --> 2:13:27.560
 When you're chasing an actual product that people are going to use, robot based product

2:13:27.560 --> 2:13:32.520
 that people are going to use, you have to contend with the reality of what it takes

2:13:32.520 --> 2:13:36.040
 to build a robot that successfully perceives the world and operates in the world and what

2:13:36.040 --> 2:13:39.160
 it takes to have a robot that interacts with other humans in the world.

2:13:39.160 --> 2:13:44.000
 And that's like, to me, one of the most interesting problems humans have ever undertaken.

2:13:44.000 --> 2:13:49.840
 Because you're in trying to create an intelligent agent that operates in a human world.

2:13:49.840 --> 2:13:55.200
 You're also understanding the nature of intelligence itself.

2:13:55.200 --> 2:13:56.400
 How hard is driving?

2:13:56.400 --> 2:13:59.640
 It's still not answered to me.

2:13:59.640 --> 2:14:01.680
 I still don't understand that.

2:14:01.680 --> 2:14:06.520
 And all the subtle cues, even little things like your interaction with a pedestrian where

2:14:06.520 --> 2:14:09.520
 you look at each other and just go, OK, go.

2:14:09.520 --> 2:14:12.080
 That's hard to do without a human driver.

2:14:12.080 --> 2:14:14.640
 And you're missing that dimension, how do you communicate that?

2:14:14.640 --> 2:14:17.840
 So there's really, really interesting elements here.

2:14:17.840 --> 2:14:18.920
 Now, here's what's beautiful.

2:14:18.920 --> 2:14:25.040
 Can you imagine that when autonomous driving is solved, how much of the technology foundation

2:14:25.040 --> 2:14:32.280
 of that space can go and have tremendous transformative impacts on other problem areas

2:14:32.280 --> 2:14:36.920
 and other spaces that have subsets of these same problems?

2:14:36.920 --> 2:14:38.720
 It's just incredible to talk about that.

2:14:38.720 --> 2:14:47.360
 It's both a pro and a con is with autonomous driving is so safety critical.

2:14:47.360 --> 2:14:51.760
 So once you solve it, it's beautiful because there's so many applications that are a lot

2:14:51.760 --> 2:14:53.720
 less safety critical.

2:14:53.720 --> 2:14:56.800
 But it's also the con of that is it's so safety.

2:14:56.800 --> 2:14:58.280
 It's so hard to solve.

2:14:58.280 --> 2:15:03.240
 And the same journalists that you mentioned to get excited for a demo are the ones who

2:15:03.240 --> 2:15:11.800
 write long articles about the failure of your company if there's one accident that's based

2:15:11.800 --> 2:15:12.800
 on a robot.

2:15:12.800 --> 2:15:17.960
 And it's just society is so tense and waiting for failure of robots.

2:15:17.960 --> 2:15:20.760
 You're in such a high stake environment.

2:15:20.760 --> 2:15:22.160
 Failure has such a high cost.

2:15:22.160 --> 2:15:23.560
 And it's slow down development.

2:15:23.560 --> 2:15:24.560
 It's hard.

2:15:24.560 --> 2:15:25.560
 It slows down development.

2:15:25.560 --> 2:15:26.560
 Yeah.

2:15:26.560 --> 2:15:29.120
 Like the team definitely noticed that once you go driverless, like we're driverless in

2:15:29.120 --> 2:15:32.760
 Phoenix and you continue to iterate, your iteration pace slows down.

2:15:32.760 --> 2:15:33.760
 Yeah.

2:15:33.760 --> 2:15:41.920
 Because your fear of regression forces so much more rigor that obviously you have to

2:15:41.920 --> 2:15:45.480
 find a compromise on like, okay, well, how often do we release driverless builds because

2:15:45.480 --> 2:15:48.400
 every time you release a driverless build, you have to go through this like validation

2:15:48.400 --> 2:15:50.080
 process, which is very expensive and so forth.

2:15:50.080 --> 2:15:52.000
 So it is interesting.

2:15:52.000 --> 2:15:54.040
 It's like it is one of the hardest things.

2:15:54.040 --> 2:15:59.280
 There's no other industry where you wouldn't release products way, way quicker when you

2:15:59.280 --> 2:16:03.680
 start to kind of provide even portions of the value that you provide healthcare maybe

2:16:03.680 --> 2:16:04.680
 is the other one.

2:16:04.680 --> 2:16:05.680
 Yeah.

2:16:05.680 --> 2:16:06.680
 But at the same time, right?

2:16:06.680 --> 2:16:08.920
 Like we've gotten there where you think of like surgery, right?

2:16:08.920 --> 2:16:13.920
 Like you have surgery, there's always a risk, but like it's really, really bounded.

2:16:13.920 --> 2:16:16.560
 You know that there's an accident rate when you go out and drive your car today, right?

2:16:16.560 --> 2:16:20.320
 Like, and you know what the fatality rate in the US is per year.

2:16:20.320 --> 2:16:24.400
 We're not banning driving because there was a car accident, but the bar for us is way

2:16:24.400 --> 2:16:28.400
 higher and we hold ourselves very serious to it where you have to not only be better

2:16:28.400 --> 2:16:32.960
 than a human, but you probably have to like at scale be far better than a human by a big

2:16:32.960 --> 2:16:38.600
 margin and you have to be able to like really, really thoughtfully explain all of the ways

2:16:38.600 --> 2:16:43.280
 that we validate that becomes very comfortable for humans to understand because a bunch of

2:16:43.280 --> 2:16:47.040
 jargon that we use internally just doesn't compute at the end of the day.

2:16:47.040 --> 2:16:51.720
 We have to be able to explain to society, how do we quantify the risk and acknowledge

2:16:51.720 --> 2:16:57.520
 that there is some non zero risk, but it's far above a human relative safety.

2:16:57.520 --> 2:17:03.160
 Here's the thing to push back a little bit and bring Cosmo back in the conversation.

2:17:03.160 --> 2:17:05.360
 You said something quite brilliant at the beginning of this conversation.

2:17:05.360 --> 2:17:10.920
 I think probably applies for autonomous driving, which is, you know, there's this desire to

2:17:10.920 --> 2:17:15.080
 make autonomous cars more safer than human driven cars.

2:17:15.080 --> 2:17:20.200
 But if you create a product that's really compelling and is able to explain both the

2:17:20.200 --> 2:17:27.160
 leadership and the engineers and the product itself can communicate intent, then I think

2:17:27.160 --> 2:17:32.000
 people may be able to be willing to put up with the thing that might be even riskier

2:17:32.000 --> 2:17:36.960
 than humans because they understand the value of taking risks.

2:17:36.960 --> 2:17:38.960
 You mentioned the speed limit.

2:17:38.960 --> 2:17:41.880
 Humans understand the value of going over the speed limit.

2:17:41.880 --> 2:17:49.400
 Humans understand the value of like going fast through a yellow light to take in when

2:17:49.400 --> 2:17:54.000
 you're in Manhattan streets, pushing through crossing pedestrians.

2:17:54.000 --> 2:17:55.000
 They understand that.

2:17:55.000 --> 2:17:59.400
 I mean, this is a much more tense topic of discussion, so this is just me talking.

2:17:59.400 --> 2:18:05.400
 So in Cosmo's case, there was something about the way this particular robot communicated,

2:18:05.400 --> 2:18:10.800
 the energy it brought, the intent it was able to communicate to the humans that you understood

2:18:10.800 --> 2:18:13.440
 that of course it needs to have a camera.

2:18:13.440 --> 2:18:15.520
 Of course it needs to have this information.

2:18:15.520 --> 2:18:20.160
 And in that same way, to me, of course a car needs to take risks.

2:18:20.160 --> 2:18:23.080
 Of course there's going to be accidents.

2:18:23.080 --> 2:18:29.760
 That's what, if you want a car that never has an accident, to have a car that just doesn't

2:18:29.760 --> 2:18:31.440
 go anywhere.

2:18:31.440 --> 2:18:37.240
 And so that, but that's tricky because that's not a robotics problem.

2:18:37.240 --> 2:18:41.040
 And many accidents are not even like due to you, obviously.

2:18:41.040 --> 2:18:44.160
 So there's a big difference though.

2:18:44.160 --> 2:18:46.920
 You are, that's not a personal decision.

2:18:46.920 --> 2:18:52.280
 You're also impacting obviously kind of the rest of the road and we're facilitating it.

2:18:52.280 --> 2:18:58.640
 And so there's a higher kind of ethical and moral bar, which obviously then translates

2:18:58.640 --> 2:19:03.840
 into as a society and from a regulatory standpoint, kind of like what comes out of it, where it's

2:19:03.840 --> 2:19:12.440
 hard for us to ever see this even being a debate in the sense that you have to be beyond

2:19:12.440 --> 2:19:15.480
 reproach from a safety standpoint, because if you're wrong about this, you could set

2:19:15.480 --> 2:19:17.280
 the entire field back a decade, right?

2:19:17.280 --> 2:19:24.400
 See, this is me speaking, I think if we look into the future, there will be, I personally

2:19:24.400 --> 2:19:31.640
 believe, this is me speaking, that there will be less and less focus on safety, still very,

2:19:31.640 --> 2:19:32.640
 very high.

2:19:32.640 --> 2:19:33.640
 Yeah.

2:19:33.640 --> 2:19:35.680
 Meaning like after autonomy is very common and accepted.

2:19:35.680 --> 2:19:36.680
 Yeah.

2:19:36.680 --> 2:19:42.240
 But not so common as everywhere, but there has to be a transition because I think for

2:19:42.240 --> 2:19:48.000
 innovation, just like you were saying, to explore ideas, you have to take risks and I think

2:19:48.000 --> 2:19:54.480
 if autonomy in the near term is to become prevalent in society, I think people need

2:19:54.480 --> 2:20:01.160
 to be more willing to understand the nature of risk, the value of risk.

2:20:01.160 --> 2:20:05.920
 It's very difficult, you're right, of course, with driving, but that's the fascinating nature

2:20:05.920 --> 2:20:13.400
 of it, it's a life and death situation that brings value to millions of people, so you

2:20:13.400 --> 2:20:17.000
 have to figure out what do we value about this world?

2:20:17.000 --> 2:20:23.720
 How much do we value, how deeply do we want to avoid hurting other humans?

2:20:23.720 --> 2:20:24.720
 That's right.

2:20:24.720 --> 2:20:29.720
 And there is a point where you can imagine a scenario where Waymo has a system that

2:20:29.720 --> 2:20:40.720
 is even when it's beyond human relative safety and provably statistically will save lives,

2:20:40.720 --> 2:20:50.440
 there is a thoughtful navigation of the that fact versus just kind of society readiness

2:20:50.440 --> 2:20:59.120
 and perception and education of society and regulators and everything else where it's

2:20:59.120 --> 2:21:06.800
 multidimensional and it's not a purely logical argument, but ironically the logic can actually

2:21:06.800 --> 2:21:12.280
 help with the emotions and just like any technology, there's early adopters and then there's kind

2:21:12.280 --> 2:21:14.920
 of like a curve that happens after it.

2:21:14.920 --> 2:21:18.840
 And eventually celebrities, you get the rock in a Waymo vehicle and then everybody just

2:21:18.840 --> 2:21:19.840
 comes along.

2:21:19.840 --> 2:21:23.320
 And then everybody just calms down because the rock likes it.

2:21:23.320 --> 2:21:24.320
 If you post the hymns.

2:21:24.320 --> 2:21:25.320
 Yeah.

2:21:25.320 --> 2:21:28.000
 And it's like, it's an open question on how this plays out, I mean, maybe we're pleasantly

2:21:28.000 --> 2:21:33.360
 surprised and it just like people just realize that this is such a enabler of life and like

2:21:33.360 --> 2:21:38.880
 efficiency and cost and everything that there's a pull like at someone has you fully believe

2:21:38.880 --> 2:21:44.720
 that this will go from a thoughtful kind of, you know, you know, movement and tiptoeing

2:21:44.720 --> 2:21:50.400
 and like kind of like a push to society realizes how wonderful of an enabler this could become

2:21:50.400 --> 2:21:53.920
 and it becomes more of a pull and hard to know exactly how that'll play out.

2:21:53.920 --> 2:21:57.960
 But at the end of the day, like both the goods transportation and the people transportation

2:21:57.960 --> 2:22:00.520
 side of it has that property where it's not easy.

2:22:00.520 --> 2:22:04.160
 There's a lot of open questions and challenges to navigate and there's obviously the technical

2:22:04.160 --> 2:22:10.200
 problems to solve as a, you know, kind of prerequisite, but they, they have such an

2:22:10.200 --> 2:22:15.720
 opportunity that is on a scale that very few industries in the last 20, 30 years have even

2:22:15.720 --> 2:22:22.320
 had a chance to tackle that I maybe were pleasantly surprised by how much, how much that tipping

2:22:22.320 --> 2:22:26.700
 point like in a really short amount of time actually turns into a societal pull to kind

2:22:26.700 --> 2:22:28.200
 of embrace the benefits of this.

2:22:28.200 --> 2:22:29.200
 Yeah.

2:22:29.200 --> 2:22:30.200
 I hope so.

2:22:30.200 --> 2:22:32.800
 It seems like in the recent few decades, there's been tipping points of technologies

2:22:32.800 --> 2:22:35.160
 where like overnight things change.

2:22:35.160 --> 2:22:40.160
 It's like from taxis to ride sharing services, all that, that shift.

2:22:40.160 --> 2:22:45.480
 I mean, there's just shift after shift after shift that requires digitization and technology.

2:22:45.480 --> 2:22:47.440
 I hope we're pleasantly surprised in this.

2:22:47.440 --> 2:22:51.800
 So there's millions of long haul trucks now in the United States.

2:22:51.800 --> 2:22:58.920
 Do you see a future where there's millions of Waymo trucks and maybe just broadly speaking

2:22:58.920 --> 2:23:07.600
 Waymo vehicles, just like ants running around the United States freeways and local roads?

2:23:07.600 --> 2:23:08.600
 Yeah.

2:23:08.600 --> 2:23:09.600
 In other countries too.

2:23:09.600 --> 2:23:14.400
 Like you look back decades from now and it might be one of those things that just feels

2:23:14.400 --> 2:23:18.680
 so natural and then it becomes almost like this kind of interesting kind of oddity that

2:23:18.680 --> 2:23:22.680
 we had none of it like, you know, kind of decades earlier.

2:23:22.680 --> 2:23:25.760
 And it'll take a long time to grow and scale.

2:23:25.760 --> 2:23:28.320
 Very different challenges appear at every stage.

2:23:28.320 --> 2:23:33.680
 But over time, like this is one of the most enabling technologies that we have in the

2:23:33.680 --> 2:23:35.880
 world today.

2:23:35.880 --> 2:23:39.080
 It'll feel like, you know, how is the world before the internet?

2:23:39.080 --> 2:23:40.080
 How is the world before mobile phones?

2:23:40.080 --> 2:23:42.480
 Like it's going to have that sort of a feeling to it on both sides.

2:23:42.480 --> 2:23:49.160
 It's hard to predict the future, but do sometimes think about weird ways in my change the world

2:23:49.160 --> 2:23:50.760
 like surprising ways.

2:23:50.760 --> 2:23:56.320
 So obviously, there's more direct ways where like there's increases efficiency, it'll enable

2:23:56.320 --> 2:24:01.240
 a lot of kind of logistics optimizations kind of things.

2:24:01.240 --> 2:24:07.280
 It will change our probably our roadways and all that kind of stuff.

2:24:07.280 --> 2:24:11.280
 But it could also change society in some kind of interesting ways.

2:24:11.280 --> 2:24:15.040
 Do you ever think about how might change cities, how might change your lives, all that kind

2:24:15.040 --> 2:24:16.040
 of stuff?

2:24:16.040 --> 2:24:17.040
 Yeah.

2:24:17.040 --> 2:24:20.680
 You can imagine city where people live versus work becoming more distributed because the

2:24:20.680 --> 2:24:24.040
 pain of commuting becomes different, just easier.

2:24:24.040 --> 2:24:28.040
 And I don't know, there's a lot of options that open up the layout of cities themselves

2:24:28.040 --> 2:24:35.160
 and how you think about car storage and parking obviously just enables a completely different

2:24:35.160 --> 2:24:39.520
 type of experience in urban environments.

2:24:39.520 --> 2:24:46.400
 I think there was like a statistic that something like 30% of the traffic in cities during rush

2:24:46.400 --> 2:24:51.000
 hour is caused by a pursuit of parking or something like some really high stats.

2:24:51.000 --> 2:24:55.320
 So those obviously kind of open up a lot of options.

2:24:55.320 --> 2:25:00.120
 Flexibility on goods will enable new industries and businesses that never existed before

2:25:00.120 --> 2:25:03.960
 because now the efficiency becomes more palatable.

2:25:03.960 --> 2:25:08.960
 Good delivery timing consistency and flexibility is going to change the way we distribute the

2:25:08.960 --> 2:25:11.080
 logistics network will change.

2:25:11.080 --> 2:25:17.220
 The way we then can integrate with warehousing, with shipping ports, you can start to think

2:25:17.220 --> 2:25:23.160
 about greater automation through the whole stack and how that supply chain, the ripples

2:25:23.160 --> 2:25:32.040
 become much more agile versus very grindy the way they are today where just the adaptation

2:25:32.040 --> 2:25:34.960
 is very tough and there's a lot of constraints that we have.

2:25:34.960 --> 2:25:38.920
 I think it'll be great for the environment, it'll be great for safety where probably

2:25:38.920 --> 2:25:46.080
 about 95% of accidents today statistically are due to just attention or things that are

2:25:46.080 --> 2:25:49.160
 preventable with the strengths of automation.

2:25:49.160 --> 2:25:54.600
 Yeah, and it'll be one of those things where like industries will shift but the net creation

2:25:54.600 --> 2:25:58.560
 is going to be massively positive and then we just have to be thoughtful about the negative

2:25:58.560 --> 2:26:03.280
 implications that will happen in local areas and adjust for those.

2:26:03.280 --> 2:26:06.640
 But I'm an optimist in general for the technology where you could argue a negative on any new

2:26:06.640 --> 2:26:13.360
 technology but you start to see that if there is a big demand for something like this, in

2:26:13.360 --> 2:26:20.160
 almost all cases, it's an enabling factor that's going to propagate through society.

2:26:20.160 --> 2:26:25.760
 And particularly as life expectancies get longer and so forth, there's just a lot more

2:26:25.760 --> 2:26:32.040
 need for a greater percentage of the population to just be serviced with a high level of efficiency

2:26:32.040 --> 2:26:35.400
 because otherwise we're going to have a really hard time scaling to what's ahead in the next

2:26:35.400 --> 2:26:36.720
 50 years.

2:26:36.720 --> 2:26:38.200
 Yeah, and you're absolutely right.

2:26:38.200 --> 2:26:43.600
 Every technology has negative consequences, positive consequences and we tend to focus

2:26:43.600 --> 2:26:45.640
 on the negative a little bit too much.

2:26:45.640 --> 2:26:53.960
 In fact, autonomous trucks are often brought up as an example of artificial intelligence

2:26:53.960 --> 2:26:57.480
 and robots in general taking our jobs.

2:26:57.480 --> 2:27:04.120
 And as we've talked about briefly here, we talk a lot with Steve, it is a concern that

2:27:04.120 --> 2:27:09.040
 automation will take away certain jobs, it'll create other jobs.

2:27:09.040 --> 2:27:15.240
 There's temporary pain, hopefully temporary, but pain is pain and people suffer and that's

2:27:15.240 --> 2:27:19.120
 human suffering is really important to think about.

2:27:19.120 --> 2:27:25.840
 But trucking is, I mean, there's a lot written on this is I would say far from the thing

2:27:25.840 --> 2:27:28.360
 that will cause the most pain.

2:27:28.360 --> 2:27:31.840
 Yeah, there's even more positive properties about trucking where not only is there just

2:27:31.840 --> 2:27:36.000
 a huge shortage which is going to increase, the average age of truck drivers is getting

2:27:36.000 --> 2:27:39.120
 closer to 50 because the younger people aren't wanting to come into it.

2:27:39.120 --> 2:27:44.760
 They're trying to incentivize, lower the age limit, like all these sort of things and the

2:27:44.760 --> 2:27:46.480
 demand is just going to increase.

2:27:46.480 --> 2:27:50.000
 And the least favorable, I mean, it depends on the person, but in most cases, the least

2:27:50.000 --> 2:27:53.760
 favorable types of routes are the massive long haul routes where you're on the road

2:27:53.760 --> 2:27:56.080
 away from your family 300 plus days a year.

2:27:56.080 --> 2:28:01.960
 Steve talked about the pain of those kind of routes from a family perspective, you're

2:28:01.960 --> 2:28:03.240
 basically away from family.

2:28:03.240 --> 2:28:09.000
 It's not just hours, work insane hours, but it's also just time away from family and

2:28:09.000 --> 2:28:10.000
 just...

2:28:10.000 --> 2:28:13.400
 Obesity rate is through the roof because you're just sitting all day, like it's really, really

2:28:13.400 --> 2:28:19.000
 tough and that's also where the biggest kind of safety risk is because of fatigue.

2:28:19.000 --> 2:28:23.280
 And so when you think of the gradual evolution of how trucking comes in, first of all, it's

2:28:23.280 --> 2:28:26.480
 not overnight, it's going to take decades to kind of phase in all the...

2:28:26.480 --> 2:28:33.160
 There's just a long, long road ahead, but the routes and the portions of trucking that

2:28:33.160 --> 2:28:37.760
 are going to require humans the longest and benefit the most from humans are the short

2:28:37.760 --> 2:28:42.640
 haul and most complicated kind of more urban routes, which are also the more pleasant ones,

2:28:42.640 --> 2:28:51.360
 which are less continual driving time, more flexibility on geography and location.

2:28:51.360 --> 2:28:54.800
 And you get to kind of sleep at your own home.

2:28:54.800 --> 2:29:01.400
 And very importantly, if you optimize the logistics, you're going to use humans much

2:29:01.400 --> 2:29:08.320
 better and thereby pay them much better because one of the biggest problems is truck drivers

2:29:08.320 --> 2:29:11.440
 currently are paid by how much they drive.

2:29:11.440 --> 2:29:17.120
 So they really feel the pain of inefficient logistics because if they're just sitting around

2:29:17.120 --> 2:29:21.560
 for hours, which they often do not driving, waiting, they're not getting paid for that

2:29:21.560 --> 2:29:22.560
 time.

2:29:22.560 --> 2:29:27.120
 And so logistics has a significant impact on the quality of life of a truck driver.

2:29:27.120 --> 2:29:31.200
 And a high percentage of trucks are empty because of inefficiencies in the system.

2:29:31.200 --> 2:29:35.360
 Yeah, it's one of those things where the other thing is when you increase the efficiency

2:29:35.360 --> 2:29:41.520
 of a system like this, the overall net volume of the system tends to increase.

2:29:41.520 --> 2:29:47.040
 The entire market cap of trucking is going to go up when the efficiency improves and

2:29:47.040 --> 2:29:51.240
 facilitates both growth in industries and better utilization of trucking.

2:29:51.240 --> 2:29:56.040
 And so that on its own just creates more and more demand, which of all the places where

2:29:56.040 --> 2:30:03.320
 AI comes in and starts to really kind of reshape an industry, this is one of those where there's

2:30:03.320 --> 2:30:07.600
 just a lot of positives that for at least any time in the foreseeable future seem really

2:30:07.600 --> 2:30:14.160
 lined up in a good way to kind of come in and help with the shortage and start to kind

2:30:14.160 --> 2:30:18.960
 of optimize for the routes that are most dangerous and most painful.

2:30:18.960 --> 2:30:25.480
 Yeah, so this is true for trucking, but if we zoom out broader, automation in AI does

2:30:25.480 --> 2:30:31.840
 technology broadly, I would say, but automation is a thing that has a potential in the next

2:30:31.840 --> 2:30:37.000
 couple of decades to shift the kind of jobs available to humans.

2:30:37.000 --> 2:30:42.480
 And so that results in, like I said, human suffering because people lose their jobs,

2:30:42.480 --> 2:30:46.760
 there's economic pain there, and there's also a pain of meaning.

2:30:46.760 --> 2:30:58.960
 So for a lot of people, work is a source of meaning, it's a source of identity, of pride,

2:30:58.960 --> 2:31:03.920
 pride in getting good at the job, pride in craftsmanship and excellence, which is what

2:31:03.920 --> 2:31:06.040
 truck drivers talk about.

2:31:06.040 --> 2:31:08.640
 But this is true for a lot of jobs.

2:31:08.640 --> 2:31:12.520
 And is that something you think about as a sort of a robotic that's zooming out from

2:31:12.520 --> 2:31:21.160
 the trucking thing, like where do you think it would be harder to find activity and work

2:31:21.160 --> 2:31:24.960
 that's a source of identity, a source of meaning in the future?

2:31:24.960 --> 2:31:30.320
 I do think about it because you want to make sure that you worry about the entire system,

2:31:30.320 --> 2:31:34.360
 not just the part of the economy plays in it, but what are the ripple effects of it down

2:31:34.360 --> 2:31:35.360
 the road.

2:31:35.360 --> 2:31:39.880
 And on enough of a time when there's a lot of opportunity to put in the right policies

2:31:39.880 --> 2:31:44.120
 and the right opportunities to kind of reshape and retrain and find those openings.

2:31:44.120 --> 2:31:49.440
 And so just to give you a few examples, both trucking and cars, we have remote assistance

2:31:49.440 --> 2:31:56.960
 facilities that are there to interface with customers and monitor vehicles and provide

2:31:56.960 --> 2:32:01.920
 like very focused kind of assistance on kind of areas where the vehicle may want to request

2:32:01.920 --> 2:32:04.200
 help in understanding an environment.

2:32:04.200 --> 2:32:07.240
 So those are jobs that kind of get created and supported.

2:32:07.240 --> 2:32:11.360
 I remember like taking a tour of one of the Amazon facilities where you've probably seen

2:32:11.360 --> 2:32:16.600
 the Kiva Systems robots, where you have these orange robots that have automated the warehouse

2:32:16.600 --> 2:32:19.840
 like kind of picking and collecting of items.

2:32:19.840 --> 2:32:21.920
 And it's like really elegant and beautiful way.

2:32:21.920 --> 2:32:25.120
 It's actually one of my favorite applications of robotics of all time.

2:32:25.120 --> 2:32:30.680
 I think it kind of came across that company in like 2006 was just amazing.

2:32:30.680 --> 2:32:31.680
 And what was the...

2:32:31.680 --> 2:32:33.680
 The warehouse robots, the transport little thing.

2:32:33.680 --> 2:32:37.120
 So basically instead of a person going and walking around and picking the seven items

2:32:37.120 --> 2:32:43.440
 in your order, these robots go and pick up a shelf and move it over in a row where like

2:32:43.440 --> 2:32:47.960
 the seven shelves that contain the seven items are lined up in a laser or whatever points

2:32:47.960 --> 2:32:51.120
 to what you need to get and you go and pick it and you place it to fill the order.

2:32:51.120 --> 2:32:53.520
 And so the people were fulfilling the final orders.

2:32:53.520 --> 2:32:57.200
 What was interesting about that is that when I was asking them about like kind of the impact

2:32:57.200 --> 2:33:01.800
 on labor, when they transitioned that warehouse, the throughput increased so much that the

2:33:01.800 --> 2:33:07.200
 jobs shifted towards the final fulfillment, even though the robots took over entirely

2:33:07.200 --> 2:33:11.320
 the search of the items themselves and the labor...

2:33:11.320 --> 2:33:15.520
 The job stayed like nobody, like there was actually the same amount of jobs, roughly

2:33:15.520 --> 2:33:19.560
 they were necessary, but the throughput increased by like, I think over two X or some amount,

2:33:19.560 --> 2:33:20.560
 right?

2:33:20.560 --> 2:33:23.920
 Like so you have these situations that are not zero some games in this really interesting

2:33:23.920 --> 2:33:24.920
 way.

2:33:24.920 --> 2:33:28.000
 And the optimist to me thinks that there's these types of solutions in almost any industry

2:33:28.000 --> 2:33:32.720
 where the growth that's enabled creates opportunities that you can then leverage, but you got to

2:33:32.720 --> 2:33:38.240
 be intentional about finding those and really helping make those links because any...

2:33:38.240 --> 2:33:42.920
 Even if you make the argument that like there's a net positive, locally there's always tough

2:33:42.920 --> 2:33:44.720
 hits that you got to be very careful about.

2:33:44.720 --> 2:33:45.720
 That's right.

2:33:45.720 --> 2:33:50.760
 You have to have an understanding of that link because there's a short period of time

2:33:50.760 --> 2:33:56.120
 whether training is acquired or just mental transition or physical or whatever is acquired,

2:33:56.120 --> 2:33:59.360
 that's still going to be short term pain, the uncertainty of it.

2:33:59.360 --> 2:34:07.160
 There's families involved, you know, it's exceptionally is difficult on a human level

2:34:07.160 --> 2:34:09.840
 and you have to really think about that.

2:34:09.840 --> 2:34:13.120
 You can't just look at economic metrics always, it's human beings.

2:34:13.120 --> 2:34:14.120
 That's right.

2:34:14.120 --> 2:34:17.680
 And you can't even just take it as like, okay, well, we need to like subsidize or whatever

2:34:17.680 --> 2:34:23.240
 because like there is an element of just personal pride where majority of people, like people

2:34:23.240 --> 2:34:26.680
 don't want to just be okay, but like they want to actually like have a craft, like you

2:34:26.680 --> 2:34:30.960
 said, and have a mission and feel like they're having a really positive impact.

2:34:30.960 --> 2:34:37.080
 And so my personal belief is that there's a lot of transferability and skill set that

2:34:37.080 --> 2:34:43.680
 is possible, especially if you create a bridge and an investment to enable it.

2:34:43.680 --> 2:34:48.320
 And to some degree, that's our responsibility as well in this process.

2:34:48.320 --> 2:34:51.440
 You mentioned Kiva robots, Amazon.

2:34:51.440 --> 2:34:56.440
 Let me ask you about the Astro robot, which is, I don't know if you've seen it.

2:34:56.440 --> 2:35:04.200
 It's Amazon has announced that it's a home robot that they have a screen looks awfully

2:35:04.200 --> 2:35:10.240
 a lot like Cosmo has, I think different vision probably.

2:35:10.240 --> 2:35:13.240
 What are your thoughts about like home robotics in this kind of space?

2:35:13.240 --> 2:35:19.640
 There's been quite a bunch of home robots, social robots that very unfortunately have

2:35:19.640 --> 2:35:25.400
 closed their doors that for various reasons, perhaps were too expensive, there's manufacturing

2:35:25.400 --> 2:35:27.240
 challenges, all that kind of stuff.

2:35:27.240 --> 2:35:29.800
 What are your thoughts about Amazon getting into the space?

2:35:29.800 --> 2:35:34.200
 Yeah, we had some signs that they were getting into like long, long, long ago.

2:35:34.200 --> 2:35:39.000
 Maybe they were a little too interested in Cosmo and right during our conversations,

2:35:39.000 --> 2:35:42.760
 but they're also very good partners actually for us as we kind of just integrated a lot

2:35:42.760 --> 2:35:43.760
 of shared technology.

2:35:43.760 --> 2:35:51.560
 If I could also get your thoughts on, you could think of Alexa as a robot as well,

2:35:51.560 --> 2:35:52.560
 Echo.

2:35:52.560 --> 2:35:57.840
 Do you see those as fundamentally different just because you can move and look around?

2:35:57.840 --> 2:36:00.960
 Is that fundamentally different than the thing that just sits in place?

2:36:00.960 --> 2:36:08.240
 It opens up options, but my first reaction is I think, my doubts that this one's going

2:36:08.240 --> 2:36:12.760
 to hit the mark because I think for the price point that it's at and the kind of functionality

2:36:12.760 --> 2:36:17.560
 and value propositions that they're trying to put out, it's still searching for the

2:36:17.560 --> 2:36:22.160
 killer application that justifies, I think it was like a $1,500 price point or kind of

2:36:22.160 --> 2:36:23.160
 somewhere on there.

2:36:23.160 --> 2:36:25.600
 That's a really high bar.

2:36:25.600 --> 2:36:29.520
 There's enthusiasts, and early adopters will obviously kind of pursue it, but you have

2:36:29.520 --> 2:36:34.080
 to really hit a high mark at that price point, which we always tried to, we were always very

2:36:34.080 --> 2:36:37.880
 cautious about jumping too quickly to the more advanced systems that we really wanted

2:36:37.880 --> 2:36:42.960
 to make, but would have raised the bar so much that you have to be able to hit it in

2:36:42.960 --> 2:36:45.640
 today's cost structures and technologies.

2:36:45.640 --> 2:36:51.680
 The mobility is an angle that hasn't been utilized, but it has to be utilized in the

2:36:51.680 --> 2:36:52.680
 right way.

2:36:52.680 --> 2:36:57.960
 That's going to be the biggest challenge is, can you meet the bar of what the mass market

2:36:57.960 --> 2:37:05.680
 consumer, think like our neighbors, our friends, parents, would they find a deep, deep value

2:37:05.680 --> 2:37:10.400
 like in this at a mass scale that justifies the price point?

2:37:10.400 --> 2:37:13.760
 I think that's in the end one of the biggest challenges for robotics, especially consumer

2:37:13.760 --> 2:37:20.160
 robotics, where you have to kind of meet that bar, it becomes very, very hard.

2:37:20.160 --> 2:37:26.960
 There's also the higher bar, just like you were saying with Cosmo, a thing that can look

2:37:26.960 --> 2:37:33.440
 one way and then turn around and look at you, that's either a super desirable quality or

2:37:33.440 --> 2:37:38.280
 a super undesirable quality, depending on how much you trust the thing.

2:37:38.280 --> 2:37:41.560
 So there's a problem of trust to solve there.

2:37:41.560 --> 2:37:42.840
 There's a problem of personalities.

2:37:42.840 --> 2:37:48.080
 The thing that is the quote unquote problem that Cosmo solved so well is that you trust

2:37:48.080 --> 2:37:52.840
 the thing and that has to do with the company, with the leadership, with the intent that's

2:37:52.840 --> 2:37:56.280
 communicated by the device and the company and everything together.

2:37:56.280 --> 2:37:59.200
 Yeah, exactly right.

2:37:59.200 --> 2:38:03.480
 And I think they also have to retrace some of the like warnings on the character side,

2:38:03.480 --> 2:38:07.880
 as usual, I think that's the place where a lot of companies are great at the hardware

2:38:07.880 --> 2:38:12.120
 side of it and can think about those elements and then there's like the thinking about the

2:38:12.120 --> 2:38:16.320
 AI challenges, particularly with the advantage of Alexa is a pretty huge boost for them.

2:38:16.320 --> 2:38:21.720
 The character side of it for technology companies is pretty novel territory and so that will

2:38:21.720 --> 2:38:23.440
 take some iterations.

2:38:23.440 --> 2:38:28.440
 But yeah, I mean, I hope this continued progress in the space and that thread doesn't kind

2:38:28.440 --> 2:38:30.800
 of go dormant for too long.

2:38:30.800 --> 2:38:35.080
 And it's not, you know, it's going to take a while to kind of evolve into like the ideal

2:38:35.080 --> 2:38:36.240
 applications.

2:38:36.240 --> 2:38:42.000
 But you know, this is one of Amazon's, I guess you could call it, it's definitely like part

2:38:42.000 --> 2:38:45.800
 of their DNA, but in many cases it's also strength where they're very willing to like

2:38:45.800 --> 2:38:50.600
 iterate kind of aggressively and move quickly.

2:38:50.600 --> 2:38:51.600
 Take risks.

2:38:51.600 --> 2:38:52.600
 And take risks.

2:38:52.600 --> 2:38:53.600
 You have deep pockets so you can.

2:38:53.600 --> 2:38:54.600
 Yeah.

2:38:54.600 --> 2:38:56.840
 And then maybe have more misfires than an Apple would.

2:38:56.840 --> 2:38:59.720
 But you know, it's different styles and different approaches.

2:38:59.720 --> 2:39:04.640
 And you know, at the end of the day, it's like, there's a few familiar kind of elements

2:39:04.640 --> 2:39:12.640
 there for sure, which was, you know, kind of, you know, homage is one way to put it.

2:39:12.640 --> 2:39:22.040
 Why is it so hard at a high level to build a robotics company, a robotics company that

2:39:22.040 --> 2:39:23.200
 lives for a long time?

2:39:23.200 --> 2:39:29.520
 So if you look at, I thought Cosmo for sure would live for a very long time.

2:39:29.520 --> 2:39:34.480
 That to me was exceptionally successful vision and idea and implementation.

2:39:34.480 --> 2:39:42.680
 iRobot is an example of a company that has pivoted in all the right ways to survive and

2:39:42.680 --> 2:39:50.440
 arguably thrive by focusing on the, having like a, have a driver that constantly provides

2:39:50.440 --> 2:39:53.560
 profit, which is the vacuum cleaner.

2:39:53.560 --> 2:39:57.640
 And of course there's like Amazon, what they're, what they're doing is they're almost like

2:39:57.640 --> 2:40:02.680
 taking risks so they can afford it because they have other sources of revenue, right?

2:40:02.680 --> 2:40:07.880
 But outside of those examples, most robotics companies fail.

2:40:07.880 --> 2:40:08.880
 Yeah.

2:40:08.880 --> 2:40:10.280
 Why, why do they fail?

2:40:10.280 --> 2:40:12.520
 Why is it so hard to run a robotics company?

2:40:12.520 --> 2:40:18.120
 iRobot's impressive because they found a really, really great fit of where the technology

2:40:18.120 --> 2:40:21.680
 could satisfy a really clear use case and need.

2:40:21.680 --> 2:40:29.840
 And they did it well and they didn't try to overshoot from a cost to benefit standpoint.

2:40:29.840 --> 2:40:31.960
 Robotics is hard because it like tends to be more expensive.

2:40:31.960 --> 2:40:35.320
 It combines way more technologies than a lot of other types of companies do.

2:40:35.320 --> 2:40:40.600
 If I were to like say one thing that is maybe the biggest risk in like a robotics company

2:40:40.600 --> 2:40:49.000
 failing is that it can be either a technology in search of application or they try to bite

2:40:49.000 --> 2:40:56.280
 off a kind of an offering that has a mismatch in kind of price to function.

2:40:56.280 --> 2:41:02.000
 And just the mass market appeal isn't there and consumer products are just hard.

2:41:02.000 --> 2:41:06.720
 It's just, I mean, after all the years and it like definitely kind of feel a lot of the

2:41:06.720 --> 2:41:11.000
 battle scars because you have, you know, you not only do you have to like hit the function,

2:41:11.000 --> 2:41:15.480
 we have to educate and explain, get awareness up, deal with different conductive consumers.

2:41:15.480 --> 2:41:20.360
 Like, you know, there's a reason why a lot of technology sometimes start in the enterprise

2:41:20.360 --> 2:41:23.240
 space and then kind of continue forward in the consumer space.

2:41:23.240 --> 2:41:27.680
 Even like, you know, you see AR like starting to kind of make that shift with HoloLens and

2:41:27.680 --> 2:41:28.680
 so forth.

2:41:28.680 --> 2:41:33.480
 In some ways, consumers and price points that they're willing to kind of be attracted in

2:41:33.480 --> 2:41:37.040
 a mass market way, and I don't mean like, you know, 10,000 enthusiasts bought it, but

2:41:37.040 --> 2:41:44.720
 I mean like, you know, 2 million, 10 million, 50 million, like mass market kind of interest,

2:41:44.720 --> 2:41:46.720
 you know, have bought it.

2:41:46.720 --> 2:41:51.280
 That bar is very, very high and typically robotics is novel enough and nonstandardized

2:41:51.280 --> 2:41:55.080
 enough to where it pushes on price points so much that you can easily get out of range

2:41:55.080 --> 2:41:59.400
 where the capabilities in today's technology or just a function that was picked just doesn't

2:41:59.400 --> 2:42:00.800
 line up.

2:42:00.800 --> 2:42:03.520
 And so that product market fit is very important.

2:42:03.520 --> 2:42:09.640
 So the space of killer apps are rather super compelling apps is much smaller because it's

2:42:09.640 --> 2:42:11.880
 easy to get outside of the price range.

2:42:11.880 --> 2:42:12.880
 Yeah, and it's both consumers.

2:42:12.880 --> 2:42:14.560
 And it's not constant, right?

2:42:14.560 --> 2:42:20.200
 Like, and that's why like we picked off entertainment because the quality was just so low in physical

2:42:20.200 --> 2:42:24.640
 entertainment that we could, we thought we could leapfrog that and still create a really

2:42:24.640 --> 2:42:27.000
 compelling offering at a price point that was defensible.

2:42:27.000 --> 2:42:33.480
 And we, like that proved out to be true and over time, that same opportunity opens up

2:42:33.480 --> 2:42:39.560
 in healthcare, in home applications, in, you know, commercial applications and kind

2:42:39.560 --> 2:42:41.760
 of broader, more generalized interface.

2:42:41.760 --> 2:42:44.760
 But there's missing pieces in order for that to happen.

2:42:44.760 --> 2:42:47.280
 And all of those have to be present for it to line up.

2:42:47.280 --> 2:42:51.600
 And we see these sort of trends in technology where, you know, kind of technologies that

2:42:51.600 --> 2:42:57.040
 start in one place evolve and kind of grow to another, something started gaming, something

2:42:57.040 --> 2:43:03.680
 started in space or aerospace and then kind of move into the consumer market.

2:43:03.680 --> 2:43:05.440
 And sometimes it's just a timing thing, right?

2:43:05.440 --> 2:43:11.240
 Where how many stabs at what became the iPhone where they're over the 20 years before that

2:43:11.240 --> 2:43:17.040
 just weren't quite ready in the function relative to the kind of price point complexity.

2:43:17.040 --> 2:43:20.760
 And sometimes it's a small detail of the implementation that makes all the difference,

2:43:20.760 --> 2:43:23.840
 which is design is so important.

2:43:23.840 --> 2:43:27.080
 Something, yeah, like the new generation UX, right?

2:43:27.080 --> 2:43:28.080
 Yeah.

2:43:28.080 --> 2:43:33.200
 And that's, it's tough and oftentimes all of them have to be there and it has to be

2:43:33.200 --> 2:43:34.360
 like a perfect storm.

2:43:34.360 --> 2:43:38.680
 And, but yeah, history repeats itself in a lot of ways in a lot of these trends, which

2:43:38.680 --> 2:43:39.680
 is pretty fascinating.

2:43:39.680 --> 2:43:42.000
 Well, let me ask you about the humanoid form.

2:43:42.000 --> 2:43:46.400
 What do you think about the Tesla bot and humanoid robotics in general?

2:43:46.400 --> 2:43:52.240
 So obviously to me, autonomous driving Waymo and the other companies working in the space,

2:43:52.240 --> 2:43:57.640
 that seems to be a great place to invest in potential revolutionary application of robotics

2:43:57.640 --> 2:44:00.520
 application, folks application.

2:44:00.520 --> 2:44:02.840
 What's the role of humanoid robotics?

2:44:02.840 --> 2:44:05.840
 Do you think Tesla bot is ridiculous?

2:44:05.840 --> 2:44:08.080
 Do you think it's super promising?

2:44:08.080 --> 2:44:10.360
 Do you think it's interesting full of mystery?

2:44:10.360 --> 2:44:11.360
 Nobody knows.

2:44:11.360 --> 2:44:12.360
 What do you think about this thing?

2:44:12.360 --> 2:44:13.360
 Yeah.

2:44:13.360 --> 2:44:16.480
 So today, humanoid form robotics is research.

2:44:16.480 --> 2:44:20.520
 There's very few situations where you actually need a humanoid form to solve a problem.

2:44:20.520 --> 2:44:23.920
 If you think about it, right, like wheels are more efficient than legs.

2:44:23.920 --> 2:44:28.880
 There's joints and degrees of freedom beyond a certain point, just add a lot of complexity

2:44:28.880 --> 2:44:29.880
 and cost, right?

2:44:29.880 --> 2:44:33.560
 So if you're doing a humanoid robot, oftentimes it's in the pursuit of a humanoid robot, not

2:44:33.560 --> 2:44:38.040
 in the pursuit of an application for the time being, especially when you have like kind

2:44:38.040 --> 2:44:42.400
 of the gaps and interface and, you know, kind of AI that we kind of talk about today.

2:44:42.400 --> 2:44:46.680
 So anything Elon does, I'm interested in following, so there's an element of that.

2:44:46.680 --> 2:44:47.680
 No matter how crazy.

2:44:47.680 --> 2:44:48.680
 Yeah.

2:44:48.680 --> 2:44:50.600
 How crazy it is, I just like, you know, I'll pay attention and I'm curious to see what

2:44:50.600 --> 2:44:51.600
 comes out of it.

2:44:51.600 --> 2:44:56.400
 So it's like, you can't ever, you know, ignore it, but, you know, it's definitely far afield

2:44:56.400 --> 2:44:59.000
 from their kind of core business, obviously.

2:44:59.000 --> 2:45:09.080
 What was interesting to me is I've disagreed, you know, Elon, a lot about this is, to me,

2:45:09.080 --> 2:45:15.720
 the compelling aspect of the humanoid form and a lot of kind of robots, Cosmo, for example,

2:45:15.720 --> 2:45:20.400
 is the human robot interaction part.

2:45:20.400 --> 2:45:25.880
 From Elon Musk's perspective, the Tesla bot has nothing to do with the human.

2:45:25.880 --> 2:45:31.720
 It's a form that's effective for the factory because the factory is designed for humans.

2:45:31.720 --> 2:45:37.320
 But to me, the reason you might argue for the humanoid form is because, you know, at

2:45:37.320 --> 2:45:41.440
 a party, it's a nice way to fit into the party.

2:45:41.440 --> 2:45:46.800
 The humanoid form has a compelling notion to it in the same way that Cosmo is compelling.

2:45:46.800 --> 2:45:54.240
 I would argue, if we're arguing about this, that it's cheaper to build a Cosmo like that

2:45:54.240 --> 2:45:55.240
 form.

2:45:55.240 --> 2:45:58.880
 But if you wanted to make an argument, which I have with Jim Keller about, you know, you

2:45:58.880 --> 2:46:03.400
 could actually make a human robot for pretty cheap, it's possible.

2:46:03.400 --> 2:46:10.280
 And then the question is, all right, if you're using an application where it can be flawed,

2:46:10.280 --> 2:46:14.800
 it can have a personality and be flawed in the same way that Cosmo is, then maybe it's

2:46:14.800 --> 2:46:18.320
 interesting for integration to human society.

2:46:18.320 --> 2:46:21.280
 That's to me, is an interesting application of a humanoid form.

2:46:21.280 --> 2:46:25.080
 Because humans are drawn, like I mentioned to you, legged robots, we're drawn to legs

2:46:25.080 --> 2:46:28.840
 and limbs and body language and all that kind of stuff.

2:46:28.840 --> 2:46:32.120
 And even a face, even if you don't have the facial features, which you might now want

2:46:32.120 --> 2:46:38.320
 to have to reduce the creepiness factor, all that kind of stuff.

2:46:38.320 --> 2:46:40.560
 But yeah, that to me, the humanoid form is compelling.

2:46:40.560 --> 2:46:46.360
 But in terms of that being the right form for the factory environment, I'm not so sure.

2:46:46.360 --> 2:46:47.360
 Yeah.

2:46:47.360 --> 2:46:51.080
 For the factory environment, like right off the bat, what are you optimizing for?

2:46:51.080 --> 2:46:52.080
 Is it strength?

2:46:52.080 --> 2:46:53.080
 Is it mobility?

2:46:53.080 --> 2:46:54.080
 Is it versatility?

2:46:54.080 --> 2:46:55.080
 Right?

2:46:55.080 --> 2:46:56.800
 Like that change is completely the look and feel of the robot that you create.

2:46:56.800 --> 2:47:02.880
 And almost certainly the human form is over designed for some dimensions and constrained

2:47:02.880 --> 2:47:03.880
 for some dimensions.

2:47:03.880 --> 2:47:06.240
 And so like, what are you grasping?

2:47:06.240 --> 2:47:07.240
 Is it big?

2:47:07.240 --> 2:47:08.240
 Is it little?

2:47:08.240 --> 2:47:09.240
 Right?

2:47:09.240 --> 2:47:14.640
 So you would customize it and make it customizable for the different needs if that was the optimization.

2:47:14.640 --> 2:47:18.560
 And then for the other one, I could totally be wrong.

2:47:18.560 --> 2:47:24.560
 I still feel that the closer you try to get to a human, the more you're subject to the

2:47:24.560 --> 2:47:30.800
 biases of what a human should be, and you lose flexibility to shift away from your weaknesses

2:47:30.800 --> 2:47:33.120
 and towards your strengths.

2:47:33.120 --> 2:47:42.880
 And that changes over time, but there's ways to make really approachable and natural interfaces

2:47:42.880 --> 2:47:53.920
 for robotic kind of characters and kind of deployments in these applications that do

2:47:53.920 --> 2:47:56.520
 not at all look like a human directly.

2:47:56.520 --> 2:48:01.520
 But that actually creates way more flexibility and capability and role and forgiveness and

2:48:01.520 --> 2:48:02.880
 interface and everything else.

2:48:02.880 --> 2:48:04.120
 Yeah, it's interesting.

2:48:04.120 --> 2:48:09.040
 But I'm still confused by the magic I see in legged robots.

2:48:09.040 --> 2:48:10.040
 Yeah.

2:48:10.040 --> 2:48:11.040
 So there is a magic.

2:48:11.040 --> 2:48:18.440
 So I'm absolutely amazed at it from a technical curiosity standpoint and like the magic that

2:48:18.440 --> 2:48:24.560
 the Boston Dynamics team can do from walking and jumping and so forth.

2:48:24.560 --> 2:48:29.880
 Now, there's been a long journey to try to find an application for that sort of technology,

2:48:29.880 --> 2:48:32.200
 but wow, that's incredible technology, right?

2:48:32.200 --> 2:48:36.440
 So then you kind of go towards, okay, are you working back from a goal of what you're

2:48:36.440 --> 2:48:37.440
 trying to solve?

2:48:37.440 --> 2:48:39.720
 Are you working forward from a technology and then looking for a solution?

2:48:39.720 --> 2:48:45.000
 And I think that's where it's kind of a bi directional search oftentimes, but the two have

2:48:45.000 --> 2:48:46.000
 to meet.

2:48:46.000 --> 2:48:51.000
 And that's where humanoid robots is kind of close to that in that like it is a decision

2:48:51.000 --> 2:48:57.760
 about a form factor and a technology that it forces that doesn't have a clear justification

2:48:57.760 --> 2:49:00.120
 on why that's the killer app from the other end.

2:49:00.120 --> 2:49:04.080
 But I think the core fascinating idea with the Tesla bot is the one that's carried by

2:49:04.080 --> 2:49:11.480
 Waymo as well is when you're solving the general robotics problem of perception control, there's

2:49:11.480 --> 2:49:17.080
 a very clear applications of driving, it's as you get better and better at it, when you

2:49:17.080 --> 2:49:23.760
 have like Waymo driver, the whole world starts to kind of start to look like a robotics problem.

2:49:23.760 --> 2:49:24.760
 So it's very interesting.

2:49:24.760 --> 2:49:25.760
 For now, your folks.

2:49:25.760 --> 2:49:31.200
 Detection, costification, segmentation, tracking, planning, like it's, yeah.

2:49:31.200 --> 2:49:39.760
 So there's no reason, I mean, I'm not speaking for Waymo here, but moving goods, there's

2:49:39.760 --> 2:49:50.040
 no reason transformer like this thing couldn't take the goods up an elevator, like that slowly

2:49:50.040 --> 2:49:58.280
 expand what it means to move goods and expand more and more of the world into a robotics

2:49:58.280 --> 2:49:59.280
 problem.

2:49:59.280 --> 2:50:00.280
 Well, that's right.

2:50:00.280 --> 2:50:05.560
 And you start to think of it as an end to end robotics problem from loading from everything.

2:50:05.560 --> 2:50:12.080
 And even like the truck itself, today's generation is integrating into today's understanding of

2:50:12.080 --> 2:50:17.800
 what a vehicle is, a Pacific Jaguar, the freight liners from Daimler.

2:50:17.800 --> 2:50:25.160
 There's nothing that stops us from down the road after starting to get to scale to expand

2:50:25.160 --> 2:50:29.120
 these partnerships to really rethink what would the next generation of a truck look

2:50:29.120 --> 2:50:34.720
 like that is actually optimized for autonomy, not for today's world.

2:50:34.720 --> 2:50:37.760
 And maybe that means a very different type of trailer.

2:50:37.760 --> 2:50:41.240
 Maybe that, like there's a lot of things you could rethink on that front, which is on its

2:50:41.240 --> 2:50:42.800
 own very, very exciting.

2:50:42.800 --> 2:50:47.840
 Let me ask you, like I said, you went to the mecca of robotics, which is CMU, Carnegie

2:50:47.840 --> 2:50:50.880
 Mellon University, you got a PhD there.

2:50:50.880 --> 2:50:58.640
 So maybe by way of advice and maybe by way of story and memories, what does it take to

2:50:58.640 --> 2:51:03.880
 get a PhD in robotics at CMU?

2:51:03.880 --> 2:51:10.480
 And maybe you can throw in there some advice for people who are thinking about doing work

2:51:10.480 --> 2:51:14.640
 in artificial intelligence and robotics and are thinking about whether to get a PhD.

2:51:14.640 --> 2:51:19.120
 So I asked you, I was at CMU for undergrad as well and didn't know anything about robotics

2:51:19.120 --> 2:51:23.840
 coming in and was doing electrical computer engineering, computer science and really

2:51:23.840 --> 2:51:26.200
 got more and more into kind of AI.

2:51:26.200 --> 2:51:29.680
 And then fell in love with autonomous driving and at that point, like that was just by a

2:51:29.680 --> 2:51:36.760
 big margin, such an incredible central spot of investment in that area.

2:51:36.760 --> 2:51:40.560
 And so what I would say is that robotics, for all the progress that's happened, is still

2:51:40.560 --> 2:51:41.560
 a really young field.

2:51:41.560 --> 2:51:43.280
 There's a huge amount of opportunity.

2:51:43.280 --> 2:51:47.480
 Now that opportunity shifted where something like autonomous driving has moved from being

2:51:47.480 --> 2:51:51.920
 very research and academics driven to being commercial driven where you see the investments

2:51:51.920 --> 2:51:53.720
 happening in commercial.

2:51:53.720 --> 2:51:58.880
 Now there's other areas that are much younger and you see kind of grasping and impulation

2:51:58.880 --> 2:52:02.680
 and making kind of the same sort of journey that like autonomy made and there's other

2:52:02.680 --> 2:52:03.880
 areas as well.

2:52:03.880 --> 2:52:07.360
 What I would say is the space moves very quickly.

2:52:07.360 --> 2:52:11.720
 Anything you do a PhD in like it is in most areas will evolve and change as technology

2:52:11.720 --> 2:52:16.120
 changes and constraints change and hardware changes and the world changes.

2:52:16.120 --> 2:52:19.280
 And so the beautiful thing about robotics is it's super broad.

2:52:19.280 --> 2:52:22.920
 It's not a narrow space at all and it could be a million different things in a million

2:52:22.920 --> 2:52:24.180
 different industries.

2:52:24.180 --> 2:52:30.080
 And so it's a great opportunity to come in and get a broad foundation on AI, machine

2:52:30.080 --> 2:52:35.040
 learning, computer vision, systems, hardware, sensors, all these different separate things.

2:52:35.040 --> 2:52:39.760
 You do need to like go deep and find something that you're like really, really passionate

2:52:39.760 --> 2:52:42.560
 about obviously like just like any PhD.

2:52:42.560 --> 2:52:47.520
 This is like a five, six year kind of endeavor.

2:52:47.520 --> 2:52:52.680
 And you have to love it enough to go super deep to learn all the things necessary to

2:52:52.680 --> 2:52:56.320
 be super deeply functioning in that area and then contribute to it in a way that hasn't

2:52:56.320 --> 2:52:57.320
 been done before.

2:52:57.320 --> 2:53:02.640
 And in robotics, that probably means more breadth because robotics is rarely kind of

2:53:02.640 --> 2:53:05.840
 like one particular kind of narrow technology.

2:53:05.840 --> 2:53:09.520
 And it means being able to collaborate with teams where like one of the coolest aspects

2:53:09.520 --> 2:53:15.200
 of like the experience that I kind of cherish in our PhD is that we actually had a pretty

2:53:15.200 --> 2:53:21.080
 large AV project that for that time was like a pretty serious initiative where you got

2:53:21.080 --> 2:53:25.440
 to like partner with a larger team and you had the experts in perception and the experts

2:53:25.440 --> 2:53:27.840
 in planning and the staff and the mechanical engineers.

2:53:27.840 --> 2:53:28.840
 For the DARPA challenge.

2:53:28.840 --> 2:53:34.120
 So I was working on a project called UPI back then, which was basically the off road version

2:53:34.120 --> 2:53:35.120
 of the DARPA challenge.

2:53:35.120 --> 2:53:39.120
 It was a DARPA funded project for basically like a large off road vehicle that you would

2:53:39.120 --> 2:53:43.240
 like drop and then give it a way point 10 kilometers away and it would have to navigate

2:53:43.240 --> 2:53:45.960
 a complete instructor in an off road environment.

2:53:45.960 --> 2:53:50.000
 So like forest, ditches, rocks, vegetation and so it was like a really, really interesting

2:53:50.000 --> 2:53:53.880
 kind of a hard problem where like wheels would be up to my shoulders is like gigantic, right?

2:53:53.880 --> 2:53:54.880
 Yeah.

2:53:54.880 --> 2:53:56.680
 By the way, AV for people stands for autonomous vehicles.

2:53:56.680 --> 2:53:57.680
 Autonomous vehicles.

2:53:57.680 --> 2:53:58.680
 Yeah.

2:53:58.680 --> 2:53:59.680
 Sorry.

2:53:59.680 --> 2:54:02.920
 And so what I think is like the beauty of robotics, but also kind of like the expectation

2:54:02.920 --> 2:54:09.600
 is that there's spaces in computer science where you can be very, very narrow and deep.

2:54:09.600 --> 2:54:14.120
 Robotics one of the necessity, but also the beauty of it is that it forces you to be excited

2:54:14.120 --> 2:54:18.540
 about that breadth and that partnership across different disciplines that enable it.

2:54:18.540 --> 2:54:23.040
 But that also opens up so many more doors where you can go and you can do robotics in

2:54:23.040 --> 2:54:27.400
 almost any category where robotics isn't really an industry.

2:54:27.400 --> 2:54:29.200
 It's like AI, right?

2:54:29.200 --> 2:54:33.960
 It's like the application of physical automation to all these other worlds.

2:54:33.960 --> 2:54:35.360
 And so you can do robotic surgery.

2:54:35.360 --> 2:54:37.080
 You can do vehicles.

2:54:37.080 --> 2:54:38.680
 You can do factory automation.

2:54:38.680 --> 2:54:45.840
 You can do healthcare or you can do like leverage the AI around the sensing to think about static

2:54:45.840 --> 2:54:47.480
 sensors and seen understanding.

2:54:47.480 --> 2:54:53.360
 So I think that's got to be the expectation and the excitement and it breeds people.

2:54:53.360 --> 2:54:58.640
 They're probably a little bit more collaborative and more excited about working in teams.

2:54:58.640 --> 2:55:05.440
 If I could briefly comment on the fact that the robotics people I've met in my life from

2:55:05.440 --> 2:55:10.740
 CMU and MIT, they're really happy people.

2:55:10.740 --> 2:55:13.080
 Because I think it's the collaborative thing.

2:55:13.080 --> 2:55:15.080
 I think you don't...

2:55:15.080 --> 2:55:19.360
 You're not like sitting in like the fourth basement.

2:55:19.360 --> 2:55:21.360
 Yes, exactly.

2:55:21.360 --> 2:55:25.880
 When you're doing machine learning purely software, it's very tempting to just disappear

2:55:25.880 --> 2:55:29.800
 into your own hole and never collaborate.

2:55:29.800 --> 2:55:36.320
 And that breeds a little bit more of the silo mentality of like, I have a problem.

2:55:36.320 --> 2:55:39.480
 It's almost like negative to talk to somebody else or something like that.

2:55:39.480 --> 2:55:43.800
 But robotics folks are just very collaborative, very friendly.

2:55:43.800 --> 2:55:49.160
 And there's also an energy of like, you get to confront the physics of reality often,

2:55:49.160 --> 2:55:53.560
 which is humbling and also exciting.

2:55:53.560 --> 2:55:56.960
 So it's humbling when it fails and exciting when it finally works.

2:55:56.960 --> 2:55:57.960
 It's like a purity of the passion.

2:55:57.960 --> 2:56:03.240
 And you got to remember that like right now, robotics and AI is like just all the rage

2:56:03.240 --> 2:56:11.000
 and autonomous vehicles and all this, 15 years ago and 20 years ago, it wasn't that deeply

2:56:11.000 --> 2:56:14.160
 lucrative, people that went into robotics, they did it because they were like, thought

2:56:14.160 --> 2:56:17.840
 it was just the coolest thing in the world to like make physical things intelligent in

2:56:17.840 --> 2:56:18.840
 the real world.

2:56:18.840 --> 2:56:22.200
 And so there's like a raw passion where they went into it for the right reasons and so

2:56:22.200 --> 2:56:23.200
 forth.

2:56:23.200 --> 2:56:24.200
 So it's really great space.

2:56:24.200 --> 2:56:27.640
 And that organizational challenge, by the way, like when you think about the challenges

2:56:27.640 --> 2:56:32.160
 in AV, we talk a lot about the technical challenges, the organizational challenges

2:56:32.160 --> 2:56:39.160
 through the roof where you think about the challenge, what it takes to build an AV system

2:56:39.160 --> 2:56:42.600
 and you have companies that are now thousands of people.

2:56:42.600 --> 2:56:47.400
 And you look at other really hard technical problems like an operating system, it's pretty

2:56:47.400 --> 2:56:48.400
 well established.

2:56:48.400 --> 2:56:53.000
 Like you kind of know that there's a file system, there's virtual memory, there's this,

2:56:53.000 --> 2:56:58.120
 there's that, there's like caching and like, and there's like a really reasonably well

2:56:58.120 --> 2:57:00.320
 established modularity and APIs and so forth.

2:57:00.320 --> 2:57:03.400
 And so you can kind of like scale it in an efficient fashion.

2:57:03.400 --> 2:57:09.040
 That doesn't exist anywhere near to that level of maturity in autonomous driving right now.

2:57:09.040 --> 2:57:12.960
 And text acts are being reinvented, organizational structures are being reinvented.

2:57:12.960 --> 2:57:15.760
 You have problems like pedestrians that are not isolated problems.

2:57:15.760 --> 2:57:20.680
 They're part sensing, part behavior prediction, part planning, part evaluation.

2:57:20.680 --> 2:57:25.360
 And like one of the biggest challenges is actually how do you solve these problems where

2:57:25.360 --> 2:57:29.640
 the mental capacity of a human is starting to get strained on how do you organize it

2:57:29.640 --> 2:57:35.680
 and think about it where, you know, you have this like multi dimensional matrix that needs

2:57:35.680 --> 2:57:36.960
 to all work together.

2:57:36.960 --> 2:57:43.400
 And so that makes it kind of cool as well, because it's not like solved at all from,

2:57:43.400 --> 2:57:46.120
 you know, like, what does it take to actually scale this, right?

2:57:46.120 --> 2:57:50.840
 And then you look at like other gigantic challenges that have, you know, that have been successful

2:57:50.840 --> 2:57:52.560
 and are way more mature.

2:57:52.560 --> 2:57:53.560
 There's a stability to it.

2:57:53.560 --> 2:57:56.720
 And like maybe the autonomous vehicle space will get there.

2:57:56.720 --> 2:58:01.360
 But right now, just as many technical challenges as they are, they're like organizational challenges

2:58:01.360 --> 2:58:06.040
 on how do you like solve these problems that touch on so many different areas and efficiently

2:58:06.040 --> 2:58:13.760
 tackle them while like maintaining progress among all these constraints while scaling.

2:58:13.760 --> 2:58:21.760
 By way of advice, what advice would you give to somebody thinking about doing a robotic

2:58:21.760 --> 2:58:22.760
 startup?

2:58:22.760 --> 2:58:27.320
 You mentioned Cosmo, somebody that wanted to carry the Cosmo flag forward, the Anki flag

2:58:27.320 --> 2:58:34.200
 forward, looking back at your experience, looking forward at a future that will obviously

2:58:34.200 --> 2:58:35.920
 have such robots.

2:58:35.920 --> 2:58:37.480
 What advice would you give to that person?

2:58:37.480 --> 2:58:39.480
 Yeah, it was the greatest experience ever.

2:58:39.480 --> 2:58:44.000
 And it's like, there's something you, there are things you learn navigating a startup

2:58:44.000 --> 2:58:48.040
 that you'll never like, you, it was very hard to encounter that in like a typical kind of

2:58:48.040 --> 2:58:49.040
 work environment.

2:58:49.040 --> 2:58:51.000
 And, and it's just, it's, it's wonderful.

2:58:51.000 --> 2:58:52.000
 You got to be ready for it.

2:58:52.000 --> 2:58:55.920
 It's not as like, you know, the, the glamour of a startup, there's just like, just brutal

2:58:55.920 --> 2:58:57.280
 emotional swings up and down.

2:58:57.280 --> 2:59:00.320
 And so having co founders actually helps a ton.

2:59:00.320 --> 2:59:05.200
 Like I would not, cannot imagine doing it solo, but having at least somebody where on your

2:59:05.200 --> 2:59:09.720
 darkest days, you can kind of like really openly just like have that conversation and, you

2:59:09.720 --> 2:59:13.960
 know, lean on to somebody that's, that's in the thick of it with you helps a lot.

2:59:13.960 --> 2:59:14.960
 What I would say,

2:59:14.960 --> 2:59:19.240
 What was the nature of darkest days and the emotional swings?

2:59:19.240 --> 2:59:20.960
 Is it worried about the funding?

2:59:20.960 --> 2:59:26.160
 Is it worried about what the, any of your ideas are any good or ever were good?

2:59:26.160 --> 2:59:28.560
 Is it like the self doubt?

2:59:28.560 --> 2:59:34.440
 Is it like facing new challenges that have nothing to do with the technology, like organizational

2:59:34.440 --> 2:59:36.480
 human resources, that kind of stuff?

2:59:36.480 --> 2:59:37.480
 What?

2:59:37.480 --> 2:59:38.480
 Yeah.

2:59:38.480 --> 2:59:42.320
 You come from a world in school where you feel that you put in a lot of effort and you'll

2:59:42.320 --> 2:59:46.640
 get the right result and input translates proportional to output.

2:59:46.640 --> 2:59:50.320
 And, you know, you need to solve, solve the set or do whatever and just kind of get it

2:59:50.320 --> 2:59:51.320
 done.

2:59:51.320 --> 2:59:54.080
 Now, PhD tests out a little bit, but at the end of the day, you put in the effort, you

2:59:54.080 --> 3:00:00.200
 tend to like kind of come out with your enough results to you kind of get a PhD in the startup

3:00:00.200 --> 3:00:04.520
 space, like, you know, like you could talk to 50 investors and they just don't see your

3:00:04.520 --> 3:00:07.240
 vision and it doesn't matter how hard you kind of tried and pitched.

3:00:07.240 --> 3:00:10.760
 You could work incredibly hard and you have a manufacturing defect.

3:00:10.760 --> 3:00:14.160
 And if you don't fix it, you're going to, you're out of business.

3:00:14.160 --> 3:00:18.520
 You need to raise money by a certain date and there's a, you got to have this milestone

3:00:18.520 --> 3:00:21.000
 in order to like have a good pitch and you do it.

3:00:21.000 --> 3:00:25.780
 You have to have this talent and you just don't have it inside the company or, you know,

3:00:25.780 --> 3:00:30.960
 you have to get 200 people or however many people kind of like along with you and kind

3:00:30.960 --> 3:00:33.320
 of buy in the journey.

3:00:33.320 --> 3:00:36.080
 You're like disagreeing with an investor and they're your investors.

3:00:36.080 --> 3:00:39.600
 So it's just like, you know, it's like you, there's no walking away from it, right?

3:00:39.600 --> 3:00:44.040
 So, and it tends to be like those things where you just kind of get clobbered in so many

3:00:44.040 --> 3:00:47.520
 different ways that like things end up being harder than you expect.

3:00:47.520 --> 3:00:52.040
 And it's like such a gauntlet, but you learn so much in the process and there's a lot of

3:00:52.040 --> 3:00:55.560
 people that actually end up rooting for you and helping you like from the outside and

3:00:55.560 --> 3:00:59.840
 you get good, great mentors and you like get fine, fantastic people that step up in the

3:00:59.840 --> 3:01:05.560
 company and you have this like magical period where everybody's like, it's life or death

3:01:05.560 --> 3:01:08.880
 for the company, but like you're all fighting for the same thing and it's the most satisfying

3:01:08.880 --> 3:01:10.880
 kind of journey ever.

3:01:10.880 --> 3:01:14.680
 The things that make it easier and that I would recommend is like be really, really thoughtful

3:01:14.680 --> 3:01:17.000
 about the, the application.

3:01:17.000 --> 3:01:21.760
 Like there's a, there's a saying of like kind of, you know, team and execution and market

3:01:21.760 --> 3:01:24.640
 and like kind of how important are each of those.

3:01:24.640 --> 3:01:29.000
 And oftentimes the market wins and you come at it thinking that if you're smart enough

3:01:29.000 --> 3:01:32.560
 and you work hard enough and you're like, have the right talented team and so forth,

3:01:32.560 --> 3:01:35.120
 like you'll always kind of find a way through.

3:01:35.120 --> 3:01:39.600
 And it's surprising how much dynamics are driven by the industry you're in and the timing

3:01:39.600 --> 3:01:41.640
 of you entering that industry.

3:01:41.640 --> 3:01:44.760
 And so just Waymo is a great example of it.

3:01:44.760 --> 3:01:49.240
 There is the, I don't know if they'll ever be another company or, or suite of companies

3:01:49.240 --> 3:01:57.240
 that has raised and continues to spend so much money at such an early phase of revenue

3:01:57.240 --> 3:02:04.520
 generation and product, productization, you know, from a PNL standpoint, like it's, it's

3:02:04.520 --> 3:02:10.000
 a anomaly, like by any measure of any industry that's ever existed, except for maybe the

3:02:10.000 --> 3:02:17.520
 US space program, like, right, like, but it's like a multiple trillion dollar opportunities,

3:02:17.520 --> 3:02:22.920
 which is so unusual to find that size of a market that just the progress that shows

3:02:22.920 --> 3:02:26.360
 that you're risking of it, you could apply whatever discounts you want off that trillion

3:02:26.360 --> 3:02:30.840
 dollar market and still justifies the investment that is happening because like being successful

3:02:30.840 --> 3:02:34.000
 in that space makes all the investment feel trivial.

3:02:34.000 --> 3:02:38.720
 Now by the same consequence, like the size of the market, the size of the target audience,

3:02:38.720 --> 3:02:42.560
 the ability to capture that market share, how hard that's going to be, who the incumbents

3:02:42.560 --> 3:02:46.080
 like, that's probably one of the lessons I appreciate, like more than anything else

3:02:46.080 --> 3:02:48.960
 where like those things really, really do matter.

3:02:48.960 --> 3:02:54.880
 And oftentimes can dominate the quality of the team or execution, because if you miss

3:02:54.880 --> 3:02:58.560
 the timing or you do it in the wrong space, you run into like the institutional kind of

3:02:58.560 --> 3:03:02.520
 headwinds of a particular environment, like, let's say you have the greatest idea in the

3:03:02.520 --> 3:03:05.680
 world, but you barrel into healthcare, but it takes 10 years to innovate in healthcare

3:03:05.680 --> 3:03:07.360
 because of a lot of challenges, right?

3:03:07.360 --> 3:03:12.240
 Like there's fundamental laws of physics that you have to think about.

3:03:12.240 --> 3:03:17.480
 And so the combination of like Anki and Waymo kind of drives that point home for me where

3:03:17.480 --> 3:03:21.600
 you can do a ton if you have the right market, the right opportunity, the right way to explain

3:03:21.600 --> 3:03:27.560
 it, and you show the progress in the right sequence, it actually can really significantly

3:03:27.560 --> 3:03:30.440
 change the course of your journey and startup.

3:03:30.440 --> 3:03:34.640
 How much of is understanding the market and how much of is creating a new market?

3:03:34.640 --> 3:03:41.240
 So how do you think about like the space robotics is really interesting, you said exactly right.

3:03:41.240 --> 3:03:47.560
 The space of applications is small, relative to the cost involved.

3:03:47.560 --> 3:03:55.040
 So how much is like truly revolutionary thinking about like what is the application?

3:03:55.040 --> 3:04:02.160
 And then yeah, but so like creating something that didn't really exist, like this is pretty

3:04:02.160 --> 3:04:07.520
 obvious to me, the whole space of home robotics, just everything that Cosmo did, I guess you

3:04:07.520 --> 3:04:11.520
 could talk to it as a toy and people will understand it because it was much more than

3:04:11.520 --> 3:04:13.640
 a toy.

3:04:13.640 --> 3:04:19.360
 And I don't think people fully understand the value of that, you have to create it and

3:04:19.360 --> 3:04:24.720
 the product will communicate it, like just like the iPhone, nobody understood the value

3:04:24.720 --> 3:04:31.040
 of no keyboard and a thing that can do web browsing.

3:04:31.040 --> 3:04:34.200
 I don't think they understood the value of that until you create it.

3:04:34.200 --> 3:04:37.880
 Yeah, having a foot in a door and an entry point still helps because at the end of the

3:04:37.880 --> 3:04:42.160
 day, like an iPhone replaced your phone and so it had a fundamental purpose and all these

3:04:42.160 --> 3:04:43.760
 things did it did better, right?

3:04:43.760 --> 3:04:48.720
 And so then you could do ABC on top of it and then like you even remember the early commercials

3:04:48.720 --> 3:04:52.120
 where there's always like one application of what it could do and then you get a phone

3:04:52.120 --> 3:04:53.120
 call.

3:04:53.120 --> 3:04:57.200
 And so that was intentionally sending a message, something familiar, but then like you can

3:04:57.200 --> 3:05:00.040
 send a text message, you can listen to music, you can surf the web, right?

3:05:00.040 --> 3:05:03.920
 And so, you know, autonomous driving obviously anchors on that as well.

3:05:03.920 --> 3:05:07.360
 You don't have to explain to somebody the functionality of an autonomous truck, right?

3:05:07.360 --> 3:05:10.960
 Like there's nuances around it, but the functionality makes sense.

3:05:10.960 --> 3:05:14.840
 In the home, you have a fundamental advantage like we always thought about this because

3:05:14.840 --> 3:05:18.800
 it was so painful to explain to people what our products did and how I got to communicate

3:05:18.800 --> 3:05:22.640
 that super cleanly, especially when something was so experiential.

3:05:22.640 --> 3:05:33.460
 And so you compare like Anki to Nest, Nest had some beautiful products where they started

3:05:33.460 --> 3:05:36.340
 scaling and like actually find like really great success.

3:05:36.340 --> 3:05:40.400
 And they had like really clean and beautiful marketing messaging because they anchored

3:05:40.400 --> 3:05:45.480
 on reinventing existing categories where it was a smart thermostat, right?

3:05:45.480 --> 3:05:51.280
 And so you kind of are able to take what's familiar, anchor that understanding and then

3:05:51.280 --> 3:05:53.480
 explain what's better about it.

3:05:53.480 --> 3:05:54.480
 That's funny.

3:05:54.480 --> 3:05:55.480
 You're right.

3:05:55.480 --> 3:05:56.480
 Cosmo is like totally new thing.

3:05:56.480 --> 3:05:57.840
 Like what is this thing?

3:05:57.840 --> 3:05:58.840
 It's we struggle.

3:05:58.840 --> 3:06:01.080
 We spent like a lot of money on marketing.

3:06:01.080 --> 3:06:02.080
 We had a heart.

3:06:02.080 --> 3:06:07.000
 Like we actually had far greater efficiency on Cosmo than anything else because we found

3:06:07.000 --> 3:06:10.800
 a way to capture the emotion in some little shorts to kind of lean into the personality

3:06:10.800 --> 3:06:12.040
 in our marketing.

3:06:12.040 --> 3:06:16.320
 And it became viral where like we had these kind of videos that would like go and get

3:06:16.320 --> 3:06:20.200
 like hundreds of thousands of views and like kind of like get spread and sometimes millions

3:06:20.200 --> 3:06:21.200
 of views.

3:06:21.200 --> 3:06:24.120
 And so, but it was like really, really hard.

3:06:24.120 --> 3:06:28.320
 And so finding a way to kind of like anchor on something that's familiar, but then grow

3:06:28.320 --> 3:06:33.240
 into something that's not is an advantage, but then again, like, you don't have like

3:06:33.240 --> 3:06:37.360
 this success is otherwise like a Lexa never had a comp, right?

3:06:37.360 --> 3:06:40.160
 You could argue that that's very novel and very new.

3:06:40.160 --> 3:06:45.640
 And there's a lot of other examples that kind of created a kind of a category out of like

3:06:45.640 --> 3:06:46.640
 Kiva systems.

3:06:46.640 --> 3:06:53.000
 I mean, they like came in and they like enterprises is a little easier because if you can, it's

3:06:53.000 --> 3:06:57.080
 less susceptible to this because if you can argue a queer value proposition, it's a more

3:06:57.080 --> 3:07:00.880
 logical conversation that you can have with customers.

3:07:00.880 --> 3:07:04.680
 It's not it's a little bit less emotional and kind of subjective, but

3:07:04.680 --> 3:07:06.680
 Yeah, in the home, you have to

3:07:06.680 --> 3:07:07.680
 Yeah.

3:07:07.680 --> 3:07:09.600
 So like a home robot, it's like, what does that mean?

3:07:09.600 --> 3:07:14.120
 And so then you really have to be crisp about the value proposition and what like really

3:07:14.120 --> 3:07:15.760
 makes it worth it.

3:07:15.760 --> 3:07:19.960
 And we by the way, went to that same one and we almost like, we almost hit a wall coming

3:07:19.960 --> 3:07:25.880
 out of 2013 where we were so big on explaining why our stuff was so high tech and all the

3:07:25.880 --> 3:07:30.080
 kind of like great technology in it and how cool it is and so forth to having to make

3:07:30.080 --> 3:07:36.440
 a super hard pivot on why is it fun and why does the random kind of family of, you know,

3:07:36.440 --> 3:07:37.440
 for need this, right?

3:07:37.440 --> 3:07:38.440
 Yeah.

3:07:38.440 --> 3:07:43.480
 So it's learnings, but that's that's the challenge and I think like robotics tends to sometimes

3:07:43.480 --> 3:07:48.960
 fall into the new category problem, but then you got to be really crisp about why it needs

3:07:48.960 --> 3:07:49.960
 to exist.

3:07:49.960 --> 3:07:56.400
 So some of robotics, depending on the category, depending on the application is a little bit

3:07:56.400 --> 3:07:59.800
 of a marketing challenge.

3:07:59.800 --> 3:08:06.920
 And I don't mean, I mean, it's the kind of marketing that Waymo is doing, that Tesla

3:08:06.920 --> 3:08:15.160
 is doing is like showing off incredible engineering, incredible technology, but convincing, like

3:08:15.160 --> 3:08:20.360
 you said, family for this, this is like, this is transformative for your life.

3:08:20.360 --> 3:08:21.600
 This is fun.

3:08:21.600 --> 3:08:22.600
 This is easy.

3:08:22.600 --> 3:08:23.600
 They don't care how much tech is in your thing.

3:08:23.600 --> 3:08:24.600
 They don't care.

3:08:24.600 --> 3:08:25.600
 They don't care.

3:08:25.600 --> 3:08:26.600
 They need to know why they want it.

3:08:26.600 --> 3:08:27.600
 And some of that is just marketing.

3:08:27.600 --> 3:08:28.600
 Yeah.

3:08:28.600 --> 3:08:34.080
 And that's like Roomba, like, yes, they didn't, you know, like go and, you know, have this

3:08:34.080 --> 3:08:39.440
 like, you know, huge, huge ramp into like the entirety of kind of a robotics and so forth,

3:08:39.440 --> 3:08:44.200
 but like, they built a really great business in, in a vacuum cleaner world and like everybody

3:08:44.200 --> 3:08:46.620
 understands where a vacuum cleaner is.

3:08:46.620 --> 3:08:48.880
 Most people are annoyed by doing it.

3:08:48.880 --> 3:08:54.280
 And now you have one that like kind of does it itself, the various degrees of quality,

3:08:54.280 --> 3:08:59.080
 but that is so compelling that like, it's easier to understand and like, and they had

3:08:59.080 --> 3:09:02.440
 a very kind of, and I think they have like 15% of the vacuum cleaner market.

3:09:02.440 --> 3:09:04.480
 So it's like pretty successful, right?

3:09:04.480 --> 3:09:09.200
 I think we need more of those types of thoughtful stepping stones in robotics, but the opportunities

3:09:09.200 --> 3:09:14.400
 are becoming bigger because hardware's cheaper, compute's cheaper, cloud's cheaper and AI's

3:09:14.400 --> 3:09:15.400
 better.

3:09:15.400 --> 3:09:16.400
 So there's a lot of opportunity.

3:09:16.400 --> 3:09:22.920
 If we zoom out from specifically startups and robotics, what advice do you have to high

3:09:22.920 --> 3:09:29.880
 school students, college students about career and living a life that you'd be proud of?

3:09:29.880 --> 3:09:34.920
 You lived one heck of a life, you're very successful in several domains.

3:09:34.920 --> 3:09:40.240
 If you can convert that into a generalizable potion, what advice would you give?

3:09:40.240 --> 3:09:41.880
 Yeah, that's a very good question.

3:09:41.880 --> 3:09:48.800
 So it's very hard to go into a space that you're not passionate about and push, like

3:09:48.800 --> 3:09:54.720
 push hard enough to be, you know, to like maximize your potential in it.

3:09:54.720 --> 3:10:00.560
 And so there's a, there's always kind of like the saying of like, okay, follow your passion.

3:10:00.560 --> 3:10:05.000
 Like try to find the overlap of where your passion overlaps with like a growing opportunity

3:10:05.000 --> 3:10:08.440
 and need in the world where it's not too different than the startup kind of argument that we

3:10:08.440 --> 3:10:11.400
 talked about where if you are...

3:10:11.400 --> 3:10:12.880
 Where your passion meets the market.

3:10:12.880 --> 3:10:13.880
 Right?

3:10:13.880 --> 3:10:14.880
 You know what I mean?

3:10:14.880 --> 3:10:18.360
 Like, because it's like, it's a, you know, that's a beautiful thing where like you can

3:10:18.360 --> 3:10:21.480
 do what you love, but it's also just opens up tons of opportunities because the world's

3:10:21.480 --> 3:10:22.480
 ready for it, right?

3:10:22.480 --> 3:10:27.800
 Like, and so, and so like, if you're interested in technology, that might point to like go

3:10:27.800 --> 3:10:30.720
 and study machine learning because you don't have to decide what career you're going to

3:10:30.720 --> 3:10:34.440
 go into, but it's going to be such a versatile space that's going to be at the root of like

3:10:34.440 --> 3:10:38.760
 everything that's going to be in front of us that you can have eight different careers

3:10:38.760 --> 3:10:43.800
 in different industries and be an absolute expert in this like kind of tool set that

3:10:43.800 --> 3:10:47.240
 you wield that can go and be applied.

3:10:47.240 --> 3:10:50.200
 And that by the way, that doesn't apply to just technology, right?

3:10:50.200 --> 3:10:54.520
 It could be the exact same thing if you want to, you know, same thought process or price

3:10:54.520 --> 3:11:00.200
 to design to marketing to, you know, to sales to anything, but that versatility where you

3:11:00.200 --> 3:11:05.960
 like when you're in a space that's going to continue to grow, it's just like what company

3:11:05.960 --> 3:11:07.200
 do you join?

3:11:07.200 --> 3:11:11.160
 One that just is going to grow and the growth creates opportunities where the surface area

3:11:11.160 --> 3:11:16.000
 is just going to increase and the problems will never get stale and you can have, you

3:11:16.000 --> 3:11:19.840
 know, many like, and so you go into a career where you have that sort of growth in the

3:11:19.840 --> 3:11:26.120
 world that you're in, you end up having so much more opportunity that organically just

3:11:26.120 --> 3:11:31.440
 appears and you can then have more shots on goal to find like that killer overlap of timing

3:11:31.440 --> 3:11:35.280
 and passion and skill set and point in life where you can like, you know, just really

3:11:35.280 --> 3:11:38.240
 be motivated and fall in love with something.

3:11:38.240 --> 3:11:41.560
 And then at the same time, like find a balance like there's been times in my life where I

3:11:41.560 --> 3:11:46.680
 worked like a little bit too obsessively and, you know, and crazy and, and I, you know,

3:11:46.680 --> 3:11:49.960
 I think we kind of like tried to correct that, you know, kind of the right opportunities,

3:11:49.960 --> 3:11:54.480
 but you know, I think I probably appreciate a lot more now friendships that go way back,

3:11:54.480 --> 3:11:57.360
 you know, family and things like that.

3:11:57.360 --> 3:12:01.960
 And I kind of have the personality where I could ease, like I have like so much desire

3:12:01.960 --> 3:12:04.720
 to really try to optimize like, you know, when I'm working on that, I can easily go

3:12:04.720 --> 3:12:09.400
 to kind of an extreme and now I'm trying to like kind of find that balance and make sure

3:12:09.400 --> 3:12:13.400
 that I have the friendships, the family, like relationship with the kids, everything that

3:12:13.400 --> 3:12:20.240
 like I don't, I push really, really hard, but it kind of find a balance and, and I think

3:12:20.240 --> 3:12:25.360
 people can be happy on actually many kind of extremes on that spectrum, but it's easy

3:12:25.360 --> 3:12:31.400
 to kind of inadvertently make a choice by how, how you approach it that then becomes

3:12:31.400 --> 3:12:33.520
 really hard to unwind.

3:12:33.520 --> 3:12:37.960
 And so being very thoughtful about kind of all of those dimensions makes a lot of sense.

3:12:37.960 --> 3:12:42.040
 And so the comment, I think those are all interrelated, but at the end of the day,

3:12:42.040 --> 3:12:49.640
 love, passion and love, love towards, you said, family, friends and hopefully one day,

3:12:49.640 --> 3:12:59.640
 if your work pans out, Boris is love towards robots, not the creepy kind, the good kind,

3:12:59.640 --> 3:13:02.600
 just, just friendship and, and fun.

3:13:02.600 --> 3:13:03.600
 Yeah.

3:13:03.600 --> 3:13:06.760
 It's like another dimension to just how we interface with the world.

3:13:06.760 --> 3:13:07.760
 Yeah.

3:13:07.760 --> 3:13:10.400
 Boris, you're one of my favorite human beings, roboticist.

3:13:10.400 --> 3:13:15.640
 We've created some incredible robots and I think inspired countless people.

3:13:15.640 --> 3:13:20.600
 And like I said, I hope Cosmo, I hope you work with Anki lives on.

3:13:20.600 --> 3:13:24.360
 And I can't wait to see what you do with Waymo.

3:13:24.360 --> 3:13:29.280
 I mean, that's, if we're talking about artificial intelligence technology that has the potential

3:13:29.280 --> 3:13:33.880
 to revolutionize so much of our world, that's it right there.

3:13:33.880 --> 3:13:38.600
 So thank you so much for the work you've done and thank you for spending your valuable time

3:13:38.600 --> 3:13:39.600
 talking with me.

3:13:39.600 --> 3:13:40.600
 Thanks.

3:13:40.600 --> 3:13:43.880
 Thanks for listening to this conversation with Boris Hoffman.

3:13:43.880 --> 3:13:47.960
 To support this podcast, please check out our sponsors in the description.

3:13:47.960 --> 3:13:52.360
 And now let me leave you with some words from Isaac Asimov.

3:13:52.360 --> 3:13:59.880
 If you were to insist I was a robot, you might not consider me capable of love in some mystic

3:13:59.880 --> 3:14:02.320
 human sense.

3:14:02.320 --> 3:14:12.320
 Thank you for listening and hope to see you next time.

