WEBVTT

00:00.000 --> 00:05.360
 The following is a conversation with Dilip George, a researcher at the intersection of

00:05.360 --> 00:10.880
 neuroscience and artificial intelligence, cofounder of Vicarious with Scott Phoenix,

00:10.880 --> 00:17.280
 and formerly cofounder of New Menta with Jeff Hawkins, who has been on this podcast, and Dana

00:17.280 --> 00:23.520
 Dubinsky. From his early work on hierarchical temporal memory to recursive cortical networks

00:23.520 --> 00:29.600
 to today, Dilip's always sought to engineer intelligence that is closely inspired by the

00:29.600 --> 00:35.760
 human brain. As a side note, I think we understand very little about the fundamental principles

00:35.760 --> 00:41.600
 underlying the function of the human brain, but the little we do know gives hints that may be

00:41.600 --> 00:46.960
 more useful for engineering intelligence than any idea in mathematics, computer science, physics,

00:46.960 --> 00:53.120
 and scientific fields outside of biology. And so the brain is a kind of existence proof that says

00:53.120 --> 01:01.040
 it's possible, keep at it. I should also say that brain inspired AI is often overhyped and use this

01:01.040 --> 01:08.000
 fodder just as quantum computing for marketing speak, but I'm not afraid of exploring these

01:08.000 --> 01:12.640
 sometimes overhyped areas since where there's smoke, there's sometimes fire.

01:13.680 --> 01:20.480
 Quick summary of the ads, three sponsors Babel, Raycon earbuds, and Masterclass. Please consider

01:20.480 --> 01:25.760
 supporting this podcast by clicking the special links in the description to get the discount.

01:25.760 --> 01:30.880
 It really is the best way to support this podcast. If you enjoy this thing, subscribe on

01:30.880 --> 01:36.400
 YouTube, review with five stars and up a podcast, support on Patreon or connect with me on Twitter

01:36.400 --> 01:42.480
 and Lex Friedman. As usual, I'll do a few minutes of ads now and never any ads in the middle that

01:42.480 --> 01:48.880
 can break the flow of the conversation. This show is sponsored by Babel, an app and website

01:48.880 --> 01:54.240
 that gets you speaking in a new language within weeks. Go to Babel.com and use code Lex to get

01:54.240 --> 02:00.800
 three months free. They offer 14 languages, including Spanish, French, Italian, German,

02:00.800 --> 02:08.640
 and yes, Russian. Daily lessons are 10 to 15 minutes, super easy, effective, designed by over 100

02:08.640 --> 02:16.480
 language experts. Let me read a few lines from the Russian poem, Notch, Ulitsa, Fanar Abteka

02:16.480 --> 02:24.560
 by Alexander Blok, that you'll start to understand if you sign up to Babel. Notch, Ulitsa, Fanar Abteka,

02:35.520 --> 02:41.440
 I say that you'll only start to understand this poem because Russian starts with a language

02:41.440 --> 02:47.600
 and ends with the vodka. Now the latter part is definitely not endorsed or provided by Babel

02:47.600 --> 02:52.640
 and will probably lose me this sponsorship. But once you graduate from Babel, you can enroll my

02:52.640 --> 02:58.000
 advanced course of late night Russian conversation over vodka. I have not yet developed enough for

02:58.000 --> 03:05.280
 that. It's in progress. So get started by visiting Babel.com and use code Lex to get three months free.

03:05.280 --> 03:12.080
 This show is sponsored by Raycon earbuds. Get them at buyraycon.com slash Lex. They become my main

03:12.080 --> 03:18.000
 method of listening to podcasts, audio books and music when I run, do pushups and pull ups, or just

03:18.000 --> 03:23.520
 living life. In fact, I often listen to brown noise with them. When I'm thinking deeply about

03:23.520 --> 03:30.480
 something, it helps me focus. They're super comfortable, pair easily, great sound, great bass,

03:30.480 --> 03:36.720
 six hours of playtime. I've been putting in a lot of miles to get ready for a potential ultramarathon

03:36.720 --> 03:45.200
 and listening to audio books on World War two. The sound is rich and really comes in clear. So again,

03:45.200 --> 03:52.800
 get them at buyraycon.com slash Lex. This show is sponsored by masterclass. Sign up at masterclass.com

03:52.800 --> 03:58.480
 slash Lex to get a discount and to support this podcast. When I first heard about masterclass,

03:58.480 --> 04:04.160
 I thought it was too good to be true. I still think it's too good to be true. For 180 bucks a year,

04:04.160 --> 04:10.160
 you get an all access pass to watch courses from, to list some of my favorites, Chris Hadfield on

04:10.160 --> 04:15.600
 Space Exploration, Neil deGrasse Tyson on Scientific Thinking and Communication, Will Wright,

04:15.600 --> 04:21.440
 creator of SimCity and Sims on Game Design. Every time I do this read, I really want to play

04:21.440 --> 04:28.800
 a city builder game. Carlos Santana on Guitar, Garry Kasparov on Chess, Daniel Lagrano on Poker

04:28.800 --> 04:34.400
 and many more. Chris Hadfield explaining how rockets work and the experience of being launched into

04:34.400 --> 04:40.640
 space alone is worth the money. By the way, you can watch it on basically any device. Once again,

04:40.640 --> 04:46.320
 sign up at masterclass.com to get a discount and to support this podcast. And now here's my

04:46.320 --> 04:53.600
 conversation with Dileep George. Do you think we need to understand the brain in order to build it?

04:53.600 --> 05:00.240
 Yes, if you want to build the brain, we definitely need to understand how it works. So Blue Brain

05:00.240 --> 05:05.520
 or Henry Markrum's project is trying to build the brain without understanding it, like, you know,

05:05.520 --> 05:14.480
 just trying to put details of the brain from neuroscience experiments into a giant simulation

05:14.480 --> 05:21.760
 by putting more and more neurons, more and more details. But that is not going to work because

05:22.880 --> 05:29.040
 when it doesn't perform as what you expect it to do, then what do you do? You just keep adding

05:29.040 --> 05:35.680
 more details. How do you debug it? So unless you understand, unless you have a theory about

05:35.680 --> 05:39.200
 how the system is supposed to work, how the pieces are supposed to fit together,

05:39.200 --> 05:42.160
 what they're going to contribute, you can't build it.

05:42.160 --> 05:47.760
 At the functional level, understand. So can you actually linger on and describe the Blue Brain

05:47.760 --> 05:56.160
 project? It's kind of a fascinating principle and idea to try to simulate the brain. We're talking

05:56.160 --> 06:03.360
 about the human brain, right? Right. Human brains and rat brains or cat brains have lots in common

06:03.360 --> 06:10.240
 that the cortex, the neocortex structure is very similar. So initially, they were trying to

06:10.240 --> 06:16.240
 to just simulate a cat brain. And to understand the nature of evil.

06:17.120 --> 06:22.400
 To understand the nature of evil. Or as it happens in most of these simulations,

06:23.440 --> 06:30.720
 you easily get one thing out, which is oscillations. Yeah, if you simulate a large number of neurons,

06:30.720 --> 06:36.240
 they oscillate. And you can adjust the parameters and say that, oh, oscillation

06:36.240 --> 06:42.000
 match the rhythm that we see in the brain, et cetera. Oh, I see. So the idea is,

06:43.280 --> 06:49.040
 is the simulation at the level of individual neurons? Yeah. So the Blue Brain project,

06:49.040 --> 06:56.880
 the original idea as proposed was, you put very detailed biophysical neurons,

06:58.000 --> 07:03.440
 biophysical models of neurons, and you interconnect them according to the

07:03.440 --> 07:08.480
 the statistics of connections that we have found from real neuroscience experiments.

07:08.480 --> 07:15.360
 And then turn it on and see what happens. And these neural models are, you know,

07:15.360 --> 07:19.600
 incredibly complicated in themselves, right? Because these neurons are

07:20.560 --> 07:27.360
 modeled using this idea called Hodgkin Huxley models, which are about how signals propagate

07:27.360 --> 07:33.920
 in a cable. And there are active dendrites, all those phenomena, which those phenomena themselves,

07:33.920 --> 07:40.960
 we don't understand that well. And then we put in connectivity, which is part guesswork,

07:40.960 --> 07:46.000
 part, you know, observed. And of course, if you do not have any theory about how it is supposed to

07:46.000 --> 07:54.160
 work, we, you know, we just have to take whatever comes out of it as, okay, this is something

07:54.160 --> 07:59.920
 interesting. But in your sense, like these models of the way signal travels along, like with the

07:59.920 --> 08:06.800
 axons and all the basic models, that's, they're too crude. Oh, well, actually, they are pretty

08:06.800 --> 08:15.200
 detailed and pretty sophisticated. And they do replicate the neural dynamics. If you take a

08:15.200 --> 08:22.160
 single neuron, and you try to turn on the different channels, the calcium channels and

08:22.160 --> 08:30.240
 the different receptors, and see what the effect of turning on or off those channels are in the

08:30.240 --> 08:36.320
 neurons spike output, people have built pretty sophisticated models of that. And they are,

08:37.040 --> 08:40.560
 I would say, you know, in the regime of correct.

08:40.560 --> 08:44.800
 Well, see, the correctness, that's interesting, because you mentioned several levels.

08:45.680 --> 08:48.960
 The correctness is measured by looking some kind of aggregate statistics.

08:48.960 --> 08:55.120
 It would be more of the, the spiking dynamics of spiking dynamics as soon as you're okay.

08:55.760 --> 09:00.400
 And, and yeah, these models, because they're, they are going to the level of mechanism,

09:00.400 --> 09:05.680
 right? So they are basically looking at, okay, what, what is the effect of turning on an ion

09:05.680 --> 09:12.560
 channel? And, and you can, you can model that using electric circuits in and then so they are

09:12.560 --> 09:18.640
 modeled. So it is not just a function fitting, it is people are looking at the mechanism underlying

09:18.640 --> 09:25.920
 it and putting that in terms of electric circuit theory, signal propagation theory and, and modeling

09:25.920 --> 09:34.960
 that. And so those models are sophisticated, but getting a single neurons model 99% right,

09:34.960 --> 09:42.000
 does not still tell you how to, you know, it would be the analog of getting a transistor

09:42.000 --> 09:49.120
 model right and now trying to build a microprocessor. And if you, if you just observe, you know,

09:49.120 --> 09:54.720
 if you did not understand how a microprocessor works, but you say, oh, I have, I know can model

09:54.720 --> 10:00.880
 one transistor well, and now I will just try to interconnect the transistors according to whatever

10:00.880 --> 10:08.000
 I could, you know, guess from the experiments and try to simulate it. Then it is very unlikely

10:08.000 --> 10:13.520
 that you will produce a functioning microprocessor. You want to, you know, when you want to produce

10:13.520 --> 10:18.640
 a functioning microprocessor, you want to understand Boolean logic, how does, how do the, the gates

10:18.640 --> 10:22.880
 work, all those things. And then, you know, understand how do those gates get implemented

10:22.880 --> 10:26.960
 using transistors. Yeah, there's actually, I remember this reminds me this is a paper,

10:26.960 --> 10:33.440
 maybe you're familiar with it. I remember going through in a reading group that approaches a

10:33.440 --> 10:41.040
 microprocessor from a perspective of a neuroscientist. I think it basically, it uses all the tools that

10:41.040 --> 10:46.320
 we have of neuroscience to try to understand, like as if we just aliens showed up to study

10:46.320 --> 10:53.120
 computers. Yeah. And to see if those tools could be used to get any kind of sense of how the

10:53.120 --> 11:00.320
 microprocessor works. I think the final, the takeaway from the, at least this initials exploration is

11:00.320 --> 11:04.880
 that we're screwed. There's no way that the tools of neuroscience would be able to get us

11:05.440 --> 11:14.240
 to anything, like not even Boolean logic. I mean, it's just any aspect of the architecture of the

11:15.600 --> 11:22.640
 function of the processes involved, the clocks, the timing, all that. You can't figure that out

11:22.640 --> 11:28.160
 from the tools of neuroscience. Yeah. So I'm very familiar with this particular paper. I think it

11:28.160 --> 11:35.040
 was called, can a neuroscientist understand a microprocessor or something like that?

11:35.680 --> 11:41.360
 Following the methodology in that paper, even electrical engineer would not understand microprocessors.

11:41.360 --> 11:48.080
 So I couldn't. So I don't think it is that bad in the sense of saying,

11:48.080 --> 11:57.280
 neuroscientists do find valuable things by observing the brain. They do find good insights.

11:58.320 --> 12:06.000
 But those insights cannot be put together just as a simulation. You have to investigate

12:06.000 --> 12:13.280
 what are the computational underpinnings of those findings? How do all of them fit together from

12:13.280 --> 12:20.080
 an information processing perspective? You have to, somebody has to painstakingly put those things

12:20.080 --> 12:25.120
 together and build hypothesis. So I don't want to, this all of neuroscience are saying, oh,

12:25.120 --> 12:30.880
 they are not finding anything. No, that paper almost went to that level of neuroscientists

12:30.880 --> 12:35.840
 will never understand. No, that's not true. I think they do find lots of useful things,

12:35.840 --> 12:38.880
 but it has to be put together in a computational framework.

12:38.880 --> 12:46.400
 Yeah, I mean, but just the AI systems will be listening to this podcast 100 years from now,

12:46.400 --> 12:52.480
 and there will probably, there's some nonzero probability they'll find your words laughable.

12:52.480 --> 12:56.960
 There's like, I remember humans thought they understood something about the brain that are

12:56.960 --> 13:02.160
 totally clueless. There's a sense about neuroscience that we may be in the very, very early days of

13:02.160 --> 13:11.360
 understanding the brain. But I mean, that's one perspective in your perspective. How far are we

13:11.360 --> 13:20.720
 into understanding any aspect of the brain? So the dynamics of the individual neural

13:20.720 --> 13:29.040
 communication to the how in a collective sense, how they're able to store information,

13:29.040 --> 13:33.920
 transfer information, how the intelligence that emerges, all that kind of stuff. Where are we

13:33.920 --> 13:39.360
 on that timeline? Yeah. So timelines are very, very hard to predict. And you can, of course,

13:39.360 --> 13:48.800
 be wrong. And it can be wrong on either side. We know that when we look back, the first flight

13:48.800 --> 13:57.280
 was in 1903. In 1900, there was a New York Times article on flying machines that do not

13:57.280 --> 14:04.160
 fly. And, you know, humans might not fly for another 100 years. That was what that article

14:04.160 --> 14:09.680
 stated. And so, but no, they flew three years after that. So it is, you know, it's very hard to,

14:10.880 --> 14:17.120
 so. And on that point, one of the Wright brothers, I think two years before,

14:17.120 --> 14:26.160
 uh, said that, uh, like he said, like some number like 50 years, he has become convinced that it's,

14:26.160 --> 14:32.240
 it's, uh, it's impossible. Even during their experimentation. Yeah. Yeah. Yeah. I mean,

14:32.240 --> 14:37.280
 that's a tribute to when it, that's like the entrepreneurial battle of like depression of

14:37.280 --> 14:41.920
 going through just like thinking there's this is impossible. But there, yeah, there's something,

14:41.920 --> 14:48.160
 even the person that's in it is not able to see, uh, estimate correctly. Exactly. But I can,

14:48.160 --> 14:51.920
 I can tell from the point of, you know, objectively, what are the things that we

14:52.480 --> 14:58.560
 know about the brain and how that can be used to build AI models, which can then go back and

14:58.560 --> 15:03.520
 inform how the brain works. Um, so my way of understanding the brain would be to basically

15:03.520 --> 15:10.480
 say, look at the insights neuroscientists have found, understand that from, uh, a computational

15:10.480 --> 15:17.760
 angle, information processing angle, build models using that. And then building the, that model,

15:17.760 --> 15:21.520
 which, which functions with them, which is a functional model, which is, which is doing the

15:21.520 --> 15:26.960
 task that we want the model to do. It is not just trying to model a phenomena in the brain. It is,

15:26.960 --> 15:32.240
 it is trying to do what the brain is trying to do on the, on the whole, uh, functional level.

15:32.240 --> 15:37.920
 And building that model will help you fill in the missing pieces that, you know, biology just

15:37.920 --> 15:44.560
 gives you the hints and building the model, you know, fills in the rest of the pieces of the puzzle.

15:44.560 --> 15:49.760
 And then you can go and connect that back to biology and say, okay, now it makes sense that

15:49.760 --> 15:57.440
 this part of the brain is, uh, doing this or this layer in the cortical circuit is doing this. Uh,

15:57.440 --> 16:03.920
 and, and, and then continue this iteratively because now that will inform new experiments

16:03.920 --> 16:08.800
 in neuroscience. And of course, you know, building the model and verifying that in the real world,

16:08.800 --> 16:14.240
 will you, will also tell you more about does the model actually work? Uh, and you can refine the

16:14.240 --> 16:21.120
 model, find better ways of putting these neuroscience insights together. So, so I would say it is,

16:21.120 --> 16:27.680
 it is, you know, it, so neuroscience alone, just from experimentation, will not be able

16:27.680 --> 16:32.800
 to build a model of the, of the brain, uh, a functional model of the brain. So we, you know,

16:32.800 --> 16:38.800
 there, there's, uh, lots of efforts, which are very impressive efforts in collecting more and more

16:38.800 --> 16:44.720
 connectivity data from the brain. Yeah. You know, how, how are the micro circuits of the brain

16:44.720 --> 16:50.000
 connected with each other? Those are beautiful, by the way. Those are beautiful. Uh, and at the

16:50.000 --> 16:56.880
 same time, those, those do not itself, um, by themselves convey the story of how does it work.

16:56.880 --> 17:02.080
 Yeah. Um, and, and somebody has to understand, okay, why are they connected like that? And

17:02.080 --> 17:08.720
 what, what are those things doing? Uh, and, and we do that by building models in AI using hints

17:08.720 --> 17:16.720
 from neuroscience and, and repeat the cycle. So what aspect of the brain are useful in this

17:16.720 --> 17:21.680
 whole endeavor, which by the way, I should say you're, you're both a neuroscientist and, and AI

17:21.680 --> 17:27.760
 person, I guess the dream is to both understand the brain and to build a GI systems. So you're,

17:27.760 --> 17:34.640
 you're, it's like an engineer's perspective of trying to understand the brain. So what aspects

17:34.640 --> 17:39.680
 of the brain, uh, function is speaking, like you said, do you find interesting? Yeah. Quite a lot

17:39.680 --> 17:46.400
 of things. All right. So one is, um, you know, if you look at the visual cortex, um, uh, and,

17:46.400 --> 17:52.320
 and, you know, the visual cortex is, is a large part of the brain. Uh, I forgot the exact fraction,

17:52.320 --> 18:00.080
 but it is, it's a huge part of our brain areas, uh, occupied by just, just vision. Um, so vision,

18:00.080 --> 18:07.040
 visual cortex is not just a feed forward cascade of neurons. Um, uh, there are a lot more feedback

18:07.040 --> 18:13.040
 connections in the brain compared to the feed forward connections. And, and it is surprising

18:13.040 --> 18:17.200
 to the level of detail neuroscientists have actually studied this. If you, if you go into

18:17.200 --> 18:22.960
 neuroscience literature and poke around and ask, you know, have they studied what will be the effect

18:22.960 --> 18:32.720
 of poking a neuron in, uh, level IT, uh, in level V1? And, um, have they studied that? Uh, and you

18:32.720 --> 18:39.520
 will say, yes, they have studied that every part of every possible combination. I mean, it's, it's a,

18:39.520 --> 18:44.400
 it's not a random exploration at all. It's a very hypothesis driven, right? They, they are very,

18:44.400 --> 18:49.120
 uh, experimental neuroscientists are very, very systematic in how they probe the brain. Uh,

18:49.120 --> 18:54.400
 because experiments are very costly to conduct. They take a lot of preparation. They, they need a

18:54.400 --> 18:59.280
 lot of control. So they, they are very hypothesis driven in how they probe the brain. And, um,

18:59.280 --> 19:07.280
 often what I find is that when we have a question in, um, in AI, uh, about have, has anybody probed,

19:07.280 --> 19:11.840
 probed how lateral connections in the brain works? And when you go and read the literature,

19:11.840 --> 19:16.000
 yes, people have probed it and people have probed it very systematically. And, and

19:16.000 --> 19:22.720
 they have hypothesis about how those lateral connections are supposedly contributing to

19:22.720 --> 19:28.640
 visual processing. Uh, but of course they haven't built very, very functional detail models of it.

19:28.640 --> 19:33.680
 By the way, how do you know studies side to interrupt? Do they, do they stimulate like a neuron

19:33.680 --> 19:38.720
 in one particular area of the visual cortex and then see how the travel of the signal travels

19:38.720 --> 19:42.160
 kind of thing? Fascinating, very, very fascinating experiments. Right. So I can,

19:42.160 --> 19:47.120
 I can give you one example I was impressed with. Um, this is, uh, so before going to that, let me,

19:47.120 --> 19:52.640
 like, let me give you, uh, uh, you know, a, uh, overview of how the, the layers in the cortex

19:52.640 --> 19:58.320
 are organized, right? Uh, visual cortex is organized into roughly four hierarchical levels.

19:58.320 --> 20:06.000
 Okay. So, uh, V1, V2, V4, IT and in V3, uh, well, yeah, there's another pathway.

20:06.000 --> 20:10.400
 Okay. So there is this, I'm talking about just the object recognition pathway.

20:10.400 --> 20:18.080
 All right. Cool. And then, um, in V1 itself, um, so it's, there is a very detailed micro

20:18.080 --> 20:23.440
 circuit in V1 itself that is, that is organization within a level itself. Um, the cortical sheet

20:23.440 --> 20:29.920
 is organized into, uh, you know, multiple layers and there are columnar structure and, and this,

20:29.920 --> 20:36.560
 this layer wise and columnar structure is repeated and V1, V2, V4, uh, IT, all of them.

20:36.560 --> 20:42.320
 Right. Uh, and, and the connections between these layers within a level with, you know,

20:42.320 --> 20:47.040
 in V1 itself, there are six layers roughly and the connections between them, there is a particular

20:47.040 --> 20:55.520
 structure to them. Uh, and, um, now, so one example of, uh, an experiment, uh, uh, people did is

20:55.520 --> 21:03.280
 when I, when you present a stimulus, uh, which is, um, let's say requires, um, separating the

21:03.280 --> 21:09.440
 foreground from the background of an object. So it is a, it's a textured triangle on a textured

21:09.440 --> 21:16.880
 background. Uh, and, um, you can check, does the surface settle first or does the contour settle

21:16.880 --> 21:25.920
 first? Settle? Settle in the sense that the, so when you find, finally form the percept of the,

21:25.920 --> 21:32.000
 of the, uh, triangle, you understand where the contours of the triangle are and you also know

21:32.000 --> 21:37.600
 where the inside of the triangle is, right? That's when you form the final percept. Uh, now you can

21:37.600 --> 21:46.560
 ask, what is the dynamics of forming that final percept? Um, do the, uh, do the neurons, um,

21:47.200 --> 21:54.480
 first find the edges and converge on where the edges are and then they find the inner surfaces

21:54.480 --> 22:00.640
 or does it go the other way around? So, so what's the answer? Uh, in this case, it, it turns out that

22:00.640 --> 22:07.600
 it find first settles on the edges. It converges on the edge hypothesis first and then the, the

22:07.600 --> 22:14.480
 surfaces are filled in from the edges to the inside. That's fascinating. Uh, and, and the detail to

22:14.480 --> 22:20.320
 which you can study this, it's, it's amazing that you can actually not only find, um, the temporal

22:20.320 --> 22:26.080
 dynamics of when this happens. Uh, uh, and then you can also find which layer in the, you know,

22:26.080 --> 22:34.000
 in V1, which layer is encoding, uh, the edges, which layer is encoding the surfaces and, uh,

22:34.000 --> 22:37.920
 which layer is encoding the feedback, which layer is encoding the feed forward and what,

22:37.920 --> 22:43.760
 what's the combination of them that produces the final person. Um, and these kinds of experiments

22:43.760 --> 22:49.680
 stand out when you try to explain illusions. Uh, one, one example of a favorite illusion of

22:49.680 --> 22:54.080
 mine is the Kanitsa triangle. I don't know whether you are familiar with this one. So this is, um,

22:54.080 --> 23:00.080
 uh, this is an example where it's a triangle, uh, but, you know, the, the corners of the,

23:00.080 --> 23:04.560
 only the corners of the triangle are shown in the stimuli, the stimulus. Uh, so they look

23:04.560 --> 23:11.200
 like kind of Pacman. Oh, the black Pacman. Yeah. And then you start to see your visual system

23:11.200 --> 23:16.080
 hallucinates the edges. Yeah. Um, and you can, you know, you, when you look at it, you will see a

23:16.080 --> 23:23.040
 faint edge, right? And you can go inside the brain and look, you know, do actually neurons

23:23.040 --> 23:29.600
 signal the presence of this edge. And, and if they signal, how do they do it? Because they are not

23:29.600 --> 23:36.320
 receiving anything from the input. The input is blank for those neurons, right? Uh, so how do

23:36.320 --> 23:41.520
 they signal it? When does the signaling happen? You know, does it, you know, so, so if a real

23:41.520 --> 23:47.520
 contour is present in the input, then the, the neurons immediately signal, okay, there is a,

23:47.520 --> 23:53.760
 there is an edge here. When, when it is an illusory edge, um, it is clearly not in the input. It is

23:53.760 --> 23:59.840
 coming from the context. So those neurons fire later and, and you can say that, okay, these are,

23:59.840 --> 24:06.320
 it's the feedback connections that is causing them to fire. Uh, and, and they happen later and you

24:06.320 --> 24:13.200
 can find the dynamics of them. So, so these studies are pretty impressive and, and very detailed.

24:13.200 --> 24:19.520
 So by the way, just, uh, just to step back, you said, uh, that there may be more feedback

24:19.520 --> 24:24.880
 connections and feed forward connections. Yeah. Uh, first of all, if it's just for like a machine

24:24.880 --> 24:32.000
 learning folks, I mean, that, that's crazy that there's all these feedback connections. I mean,

24:32.000 --> 24:41.280
 we often think about, I think, thanks to deep learning, you start to think about, um, the human

24:41.280 --> 24:48.560
 brain is a kind of feed forward mechanism. Right. Uh, so what the heck are these feedback connections?

24:48.560 --> 24:54.000
 Yeah. What's their, what's the dynamics or what are we supposed to think about them?

24:54.000 --> 24:59.360
 Yeah. So this is, this fits into a very beautiful picture about how the brain works, right? Um,

24:59.360 --> 25:05.840
 so the, the beautiful picture of how the brain works is that our brain is building a model of the

25:05.840 --> 25:13.280
 world. Uh, I know. So our visual system is building a model of how objects behave in the world. And,

25:13.280 --> 25:19.600
 and we are constantly projecting that model back onto the world. So what we are seeing is not just

25:19.600 --> 25:25.280
 a feed forward thing that just gets interpreted in a forward part. We are constantly projecting

25:25.280 --> 25:31.280
 our expectations onto the world. And, and what the final percept is a combination of what we

25:31.280 --> 25:36.720
 project onto the world, uh, combined with what the actual sensory input is. Uh,

25:36.720 --> 25:40.880
 almost like trying to calculate the difference and then trying to interpret the difference.

25:40.880 --> 25:45.360
 Yeah. It's, it's, um, I wouldn't put it as calculating the difference. It's more like,

25:45.360 --> 25:51.680
 what is the best explanation for the input stimulus based on the model of the world I have.

25:52.320 --> 25:55.840
 Got it. Got it. And that's where all the illusions come in. And that's,

25:55.840 --> 26:00.560
 but that's, that's an incredibly efficient, so, uh, efficient process. So the feedback

26:00.560 --> 26:07.280
 mechanism, it just helps you constantly. Uh, yeah. So hallucinate how the world should be

26:07.280 --> 26:15.120
 based on your world model. And then just looking at, uh, if there's novelty, uh, like trying to

26:15.120 --> 26:20.800
 explain it, hence that's why movement would detect movement really well. There's all these

26:20.800 --> 26:26.880
 kinds of things. And this is like at all different levels of the cortex you're saying that this

26:26.880 --> 26:32.240
 happens at the lowest level, the highest level. Yes. Yeah. Feedback connections are more prevalent

26:32.240 --> 26:37.840
 in everywhere in the cortex. And, and, um, so one way to think about it, and there's a lot of

26:37.840 --> 26:44.080
 evidence for this is inference. Um, so, you know, so basically if you have a model of the world

26:44.080 --> 26:50.720
 and when, when some evidence comes in, what you are doing is inference, right? You are trying to

26:50.720 --> 26:57.200
 now explain this evidence using your model of the world. And this inference includes

26:57.760 --> 27:04.160
 projecting your model onto the evidence and taking the evidence, uh, back into the model and, and

27:04.160 --> 27:10.240
 doing an iterative procedure. Um, and, uh, this iterative procedure is what happens

27:10.240 --> 27:15.920
 using the feed forward feedback propagation. Uh, and feedback affects what you see in the

27:15.920 --> 27:21.360
 world and you know, it also affects feed forward propagation and examples are everywhere. We,

27:21.360 --> 27:27.520
 we see these kinds of things everywhere. The idea that there can be multiple competing

27:27.520 --> 27:34.720
 hypothesis, uh, in our model, trying to explain the same evidence and then you have to kind of

27:34.720 --> 27:41.360
 make them compete and one hypothesis will explain away the other hypothesis through this competition

27:41.360 --> 27:48.160
 process. Wait, what? So you have competing models of the world that try to explain,

27:48.160 --> 27:54.000
 what do you mean by explain away? So this is a classic example in, uh, uh, graphical models,

27:54.000 --> 28:02.400
 probabilistic models. Um, so if you, what are those? Um, okay. Um, I think it's useful to mention

28:02.400 --> 28:09.280
 because we'll talk about them more. Yeah. Yeah. So neural networks are one class of machine

28:09.280 --> 28:15.280
 learning models. Um, you know, you have distributed set of, uh, nodes, which are called the neurons,

28:15.280 --> 28:18.960
 you know, each one is doing a dot product and you can, you can approximate any function

28:18.960 --> 28:24.640
 using this, uh, multi level, uh, network of neurons. So that's, uh, uh, a class of models

28:24.640 --> 28:29.280
 which are useful, useful for function approximation. There is another class of models

28:29.280 --> 28:35.280
 in machine learning, uh, called probabilistic graphical models. And you can think of them as

28:35.280 --> 28:41.920
 each node in that model is variable, which is, which is talking about something, you know,

28:41.920 --> 28:49.440
 it can be a variable representing is, is an edge present in the input or not. Uh, and at the top

28:49.440 --> 28:56.640
 of the, uh, uh, network, uh, node can be, uh, representing, is there an object present in the

28:56.640 --> 29:04.960
 world or not? And, and then, so it can, it is, it is another way of encoding knowledge and, um,

29:04.960 --> 29:12.400
 and then you, once you encode the knowledge, you can, uh, do inference in the right way. You know,

29:12.400 --> 29:18.720
 what is the best way to, uh, you know, explain some set of evidence using this model that you

29:18.720 --> 29:23.120
 encoded, you know. So when you encode the model, you are encoding the relationship between these

29:23.120 --> 29:27.920
 different variables. How is the edge connected to my, uh, the model of the object? How is the

29:27.920 --> 29:33.120
 surface connected to the model of the object? Um, and then, um, of course, this is a very

29:33.120 --> 29:40.160
 distributed, complicated model. And inference is how do you explain a piece of evidence when,

29:40.160 --> 29:45.200
 when a set of stimulus comes in? If somebody tells me there is a 50% probability that there is an edge

29:45.200 --> 29:51.200
 here in this part of the model, how does that affect my belief on whether I should think that

29:51.200 --> 29:56.880
 there should be a, is the square percent in the image? So, so this is the process of inference.

29:56.880 --> 30:04.000
 So one example of inference is having this expiring away effect between multiple causes. So, uh,

30:04.000 --> 30:11.920
 graphical models can be used to represent causality in the world. Um, so let's say, um, you know,

30:11.920 --> 30:21.440
 your, uh, alarm, uh, the, uh, at home can be, uh, triggered by a, uh, burglar getting into your

30:21.440 --> 30:27.920
 house, uh, or it can be triggered by an earthquake. Both, both can be causes of the alarm going off.

30:27.920 --> 30:33.520
 So now you, you are, you know, you're in your office, you heard burglar alarm going off. You

30:33.520 --> 30:39.840
 are heading home thinking that there's a burglar. But while driving home, if you hear on the radio

30:39.840 --> 30:45.920
 that there was an earthquake in the vicinity, now you're hype, you know, uh, strength of evidence

30:45.920 --> 30:52.240
 for, uh, a burglar getting into their house is diminished because now that, that piece of evidence

30:52.240 --> 30:57.680
 is explained by the earthquake being present. So if you, if you think about these two causes

30:57.680 --> 31:03.920
 explaining at lower level, uh, variable, which is alarm, now what we're seeing is that increasing

31:03.920 --> 31:09.520
 the evidence for some cause, you know, there is evidence coming in from below for alarm being

31:09.520 --> 31:16.000
 present. And initially it was flowing to a burglar being present, but now since somebody,

31:16.000 --> 31:21.360
 some, there is side evidence for this other cause, it explains away this evidence and it

31:21.360 --> 31:26.480
 evidence will now flow to the other cause. This is, you know, two competing causal, uh, things

31:26.480 --> 31:31.360
 trying to explain the same evidence. And the brain has a similar kind of mechanism for, uh,

31:31.360 --> 31:38.400
 for doing so. That's kind of interesting. I mean, and that, how's that all encoded in the brain?

31:38.400 --> 31:43.600
 Like, where's the storage of information? Are we talking just maybe to get it, uh,

31:43.600 --> 31:50.080
 a little bit more specific? Is it in the hardware of the actual connections? Is it in, uh, chemical

31:50.080 --> 31:56.000
 communication? Is it electrical communication? Do we, do we know? So this is, you know, a paper

31:56.000 --> 32:01.200
 that we are bringing out soon. Which one was this? Um, this is the cortical microcircuit paper

32:01.200 --> 32:06.880
 that I sent you a draft of. Of course this is, uh, a lot of it is still hypothesis. One hypothesis

32:06.880 --> 32:13.920
 that a, you can think of a cortical column as encoding a, a concept, a concept, you know,

32:13.920 --> 32:22.160
 think of it as a, uh, a, um, cons, an example of a concept is, um, is an edge present or not,

32:22.160 --> 32:27.360
 or is, is an object present or not. Okay. So it can, you can think of it as a binary variable,

32:27.360 --> 32:32.080
 a binary random variable, the presence of an edge or not, or the presence of an object or not.

32:32.080 --> 32:38.080
 So each cortical column can be thought of as representing that one concept, one variable.

32:38.080 --> 32:43.200
 And then the connections between these cortical columns are basically encoding the

32:43.200 --> 32:47.920
 relationship between these random variables. And then there are connections within the

32:47.920 --> 32:53.040
 cortical column. There are, each cortical column is implemented using multiple layers of neurons

32:53.040 --> 32:59.200
 with very, very, very rich, um, structure there. You know, there are thousands of neurons in a

32:59.200 --> 33:04.000
 cortical column, but, but that structure is similar across the different cortical columns.

33:04.000 --> 33:09.120
 Yeah. Correct. And also these cortical columns collect, connect to a substructure called thalamus

33:09.120 --> 33:14.160
 in the, uh, you know, so all, all cortical columns to pass through this substructure.

33:14.160 --> 33:20.160
 So our hypothesis is, is that the connections between the cortical columns implement this,

33:20.160 --> 33:24.640
 uh, you know, that's where the knowledge is stored about, you know, how these different

33:24.640 --> 33:30.800
 concepts, concepts connect to each other. And then the, the neurons inside this cortical column

33:30.800 --> 33:38.000
 and in thalamus in combination implement this, uh, actual computations needed for inference,

33:38.000 --> 33:42.640
 which includes explaining away and competing between the different, uh, hypothesis.

33:43.680 --> 33:49.680
 And it is all very, so what is amazing is that, uh, neuroscientists have actually

33:49.680 --> 33:55.280
 done ex experiments to the tune of showing these things. Uh, they might not be putting

33:55.280 --> 34:01.760
 it in the overall inference framework, but they will show things like if I poke this higher level

34:01.760 --> 34:07.680
 neuron, uh, it will inhibit through this complicated loop through the thalamus, it will inhibit this

34:07.680 --> 34:14.080
 other column. Uh, so they will, they will do such experiments. Do they use terminology of concepts,

34:14.080 --> 34:22.960
 for example? So, so you're, I mean, uh, is it, uh, is it something where it's easy to anthropomorphize

34:23.600 --> 34:29.760
 and think about concepts? Like, uh, you started moving into logic based kind of reasoning systems.

34:29.760 --> 34:37.920
 So, um, are we just thinking of concepts in that kind of way, or is it, uh, is it a lot messier,

34:37.920 --> 34:45.600
 a lot more gray area, you know, even, even more gray, even more messy than, uh, the artificial

34:45.600 --> 34:50.480
 neural network kinds, kinds of abstractions. It's easiest way to think of it as a variable,

34:50.480 --> 34:56.240
 right? It's a binary variable, which is showing the presence or absence of something. But I guess

34:56.240 --> 35:02.400
 what I'm asking is, is that something, uh, we're supposed to think of something that's human

35:02.400 --> 35:06.960
 interpretable of that something. It doesn't need to be. It doesn't need to be human interpretable.

35:06.960 --> 35:12.640
 There's no need for it to be human interpretable. Uh, but it's, it's almost like, um,

35:13.760 --> 35:20.320
 you, you will be able to find some interpretation of it, uh, because it is connected to the other

35:20.320 --> 35:27.520
 things that you know about. And the point is it's useful somehow. It's useful as an entity

35:28.560 --> 35:34.160
 in the graph that, in connecting to the other entities that are, let's call them concepts.

35:34.160 --> 35:39.840
 Right. Okay. So, uh, by the way, what's, are these the cortical micro circuits?

35:39.840 --> 35:44.000
 Correct. These are the cortical micro circuits. You know, that's what neuroscientists use to

35:44.000 --> 35:49.920
 talk about the circuits in, in, uh, within a level of the cortex. So you can think of,

35:49.920 --> 35:54.160
 you know, let's think of in neural network, you know, artificial neural network terms,

35:54.160 --> 35:58.560
 you know, people talk about the architecture of though, you know, so how many, how many layers

35:58.560 --> 36:03.120
 they build, uh, you know, what is the fan in fan out, et cetera. That is the macro architecture.

36:03.120 --> 36:11.280
 Um, so, and then within a layer of the neural network, you can, you know, the cortical neural

36:11.280 --> 36:15.920
 network is much more structured within, you know, within a level, there's a lot more intricate

36:15.920 --> 36:21.200
 structure there. Uh, but even, um, even within an artificial neural network, you can think of

36:21.200 --> 36:26.480
 in feature detection plus pooling as one, one level. And so that is kind of a micro circuit.

36:26.480 --> 36:34.160
 Uh, it's much more, uh, complex in the real brain. Uh, and, uh, and so within a level,

36:34.160 --> 36:38.640
 whatever is that circuitry within a column of the cortex and between the layers of the

36:38.640 --> 36:43.600
 cortex, that's the micro circuitry. I love that terminology. Uh, machine learning people don't

36:43.600 --> 36:50.720
 use the circuit terminology, but they should. It's, uh, it's a nice. So okay. Uh, okay. So that's, uh,

36:50.720 --> 36:56.720
 uh, that's the, the, the cortical micro circuit. So what's interesting about, uh, what, what can

36:56.720 --> 37:02.640
 we say? What is the paper that, uh, you're working on, uh, propose about the ideas around

37:02.640 --> 37:09.680
 these cortical micro circuits. So this is a fully functional model for the micro circuits of the

37:09.680 --> 37:15.040
 visual cortex. So the, the paper focuses on your idea and our discussions now is focusing on vision.

37:15.040 --> 37:21.040
 Yeah. The, uh, visual cortex. Okay. Yeah. This is a model. This is a full model. This is,

37:21.040 --> 37:25.040
 this is how vision works. But this is, this is a, yeah, model.

37:26.400 --> 37:33.200
 Okay. So let me, let me step back a bit. Um, so we looked at neuroscience for insights on

37:33.200 --> 37:38.800
 how to build a vision model, right? And, and, and we synthesized all those insights into a

37:38.800 --> 37:43.840
 computational model. This is called the recursive cortical network model that we, we used for

37:43.840 --> 37:50.320
 breaking captures and, and we are using the same model for robotic picking and, uh, tracking of

37:50.320 --> 37:54.400
 objects. And that again is a vision system. That's a vision system. Computer vision system.

37:54.400 --> 38:00.480
 That's a computer vision system. Takes in images and outputs. What? On one side, it outputs the

38:00.480 --> 38:07.280
 class of the image, uh, and also segments the image. Uh, and you can also ask it further queries.

38:07.280 --> 38:11.600
 Where is the edge of the object? Where is the interior of the object? So, so it's a, it's a

38:11.600 --> 38:17.120
 model that you build to answer multiple questions. So you're not trying to build a model for just

38:17.120 --> 38:22.720
 classification or just segmentation, et cetera. It's a, it's a, it's a joint model that can do

38:22.720 --> 38:30.320
 multiple things. Um, and, um, so, so that's the model that we built using insights from neuroscience.

38:30.320 --> 38:35.120
 And some of those insights are what is the role of feedback connections? What is the role of lateral

38:35.120 --> 38:40.720
 connections? Uh, so all those things went into the model. The model actually uses feedback connections.

38:40.720 --> 38:45.600
 All these ideas from, from neuroscience. Uh, so what, what, what the heck is a recursive

38:45.600 --> 38:50.400
 cortical network? Like what, what are the architecture approaches, interesting aspects here

38:51.840 --> 38:56.400
 which is essentially a brain inspired approach to a computer vision?

38:56.400 --> 39:01.760
 Yeah. So there are multiple layers to this question. I can go from the very, very top and

39:01.760 --> 39:07.760
 then zoom in. Okay. So one important thing constrained that went into the model is that

39:07.760 --> 39:13.520
 you should not think vision, think of vision as something in isolation. We should not think

39:13.520 --> 39:21.040
 perception as something as a pre processor for cognition, perception and cognition are interconnected.

39:21.600 --> 39:26.800
 And so you should not think of one problem in separation from the other problem. Um, and so

39:26.800 --> 39:32.480
 that means if you finally want to have a system that understand concepts, uh, about the world and

39:32.480 --> 39:37.840
 can learn in a very conceptual model of the world and can reason and connect to language, all of

39:37.840 --> 39:42.800
 those things, you need to, you need to have think all the way through and make sure that your

39:42.800 --> 39:48.080
 perception system is compatible with your cognition system and language system and all of them.

39:48.080 --> 39:55.600
 And one aspect of that is top down controllability. Um, what does that mean? So that means, you

39:55.600 --> 40:01.920
 know, so, so think of, you know, you can close your eyes and think about the details of one object.

40:02.640 --> 40:07.440
 Right. I can, I can zoom in further and further. I can, you know, so, so think of the bottle in

40:07.440 --> 40:14.000
 front of me. Right. And now you can think about, okay, what the cap of that bottle looks. Uh,

40:14.000 --> 40:19.760
 I know we can think about what's the texture on that bottle of the, uh, the cap, you know, you can

40:19.760 --> 40:25.520
 think about, you know, what will happen if, uh, something hits that. Uh, so you can, you can,

40:25.520 --> 40:33.520
 you can manipulate your visual knowledge in, uh, cognition driven ways. Yes. Uh, and so

40:33.520 --> 40:39.760
 this top down controllability, uh, and being able to simulate scenarios in the world.

40:40.400 --> 40:47.280
 So you're not just a passive, uh, player in this perception game. You can, you can control it.

40:47.280 --> 40:52.160
 You can, you, you have imagination. Correct. Correct. So, so, so basically, you know,

40:52.160 --> 40:57.920
 basically having a generating network, uh, which is a model and, and it is not just some arbitrary

40:57.920 --> 41:02.800
 generating network. It has to be, it has to be built in a way that it is controllable top down.

41:02.800 --> 41:08.000
 It is, it is not just trying to generate a whole picture at once. Uh, you know, it's not trying

41:08.000 --> 41:12.480
 to generate photorealistic things of the world. You, you know, you don't have good photorealistic

41:12.480 --> 41:17.200
 models of the world. Human brains do not have. If I, if I, for example, ask you the question, uh,

41:17.200 --> 41:23.760
 what is the color of the letter E in the Google logo? You have no idea. No idea.

41:23.760 --> 41:29.280
 Although you have seen it millions of times or hundreds of times. So, uh, so it's not,

41:29.280 --> 41:34.720
 our model is not photorealistic, but, but it is, but it has other properties that we can manipulate

41:34.720 --> 41:39.280
 it, uh, in the, uh, and you can think about filling in a different color in that logo.

41:39.280 --> 41:45.360
 You can think about expanding the, the letter E, you know, you can see what'll in, so you can imagine

41:45.360 --> 41:50.160
 the consequence of, you know, actions that you have never performed. So, so these are the kind

41:50.160 --> 41:54.400
 of characteristics the generative model need to have. So this is one constraint that went into

41:54.400 --> 41:59.040
 our model. Like, you know, so this is when you read the, just the perception side of the paper,

41:59.040 --> 42:03.520
 it is not obvious that this was a constraint into the, that went into the model, this top

42:03.520 --> 42:09.440
 down controllability of the generative model. Uh, so what, what does top down controllability in a

42:09.440 --> 42:15.840
 model look like? It's a really interesting concept, fascinating concept. What is that,

42:15.840 --> 42:21.120
 is that the recursive, recursiveness gives you that or how do you, how do you do it?

42:21.120 --> 42:26.400
 Quite a few things. It's like, what, what does the model factor factorize? You know, what are the,

42:26.400 --> 42:30.480
 what is the model representing as different pieces in the puzzle? Like, you know, so,

42:30.480 --> 42:36.880
 so in the RCN, uh, network, it, it thinks of the world, you know, so what I say, the background of

42:37.440 --> 42:43.360
 an image is modeled separately from the foreground of the image. So, so the objects are separate

42:43.360 --> 42:47.360
 from the background. They're different entities. So there's a kind of segmentation that's built

42:47.360 --> 42:54.080
 in fundamentally. And, and, and then even that object is composed of parts and also, and another

42:54.080 --> 43:00.560
 one is the, the shape of the object, uh, is differently modeled from the texture of the object.

43:01.920 --> 43:07.680
 Got it. So there's like these, um, uh, I've been, you know, who Franois Chalet is,

43:08.400 --> 43:15.440
 uh, he's, so there's, uh, he developed this like IQ test type of thing for ARC challenge for,

43:16.400 --> 43:22.640
 and it's kind of cool that there's, um, these concepts, priors that he defines that you bring

43:22.640 --> 43:28.560
 to the table in order to be able to reason about basic shapes and things in IQ tests.

43:28.560 --> 43:34.080
 So here you're making it quite explicit that here, here are the things that you should be,

43:34.800 --> 43:40.000
 these are like distinct things that you should be able to, um, model in this.

43:40.000 --> 43:44.880
 Keep in mind that you, you can derive these from much more general principles. It doesn't,

43:44.880 --> 43:49.840
 you don't need to explicitly put it as, oh, objects versus foreground versus background,

43:49.840 --> 43:55.440
 uh, the surface versus texture. No, these are, these are derived from, uh, more fundamental

43:55.440 --> 44:00.800
 principles of how, you know, what's the property of continuity of natural signals.

44:02.480 --> 44:05.360
 What's the property of continuity of natural signals?

44:05.360 --> 44:05.760
 Yeah.

44:05.760 --> 44:09.920
 By the way, that sounds very poetic, but yeah. Uh, so you're saying that's a,

44:10.800 --> 44:15.440
 there's some low level properties from which emerges the idea that shapes would be different

44:15.440 --> 44:20.560
 than, like, uh, this should be a part of an object. There should be, I mean, kind of like

44:20.560 --> 44:25.840
 friends will talk. I mean, there's objectness. There's all these things that it's kind of crazy

44:25.840 --> 44:31.920
 that we humans, uh, I guess evolved to have because it's useful for us to perceive the world.

44:31.920 --> 44:38.880
 Yeah. Correct. And it, it derives mostly from the properties of natural signals and, and so, um,

44:38.880 --> 44:44.320
 natural signals. So natural signals are the kind of things we'll perceive in the, in the natural

44:44.320 --> 44:48.560
 world. I don't know. I don't know why that sounds so beautiful. Natural signals. Yeah.

44:48.560 --> 44:52.640
 As opposed to a QR code, right? Which is an artificial signal that we created.

44:52.640 --> 44:57.600
 Humans are not very good at classifying QR codes. We are very good at saying something is a cat or

44:57.600 --> 45:01.920
 a dog, but not very good at, you know, we're classifying, whether computers are very good at

45:01.920 --> 45:08.480
 classifying QR codes. Um, so our, our visual system is tuned for natural signals. Uh, and

45:08.480 --> 45:12.800
 there are fundamental assumptions in the architecture that are derived from natural

45:12.800 --> 45:19.040
 signals, uh, properties. I wonder when you take, uh, hosts and genetic drugs, does that go into

45:19.040 --> 45:25.120
 natural or is that closer to the QR code? It's still natural. It's still natural. Yeah. Because

45:25.120 --> 45:30.000
 it's, it is still operating using our brains. By the way, on that, on that topic, I, I mean,

45:30.000 --> 45:34.800
 I haven't been following. I think they're becoming legalized and certain. I can't wait until they

45:34.800 --> 45:40.640
 become legalized to a degree that you like vision science researchers could study it. Yeah. Just

45:40.640 --> 45:47.600
 like through, through medical chemical ways, modify. There could be ethical concerns, but

45:47.600 --> 45:53.280
 modify. That's another way to study the brain is to be, be able to chemically modify it. It's

45:53.280 --> 45:59.840
 probably, um, probably very long a way to, to figure out how to do it ethically. Yeah. But I,

45:59.840 --> 46:05.520
 I think there are studies on that already. Yeah. I think so. Uh, because it's, it's not unethical

46:05.520 --> 46:13.040
 to give, uh, it to rats. Oh, that's true. That's true. There's a lot, there's a lot of

46:13.040 --> 46:18.480
 shrugged up rats out there. Okay. Cool. Sorry. Sorry to, it's okay. So there's, uh, so there's

46:18.480 --> 46:26.480
 these, uh, uh, low level, uh, things from natural signals that, uh, that, that can,

46:26.480 --> 46:33.360
 from which these properties will emerge. Yes. Uh, but it is still a very hard problem on how to

46:33.360 --> 46:38.960
 encode that. You know, so you don't, you know, there is no, uh, so, uh, you mentioned, um, the,

46:38.960 --> 46:44.960
 the, the priors, uh, Franchot wanted to encode in, uh, in the, uh, abstract reasoning challenge,

46:44.960 --> 46:51.040
 but it is not straightforward how to encode those priors. Um, so, so some of those, uh, challenges

46:51.040 --> 46:56.240
 like, you know, the objective completion challenges are things that we purely use our

46:56.240 --> 47:00.480
 visual system to do. It is, uh, it looks like abstract reasoning, but it is purely an output

47:00.480 --> 47:05.280
 of a, uh, the, the vision system. For example, completing the corners of that connoisseur triangle,

47:05.280 --> 47:09.120
 completing the lines of that connoisseur triangle. It's a purely a visual system property. You know,

47:09.120 --> 47:14.800
 it's not, there is no abstract reasoning involved. It uses all these priors, but it is stored in our

47:14.800 --> 47:21.920
 visual system in a particular way that is amenable to inference. And, and, and that is one of the

47:21.920 --> 47:27.200
 things that we tackled in the, you know, basically saying, okay, these are the prior knowledge, uh,

47:27.200 --> 47:32.400
 which, which will be derived from the word, but then how is that prior knowledge represented

47:32.400 --> 47:38.560
 in the model such that inference when, when some piece of evidence comes in can be done very

47:38.560 --> 47:44.160
 efficiently and in a very distributed way. Um, because it is very, there are so many ways of

47:44.160 --> 47:50.000
 representing knowledge, which is not amenable to very quick inference, you know, quick lookups.

47:50.000 --> 47:58.320
 Uh, and so that's one, um, core part of what we tackled in, uh, the RCN model. Um, uh,

47:58.320 --> 48:02.800
 how do you encode visual knowledge to, uh, do very quick inference and yeah.

48:02.800 --> 48:07.920
 Can you maybe comment on, uh, so folks listening to this and in general may be familiar with

48:08.560 --> 48:13.680
 different kinds of architectures of a neural networks. What, what are we talking about with

48:13.680 --> 48:17.760
 the RCN? Uh, what, what does, what does the architecture look like? What are different

48:17.760 --> 48:22.080
 components? Is it close to neural networks? Is it far away from neural networks? What does it

48:22.080 --> 48:27.920
 look like? Yeah. So, so you can, uh, think of the delta between the model and a convolutional

48:27.920 --> 48:32.720
 neural network if, if people are familiar with convolutional networks. So convolutional networks

48:32.720 --> 48:38.480
 have this feed forward processing cascade, which is called, uh, feature detectors and pooling.

48:38.480 --> 48:44.320
 And that is repeated in the, in the hierarchy in a, in a, uh, multi level, uh, system. Um, and

48:44.320 --> 48:49.440
 if you, if you want to, an intuitive idea of what, what is happening, feature detectors are,

48:49.440 --> 48:56.640
 uh, you know, detecting interesting co occurrences in the input. It can be a line, a corner,

48:56.640 --> 49:04.560
 a, an eye or a piece of texture, et cetera. And the pooling neurons are doing some local

49:04.560 --> 49:09.360
 transformation of that and making it invariant to local transformations. So this is what the

49:09.360 --> 49:16.400
 structure of convolutional neural network is. Um, recursive cortical network has a similar structure

49:16.400 --> 49:21.200
 when you look at just the feed forward pathway. But in addition to that, it is also structured in

49:21.200 --> 49:27.600
 a way that it is generative so that it can run it backward and combine the forward with the backward.

49:28.400 --> 49:35.440
 Another aspect that it has is it has lateral connections. These lateral connections,

49:35.440 --> 49:41.280
 um, which is between, so if you have an edge here and an edge here, it has connections between

49:41.280 --> 49:47.280
 these edges. It is not just feed forward connections. It is, um, something between these edges,

49:47.280 --> 49:52.160
 which is, uh, the, the nodes representing these edges, which is to enforce compatibility between

49:52.160 --> 49:56.800
 them. So otherwise what will happen is that constraints, it's a constraint. It's basically,

49:56.800 --> 50:03.760
 if you, if you do just feature detection followed by pooling, then your, your transformations in

50:03.760 --> 50:09.680
 different parts of the visual field are not coordinated. Uh, and so you can, you will create a

50:09.680 --> 50:15.200
 jagged, when you, when you generate from the model, you will create jagged, um, uh, things and

50:15.200 --> 50:21.840
 uncoordinated transformations. So these lateral connections are enforcing the, the transformations.

50:21.840 --> 50:28.960
 Is the whole thing still differentiable? Uh, no. Okay. No. It's not, it's not trade using, uh,

50:28.960 --> 50:34.560
 back prop. Okay. That's really important. So, uh, so there's these feed forward, there's feedback

50:34.560 --> 50:40.160
 mechanisms. There's some interesting connectivity things. It's still layered like, uh, uh,

50:40.160 --> 50:47.360
 multiple layers. Okay. Very, very interesting. Uh, and yeah. Okay. So the interconnection

50:47.360 --> 50:54.400
 between, um, adjacent, the connections across service constraints that they keep the thing

50:54.400 --> 51:01.120
 stable. Correct. Okay. So what, what else? Uh, and then there's this idea of doing inference.

51:01.120 --> 51:08.160
 A neural network does not do inference on the fly. So an example of why this inference is

51:08.160 --> 51:13.440
 important is, you know, so one of the first applications, uh, that we showed in the paper

51:13.440 --> 51:20.960
 was to crack, uh, text based captures. What are captures by the way? Uh, yeah. By the way,

51:20.960 --> 51:25.440
 one of the most awesome, like the people don't use this term anymore as human computation, I think.

51:26.320 --> 51:30.720
 Uh, I love this term. The guy who created captures, I think came up with this term.

51:30.720 --> 51:38.640
 Yeah. I love it. Anyway. Uh, yeah. Uh, what, what are captures? So captures are those strings

51:38.640 --> 51:43.200
 that you fill in, uh, when you're, you know, when, if you're opening a new account in Google,

51:43.200 --> 51:48.880
 they show you a picture. Hey, you know, usually it used to be set of garbled letters, uh, that you

51:48.880 --> 51:53.840
 have to kind of, uh, figure out what, what, what is that string of characters and type in. And the

51:53.840 --> 52:01.920
 reason captures exist is because, you know, um, Google or Twitter do not want automatic creation

52:01.920 --> 52:08.800
 of accounts. You can use a computer to create millions of accounts, uh, and, uh, use that for

52:08.800 --> 52:14.560
 in nefarious purposes. Uh, so you want to make sure that to the extent possible, the interaction

52:14.560 --> 52:20.800
 that, uh, their system is having is with a human. So it's a, it's called a human interaction proof.

52:20.800 --> 52:26.560
 A capture is a human interaction proof. Um, so, so this is a captures are by design,

52:27.280 --> 52:33.440
 things that are easy for humans to solve, but hard for computer. Hard for robots. Yeah. Um, so,

52:33.440 --> 52:41.040
 and text based captures, well, was the one which is prevalent and around 2014 because at that time

52:41.040 --> 52:47.600
 text based risk captures were hard for computers to crack. Even now they are actually in the sense of

52:47.600 --> 52:53.520
 an arbitrary text based capture will be unsolvable even now. But with the techniques that we have

52:53.520 --> 52:58.880
 developed, it can be, you know, you can quickly develop a mechanism that solves the capture.

52:58.880 --> 53:03.760
 They've probably gotten a lot harder too. They've been getting clever and clever

53:03.760 --> 53:09.680
 generating these text. Yeah. So okay. So that was one of the things you've tested on is these

53:09.680 --> 53:17.680
 kinds of captures in 2014, 15, that kind of stuff. So what, uh, well, I mean, why, by the way, why

53:17.680 --> 53:24.560
 captures? Yeah. Yeah. Even now I would say capture is a very, very good challenge problem. Uh, if you

53:24.560 --> 53:30.400
 want to understand how human perception works and if you want to build, uh, systems that work

53:30.400 --> 53:35.680
 like the human brain. Uh, and I wouldn't say capture is a solved problem. We have cracked

53:35.680 --> 53:41.280
 the fundamental defense of captures, but it is not solved in the way that humans solve it.

53:41.280 --> 53:47.280
 Um, so I can give you an example. I can, um, take a five year old child who has just learned

53:47.280 --> 53:54.320
 characters, uh, and, uh, show them any new capture that we create, they will be able to solve it.

53:55.360 --> 54:01.440
 Uh, I can show you pretty much any new capture from any new website. You'll be able to solve it

54:01.440 --> 54:06.320
 without getting any training examples from that particular style of capture. You're assuming

54:06.320 --> 54:13.120
 I'm human. Yeah. Yes. Yeah. Uh, that's right. So if you are human, otherwise I will be able to figure

54:13.120 --> 54:19.120
 that out using this one, but, uh, this whole podcast is just a touring test. That's a long,

54:19.120 --> 54:24.960
 a long touring test. Anyway, I'm sorry. So yeah. So she's human, humans can figure it out with very

54:24.960 --> 54:30.080
 few examples or no training examples, like no training examples from that particular style of

54:30.080 --> 54:37.360
 capture. Um, and, and so you can, you know, so, uh, even now this is unreachable for, uh, the current

54:37.360 --> 54:41.680
 deep learning system. So basically there is no, I don't think a system exists where you can basically

54:41.680 --> 54:48.240
 say train on whatever you want. And then now say, Hey, I will show you a new capture, which I did

54:48.240 --> 54:53.920
 not show you in, in the, in the training setup. Will the system be able to solve it? Um, it still

54:53.920 --> 55:00.640
 doesn't exist. So that is the magic of human perception. Yeah. And Doug Hofstadter, uh, put

55:00.640 --> 55:08.560
 this, uh, very beautifully in, uh, one of his, uh, talks, the, the central problem in AI is what is

55:08.560 --> 55:16.080
 the letter A. If you can, if you can build a system that reliably can detect all the variations of

55:16.080 --> 55:21.600
 the letter A, you don't even need to go to the, the, the V and the C. Yeah. You don't even know

55:21.600 --> 55:26.320
 to go to the V and the C or the strings of characters. And, uh, so that, that is the spirit

55:26.320 --> 55:30.320
 at which, you know, with, which we, uh, tackle that problem. What does it mean by that? I mean,

55:30.320 --> 55:38.880
 is, is it, uh, like without training examples, try to figure out the fundamental, uh, elements

55:38.880 --> 55:45.760
 that make up the letter A in all of its forms. In all of its forms, it can be, A can be made

55:45.760 --> 55:50.560
 with the two humans standing, leaning against each other, holding the hands and, uh, it can

55:50.560 --> 55:55.520
 be made of leaves. It can be. Yeah. You might have to understand, uh, everything about this world

55:55.520 --> 56:00.880
 in order to understand letter A. Yeah. So it's common sense reasoning, essentially. Yeah. Right.

56:00.880 --> 56:08.080
 So, so to finally, to really solve, finally to say that you have solved capture, uh, you have

56:08.080 --> 56:15.680
 to solve the whole problem. Yeah. Okay. So what, how does, uh, this kind of the RCN architecture

56:15.680 --> 56:21.600
 help us to get, uh, do a better job of that kind of thing? Yeah. So, uh, as I mentioned, one of

56:21.600 --> 56:27.440
 the important things was being able to do inference, being able to dynamically do inference. Can you,

56:27.440 --> 56:32.400
 can you, uh, can you, uh, clarify what you mean? Cause could you say like neural networks don't do

56:32.400 --> 56:38.160
 inference? Yeah. So what do you mean by inference in this context then? So, okay. So in captures,

56:38.160 --> 56:44.480
 what they do to confuse people is to make these characters crowd together. Yes. Okay. And when

56:44.480 --> 56:49.120
 you make the characters crowd together, what happens is that you will now start seeing

56:49.120 --> 56:53.920
 combinations of characters as some other new character or, or an existing character. So you

56:53.920 --> 57:00.720
 would, you would put an R and N together. It will start looking like an M. Uh, and, and so locally,

57:00.720 --> 57:07.600
 they are, you know, there, there is very strong evidence for it being, uh, some, uh, incorrect

57:07.600 --> 57:14.080
 character. But globally, the only explanation that fits together is something that is different

57:14.080 --> 57:21.200
 from what you find locally. Yes. So, so, so this is inference. You are basically taking, uh, local

57:21.200 --> 57:28.240
 evidence and putting it in the global context and often coming to a conclusion locally, which is

57:28.240 --> 57:34.480
 conflicting with the local information. So actually, so you mean inference, like, uh, in the way it's

57:34.480 --> 57:39.840
 used to, when you talk about reasoning, for example, uh, as opposed to like inference, which is this

57:39.840 --> 57:44.480
 with, you know, with artificial neural networks, which is a single pass to the network. Okay.

57:44.480 --> 57:50.080
 Okay. So like you're basically doing some basic forms of reasoning, like integration of like,

57:51.120 --> 57:55.760
 how local things fit into the, the global picture. And, and, and things like explaining

57:55.760 --> 58:00.320
 a way coming into this one, because you are, you are, uh, explaining that piece of evidence,

58:00.880 --> 58:06.080
 uh, as something else, uh, because globally, that's the only thing that makes sense. Um,

58:06.080 --> 58:13.680
 so now, uh, you can amortize this inference by, you know, in a neural network, if you want to do

58:13.680 --> 58:18.560
 this, what do you, you can, you can brute force it. You can just show it all combinations of things,

58:19.280 --> 58:25.280
 that you want to, you want to, uh, your reasoning to work over and you can, you know, like just

58:25.280 --> 58:30.400
 train the help out of that neural network and it will look like it is doing, uh, you know,

58:30.400 --> 58:36.080
 inference on the fly, but it is, it is really just doing amortized inference. It is because you,

58:36.080 --> 58:41.920
 you have shown it a lot of these combinations during training time. Um, so what you want to do is

58:42.640 --> 58:47.280
 be able to do dynamic inference rather than just being able to show all those combinations

58:47.280 --> 58:50.640
 in the training time. And that's something we emphasized in the model.

58:51.760 --> 58:56.160
 What does it mean dynamic inference? Is that, that has to do with the feedback thing?

58:56.160 --> 59:01.840
 Yes. Like what, what is dynamic? I'm trying to visualize what dynamic inference would be in

59:01.840 --> 59:06.640
 this case. Like what is it doing with the input? It's shown the input the first time.

59:07.680 --> 59:13.840
 Yeah. And is, is like, what's changing over temporarily over? What's the dynamics of this

59:13.840 --> 59:18.800
 inference process? So, so you can think of it as you have, um, at the top of the model,

59:18.800 --> 59:25.040
 the characters that you are trained on, they are the causes that you're trying to explain the pixels

59:25.040 --> 59:31.920
 using the characters as the causes. The, you know, the characters are the things that cause the pixels.

59:32.960 --> 59:37.600
 Yeah. So there's this causality thing. So the reason you mentioned causality, I guess,

59:37.600 --> 59:39.920
 is because there's a temporal aspect of this whole thing.

59:40.720 --> 59:44.160
 In this particular case, the temporal aspect is not important. It is more like when,

59:44.720 --> 59:50.400
 if, if I turn the character on the, the pixels will turn on. Yeah. It'll be after there's a

59:50.400 --> 59:55.520
 little bit, but yeah. So that is the causality in the sense of like a logic causality, like

59:55.520 --> 1:00:02.480
 hence inference. Okay. The dynamics is that, uh, even though locally it will look like, okay,

1:00:02.480 --> 1:00:08.960
 this is an a, uh, and, and locally just when I look at just that patch of the image,

1:00:08.960 --> 1:00:14.320
 it looks like an a, but when I look at it in the context of all the other causes,

1:00:14.320 --> 1:00:18.240
 it might not, you know, a is not the something that makes sense. So that is something you have to

1:00:18.240 --> 1:00:24.720
 kind of, you know, recursively figure out. Yeah. So, okay. So, uh, and, uh, this thing performed

1:00:24.720 --> 1:00:32.000
 pretty well on the captures. Correct. And, uh, I mean, is there some kind of interesting intuition

1:00:32.000 --> 1:00:37.280
 you can provide? Why did well, like what did it look like? Is there visualizations that

1:00:37.280 --> 1:00:42.240
 could be human interpretable to us humans? Yes. Yeah. So the good thing about the model is that

1:00:42.240 --> 1:00:48.240
 it is extremely, um, so it is not just doing a classification, right? It is, it is, it is, it is

1:00:48.240 --> 1:00:55.040
 providing a full explanation for the scene. So when, when it, when it, uh, operates on a scene,

1:00:55.040 --> 1:01:01.200
 it is coming at back and saying, look, this is the part is the a and these are the pixels that

1:01:01.200 --> 1:01:08.320
 turned on, uh, these are the pixels in the input that tells makes me think that it is an a and

1:01:08.320 --> 1:01:14.640
 also these are the portions I hallucinated. It, you know, it provides a complete explanation

1:01:14.640 --> 1:01:20.720
 of that form. And then these are the contours. These are, this is the interior and this is

1:01:20.720 --> 1:01:27.040
 in front of this other object. So that, that's the kind of, um, explanation it, uh, the inference

1:01:27.040 --> 1:01:36.400
 network provides. So, so that, that is useful and interpretable. Um, and, uh, um, then the kind

1:01:36.400 --> 1:01:44.240
 of errors it makes are also, I don't want to, um, read too much into it, but the kind of errors

1:01:44.240 --> 1:01:49.680
 the network makes are, uh, very similar to the kinds of errors humans would make in a, in a

1:01:49.680 --> 1:01:54.320
 similar situation. So there's something about the structure that's, uh, feels reminiscent of the way

1:01:54.320 --> 1:02:03.200
 humans, uh, visual system works. Well, I mean, uh, how hard coded is this to the capture problem?

1:02:03.200 --> 1:02:08.560
 This idea? Uh, not really hard coded because it's the, uh, the assumptions as I mentioned are

1:02:08.560 --> 1:02:14.560
 general, right? It is more, um, and, and those themselves can be applied in many situations

1:02:14.560 --> 1:02:20.640
 which are natural signals. Um, so it's, it's the foreground versus, uh, background factorization

1:02:20.640 --> 1:02:26.400
 and, uh, the factorization of the surfaces versus the contours. So these are all generally

1:02:26.400 --> 1:02:34.240
 applicable assumptions in all vision. So why, why capture, why attack the capture problem,

1:02:34.240 --> 1:02:39.680
 which is quite unique in the computer vision context versus like the traditional benchmarks

1:02:39.680 --> 1:02:44.880
 of image net and all those kinds of image classification or even segmentation tasks,

1:02:44.880 --> 1:02:49.200
 all that kind of stuff. Do you feel like that's, uh, I mean, what, what's your thinking about

1:02:49.200 --> 1:02:55.120
 those kinds of benchmarks in, um, in this, in this context? I mean, those benchmarks are useful

1:02:55.120 --> 1:03:00.720
 for deep learning kind of algorithms where you, you know, so the, the settings, uh, that deep

1:03:00.720 --> 1:03:07.040
 learning works in our, here is my huge training set and here is my test set. So the, the training

1:03:07.040 --> 1:03:13.440
 set is almost, uh, you know, 100x, 1000x bigger than, uh, the test set in many, many, many cases.

1:03:14.320 --> 1:03:21.680
 What we wanted to do was invert that. The training set is very smaller than the, the test set.

1:03:21.680 --> 1:03:29.760
 Yes. Uh, and, uh, uh, and, you know, uh, capture is a problem that is by definition

1:03:29.760 --> 1:03:35.280
 hard for computers and it has these good properties of strong generalization, strong

1:03:35.840 --> 1:03:41.360
 out of training distribution generalization. If you are interested in studying that, uh, and putting,

1:03:42.480 --> 1:03:46.720
 having your model have that property, then it's a, it's a good data set to tackle.

1:03:46.720 --> 1:03:52.880
 So is there, have you attempted to, which I think I believe there's quite a growing body of work

1:03:53.520 --> 1:04:02.080
 on looking at MNIST and ImageNet without training. So like taking, like the basic challenge is how,

1:04:02.080 --> 1:04:08.720
 what tiny fraction of the training set can we take in order to do a reasonable job

1:04:08.720 --> 1:04:14.960
 of the classification task? Have, have you explored that angle in these classic benchmarks?

1:04:14.960 --> 1:04:20.240
 Yes. So, so we did do MNIST. So, um, you know, so it's not just capture. We, uh, so there was,

1:04:20.240 --> 1:04:26.560
 uh, also, uh, uh, uh, versions of multiple versions of MNIST, including the, the standard

1:04:26.560 --> 1:04:30.800
 version, which, where we inverted the problem, which is basically saying, rather than train on

1:04:30.800 --> 1:04:38.320
 60,000, uh, training data, uh, you know, how, uh, quickly can you get, uh, to high level accuracy

1:04:38.320 --> 1:04:39.680
 with very little training data?

1:04:39.680 --> 1:04:45.200
 Was, is there some, uh, performance that you remember? Like how well, how well did it do?

1:04:45.200 --> 1:04:47.360
 How many examples did it need?

1:04:47.360 --> 1:04:53.600
 Yeah. I, I, you know, I remember that it was, you know, uh, on the order of, uh,

1:04:55.120 --> 1:05:00.880
 tens or hundreds of examples to get into, uh, 95% accuracy. And it was, it was definitely

1:05:00.880 --> 1:05:03.840
 better than the systems, other systems out there at that time.

1:05:03.840 --> 1:05:04.720
 At that time. Yeah.

1:05:04.720 --> 1:05:08.320
 Yeah. They're really pushing it. I think that's a really interesting space, actually.

1:05:08.320 --> 1:05:16.800
 Uh, I think there's an actual name for MNIST that, uh, like there's different names to the

1:05:16.800 --> 1:05:21.600
 different sizes of training sets. I mean, people are like attacking this problem. I think it's

1:05:21.600 --> 1:05:22.720
 super interesting. Yeah.

1:05:22.720 --> 1:05:30.720
 It's funny how like the MNIST will probably be with us all the way to AGI as the data set that

1:05:30.720 --> 1:05:37.520
 just sticks by. It is, it's a clean, simple, uh, data set to, uh, to study the fundamentals

1:05:37.520 --> 1:05:42.800
 of learning with just like CAPTCHAs. It's interesting. Not enough people, I don't know,

1:05:42.800 --> 1:05:47.840
 maybe you can correct me, but I feel like CAPTCHAs don't show up as often in papers as they probably

1:05:47.840 --> 1:05:53.600
 should. That's correct. Yeah. Because, you know, um, usually these things have a momentum, uh,

1:05:53.600 --> 1:05:58.880
 you know, once, once, uh, something gets established as a standard benchmark.

1:05:58.880 --> 1:06:04.560
 Yeah. That is a, there is a, uh, there is a dynamics of, uh, how graduate students operate

1:06:04.560 --> 1:06:10.720
 and how the academic system works that, uh, pushes people to track that, uh, benchmark.

1:06:10.720 --> 1:06:12.000
 Yeah. To folk.

1:06:12.000 --> 1:06:15.280
 Yeah. So nobody wants to think outside the box. Okay.

1:06:16.640 --> 1:06:22.560
 Okay. So good performance on the CAPTCHAs. What else is there interesting, um,

1:06:22.560 --> 1:06:25.520
 on the RCN side before we talk about the cortical microspeed?

1:06:25.520 --> 1:06:31.120
 Yeah. So the, the same model, so the, the, the important part of the model was that it

1:06:31.120 --> 1:06:36.560
 trains very quickly with very little training data and it's, uh, you know, quite robust to

1:06:36.560 --> 1:06:44.240
 out of distribution, uh, perturbations. Um, and, uh, and we are using that, uh, very, uh, fruitfully

1:06:44.240 --> 1:06:48.000
 in, uh, advocatious in many of the robotics tasks we are solving.

1:06:48.000 --> 1:06:55.040
 So, you know, let me ask you this kind of touchy question. I have to, I've spoken with, uh, your

1:06:55.040 --> 1:07:00.800
 friend, colleague, Jeff Hawkins, too. I mean, he's, uh, I have to kind of ask, there is a bit,

1:07:01.920 --> 1:07:08.400
 whenever you have brain inspired stuff and you make big claims, uh, big sexy claims,

1:07:08.400 --> 1:07:14.480
 there's a, you know, uh, there's critics, I mean, machine learning subreddit.

1:07:14.480 --> 1:07:19.120
 Don't get me started on those people. Uh, they're hard. I mean, criticism is good,

1:07:19.120 --> 1:07:25.440
 but they're a bit, uh, they're a bit over the top. Um, there is quite a bit of sort of skepticism

1:07:25.440 --> 1:07:32.000
 and criticism. You know, is this work really as good as it promises to be? Yeah. Do you have thoughts

1:07:32.000 --> 1:07:37.840
 on that kind of skepticism? Do you have comments on the kind of criticism I might have received,

1:07:37.840 --> 1:07:44.480
 uh, about, you know, is this approach legit? Is this, is this a promising approach? Yeah.

1:07:44.480 --> 1:07:50.960
 Or at least as promising as it seems to be, you know, advertised as? Yeah, I can comment on it.

1:07:50.960 --> 1:07:56.560
 Um, so, you know, our, uh, our scene paper is, uh, published in science, which I would argue is,

1:07:56.560 --> 1:08:02.160
 is a very high quality journal, very hard to, uh, publish in and use, you know, usually it is

1:08:02.160 --> 1:08:10.160
 indicative of the, of the quality of the work. And, um, uh, I can, I can, I am very, very certain that

1:08:10.160 --> 1:08:15.120
 the ideas that we brought together in that paper, uh, in terms of the importance of feedback connections,

1:08:15.120 --> 1:08:21.440
 uh, recursive inference, lateral connections, uh, coming to best explanation of the scene as the

1:08:21.440 --> 1:08:28.720
 problem to solve, trying to solve, uh, recognition, segmentation, uh, all jointly in a way that is

1:08:28.720 --> 1:08:32.880
 compatible with higher level cognition, top down attention, all those ideas that we brought

1:08:32.880 --> 1:08:36.960
 together into something, you know, coherent and workable in the, uh, in the world and solving

1:08:36.960 --> 1:08:41.920
 and challenging, tackling a challenging problem. I think that will, that will stay and that, that

1:08:41.920 --> 1:08:48.560
 contribution I stand by, right? Now, uh, I can, I can tell you a story, uh, which is funny in the,

1:08:48.560 --> 1:08:52.640
 in the context of this, right? Um, so if you read the abstract of the paper and like, you know,

1:08:52.640 --> 1:08:56.720
 the argument we are putting in, you know, we are putting in, look, current deep learning systems

1:08:56.720 --> 1:09:02.400
 take a lot of training data. Uh, they don't use these insights and here is our new model,

1:09:02.400 --> 1:09:05.680
 which is not a deep neural network. It's a graphical model. It does inference. This is

1:09:05.680 --> 1:09:11.200
 what, how the paper is, right? Now, once the paper was accepted and everything, um, it went

1:09:11.200 --> 1:09:15.840
 to the press department in, in science, you know, to play as science office. We, we didn't do any

1:09:15.840 --> 1:09:19.600
 press release when it was published. It was, it went to the press department. What did the,

1:09:19.600 --> 1:09:23.200
 what was the press release that they wrote up? A new deep learning model.

1:09:24.880 --> 1:09:30.720
 Solves captchas. Solves captchas. And, uh, so, so you can see where was, you know, what was being

1:09:30.720 --> 1:09:38.400
 hyped in that, uh, thing, right? So there is the, there is a dynamic in the, uh, in the community

1:09:38.400 --> 1:09:43.840
 of, you know, so, uh, um, that's especially happens when there are lots of new people

1:09:43.840 --> 1:09:48.480
 coming into the field and they get attracted to one thing and some people are trying to think

1:09:48.480 --> 1:09:54.000
 different compared to that. So there is, there is some, uh, I think skepticism is science is

1:09:54.000 --> 1:10:01.120
 important and it is, um, you know, very much, uh, required, but it's also, it's not, uh, skepticism

1:10:01.120 --> 1:10:06.080
 usually it's mostly bandwagon effect that is happening rather than in, well, well, but that's

1:10:06.080 --> 1:10:11.840
 not even that. I mean, I'll tell you what they react to, which is like, uh, I'm sensitive to as

1:10:11.840 --> 1:10:18.080
 well. If you, if you look at just companies open AI deep mind, um, vicarious, I mean, they just,

1:10:18.080 --> 1:10:26.320
 there's, uh, there's a little bit of a race to the top and hype, right? It's, it's like,

1:10:27.120 --> 1:10:36.480
 it doesn't pay off to be humble. So like, uh, and, and the press is just, uh, irresponsible

1:10:36.480 --> 1:10:41.200
 often they, they just, I mean, don't get me started on the state of journalism today.

1:10:41.200 --> 1:10:46.480
 Like it seems like the people who write articles about these things, they literally have not even

1:10:46.480 --> 1:10:52.480
 spent an hour on the Wikipedia article about what is neural networks. Like they haven't like

1:10:52.480 --> 1:11:03.280
 invested just even the language to laziness. It's like, uh, robots beat humans. Like they,

1:11:03.280 --> 1:11:08.480
 they write this kind of stuff that just, uh, and then, and then of course the researchers are

1:11:08.480 --> 1:11:13.040
 quite sensitive to that, uh, because it gets a lot of attention. They're like, why did this

1:11:13.040 --> 1:11:18.880
 work get so much attention? Uh, you know, that's, that's over the top and people get really sensitive,

1:11:18.880 --> 1:11:23.920
 you know, the same kind of criticism with, um, opening. I did work with the Rubik's cube with

1:11:23.920 --> 1:11:30.640
 the robot that people criticized, uh, same with GPT two and three. They criticize, uh, same thing

1:11:30.640 --> 1:11:38.240
 with, uh, deep minds with alpha zero. I mean, yeah, I, I'm sensitive to it. Um, but, and of

1:11:38.240 --> 1:11:43.360
 course with your work, it mentioned deep learning, but there's something super sexy to the public

1:11:43.360 --> 1:11:49.040
 about brain inspired. I mean, that immediately grabs people's imagination, not even like

1:11:50.000 --> 1:11:57.360
 neural networks, but like really brain inspired. Like brain, like brain like neural networks,

1:11:57.360 --> 1:12:03.360
 that seems really compelling to people. And, um, to me as well, to, to the world as a narrative.

1:12:03.360 --> 1:12:11.920
 And so, uh, people hook up, hook on to that. And, uh, sometimes you, uh, the skepticism engine turns

1:12:11.920 --> 1:12:19.520
 on in the research community and they're skeptical, but I think putting aside the ideas of the actual

1:12:19.520 --> 1:12:25.040
 performance and captures or performance, any data set, I mean, to me, all these data sets are

1:12:25.760 --> 1:12:31.040
 useless anyway. It's nice to have them. Uh, but in the grand scheme of things, they're silly toy

1:12:31.040 --> 1:12:37.920
 examples. The point is, is there intuition about the, the idea is just like you mentioned,

1:12:38.480 --> 1:12:43.600
 bringing the ideas together in the unique way. Is there something there? Is there some value

1:12:43.600 --> 1:12:47.680
 there? And is it going to stand the test of time? Yes. And that's the hope. That's the hope.

1:12:47.680 --> 1:12:54.080
 Yes. Uh, I'm, my confidence there is very high. I, you know, I don't treat brain inspired as a

1:12:54.080 --> 1:13:01.760
 marketing term. Uh, you know, I am looking into the details of biology and, and puzzling over,

1:13:02.560 --> 1:13:07.840
 uh, those things. And I am, I am grappling with those things. And so it is, it is not a marketing

1:13:07.840 --> 1:13:12.160
 term at all. It, you know, you can use it as a marketing term and, and people often use it.

1:13:12.160 --> 1:13:17.360
 And you can get combined with them. And when, when people don't understand how we are approaching

1:13:17.360 --> 1:13:22.640
 the problem, it is, it is easy to be, uh, misunderstood and, you know, think of it as,

1:13:22.640 --> 1:13:26.800
 you know, purely, uh, marketing, but that's not the way, uh, we are. So you really,

1:13:28.000 --> 1:13:35.280
 I mean, as a scientist, you believe that if we kind of just stick to really understanding the brain,

1:13:35.280 --> 1:13:40.000
 that's going to, that's the right, like you, you should constantly meditate on the,

1:13:40.720 --> 1:13:44.960
 how does the brain do this? Because that's going to be really helpful for engineering

1:13:44.960 --> 1:13:51.280
 intelligent systems. Yes. You need to, so I think it is, it's one input and it is, it is helpful,

1:13:51.280 --> 1:13:58.480
 but you, you should know when to deviate from it too. Um, so an example is convolutional neural

1:13:58.480 --> 1:14:05.360
 networks, right? Uh, convolution is not an operation brain in, uh, implements. The visual

1:14:05.360 --> 1:14:11.280
 cortex is not convolutional. Visual cortex has local receptive fields, local connectivity,

1:14:11.280 --> 1:14:18.080
 but they, you know, the, um, there is no translation in, in variants in the, um,

1:14:18.080 --> 1:14:25.200
 uh, the network weights, um, in, in the visual cortex, that is a, uh, computational

1:14:25.200 --> 1:14:29.920
 trick, which is a very good engineering trick that we use for sharing the training between

1:14:29.920 --> 1:14:35.760
 the different, uh, nodes. Um, so, uh, and, and that trick will be with us for some time. It will

1:14:35.760 --> 1:14:44.400
 go away when we have, um, uh, uh, robots with eyes and heads that move. Uh, and so then the,

1:14:44.400 --> 1:14:48.320
 that trick will go away. It will not be, uh, useful at that time. So,

1:14:48.320 --> 1:14:53.920
 uh, so the brain doesn't, so the brain doesn't have translational invariance. It has the focal

1:14:53.920 --> 1:14:58.640
 point. Like it has a thing it focuses on. Correct. It has, it has a phobia and, and because of the

1:14:58.640 --> 1:15:04.800
 phobia, um, the, the receptive fields are not like the copying of the weights. Like the, the,

1:15:04.800 --> 1:15:07.680
 the weights in the center are very different from the weights in the periphery.

1:15:07.680 --> 1:15:13.600
 Yes. At the periphery. I mean, I did this, uh, actually wrote a paper and just gotten a chance

1:15:13.600 --> 1:15:21.840
 to really study peripheral peripheral vision, which is a fascinating thing. Very under understood

1:15:21.840 --> 1:15:28.480
 thing of what the brain, you know, at every level the brain does with the periphery. It does some

1:15:28.480 --> 1:15:35.280
 funky stuff. Yeah. So it's, uh, it's another kind of trick than, uh, convolutional. Like it does,

1:15:35.280 --> 1:15:42.720
 it, uh, it's a, you know, convolution, your convolution in neural networks is a trick to,

1:15:42.720 --> 1:15:47.040
 for efficiency is efficiency trick. And the brain does a whole nother kind of thing.

1:15:47.040 --> 1:15:53.200
 Correct. Correct. So, so you need to understand the principles or processing so that you can

1:15:53.200 --> 1:15:57.680
 still apply engineering tricks when, where you want it to be. You don't want to be slavishly

1:15:57.680 --> 1:16:02.720
 mimicking all the things of the brain. Um, and, and so yeah, so it should be one input. And I

1:16:02.720 --> 1:16:08.400
 think it is extremely helpful. Uh, but you, it should be the point of really understanding so

1:16:08.400 --> 1:16:14.800
 that you know when to deviate from it. So, okay. That's really cool. That's worked from a few years

1:16:14.800 --> 1:16:22.000
 ago. So you, uh, you did work in New Menta with Jeff Hawkins. Yeah. Uh, with, uh, hierarchical

1:16:22.000 --> 1:16:30.000
 temporal memory. How is your just, if you could give a brief history, how is your view of the way

1:16:30.000 --> 1:16:36.160
 the models of the brain changed over the past few years leading up to, to now? Is there some

1:16:36.160 --> 1:16:42.000
 interesting aspects where there was an adjustment to your understanding of the brain or is it all

1:16:42.000 --> 1:16:47.760
 just building on top of each other? In terms of the higher level ideas, uh, especially the ones

1:16:47.760 --> 1:16:52.400
 Jeff wrote about in the book, if you, if you blur out, right, you know, on intelligence, right,

1:16:52.400 --> 1:16:56.720
 on intelligence, if you, if you blur out the details and, and if you just zoom out and the

1:16:56.720 --> 1:17:01.920
 higher level idea, uh, things are, I would say consistent with what he wrote about, but,

1:17:02.560 --> 1:17:05.680
 but many things will be consistent with that because it is, it's a blur, you know, when you,

1:17:05.680 --> 1:17:10.960
 when you, you know, deep learning systems are also, you know, multilevel hierarchical, all of

1:17:10.960 --> 1:17:18.080
 those things, right? So, so at the, but, um, in terms of the detail, a lot of things are different,

1:17:18.080 --> 1:17:25.680
 uh, and, and, and those details matter a lot. Um, so, so one point of difference I had with Jeff,

1:17:25.680 --> 1:17:33.600
 uh, uh, was, uh, how to approach, you know, how much of biological possibility and realism

1:17:33.600 --> 1:17:40.880
 do you want in the learning algorithms? Um, so, uh, when I was there, uh, this was, you know,

1:17:40.880 --> 1:17:47.280
 almost 10 years ago now. So, yeah, I don't know, I don't know what Jeff thinks now, but 10 years

1:17:47.280 --> 1:17:55.200
 ago, uh, the difference was that I did not want to be so constrained on saying, uh, my learning

1:17:55.200 --> 1:18:01.040
 algorithms want to need to be biologically plausible, um, based on some filter of biological

1:18:01.040 --> 1:18:07.360
 possibility available at that time. To me, that is a dangerous cut to make because we are, you know,

1:18:07.920 --> 1:18:12.320
 discovering more and more things about the brain all the time, new biophysical mechanisms, new

1:18:12.320 --> 1:18:18.880
 channels, uh, are being discovered all the time. So I don't want to upfront kill off and, uh, a

1:18:18.880 --> 1:18:25.920
 learning algorithm just because we don't really understand the fold, uh, the full, uh, the biophysics

1:18:25.920 --> 1:18:30.800
 or whatever of how the brain learns. Exactly. Exactly. But let me ask a search and drop,

1:18:30.800 --> 1:18:36.000
 like what's our, what's your sense? What's our best understanding of how the brain learns?

1:18:36.560 --> 1:18:42.640
 So things like back propagation, credit assignment. So, so many of these algorithms

1:18:42.640 --> 1:18:48.080
 have learning algorithms have things in common, right? It is a back propagation is one way of

1:18:48.080 --> 1:18:53.040
 credit assignment. There is another algorithm called expectation maximization, which is,

1:18:53.040 --> 1:18:58.000
 you know, another weight adjustment algorithm. But is it your sense the brain does something

1:18:58.000 --> 1:19:04.400
 like this? Has to. There is no way around it in the sense of saying that you do have to adjust the

1:19:05.280 --> 1:19:08.720
 the connections. So yeah. And you're saying credit assignment, you have to reward the

1:19:08.720 --> 1:19:13.200
 connections that were useful in making a correct prediction and not, yeah, I guess,

1:19:13.200 --> 1:19:18.240
 brought up, but yeah, it doesn't have to be differentiable. I mean, it doesn't have to be

1:19:18.240 --> 1:19:24.240
 differentiable. Yeah. But you have to have a, you know, you have a model that you start with,

1:19:24.240 --> 1:19:30.880
 you have data comes in and you have to have a way of adjusting the model such that it better

1:19:30.880 --> 1:19:36.320
 fits the data. Yeah. So that, that is all of learning, right? And some of them can be using

1:19:36.320 --> 1:19:44.320
 backprop to do that. Some of it can be using, you know, very local graph changes to do that.

1:19:45.440 --> 1:19:52.640
 There can be, you know, many of these learning algorithms have similar update properties locally

1:19:53.920 --> 1:19:59.280
 in terms of what the neurons need to do locally. I wonder if small differences in learning

1:19:59.280 --> 1:20:03.520
 algorithms can have huge differences in the actual effect. So the dynamics of,

1:20:03.520 --> 1:20:13.520
 I mean, sort of the reverse like spiking, like if credit assignment is like a lightning versus

1:20:13.520 --> 1:20:22.320
 like a rainstorm or something, like whether, whether there's like a looping local type of

1:20:22.320 --> 1:20:29.920
 situation with the credit assignment, whether there is like regularization, like how,

1:20:29.920 --> 1:20:37.760
 how, how it injects robustness into the whole thing, like whether it's chemical or electrical

1:20:38.640 --> 1:20:44.400
 or mechanical. Yeah. All those kinds of things. Yes. I feel like it, that,

1:20:45.760 --> 1:20:51.040
 yeah, I feel like those differences could be essential, right? It could be. It's just that

1:20:51.040 --> 1:20:59.200
 you don't know enough to, on the learning side, you don't know enough to say that is definitely

1:20:59.200 --> 1:21:04.400
 not the way the brain does it. Got it. So you don't want to be stuck to it. So that, yeah. So

1:21:04.400 --> 1:21:09.120
 you've been open minded on that side of things. On the inference side, on the recognition side,

1:21:09.120 --> 1:21:14.800
 I am much more amenable to being constrained because it's much easier to do experiments

1:21:14.800 --> 1:21:18.480
 because, you know, it's like, okay, here's the stimulus. You know, how many steps did it get

1:21:18.480 --> 1:21:24.320
 to take the answer? I can trace it back. I can, I can understand the speed of that computation,

1:21:24.320 --> 1:21:29.920
 et cetera, much more readily on the inference side. Got it. And then you can't do good experiments

1:21:29.920 --> 1:21:36.640
 on the learning side. Correct. So that, let's, let's go right into the cortical microcircuits

1:21:36.640 --> 1:21:44.160
 right back. So what, what are these ideas beyond recursive cortical network that you're looking

1:21:44.160 --> 1:21:52.080
 at now? So we have made a, you know, pass through multiple of the steps that, you know, as I mentioned

1:21:52.080 --> 1:21:56.720
 earlier, you know, we were looking at perception from the angle of cognition, right? It was not

1:21:56.720 --> 1:22:01.920
 just perception for perception sake. How do you, how do you connect it to cognition? How do you

1:22:01.920 --> 1:22:07.920
 learn concepts? And how do you learn abstract reasoning? Similar to some of the things Francois

1:22:08.720 --> 1:22:16.320
 talked about, right? So, so we have taken one pass through it, basically saying,

1:22:16.320 --> 1:22:22.880
 what is the basic cognitive architecture that you need to have, which has a perceptual system,

1:22:22.880 --> 1:22:29.840
 which has a system that learns dynamics of the world, and then has something like a routine

1:22:29.840 --> 1:22:35.440
 program learning system on top of it to learn concepts. So we have, we have built one, the,

1:22:35.440 --> 1:22:41.680
 you know, the version point one of that system. This was another science robotics paper. It is,

1:22:41.680 --> 1:22:46.400
 it's the title of that paper was, you know, something like cognitive programs. How do you build

1:22:46.400 --> 1:22:53.440
 cognitive programs? And, and the application there was on manipulation, robotic manipulation?

1:22:53.440 --> 1:23:00.400
 It was, it was, so think of it like this. Suppose you wanted to tell a new person

1:23:01.200 --> 1:23:05.840
 that you met, you don't know the language, or that person uses, you want to communicate to that

1:23:05.840 --> 1:23:13.840
 person to achieve some task, right? So I want to say, hey, you need to pick up all the red

1:23:13.840 --> 1:23:19.360
 cups from the kitchen counter, and put it here, right? How do you communicate that, right? You

1:23:19.360 --> 1:23:25.600
 can show pictures, you can basically say, look, this is the starting state, the things are here,

1:23:25.600 --> 1:23:30.400
 this is the ending state. And, and what does the person need to understand from that, the person

1:23:30.400 --> 1:23:34.960
 need to understand what conceptually happened in those pictures from the input to the output,

1:23:34.960 --> 1:23:42.480
 right? So, so we are looking at preverbal conceptual understanding without language.

1:23:42.480 --> 1:23:49.360
 How do you, how do you have a set of concepts that you can manipulate in your head? And from a

1:23:50.000 --> 1:23:55.680
 in a set of images of input and output, can you infer what is happening in those images?

1:23:56.800 --> 1:24:01.360
 Got it. With concepts that are pre language. Okay. So what does it mean for concept to be

1:24:01.360 --> 1:24:12.080
 pre language? Like, yeah, why so why is language so important here? So I want to make a distinction

1:24:12.080 --> 1:24:19.600
 between concepts that are just learned from text by just just feeding brute force text.

1:24:20.560 --> 1:24:26.880
 You can, you can start extracting things like, okay, cow is likely to be on grass.

1:24:26.880 --> 1:24:34.640
 So those kinds of things, you can extract purely from text. But that's kind of a simple

1:24:34.640 --> 1:24:40.000
 association thing rather than a concept as an abstraction of something that happens in the

1:24:40.000 --> 1:24:46.800
 real world, you know, in a grounded way, that I can, I can simulate it in my mind and connect it

1:24:46.800 --> 1:24:53.360
 back to the real world. And you think kind of the visual, the visual world concepts in the visual

1:24:53.360 --> 1:25:00.720
 world are somehow lower level than just the language. The lower level kind of makes it feel

1:25:00.720 --> 1:25:09.520
 like, okay, that's like an unimportant like it's more like, I would say the concepts in the visual

1:25:10.160 --> 1:25:17.200
 and the motor system and, you know, the concept learning system, which if you cut off the language

1:25:17.200 --> 1:25:21.920
 part, just the just what we learn by interacting with the world and abstractions from that,

1:25:21.920 --> 1:25:25.600
 that is a prerequisite for any real language understanding.

1:25:26.480 --> 1:25:31.520
 So you're, so you disagree with Chomsky, because he says language is at the bottom of everything.

1:25:32.080 --> 1:25:38.800
 No, I, I, yeah, I disagree with Chomsky completely from universal grammar to, yeah.

1:25:39.760 --> 1:25:43.120
 So that was a paper in science beyond the recursive cortical network.

1:25:44.080 --> 1:25:49.120
 What, what other interesting problems are there, the open problems and brain inspired

1:25:49.120 --> 1:25:53.760
 approaches that you're thinking about? I mean, everything is over, right? Like,

1:25:53.760 --> 1:26:01.600
 you know, no, no, no problem is solved, solved, right? I think of perception as kind of the

1:26:01.600 --> 1:26:08.320
 first thing that you have to build, but the last thing that you will be actually solved.

1:26:09.680 --> 1:26:15.040
 So, because if you do not build perception system in the right way, you cannot build

1:26:15.040 --> 1:26:19.920
 concept system in the right way. So, so you have to build a perception system. However,

1:26:19.920 --> 1:26:24.640
 wrong that might be, you have to still build that and learn concepts from there and then,

1:26:24.640 --> 1:26:30.480
 you know, keep it trading. And, and finally, perception will get solved fully when perception,

1:26:30.480 --> 1:26:33.280
 cognition, language, all those things work together. Finally.

1:26:33.920 --> 1:26:40.160
 So what, and so great, we've talked a lot about perception, but then maybe on the concept side

1:26:40.160 --> 1:26:47.280
 and like common sense, or just general reasoning side, is there some, some intuition you can

1:26:47.280 --> 1:26:53.760
 draw from the brain about how we could do that? So I have, I have this classic example I give.

1:26:55.440 --> 1:27:01.120
 So suppose I give you a few sentences, and then ask you a question following that sentence,

1:27:01.120 --> 1:27:06.720
 this is a natural language processing problem, right? So here goes. I'm telling you,

1:27:06.720 --> 1:27:14.560
 Sally pounded a nail on the ceiling. Okay. That's a sentence. Now I'm asking you a question.

1:27:14.560 --> 1:27:19.760
 What's the nail horizontal or vertical? Vertical. Okay. How did you answer that?

1:27:22.240 --> 1:27:26.640
 Well, I imagined Sally, it was kind of hard to imagine what the hell she was doing, but

1:27:29.760 --> 1:27:36.160
 but I imagined I had a visual of the whole situation. Exactly. Exactly. So, so, so here,

1:27:36.160 --> 1:27:41.360
 you know, I, I posed a question in natural language. The answer to that question was you,

1:27:41.360 --> 1:27:46.880
 you got the answer from actually simulating the scene. Now I can go more and more detail about,

1:27:46.880 --> 1:27:51.680
 okay, was Sally standing on something while doing this, you know, could, could she have been

1:27:52.480 --> 1:27:56.960
 standing on a light bulb to do this? You know, I could, I could ask more and more questions

1:27:56.960 --> 1:28:01.680
 about this. And I can ask, make you simulate the scene in, seen in more and more detail, right?

1:28:01.680 --> 1:28:08.560
 Where is all that knowledge that you're accessing stored? It is not in your language system. It is

1:28:08.560 --> 1:28:14.720
 not, it was not just by reading text, you got that knowledge. It is stored from the everyday

1:28:14.720 --> 1:28:20.560
 experiences that you have had from and by the, by the age of five, you, you have pretty much all

1:28:20.560 --> 1:28:26.640
 of this, right? And it is stored in your visual system, motor system in a way such that it can

1:28:26.640 --> 1:28:33.440
 be accessed through language. I got it. I mean, right. So here, the language is just almost

1:28:33.440 --> 1:28:38.160
 serves the query into the whole visual cortex and that does the whole feedback thing. But I mean,

1:28:38.160 --> 1:28:45.680
 it is all reasoning kind of connected to the perception system in some way. You can do a lot

1:28:45.680 --> 1:28:52.400
 of it, you know, you can still do a lot of it by quick associations without having to go into the

1:28:52.400 --> 1:28:57.920
 depth. And most of the time, you will be right, right? You can just do quick associations, but

1:28:57.920 --> 1:29:02.160
 I can easily create tricky situations for you where that quick associations is wrong,

1:29:02.160 --> 1:29:07.920
 and you have to actually run the simulation. So the figuring out the, how these concepts connect.

1:29:09.600 --> 1:29:14.720
 Do you have a good idea of how to do that? That's exactly what that's the one of the problems

1:29:14.720 --> 1:29:19.920
 that we are working on. And, and, and the, the way we are approaching that is basically saying,

1:29:19.920 --> 1:29:28.960
 okay, you need to, so the, the takeaway is that language is simulation control. And your perceptual

1:29:28.960 --> 1:29:36.960
 plus motor system is building a simulation of the world. And so, so that's basically the way

1:29:36.960 --> 1:29:42.080
 we are approaching it. And the first thing that we built was a controllable perceptual system.

1:29:42.080 --> 1:29:47.440
 And we built a schema networks, which was a controllable dynamic system. Then we built a

1:29:47.440 --> 1:29:53.120
 concept learning system that puts all these things together into programs as abstractions that you

1:29:53.120 --> 1:29:59.280
 can run and simulate. And now we are taking the step of connecting it to language. And, and

1:30:00.240 --> 1:30:04.880
 it will be very simple examples initially, it will not be the GPT three like examples,

1:30:04.880 --> 1:30:12.640
 but it will be grounded simulation based language. And for like the, the querying would be like question

1:30:12.640 --> 1:30:19.760
 answering kind of thing. And it will be in some simple world initially on, you know, but it will

1:30:19.760 --> 1:30:25.840
 be about, okay, can the system connect the language and ground it in the right way and run the right

1:30:25.840 --> 1:30:30.160
 simulations to come up with the answer. And the goal is to try to do things that, for example,

1:30:30.160 --> 1:30:39.360
 GPT three couldn't do. Speaking of which, if we could talk about GPT three a little bit, I think

1:30:39.360 --> 1:30:45.760
 it's an interesting thought provoking set of ideas that open the eyes pushing forward. I think it's

1:30:45.760 --> 1:30:50.800
 good for us to talk about the limits and the possibilities and you know, that work. So in

1:30:50.800 --> 1:30:58.000
 general, what are your thoughts about this recently released very large 175 billion parameter

1:30:58.000 --> 1:31:04.240
 language model? So I have, I haven't directly evaluated it yet from what I have seen on Twitter

1:31:04.240 --> 1:31:09.600
 and you know, other people evaluating it, it looks very intriguing. You know, I am very intrigued by

1:31:09.600 --> 1:31:16.960
 some of the properties it is displaying. And of course, the text generation part of that was

1:31:16.960 --> 1:31:23.840
 already evident in GPT two, you know, that it can generate coherent text over long distances.

1:31:23.840 --> 1:31:29.680
 That was, but of course, the weaknesses are also pretty visible in saying that, okay,

1:31:29.680 --> 1:31:36.080
 it is not really carrying a world state around. And, you know, sometimes you get sentences like,

1:31:36.080 --> 1:31:41.360
 I went up the hill to reach the valley or the thing that some, you know, completely

1:31:41.360 --> 1:31:46.560
 incompatible statements or when you're traveling from one place to the other, it doesn't take

1:31:46.560 --> 1:31:51.600
 into account the time of travel, things like that. So those things, I think will happen less than

1:31:51.600 --> 1:31:58.320
 GPT three, because it is trained on even more data. And so, and it has, it can do even more

1:31:58.320 --> 1:32:05.600
 longer distance coherence. But it will still have the fundamental limitations that it doesn't

1:32:05.600 --> 1:32:12.000
 have a world model. And it can't run simulations in its head to find whether something is true

1:32:12.000 --> 1:32:17.680
 in the world or not. Do you think within, so it's taking a huge amount of text from the internet

1:32:17.680 --> 1:32:24.800
 and forming a compressed representation, do you think in that could emerge something that's an

1:32:24.800 --> 1:32:33.200
 approximation of a world model, which essentially could be used for reasoning? I'm not talking

1:32:33.200 --> 1:32:38.800
 about GPT three, I'm talking about GPT four, five and GPT 10. Yeah, I mean, they will look

1:32:38.800 --> 1:32:45.360
 more impressive than GPT three. So you can, if you take that to the extreme, then a Markov chain

1:32:45.360 --> 1:32:54.080
 of just first order, and if you go to, I'm taking the other extreme, if you read Shannon's book,

1:32:54.080 --> 1:33:00.320
 right? He has a model of English text, which is based on first order Markov chains,

1:33:00.320 --> 1:33:03.760
 second order Markov chains, third order Markov chains and saying that, okay, third order Markov

1:33:03.760 --> 1:33:10.640
 chains look better than first order Markov chains. So does that mean a first order Markov chain has

1:33:10.640 --> 1:33:19.120
 a model of the world? Yes, it does. So yes, in that level, when you grow higher order models,

1:33:19.120 --> 1:33:25.200
 or more sophisticated structure in the model like the transformer networks have, yes, they have a

1:33:25.200 --> 1:33:33.440
 model of the text world. But that is not a model of the world. It's a model of the text world and

1:33:33.440 --> 1:33:42.240
 it will have interesting properties and it will be useful. But just scaling it up is not going to

1:33:42.240 --> 1:33:53.360
 give us AGI or natural language understanding or meaning. The question is whether being forced

1:33:53.360 --> 1:34:01.200
 to compress a very large amount of text forces you to construct things that are very much like,

1:34:02.960 --> 1:34:11.520
 because the ideas of concepts and meaning is a spectrum. Sure. So in order to form that kind

1:34:11.520 --> 1:34:23.120
 of compression, maybe it will be forced to figure out abstractions which look awfully a lot like

1:34:23.120 --> 1:34:31.120
 the kind of things that we think about as concepts, as world models, as common sense. Is that possible?

1:34:31.120 --> 1:34:34.000
 No, I don't think it is possible because the information is not there.

1:34:34.000 --> 1:34:38.640
 Well, the information is there behind the text, right?

1:34:38.640 --> 1:34:44.400
 No, unless somebody has written down all the details about how everything works in the world

1:34:44.400 --> 1:34:49.360
 to the absurd amounts like, okay, it is easier to walk forward than backward,

1:34:50.240 --> 1:34:54.160
 that you have to open the door to go out of the thing, doctors wear underwear,

1:34:55.200 --> 1:34:59.520
 unless all these things somebody has written down somewhere or somehow the program found

1:34:59.520 --> 1:35:04.480
 it to be useful for compression from some other text, the information is not there.

1:35:04.480 --> 1:35:13.040
 So that is an argument that text is a lot lower fidelity than the experience of our physical world.

1:35:13.040 --> 1:35:15.680
 Correct. It is worth a thousand words.

1:35:17.440 --> 1:35:23.600
 Well, in this case, pictures are not really, so the richest aspect of the physical world

1:35:23.600 --> 1:35:28.320
 is not even just pictures, it is the interactivity of the world.

1:35:28.320 --> 1:35:39.040
 Exactly. Yeah, it is being able to interact. It is almost like if you could interact.

1:35:40.080 --> 1:35:45.280
 I disagree. Well, maybe I agree with you that pictures were a thousand words, but a thousand...

1:35:45.840 --> 1:35:49.760
 You could say you could capture it with the GPTX.

1:35:49.760 --> 1:35:54.080
 So I wonder if there is some interactive element where a system could live in text world,

1:35:54.080 --> 1:35:59.600
 where it could be part of the chat, be part of talking to people.

1:36:00.640 --> 1:36:06.640
 It is interesting. Fundamentally, you are making a statement about the limitation of text.

1:36:07.520 --> 1:36:17.360
 Okay, so let us say we have a text corpus that includes basically every experience we could

1:36:17.360 --> 1:36:23.120
 possibly have. I mean, just a very large corpus of text and also interactive components.

1:36:23.120 --> 1:36:28.560
 I guess the question is whether the neural network architecture, these very simple transformers,

1:36:28.560 --> 1:36:36.320
 but if they had hundreds of trillions or whatever comes after a trillion parameters,

1:36:36.320 --> 1:36:45.120
 whether that could store the information needed. That is architecturally. Do you have thoughts

1:36:45.120 --> 1:36:48.880
 about the limitation on that side of things with neural networks?

1:36:48.880 --> 1:36:56.480
 I mean, so transformer is still a feed forward neural network. It has a very interesting

1:36:56.480 --> 1:37:01.520
 architecture, which is good for text modeling and probably some aspects of video modeling,

1:37:01.520 --> 1:37:06.240
 but it is still a feed forward architecture. Do you believe in the feedback mechanism, the

1:37:06.240 --> 1:37:14.160
 recursion? Oh, and also causality, being able to do counterfactual reasoning, being able to do

1:37:14.160 --> 1:37:22.240
 interventions, which is actions in the world. So all those things require different kinds of

1:37:22.240 --> 1:37:30.720
 models to be built. I don't think a transformer captures that family. It is very good at statistical

1:37:30.720 --> 1:37:37.520
 modeling of text. And it will become better and better with more data, bigger models.

1:37:37.520 --> 1:37:44.880
 But that is only going to get so far. Finally, when you... So I had this joke on Twitter saying

1:37:44.880 --> 1:37:52.080
 that, hey, this is a model that has read all of quantum mechanics and theory of relativity,

1:37:52.080 --> 1:37:56.960
 and we are asking you to do text completion, or we are asking you to solve simple puzzles.

1:37:58.960 --> 1:38:03.200
 When you have AGI, that's not what you ask a system to do. If it has...

1:38:03.200 --> 1:38:10.880
 We'll ask the system to do experiments and come up with hypothesis and revise the hypothesis

1:38:10.880 --> 1:38:14.720
 based on evidence from experiments, all those things. Those are the things that we want the

1:38:14.720 --> 1:38:21.440
 system to do when we have AGI, not solve simple puzzles. Like impressive demo,

1:38:21.440 --> 1:38:29.440
 somebody generating a red button in HTML, which are all useful. There's no dissing the usefulness.

1:38:29.440 --> 1:38:35.680
 So I get... By the way, I'm playing a little bit of a devil's advocate. So calm down,

1:38:35.680 --> 1:38:47.120
 internet. So I'm curious almost in which ways will a dumb but large neural network will surprise us.

1:38:48.480 --> 1:38:54.960
 So it's kind of your... I completely agree with your intuition. It's just that I don't want to

1:38:54.960 --> 1:39:04.320
 dogmatically, 100% put all the chips there. We've been surprised so much. Even the current

1:39:04.320 --> 1:39:14.560
 GPT2 and 3 are so surprising. The self play mechanisms of AlphaZero are really surprising.

1:39:18.160 --> 1:39:22.320
 The fact that reinforcement learning works at all to me is really surprising. The fact that

1:39:22.320 --> 1:39:28.080
 neural networks work at all is quite surprising. Given how nonlinear the space is, the fact that

1:39:28.080 --> 1:39:36.080
 it's able to find local minima that are all reasonable, it's very surprising. So I wonder

1:39:36.080 --> 1:39:46.480
 sometimes whether us humans just want it to not... For AGI not to be such a dumb thing.

1:39:46.480 --> 1:39:54.880
 Because exactly what you're saying is like the ideas of concepts and be able to reason with

1:39:54.880 --> 1:40:01.360
 those concepts and connect those concepts in like hierarchical ways and then to be able to have

1:40:02.480 --> 1:40:08.720
 world models. Just everything we're describing in human language in this poetic way seems to

1:40:08.720 --> 1:40:13.120
 make sense that that is what intelligence and reasoning are like. I wonder if at the core of

1:40:13.120 --> 1:40:19.760
 it it could be much dumber. Well, finally it is still connections and messages passing over them.

1:40:23.600 --> 1:40:29.760
 So I guess the recursion, the feedback mechanism, that does seem to be a fundamental kind of thing.

1:40:31.280 --> 1:40:38.560
 Yeah, yeah. The idea of concepts, also memory. Correct. Having an episodic memory.

1:40:38.560 --> 1:40:44.320
 Yeah. That seems to be an important thing. So how do we get memory? So yeah, we have another

1:40:44.320 --> 1:40:50.640
 piece of work that which came out recently on how do you form episodic memories and form

1:40:50.640 --> 1:40:56.240
 abstractions from them. And we haven't figured out all the connections of that to the overall

1:40:56.240 --> 1:41:03.040
 cognitive architecture. But yeah, what are your ideas about how you could have episodic memory?

1:41:03.040 --> 1:41:08.720
 So at least it's very clear that you need to have two kinds of memory. That's very, very clear.

1:41:08.720 --> 1:41:17.040
 Which is there are things that happen as statistical patterns in the world. But then there is the

1:41:17.840 --> 1:41:23.840
 one timeline of things that happen only once in your life. And this day is not going to happen

1:41:23.840 --> 1:41:32.560
 ever again. And so, and that needs to be stored as a, you know, just a stream of string. This is my

1:41:32.560 --> 1:41:38.960
 experience. And then the question is about how do you take that experience and connect it to the

1:41:38.960 --> 1:41:44.640
 statistical part of it? How do you now say that, okay, I experienced this thing. Now I want to

1:41:45.200 --> 1:41:52.560
 be careful about similar situations. And so you need to be able to index that similarity

1:41:52.560 --> 1:41:58.880
 using your other giants, that is, you know, the model of the world that you have learned. Although

1:41:58.880 --> 1:42:02.960
 the situation came from the episode, you need to be able to index the other one. So

1:42:04.960 --> 1:42:13.440
 the episodic memory being implemented as an indexing over the other model that you're building.

1:42:13.440 --> 1:42:22.800
 So the memories remain and they, they, they're an index into this, like the statistical thing that

1:42:22.800 --> 1:42:27.840
 you formed. Yeah, statistical or causal structural model that you built over time. So,

1:42:28.560 --> 1:42:35.680
 so it's basically the idea is that the hippocampus is just storing or sequencing

1:42:35.680 --> 1:42:43.920
 a, a, in a set of pointers that happens over time. And then whenever you want to reconstitute that

1:42:43.920 --> 1:42:50.560
 memory and evaluate the different aspects of it, whether it was good, bad, do I need to encounter

1:42:50.560 --> 1:42:56.720
 the situation again, you need the cortex to reinstant it to replay that memory.

1:42:57.840 --> 1:43:01.760
 So how do you find that memory? Like, which direction is the important direction?

1:43:01.760 --> 1:43:07.440
 Both directions are, you know, it's again bi directional. I mean, I guess how do you retrieve

1:43:07.440 --> 1:43:12.480
 the memory? So this is again hypothesis, right? We're making this up. So when you, when you come

1:43:12.480 --> 1:43:19.920
 to a new situation, right, your, your cortex is doing inference over in the new situation. And

1:43:19.920 --> 1:43:25.840
 then, of course, hippocampus is connected to different parts of the cortex. And, and you have

1:43:25.840 --> 1:43:35.280
 this deja vu situation, right? Okay, I have seen this thing before. And, and then in the hippocampus,

1:43:35.280 --> 1:43:42.000
 you can have an index of, okay, this is when it happened as a timeline. And, and, and then,

1:43:42.640 --> 1:43:48.960
 then you can use the hippocampus to drive the, the similar timelines to say, now I am, I am,

1:43:48.960 --> 1:43:55.840
 rather than being driven by my current input stimuli, I am going back in time and rewinding

1:43:55.840 --> 1:44:00.240
 my experience from replaying it, but putting back into the cortex and then putting it back

1:44:00.240 --> 1:44:04.960
 into the cortex, of course affects what you're going to see next in your current situation.

1:44:04.960 --> 1:44:09.520
 Got it. Yeah. So that's, that's the whole thing, having a world model and then yeah,

1:44:10.160 --> 1:44:14.560
 connecting to the perception. Yeah, it does seem to be that that's what's happening to be

1:44:14.560 --> 1:44:20.800
 on the neural network side. It's, it's interesting to think of how we actually do that.

1:44:21.600 --> 1:44:28.160
 Yeah. Yeah. So have a knowledge base. Yes. It is possible that you can put many of these structures

1:44:28.160 --> 1:44:36.880
 into neural networks and we will find ways of combining properties of neural networks and

1:44:36.880 --> 1:44:42.720
 graphical models. So, I mean, it's already started happening. Yes. Graph neural networks are kind

1:44:42.720 --> 1:44:49.040
 of a merge between them. And there will be more of that thing. So, but to me, it is the direction

1:44:49.040 --> 1:44:57.600
 is pretty clearly in looking at biology and the history of evolutionary history of intelligence.

1:44:57.600 --> 1:45:05.520
 It is pretty clear that, okay, what needs is more structure in the models and modeling of the world

1:45:05.520 --> 1:45:12.960
 and supporting dynamic inference. Well, let me ask you. There's a guy named Elon Musk. There's a

1:45:12.960 --> 1:45:18.080
 company called Neuralink and there's a general field called brain computer interfaces. Yeah.

1:45:18.080 --> 1:45:26.080
 It's kind of a interface between your two loves. Yes. The brain and the intelligence. So there's

1:45:26.080 --> 1:45:32.080
 like very direct applications of brain computer interfaces for people with different conditions,

1:45:32.080 --> 1:45:38.800
 more in the short term. Yeah. But there's also these sci fi futuristic kinds of ideas of AI systems

1:45:38.800 --> 1:45:46.720
 being able to communicate in a high bandwidth way with the brain by directional. Yeah. What are

1:45:46.720 --> 1:45:55.920
 your thoughts about Neuralink and BCI in general as a possibility? So I think BCI is a cool research

1:45:55.920 --> 1:46:03.120
 area. And in fact, when I got interested in brains initially, when I was enrolled at Stanford,

1:46:03.120 --> 1:46:08.880
 and when I got interested in brains, it was through a brain computer interface talk

1:46:08.880 --> 1:46:14.800
 that Krishna Shannoi gave. That's when I even started thinking about the problem. So it is

1:46:14.800 --> 1:46:21.120
 definitely a fascinating research area. And it is the applications are enormous. So there's

1:46:21.120 --> 1:46:26.240
 the science fiction scenario of brains directly communicating. Let's keep that aside for the

1:46:26.240 --> 1:46:32.480
 time being. Even just the intermediate milestones that pursuing, which are very reasonable as far

1:46:32.480 --> 1:46:40.560
 as I can see, being able to control an external limb using direct connections from the brain

1:46:40.560 --> 1:46:48.560
 and being able to write things into the brain. So those are all good steps to take. And they have

1:46:48.560 --> 1:46:55.120
 enormous applications, people losing limbs, being able to control prosthetics, quadriplegics,

1:46:55.120 --> 1:47:01.440
 being able to control something. So I'm therapeutics. And I also know about another company working in

1:47:01.440 --> 1:47:09.120
 the space called Paradromix. They're based on a different electrode array, but trying to attack

1:47:09.120 --> 1:47:14.800
 some of the same problems. So I think it's a very... Also surgery? Correct, surgically implanted

1:47:14.800 --> 1:47:22.560
 electrons. Yeah. So yeah, I think of it as a very, very promising field, especially when it is

1:47:22.560 --> 1:47:28.640
 helping people overcome some limitations. Now, at some point, of course, it will advance the level

1:47:28.640 --> 1:47:35.760
 of being able to communicate. How hard is that problem, do you think? So okay, let's say we

1:47:35.760 --> 1:47:41.680
 magically solve what I think is a really hard problem of doing all of this safely.

1:47:41.680 --> 1:47:50.000
 Yeah. So being able to connect electrodes and not just thousands, but like millions to the brain.

1:47:50.000 --> 1:47:56.800
 Yeah. I think it's very, very hard because you also do not know what will happen to the brain

1:47:56.800 --> 1:47:59.920
 with that, right? In the sense of how does the brain adapt to something like that?

1:47:59.920 --> 1:48:07.200
 And as we were learning, the brain is quite... In terms of neuroplasticity, it's pretty malleable.

1:48:07.200 --> 1:48:12.400
 Correct. So it's going to adjust. Correct. So the machine learning side, the computer side is going

1:48:12.400 --> 1:48:16.880
 to adjust and then the brain is going to adjust. Exactly. And then what soup does this land us

1:48:16.880 --> 1:48:22.960
 into is... The kind of hallucinations you might get from this. That might be pretty intense.

1:48:22.960 --> 1:48:29.600
 Yeah. Yeah. So just connecting to all of Wikipedia. It's interesting whether we need to be able to

1:48:29.600 --> 1:48:36.400
 figure out the basic protocol of the brain's communication schemes in order to get them to

1:48:36.400 --> 1:48:41.760
 to the machine and the brain to talk. Because another possibility is the brain actually just

1:48:41.760 --> 1:48:46.240
 adjust to whatever the heck the computer is doing. Exactly. That's the way I think that I find that

1:48:46.240 --> 1:48:52.080
 to be a more promising way. It's basically saying, you know, okay, attach electrodes to some part of

1:48:52.080 --> 1:48:58.480
 the cortex. Okay. And maybe if it is done from birth, the brain will adapt. It says that, you

1:48:58.480 --> 1:49:02.640
 know, that part is not damaged. It was not used for anything. These electrodes are attached there,

1:49:02.640 --> 1:49:09.120
 right? And now you train that part of the brain to do this high bandwidth communication between

1:49:09.120 --> 1:49:15.600
 something else, right? And if you do it like that, then it is brain adapting to and of course,

1:49:15.600 --> 1:49:20.000
 your external system is designed such that it is adaptable. You know, just like we, you know,

1:49:20.000 --> 1:49:27.920
 design computers or mouse keyboard, all of them to be interacting with humans. So of course,

1:49:27.920 --> 1:49:36.080
 that feedback system is designed to be human compatible. But now it is not trying to record

1:49:36.080 --> 1:49:41.680
 from the all of the brain and, you know, now, you know, two systems trying to adapt to each other.

1:49:41.680 --> 1:49:47.440
 It's a brain adapting into one way. That's fascinating. The brain is connected to like the

1:49:47.440 --> 1:49:53.360
 internet. It's connected. Just imagine it's connecting it to Twitter and just just taking

1:49:53.360 --> 1:50:01.200
 that stream of information. Yeah. But again, if we take a step back, I don't know what your

1:50:01.200 --> 1:50:11.280
 intuition is. I feel like that is not as hard of a problem as the doing it safely. There's,

1:50:11.280 --> 1:50:17.520
 there's a huge barrier to surgery. Right. That because, because the biological system,

1:50:17.520 --> 1:50:24.560
 it's a mush of like weird stuff. Correct. So that the surgery part of it, biology part of it,

1:50:24.560 --> 1:50:30.880
 the long term repercussions part of it. Again, I don't know what else will, you know, we,

1:50:30.880 --> 1:50:38.080
 we often find after a long time in biology that, okay, that idea was wrong, right? You know, so

1:50:38.080 --> 1:50:45.680
 people used to cut off this, the gland called the thymus or something. And then they found that,

1:50:45.680 --> 1:50:53.440
 oh, no, that actually causes cancer. And then there's a subtle like millions of variables

1:50:53.440 --> 1:51:00.320
 involved. But this whole process, the nice thing, and just like, again, with Elon, just like colonizing

1:51:00.320 --> 1:51:07.040
 Mars seems like a ridiculously difficult idea. But in the process of doing it, we might learn a lot

1:51:07.040 --> 1:51:12.160
 about the biology of the neurobiology of the brain, the neuroscience side of things. It's like,

1:51:12.160 --> 1:51:17.840
 if you want to learn something, do the most difficult version of it and see what you learn.

1:51:18.400 --> 1:51:22.240
 The intermediate steps that they are taking sounded all very reasonable to me.

1:51:22.240 --> 1:51:28.480
 Yeah. It's great. Well, but like everything with Elon is the timeline seems insanely fast. So

1:51:29.680 --> 1:51:36.080
 that's, that's the only awful question. Well, what we've been talking about cognition a little bit.

1:51:36.080 --> 1:51:42.800
 So like reasoning, we haven't mentioned the other C word, which is consciousness. Do you ever think

1:51:42.800 --> 1:51:50.320
 about that one? Is that useful at all in this whole context of what it takes to create an

1:51:50.320 --> 1:51:57.200
 intelligent reasoning being? Or is that completely outside of your, like the engineering perspective

1:51:57.200 --> 1:52:03.600
 of intelligence? So it is not outside the realm, but it doesn't, on a day to day way, you know,

1:52:03.600 --> 1:52:10.640
 basis inform what we do. But it's more, so in many ways, the company name is connected to

1:52:10.640 --> 1:52:13.680
 this idea of consciousness. What's the company name?

1:52:13.680 --> 1:52:23.120
 Vicarious. So Vicarious is the company name. And so what does Vicarious mean? At the first level,

1:52:23.120 --> 1:52:30.640
 it is about modeling the world. And it is internalizing the external actions. So you interact

1:52:30.640 --> 1:52:36.080
 with the world and learn a lot about the world. And now after having learned a lot about the world,

1:52:36.080 --> 1:52:43.120
 you can run those things in your mind without actually having to act in the world. So you can

1:52:43.120 --> 1:52:49.520
 run things vicariously, just in your, in your, in your brain. And similarly, you can experience

1:52:49.520 --> 1:52:56.080
 another person's thoughts by, you know, having a model of how that person works and, and running

1:52:56.080 --> 1:53:01.360
 their, you know, putting yourself in some other person's shoes. So that is being vicarious.

1:53:01.360 --> 1:53:06.800
 Now, it's the same modeling apparatus that you're using to model the external world

1:53:06.800 --> 1:53:13.200
 or some other person's thoughts. You can turn it to yourself. You can up, you know, if that same

1:53:13.200 --> 1:53:19.760
 modeling thing is applied to your own modeling apparatus, then that is what gives rise to

1:53:19.760 --> 1:53:24.720
 consciousness, I think. Well, that's more like self awareness. There's the hard problem of

1:53:24.720 --> 1:53:32.640
 consciousness, which is like, when the model becomes, when the model feels like something,

1:53:32.640 --> 1:53:40.560
 when the whole process is like, it's like, you really are in it. You feel like an entity in

1:53:40.560 --> 1:53:46.080
 this world, not just, you know that you're an entity, but it feels like something to be that

1:53:46.080 --> 1:53:55.760
 entity. It, it, you know, and thereby we attribute this, you know, then it starts to be where in

1:53:55.760 --> 1:54:00.480
 something that has consciousness can suffer, you start to have these kinds of things that we can

1:54:00.480 --> 1:54:09.200
 reason about that is much, much heavier. It seems like there's much greater cost to your,

1:54:09.200 --> 1:54:16.080
 your decisions. And like mortality is tied up into that, like the fact that these things end.

1:54:16.800 --> 1:54:24.480
 Right. First of all, I end at some point, and then other things end. And, you know, that, that

1:54:24.480 --> 1:54:32.480
 somehow seems to be, at least for us humans, a deep motivator. Yes. And that, you know, that,

1:54:32.480 --> 1:54:39.840
 that idea of motivation in general, we talk about goals in AI, but the goals aren't quite the same

1:54:39.840 --> 1:54:46.240
 thing as like the, our mortality. It feels like, it feels like, first of all, humans don't have a

1:54:46.240 --> 1:54:51.920
 goal. And they just kind of create goals at different levels. They like make up goals.

1:54:52.880 --> 1:55:01.600
 Because we're terrified by the mystery of the thing that gets us all. So we make these goals

1:55:01.600 --> 1:55:08.640
 up. So we're like a goal generation machine, as opposed to a machine which optimizes the trajectory

1:55:08.640 --> 1:55:16.720
 towards a singular goal. So it feels like that's an important part of cognition, that whole mortality

1:55:16.720 --> 1:55:27.360
 thing. Well, it is, it is a part of human cognition. But there is no reason for that mortality to come

1:55:27.360 --> 1:55:36.160
 to the equation for a artificial system, because we can copy the artificial system. The problem with

1:55:36.160 --> 1:55:41.760
 humans is that we can't, I can't clone you. I can't, you know, I can, I can, even if I clone

1:55:41.760 --> 1:55:48.800
 you as a, you know, the hardware, your experience that was stored in your brain, your episodic

1:55:48.800 --> 1:55:54.960
 memory, all those will not be captured in the, in the new clone. So, but that's not the same with

1:55:54.960 --> 1:56:02.880
 an AI system, right? So, but it's also possible that the, the thing that you mentioned with us

1:56:02.880 --> 1:56:07.760
 humans is actually fundamental, fundamental importance for intelligence. So like the fact

1:56:07.760 --> 1:56:15.440
 that you can copy an AI system means that that AI system is not yet an AGI. So like,

1:56:16.080 --> 1:56:22.720
 so if you look at existence proof, if we reason, based on existence proof, you could say that it

1:56:22.720 --> 1:56:29.680
 doesn't feel like death is a fundamental property of an intelligence system, but we don't yet,

1:56:30.320 --> 1:56:36.960
 give me an example of an immortal intelligent being. We don't have those. It could, it's very

1:56:36.960 --> 1:56:44.480
 possible that, you know, that's, that is a fundamental property of intelligence is a thing that

1:56:45.520 --> 1:56:51.840
 has a deadline for itself. So you can think of it like this. So suppose you invent a way to

1:56:51.840 --> 1:56:59.600
 freeze people for a long time. It's not dying, right? So, so you can be frozen and woken up

1:56:59.600 --> 1:57:02.960
 thousands of years from now. So it's no fear of death.

1:57:04.560 --> 1:57:11.200
 Well, no, you're still, it's not, it's not about time. It's about the knowledge that it's temporary.

1:57:12.400 --> 1:57:21.520
 And the, that aspect of it, the finiteness of it, I think creates a kind of urgency.

1:57:21.520 --> 1:57:27.040
 Correct. For us, for humans. Yeah, for humans. Yes. And that, that is part of our drives.

1:57:27.920 --> 1:57:37.920
 But, and that's why I'm not too worried about AI, you know, having motivations to kill all humans

1:57:37.920 --> 1:57:45.440
 and those kinds of things. Why just wait, you know? So, why do you need to do that?

1:57:45.440 --> 1:57:52.800
 I've never heard that before. That's a good point. Yeah, just murder seems like a lot of work.

1:57:52.800 --> 1:58:00.480
 Just wait, wait it out. They'll probably hurt themselves. Let me ask you, people often kind

1:58:00.480 --> 1:58:09.040
 of wonder, world class researchers such as yourself, what kind of books, technical fiction,

1:58:09.040 --> 1:58:17.520
 philosophical were, had an impact on you in your life? And maybe ones you could possibly

1:58:17.520 --> 1:58:22.480
 recommend that others read? Maybe if you have three books that pop into mind?

1:58:23.120 --> 1:58:29.760
 Yeah. So I definitely liked Judea Pearl's book, probabilistic reasoning and intelligent systems.

1:58:29.760 --> 1:58:35.280
 It's, it's a very deep technical book. But what I liked is that in, so there are many

1:58:35.280 --> 1:58:40.960
 places where you can learn about probabilistic graphical models from. But throughout this book,

1:58:40.960 --> 1:58:47.200
 Judea Pearl kind of sprinkles his philosophical observations and, and he thinks about connections

1:58:47.200 --> 1:58:52.080
 to how the brain thinks and attentions and resources, all those things. So, so that whole

1:58:52.080 --> 1:58:57.280
 thing makes it more interesting to read. He emphasizes the importance of causality.

1:58:57.840 --> 1:59:01.680
 So that was in his later book. So this was the first book probabilistic reasoning in

1:59:01.680 --> 1:59:06.640
 intelligent systems. He mentions causality, but he hadn't really sunk his teeth into,

1:59:07.280 --> 1:59:12.800
 like, you know, how do you actually formalize that? Yeah. And the second book causality was

1:59:12.800 --> 1:59:17.040
 2000, the one in 2000, that one is really hard. So I wouldn't recommend that.

1:59:17.680 --> 1:59:21.840
 Oh, yeah. So that looks at the, like the mathematical, like his model of

1:59:23.120 --> 1:59:27.760
 do calculus. Yeah, it was pretty dense mathematics. Right. Right. The book of why is

1:59:27.760 --> 1:59:32.320
 definitely more enjoyable. Oh, for sure. Yeah. So, yeah. So I would, I would recommend probabilistic

1:59:32.320 --> 1:59:38.400
 reasoning in intelligent systems. Another book I liked was one from Doug Hofstadter.

1:59:39.120 --> 1:59:43.440
 This is a long time ago. He has a book, he had a book, I think call it was called the mind's eye.

1:59:43.440 --> 1:59:51.120
 It was probably Hofstadter and Daniel Dennett together. Yeah. So, and I actually was,

1:59:51.120 --> 1:59:58.240
 I bought that book so much I haven't read it yet, but I couldn't get an electronic version of it,

1:59:58.240 --> 2:00:04.400
 which is annoying because I read everything on Kindle. Oh, okay. I had to actually purchase

2:00:04.400 --> 2:00:08.960
 the physical. It's like one of the only physical books I have. Yeah. Anyway, there's a lot of

2:00:08.960 --> 2:00:14.480
 people recommended it highly. So, yeah. And the third one I would definitely recommend reading is

2:00:14.480 --> 2:00:22.240
 this is not a technical book. It is history. It's called, it's the name of the book, I think,

2:00:22.240 --> 2:00:32.720
 is Bishop's Boys. It's about Wright Brothers and their path and how it was, there are multiple

2:00:32.720 --> 2:00:42.960
 books on this topic and all of them are great. It's fascinating how flight was treated as an

2:00:42.960 --> 2:00:50.640
 unsolvable problem. And also, what aspects did people emphasize? People thought, oh,

2:00:50.640 --> 2:00:56.640
 it is all about just powerful engines. We just need to have powerful lightweight engines.

2:00:57.920 --> 2:01:04.960
 And so, some people thought of it as, how far can we just throw the thing? Just throw it.

2:01:04.960 --> 2:01:12.560
 Like a catapult. Yeah. So, it's very fascinating. And even after they made the invention,

2:01:13.120 --> 2:01:16.800
 people are not believing it. And the social aspect of it, yeah.

2:01:16.800 --> 2:01:24.240
 The social aspect, because it's very fascinating. Do you draw any parallels between birds fly?

2:01:25.200 --> 2:01:30.800
 So, there's the natural approach to flight and then there's the engineered approach. Do you

2:01:30.800 --> 2:01:35.840
 see the same kind of thing with the brain and are trying to engineer intelligence?

2:01:37.280 --> 2:01:42.960
 Yeah, it's a good analogy to have. Of course, all analogies have their, you know,

2:01:43.600 --> 2:01:53.520
 limits. So, people in AI often use airplanes as an example of, hey, we didn't learn anything

2:01:53.520 --> 2:02:01.040
 from birds. Look, but the funny thing is that, and the saying is, airplanes don't flap wings.

2:02:01.600 --> 2:02:08.000
 This is what they say. The funny thing and the ironic thing is that, that you don't need to flap

2:02:08.000 --> 2:02:16.640
 to fly is something right with those found by observing birds. So, they have in their notebook,

2:02:17.440 --> 2:02:22.480
 in some of these books, they show their notebook drawings. They make detailed

2:02:22.480 --> 2:02:29.600
 notes about buzzards just soaring over thermals. And they basically say, look, flapping is not

2:02:29.600 --> 2:02:34.400
 the important, propulsion is not the important problem to solve here. We want to solve control.

2:02:35.360 --> 2:02:40.400
 And once you solve control, propulsion will fall into place. All of these are people,

2:02:40.400 --> 2:02:42.720
 you know, they relate this by observing birds.

2:02:44.480 --> 2:02:49.280
 Beautifully put. That's actually brilliant. Because people do use that analogy a lot. I'm

2:02:49.280 --> 2:02:54.480
 going to have to remember that one. Do you have advice for people interested in artificial

2:02:54.480 --> 2:02:58.160
 intelligence like young folks today? I talked to undergraduate students all the time,

2:02:59.200 --> 2:03:03.840
 interested in neuroscience, interested in understanding how the brain works. Is there

2:03:03.840 --> 2:03:08.720
 advice you would give them about their career, maybe about their life in general?

2:03:09.520 --> 2:03:13.760
 Sure. I think every, you know, every piece of advice should be taken with a pinch of salt,

2:03:14.720 --> 2:03:19.200
 because, you know, each person is different. Their motivations are different. But I can

2:03:19.200 --> 2:03:26.880
 I can definitely say if your goal is to understand the brain from the angle of wanting to build one,

2:03:26.880 --> 2:03:34.320
 you know, then being an experimental neuroscientist might not be the way to go about it.

2:03:36.800 --> 2:03:42.640
 A better way to pursue it might be through computer science, electrical engineering,

2:03:42.640 --> 2:03:46.240
 machine learning, and AI. And of course, you have to study up the neuroscience,

2:03:46.240 --> 2:03:53.920
 but that you can do on your own. If you're more attracted by finding something intriguing about

2:03:53.920 --> 2:03:58.960
 discovering something intriguing about the brain, then of course, it is better to be an

2:03:58.960 --> 2:04:03.120
 experimentalist. So find that motivation. What are you intrigued by? And of course,

2:04:03.120 --> 2:04:09.440
 find your strengths too. Some people are very good experimentalists, and they enjoy doing that.

2:04:09.440 --> 2:04:16.240
 And it's interesting to see which department, if you're, if you're picking in terms of like

2:04:16.240 --> 2:04:23.760
 your education path, whether to go with like an MIT, it's brain and computer, no,

2:04:25.520 --> 2:04:33.360
 BCS. Brain and cognitive sciences, yeah. Or the CS side of things. And actually,

2:04:33.360 --> 2:04:40.000
 the brain folks, the neuroscience folks are more and more now embracing of the, you know,

2:04:40.000 --> 2:04:48.000
 learning TensorFlow and PyTorch, right? They see the power of trying to engineer ideas

2:04:49.200 --> 2:04:54.560
 that they get from the brain into and then explore how those could be used to

2:04:55.360 --> 2:04:58.640
 create intelligent systems. So that might be the right department, actually.

2:04:58.640 --> 2:05:04.400
 Yeah. So this was a question in, you know, one of the Red Bull Neuroscience Institute

2:05:04.400 --> 2:05:11.120
 workshops that Jeff Hawkins organized almost 10 years ago. This question was put to a panel,

2:05:11.120 --> 2:05:16.160
 right? What should be the undergrad major you should take if you want to understand the brain?

2:05:16.160 --> 2:05:20.800
 And the majority opinion that one was electrical engineering.

2:05:22.080 --> 2:05:26.960
 Interesting. Because, I mean, I'm a doubly undergrad, so I got lucky in that way.

2:05:26.960 --> 2:05:33.280
 But I think it does have some of the right ingredients because you learn about circuits,

2:05:33.280 --> 2:05:39.200
 you learn about how you can construct circuits to, you know, approach, you know, do functions.

2:05:40.080 --> 2:05:44.720
 You learn about microprocessors, you learn information theory, you learn signal processing,

2:05:45.280 --> 2:05:52.160
 you learn continuous math. So in that way, it's a good step to, if you want to go to

2:05:52.160 --> 2:05:58.000
 computer science or neuroscience, you can, it's a good step. The downside, you're more likely to

2:05:58.000 --> 2:06:09.040
 be forced to use MATLAB. So one of the interesting things about, I mean, this is changing, the world

2:06:09.040 --> 2:06:15.920
 is changing, but like certain departments lagged on the programming side of things, on developing

2:06:15.920 --> 2:06:21.520
 good, good happens, there's a software engineering, but I think that's more and more changing. And, and

2:06:21.520 --> 2:06:27.520
 students can take the answer of their own hands, like learn to program. I feel like everybody

2:06:27.520 --> 2:06:35.360
 should learn to program, because it, it like everyone in the sciences, because it empowers,

2:06:35.360 --> 2:06:40.800
 it puts the data at your fingertips. So you can organize it, you can find all kinds of things in

2:06:40.800 --> 2:06:47.840
 the data. And then you can also, for the appropriate sciences, build systems that, like based on that.

2:06:47.840 --> 2:06:53.760
 So like then engineer intelligence systems. We already talked about mortality. So we hit

2:06:55.760 --> 2:07:01.680
 a ridiculous point, but let me ask you the, you know,

2:07:01.680 --> 2:07:12.960
 one of the things about intelligence is it's goal driven. And you study the brain. So the

2:07:12.960 --> 2:07:17.360
 question is like, what's the goal that the brain is operating under? What's the meaning of it all

2:07:17.360 --> 2:07:23.920
 for us humans in your view? What's the meaning of life? The meaning of life is whatever you

2:07:23.920 --> 2:07:31.760
 construct out of it. It's completely open. It's open. So there's nothing, like you mentioned,

2:07:31.760 --> 2:07:40.400
 you like constraints. So there's what's, it's wide open. Is there, is there some useful aspect

2:07:40.400 --> 2:07:47.440
 that you think about in terms of like the openness of it and just the basic mechanisms of generating

2:07:47.440 --> 2:07:55.920
 goals in studying cognition in the brain that you think about? Or is it just about, because

2:07:55.920 --> 2:07:59.760
 everything we've talked about kind of the perception system is to understand the environment.

2:07:59.760 --> 2:08:06.240
 That's like to be able to like not die. Exactly. Like not fall over and like be able to,

2:08:07.600 --> 2:08:12.320
 you don't think we need to think about anything bigger than that.

2:08:12.320 --> 2:08:18.960
 Yeah, I think so, because it's basically being able to understand the machinery of the world

2:08:20.080 --> 2:08:24.720
 such that you can pursue whatever goals you want, right? So the machinery of the world is

2:08:24.720 --> 2:08:28.960
 is really ultimately what we should be striving to understand. The rest is just,

2:08:28.960 --> 2:08:33.840
 the rest is just whatever the heck you want to do or whatever, whatever, whatever is culturally

2:08:33.840 --> 2:08:43.280
 popular. I think that's, that's beautifully put. I don't think there's a better way to

2:08:44.160 --> 2:08:50.240
 end it. I'm so honored that you would show up here and waste your time with me. It's been

2:08:50.240 --> 2:08:54.400
 awesome conversation. Thanks so much for talking today. Oh, thank you so much. This was, this was

2:08:54.400 --> 2:09:00.880
 so much more fun than I expected. Thank you. Thanks for listening to this conversation with

2:09:00.880 --> 2:09:07.920
 Delete George. And thank you to our sponsors Babbo, Raycon earbuds and masterclass. Please

2:09:07.920 --> 2:09:14.720
 consider supporting this podcast by going to babble.com and use code Lex going to buy raycon.com

2:09:14.720 --> 2:09:21.360
 slash Lex and signing up a masterclass.com slash Lex. Click the links, get the discount.

2:09:21.360 --> 2:09:26.720
 It really is the best way to support this podcast. If you enjoy this thing, subscribe on YouTube,

2:09:26.720 --> 2:09:32.240
 review the five stars and app a podcast supporting on Patreon. I'll connect with me on Twitter,

2:09:32.240 --> 2:09:41.520
 Alex Friedman spelled yes without the E just F R I D M A M. And now let me leave you with some

2:09:41.520 --> 2:09:50.400
 words from Marcus Aurelius. You have power over your mind, not outside events. Realize this

2:09:50.400 --> 2:09:58.240
 and you will find strength. Thank you for listening and hope to see you next time.

