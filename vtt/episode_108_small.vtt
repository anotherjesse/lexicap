WEBVTT

00:00.000 --> 00:05.360
 The following is a conversation with Sergei Levine, a professor at Berkeley and a world

00:05.360 --> 00:09.760
 class researcher in deep learning, reinforcement learning, robotics, and computer vision,

00:10.320 --> 00:15.040
 including the development of algorithms for end to end training of neural network policies

00:15.040 --> 00:20.320
 that combine perception and control, scalable algorithms for inverse reinforcement learning,

00:20.320 --> 00:23.280
 and, in general, deep RL algorithms.

00:23.840 --> 00:25.200
 Quick summary of the ads.

00:25.200 --> 00:30.320
 Two sponsors, Cash App and ExpressVPN. Please consider supporting the podcast by

00:30.320 --> 00:38.000
 downloading Cash App and using code LexPodcast and signing up at expressvpn.com slash LexPod.

00:38.640 --> 00:44.160
 Click the links, buy the stuff, it's the best way to support this podcast and, in general,

00:44.160 --> 00:45.040
 the journey I'm on.

00:46.080 --> 00:50.720
 If you enjoy this thing, subscribe on YouTube, review it with 5 stars on Apple Podcast,

00:50.720 --> 00:56.480
 follow on Spotify, support it on Patreon, or connect with me on Twitter at LexFreedman.

00:57.520 --> 01:01.520
 As usual, I'll do a few minutes of ads now and never any ads in the middle that can break

01:01.520 --> 01:06.960
 the flow of the conversation. This show is presented by Cash App, the number one finance

01:06.960 --> 01:13.600
 app in the App Store. When you get it, use code LexPodcast. Cash App lets you send money to friends

01:13.600 --> 01:19.600
 by Bitcoin and invest in the stock market with as little as $1. Since Cash App does fractional

01:19.600 --> 01:25.040
 share trading, let me mention that the order execution algorithm that works behind the scenes

01:25.040 --> 01:30.720
 to create the abstraction of the fractional orders is an algorithmic marvel. So big props to

01:30.720 --> 01:35.680
 the Cash App engineers for taking a step up to the next layer of abstraction over the stock market,

01:35.680 --> 01:40.720
 making trading more accessible for new investors and diversification much easier.

01:41.520 --> 01:47.440
 So again, if you get Cash App from the App Store, Google Play and use the code LexPodcast,

01:47.440 --> 01:54.240
 you get $10 and Cash App will also donate $10 the first, an organization that is helping to

01:54.240 --> 02:01.600
 advance robotics and STEM education for young people around the world. This show is also sponsored

02:01.600 --> 02:11.040
 by ExpressVPN. Get it at expressvpn.com slash LexPod to support this podcast and to get an extra

02:11.040 --> 02:17.920
 three months free on a one year package. I've been using ExpressVPN for many years. I love it.

02:18.480 --> 02:24.240
 I think ExpressVPN is the best VPN out there. They told me to say it, but it happens to be true

02:24.240 --> 02:30.320
 in my humble opinion. It doesn't log your data. It's crazy fast and it's easy to use literally

02:30.320 --> 02:36.400
 just one big power on button. Again, it's probably obvious to you, but I should say it again,

02:36.400 --> 02:42.240
 it's really important that they don't log your data. It works on Linux and every other operating

02:42.240 --> 02:48.800
 system, but Linux of course is the best operating system. Shout out to my favorite flavor, Ubuntu

02:48.800 --> 02:56.560
 Mate 2004. Once again, get it at expressvpn.com slash LexPod to support this podcast and to get

02:56.560 --> 03:04.240
 an extra three months free on a one year package. And now here's my conversation with Sergey Levine.

03:04.240 --> 03:10.000
 What's the difference between a state of the art human, such as you and I, well,

03:10.000 --> 03:14.320
 I don't know if we qualify as state of the art humans, but a state of the art human and a state

03:14.320 --> 03:21.440
 of the art robot? That's a very interesting question. Robot capability is, it's kind of a,

03:22.320 --> 03:28.080
 I think it's a very tricky thing to understand because there are some things that are difficult

03:28.080 --> 03:31.360
 that we wouldn't think are difficult and some things that are easy that we wouldn't think are easy.

03:31.360 --> 03:36.640
 And there's also a really big gap between capabilities of robots in terms of

03:37.280 --> 03:40.880
 hardware and their physical capability and capabilities of robots in terms of what they

03:40.880 --> 03:46.720
 can do autonomously. There is a little video that I think robotics researchers really like

03:46.720 --> 03:51.600
 to show, especially robotics learning researchers like myself from 2004 from Stanford,

03:52.320 --> 03:57.840
 which demonstrates a prototype robot called the PR1. And the PR1 was a robot that was designed as a

03:57.840 --> 04:02.880
 home assistance robot. And there's this beautiful video showing the PR1 tidying up a living room,

04:03.440 --> 04:08.800
 putting away toys, and at the end, bringing a beer to the person sitting on the couch,

04:09.440 --> 04:14.960
 which looks really amazing. And then the punchline is that this robot is entirely controlled by a

04:14.960 --> 04:19.760
 person. So you can, in some ways, the gap between a state of the art human and a state of the art

04:19.760 --> 04:24.240
 robot, if the robot has a human brain, is actually not that large. Now, obviously,

04:24.240 --> 04:28.800
 like human bodies are sophisticated and very robust and resilient in many ways. But on the whole,

04:29.360 --> 04:32.400
 if we're willing to like spend a bit of money and do a bit of engineering,

04:32.400 --> 04:40.160
 we can kind of close the hardware gap almost. But the intelligence gap, that one is very wide.

04:40.160 --> 04:44.160
 And when you say hardware, you're referring to the physical sort of the actuators, the actual

04:44.160 --> 04:49.040
 body of the robot as opposed to the hardware on which the cognition, the nervous, the hardware of

04:49.040 --> 04:52.960
 the nervous system. Yes, exactly. I'm referring to the body rather than the mind.

04:52.960 --> 04:58.240
 So that means that the kind of the work is cut out for us. While we can still make the

04:58.240 --> 05:01.440
 body better, we kind of know that the big bottleneck right now is really the mind.

05:02.560 --> 05:09.600
 And how big is that gap? How big is the difference in your sense of ability to learn,

05:09.600 --> 05:15.920
 ability to reason, ability to perceive the world between humans and our best robots?

05:15.920 --> 05:23.600
 The gap is very large, and the gap becomes larger the more unexpected events can happen

05:23.600 --> 05:30.240
 in the world. So essentially, the spectrum along which you can measure the size of that gap is

05:30.240 --> 05:33.680
 the spectrum of how open the world is. If you control everything in the world very tightly,

05:33.680 --> 05:38.240
 if you put the robot in like a factory and you tell it where everything is and you rigidly

05:38.240 --> 05:43.440
 program its motion, then it can do things, one might even say, in a superhuman way,

05:43.440 --> 05:48.080
 it can move faster, it's stronger, it can lift up a car and things like that. But as soon as

05:48.080 --> 05:52.480
 anything starts to vary in the environment, now it'll trip up. And if many, many things vary,

05:52.480 --> 05:57.440
 like they would like in your kitchen, for example, then things are pretty much like wide open.

05:58.720 --> 06:02.000
 Now, again, we're going to stick a bit on the philosophical questions, but

06:03.120 --> 06:10.400
 how much on the human side of the cognitive abilities in your sense is nature versus nurture?

06:10.400 --> 06:18.560
 So how much of it is a product of evolution and how much of it is something we'll learn from

06:18.560 --> 06:24.240
 sort of scratch from the day we're born? I'm going to read into your question as asking about

06:24.240 --> 06:29.360
 the implications of this for AI, because I'm not a biologist, I can't really like speak

06:29.360 --> 06:37.280
 authoritatively. So until we learn it, if it's all about learning, then there's more hope for AI.

06:37.280 --> 06:40.000
 Yeah. So the way that I look at this is that,

06:42.400 --> 06:47.680
 you know, well, first, of course, biology is very messy. And it's, if you ask the question,

06:47.680 --> 06:50.480
 how does a person do something, or how does a person's mind do something,

06:51.040 --> 06:54.800
 you can come up with a bunch of hypotheses, and oftentimes you can find support for many

06:54.800 --> 07:00.800
 different often conflicting hypotheses. One way that we can approach the question of

07:00.800 --> 07:04.640
 what the implications of this for AI are, is we can think about what's sufficient.

07:04.640 --> 07:10.320
 So, you know, maybe a person is, from birth, very, very good at some things like, for example,

07:10.320 --> 07:13.520
 recognizing faces. There's a very strong evolutionary pressure to do that. If you can

07:13.520 --> 07:18.640
 recognize your mother's face, then you're more likely to survive, and therefore people are good

07:18.640 --> 07:23.920
 at this. But we can also ask like, what's the minimum sufficient thing? And one of the ways

07:23.920 --> 07:27.840
 that we can study the minimal sufficient thing is we could, for example, see what people do in

07:27.840 --> 07:31.840
 unusual situations. If you present them with things that evolution couldn't have prepared them for,

07:31.840 --> 07:36.720
 you know, our daily lives actually do this to us all the time. We didn't evolve to deal with,

07:36.720 --> 07:41.280
 you know, automobiles and space flight and whatever. So, there are all these situations

07:41.280 --> 07:46.320
 that we can find ourselves in. And we do very well there. Like, I can give you a joystick

07:46.320 --> 07:50.880
 to control a robotic arm, which you've never used before. And you might be pretty bad for the

07:50.880 --> 07:54.960
 first couple of seconds. But if I tell you, like, your life depends on using this robotic arm to,

07:54.960 --> 08:00.000
 like, open this door, you'll probably manage it. Even though you've never seen this device before,

08:00.000 --> 08:04.320
 you've never used the joystick to control us, and you'll kind of muddle through it. And that's

08:04.320 --> 08:10.640
 not your evolved natural ability. That's your flexibility, your adaptability. And that's exactly

08:10.640 --> 08:13.120
 where our current robotic systems really kind of fall flat.

08:13.120 --> 08:18.880
 But I wonder how much general, almost what we think of as common sense,

08:20.240 --> 08:25.040
 pre trained models underneath all of that. So that ability to adapt to a joystick

08:25.040 --> 08:31.440
 requires you to have a kind of, you know, I'm human. So it's hard for me to introspect all

08:31.440 --> 08:37.440
 the knowledge I have about the world. But it seems like there might be an iceberg underneath

08:37.440 --> 08:41.520
 of the amount of knowledge we actually bring to the table. That's kind of the open question.

08:41.520 --> 08:45.520
 I think there's absolutely an iceberg of knowledge that we bring to the table. But

08:45.520 --> 08:49.600
 I think it's very likely that iceberg of knowledge is actually built up over our

08:49.600 --> 08:56.640
 lifetimes. Because we have, you know, we have a lot of prior experience to draw on. And it kind

08:56.640 --> 09:03.520
 of makes sense that the right way for us to, you know, to optimize our efficiency, our evolutionary

09:03.520 --> 09:09.280
 fitness, and so on, is to utilize all that experience to build up the best iceberg we can

09:09.280 --> 09:13.920
 get. And that's actually one, you know, while that sounds an awful lot like what machine

09:13.920 --> 09:18.400
 learning actually does, I think that for modern machine learning, it's actually one of the

09:18.400 --> 09:21.600
 things that for modern machine learning, it's actually a really big challenge to take this

09:21.600 --> 09:26.560
 unstructured mass of experience and distill out something that looks like a common sense

09:26.560 --> 09:31.120
 understanding of the world. And perhaps part of that is it's not because something about

09:31.120 --> 09:36.960
 machine learning itself is broken or hard, but because we've been a little too rigid

09:36.960 --> 09:41.440
 in subscribing to a very supervised, very rigid notion of learning, you know, kind of the input

09:41.440 --> 09:48.320
 output Xs go to Ys sort of model. And maybe what we really need to do is to view the world more

09:48.320 --> 09:53.760
 as like a massive experience that is not necessarily providing any rigid supervision,

09:53.760 --> 09:57.520
 but sort of providing many, many instances of things that could be. And then you take that

09:57.520 --> 10:00.400
 and you distill it into some sort of common sense understanding.

10:02.000 --> 10:06.720
 I see. Well, you're painting an optimistic, beautiful picture, especially from the robotics

10:06.720 --> 10:11.200
 perspective, because that means we just need to invest and build better learning algorithms,

10:12.240 --> 10:18.160
 figure out how we can get access to more and more data for those learning algorithms to extract

10:18.160 --> 10:24.000
 signal from and then accumulate that iceberg of knowledge. It's a beautiful picture. It's a

10:24.000 --> 10:30.080
 hopeful one. I think it's potentially a little bit more than just that. And this is where we

10:30.080 --> 10:34.880
 perhaps reach the limits of our current understanding. But one thing that I think that

10:34.880 --> 10:40.160
 the research community hasn't really resolved in a satisfactory way is how much it matters

10:40.160 --> 10:44.400
 where that experience comes from. Like, you know, do you just like download everything on the

10:44.400 --> 10:51.280
 internet and cram it into essentially the 21st century analog of the giant language model and

10:51.280 --> 10:56.240
 then see what happens? Or does it actually matter whether your machine physically experiences the

10:56.240 --> 11:01.760
 world or in the sense that it actually attempts things, observes the outcome of its actions and

11:01.760 --> 11:06.000
 kind of augments its experience that way? That it chooses which parts of the world it

11:06.960 --> 11:12.720
 gets to interact with and observe and learn from. Right. It may be that the world is so complex

11:12.720 --> 11:20.000
 that simply obtaining a large mass of sort of IID samples of the world is a very difficult way

11:20.000 --> 11:25.040
 to go. But if you are actually interacting with the world and essentially performing this sort of

11:25.040 --> 11:30.400
 hard negative mining by attempting what you think might work, observing the sometimes happy and

11:30.400 --> 11:35.600
 sometimes sad outcomes of that, and augmenting your understanding using that experience and

11:35.600 --> 11:40.800
 you're just doing this continually for many years, maybe that sort of data in some sense

11:40.800 --> 11:45.200
 is actually much more favorable to obtaining a common sense understanding. One reason we might

11:45.200 --> 11:51.920
 think that this is true is that what we associate with common sense or lack of common sense

11:51.920 --> 11:56.640
 is often characterized by the ability to reason about kind of counterfactual questions. Like,

11:57.360 --> 12:02.320
 if I were to... Here, this bottle of water is sitting on the table, everything is fine,

12:02.320 --> 12:06.400
 if I were to knock it over, which I'm not going to do, but if I were to do that, what would happen?

12:06.400 --> 12:11.680
 And I know that nothing good would happen from that, but if I have a bad understanding of the

12:11.680 --> 12:18.160
 world, I might think that that's a good way for me to gain more utility. If I actually go about

12:19.200 --> 12:23.120
 daily life doing the things that my current understanding of the world suggests will give

12:23.120 --> 12:30.640
 me high utility, in some ways, I'll get exactly the right supervision to tell me not to do those

12:30.640 --> 12:36.960
 bad things and to keep doing the good things. So, there's a spectrum between IID, random walk

12:36.960 --> 12:43.600
 through the space of data, and what we humans do. I don't even know if we do it optimal, but

12:43.600 --> 12:51.520
 there might be beyond. So, this open question that you raised, where do you think systems,

12:51.520 --> 12:57.360
 intelligent systems that would be able to deal with this world fall? Can we do pretty well

12:57.360 --> 13:04.720
 by reading all of Wikipedia, randomly sampling it, like language models do, or do we have to be

13:04.720 --> 13:10.320
 exceptionally selective and intelligent about which aspects of the world we try?

13:11.840 --> 13:15.760
 So, I think this is first an open scientific problem, and I don't have a clear answer,

13:15.760 --> 13:20.800
 but I can speculate a little bit. And what I would speculate is that you don't need to be

13:20.800 --> 13:27.680
 super, super careful. I think it's less about being careful to avoid the useless stuff,

13:27.680 --> 13:32.640
 and more about making sure that you hit on the really important stuff. So, perhaps it's okay

13:32.640 --> 13:38.800
 if you spend part of your day just guided by your curiosity, visiting interesting regions of

13:38.800 --> 13:43.840
 your state space, but it's important for you to, every once in a while, make sure that you really

13:43.840 --> 13:49.680
 try out the solutions that your current model of the world suggests might be effective,

13:49.680 --> 13:55.040
 and observe whether those solutions are working as you expect or not. And perhaps some of that

13:55.040 --> 14:00.480
 is really essential to have a perpetual improvement loop. This perpetual improvement

14:00.480 --> 14:06.480
 loop is really the key that's going to potentially distinguish the best current methods from the

14:06.480 --> 14:11.840
 best methods of tomorrow, in a sense. How important do you think is exploration or total out of the

14:11.840 --> 14:20.080
 box thinking exploration in this space to jump to totally different domains? So, you mentioned

14:20.080 --> 14:26.080
 there's an optimization problem, you explore the specifics of a particular strategy, whatever the

14:26.080 --> 14:32.080
 thing you're trying to solve. How important is it to explore totally outside of the strategies

14:32.080 --> 14:34.960
 that have been working for you so far? What's your intuition there?

14:34.960 --> 14:39.760
 Yeah, I think it's a very problem dependent kind of question. And I think that that's actually,

14:39.760 --> 14:47.120
 you know, in some ways, that question gets at one of the big differences between

14:48.400 --> 14:54.640
 sort of the classic formulation of a reinforcement learning problem and some of the sort of more

14:54.640 --> 14:58.160
 open ended reformulations of that problem that had been explored in recent years. So,

14:58.160 --> 15:03.040
 classically, reinforcement learning is framed as a problem of maximizing utility, like any kind of

15:03.040 --> 15:07.440
 rational AI agent, and then anything you do is in service to maximizing that utility.

15:07.440 --> 15:15.440
 But a very interesting kind of way to look at, I'm not necessarily saying this is the best way to

15:15.440 --> 15:19.680
 look at it, but an interesting alternative way to look at these problems is as something where

15:19.680 --> 15:24.960
 you first get to explore the world however you please, and then afterwards you will be tasked

15:24.960 --> 15:29.200
 with doing something. And that might suggest a somewhat different solution. So, if you don't

15:29.200 --> 15:32.880
 know what you're going to be tasked with doing, and you just want to prepare yourself optimally

15:32.880 --> 15:37.920
 for whatever you're on certain future holds, maybe then you will choose to attain some sort of

15:37.920 --> 15:44.000
 coverage, build up sort of an arsenal of cognitive tools, if you will, such that later on when someone

15:44.000 --> 15:48.480
 tells you, now your job is to fetch the coffee for me, you will be well prepared to undertake that

15:48.480 --> 15:53.680
 task. And that you see that as the modern formulation of the reinforcement learning

15:53.680 --> 15:59.040
 problem as a kind of the more multitask, the general intelligence kind of formulation.

15:59.040 --> 16:04.160
 I think that's one possible vision of where things might be headed. I don't think that's by any means

16:04.160 --> 16:08.480
 the mainstream or standard way of doing things, and it's not like if I had to...

16:08.480 --> 16:14.400
 But I like it. It's a beautiful vision. So, maybe actually take a step back. What is the goal of

16:14.400 --> 16:18.480
 robotics? What's the general problem of robotics we're trying to solve? You actually kind of painted

16:18.480 --> 16:23.360
 two pictures here, one of sort of the narrow, one of the general. What in your view is the big

16:23.360 --> 16:29.200
 problem of robotics? Again, ridiculously philosophical question.

16:29.200 --> 16:34.640
 I think that maybe there are two ways I can answer this question. One is there's a very

16:34.640 --> 16:41.440
 pragmatic problem, which is what would make robots... What would sort of maximize the

16:41.440 --> 16:50.400
 usefulness of robots? And there the answer might be something like a system where a system that

16:50.400 --> 16:58.800
 a system that can perform whatever task a human user sets for it, within the physical constraints,

16:58.800 --> 17:03.520
 of course. If you teleport to another planet, it probably can't do that. But if you ask it to do

17:03.520 --> 17:07.920
 something that's within its physical capability, then potentially with a little bit of additional

17:07.920 --> 17:11.760
 training or a little bit of additional trial and error, it ought to be able to figure it out

17:11.760 --> 17:16.160
 in much the same way as like a human teleoperator ought to figure out how to drive the robot to

17:16.160 --> 17:22.720
 do that. That's kind of the very pragmatic view of what it would take to kind of solve the robotics

17:22.720 --> 17:28.800
 problem, if we will. But I think that there is a second answer, and that answer is a lot closer

17:28.800 --> 17:34.240
 to why I want to work on robotics, which is that I think it's less about what it would take to do a

17:34.240 --> 17:39.040
 really good job in the world of robotics, but more the other way around of what robotics

17:39.040 --> 17:42.800
 can bring to the table to help us understand artificial intelligence.

17:42.800 --> 17:47.520
 So your dream, fundamentally, is to understand intelligence?

17:47.520 --> 17:53.840
 Yes. I think that's the dream for many people who actually work in this space. I think that

17:54.560 --> 17:59.200
 there's something very pragmatic and very useful about studying robotics. But I do think that a

17:59.200 --> 18:04.960
 lot of people that go into this field, actually, the things that they draw inspiration from are the

18:05.520 --> 18:10.160
 potential for robots to help us learn about intelligence and about ourselves.

18:10.160 --> 18:17.600
 So that's fascinating that robotics is basically the space by which you can get closer to

18:17.600 --> 18:22.720
 understanding the fundamentals of artificial intelligence. So what is it about robotics

18:22.720 --> 18:27.760
 that's different from some of the other approaches? So if we look at some of the early breakthroughs

18:27.760 --> 18:32.400
 in deep learning or in the computer vision space and the natural language processing,

18:32.400 --> 18:36.880
 there's really nice, clean benchmarks that a lot of people competed on, and thereby came

18:36.880 --> 18:42.080
 up with a lot of brilliant ideas. What's the fundamental difference between computer vision

18:42.080 --> 18:45.680
 purely defined and ImageNet and kind of the bigger robotics problem?

18:46.400 --> 18:54.160
 So there are a couple of things. One is that with robotics, you kind of have to take away

18:54.160 --> 19:00.880
 many of the crutches. So you have to deal with both the particular problems of perception,

19:00.880 --> 19:03.600
 control, and so on. But you also have to deal with the integration of those things.

19:03.600 --> 19:08.560
 And, you know, classically, we've always thought of the integration as kind of a separate problem.

19:08.560 --> 19:12.160
 So a classic kind of modular engineering approach is that we solve the individual

19:12.160 --> 19:16.400
 sub problems, then wire them together, and then the whole thing works. And one of the

19:16.400 --> 19:19.840
 things that we've been seeing over the last couple of decades is that we'll maybe

19:20.400 --> 19:24.800
 studying the thing as a whole might lead to just like very different solutions than if we were

19:24.800 --> 19:29.680
 to study the parts and wire them together. So the integrative nature of robotics research

19:29.680 --> 19:35.600
 helps us see, you know, the different perspectives on the problem. Another part of the answer is that

19:36.160 --> 19:42.960
 with robotics, it casts a certain paradox into very clever relief. So this is sometimes referred

19:42.960 --> 19:50.480
 to as Morvix paradox, the idea that in artificial intelligence, things that are very hard for people

19:50.480 --> 19:53.760
 can be very easy for machines. And vice versa, things that are very easy for people can be

19:53.760 --> 20:01.120
 very hard for machines. So, you know, integral and differential calculus is pretty difficult to

20:01.120 --> 20:05.600
 learn for people. But if you program a computer, do it, it can derive derivatives and integrals

20:05.600 --> 20:11.600
 for you all day long without any trouble. Whereas some things like, you know, drinking from a cup

20:11.600 --> 20:17.920
 of water, very easy for a person to do very hard for a robot to deal with. And sometimes when we

20:17.920 --> 20:22.160
 see such blatant discrepancies, that gives us a really strong hint that we're missing something

20:22.160 --> 20:27.120
 important. So if we really try to zero in on those discrepancies, we might find that little bit

20:27.120 --> 20:31.840
 that we're missing. And it's not that we need to make machines better or worse at math and better

20:31.840 --> 20:37.600
 at drinking water, but just that by studying those discrepancies, we might find some new insight.

20:37.600 --> 20:42.640
 So that could be, that could be in any space, doesn't have to be robotics. But you're saying,

20:43.600 --> 20:49.280
 I mean, I get, it's kind of interesting that robotics seems to have a lot of those discrepancies.

20:49.280 --> 20:55.920
 So the, the, the Hans Maravak paradox is probably referring to the space of the physical

20:55.920 --> 21:00.560
 interaction, like you said, object manipulation, walking, all the kind of stuff we do in the physical

21:00.560 --> 21:13.200
 world. How do you make sense if you were to try to disentangle the Maravaks paradox? Like, why is

21:13.200 --> 21:20.000
 there such a gap in our intuition about it? Why do you think manipulating objects is so

21:20.000 --> 21:24.720
 hard from everything you've learned from applying reinforcement learning in this space?

21:25.520 --> 21:33.600
 Yeah, I think that one reason is maybe that for many of the, for many of the other problems that

21:33.600 --> 21:41.120
 we've studied in AI and computer science and so on, the notion of input, output and supervision

21:41.120 --> 21:45.600
 is much, much cleaner. So computer vision, for example, deals with very complex inputs,

21:45.600 --> 21:52.400
 but it's comparatively a bit easier, at least up to some level of abstraction, to cast it as a very

21:52.400 --> 21:58.560
 tightly supervised problem. It's comparatively much, much harder to cast robotic manipulation as a

21:58.560 --> 22:03.520
 very tightly supervised problem. You can do it. It just doesn't seem to work all that well. So you

22:03.520 --> 22:07.600
 could say that, well, maybe we get a labeled data set where we know exactly which motor commands

22:07.600 --> 22:12.400
 to send and then we train on that. But for various reasons, that's not actually like such a great

22:12.400 --> 22:17.280
 solution. And it also doesn't seem to be even remotely similar to how people and animals learn

22:17.280 --> 22:22.960
 to do things because we're not told by like our parents, here's how you fire your muscles in order

22:22.960 --> 22:29.120
 to walk. We do get some guidance, but the really low level detailed stuff we figure out mostly on

22:29.120 --> 22:34.320
 our own. And that's what you mean by tightly coupled that every single little subaction gets a

22:34.320 --> 22:39.040
 supervised signal of whether it's a good one or not. Right. So while in computer vision, you

22:39.040 --> 22:43.280
 could sort of imagine up to a level of abstraction that maybe somebody told you this is a car and

22:43.280 --> 22:47.520
 this is a cat and this is a dog. In motor control, it's very clear that that was not the case.

22:49.120 --> 22:57.120
 If we look at sort of the subspaces of robotics that, again, as you said, robotics integrates

22:57.120 --> 23:02.080
 all of them together and we get to see how this beautiful mess interplays. But so there's nevertheless

23:02.080 --> 23:08.800
 still perception. So it's the computer vision problem, broadly speaking, understanding the

23:08.800 --> 23:13.760
 environment. Then there's also, maybe you can correct me on this kind of categorization of the

23:13.760 --> 23:19.920
 space, then there's prediction in trying to anticipate what things are going to do into

23:19.920 --> 23:26.880
 the future in order for you to be able to act in that world. And then there's also this game

23:26.880 --> 23:35.120
 theoretic aspect of how your actions will change the behavior of others. In this kind of space,

23:36.000 --> 23:39.760
 and this is bigger than reinforcement learning, this is just broadly looking at the problem

23:39.760 --> 23:45.440
 of robotics. What's the hardest problem here? Or is there, or is what you said

23:47.680 --> 23:52.640
 true that when you start to look at all of them together, that's a whole

23:52.640 --> 23:58.080
 another thing. You can't even say which one individually is harder because all of them

23:58.080 --> 24:02.720
 together, you should only be looking at them all together. I think when you look at them all together,

24:03.280 --> 24:06.880
 some things actually become easier. And I think that's actually pretty important.

24:09.200 --> 24:16.160
 Back in 2014, we had some work, basically our first work on end to end reinforcement learning

24:16.160 --> 24:21.520
 for robotic manipulation skills from vision, which at the time was something that seemed

24:21.520 --> 24:28.000
 a little inflammatory and controversial in the robotics world. But other than the inflammatory

24:28.000 --> 24:32.560
 and controversial part of it, the point that we were actually trying to make in that work is that

24:32.560 --> 24:37.200
 for the particular case of combining perception and control, you could actually do better if you

24:37.200 --> 24:41.120
 treat them together than if you try to separate them. And the way that we tried to demonstrate

24:41.120 --> 24:46.640
 this is we picked a fairly simple motor control task where a robot had to insert a little red

24:46.640 --> 24:52.880
 trapezoid into a trapezoidal hole. And we had our separated solution, which involved first

24:52.880 --> 24:57.360
 detecting the hole using a pose detector, and then actuating the arm to put it in,

24:57.360 --> 25:02.880
 and then our intent solution, which just mapped pixels to the torques. And one of the things we

25:02.880 --> 25:06.960
 observed is that if you use the intent solution, essentially the pressure on the perception part

25:06.960 --> 25:10.480
 of the model is actually lower. It doesn't have to figure out exactly where the thing is in 3D

25:10.480 --> 25:16.000
 space. It just needs to figure out where it is distributing the errors in such a way that

25:16.000 --> 25:19.360
 the horizontal difference matters more than the vertical difference, because vertically it just

25:19.360 --> 25:23.840
 pushes it down all the way until it can't go any further. And their perceptual errors are a lot

25:23.840 --> 25:27.680
 less harmful, whereas perpendicular to the direction of motion, perceptual errors are much

25:27.680 --> 25:33.440
 more harmful. So the point is that if you combine these two things, you can trade off errors between

25:33.440 --> 25:39.600
 the components optimally to best accomplish the task. And the components can actually be weaker

25:39.600 --> 25:41.760
 while still leading to better overall performance.

25:41.760 --> 25:47.920
 It has a profound idea. I mean, in the space of pegs and things like that, it's quite simple.

25:48.560 --> 25:55.280
 It almost is tempting to overlook. But that seems to be at least intuitively an idea that should

25:56.000 --> 25:59.760
 generalize the basically all aspects of perception and control.

25:59.760 --> 26:00.160
 Of course.

26:00.160 --> 26:01.840
 That one strengthens the other.

26:01.840 --> 26:07.840
 Yeah. And people who have studied perceptual heuristics in humans and animals find things

26:07.840 --> 26:12.160
 like that all the time. So one very well known example is something called the gaze heuristic,

26:12.160 --> 26:17.440
 which is a little trick that you can use to intercept a flying object. So if you want to

26:17.440 --> 26:22.480
 catch a ball, for instance, you could try to localize it in 3D space, estimate its velocity,

26:22.480 --> 26:25.920
 estimate the effect of wind resistance, solve a complex system of differential equations in your

26:25.920 --> 26:32.880
 head. Or you can maintain a running speed so that the object stays in the same position

26:32.880 --> 26:36.400
 as in your field of view. So if it dips a little bit, you speed up. If it rises a little bit,

26:36.400 --> 26:40.320
 you slow down. And if you follow the simple rule, you'll actually arrive at exactly the

26:40.320 --> 26:45.040
 place where the object lands and you'll catch it. And humans use it when they play baseball.

26:45.040 --> 26:48.240
 Human pilots use it when they fly airplanes to figure out if they're about to collide with

26:48.240 --> 26:52.560
 somebody. Frogs use us to catch insects and so on and so on. So this is something that

26:52.560 --> 26:56.160
 actually happens in nature. And I'm sure this is just one instance of it that we were able to

26:56.160 --> 26:59.840
 identify just because all that scientists were able to identify because it's so prevalent,

26:59.840 --> 27:00.800
 but there are probably many others.

27:00.800 --> 27:07.040
 Do you have a, just so we can zoom in as we talk about robotics, do you have a canonical problem,

27:07.040 --> 27:14.080
 sort of a simple, clean, beautiful representative problem in robotics that you think about when

27:14.080 --> 27:18.800
 you're thinking about some of these problems? We talked about robotic manipulation. To me,

27:18.800 --> 27:26.720
 that seems intuitively, at least the robotics community has conversed towards that as a space

27:26.720 --> 27:32.080
 that's the canonical problem. If you agree, then maybe you do zoom in in some particular

27:32.080 --> 27:36.880
 aspect of that problem that you just like. Like if we solve that problem perfectly,

27:36.880 --> 27:42.480
 it'll unlock a major step towards human level intelligence.

27:43.920 --> 27:47.760
 I don't think I have like a really great answer to that. And I think partly the reason I don't

27:47.760 --> 27:54.800
 have a great answer kind of has to do with the, it has to do with the fact that the difficulty

27:54.800 --> 27:59.760
 is really in the flexibility and adaptability rather than in doing a particular thing really,

27:59.760 --> 28:08.000
 really well. So it's hard to just say like, oh, if you can shuffle a deck of cards as fast as like

28:08.000 --> 28:15.280
 a Vegas casino dealer, then you'll be very proficient. It's really the ability to quickly

28:15.280 --> 28:25.920
 figure out how to do some arbitrary new thing well enough to move on to the next arbitrary thing.

28:25.920 --> 28:33.920
 But the source of newness and uncertainty, have you found problems in which it's easy to

28:34.800 --> 28:39.680
 generate new newnessnessness is new types of newness?

28:39.680 --> 28:46.560
 Yeah. So a few years ago, if you'd asked me this question around like 2016, maybe,

28:46.560 --> 28:51.520
 I would have probably said that robotic grasping is a really great example of that because

28:51.520 --> 28:57.040
 it's a task with great real world utility. Like you will get a lot of money if you can do it well.

28:57.040 --> 28:58.720
 What is robotic grasping?

28:58.720 --> 29:00.720
 Picking up any object.

29:00.720 --> 29:02.240
 With a robotic hand.

29:02.240 --> 29:06.160
 Exactly. So you will get a lot of money if you do it well because lots of people want to run

29:06.160 --> 29:12.480
 warehouses with robots. And it's highly non trivial because very different objects will

29:12.480 --> 29:17.360
 require very different grasping strategies. But actually, since then, people have gotten

29:18.000 --> 29:22.320
 really good at building systems to solve this problem to the point where I'm not actually

29:22.320 --> 29:28.560
 sure how much more progress we can make with that as like the main guiding thing.

29:29.280 --> 29:33.120
 But it's kind of interesting to see the kind of methods that have actually worked well in that

29:33.120 --> 29:39.760
 space because robotic grasping classically used to be regarded very much as kind of almost like

29:39.760 --> 29:45.920
 a geometry problem. So people who have studied the history of computer vision will find this

29:45.920 --> 29:49.440
 very familiar that kind of in the same way that in the early days of computer vision,

29:49.440 --> 29:53.440
 people thought of it very much as like an inverse graphics thing. In robotic grasping,

29:53.440 --> 29:58.480
 people thought of it as an inverse physics problem. Essentially, you look at what's in front of you,

29:58.480 --> 30:02.720
 figure out the shapes, then use your best estimate of the laws of physics to figure out

30:02.720 --> 30:07.120
 where to put your fingers on, you pick up the thing. And it turns out that what works really

30:07.120 --> 30:12.240
 well for robotic grasping instantiated in many different recent works, including our own, but

30:12.240 --> 30:19.280
 also ones from many other labs is to use learning methods with some combination of either exhaustive

30:19.280 --> 30:23.120
 simulation or like actual real world trial and error. And it turns out that those things actually

30:23.120 --> 30:26.640
 work really well. And then you don't have to worry about solving geometry problems or physics

30:26.640 --> 30:35.120
 problems. So just by the way in the grasping, what are the difficulties that have been worked on?

30:35.120 --> 30:41.200
 So one is like the materials of things, maybe occlusions on the perception side. Why is it

30:41.200 --> 30:47.120
 such a difficult? Why is picking stuff up such a difficult problem? Yeah, it's a difficult problem

30:47.120 --> 30:52.560
 because the number of things that you might have to deal with or the variety of things that you

30:52.560 --> 30:58.400
 have to deal with is extremely large. And oftentimes, things that work for one class of

30:58.400 --> 31:03.680
 objects won't work for other class of objects. So if you get really good at picking up boxes,

31:03.680 --> 31:09.440
 and now you have to pick up plastic bags, you just need to employ a very different strategy.

31:09.440 --> 31:15.120
 And there are many properties of objects that are more than just their geometry,

31:15.120 --> 31:19.600
 it has to do with the bits that are easier to pick up, the bits that are hard to pick up,

31:19.600 --> 31:23.920
 the bits that are more flexible, the bits that will cause the thing to pivot and bend and drop

31:23.920 --> 31:28.960
 out of your hand versus the bits that result in a nice secure grasp, things that are flexible,

31:28.960 --> 31:32.800
 things that if you pick them up the wrong way, they'll fall upside down and the contents will

31:32.800 --> 31:38.160
 spill out. So there's all these little details that come up. But the task is still kind of can

31:38.160 --> 31:42.240
 be characterized as one task, like there's a very clear notion of you did it or you didn't do it.

31:42.240 --> 31:51.040
 So in terms of spilling things, there creeps in this notion that starts to sound and feel like

31:51.040 --> 31:59.280
 common sense reasoning. Do you think solving the general problem of robotics requires

31:59.920 --> 32:07.040
 common sense reasoning, requires general intelligence, this kind of human level capability of,

32:07.040 --> 32:13.360
 you know, like you said, be robust and deal with uncertainty, but also be able to sort of reason

32:13.360 --> 32:21.040
 and assimilate different pieces of knowledge that you have. Yeah. What are your thoughts on

32:22.000 --> 32:27.680
 the needs of common sense reasoning in the space of the general robotics problem?

32:28.400 --> 32:32.240
 So I'm going to slightly dodge that question and say that I think maybe actually,

32:32.240 --> 32:37.520
 it's the other way around is that studying robotics can help us understand how to put

32:37.520 --> 32:44.080
 common sense into RAI systems. One way to think about common sense is that, and why our current

32:44.080 --> 32:48.800
 systems might lack common sense, is that common sense is an emergent property of

32:50.080 --> 32:54.880
 actually having to interact with a particular world, a particular universe, and get things done

32:54.880 --> 32:59.680
 in that universe. So you might think that, for instance, like an image captioning system,

32:59.680 --> 33:06.160
 maybe it looks at pictures of the world and it types out English sentences. So it kind of

33:06.160 --> 33:11.040
 deals with our world. And then you can easily construct situations where image captioning

33:11.040 --> 33:15.520
 systems do things that defy common sense, like give it a picture of a person wearing a fur coat,

33:15.520 --> 33:20.080
 and we'll say it's a teddy bear. But what I think what's really happening in those settings

33:20.080 --> 33:24.960
 is that the system doesn't actually live in our world, it lives in its own world that consists

33:24.960 --> 33:30.240
 of pixels and English sentences, and doesn't actually consist of having to put on a fur coat

33:30.240 --> 33:35.840
 in the winter so you don't get cold. So perhaps the reason for the disconnect is that the

33:36.640 --> 33:41.680
 systems that we have now simply inhabit a different universe. And if we build AI systems

33:41.680 --> 33:46.400
 that are forced to deal with all of the messiness and complexity of our universe, maybe they will

33:46.400 --> 33:51.520
 have to acquire common sense to essentially maximize their utility. Whereas the systems

33:51.520 --> 33:55.120
 we're building now don't have to do that, they can take some shortcut.

33:55.120 --> 34:00.320
 That's fascinating. You've a couple of times already sort of reframed the role of robotics

34:00.320 --> 34:05.920
 in this whole thing. And for some reason, I don't know if my way of thinking is common,

34:05.920 --> 34:11.280
 but I thought like, we need to understand and solve intelligence in order to solve robotics.

34:12.080 --> 34:16.880
 And you're kind of framing it as, no, robotics is one of the best ways to just study

34:16.880 --> 34:22.480
 artificial intelligence and build sort of like robotics is like the right space in which

34:23.040 --> 34:29.360
 you get to explore some of the fundamental learning mechanisms, fundamental sort of

34:29.360 --> 34:36.000
 multimodal, multitask aggregation of knowledge mechanisms that are required for general

34:36.000 --> 34:40.320
 intelligence. That's a really interesting way to think about it. But let me ask about learning.

34:41.280 --> 34:46.720
 Can the general sort of robotics, the epitome of the robotics problem be solved purely

34:46.720 --> 34:53.280
 through learning, perhaps end to end learning, sort of learning from scratch,

34:54.480 --> 34:58.720
 as opposed to injecting human expertise and rules and heuristics and so on?

34:59.920 --> 35:05.840
 I think that in terms of the spirit of the question, I would say yes. I mean, I think that

35:06.640 --> 35:12.480
 though in some ways it may be like an overly sharp dichotomy, like, you know, I think that

35:12.480 --> 35:17.600
 in some ways when we build algorithms, at some point, a person does something.

35:19.600 --> 35:24.640
 A person turned on the computer, a person implemented TensorFlow.

35:26.160 --> 35:29.760
 But yeah, I think that in terms of the point that you're getting at, I do think the answer

35:29.760 --> 35:36.480
 is yes. I think that we can solve many problems that have previously required meticulous manual

35:36.480 --> 35:40.960
 engineering through automated optimization techniques. And actually, one thing I will say

35:40.960 --> 35:46.080
 on this topic is I don't think this is actually a very radical or very new idea. I think people

35:46.080 --> 35:51.680
 have been thinking about automated optimization techniques as a way to do control for a very,

35:51.680 --> 35:59.680
 very long time. And in some ways, what's changed is really more the name. So today we would say that,

35:59.680 --> 36:04.560
 oh, my robot does machine learning, it does reinforcement learning, maybe in the 1960s,

36:04.560 --> 36:10.400
 you'd say, oh, my robot is doing optimal control. And maybe the difference between typing out a

36:10.400 --> 36:15.600
 system of differential equations and doing feedback linearization versus training in neural net,

36:15.600 --> 36:21.040
 maybe it's not such a large difference. It's just pushing the optimization deeper and deeper into

36:21.040 --> 36:25.920
 the thing. Well, it is interesting, you think that way, but especially with deep learning,

36:26.480 --> 36:35.920
 that the accumulation of experiences in data form to form deep representations starts to feel

36:35.920 --> 36:41.120
 like knowledge as opposed to optimal control. So this feels like there's an accumulation of

36:41.120 --> 36:44.880
 knowledge through the learning process. Yes. Yeah. So I think that is a good point that

36:45.760 --> 36:50.240
 one big difference between learning based systems and classic optimal control systems is that

36:50.240 --> 36:54.400
 learning based systems in principle should get better and better the more they do something.

36:54.960 --> 36:57.440
 And I do think that that's actually a very, very powerful difference.

36:58.000 --> 37:03.200
 So look back at the world of expert systems, the symbolic AI and so on,

37:03.200 --> 37:10.400
 of using logic to accumulate expertise, human expertise, human encoded expertise.

37:10.960 --> 37:16.080
 Do you think that will have a role at some points? The deep learning, machine learning,

37:16.080 --> 37:22.720
 reinforcement learning has shown incredible results and breakthroughs and just inspired

37:23.280 --> 37:30.480
 thousands, maybe millions of researchers. But there's this less popular now,

37:30.480 --> 37:34.000
 but it used to be popular idea of symbolic AI. Do you think that will have a role?

37:35.040 --> 37:44.720
 I think in some ways, the kind of the descendants of symbolic AI actually already have a role. So

37:45.840 --> 37:50.560
 this is the highly biased history from my perspective. You say that, well, initially we

37:50.560 --> 37:56.480
 thought that rational decision making involves logical manipulation. So you have some model of

37:56.480 --> 38:03.120
 the world expressed in terms of logic. You have some query like, what action do I take in order to

38:03.120 --> 38:08.240
 for X to be true? And then you manipulate your logical symbolic representation to get an answer.

38:08.240 --> 38:14.160
 What that turned into somewhere in the 1990s is, well, instead of building kind of predicates

38:14.160 --> 38:20.720
 and statements that have true or false values, we'll build probabilistic systems where things

38:20.720 --> 38:23.520
 have probabilities associated and probabilities of being true and false. And that turned into

38:23.520 --> 38:29.760
 Bayes Nets. And that provided sort of a boost to what were really still essentially logical

38:29.760 --> 38:34.240
 inference systems, just probabilistic logical inference systems. And then people said, well,

38:34.240 --> 38:40.560
 let's actually learn the individual probabilities inside these models. And then people said, well,

38:40.560 --> 38:45.520
 let's not even specify the nodes in the models, let's just put a big neural net in there. But in

38:45.520 --> 38:49.440
 many ways, I see these as actually kind of descendants from the same idea. It's essentially

38:49.440 --> 38:54.960
 instantiating rational decision making by means of some inference process, and learning by means

38:54.960 --> 39:00.640
 of an optimization process. So in a sense, I would say yes, that it has a place. And in many

39:00.640 --> 39:05.600
 ways, that place is over, you know, it already holds that place. It's already in there. Yeah,

39:05.600 --> 39:09.680
 it's just by different, it looks slightly different than it was before. But in some,

39:09.680 --> 39:13.360
 there are some things that we can think about that make this a little bit more obvious. Like,

39:13.360 --> 39:18.800
 if I train a big neural net model to predict what will happen in response to my robot's actions,

39:18.800 --> 39:23.680
 and then I run probabilistic inference, meaning I invert that model to figure out the actions that

39:23.680 --> 39:28.320
 lead to some plausible outcome. Like, to me, that seems like a kind of logic. You have a model of

39:28.320 --> 39:33.040
 the world, it just happens to be expressed by a neural net. And you are doing some inference

39:33.040 --> 39:38.240
 procedure, some sort of manipulation on that model to figure out, you know, the answer to a

39:38.240 --> 39:43.280
 query that you have. It's the interpretability, it's the explainability, though, that seems to

39:43.280 --> 39:49.040
 be lacking more so because the nice thing about sort of expert systems is you can follow the

39:49.040 --> 39:56.480
 reasoning of the system that to us, mere humans is somehow compelling. It would, it's just,

39:58.080 --> 40:05.040
 I don't know what to make of this fact that there's a human desire for intelligent systems to be able

40:05.040 --> 40:14.480
 to convey in a poetic way to us why it made the decisions it did. Like, tell a convincing story.

40:15.040 --> 40:22.240
 And perhaps that's like a silly human thing. Like, we shouldn't expect that of intelligent

40:22.240 --> 40:27.680
 systems. Like, we should be super happy that there is intelligent systems out there. But

40:29.200 --> 40:34.080
 if I were to sort of psychoanalyze the researchers at the time, I would say expert systems connected

40:34.080 --> 40:40.880
 to that part, that desire for AI researchers for systems to be explainable. I mean, maybe on that

40:40.880 --> 40:50.160
 topic, do you have a hope that sort of inference systems of learning based systems will be as

40:50.160 --> 40:56.080
 explainable as the dream was with expert systems, for example? I think it's a very complicated

40:56.080 --> 41:03.040
 question because I think that in some ways, the question of explainability is kind of very closely

41:03.040 --> 41:09.280
 tied to the question of performance. Like, why do you want your system to explain itself? Well,

41:09.280 --> 41:15.280
 it's so that when it screws up, you can kind of figure out why it did it. But in some ways,

41:15.280 --> 41:20.080
 that's a much bigger problem, actually. Like, your system might screw up and then it might

41:20.080 --> 41:25.760
 screw up in how it explains itself, or you might have some bug somewhere so that it's not actually

41:25.760 --> 41:30.880
 doing what it was supposed to do. So, maybe a good way to view that problem is really as a

41:30.880 --> 41:37.680
 problem, as a bigger problem of verification and validation of which explainability is sort of

41:37.680 --> 41:43.920
 one component. I see. I just see it differently. I see explainability. You put it beautifully. I

41:43.920 --> 41:48.320
 think you actually summarized the field of explainability. But to me, there's another

41:48.320 --> 41:54.960
 aspect of explainability, which is like storytelling that has nothing to do with errors or with

41:54.960 --> 42:05.200
 like the sort of it doesn't it uses errors as as elements of its story, as opposed to a fundamental

42:05.200 --> 42:10.880
 need to be explainable when errors occur. It's just that for other intelligence systems to be in

42:10.880 --> 42:17.360
 our world, we seem to want to tell each other stories. And that that's true in the political

42:17.360 --> 42:23.280
 world. That's true in the academic world. And that I, you know, neural networks are less capable

42:23.280 --> 42:27.520
 of doing that, or perhaps they're equally capable of storytelling, storytelling, maybe it doesn't

42:27.520 --> 42:31.920
 matter what the fundamentals of the system are, you just need to be a good storyteller.

42:32.560 --> 42:38.320
 Maybe one specific story I can tell you about in that space is actually about some work that was

42:38.320 --> 42:44.160
 done by about my former collaborator, who's now a professor at MIT named Jacob Andreas. Jacob actually

42:44.160 --> 42:48.240
 works in natural language processing, but he had this idea to do a little bit of work in reinforcement

42:48.240 --> 42:54.160
 learning, and how on how natural language can basically structure the internals of policies

42:54.160 --> 43:00.800
 trained with RL. And one of the things he did is he set up a model that attempts to perform some

43:00.800 --> 43:06.320
 tasks that's defined by a reward function. But the model reads in a natural language instruction.

43:06.320 --> 43:09.760
 So this is a pretty common thing to do an instruction following. So you tell it like,

43:09.760 --> 43:13.840
 you know, go to the red house, and then it's supposed to go to the red house. But then one

43:13.840 --> 43:19.360
 of the things that Jacob did is he treated that sentence not as a command from a person,

43:19.360 --> 43:26.480
 but as a representation of the internal kind of state of the mind of this policy, essentially,

43:26.480 --> 43:31.040
 so that when it was faced with a new task, what it would do is it would basically try to think of

43:31.040 --> 43:35.440
 possible language descriptions, attempt to do them and see if they led to the right outcome.

43:35.440 --> 43:38.800
 So would it kind of think out loud, like, you know, I'm faced with this new task, what am I

43:38.800 --> 43:43.680
 going to do? Let me go to the red house. Oh, that didn't work. Let me go to the blue room or something,

43:43.680 --> 43:47.280
 let me go to the green plant. And once it got some reward, it would say, oh, go to the green

43:47.280 --> 43:50.160
 plant, that's what's working, I'm going to go to the green plant. And then you could look at the

43:50.160 --> 43:53.120
 string that it came up with, and that was a description of how it thought it should solve

43:53.120 --> 43:58.400
 the problem. So you could do, you could basically incorporate language as internal state, and you

43:58.400 --> 44:02.960
 can start getting some handle on these kinds of things. And then what I was kind of trying to

44:02.960 --> 44:09.280
 get to is that also if you add to the reward function, the convincingness of that story.

44:09.280 --> 44:15.760
 So I have another reward signal of like, people who review that story, how much they like it.

44:16.560 --> 44:22.800
 So that, you know, initially that could be a hyper parameter sort of hard coded heuristic

44:22.800 --> 44:30.640
 type of thing, but it's an interesting notion of the convincingness of the story becoming part

44:30.640 --> 44:36.320
 of the reward function, the objective function of the explainability. It's in the world of sort of

44:36.320 --> 44:42.640
 Twitter and fake news that might be a scary notion that the nature of truth may not be as

44:42.640 --> 44:47.680
 important as the convincingness of the how convincing you are in telling the story around

44:48.640 --> 44:56.960
 the facts. Well, let me ask the basic question. You're one of the world class researchers in

44:56.960 --> 45:00.800
 reinforcement learning, deeper enforcement learning, certainly in the robotics space.

45:01.600 --> 45:05.920
 What is reinforcement learning? I think that what reinforcement learning

45:05.920 --> 45:12.800
 refers to today is really just the kind of the modern incarnation of learning based control.

45:12.800 --> 45:16.320
 So classically, reinforcement learning has a much more narrow definition, which is that

45:16.320 --> 45:20.080
 it's, you know, literally learning from reinforcement, like the thing does something

45:20.080 --> 45:24.560
 and then it gets a reward or punishment. But really, I think the way the term is used today is

45:24.560 --> 45:29.520
 it's used for more broadly to learning based control. So some kind of system that's supposed

45:29.520 --> 45:36.160
 to be controlling something and it uses data to get better. And what does control mean? So this

45:36.160 --> 45:40.080
 action is the fundamental element there. It means making rational decisions.

45:40.880 --> 45:44.160
 And rational decisions are decisions that maximize a measure of utility.

45:44.160 --> 45:48.720
 And sequentially, so you made decisions time and time and time again. Now, like,

45:49.840 --> 45:56.240
 it's easier to see that kind of idea in the space of maybe games and the space of robotics.

45:56.240 --> 46:02.800
 Do you see it bigger than that? Is it applicable? Like, where are the limits of the applicability

46:02.800 --> 46:08.080
 of reinforcement learning? Yeah, so rational decision making is essentially the

46:09.120 --> 46:14.800
 encapsulation of the AI problem viewed through a particular lens. So any problem that we would

46:14.800 --> 46:20.400
 want a machine to do, an intelligent machine can likely be represented as a decision making problem.

46:20.400 --> 46:24.720
 Classifying images is a decision making problem, although not a sequential one typically.

46:24.720 --> 46:29.120
 You know, controlling a chemical plant as a decision making problem,

46:30.240 --> 46:33.680
 deciding what videos to recommend on YouTube is a decision making problem.

46:34.320 --> 46:39.680
 And one of the really appealing things about reinforcement learning is, if it does encapsulate

46:39.680 --> 46:43.920
 the range of all these decision making problems, perhaps working on reinforcement learning is,

46:44.640 --> 46:48.400
 you know, one of the ways to reach a very broad swath of AI problems.

46:48.400 --> 46:55.680
 But what do you use the fundamental difference between reinforcement learning and maybe supervised

46:55.680 --> 47:01.520
 machine learning? So reinforcement learning can be viewed as a generalization of supervised

47:01.520 --> 47:05.520
 machine learning. You can certainly cast supervised learning as a reinforcement learning problem.

47:05.520 --> 47:09.680
 You can just say your loss function is the negative of your reward. But you have stronger

47:09.680 --> 47:13.440
 assumptions. You have the assumption that someone actually told you what the correct answer was,

47:13.440 --> 47:18.960
 that your data was IID and so on. So you could view reinforcement learning essentially relaxing

47:18.960 --> 47:22.000
 some of those assumptions. Now, that's not always a very productive way to look at it,

47:22.000 --> 47:25.120
 because if you actually have a supervised learning problem, you'll probably solve it

47:25.120 --> 47:29.520
 much more effectively by using supervised learning methods, because it's easier. But

47:30.240 --> 47:32.320
 you can view reinforcement learning as a generalization of that.

47:32.320 --> 47:37.040
 No, for sure. But they're fundamentally different. That's a mathematical statement.

47:37.040 --> 47:42.240
 That's absolutely correct. But it seems that reinforcement learning, the kind of tools

47:42.240 --> 47:47.440
 we're bringing to the table today, after today, so maybe down the line, everything will be a

47:47.440 --> 47:52.560
 reinforcement learning problem. Just like you said, image classification should be mapped to a

47:52.560 --> 47:57.920
 reinforcement learning problem. But today, the tools and ideas, the way we think about them are

47:57.920 --> 48:04.640
 different. Sort of supervised learning has been used very effectively to solve basic,

48:04.640 --> 48:13.200
 narrow AI problems. Reinforcement learning kind of represents the dream of AI. It's very much so

48:13.200 --> 48:18.640
 in the research space now, in sort of captivating the imagination of people of what we can do with

48:18.640 --> 48:24.480
 intelligent systems. But it hasn't yet had as wide of an impact as the supervised learning

48:24.480 --> 48:30.480
 approaches. So my question comes in a more practical sense. What do you see as the

48:30.480 --> 48:37.200
 gap between the more general reinforcement learning and the very specific, yes,

48:37.200 --> 48:42.400
 it's a question of decision making with one step in the sequence of the supervised learning?

48:42.960 --> 48:49.280
 So from a practical standpoint, I think that one thing that is potentially a little tough now,

48:49.280 --> 48:53.040
 and this is, I think, something that we'll see, this is a gap that we might see closing over

48:53.040 --> 48:57.680
 the next couple of years, is the ability of reinforcement learning algorithms to effectively

48:57.680 --> 49:03.280
 utilize large amounts of prior data. So one of the reasons why it's a bit difficult today

49:03.280 --> 49:07.040
 to use reinforcement learning for all the things that we might want to use it for,

49:07.040 --> 49:11.200
 is that in most of the settings where we want to do rational decision making,

49:12.000 --> 49:17.520
 it's a little bit tough to just deploy some policy that does crazy stuff and learns purely

49:17.520 --> 49:22.480
 through trial and error. It's much easier to collect a lot of data, a lot of logs of some

49:22.480 --> 49:27.520
 other policy that you've got, and then maybe you, you know, if you can get a good policy out of that,

49:27.520 --> 49:31.760
 then you deploy it and let it kind of fine tune a little bit. But algorithmically,

49:31.760 --> 49:36.800
 it's quite difficult to do that. So I think that once we figure out how to get reinforcement learning

49:36.800 --> 49:43.360
 to bootstrap effectively from large datasets, then we'll see very, very rapid growth in

49:43.360 --> 49:46.880
 applications of these technologies. So this is what's referred to as off policy reinforcement

49:46.880 --> 49:52.320
 learning or offline RL or batch RL. And I think we're seeing a lot of research right now that

49:52.320 --> 49:57.040
 does bring us closer and closer to that. Can you maybe paint the picture of the different methods,

49:57.040 --> 50:02.640
 as you said, off policy, what's value based, reinforcement learning, what's policy based,

50:02.640 --> 50:06.880
 what's model based, what's off policy on policy, what are the different categories of reinforcement

50:06.880 --> 50:13.360
 learning? Yeah. So one way we can think about reinforcement learning is that it's in some

50:13.360 --> 50:20.240
 very fundamental way. It's about learning models that can answer kind of what if questions. So

50:20.240 --> 50:25.040
 what would happen if I take this action that I hadn't taken before? And you do that, of course,

50:25.040 --> 50:29.840
 from experience, from data. And oftentimes you do it in a loop. So you build a model that answers

50:29.840 --> 50:34.400
 these what if questions, use it to figure out the best action you can take, and then go and try

50:34.400 --> 50:40.560
 taking that and see if the outcome agrees with what you predicted. So the different kinds of

50:40.560 --> 50:45.440
 techniques basically refer to different ways of doing it. So model based methods answer a question

50:45.440 --> 50:50.080
 of what state you would get, basically, what would happen to the world if you were to take a

50:50.080 --> 50:54.880
 certain action value based methods, they answer the question of what value you would get, meaning

50:54.880 --> 50:59.520
 what utility you would get. But in a sense, they're not really all that different, because

50:59.520 --> 51:05.040
 they're both really just answering these what if questions. Now, unfortunately, for us, with

51:05.040 --> 51:08.640
 current machine learning methods, answering what if questions can be really hard, because

51:08.640 --> 51:13.040
 they are really questions about things that didn't happen. If you wanted to answer what if questions

51:13.040 --> 51:15.920
 about things that did happen, you wouldn't need to learn model, you would just like repeat the

51:15.920 --> 51:23.680
 thing that worked before. And that's really a big part of why RL is a little bit tough. So if you

51:23.680 --> 51:29.360
 have a purely on policy kind of online process, then you ask these what if questions, you make

51:29.360 --> 51:33.840
 some mistakes, then you go and try doing those mistaken things. And then you observe kind of

51:33.840 --> 51:37.920
 the counter examples that will teach you not to do those things again. If you have a bunch of

51:37.920 --> 51:43.200
 off policy data, and you just want to synthesize the best policy you can out of that data, then

51:43.200 --> 51:47.200
 you really have to deal with the challenges of making these counterfactual.

51:47.200 --> 51:48.640
 First of all, what's a policy?

51:49.760 --> 51:58.960
 A policy is a model or some kind of function that maps from observations of the world to actions.

51:58.960 --> 52:04.960
 So in reinforcement learning, we often refer to the current configuration of the world as the

52:04.960 --> 52:09.520
 state. So we say the state kind of encompasses everything you need to fully define where the

52:09.520 --> 52:14.000
 world is at at the moment. And depending on how we formulate the problem, we might say you either

52:14.000 --> 52:18.880
 get to see the state or you get to see an observation, which is some snapshots and piece of the state.

52:19.520 --> 52:25.440
 So policy is just includes everything in it in order to be able to act in this world.

52:25.440 --> 52:26.000
 Yes.

52:26.000 --> 52:28.480
 And so what does off policy mean?

52:29.440 --> 52:32.720
 Yeah, so the terms on policy and off policy refer to how you get your data.

52:32.720 --> 52:37.280
 So if you get your data from somebody else who was doing some other stuff, maybe you get your data

52:37.280 --> 52:43.680
 from some manually programmed system that was just running in the world before,

52:43.680 --> 52:48.480
 that's referred to as off policy data. But if you got the data by actually acting in the world based

52:48.480 --> 52:53.200
 on what your current policy thinks is good, we call that on policy data. And obviously,

52:53.200 --> 52:58.560
 on policy data is more useful to you because if your current policy makes some bad decisions,

52:58.560 --> 53:02.800
 you will actually see that those decisions are bad. Off policy data, however, might be much easier

53:02.800 --> 53:06.560
 to obtain because maybe that's all the log data that you have from before.

53:07.840 --> 53:13.520
 So we talked about offline, talked about autonomous vehicles, so you can envision

53:13.520 --> 53:18.880
 off policy kind of approaches in robotic spaces where there's already a ton of robots out there,

53:18.880 --> 53:25.440
 but they don't get the luxury of being able to explore based on a reinforced learning framework.

53:25.440 --> 53:31.440
 So how do we make, again, open question, but how do we make off policy methods work?

53:32.240 --> 53:37.520
 Yeah, so this is something that has been kind of a big open problem for a while. And in the last

53:37.520 --> 53:43.120
 few years, people have made a little bit of progress on that. I can tell you about, and it's

53:43.120 --> 53:46.720
 not by any means solved yet, but I can tell you some of the things that, for example, we've done to

53:46.720 --> 53:52.560
 try to address some of the challenges. It turns out that one really big challenge with off policy

53:52.560 --> 53:58.640
 reinforcement learning is that you can't really trust your models to give accurate predictions

53:58.640 --> 54:05.120
 for any possible action. So if I've never tried to, if in my data set I never saw somebody steering

54:05.120 --> 54:10.800
 the car off the road onto the sidewalk, my value function or my model is probably not going to

54:10.800 --> 54:14.480
 predict the right thing if I ask what would happen if I were to steer the car off the road onto the

54:14.480 --> 54:20.560
 sidewalk. So one of the important things you have to do to get off policy RL to work is you have

54:20.560 --> 54:25.120
 to be able to figure out whether a given action will result in a trustworthy prediction or not.

54:25.120 --> 54:31.120
 And you can use kind of distribution estimation methods, kind of density estimation methods

54:31.120 --> 54:34.640
 to try to figure that out. So you could figure out that, well, this action, my model is telling me

54:34.640 --> 54:38.400
 that it's great, but it looks totally different from any action I've taken before. So my model is

54:38.400 --> 54:44.080
 probably not correct. And you can incorporate regularization terms into your learning objective

54:44.080 --> 54:50.720
 that will essentially tell you not to ask those questions that your model is unable to answer.

54:50.720 --> 54:56.640
 What would lead to breakthroughs in this space, do you think? Like what's needed? Is this a data set

54:56.640 --> 55:02.640
 question? Do we need to collect big benchmark data sets that allows to explore the space?

55:03.600 --> 55:11.520
 Is it a new kinds of methodologies? Like what's your sense? Or maybe coming together in a space

55:11.520 --> 55:16.400
 of robotics and defining the right problem to be working on? I think for off policy reinforcement

55:16.400 --> 55:21.040
 learning in particular, it's very much an algorithms question right now. And this is something that

55:21.760 --> 55:26.960
 I think is great because an algorithms question is that that just takes some very smart people to

55:26.960 --> 55:32.400
 get together and think about it really hard. Whereas if it was like a data problem or hardware

55:32.400 --> 55:36.640
 problem, that would take some serious engineering. So that's why I'm pretty excited about that

55:36.640 --> 55:40.160
 problem because I think that we're in a position where we can make some real progress on it just

55:40.160 --> 55:45.520
 by coming up with the right algorithms in terms of which algorithms they could be. The problems at

55:45.520 --> 55:52.400
 their core are very related to problems in things like causal inference because what you're really

55:52.400 --> 55:57.760
 dealing with is situations where you have a model, a statistical model that's trying to make predictions

55:57.760 --> 56:03.120
 about things that it hadn't seen before. And if it's a model that's generalizing properly,

56:03.120 --> 56:07.200
 that'll make good predictions. If it's a model that picks up on various correlations that will

56:07.200 --> 56:11.600
 not generalize properly, and then you have an arsenal of tools you can use, you could for example

56:11.600 --> 56:15.840
 figure out what are the regions where it's trustworthy, or on the other hand, you could try

56:15.840 --> 56:21.520
 to make it generalize better somehow or some combination of the two. Is there a room for

56:22.720 --> 56:31.040
 mixing where most of it, like 90, 95% is off policy, you already have the data set,

56:31.040 --> 56:37.120
 and then you get to send the robot out to do a little exploration? What's that role of

56:37.120 --> 56:41.280
 mixing them together? Yeah, absolutely. I think that this is something that you actually

56:42.960 --> 56:47.200
 described very well at the beginning of our discussion when you talked about the iceberg.

56:47.200 --> 56:52.000
 This is the iceberg, that the 99% of your prior experience, that's your iceberg. You'd use that

56:52.000 --> 56:57.920
 for off policy reinforcement learning. And then of course, if you've never opened that particular

56:57.920 --> 57:01.760
 kind of door with that particular lock before, then you have to go out and fiddle with it a little

57:01.760 --> 57:05.840
 bit. And that's that additional 1% to help you figure out a new task. And I think that's actually

57:05.840 --> 57:11.920
 like a pretty good recipe going forward. Is this to you the most exciting space of reinforcement

57:11.920 --> 57:18.160
 learning now? Or is there, what's, maybe taking a step back, not just now, but what's to use the

57:18.160 --> 57:24.320
 most beautiful idea? Apologize for the romanticized question, but the beautiful idea or concept in

57:24.320 --> 57:31.680
 reinforcement learning? In general, I actually think that one of the things that is a very beautiful

57:31.680 --> 57:40.080
 idea in reinforcement learning is just the idea that you can obtain a near optimal control or

57:40.080 --> 57:48.720
 near optimal policy without actually having a complete model of the world. It's something that

57:48.720 --> 57:54.400
 feels perhaps kind of obvious if you just hear the term reinforcement learning or you think about

57:54.400 --> 57:58.720
 trial and error learning. But from a control's perspective, it's a very weird thing because

57:58.720 --> 58:07.040
 classically, we think about engineered systems and controlling engineered systems as the problem

58:07.040 --> 58:11.200
 of writing down some equations and then figuring out, given these equations, basically like solve

58:11.200 --> 58:18.560
 for x, figure out the thing that maximizes its performance. And the theory of reinforcement

58:18.560 --> 58:22.720
 learning actually gives us a mathematically principled framework to think, to reason about

58:22.720 --> 58:28.640
 optimizing some quantity when you don't actually know the equations that govern that system.

58:28.640 --> 58:35.200
 And to me, that actually seems kind of very elegant, not something that

58:36.880 --> 58:39.920
 becomes immediately obvious, at least in the mathematical sense.

58:39.920 --> 58:42.080
 Does it make sense to you that it works at all?

58:43.520 --> 58:48.000
 Well, I think it makes sense when you take some time to think about it, but it is a little

58:48.000 --> 58:55.840
 surprising. Well, then taking a step into the more deeper representations, which is also very

58:55.840 --> 59:03.520
 surprising of sort of the richness of the state space, the space of environments that

59:04.160 --> 59:09.920
 this kind of approach can operate in. Can you maybe say what is deep reinforcement learning?

59:10.880 --> 59:16.160
 Well, deep reinforcement learning simply refers to taking reinforcement learning algorithms and

59:16.160 --> 59:22.880
 combining them with high capacity neural net representations, which might at first seem like

59:22.880 --> 59:26.640
 a pretty arbitrary thing, just take these two components and stick them together. But the

59:26.640 --> 59:33.040
 reason that it's something that has become so important in recent years is that reinforcement

59:33.040 --> 59:39.200
 learning, it kind of faces an exacerbated version of a problem that has faced many other machine

59:39.200 --> 59:46.000
 learning techniques. So if we go back to the early 2000s or the late 90s, we'll see a lot

59:46.000 --> 59:51.280
 of research on machine learning methods that have some very appealing mathematical properties,

59:51.280 --> 59:56.160
 like they reduce the convex optimization problems, for instance. But they require very

59:56.160 --> 1:00:01.680
 special inputs. They require a representation of the input that is clean in some way, like for

1:00:01.680 --> 1:00:06.480
 example, clean in the sense that the classes in your multi class classification problems

1:00:06.480 --> 1:00:10.640
 separate linearly. So they have some kind of good representation, and we call this a feature

1:00:10.640 --> 1:00:15.440
 representation. And for a long time, people were very worried about features in the world of supervised

1:00:15.440 --> 1:00:19.280
 learning, because somebody had to actually build those features, so you couldn't just take an image

1:00:19.280 --> 1:00:23.600
 and plug it into your logistic regression or your SVM or something, someone had to take that image

1:00:23.600 --> 1:00:28.240
 and process it using some handwritten code. And then neural nets came along and they could

1:00:28.240 --> 1:00:33.280
 actually learn the features. And suddenly, we could apply learning directly to the raw inputs,

1:00:33.280 --> 1:00:37.920
 which was great for images, but it was even more great for all the other fields where people hadn't

1:00:37.920 --> 1:00:41.680
 come up with good features yet. And one of those fields actually reinforcement learning,

1:00:41.680 --> 1:00:45.360
 because in reinforcement learning, the notion of features, if you don't use neural nets and you

1:00:45.360 --> 1:00:51.920
 have to design your own features, is very opaque. It's very hard to imagine, let's say I'm playing

1:00:51.920 --> 1:00:58.320
 chess or Go. What is a feature with which I can represent the value function for Go or even the

1:00:58.320 --> 1:01:03.200
 optimal policy for Go linearly? I don't even know how to start thinking about it. And people

1:01:03.200 --> 1:01:07.520
 tried all sorts of things that would write down an expert chess player looks for whether the

1:01:07.520 --> 1:01:11.040
 knight is in the middle of the board or not. So that's a feature is knight in middle of board.

1:01:11.040 --> 1:01:16.160
 And they would write these like long lists of kind of arbitrary made up stuff. And that was

1:01:16.160 --> 1:01:20.400
 really kind of getting us nowhere. And that's a little chess is a little more accessible than

1:01:20.400 --> 1:01:25.840
 the robotics problem. Absolutely. Right. That's there's at least experts in the in the different

1:01:25.840 --> 1:01:34.000
 features for chess. But still like the neural network there, to me, that's, I mean, you put it

1:01:34.000 --> 1:01:39.760
 eloquently and almost made it seem like a natural step to add neural networks. But the fact that

1:01:39.760 --> 1:01:45.680
 neural networks are able to discover features in the control problem. It's very interesting. It's

1:01:45.680 --> 1:01:51.920
 hopeful. I'm not sure what to think about it, but it feels hopeful that the control problem has

1:01:51.920 --> 1:02:01.440
 features to be learned. Like, I guess my question is, is it surprising to you how far the deep side

1:02:01.440 --> 1:02:05.920
 of deeper enforcement learning was able to like what the space of promise has been able to tackle

1:02:05.920 --> 1:02:16.080
 from especially in games with the Alpha star and and Alpha zero and just the representation power

1:02:16.080 --> 1:02:23.200
 there and in the robotics space. And what is your sense of the limits of this representation power

1:02:23.200 --> 1:02:32.320
 and the control context? I think that in regard to the limits that here, I think that one thing

1:02:32.320 --> 1:02:39.120
 that makes it a little hard to fully answer this question is because in settings where we would

1:02:39.120 --> 1:02:45.840
 like to push push these things to the limit, we encounter other bottlenecks. So like the reason

1:02:45.840 --> 1:02:53.600
 that I can't get my robot to learn how to like, I don't know, do the dishes in the kitchen. It's

1:02:53.600 --> 1:02:59.600
 not because it's neural net is not big enough. It's because when you try to actually do trial

1:02:59.600 --> 1:03:04.640
 and error learning, reinforcement learning directly in the real world, where you have the

1:03:04.640 --> 1:03:09.760
 potential to gather these large, very, you know, highly varied and complex data sets,

1:03:09.760 --> 1:03:15.200
 you start running into other problems, like one problem you run into very quickly. It's it'll

1:03:15.200 --> 1:03:18.400
 first sound like a very pragmatic problem, but it actually turns out to be a pretty deep scientific

1:03:18.400 --> 1:03:22.400
 problem. Take the robot put in your kitchen, have it try to learn to do the dishes with trial and

1:03:22.400 --> 1:03:27.600
 error, it'll break all your dishes, and then we'll have no more dishes to clean. Now you might think

1:03:27.600 --> 1:03:30.960
 this is a very practical issue, but there's something to this, which is that if you have a

1:03:30.960 --> 1:03:34.400
 person trying to do this, you know, a person will have some degree of common sense, they'll

1:03:34.400 --> 1:03:37.920
 break one dish, they'll be a little more careful with the next one. And if they break all of them,

1:03:37.920 --> 1:03:42.880
 they're going to go and get more or something like that. So there's all sorts of scaffolding

1:03:42.880 --> 1:03:48.240
 that that comes very naturally to us for our learning process. Like, you know, if I have to

1:03:48.240 --> 1:03:52.000
 learn something through trial and error, I have a common sense to know that I have to, you know,

1:03:52.000 --> 1:03:56.320
 try multiple times. If I screw something up, I ask for help or I reset things or something like that.

1:03:56.320 --> 1:04:01.040
 And all of that is kind of outside of the classic reinforcement learning problem formulation.

1:04:01.840 --> 1:04:06.320
 There are other things that are that can also be categorized as kind of scaffolding,

1:04:06.320 --> 1:04:09.760
 but are very important, like for example, where you do get your reward function. If I want to

1:04:09.760 --> 1:04:15.840
 learn how to pour a cup of water, well, how do I know if I've done it correctly? Now that probably

1:04:15.840 --> 1:04:19.840
 requires an entire computer vision system to be built just to determine that. And that seems a

1:04:19.840 --> 1:04:24.000
 little bit inelegant. So there are all sorts of things like this that start to come up when we

1:04:24.000 --> 1:04:28.160
 think through what we really need to get reinforcement learning to happen at scale in the real world.

1:04:28.160 --> 1:04:32.320
 And many of these things actually suggest a little bit of a shortcoming in the problem

1:04:32.320 --> 1:04:37.360
 formulation and a few deeper questions that we have to resolve. That's really interesting. I

1:04:37.360 --> 1:04:45.120
 talked to like David Silver, about AlphaZero. And it seems like there's no, again, that we

1:04:45.120 --> 1:04:52.000
 haven't hit the limit at all in the context when there's no broken dishes. So in the case of Go,

1:04:52.000 --> 1:04:58.720
 you can, it's really about just scaling compute. So again, like the bottleneck is the amount of

1:04:58.720 --> 1:05:04.320
 money you're willing to invest in compute, and then maybe the different, the scaffolding around

1:05:04.320 --> 1:05:09.840
 how difficult it is to scale compute, maybe. But there there's no limit. And it's interesting.

1:05:09.840 --> 1:05:14.160
 Now we move to the real world, and there's the broken dishes, there's all the, and the reward

1:05:14.160 --> 1:05:20.160
 function like you mentioned, that's really nice. So what, how do we push forward there? Do you think

1:05:20.160 --> 1:05:26.560
 there's, there's this kind of sample efficiency question that people bring up, you know, not

1:05:26.560 --> 1:05:34.800
 having to break 100,000 dishes? Is this an algorithm question? Is this a data selection

1:05:34.800 --> 1:05:39.920
 like question? What do you think? How do we, how do we not break too many dishes?

1:05:39.920 --> 1:05:49.280
 Yeah. Well, one way we can think about that is that maybe we need to be better at

1:05:49.280 --> 1:05:55.760
 at reusing our data, building that, that iceberg. So perhaps, perhaps it's too much to hope that

1:05:56.800 --> 1:06:03.440
 you can have a machine that in isolation in the vacuum without anything else can just master

1:06:03.440 --> 1:06:08.240
 complex tasks in like, in minutes, the way that people do. But perhaps it also doesn't have to,

1:06:08.240 --> 1:06:13.600
 perhaps what it really needs to do is have an existence, a lifetime where it does many things

1:06:13.600 --> 1:06:18.160
 and the previous things that it has done, prepare it to do new things more efficiently.

1:06:18.160 --> 1:06:22.640
 And, you know, the study of these kinds of questions typically falls under categories

1:06:22.640 --> 1:06:27.360
 like multitask learning or meta learning. But they all fundamentally deal with the same

1:06:27.360 --> 1:06:34.000
 general theme, which is use experience for doing other things to learn to do new things

1:06:34.000 --> 1:06:38.880
 efficiently and quickly. So what do you think about if you just look at the one particular

1:06:38.880 --> 1:06:44.960
 case study of Tesla autopilot that has quickly approaching towards a million vehicles on the

1:06:44.960 --> 1:06:51.440
 road, where some percentage of the time 30, 40% of the time is driving using the computer vision,

1:06:51.440 --> 1:06:59.680
 multitask, hydranet, right? And then the other percent, that's what they call it hydranet.

1:07:00.960 --> 1:07:08.480
 The other percent is human controlled. From the human side, how can we use that data? What's

1:07:08.480 --> 1:07:14.480
 your sense? So like, what's the signal? Do you have ideas in this autonomous vehicle space

1:07:14.480 --> 1:07:20.880
 when people can lose their lives? You know, it's a safety critical environment. So how do we use

1:07:20.880 --> 1:07:30.320
 that data? So I think that actually the kind of problems that come up when we want systems that

1:07:30.320 --> 1:07:35.680
 are reliable and that can kind of understand the limits of their capabilities, they're actually

1:07:35.680 --> 1:07:39.360
 very similar to the kind of problems that come up when we're doing off policy reinforcement

1:07:39.360 --> 1:07:44.080
 learning. So as I mentioned before, in off policy reinforcement learning, the big problem is you

1:07:44.080 --> 1:07:49.680
 need to know when you can trust the predictions of your model, because if you're trying to evaluate

1:07:49.680 --> 1:07:53.120
 some pattern of behavior for which your model doesn't give you an accurate prediction, then you

1:07:53.120 --> 1:07:57.520
 shouldn't use that to modify your policy. It's actually very similar to the problem that we're

1:07:57.520 --> 1:08:02.240
 faced when we actually then deploy that thing. And we want to decide whether we trust it in the

1:08:02.240 --> 1:08:08.000
 moment or not. So perhaps we just need to do a better job of figuring out that part. And that's

1:08:08.000 --> 1:08:11.120
 a very deep research question, of course. But it's also a question that a lot of people are

1:08:11.120 --> 1:08:14.560
 working on. So I'm pretty optimistic that we can make some progress on that over the next few years.

1:08:15.600 --> 1:08:19.760
 What's the role of simulation in reinforcement learning, deeper reinforcement learning,

1:08:19.760 --> 1:08:25.920
 reinforcement learning? Like how essential is it? It's been essential for the breakthroughs so far,

1:08:26.480 --> 1:08:32.080
 for some interesting breakthroughs. Do you think it's a crutch that we rely on? I mean,

1:08:32.080 --> 1:08:37.440
 again, it's the connection to our off policy discussion. But do you think we can ever get rid

1:08:37.440 --> 1:08:40.800
 of simulation? Or do you think simulation will actually take over? Will create more and more

1:08:40.800 --> 1:08:47.120
 realistic simulations that will allow us to solve actual real world problems, like transfer the models

1:08:47.120 --> 1:08:52.480
 we'll learn in simulation to real world problems? I think that simulation is a very pragmatic tool

1:08:52.480 --> 1:08:57.520
 that we can use to get a lot of useful stuff to work right now. But I think that in the long run,

1:08:57.520 --> 1:09:02.640
 we will need to build machines that can learn from real data, because that's the only way that we'll

1:09:02.640 --> 1:09:08.000
 get them to improve perpetually. Because if we can't have our machines learn from real data,

1:09:08.000 --> 1:09:11.600
 if they have to rely on simulated data, eventually the simulator becomes the bottleneck.

1:09:12.240 --> 1:09:17.760
 In fact, this is a general thing. If your machine has any bottleneck that is built by humans,

1:09:17.760 --> 1:09:21.680
 and that doesn't improve from data, it will eventually be the thing that holds it back.

1:09:22.960 --> 1:09:26.720
 And if you're entirely reliant on your simulator, that'll be the bottleneck. If you're entirely

1:09:26.720 --> 1:09:31.680
 reliant on a manually designed controller, that's going to be the bottleneck. So simulation is very

1:09:31.680 --> 1:09:38.240
 useful. It's very pragmatic. But it's not a substitute for being able to utilize real experience.

1:09:39.600 --> 1:09:44.080
 And this is, by the way, this is something that I think is quite relevant now, especially in the

1:09:44.080 --> 1:09:48.640
 context of some of the things we've discussed, because some of these kind of scaffolding issues

1:09:48.640 --> 1:09:52.080
 that I mentioned, things like the broken dishes and the unknown reward functions, like these are

1:09:52.080 --> 1:09:58.480
 not problems that you would ever stumble on when working in a purely simulated kind of environment.

1:09:58.480 --> 1:10:02.240
 But they become very apparent when we try to actually run these things in the real world.

1:10:03.040 --> 1:10:07.120
 Do you throw a brief wrench into our discussion? Let me ask, do you think we're living in a simulation?

1:10:07.920 --> 1:10:08.880
 Oh, I have no idea.

1:10:09.760 --> 1:10:16.800
 Do you think that's a useful thing to even think about the fundamental physics nature of reality?

1:10:16.800 --> 1:10:23.520
 Or another perspective? The reason I think the simulation hypothesis is interesting is

1:10:24.400 --> 1:10:32.240
 to think about how difficult is it to create sort of a virtual reality game type situation

1:10:32.800 --> 1:10:38.880
 that will be sufficiently convincing to us humans or sufficiently enjoyable that we wouldn't want

1:10:38.880 --> 1:10:45.440
 to leave. That's actually a practical engineering challenge. And I personally really enjoy virtual

1:10:45.440 --> 1:10:50.480
 reality, but it's quite far away. But I kind of think about, what would it take for me to want

1:10:50.480 --> 1:10:57.600
 to spend more time in virtual reality versus the real world? And that's a, that's a sort of a nice

1:10:57.600 --> 1:11:04.640
 clean question. Because at that point, we've reached, if I want to live in a virtual reality,

1:11:04.640 --> 1:11:08.640
 that means we're just a few years away, we're a majority, the population lives in a virtual

1:11:08.640 --> 1:11:12.800
 reality. And that's how we create the simulation, right? You don't need to actually simulate the

1:11:12.800 --> 1:11:20.400
 the quantum gravity and just every aspect of the of the universe. And that's a really,

1:11:20.400 --> 1:11:24.800
 that's an interesting question for reinforcement learning too, is if we want to make sufficiently

1:11:24.800 --> 1:11:30.240
 realistic simulations that may, it blend the difference between sort of the real world and

1:11:30.240 --> 1:11:36.800
 the simulation, thereby, just some of the things we've been talking about, kind of the problems

1:11:36.800 --> 1:11:41.520
 go away, if we can create actually interesting, rich simulations. It's an interesting question.

1:11:41.520 --> 1:11:46.720
 And it actually, I think your question casts your previous question in a very interesting light,

1:11:46.720 --> 1:11:53.920
 because in some ways, asking whether we can, well, the more practical version is like,

1:11:53.920 --> 1:11:59.680
 can we build simulators that are good enough to train essentially AI systems that will work

1:11:59.680 --> 1:12:06.160
 in the world? And it's kind of interesting to think about this, about what this implies. If true,

1:12:06.160 --> 1:12:09.600
 it kind of implies that it's easier to create the universe than it is to create a brain.

1:12:09.600 --> 1:12:13.520
 And that seems like put this way, it seems kind of weird.

1:12:14.320 --> 1:12:19.600
 The aspect of the simulation most interesting to me is the simulation of other humans.

1:12:20.800 --> 1:12:27.920
 That seems to be a complexity that makes the robotics problem harder. Now,

1:12:27.920 --> 1:12:33.440
 I don't know if every robotics person agrees with that notion. Just as a quick aside,

1:12:33.440 --> 1:12:39.840
 what are your thoughts about when the human enters the picture of the robotics problem? How

1:12:39.840 --> 1:12:44.000
 does that change the reinforcement learning problem, the learning problem in general?

1:12:44.880 --> 1:12:52.880
 Yeah, I think that's a kind of a complex question. And I guess my hope for a while had been that

1:12:53.520 --> 1:13:00.880
 if we build these robotic learning systems that are multitask, that utilize lots of prior data,

1:13:00.880 --> 1:13:05.440
 and that learn from their own experience, the bit where they have to interact with people

1:13:05.440 --> 1:13:09.440
 will be perhaps handled in much the same way as all the other bits. So if they have prior

1:13:09.440 --> 1:13:13.440
 experience of attracting with people and they can learn from their own experience of attracting

1:13:13.440 --> 1:13:19.120
 with people for this new task, maybe that'll be enough. Now, of course, if it's not enough,

1:13:19.120 --> 1:13:22.560
 there are many other things we can do. And there's quite a bit of research in that area.

1:13:22.560 --> 1:13:29.920
 But I think it's worth a shot to see whether the multi agent interaction, the ability to understand

1:13:29.920 --> 1:13:34.960
 that other beings in the world have their own goals and intentions and thoughts and so on,

1:13:34.960 --> 1:13:41.360
 whether that kind of understanding can emerge automatically from simply learning to do things

1:13:41.360 --> 1:13:46.960
 with and maximize utility. That information arises from the data. You've said something

1:13:46.960 --> 1:13:53.040
 about gravity, sort of that you don't need to explicitly inject anything into the system,

1:13:53.040 --> 1:13:57.360
 they can be learned from the data. And gravity is an example of something that could be learned

1:13:57.360 --> 1:14:06.800
 from data, sort of like the physics of the world. What are the limits of what we can learn from

1:14:06.800 --> 1:14:15.280
 data? So a very simple, clean way to ask that is, do you really think we can learn gravity

1:14:15.280 --> 1:14:23.040
 from just data, the idea, the laws of gravity? So something that I think is a common kind of

1:14:23.040 --> 1:14:30.800
 pitfall when thinking about prior knowledge and learning is to assume that just because we know

1:14:30.800 --> 1:14:35.600
 something, then that it's better to tell the machine about that rather than have it figured out

1:14:35.600 --> 1:14:43.440
 and so on. In many cases, things that are important, that affect many of the events

1:14:43.440 --> 1:14:48.960
 that the machine will experience are actually pretty easy to learn. If every time you drop

1:14:48.960 --> 1:14:55.120
 something, it falls down. Yeah, you might get the Newton's version, not Einstein's version,

1:14:55.840 --> 1:15:00.000
 but it'll be pretty good and it will probably be sufficient for you to act rationally in the world

1:15:00.720 --> 1:15:05.200
 because you see the phenomenon all the time. So things that are readily apparent from the data,

1:15:06.000 --> 1:15:09.120
 we might not need to specify those by hand. It might actually be easier to let the machine

1:15:09.120 --> 1:15:13.360
 figure them out. It just feels like that there might be a space of many local

1:15:13.360 --> 1:15:20.240
 minima in terms of theories of this world that we would discover and get stuck on.

1:15:20.240 --> 1:15:21.120
 Yeah, of course.

1:15:21.120 --> 1:15:26.800
 That Newtonian mechanics is not necessarily easy to come by.

1:15:27.520 --> 1:15:32.480
 Yeah, and well, in fact, in some fields of science, for example, human civilizations

1:15:32.480 --> 1:15:36.880
 that sell full of these local optimism. So for example, if you think about how people

1:15:36.880 --> 1:15:43.600
 try to figure out biology and medicine for the longest time, the kind of rules, the kind of

1:15:44.080 --> 1:15:47.760
 principles that serve us very well in our day to day lives actually serve us very poorly

1:15:47.760 --> 1:15:53.920
 in understanding medicine and biology. We had very superstitious and weird ideas about how

1:15:53.920 --> 1:15:59.120
 the body worked until the advent of the modern scientific method. So that does seem to be

1:15:59.920 --> 1:16:03.040
 a failing of this approach, but it's also a failing of human intelligence arguably.

1:16:03.040 --> 1:16:10.000
 Yeah, maybe a smaller side, but the idea of self play is fascinating in reinforcement learning,

1:16:10.000 --> 1:16:15.760
 sort of these competitive, creating a competitive context in which agents can play against each

1:16:15.760 --> 1:16:20.960
 other in sort of at the same skill level and thereby increasing each other skill level.

1:16:20.960 --> 1:16:26.320
 It seems to be this kind of self improving mechanism is exceptionally powerful in the

1:16:26.320 --> 1:16:32.720
 context where it could be applied. First of all, is that beautiful to you that this mechanism

1:16:32.720 --> 1:16:40.080
 work as well as it does and also can be generalized to other contexts like in the robotic space

1:16:40.880 --> 1:16:43.120
 or anything that's applicable to the real world?

1:16:43.760 --> 1:16:51.520
 I think that it's a very interesting idea, but I suspect that the bottleneck to actually

1:16:51.520 --> 1:16:54.720
 generalizing it to the robotic setting is actually going to be the same as

1:16:55.760 --> 1:17:01.040
 the bottleneck for everything else, that we need to be able to build machines that can get better

1:17:01.040 --> 1:17:06.480
 and better through natural interaction with the world. And once we can do that, then they can go

1:17:06.480 --> 1:17:11.520
 out and play with each other, they can play with people, they can play with the natural environment.

1:17:12.720 --> 1:17:16.160
 But before we get there, we've got all these other problems we have to get out of the way.

1:17:16.160 --> 1:17:19.840
 So there's no shortcut around that. You have to interact with the natural environment that...

1:17:20.880 --> 1:17:25.760
 Well, because in a self play setting, you still need a mediating mechanism. So the reason that

1:17:25.760 --> 1:17:31.200
 self play works for a board game is because the rules of that board game

1:17:31.200 --> 1:17:35.520
 mediate the interaction between the agents. So the kind of intelligent behavior that will

1:17:35.520 --> 1:17:38.800
 emerge depends very heavily on the nature of that mediating mechanism.

1:17:39.680 --> 1:17:44.480
 So on the side of reward functions, that's coming up with good reward function seems to

1:17:44.480 --> 1:17:51.360
 be the thing that we associate with general... like human beings seem to value the idea of

1:17:51.360 --> 1:17:59.440
 developing our own reward functions, of arriving at meaning and so on. And yet for reinforcement

1:17:59.440 --> 1:18:07.680
 learning, we often specify that's the given. What's your sense of how we develop good reward

1:18:07.680 --> 1:18:12.720
 functions? Yeah, I think that's a very complicated and very deep question. And you're completely

1:18:12.720 --> 1:18:19.200
 right that classically in reinforcement learning, this question has been treated as an on issue,

1:18:19.200 --> 1:18:25.760
 that you treat the reward as this external thing that comes from some other bit of your biology

1:18:26.320 --> 1:18:31.920
 and you don't worry about it. And I do think that that's actually a little bit of a mistake that

1:18:31.920 --> 1:18:35.760
 we should worry about it. And we can approach it in a few different ways. We can approach it,

1:18:35.760 --> 1:18:39.440
 for instance, by thinking of reward as a communication medium. We can say, well,

1:18:39.440 --> 1:18:45.120
 how does a person communicate to a robot what its objective is? You can approach it also as

1:18:45.120 --> 1:18:49.360
 sort of more of an intrinsic motivation medium. You could say, can we write down

1:18:50.320 --> 1:18:55.920
 kind of a general objective that leads to good capability? Like, for example, can you write

1:18:55.920 --> 1:19:00.080
 down some objectives such that even in the absence of any other task, if you maximize that objective,

1:19:00.080 --> 1:19:06.000
 you'll sort of learn useful things. This is something that has sometimes been called unsupervised

1:19:06.000 --> 1:19:10.640
 reinforcement learning, which I think is a really fascinating area of research, especially today.

1:19:11.360 --> 1:19:14.640
 We've done a bit of work on that recently. One of the things we've studied is whether

1:19:14.640 --> 1:19:20.960
 we can have some notion of unsupervised reinforcement learning by means of

1:19:22.000 --> 1:19:26.640
 information theoretic quantities, like, for instance, minimizing a Bayesian measure of surprise. This

1:19:26.640 --> 1:19:30.880
 is an idea that was pioneered actually in the computational neuroscience community by folks

1:19:30.880 --> 1:19:34.960
 like Carl Friston. And we've done some work recently that shows that you can actually learn

1:19:34.960 --> 1:19:41.600
 pretty interesting skills by essentially behaving in a way that allows you to make accurate predictions

1:19:41.600 --> 1:19:46.000
 about the world. It seems a little circular. Do the things that will lead to you getting the right

1:19:46.000 --> 1:19:52.880
 answer for prediction. But by doing this, you can sort of discover stable niches in the world.

1:19:52.880 --> 1:19:58.240
 You can discover that if you're playing Tetris, then correctly clearing the rows will let you

1:19:58.240 --> 1:20:02.320
 play Tetris for longer and keep the board nice and clean, which sort of satisfies some desire

1:20:02.320 --> 1:20:06.560
 for order in the world. And as a result, get some degree of leverage over your domain.

1:20:06.560 --> 1:20:12.560
 So we're exploring that pretty actively. Is there a role for a human notion of curiosity

1:20:12.560 --> 1:20:18.080
 in itself being the reward sort of discovering new things about the world?

1:20:19.600 --> 1:20:22.080
 So one of the things that I'm pretty interested in is actually whether

1:20:23.040 --> 1:20:28.640
 discovering new things can actually be an emergent property of some other objective

1:20:28.640 --> 1:20:36.000
 that quantifies capability. So new things for the sake of new things, maybe might not by itself

1:20:36.000 --> 1:20:41.760
 be the right answer, but perhaps we can figure out an objective for which discovering new things

1:20:41.760 --> 1:20:45.680
 is actually the natural consequence. That's something we're working on right now,

1:20:45.680 --> 1:20:48.560
 but I don't have a clear answer for you there yet. That's still a work in progress.

1:20:49.360 --> 1:20:57.680
 You mean just as a curious observation to see sort of creative patterns of curiosity

1:20:57.680 --> 1:21:00.720
 on the way to optimize for a particular task?

1:21:00.720 --> 1:21:03.520
 On the way to optimize for a particular measure of capability.

1:21:03.520 --> 1:21:13.920
 Is there ways to understand or anticipate unexpected, unintended consequences of

1:21:14.480 --> 1:21:21.840
 particular reward functions? Sort of anticipate the kind of strategies that might be developed

1:21:21.840 --> 1:21:25.680
 and try to avoid highly detrimental strategies?

1:21:25.680 --> 1:21:30.240
 Yeah. So classically, this is something that has been pretty hard in reinforcement learning

1:21:30.240 --> 1:21:35.600
 because it's difficult for a designer to have good intuition about what a learning algorithm

1:21:35.600 --> 1:21:40.080
 will come up with when they give it some objective. There are ways to mitigate that.

1:21:40.080 --> 1:21:45.280
 One way to mitigate it is to actually define an objective that says, don't do weird stuff.

1:21:45.840 --> 1:21:50.480
 You can actually quantify it and say just don't enter situations that have low probability

1:21:51.200 --> 1:21:53.520
 under the distribution of states you've seen before.

1:21:54.480 --> 1:21:57.760
 It turns out that that's actually one very good way to do off policy reinforcement learning,

1:21:57.760 --> 1:22:00.880
 actually. So we can do some things like that.

1:22:02.320 --> 1:22:08.480
 If we slowly venture in speaking about reward functions into greater and greater levels of

1:22:08.480 --> 1:22:16.240
 intelligence, there's, I mean, Stuart Russell thinks about this, the alignment of AI systems

1:22:16.240 --> 1:22:22.800
 with us humans. So how do we ensure that AGI systems align with us humans?

1:22:22.800 --> 1:22:31.840
 It's kind of a reward function question of specifying the behavior of AI systems

1:22:31.840 --> 1:22:40.160
 such that their success aligns with the broader intended success interests of human beings.

1:22:40.160 --> 1:22:45.120
 Do you have thoughts on this? Do you have concerns of where reinforcement learning fits into this?

1:22:45.120 --> 1:22:50.080
 Or are you really focused on the current moment of us being quite far away and trying to solve

1:22:50.080 --> 1:22:56.720
 the robotics problem? I don't have a great answer to this. And I do think that this is a problem

1:22:56.720 --> 1:23:03.040
 that's important to figure out. For my part, I'm actually a bit more concerned about the other

1:23:03.040 --> 1:23:11.280
 side of this equation that maybe rather than unintended consequences for objectives that

1:23:11.280 --> 1:23:15.520
 are specified too well, I'm actually more worried right now about unintended consequences for

1:23:15.520 --> 1:23:21.120
 objectives that are not optimized well enough, which might become a very pressing problem

1:23:21.120 --> 1:23:25.440
 when we, for instance, try to use these techniques for safety critical systems like

1:23:26.000 --> 1:23:30.960
 cars and aircraft and so on. I think at some point we'll face the issue of objectives being

1:23:30.960 --> 1:23:35.760
 optimized too well, but right now I think we're more likely to face the issue of them not being

1:23:35.760 --> 1:23:39.840
 optimized well enough. But you don't think unintended consequences can arise even when

1:23:39.840 --> 1:23:44.640
 you're far from optimality, sort of like on the path to it? Oh, no, I think unintended

1:23:44.640 --> 1:23:50.320
 consequences can absolutely arise. It's just I think right now the bottleneck for improving

1:23:50.320 --> 1:23:56.400
 reliability, safety and things like that is more with systems that need to work better,

1:23:56.400 --> 1:24:01.760
 that need to optimize their objective better. Do you have thoughts, concerns about existential

1:24:01.760 --> 1:24:10.080
 threats of human level intelligence? If we put on our hat of looking in 10, 20, 100, 500 years from

1:24:10.080 --> 1:24:16.560
 now, do you have concerns about existential threats of AI systems? I think there are absolutely

1:24:16.560 --> 1:24:20.400
 existential threats for AI systems just like there are for any powerful technology.

1:24:22.240 --> 1:24:28.880
 But I think that these kinds of problems can take many forms and some of those forms will

1:24:29.440 --> 1:24:37.280
 come down to people with nefarious intent. Some of them will come down to AI systems that have

1:24:37.280 --> 1:24:42.400
 some fatal flaws and some of them will of course come down to AI systems that are too capable in

1:24:42.400 --> 1:24:50.320
 some way. But among this set of potential concerns, I would actually be much more concerned about the

1:24:50.320 --> 1:24:55.200
 first two right now and principally the one with nefarious humans because just through all

1:24:55.200 --> 1:24:58.080
 of human history actually it's the nefarious humans that have been the problem not the nefarious

1:24:58.080 --> 1:25:05.200
 machines than I am about the others. And I think that right now the best that I can do to make

1:25:05.200 --> 1:25:10.000
 sure things go well is to build the best technology I can and also hopefully promote

1:25:10.000 --> 1:25:17.680
 responsible use of that technology. Do you think RL systems has something to teach us humans?

1:25:18.800 --> 1:25:23.680
 You said nefarious humans getting us in trouble. I mean machine learning systems have in some ways

1:25:23.680 --> 1:25:30.560
 have revealed to us the ethical flaws in our data in that same kind of way. Can reinforcement learning

1:25:30.560 --> 1:25:37.200
 teach us about ourselves? Has it taught something? What have you learned about yourself from trying

1:25:37.200 --> 1:25:44.720
 to build robots and reinforcement learning systems? I'm not sure what I've learned about myself but

1:25:45.280 --> 1:25:53.200
 maybe part of the answer to your question might become a little bit more apparent once we see

1:25:53.200 --> 1:25:59.440
 more widespread deployment of reinforcement learning for decision making support in domains

1:25:59.440 --> 1:26:05.440
 like healthcare, education, social media, etc. And I think we will see some interesting stuff

1:26:05.440 --> 1:26:10.160
 emerge there. We will see for instance what kind of behaviors these systems come up with

1:26:11.280 --> 1:26:16.720
 in situations where there is interaction with humans and where they have possibility of

1:26:16.720 --> 1:26:21.520
 influencing human behavior. I think we're not quite there yet but maybe in the next two years

1:26:21.520 --> 1:26:25.920
 we'll see some interesting stuff come out in that area. I hope outside the research space because

1:26:25.920 --> 1:26:31.440
 the exciting space where this could be observed is sort of large companies that deal with large

1:26:31.440 --> 1:26:37.440
 data and I hope there's some transparency. One of the things that's unclear when I look at social

1:26:37.440 --> 1:26:44.960
 networks and just online is why an algorithm did something or whether even an algorithm was involved

1:26:44.960 --> 1:26:52.960
 and that'd be interesting as a from a research perspective just to observe the results of algorithms

1:26:52.960 --> 1:26:59.680
 to open up that data or to at least be sufficiently transparent about the behavior of these AS systems

1:26:59.680 --> 1:27:05.600
 in the real world. What's your sense? I don't know if you looked at the blog post bitter lesson

1:27:05.600 --> 1:27:14.800
 by Rich Sutton where it looks at sort of the big lesson of research in AI and reinforcement learning

1:27:14.800 --> 1:27:22.240
 is that simple methods, general methods that leverage computation seem to work well. So basically

1:27:22.240 --> 1:27:28.800
 don't try to do any kind of fancy algorithms just wait for computation to get fast. Do you share

1:27:28.800 --> 1:27:34.960
 this kind of intuition? I think the high level idea makes a lot of sense. I'm not sure that my

1:27:34.960 --> 1:27:39.920
 takeaway would be that we don't need to work on algorithms. I think that my takeaway would be that

1:27:39.920 --> 1:27:50.000
 we should work on general algorithms and actually I think that this idea of needing to better automate

1:27:50.000 --> 1:27:56.720
 the acquisition of experience in the real world actually follows pretty naturally from Rich

1:27:56.720 --> 1:28:05.040
 Sutton's conclusion. So if the claim is that automated general methods plus data leads to good

1:28:05.040 --> 1:28:08.880
 results, then it makes sense that we should build general methods and we should build the kind of

1:28:08.880 --> 1:28:12.720
 methods that we can deploy and get them to go out there and collect their experience autonomously.

1:28:13.760 --> 1:28:19.040
 I think that one place where I think that the current state of things falls a little bit short

1:28:19.040 --> 1:28:24.080
 of that is actually that the going out there and collecting the data autonomously, which is easy to

1:28:24.080 --> 1:28:28.640
 do in a simulated board game but very hard to do in the real world. Yeah, it keeps coming back to

1:28:28.640 --> 1:28:38.160
 this one problem, right? So your mind is focused there now in this real world. It just seems scary

1:28:38.160 --> 1:28:45.120
 the step of collecting the data and it seems unclear to me how we can do it effectively.

1:28:45.120 --> 1:28:49.600
 Well, you know, 7 billion people in the world, each of them had to do that at some point in

1:28:49.600 --> 1:28:55.600
 their lives. And we should leverage that experience that they've all done. We should be able to try

1:28:55.600 --> 1:29:05.920
 to collect that kind of data. Okay, big questions. Maybe stepping back to your life, wood book or

1:29:05.920 --> 1:29:13.920
 books, technical or fiction or philosophical had a big impact on the way you saw the world,

1:29:13.920 --> 1:29:20.800
 and the way you thought about in the world, your life in general. And maybe what books,

1:29:20.800 --> 1:29:25.440
 if it's different, would you recommend people consider reading on their own intellectual

1:29:25.440 --> 1:29:31.120
 journey? It could be within reinforcement learning, but it could be very much bigger.

1:29:32.160 --> 1:29:39.280
 I don't know if this is like a scientifically, like, particularly meaningful answer, but

1:29:39.280 --> 1:29:45.680
 like, the honest answer is that I actually found a lot of the work by Isaac Hasimov to be very

1:29:45.680 --> 1:29:49.920
 inspiring when I was younger. I don't know if that has anything to do with AI necessarily.

1:29:49.920 --> 1:29:52.400
 You don't think it had a ripple effect in your life?

1:29:52.400 --> 1:30:03.360
 Maybe it did. But yeah, I think that a vision of a future where, well, first of all,

1:30:03.360 --> 1:30:08.400
 artificial, I might say artificial intelligence system, artificial robotic systems,

1:30:08.400 --> 1:30:12.720
 robotic systems have, you know, kind of a big place, a big role in society,

1:30:13.280 --> 1:30:20.480
 and where we try to imagine the sort of the limiting case of technological advancement

1:30:20.480 --> 1:30:28.080
 and how that might play out in our future history. But yeah, I think that that was

1:30:28.960 --> 1:30:34.560
 in some way influential. I don't really know how, but I would recommend it. I mean,

1:30:34.560 --> 1:30:39.600
 if nothing else, you'd be well entertained. When did you first, yourself, like fall in love with

1:30:39.600 --> 1:30:43.360
 idea of artificial intelligence get captivated by this field?

1:30:44.880 --> 1:30:51.760
 So my honest answer here is actually that I only really started to think about it as a,

1:30:51.760 --> 1:30:55.280
 that's something that I might want to do actually in graduate school pretty late.

1:30:55.840 --> 1:31:01.040
 And a big part of that was that until, you know, somewhere around 2009, 2010,

1:31:01.040 --> 1:31:06.640
 then just wasn't really high on my priority list because I didn't think that it was something

1:31:06.640 --> 1:31:12.880
 where we're going to see very substantial advances in my lifetime. And, you know, maybe

1:31:14.160 --> 1:31:20.400
 in terms of my career, the time when I really decided I wanted to work on this was when I

1:31:20.400 --> 1:31:25.760
 actually took a seminar course that was taught by Professor Andrew Ng. And, you know, at that

1:31:25.760 --> 1:31:30.000
 point, I of course had some, had like a decent understanding of the technical things involved.

1:31:30.000 --> 1:31:33.600
 But one of the things that really resonated with me was when he said in the opening lecture,

1:31:33.600 --> 1:31:37.920
 something the effect of like, well, he used to have graduate students come to him and talk about

1:31:37.920 --> 1:31:41.840
 how they want to work on AI and he would kind of chuckle and give them some math problem to deal

1:31:41.840 --> 1:31:45.840
 with. But now he's actually thinking that this is an area where we might see like substantial

1:31:45.840 --> 1:31:51.200
 advances in our lifetime. And that kind of got me thinking because, you know, in some abstract

1:31:51.200 --> 1:31:56.400
 sense, yeah, like you can kind of imagine that. But in a very real sense, when someone who had

1:31:56.400 --> 1:32:01.680
 been working on that kind of stuff their whole career suddenly says that, yeah, like that,

1:32:01.680 --> 1:32:06.640
 that had some effect on me. Yeah, this might be a special moment in the history of the field.

1:32:07.680 --> 1:32:14.880
 That this is where we might see some, some interesting breakthroughs. So in the space of

1:32:14.880 --> 1:32:19.920
 advice, somebody who's interested in getting started in machine learning or reinforcement

1:32:19.920 --> 1:32:25.040
 learning, what advice would you give to maybe an undergraduate student or maybe even younger,

1:32:25.040 --> 1:32:31.600
 how what are the first steps to take? And further on, what are the steps to take on that journey?

1:32:32.560 --> 1:32:43.600
 So something that I think is important to do is to not be afraid to spend time imagining

1:32:43.600 --> 1:32:48.800
 the kind of outcome that you might like to see. So one outcome might be a successful career,

1:32:49.440 --> 1:32:53.440
 a large paycheck or something, or state of the art results on some benchmark.

1:32:53.440 --> 1:32:56.400
 But hopefully that's not the thing that's like the main driving force for somebody.

1:32:57.440 --> 1:33:04.080
 But I think that if someone who's a student considering a career in AI, like, takes a

1:33:04.080 --> 1:33:08.240
 little while, sits down and thinks like, what do I really want to see? What do I want to see a machine

1:33:08.240 --> 1:33:11.360
 do? What do I want? What do I want to see a robot do? What do I want to do? And what do I want to

1:33:11.360 --> 1:33:16.400
 see a natural language system? Just like imagine, you know, imagine it almost like a commercial

1:33:16.400 --> 1:33:20.400
 for a future product or something or like like something that you'd like to see in the world,

1:33:20.400 --> 1:33:24.320
 and then actually sit down and think about the steps that are necessary to get there.

1:33:24.880 --> 1:33:29.200
 And hopefully that thing is not a better number on image net classification. It's like it's

1:33:29.200 --> 1:33:33.040
 probably like an actual thing that we can't do today that would be really awesome, whether it's

1:33:33.040 --> 1:33:38.720
 a robot butler or a, you know, a really awesome healthcare decision making support system,

1:33:38.720 --> 1:33:43.680
 whatever it is that you find inspiring. And I think that thinking about that and then

1:33:43.680 --> 1:33:46.880
 backtracking from there and imagining the steps needed to get there will actually

1:33:46.880 --> 1:33:51.360
 lead to much better research. It'll lead to rethinking the assumptions. It'll lead to

1:33:51.360 --> 1:33:54.480
 working on the bottlenecks that other people aren't working on.

1:33:55.600 --> 1:34:01.040
 And then naturally to turn to you, we've talked about reward functions, and you just give an

1:34:01.040 --> 1:34:05.680
 advice on looking forward to how you'd like to see what kind of change you would like to make

1:34:05.680 --> 1:34:10.640
 in the world. What do you think, ridiculous, big question? What do you think is the meaning

1:34:10.640 --> 1:34:18.080
 of life? What is the meaning of your life? What gives you fulfillment, purpose, happiness, and

1:34:18.080 --> 1:34:27.440
 meaning? That's a very big question. What's the reward function under which you're operating?

1:34:27.440 --> 1:34:32.080
 Yeah, I think one thing that does give, you know, if not meaning at least satisfaction is

1:34:33.360 --> 1:34:37.840
 some degree of confidence that I'm working on a problem that really matters. I feel like it's

1:34:37.840 --> 1:34:46.080
 less important to me to actually solve a problem, but it's quite nice to take things to spend my

1:34:46.080 --> 1:34:51.840
 time on that I believe really matter. And I try pretty hard to look for that.

1:34:52.880 --> 1:34:59.680
 I don't know if it's easy to answer this, but if you're successful, what does that look like?

1:34:59.680 --> 1:35:06.800
 What's the big dream? Of course, success is built on top of success and you keep going forever,

1:35:06.800 --> 1:35:14.720
 but what is the dream? Yeah, so one very concrete thing or maybe as concrete as it's going to get

1:35:15.280 --> 1:35:23.200
 here is to see machines that actually get better and better the longer they exist in the world.

1:35:23.200 --> 1:35:26.960
 And that kind of seems like on the surface, one might even think that that's something that we

1:35:26.960 --> 1:35:34.960
 have today, but I think we really don't. I think that there is an ending complexity in the universe

1:35:34.960 --> 1:35:42.160
 and to date, all of the machines that we've been able to build don't sort of improve up to the limit

1:35:42.160 --> 1:35:47.200
 of that complexity. They hit a wall somewhere. Maybe they hit a wall because they're in a simulator

1:35:47.200 --> 1:35:52.160
 that has that is only a very limited, very pale limitation of the real world or they hit a wall

1:35:52.160 --> 1:35:57.520
 because they rely on a labeled dataset, but they never hit the wall of like running out of stuff

1:35:57.520 --> 1:36:03.760
 to see. So, you know, I'd like to build a machine that can go as far as possible.

1:36:03.760 --> 1:36:08.000
 And that runs up against the ceiling of the complexity of the universe. Yes.

1:36:09.280 --> 1:36:13.280
 Well, I don't think there's a better way to end it, Sergei. Thank you so much. It's a huge honor.

1:36:13.280 --> 1:36:20.560
 I can't wait to see the amazing work that you have to publish and in education space in terms

1:36:20.560 --> 1:36:24.000
 of reinforcement learning. Thank you for inspiring the world. Thank you for the great research you

1:36:24.000 --> 1:36:29.200
 do. Thank you. Thanks for listening to this conversation with Sergei Levine and thank you

1:36:29.200 --> 1:36:35.600
 to our sponsors, Cash App and ExpressVPN. Please consider supporting this podcast by

1:36:35.600 --> 1:36:42.640
 downloading Cash App and using code LEX Podcast and signing up at expressvpn.com

1:36:42.640 --> 1:36:50.160
 slash lexpod. Click all the links, buy all the stuff. It's the best way to support this podcast

1:36:50.160 --> 1:36:55.200
 and the journey I'm on. If you enjoy this thing, subscribe on YouTube, review it with

1:36:55.200 --> 1:37:01.200
 Firestars and Apple Podcast. Support on Patreon or connect with me on Twitter at Lex Freedman

1:37:01.200 --> 1:37:07.920
 spelled somehow if you can figure out how without using the letter E, just F R I D M A N.

1:37:08.800 --> 1:37:14.960
 And now let me leave you with some words from Salvador Dali. Intelligence without ambition

1:37:14.960 --> 1:37:24.960
 is a bird without wings. Thank you for listening and hope to see you next time.

