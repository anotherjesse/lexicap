WEBVTT

00:00.000 --> 00:07.040
 there's a broader question here, right? As we build socially and emotionally intelligent machines,

00:07.920 --> 00:12.480
 what does that mean about our relationship with them? And then we're broadly our relationship

00:12.480 --> 00:18.160
 with one another, right? Because this machine is going to be programmed to be amazing at empathy,

00:18.160 --> 00:22.480
 by definition, right? It's going to always be there for you. It's not going to get bored.

00:23.360 --> 00:25.680
 I don't know how I feel about that. I think about that a lot.

00:25.680 --> 00:34.640
 The following is a conversation with Rana L. Colubi, a pioneer in the field of emotion recognition

00:34.640 --> 00:41.680
 and human centric artificial intelligence. She is the founder of Effectiva, deputy CEO of SmartEye,

00:41.680 --> 00:48.240
 author of Girl Decoded, and one of the most brilliant, kind, inspiring, and fun human beings

00:48.240 --> 00:53.360
 I've gotten the chance to talk to. This is the Lex Friedman podcast. To support it,

00:53.360 --> 00:59.520
 please check out our sponsors in the description. And now, dear friends, here's Rana L. Colubi.

01:00.560 --> 01:06.240
 You grew up in the Middle East in Egypt. What is a memory from that time that makes you smile?

01:06.240 --> 01:11.440
 Or maybe a memory that stands out as helping your mind take shape and helping you define

01:11.440 --> 01:17.440
 yourself in this world? So the memory that stands out is we used to live in my grandma's house.

01:17.440 --> 01:23.280
 She used to have these mango trees in her garden in the summer, and so mango season was like July

01:23.280 --> 01:28.160
 and August. And so in the summer, she would invite all my aunts and uncles and cousins,

01:28.160 --> 01:32.800
 and it was just like maybe there were 20 or 30 people in the house, and she would cook all this

01:32.800 --> 01:39.840
 amazing food. And us, the kids, we would go down the garden, and we would pick all these mangoes.

01:41.200 --> 01:46.960
 And I don't know. I think it's just bringing people together that always stuck with me, the warmth.

01:46.960 --> 01:52.800
 Around the mango tree. Yeah, around the mango tree. And it's just the joy, the joy of being

01:52.800 --> 02:01.280
 together around food. And I'm a terrible cook, so I guess that memory didn't translate to me

02:01.280 --> 02:06.160
 kind of doing the same. I love hosting people. Do you remember colors, smells? Is that what?

02:07.200 --> 02:14.320
 How does memory work? What do you visualize? Do you visualize people's faces, smiles? Is there

02:14.320 --> 02:20.480
 colors? Is there like a theme to the colors? Is it smells because of food involved?

02:21.120 --> 02:26.800
 Yeah, I think that's a great question. So those Egyptian mangoes, there's a particular type

02:26.800 --> 02:31.120
 that I love, and it's called Darwesi mangoes, and they're kind of, you know, they're oval,

02:31.120 --> 02:37.360
 and they have a little red in them. So they're red and mango colored on the outside. So I remember

02:37.360 --> 02:45.040
 that. Is red indicate like extra sweetness? Isn't that? Yes. That means like it's nice and ripe

02:45.040 --> 02:52.720
 and stuff. Yeah. What's like a definitive food of Egypt? You know, there's like these almost

02:52.720 --> 03:00.400
 stereotypical foods in different parts of the world, like Ukraine invented borsh. Borsh is this

03:00.400 --> 03:09.520
 beet soup that you put sour cream on. If you know what it is, I think you know it's delicious,

03:09.520 --> 03:14.400
 but if I explain it, it's just not going to sound delicious. I feel like beet soup. This

03:14.400 --> 03:18.800
 doesn't make any sense, but that's kind of, and you probably have actually seen pictures of it,

03:18.800 --> 03:23.760
 because it's one of the traditional foods in Ukraine, in Russia, in different parts of the

03:23.760 --> 03:30.800
 Slavic world. But it's become so cliche and stereotypical that you almost don't mention it,

03:30.800 --> 03:37.120
 but it's still delicious. I visited Ukraine, I eat that every single day. Do you make it

03:37.120 --> 03:41.840
 yourself? How hard does it make? No, I don't know. I think to make it well, like anything,

03:41.840 --> 03:48.160
 like Italians, they say, well, tomato sauce is easy to make, but to make it right,

03:48.160 --> 03:53.840
 that's like a generational skill. So anyway, is there something like that in Egypt? Is there

03:53.840 --> 04:00.240
 a culture of food? There is. And actually, we have a similar kind of soup. It's called molochaya,

04:00.880 --> 04:06.800
 and it's made of this green plant. It's somewhere between spinach and kale,

04:06.800 --> 04:11.920
 and you mince it, and then you cook it in chicken broth. And my grandma used to make,

04:11.920 --> 04:16.800
 and my mom makes it really well, and I try to make it, but it's not as great. So we used to

04:16.800 --> 04:21.840
 have that, and then we used to have it alongside stuffed pigeons. I'm pescatarian now, so I don't

04:21.840 --> 04:26.640
 eat that anymore, but... Stuffed pigeons. Yeah, it's like, it was really yummy. It's the one thing

04:26.640 --> 04:33.280
 I miss about, you know, now that I'm pescatarian, and I don't eat... The stuffed pigeons? Yeah,

04:33.280 --> 04:39.120
 the stuffed pigeons. Is it, what are they stuffed with? If that doesn't bother you too much to

04:39.120 --> 04:46.000
 describe? No, no, it's stuffed with a lot of like just rice, and yeah, it's just rice. Yeah, so.

04:46.000 --> 04:52.560
 And you also said that your first in your book, that your first computer was an Atari,

04:52.560 --> 04:58.000
 and Space Invaders was your favorite game. Is that when you first fell in love with computers?

04:58.000 --> 05:03.200
 Would you say? Yeah, I would say so. Video games or just the computer itself? Just something about

05:03.200 --> 05:10.480
 the machine? Ooh, this thing... It's magic in here. Yeah, I think the magical moment is definitely

05:10.480 --> 05:15.520
 like playing video games with my... I have two younger sisters, and we just like had fun together,

05:15.520 --> 05:22.720
 like playing games. But the other memory I have is my first code. The first code I wrote, I wrote...

05:23.840 --> 05:29.600
 I drew a Christmas tree, and I'm Muslim, right? So it's kind of... It was kind of funny that I...

05:29.600 --> 05:35.840
 The first thing I did was like this Christmas tree, so yeah. And that's when I realized, wow,

05:35.840 --> 05:42.160
 you can write code to do all sorts of like really cool stuff. I must have been like six or seven

05:42.160 --> 05:48.640
 at the time. So you can write programs, and the programs do stuff for you. That's power.

05:49.760 --> 05:55.120
 If you think about it, that's empowering. That's AI. Yeah, I know what it is. I don't know if that...

05:55.120 --> 06:00.000
 See, like, I don't know if many people think of it that way when they first learned to program.

06:00.000 --> 06:04.400
 They just love the puzzle of it. Like, oh, this is cool. This is pretty. It's a Christmas tree,

06:04.400 --> 06:09.840
 but like, it's power. It is power. Like, you... Eventually, I guess you couldn't at the time,

06:09.840 --> 06:14.640
 but eventually this thing, if it's interesting enough, if it's a pretty enough Christmas tree,

06:14.640 --> 06:19.920
 it can be run by millions of people and bring them joy, like that little thing. And then,

06:19.920 --> 06:24.560
 because it's digital, it's easy to spread. So like, you just created something that's

06:24.560 --> 06:29.840
 easily spreadable to millions of people. Totally. It's hard to think that way when you're six.

06:30.800 --> 06:37.040
 In the book, you write, I am who I am because I was raised by a particular set of parents,

06:37.040 --> 06:43.120
 both modern and conservative, forward thinking and yet locked in tradition. I'm a Muslim,

06:43.120 --> 06:48.000
 and I feel I'm stronger and more centered for it. I adhere to the values of my religion,

06:48.000 --> 06:54.640
 even if I'm not as dutiful as I once was. And I am a new American, and I'm thriving on the

06:54.640 --> 07:00.880
 energy, vitality, and entrepreneurial spirit of this great country. So let me ask you about your

07:00.880 --> 07:05.280
 parents. What have you learned about life from them, especially when you were young?

07:05.280 --> 07:10.160
 So both my parents, they're Egyptian, but they moved to Kuwait right out. Actually,

07:10.160 --> 07:15.600
 there's a cute story about how they met. So my dad taught Kobel in the 70s. Nice.

07:15.600 --> 07:21.040
 And my mom decided to learn programming. So she signed up to take his Kobel programming class.

07:22.320 --> 07:27.200
 And he tried to date her, and she was like, no, no, no, I don't date. And so he's like,

07:27.200 --> 07:30.960
 okay, I'll propose. And that's how they got married. Whoa, strong move.

07:30.960 --> 07:38.160
 Right, exactly right. That's really impressive. So those, those Kobel guys know how to, how to

07:38.160 --> 07:43.520
 impress a lady. So, so yeah, so what have you learned from them?

07:43.520 --> 07:48.880
 So definitely grit. One of the core values in our family is just hard work. There were no

07:48.880 --> 07:54.160
 slackers in our family. And that's something I've definitely, that's definitely stayed with me,

07:54.160 --> 08:01.920
 both, both as a professional, but also in my personal life. But I also think my mom, my mom

08:01.920 --> 08:08.240
 always used to like, I don't know, it was like unconditional love. Like I just knew my parents

08:08.240 --> 08:15.520
 would be there for me, kind of regardless of what I chose to do. And I think that's very powerful.

08:15.520 --> 08:20.160
 And they got tested on it because I kind of challenged, you know, I challenge cultural norms,

08:20.160 --> 08:26.720
 and I kind of took a different path, I guess, than what's expected of, you know, a woman in

08:26.720 --> 08:31.600
 the Middle East. And then they, and I, you know, they still love me, which is, which is, I'm so

08:31.600 --> 08:36.160
 grateful for that. When was like a moment that was the most challenging for them? Which moment

08:36.880 --> 08:42.880
 were they kind of, they had to come face to face with the fact that you're a bit of a rebel?

08:42.880 --> 08:52.800
 I think the first big moment was when I had just gotten married, but I decided to go do my PhD at

08:52.800 --> 09:00.080
 Cambridge University. And because my husband at the time, he's now my ex, ran a company in Cairo,

09:00.080 --> 09:03.440
 he was going to stay in Egypt. So it was going to be a long distance relationship.

09:04.080 --> 09:09.040
 And that's very unusual in the Middle East for a woman to just head out and kind of,

09:09.040 --> 09:15.680
 you know, pursue her career. And so my dad actually, my dad and my parents in law both said,

09:16.880 --> 09:21.760
 you know, we do not approve of you doing this, but now you're under the jurisdiction of your

09:21.760 --> 09:29.600
 husband, so he can make the call. And luckily for me, he was supportive. He said, you know,

09:29.600 --> 09:33.840
 this is your dream come true. We've always wanted to do a PhD. I'm going to support you.

09:33.840 --> 09:41.040
 So I think that was the first time where, you know, I challenged the cultural norms.

09:41.040 --> 09:44.640
 Was that scary? Oh my God, yes. It was totally scary.

09:44.640 --> 09:52.160
 What's the biggest culture shock from there to Cambridge, to London?

09:52.720 --> 09:58.880
 Well, that was also during right around September 11th. So everyone thought that there was going

09:58.880 --> 10:06.720
 to be a third world war. And I, at the time, I used to wear the hijab. So I was very visibly

10:06.720 --> 10:12.800
 Muslim. And so my parents just were, they were afraid for my safety. But anyways, when I got

10:12.800 --> 10:17.440
 to Cambridge, because I was so scared, I decided to take off my headscarf and wear a hat instead.

10:17.440 --> 10:22.640
 So I just went to class wearing these like British hat, which was, in my opinion, actually worse

10:22.640 --> 10:26.880
 than just showing up in a headscarf. Because it was just so awkward, right? Like fitting in class

10:26.880 --> 10:33.840
 with like all these. Trying to fit in. Yeah. So after a few weeks of doing that, I was like,

10:33.840 --> 10:37.200
 to heck with that, I'm just going to go back to wearing my headscarf.

10:37.200 --> 10:46.240
 Yeah, you wore the hijab. So starting in 2000 and for 12 years after. So it's always whenever

10:46.240 --> 10:52.160
 you're in public, you have to wear the hat covering. Can you speak to that, the hijab,

10:52.160 --> 10:56.480
 maybe your mixed feelings about it? Like what does it represent in its best case?

10:56.480 --> 11:01.200
 What does it represent in the worst case? Yeah. You know, I think there's a lot of,

11:02.320 --> 11:06.800
 I guess I'll first start by saying I wore it voluntarily. I was not forced to wear it.

11:06.800 --> 11:11.360
 And in fact, I was one of the very first women in my family to decide to put on the hijab.

11:12.080 --> 11:16.480
 And my family thought it was really odd, right? Like there was, they were like,

11:16.480 --> 11:22.560
 why do you want to put this on? And at its best, it's the sign of modesty, humility.

11:22.560 --> 11:28.320
 Yeah. It's like me wearing a suit. People are like, why are you wearing a suit? It's a step back

11:28.320 --> 11:34.160
 into some kind of tradition, a respect for tradition of sorts. So you said because it's

11:34.160 --> 11:39.920
 by choice, you're kind of free to make that choice to celebrate a tradition of modesty.

11:39.920 --> 11:45.920
 Exactly. And I actually like mated my own. I remember I would really match the color of my

11:45.920 --> 11:51.600
 headscarf with what I was wearing. Like it was a form of self expression. And at its best,

11:51.600 --> 11:57.360
 I loved wearing it. You know, I have a lot of questions around how we practice religion and

11:57.360 --> 12:03.680
 religion and, you know, and I think, I think also it was a time where I was spending a lot of time

12:03.680 --> 12:08.400
 going back and forth between the US and Egypt. And I started meeting a lot of people in the US

12:08.400 --> 12:15.280
 who are just amazing people, very purpose driven, people who have very strong core values,

12:15.280 --> 12:22.000
 but they're not Muslim. That's okay, right? And so that was when I just had a lot of questions.

12:22.000 --> 12:27.360
 And politically, also the situation in Egypt was when the Muslim Brotherhood ran the country and I

12:27.360 --> 12:34.400
 didn't agree with their ideology. It was at a time when I was going through a divorce. Like it was

12:34.400 --> 12:40.160
 like just the perfect storm of like political personal conditions where I was like, this doesn't

12:40.160 --> 12:46.320
 feel like me anymore. And it took a lot of courage to take it off because culturally, it's not,

12:46.320 --> 12:50.800
 it's okay if you don't wear it, but it's really not okay to wear it and then take it off.

12:51.920 --> 12:58.560
 But you're still, so you have to do that while still maintaining a deep core and pride in the

12:59.120 --> 13:06.720
 origins, in your origin story. Totally. So still being Egyptian, still being a Muslim.

13:06.720 --> 13:14.240
 Right. And being, I think generally like faith driven, but yeah.

13:14.240 --> 13:18.880
 But what that means changes year by year for you. It's like a personal journey.

13:18.880 --> 13:23.120
 Yeah, exactly. What would you say is the role of faith in that part of the world?

13:23.920 --> 13:27.520
 Like how do you say, you mentioned it a bit in the book too.

13:27.520 --> 13:31.040
 Yeah. I mean, I think there is something really

13:31.040 --> 13:38.160
 powerful about just believing that there's a bigger force. You know, there's a kind of

13:38.160 --> 13:43.280
 surrendering, I guess, that comes with religion and you surrender and you have this deep conviction

13:43.280 --> 13:47.920
 that it's going to be okay. Right? Like the universe is out to like do amazing things for you

13:47.920 --> 13:52.480
 and it's going to be okay. And there's strength to that. Like even when you're going through adversity,

13:54.960 --> 14:00.000
 you just know that it's going to work out. Yeah, it gives you like an inner piece of calmness.

14:00.000 --> 14:05.600
 Exactly. Exactly. Yeah. It's faith in all the meanings of that word.

14:05.600 --> 14:09.760
 Right. Faith that everything is going to be okay. And it is because time passes

14:10.800 --> 14:16.320
 and time cures all things. It's like a calmness with the chaos of the world. Yeah.

14:16.320 --> 14:22.400
 And also there's like a silver, I'm a true believer of this, that something at the specific

14:22.400 --> 14:29.360
 moment in time can look like it's catastrophic and it's not what you wanted in life. But then

14:29.360 --> 14:34.720
 time passes and then you look back and there's a silver lining, right? It may be close the door,

14:34.720 --> 14:39.840
 but it opened a new door for you. And so I'm a true believer in that, that, you know,

14:39.840 --> 14:44.960
 there's a silver lining and almost anything in life, you just have to have this like,

14:45.680 --> 14:48.240
 have faith or conviction that it's going to work out. So,

14:48.240 --> 14:52.560
 such a beautiful way to see a shady feeling. So if you're, if you feel shady about it,

14:52.560 --> 15:03.440
 current situation, I mean, it almost is always true unless it's the cliche thing of if it doesn't

15:03.440 --> 15:08.960
 kill you, whatever it doesn't kill you makes you stronger. It's, it does seem that over time,

15:08.960 --> 15:15.760
 when you take a perspective on things that the hardest moments and periods of your life are

15:15.760 --> 15:24.000
 the most meaningful. Yeah. Yeah. So over time you get to have that perspective. Right. What,

15:24.000 --> 15:31.840
 what about, because you mentioned Kuwait, what about, let me ask you about war. What's the

15:31.840 --> 15:38.080
 role of war and peace? Maybe even the big love and hate in that part of the world, because it

15:38.080 --> 15:43.680
 does seem to be a part of the world where there's turmoil. There was turmoil, there's still turmoil.

15:43.680 --> 15:51.920
 It is so unfortunate, honestly. It's, it's such a waste of human resources and, and,

15:53.200 --> 15:57.840
 yeah, and human mind share. I mean, and at the end of the day, we all kind of want the same

15:57.840 --> 16:03.280
 things. We want, you know, we want a human connection, we want joy, we want to feel fulfilled,

16:03.280 --> 16:11.040
 we want to feel, you know, a life of purpose. And I just, I just find it baffling, honestly,

16:11.040 --> 16:17.040
 that we are still having to grapple with that. I have a story to share about this. You know,

16:17.040 --> 16:22.800
 I grew up in Egypt, I'm Egyptian, American now, but, but, you know, originally from Egypt.

16:22.800 --> 16:28.080
 And when I first got to Cambridge, it turned out my office mate, like my PhD kind of,

16:28.880 --> 16:32.800
 you know, she ended up, you know, we ended up becoming friends, but she was from Israel.

16:33.920 --> 16:37.280
 And we didn't know, yeah, we didn't know how it was going to be like.

16:37.280 --> 16:41.040
 Did you guys sit there just staring at each other for a bit?

16:42.400 --> 16:48.720
 Actually, she, because I arrived before she did. And it turns out she emailed our PhD advisor

16:49.600 --> 16:55.040
 and asked him if she thought it was going to be okay. Yeah. Oh, this is around 9 11 too.

16:55.040 --> 17:00.400
 Yeah. And, and Peter, Peter Robinson, our PhD advisor was like, yeah, like,

17:00.400 --> 17:05.120
 this is an academic institution, just show up. And we became super good friends. We were both

17:05.120 --> 17:09.600
 um, new moms, like we both had our kids during our PhD. We were both doing artificial emotional

17:09.600 --> 17:14.400
 intelligence. She was looking at speech. I was looking at the face. We just had so the culture

17:14.400 --> 17:21.120
 was so similar. Our jokes were similar. It was just, I was like, why on earth are our countries?

17:21.920 --> 17:26.160
 Why is there all this like war and tension? And I think it falls back to the narrative, right?

17:26.160 --> 17:31.360
 If you change the narrative, like whoever creates this narrative of war, I don't know,

17:31.360 --> 17:36.480
 I don't know. We should have women around the world. Yeah, that's that's one solution.

17:37.520 --> 17:41.360
 The good women, because there's also evil women in the world. True, true. Okay.

17:43.920 --> 17:49.040
 But yes, yes, there could be less war if women around the world. The the other aspect is,

17:50.160 --> 17:56.400
 doesn't matter the gender, the people in power. You know, I get to see this with with Ukraine

17:56.400 --> 18:02.480
 and Russia, different parts of the world around that conflict now. And that's happening in Yemen

18:02.480 --> 18:09.280
 as well, and everywhere else. There's these narratives told by the leaders to the populace.

18:09.840 --> 18:14.800
 And those narratives take hold and everybody believes that and they have a distorted view

18:15.520 --> 18:20.960
 of the humanity on the other side. In fact, especially during war, you don't even see

18:20.960 --> 18:29.680
 the people on the other side as as as human, or as equal intelligence, or worth, or value,

18:29.680 --> 18:38.800
 as as you, you tell all kinds of narratives about them being Nazis, or Dom, or whatever,

18:38.800 --> 18:46.320
 whatever narrative you want to weave around that, or evil. But I think when you actually meet them

18:46.320 --> 18:52.560
 face to face, you realize they're like the same. Exactly, right? It's actually a big shock for

18:52.560 --> 19:00.000
 people to realize like, that they've been they've been essentially lied to, within their country.

19:00.000 --> 19:06.720
 And I kind of have faith that social media is as ridiculous as it is to say, or any kind of technology

19:07.280 --> 19:14.240
 is able to bypass the the walls that governments put up, and connect people directly. And then

19:14.240 --> 19:21.280
 you get to realize who like people fall in love across different nations, and religions, and so

19:21.280 --> 19:26.880
 on. And that I think ultimately can cure a lot of our ills, especially sort of in person. Just I

19:26.880 --> 19:34.000
 also think that if leaders met in person, to have a conversation that would cure a lot of ills of

19:34.000 --> 19:41.280
 the world, especially in private. Let me ask you about the women running, running the world.

19:41.280 --> 19:49.440
 Okay. So gender does in part, perhaps shape the landscape of just our human experience.

19:51.040 --> 19:57.920
 So in what ways was the limiting in in what ways was it empowering for you to be a woman in the

19:57.920 --> 20:03.120
 Middle East? I think just kind of just going back to like my comment on like women running the world,

20:03.120 --> 20:07.920
 I think it comes back to empathy, right, which which has been a common threat throughout my

20:07.920 --> 20:14.240
 my entire career. And it's this this idea of human connection. Once you build common ground with a

20:14.240 --> 20:20.640
 person or a group of people, you build trust, you build loyalty, you build friendship, and then and

20:20.640 --> 20:25.840
 then you can turn that into like behavior change and motivation and persuasion. So so it's like

20:25.840 --> 20:33.920
 empathy and emotions are just at the center of of everything we do. And and I think being being

20:33.920 --> 20:40.560
 from the Middle East, kind of this human connection is very strong. Like we have this running joke that

20:40.560 --> 20:45.680
 if you come to Egypt for a visit, people are going to we'll know everything about your life

20:45.680 --> 20:51.520
 like right away, right? I have no problems asking you about your personal life. There's no like no

20:51.520 --> 20:55.920
 boundaries really, no personal boundaries in terms of getting to know people we get emotionally

20:55.920 --> 21:00.560
 intimate like very, very quickly. But I think people just get to know each other like,

21:00.560 --> 21:06.240
 authentically, I guess, you know, there isn't this like superficial level of getting to know people,

21:06.240 --> 21:10.960
 you just try to get to know people really deeply part of that. Totally. Because you can put yourself

21:10.960 --> 21:17.760
 in this person's shoe and kind of, yeah, imagine, you know, what what challenges they're going through.

21:17.760 --> 21:25.280
 And so I think I've definitely taken that with me. Generosity is another one too, like just being

21:25.280 --> 21:32.400
 like just being generous with your time and love and attention and even with your wealth, right?

21:32.400 --> 21:35.760
 Even if you don't have a lot of it, you're still very generous. And I think that's another

21:36.640 --> 21:43.360
 enjoying the humanity of other people. And so you think there's a useful difference between men

21:43.360 --> 21:55.520
 and women in that aspect and empathy? Or is doing these kind of big general groups, does that hinder

21:55.520 --> 22:01.760
 progress? Yeah, I don't I actually don't want to over generalize. I mean, I some of the men I know

22:01.760 --> 22:07.040
 are like the most empathetic humans. Yeah, I strive to be. Yeah, you're you're actually very empathetic.

22:07.040 --> 22:16.960
 Yeah, so I so I don't want to over generalize. Although one of the researchers I worked with

22:16.960 --> 22:22.080
 when I was at Cambridge, Professor Simon Bering Cohen, he's Sasha Bering Cohen's cousin. Yeah.

22:23.280 --> 22:28.640
 And he runs the autism research center at Cambridge. And he's written multiple books

22:30.000 --> 22:35.280
 on autism. And one of his one of his theories is the empathy scale, like the systemisers and

22:35.280 --> 22:43.360
 the empathizers. And it there's a disproportionate amount of computer scientists and engineers who

22:43.360 --> 22:51.760
 are systemisers, and perhaps not great empathizers. And then, you know, there's and there's more men

22:51.760 --> 22:56.560
 in that bucket, I guess, than women. And then there's more women in the empathizers bucket.

22:56.560 --> 23:02.080
 So again, not not to over generalize. I sometimes wonder about that. It's been frustrating to me

23:02.080 --> 23:07.840
 how many, I guess, systemisers there are in the field of robotics. Yeah, it's actually encouraging

23:07.840 --> 23:15.440
 to me because I care about, obviously, social robotics. And because it's it, there's more

23:15.440 --> 23:22.160
 opportunity for people that are empathic. Exactly. I totally agree. Well, right. So it's nice. Yes.

23:22.160 --> 23:26.240
 So every robotics I talk to, they don't see the the human as interesting as

23:26.240 --> 23:33.840
 like it is. It's not exciting. You want to avoid the human at all costs. It's a it's a safety concern

23:33.840 --> 23:40.160
 to be touching the human, which it is. But it is also an opportunity for a deep connection

23:41.040 --> 23:46.160
 or collaboration or all that kind of stuff. So and because most most brilliant roboticist

23:46.160 --> 23:51.120
 don't care about the human, it's an opportunity, right, for in your case, it's a business opportunity

23:51.120 --> 23:57.760
 to be in general an opportunity to explore those ideas. So in this beautiful journey to Cambridge,

23:59.200 --> 24:04.960
 to, you know, UK, and then to America, what, what's the moment or moments were there were

24:04.960 --> 24:10.960
 most transformational for you as a scientist and as a leader. So you became an exceptionally

24:10.960 --> 24:21.920
 successful CEO, founder, researcher, scientist, and so on. Was there a face shift there where

24:21.920 --> 24:25.680
 like, I can be somebody, I can I can really do something in this world.

24:26.640 --> 24:32.480
 Yeah, so I actually just kind of a little bit of background. So the reason why I moved from

24:32.480 --> 24:38.320
 Cairo to Cambridge UK to do my PhD is because I had a very clear career plan. I was like,

24:38.320 --> 24:44.480
 okay, I'll go abroad, get my PhD, gonna crush it in three or four years, come back to Egypt and

24:44.480 --> 24:51.120
 teach. It was very clear, very well laid out. Was topic clear or no? The topic. Well, I did,

24:51.120 --> 24:54.960
 I did my PhD around building artificial emotional intelligence and looking at the

24:54.960 --> 24:59.040
 in your master plan ahead of time when you're sitting by the mango tree. Did you did you know

24:59.040 --> 25:03.920
 it's going to be artificial intelligence? No, no, no, that I did not know. Although I think I kind

25:03.920 --> 25:09.600
 of knew that I was going to be doing computer science, but I didn't know the specific area.

25:10.160 --> 25:16.320
 But I love teaching. I mean, I still love teaching. So I just, yeah, I just wanted to go abroad,

25:16.320 --> 25:22.240
 get a PhD, come back, teach. Why computer science? Can we just link on that? Because you're such an

25:22.240 --> 25:28.240
 empathic person who cares about emotion and humans and so on. Aren't computers cold and

25:28.240 --> 25:37.360
 emotionless. We're changing that. Yeah, I know. But like, isn't that the, or did you see computers

25:37.360 --> 25:44.560
 as the having the capability to actually connect with humans? I think that was like my takeaway

25:44.560 --> 25:49.520
 from my experience just growing up, like computers sit at the center of how we connect and communicate

25:49.520 --> 25:54.560
 with one another, right? Or technology in general, like I remember my first experience being away

25:54.560 --> 25:59.040
 from my parents, we communicated with a fax machine. But thank goodness for the fax machine,

25:59.040 --> 26:03.040
 because we could send letters back and forth to each other. This was pre emails and stuff.

26:04.720 --> 26:10.320
 So I think, I think there's, I think technology can be not just transformative in terms of

26:10.320 --> 26:14.880
 productivity, etc. It actually does change how we connect with one another. And

26:15.520 --> 26:22.400
 Can I just defend the fax machine? There's something like the haptic feel is the email

26:22.400 --> 26:26.320
 is all digital. There's something really nice. I still write letters to people.

26:27.120 --> 26:31.680
 There's something nice about the haptic aspect of the fax machine, because you still have to press,

26:31.680 --> 26:35.600
 you still have to do something in the physical world to make this thing a reality,

26:36.160 --> 26:40.320
 the sense, right? And then it like comes out as a printout and you can actually touch it and read

26:40.320 --> 26:47.600
 it. Yeah, there's something, there's something lost when it's just an email. Obviously, I wonder

26:47.600 --> 26:53.120
 how we can regain some of that in the digital world, which goes to the metaverse and all those

26:53.120 --> 26:58.080
 kinds of things. We'll talk about it anyway. So actually, do you question on that one? Do you

26:58.080 --> 27:06.240
 still, do you have photo albums anymore? Do you still print photos? No, no, but I'm a minimalist.

27:06.240 --> 27:11.520
 Okay. So it was one of the one of the painful steps in my life was to scan all the photos

27:11.520 --> 27:18.320
 and let go of them and then let go of all my books. You let go of your books? Yeah, switched

27:18.320 --> 27:28.240
 to Kindle, everything Kindle. So I thought, I thought, okay, think 30 years from now, nobody's

27:28.240 --> 27:31.680
 going to have books anymore. The technology of digital books is going to get better and better

27:31.680 --> 27:36.320
 and better. Are you really going to be the guy that's still romanticizing physical books? Are

27:36.320 --> 27:42.880
 you going to be the old man on the porch who was like, yes. So just get used to it because it felt,

27:42.880 --> 27:50.720
 it still feels a little bit uncomfortable to read on a Kindle, but get used to it. I mean,

27:50.720 --> 27:54.880
 I'm trying to learn new programming languages always. With technology, you have to kind of

27:54.880 --> 28:00.480
 challenge yourself to adapt to it. I force myself to use TikTok now. That thing doesn't

28:00.480 --> 28:05.920
 need much forcing. It pulls you in like the worst kind of, or the best kind of drug.

28:05.920 --> 28:14.160
 Anyway, yeah. But I do love haptic things. There's a magic to the haptic. Even like touch

28:14.160 --> 28:23.360
 screens, it's tricky to get right, to get the experience of a button. Anyway, what were we

28:23.360 --> 28:29.680
 talking about? So AI, so the journey, your whole plan was to come back to Cairo and teach.

28:29.680 --> 28:36.560
 Right. And then what did the plan go wrong? Yeah, exactly. Right. And then I got to Cambridge

28:36.560 --> 28:41.920
 and I fall in love with the idea of research and kind of embarking on a path. Nobody's

28:42.560 --> 28:45.840
 explored this path before. You're building stuff that nobody's built before and it's

28:45.840 --> 28:50.960
 challenging and it's hard and there's a lot of nonbelievers. I just totally love that.

28:50.960 --> 28:56.320
 And at the end of my PhD, I think it's the meeting that changed the trajectory of my life.

28:56.320 --> 29:02.160
 Professor Roslyn Picard, who's, she runs the affective computing group at the MIT Media Lab.

29:02.160 --> 29:07.520
 I had read her book. I was like following all her research.

29:07.520 --> 29:15.600
 AKA Ros. Yes, AKA Ros. And she was giving a talk at a pattern recognition conference in Cambridge

29:16.320 --> 29:19.680
 and she had a couple of hours to kill. So she emailed the lab and she said,

29:19.680 --> 29:26.960
 you know, if any students want to meet with me, just sign up here. And so I signed up for slots

29:26.960 --> 29:30.240
 and I spent like the weeks leading up to it preparing for this meeting.

29:30.880 --> 29:36.000
 And I want to show her a demo of my research and everything. And we met and we ended up

29:36.000 --> 29:40.640
 hitting it off like we totally clicked. And at the end of the meeting, she said,

29:40.640 --> 29:45.920
 do you want to come work with me as a postdoc at MIT? And this is what I told her. I was like,

29:45.920 --> 29:49.760
 okay, this would be a dream come true, but there's a husband waiting for me in Cairo.

29:49.760 --> 29:55.600
 I kind of have to go back. And she said, it's fine. Just commute. And I literally started

29:55.600 --> 30:01.680
 commuting between Cairo and Boston. Yeah, it was, it was a long commute. And I didn't,

30:01.680 --> 30:06.320
 I did that like every few weeks, I would, you know, hop on a plane and go to Boston.

30:06.320 --> 30:09.120
 But that, that changed the trajectory of my life. There was no,

30:09.120 --> 30:16.720
 I kind of outgrew my dreams, right? I didn't want to go back to Egypt anymore and be faculty.

30:16.720 --> 30:21.920
 Like that was no longer my dream. I had a dream. What was the, what was it like to be at MIT?

30:22.480 --> 30:26.960
 What was that culture shock? You mean America in general, but also

30:28.560 --> 30:34.000
 I mean Cambridge is its own culture. So what was MIT like? And what was America like?

30:34.000 --> 30:39.280
 I think, I wonder if that's similar to your experience at MIT. I was just

30:41.200 --> 30:46.400
 at the media lab in particular, I was just really impressed is not the right word.

30:47.040 --> 30:55.760
 I didn't expect the openness to like innovation and the acceptance of taking a risk and failing.

30:55.760 --> 30:59.760
 Like failure isn't really accepted back in Egypt, right? You don't want to fail.

30:59.760 --> 31:03.920
 Like there's a fear of failure, which I think has been hardwired in my brain.

31:04.880 --> 31:08.400
 But you get to MIT and it's okay to start things. And if they don't work out,

31:08.400 --> 31:13.680
 like it's okay, you pivot to another idea. And that kind of thinking was just very new to me.

31:13.680 --> 31:19.440
 That's liberating. What media lab for people don't know, MIT Media Lab is its own beautiful thing

31:19.440 --> 31:26.320
 because they, I think more than other places at MIT reach for big ideas and like they try,

31:26.320 --> 31:31.760
 I mean, I think, I mean, depending of course on who, but certainly with Rosalind is you try

31:31.760 --> 31:38.560
 wild stuff, you try big things and crazy things and also try to take things to completion so

31:38.560 --> 31:45.760
 you can demo them. So always, always, always have a demo. Like if you go, one of the sad things to

31:45.760 --> 31:52.720
 me about robotics labs at MIT and there's like over 30, I think, is like usually when you show up

31:52.720 --> 31:58.800
 to robotics lab, there's not a single working robot. They're all broken. All the robots are broken,

31:58.800 --> 32:03.200
 which is like the normal state of things because you're working on them. But it would be nice

32:03.200 --> 32:10.320
 if we lived in a world where robotics labs had some robots functioning. One of my like favorite

32:10.320 --> 32:16.320
 moments that just sticks with me, I visited Boston Dynamics and there was a, first of all,

32:16.320 --> 32:27.200
 seeing so many spots, so many legged robots in one place. I'm like, I'm home. But this is where

32:27.200 --> 32:35.280
 I was built. The cool thing was just to see there was a random robot spot was walking down the hall.

32:35.280 --> 32:39.440
 It's probably doing mapping, but it looked like he wasn't doing anything and he was wearing key or

32:39.440 --> 32:47.280
 sheet. I don't know. In my mind, they're people. They have a backstory, but this one in particular

32:47.280 --> 32:53.280
 definitely has a backstory because he was wearing a cowboy hat. So I just saw a spot robot with a

32:53.280 --> 33:01.360
 cowboy hat walking down the hall and there was just this feeling like there's a life. He has a life.

33:01.360 --> 33:07.600
 He probably has to commute back to his family at night. There's a feeling like there's life

33:07.600 --> 33:12.080
 instilled in this robot. And that's magical. I don't know. It was kind of inspiring to see.

33:12.080 --> 33:18.160
 Did he say hello to you? No, it's very focused. There's a focused nature to the robot. No,

33:18.160 --> 33:23.600
 no, listen, I love competence and focus and great. Like he was not going to get distracted by the

33:24.480 --> 33:31.200
 shallowness of small talk. There's a job to be done and he was doing it. So anyway, the fact

33:31.200 --> 33:36.400
 that it was working is a beautiful thing. And I think Media Lab really prides itself on trying to

33:36.400 --> 33:41.120
 always have a thing that's working that he could show off. Yes, we used to call it a demo or die.

33:43.040 --> 33:47.440
 Yeah, you could not show up with PowerPoint or something. You actually had to have it working.

33:47.440 --> 33:53.280
 You know what? My son, who is now 13, I don't know if this is still his lifelong goal or not,

33:53.280 --> 33:59.200
 but when he was a little younger, his dream is to build an island that's just inhabited by robots,

33:59.200 --> 34:02.720
 like no humans. He just wants all these robots to be connecting and having fun.

34:02.720 --> 34:09.360
 And there you go. Does he have an idea of which robots he loves most? Is it

34:10.640 --> 34:15.680
 Roomba like robots? Is it humanoid robots, robot dogs, or is not clear yet?

34:16.800 --> 34:22.240
 We used to have a Jibo, which was one of the MIT Media Lab spinouts. And he used to love Jibo.

34:22.240 --> 34:25.520
 The thing with a giant head. Yes, it spins. Right, exactly.

34:25.520 --> 34:33.360
 You can rotate. And it's an eye. Not glowing. Right, exactly. It's like Hal 9000, but the

34:33.360 --> 34:43.680
 friendly version. He loved that. And then he just loves, I think he loves all forms of robots,

34:43.680 --> 34:50.480
 actually. So it embodies intelligence. Yes. I personally like elegant robots, especially.

34:50.480 --> 34:59.760
 Anything that can wiggle its butt. No, that's not the definition of what I love. But that's

34:59.760 --> 35:03.280
 just technically what I've been working on recently, except I have a bunch of legged

35:03.280 --> 35:09.360
 robots now in Austin. And I've been doing, I've been trying to have them communicate

35:09.360 --> 35:16.240
 affection with their body in different ways, just for art. For art, really. Because I love the idea

35:16.240 --> 35:22.000
 of walking around with robots, as you would with a dog. I think it's inspiring to a lot of people,

35:22.000 --> 35:28.640
 especially young people. Kids love robots. Kids love it. Parents, adults are scared of robots,

35:28.640 --> 35:33.760
 but kids don't have this kind of weird construction of the world that's full of evil. They love

35:33.760 --> 35:39.760
 cool things. Yeah. I remember when Adam was in first grade, so he must have been like seven or

35:39.760 --> 35:46.160
 so. I went into his class with a whole bunch of robots and the Emotion AI demo and I asked

35:46.160 --> 35:53.120
 the kids, I was like, do you, would you kids want to have a robot, you know, robot friend or robot

35:53.120 --> 35:57.760
 companion? Everybody said yes. And they wanted it for all sorts of things, like to help them with

35:57.760 --> 36:04.480
 their math homework and to like be a friend. So there's, it just struck me how there was no fear

36:04.480 --> 36:12.240
 of robots. Was a lot of adults have that like us versus them. Yeah, none of that. Of course,

36:12.240 --> 36:17.680
 you want to be very careful because you still have to look at the lessons of history and how

36:17.680 --> 36:22.240
 robots can be used by the power centers of the world to abuse your rights and all that kind of

36:22.240 --> 36:30.960
 stuff. But mostly it's good to enter anything new with an excitement and optimism. Speaking of

36:30.960 --> 36:36.400
 Roz, what have you learned about science and life from Rosalind Picard? Oh my God, I've learned so

36:36.400 --> 36:44.880
 many things about life from Roz. I think the thing I learned the most is perseverance.

36:47.600 --> 36:53.280
 When I first met Roz, we apply and she invited me to be her postdoc, we applied for a grant to the

36:53.280 --> 37:01.040
 National Science Foundation to apply some of our research to autism and we got back, we were

37:01.040 --> 37:08.400
 rejected. The first time you were rejected for fun. Yeah, it was and I basically, I just took

37:08.400 --> 37:13.520
 the rejection to mean, okay, we're rejected. It's done like end of story, right? And Roz was like,

37:14.160 --> 37:19.520
 it's great news. They love the idea. They just don't think we can do it. So let's build it,

37:19.520 --> 37:28.560
 show them and then reapply. Oh my God, that story totally stuck with me. And she's like that in

37:28.560 --> 37:34.080
 every aspect of her life. She just does not take no for an answer. To reframe all negative feedback

37:35.360 --> 37:42.240
 was a challenge. Yes, they liked this. Yeah, it was, it was a riot.

37:43.200 --> 37:47.120
 What else about science in general, about how you see computers and

37:48.960 --> 37:54.240
 also business and just everything about the world. She's a very powerful, brilliant woman

37:54.240 --> 37:59.440
 like yourself. So is there some aspect of that too? Yeah, I think Roz is actually also very

37:59.440 --> 38:06.400
 faith driven. She has this like deep belief in conviction. Yeah, and in the good in the world

38:06.400 --> 38:12.880
 and humanity. And I think that was meeting her and her family was definitely like a defining

38:12.880 --> 38:17.360
 moment for me because that was when I was like, wow, like, you can be of a different background

38:17.360 --> 38:25.200
 and religion and whatever. And you can still have the same core values. So that was, that was, yeah.

38:26.800 --> 38:31.760
 I'm grateful to her. So Roz, if you're listening, thank you. Yeah, she's great. She's been on this

38:31.760 --> 38:40.720
 podcast before. I hope she'll be on, I'm sure she'll be on again. You were the founder and CEO of

38:40.720 --> 38:47.520
 Affectiva, which is a big company that was acquired by another big company, Smart Eye. And you're now

38:47.520 --> 38:52.800
 the deputy CEO of Smart Eye. So you're a powerful leader, you're brilliant, you're brilliant scientists.

38:53.520 --> 38:58.880
 A lot of people are inspired by you. What advice would you give, especially to young women, but

38:58.880 --> 39:04.560
 people in general who dream of becoming powerful leaders like yourself in a world where perhaps,

39:04.560 --> 39:16.800
 in a world that perhaps doesn't give them a clear, easy path to do so, whether we're talking about

39:16.800 --> 39:25.920
 Egypt or elsewhere? You know, here, you kind of describe me that way, kind of encapsulates,

39:27.040 --> 39:31.280
 I think, what I think is the biggest challenge of all, which is believing in yourself, right?

39:31.280 --> 39:37.520
 I have had to like grapple with this, what I call now the Debbie Downer voice in my head.

39:39.360 --> 39:44.240
 The kind of basically, it's just chattering all the time. It's basically saying, oh, no, no, no,

39:44.240 --> 39:47.440
 you can't do this. Like, you're not going to raise money. You can't start a company. Like,

39:47.440 --> 39:51.200
 what business do you have, like, starting a company or running a company or selling a company?

39:51.200 --> 39:59.040
 Like, you name it. It's always like, and I think my biggest advice to not just women,

39:59.040 --> 40:04.960
 but people who have, who are taking a new path and, you know, they're not sure,

40:05.760 --> 40:09.840
 is to not let yourself and let your thoughts be the biggest obstacle in your way.

40:10.880 --> 40:17.920
 And I've had to like really work on myself to not be my own biggest obstacle.

40:17.920 --> 40:19.840
 So you got that negative voice?

40:19.840 --> 40:20.720
 Yeah.

40:20.720 --> 40:22.160
 So is that?

40:22.160 --> 40:24.240
 Am I the only one? I don't think I'm the only one.

40:24.240 --> 40:30.960
 No, I have that negative voice. I'm not exactly sure if it's a bad thing or a good thing. I've

40:30.960 --> 40:37.200
 been really torn about it because it's been a lifelong companion. It's hard to know.

40:38.800 --> 40:45.440
 It's kind of a, it drives productivity and progress, but it can't hold you back from

40:45.440 --> 40:52.720
 taking big leaps. I think you, the best I can say is probably you have to somehow

40:52.720 --> 40:59.440
 be able to control it. So turn it off when it's not useful and turn it on when it's useful.

41:00.240 --> 41:02.800
 Like I have from almost like a third person perspective.

41:02.800 --> 41:04.720
 Right. Somebody who's sitting there like.

41:04.720 --> 41:10.720
 Yeah. Like because it is useful to, to be critical. Like after,

41:12.800 --> 41:20.800
 like I just gave a talk yesterday at MIT and I was just, you know, there's so much love and it

41:20.800 --> 41:26.160
 was such an incredible experience. So many amazing people I got. She has to talk to, but

41:26.160 --> 41:32.000
 you know, afterwards when I, when I went home and just took this long walk, it was mostly

41:32.000 --> 41:38.800
 just negative thoughts about me. I don't like one basic stuff. Like I don't deserve any of it.

41:38.800 --> 41:44.880
 And second is like, like, why did you, that was so dumb that you said this, that's so dumb.

41:44.880 --> 41:48.640
 Like you got, you should have prepared that better. Why did you say this?

41:48.640 --> 41:56.160
 But I think it's good to hear that voice out. All right. And like sit in that.

41:56.160 --> 42:00.640
 And ultimately, I think you grow from that. Now, when you're making really big decisions about

42:00.640 --> 42:09.280
 funding or starting a company or taking a leap to go to the UK or take a leap to go to America,

42:09.280 --> 42:18.960
 to work in media lab, though, yeah, there's a, that's, you should be able to shut that off then

42:18.960 --> 42:25.440
 because you should have like this weird confidence, almost like faith that you said before that

42:25.440 --> 42:33.040
 everything's going to work out to take the leap of faith. Despite all the negativity. I mean,

42:33.040 --> 42:40.640
 there's, there's some of that you actually tweeted a really nice tweet thread. It says, quote,

42:40.640 --> 42:47.600
 a year ago, a friend recommended I do daily affirmations and I was skeptical. But I was

42:47.600 --> 42:51.920
 going through major transitions in my life. So I gave it a shot and it set me on a journey of

42:51.920 --> 42:59.280
 self acceptance and self love. So what was that like image? Maybe talk through this idea of

42:59.280 --> 43:05.520
 affirmations and how that helped you? Yeah, because really, like, I'm just like me, I'm a kind,

43:05.520 --> 43:10.320
 I'd like to think of myself as a kind person in general, but I'm kind of mean to myself sometimes.

43:11.280 --> 43:18.400
 And so I've been doing journaling for almost 10 years now. I use an app called day one and

43:18.400 --> 43:22.480
 it's awesome. I just journal and I use it as an opportunity to almost have a conversation with

43:22.480 --> 43:27.040
 the Debbie Downer voice in my, it's like a rebuttal, right? Like Debbie Downer says, oh,

43:27.040 --> 43:30.880
 my God, like you, you know, you won't be able to raise this round of funny. I'm like, okay,

43:30.880 --> 43:37.200
 let's talk about it. I have a track record of doing X, Y and Z. I think I can do this.

43:37.200 --> 43:42.320
 And it's, it's literally like, so I wouldn't, I don't know that I can shut off the voice,

43:42.320 --> 43:47.840
 but I can have a conversation with it. And it just, it just, and I bring data to the table, right?

43:49.840 --> 43:53.200
 Nice. So, so that was the journaling part, which I found very helpful.

43:53.200 --> 43:59.120
 But the affirmation took it to a whole next level and I just love it. I'm, I'm, I'm a year

43:59.120 --> 44:03.120
 into doing this. And you literally wake up in the morning and the first thing you do,

44:04.240 --> 44:09.600
 I meditate first. And then, and then I write my affirmations and it's, it's the energy I want to

44:09.600 --> 44:14.880
 put out in the world that hopefully will come right back to me. So I will say, I always start

44:14.880 --> 44:19.520
 with my smile lights up the whole world. And I kid you not, like people in the street will stop me

44:19.520 --> 44:26.640
 and say, Oh my God, like we love your smile. Like, yes. So, so my affirmations will change depending

44:26.640 --> 44:31.360
 on, you know, what's happening this day. Is it funny? I know, don't judge, don't judge.

44:31.360 --> 44:37.200
 No, that's not, what? Laughter is not judgment. It's just awesome. I mean, it, it's true,

44:37.200 --> 44:44.880
 but you're saying affirmations somehow help kind of, what is it that they do work to like

44:44.880 --> 44:49.680
 remind you of the kind of person you are and the kind of person you want to be, which

44:50.880 --> 44:55.040
 actually may be inverse order, the kind of person you want to be, and that helps you become the,

44:55.040 --> 45:00.880
 the kind of person you actually are. It just, it's, it brings intentionality to like what you're

45:00.880 --> 45:07.280
 doing, right? And so, by the way, I was laughing because my affirmations, which I also do are the

45:07.280 --> 45:14.800
 opposite. Oh, you do? Oh, what do you do? I don't, I don't have a, my smile left up the water. Maybe

45:14.800 --> 45:23.120
 I should add that because like I, I have, I just, I have, oh boy, I just, it's, it's much more stoic,

45:23.120 --> 45:31.360
 like about focused about this kind of stuff, but the joy, the emotion that you're just in that

45:31.360 --> 45:36.560
 little affirmation is beautiful. So maybe I should add that. I have some, I have some like

45:36.560 --> 45:40.960
 focused stuff, but that's usually, but that's a cool start. That's just after all the like

45:40.960 --> 45:46.080
 smiling and playful and joyful and all that, and then it's like, okay, I kick butt. Let's get you

45:46.080 --> 45:51.040
 done. Right. Let's get you done affirmation. Okay, cool. So like what else is on there?

45:52.240 --> 46:00.000
 Oh, what else is on there? Um, well, I, I have, I'm a, I'm, I'm a magnet for all sorts of things.

46:00.000 --> 46:05.360
 So I'm an amazing people magnet. I attract like awesome people into my universe. I,

46:05.360 --> 46:10.640
 so that's an actual affirmation. Yes. That's great. Yeah. So that, that's, and that, yeah,

46:10.640 --> 46:16.720
 and that somehow manifests itself into like working. I think so. Yeah. Like, can you speak to like,

46:16.720 --> 46:23.280
 why it feels good to do the affirmations? I honestly think it just grounds the day.

46:24.080 --> 46:29.920
 And then it allows me to, instead of just like being pulled back and forth, like throughout

46:29.920 --> 46:35.840
 the day, it just like grounds me like, okay, like this thing happened. It's not exactly what I wanted

46:35.840 --> 46:42.240
 it to be, but I'm patient or I'm, you know, I'm, I trust that the universe will do amazing things

46:42.240 --> 46:47.760
 for me, which is one of my other consistent affirmations. Or I'm an amazing mom, right? And so

46:47.760 --> 46:53.760
 I can grapple with all the feelings of mom guilt that I have all the time. Or here's another one.

46:53.760 --> 46:58.800
 I'm a love magnet. And I literally say, I will kind of picture the person that I'd love to end up

46:58.800 --> 47:03.920
 with. And I write it all down and hasn't happened yet. But what do you, what do you picture?

47:03.920 --> 47:08.400
 This is Brad Pitt. Because that's what I picture. Okay. That's what you picture? Okay.

47:08.400 --> 47:16.400
 Yeah. I'm running, holding hands, running together. No, more like Fight Club that,

47:16.400 --> 47:20.720
 the Fight Club Brad Pitt, where he's like standing. All right, people will know. Okay. I'm sorry.

47:20.720 --> 47:25.840
 I'll get off of that. Do you have a, like when you're thinking about the being a love magnet in

47:25.840 --> 47:29.920
 that way? Are you picturing specific people? Or is this almost like

47:33.680 --> 47:41.760
 in the space of like energy? Right. It's somebody who is smart and well accomplished

47:41.760 --> 47:47.520
 and successful in their life, but they're generous and they're well traveled and they want to travel

47:47.520 --> 47:52.480
 the world. It's things like that. Like their head over heels into me is like, I know it sounds

47:52.480 --> 47:56.320
 super silly, but it's literally what I write. Yeah. And I believe it'll happen one day. Oh,

47:56.320 --> 48:00.320
 you actually write so you don't say it out loud? No, I write it. I write all my affirmations.

48:01.120 --> 48:08.240
 I do the opposite. I say it. Yeah. If I'm alone, I'll say it out loud. Yeah. I should try that.

48:10.000 --> 48:19.280
 I think it's which, what feels more powerful to you, to me, more powerful saying stuff feels

48:19.280 --> 48:31.440
 more powerful. Yeah. Writing is, writing feels like I'm losing, losing the words, like losing the

48:31.440 --> 48:37.520
 power of the words, maybe because I write slow. Do you hand write? No, I type. It's on this app.

48:37.520 --> 48:41.520
 It's day one basically. And I just, I can, the best thing about it is I can look back

48:41.520 --> 48:50.640
 and see like a year ago, what was I affirming, right? Also changes over time. It hasn't like

48:50.640 --> 48:56.240
 changed a lot, but it, but the focus kind of changes over time. I got it. Yeah. I say the same

48:56.240 --> 49:00.720
 exact thing over and over and over. Oh, you do? Okay. There's a comfort in the, in the sameness of

49:00.720 --> 49:06.400
 it. Well, actually, let me jump around because let me ask you about, because I talked, all this talk

49:06.400 --> 49:11.920
 about Brad Pitt or maybe it's just going on my side of my head. Let me ask you about dating in

49:11.920 --> 49:20.240
 general. You tweeted, are you based in Boston in single question mark? And then you pointed to a

49:20.240 --> 49:26.240
 startup singles night sponsored by a small dating app. Because I mean, this is jumping around a

49:26.240 --> 49:33.840
 little bit, but since, since you mentioned, can AI help solve this dating love problem?

49:33.840 --> 49:40.880
 What do you think? This problem of connection that is part of the human condition. Can AI help

49:40.880 --> 49:48.240
 that you yourself are in the search affirming? Maybe that's what I should affirm, like build an AI.

49:48.240 --> 49:55.360
 Build an AI that finds love. I think, I think there must be a science behind

49:56.800 --> 50:02.480
 that first moment you meet a person and you either have chemistry or you don't, right? Like,

50:02.480 --> 50:07.040
 you, I guess that was the question I was asking, would you put it brilliantly? Is that a science or

50:07.040 --> 50:14.480
 an art? Ooh, I think there are like, there's actual chemicals that get exchanged when people,

50:14.480 --> 50:20.080
 two people meet. Oh, well, I don't know about that. I like how you're changing. Yeah, yeah,

50:20.080 --> 50:25.760
 changing your mind as we're describing it, but it feels that way. But it's what science shows us

50:25.760 --> 50:32.240
 is sometimes we can explain with the rigor, the things that feel like magic. Right. So maybe

50:32.240 --> 50:38.480
 you can remove all the magic. Maybe it's like, I honestly think, like I said, like Goodreads should

50:38.480 --> 50:45.840
 be a dating app, which like books, I wonder, I wonder if you look at just like books or content

50:45.840 --> 50:50.960
 you've consumed. I mean, that's essentially what YouTube does when it does recommendation. If you

50:50.960 --> 50:56.960
 just look at your footprint of content consumed, if there's an overlap, but maybe interesting

50:56.960 --> 51:02.240
 difference with an overlap, there's some I'm sure this is a machine learning problem that's solvable.

51:03.520 --> 51:10.560
 Like this person is very likely to be not only there to be chemistry in the short term,

51:10.560 --> 51:15.600
 but a good lifelong partner to grow together. I bet you it's a good machine learning problem.

51:15.600 --> 51:20.720
 You should need the data. Let's do it. Well, actually, I do think there's so much data about

51:20.720 --> 51:24.560
 each of us that there ought to be a machine learning algorithm that can ingest all this

51:24.560 --> 51:30.400
 data and basically say, I think the following 10 people would be interesting connections for you,

51:30.400 --> 51:37.360
 right? And so Smile Dating app kind of took one particular angle, which is humor. It matches

51:37.360 --> 51:42.400
 people based on their humor styles, which is one of the main ingredients of a successful

51:42.400 --> 51:46.640
 relationship. Like if you meet somebody and they can make you laugh, like that's a good thing.

51:47.200 --> 51:53.280
 And if you develop like internal jokes, like inside jokes and you're bantering, like that's fun.

51:53.280 --> 51:59.360
 Yeah. So I think. Yeah, definitely. But yeah, that's the

52:00.800 --> 52:08.000
 number and the rate of inside joke generation. You could probably measure that and then optimize

52:08.000 --> 52:11.920
 it over the first few days. You can see. Right. And then we're just turning this into a machine

52:11.920 --> 52:18.160
 learning problem. I love it. But for somebody like you, who's exceptionally successful and busy,

52:18.160 --> 52:28.720
 is there, is there signs to that aspect of dating? Is it tricky? Is there advice you can give?

52:28.720 --> 52:32.560
 Oh my God, I'd give the worst advice. Well, I can tell you like I have a spreadsheet.

52:33.520 --> 52:38.320
 Spreadsheet. That's great. Is that a good or a bad thing? Do you regret the spreadsheet?

52:39.760 --> 52:42.400
 Well, I don't know. What's the name of the spreadsheet? Is it Love?

52:42.400 --> 52:47.680
 It's the date track, dating tracker. It's very like. Love tracker. Yeah.

52:48.400 --> 52:52.240
 And there's a rating system, I'm sure. Yeah, there's like weights and stuff.

52:52.240 --> 52:56.000
 It's too close to home. Oh, is it? Do you also have a spreadsheet? Well, I don't have a spreadsheet,

52:56.000 --> 52:59.920
 but I would, now that you say it, it seems like a good idea. Oh, no.

53:02.560 --> 53:09.200
 Turning into data. I do wish that somebody else had a spreadsheet about me.

53:09.200 --> 53:18.240
 Like I said, like you said, convert, collect a lot of data about us in a way that's

53:18.240 --> 53:23.600
 privacy preserving, that I own the data, I can control it, and then use that data to find,

53:23.600 --> 53:29.440
 I mean, not just romantic love, but collaborators, friends, all that kind of stuff. It seems like

53:29.440 --> 53:34.720
 the data is there. That's the problem social networks are trying to solve, but I think they're

53:34.720 --> 53:40.560
 doing a really poor job. Even Facebook tried to get into a dating app business. And I think there's

53:40.560 --> 53:47.120
 so many components to running a successful company that connects human beings. And part of that is

53:50.240 --> 53:55.440
 having engineers that care about the human side, right? As you know extremely well, it's not

53:56.560 --> 54:00.640
 easy to find those, but you also don't want just people that care about the human,

54:00.640 --> 54:06.320
 they also have to be good engineers. So it's like, you have to find this beautiful mix. And for some

54:06.320 --> 54:13.440
 reason, just empirically speaking, people have not done a good job of that, of building companies

54:13.440 --> 54:19.920
 like that. It must mean that it's a difficult problem to solve. Dating apps, it seems difficult.

54:19.920 --> 54:28.400
 Okay, Cupid, Tinder, all those kind of stuff. They seem to find, of course they work, but they seem

54:28.400 --> 54:35.120
 to not work as well as I would imagine it's possible. With data, wouldn't you be able to find

54:35.120 --> 54:41.040
 better human connection? It's like arranged marriages on steroids essentially, arranged by

54:41.040 --> 54:42.560
 machine learning algorithm.

54:42.560 --> 54:46.480
 Arranged by machine learning algorithm, but not a superficial one. I think a lot of the dating

54:46.480 --> 54:51.920
 apps out there are just so superficial. They're just matching on high level criteria that aren't

54:51.920 --> 54:58.320
 ingredients for successful partnership. But you know what's missing though too? I don't know how

54:58.320 --> 55:04.080
 to fix that, the serendipity piece of it. Like how do you engineer serendipity? Like this random

55:04.080 --> 55:09.040
 chance encounter and then you fall in love with the person. I don't know how a dating app can do

55:09.040 --> 55:17.200
 that. So that has to be a little bit of randomness. Maybe every 10th match is just a, you know,

55:17.200 --> 55:23.200
 yeah, somebody that the algorithm wouldn't have necessarily recommended, but it allows for a

55:23.200 --> 55:31.920
 little bit of... Well, it can also trick you into thinking of serendipity by somehow showing

55:31.920 --> 55:37.600
 you a tweet of a person that he thinks you'll match well with, but do it accidentally as

55:37.600 --> 55:43.680
 part of another search. And you just notice it and then you go down a rabbit hole and you

55:43.680 --> 55:49.840
 get, you go down a rabbit hole and you connect them outside the app too. Like you connect with

55:49.840 --> 55:54.480
 this person outside the app somehow. So it creates that moment of meeting. Of course,

55:54.480 --> 55:58.800
 you have to think of from an app perspective how you can turn that into a business. But I think

55:58.800 --> 56:05.680
 ultimately a business that helps people fall in love in any way, like that's what Apple was about.

56:05.680 --> 56:10.400
 Create products that people love that's beautiful. I mean, you got to make money somehow.

56:10.400 --> 56:17.280
 If you help people fall in love personally with the product, find self love or

56:17.840 --> 56:21.440
 another human being, you're going to make money. You're going to figure out a way to make money.

56:22.960 --> 56:27.840
 I just feel like the dating apps often will optimize for something else than love.

56:28.560 --> 56:33.040
 It's the same with social networks. They optimize for engagement as opposed to like a deep,

56:33.040 --> 56:39.040
 meaningful connection that's ultimately grown in like personal growth. You as a human being

56:39.040 --> 56:46.560
 growing and all that kind of stuff. Let me do like a pivot to a dark topic, which you open the book

56:46.560 --> 56:56.000
 with. A story, because I'd like to talk to you about just emotion and artificial intelligence.

56:56.000 --> 57:01.600
 I think this is a good story to start to think about emotional intelligence. You open the book

57:01.600 --> 57:07.760
 with a story of a Central Florida man, Jamel Dunn, who was drowning and drowned while five

57:07.760 --> 57:13.360
 teenagers watched and laughed saying things like, you're going to die and when Jamel disappeared

57:13.360 --> 57:18.880
 below the surface of the water, one of them said he just died and the others laughed. What is this

57:18.880 --> 57:24.080
 incident teach you about human nature and the response to it perhaps?

57:25.120 --> 57:32.560
 I mean, I think this is a really, really, really sad story and it highlights what I believe is a

57:32.560 --> 57:39.040
 it's a real problem in our world today. It's an empathy crisis. Yeah, we're living through an

57:39.040 --> 57:47.200
 empathy crisis. And I mean, we've talked about this throughout our conversation. We dehumanize

57:47.200 --> 57:54.000
 each other. And unfortunately, yes, technology is bringing us together. But in a way, it's just

57:54.000 --> 58:02.720
 dehumanizing. It's creating this dehumanizing of the other. And I think that's a huge problem. The

58:02.720 --> 58:08.080
 good news is I think the solution could be technology based. I think if we rethink the

58:08.080 --> 58:14.080
 way we design and deploy our technologies, we can solve parts of this problem. But I worry about

58:14.080 --> 58:22.640
 it. I mean, even with my son, a lot of his interactions are computer mediated. And I just

58:22.640 --> 58:28.160
 question what that's doing to his empathy skills and you know, his ability to really connect with

58:28.160 --> 58:36.480
 people. So you think you think it's not possible to form empathy through the digital medium?

58:38.240 --> 58:45.600
 I think it is. But we have to be thoughtful about because the way the way we engage face to face,

58:45.600 --> 58:50.240
 which is what we're doing right now, right, there's the nonverbal signals, which are a majority of

58:50.240 --> 58:54.960
 how we communicate. It's like 90% of how we communicate is your facial expressions.

58:55.920 --> 59:00.320
 You know, I'm saying something and you're nodding your head now and that creates a feedback loop.

59:00.320 --> 59:03.840
 And if you break that. And now I have anxiety about it.

59:06.000 --> 59:11.440
 Poor Lex. I am not scrutinizing your facial expressions during this interview.

59:11.440 --> 59:21.520
 I am. Look normal, look human, nod head. Yeah, nod head in agreement.

59:21.520 --> 59:28.640
 If Rana says yes, then nod head else. Don't do it too much because it might be at the wrong time.

59:28.640 --> 59:34.080
 And then it will send the wrong signal. Oh, God. And make eye contact sometimes because humans

59:34.080 --> 59:42.400
 appreciate that. Okay. Yeah, but something about the, especially when we say mean things in person,

59:42.400 --> 59:46.720
 you get to see the pain of the other person. If you're tweeting it at a person and you have no

59:46.720 --> 59:50.880
 idea how it's going to land, you're more likely to do that on social media than you are in face

59:50.880 --> 59:55.840
 to face conversations. So what do you think is more important?

59:55.840 --> 1:00:06.400
 And EQ or IQ, EQ being emotional intelligence in terms of in what makes us human?

1:00:08.080 --> 1:00:14.800
 I think emotional intelligence is what makes us human. It's how we connect with one another.

1:00:14.800 --> 1:00:22.080
 It's how we build trust. It's how we make decisions, right? Like your emotions drive

1:00:22.080 --> 1:00:26.880
 kind of what you had for breakfast, but also where you decide to live and what do you want to do for

1:00:26.880 --> 1:00:31.360
 the rest of your life. So I think emotions are underrated.

1:00:32.800 --> 1:00:39.120
 But so emotional intelligence isn't just about the effective expression of your own emotions.

1:00:39.120 --> 1:00:44.640
 It's about a sensitivity and empathy to other people's emotions and that sort of being able

1:00:44.640 --> 1:00:48.240
 to effectively engage in the dance of emotions with other people.

1:00:48.240 --> 1:00:55.120
 Yeah, I like that explanation. I like that kind of, yeah, thinking about it as a dance,

1:00:55.120 --> 1:00:59.200
 because it is really about that. It's about sensing what state the other person's in and

1:00:59.200 --> 1:01:06.720
 using that information to decide on how you're going to react. And I think it can be very powerful.

1:01:06.720 --> 1:01:13.840
 Like people who are the best, most persuasive, most persuasive leaders in the world tap into,

1:01:13.840 --> 1:01:20.000
 you know, they have, if you have higher EQ, you're more likely to be able to motivate people

1:01:20.000 --> 1:01:24.400
 to change their behaviors. So it can be very powerful.

1:01:24.960 --> 1:01:31.760
 On a more kind of technical, maybe philosophical level, you've written that emotion is universal.

1:01:32.560 --> 1:01:39.040
 It seems that sort of like Chomsky says language is universal. There's a bunch of other stuff like

1:01:39.040 --> 1:01:45.840
 cognition, consciousness. Seems a lot of us have these aspects. So the human mind generates all

1:01:46.800 --> 1:01:52.640
 this. And so what do you think is the, they all seem to be like echoes of the same thing.

1:01:53.840 --> 1:02:00.080
 What do you think emotion is exactly? Like how deep does it run? Is it a surface level thing

1:02:00.640 --> 1:02:05.280
 that we display to each other? Is it just another form of language or something deep within?

1:02:05.280 --> 1:02:11.920
 I think it's, it's really deep. It's how, you know, we started with memory. I think emotions

1:02:11.920 --> 1:02:17.920
 play a really important role. Yeah, emotions play a very important role in how we encode

1:02:17.920 --> 1:02:22.480
 memories, right? Our, our memories are often encoded, almost indexed by emotions. Yeah.

1:02:24.800 --> 1:02:31.840
 Yeah, it's at this core of how, you know, our decision making engine is also heavily influenced

1:02:31.840 --> 1:02:37.600
 by our emotions. So emotions is part of cognition. It's totally, it's intermix into the whole thing.

1:02:37.600 --> 1:02:42.800
 Yes, absolutely. And in fact, when you take it away, people are unable to make decisions.

1:02:42.800 --> 1:02:47.200
 They're really paralyzed. Like they can't go about their daily or their, you know, personal or

1:02:47.200 --> 1:02:57.440
 professional lives. So it does seem like there's probably some interesting interweaving of emotion

1:02:57.440 --> 1:03:03.120
 and consciousness. I wonder if it's possible to have, like if they're next door neighbors somehow,

1:03:03.680 --> 1:03:11.840
 or if they're actually flatmates. I don't, I don't, it feels like the, the hard problem of

1:03:11.840 --> 1:03:18.160
 consciousness where it's some, it feels like something to experience the thing. Like red

1:03:18.160 --> 1:03:24.400
 feels like red and it's, you know, when you eat a mango, sweet, the taste, the, the, the sweetness

1:03:24.400 --> 1:03:31.280
 that it feels like something to experience that sweetness, that whatever generates emotions.

1:03:33.200 --> 1:03:39.440
 But then like, I feel like emotion is part of communication. It's very much about communication.

1:03:39.440 --> 1:03:49.680
 And then that means it's also deeply connected to language. But then probably human intelligence

1:03:49.680 --> 1:03:53.920
 is deeply connected to the collective intelligence between humans. It's not just the standalone

1:03:53.920 --> 1:03:59.200
 thing. So the whole thing is really connected. So emotion is connected to language, language is

1:03:59.200 --> 1:04:04.080
 connected to intelligence. And then intelligence connected to consciousness and consciousness

1:04:04.080 --> 1:04:09.360
 is connected to emotion. The whole thing is, it's a beautiful mess. So

1:04:12.640 --> 1:04:16.480
 Can I comment on the emotions being a communication mechanism? Because I think

1:04:16.480 --> 1:04:24.400
 there are two facets of, of our emotional experiences. One is communication, right?

1:04:24.400 --> 1:04:29.760
 Like we use emotions, for example, facial expressions or other nonverbal cues to connect

1:04:29.760 --> 1:04:38.880
 with other human beings and with other beings in the world, right? But even if it's not a

1:04:38.880 --> 1:04:43.920
 communication context, we still experience emotions and we still process emotions and

1:04:43.920 --> 1:04:49.920
 we still leverage emotions to make decisions and to learn and, you know, to experience life. So

1:04:50.880 --> 1:04:55.280
 it isn't always just about communication. And we learned that very early on in our,

1:04:55.280 --> 1:05:01.520
 in kind of our work at Affectiva. One of the very first applications we brought to market was

1:05:01.520 --> 1:05:05.040
 understanding how people respond to content, right? So if they're watching this video of

1:05:05.040 --> 1:05:09.520
 ours, like, are they interested? Are they inspired? Are they bored to death? And so we

1:05:09.520 --> 1:05:16.320
 watched their facial expressions. And we had, we weren't sure if people would express any emotions

1:05:16.320 --> 1:05:21.120
 if they were sitting alone. Like if you're in your bed at night, watching a Netflix TV series,

1:05:21.120 --> 1:05:25.440
 would we still see any emotions on your face? And we were surprised that, yes, people still

1:05:25.440 --> 1:05:29.680
 emote, even if they're alone, even if you're in your car driving around, you're singing along

1:05:29.680 --> 1:05:35.280
 a song and you're joyful. We'll see these expressions. So it's not just about communicating

1:05:35.280 --> 1:05:40.320
 with another person. It's sometimes really is it just about experiencing the world.

1:05:41.680 --> 1:05:47.920
 First of all, I wonder if some of that is because we develop our intelligence and our

1:05:47.920 --> 1:05:53.280
 emotional intelligence by communicating with other humans. And so when other humans disappear

1:05:53.280 --> 1:05:57.840
 from the picture, we're still kind of a virtual human. The code still runs basically. Yeah,

1:05:57.840 --> 1:06:02.800
 the code still runs. And but you're also kind of, you're still, there's like virtual humans,

1:06:02.800 --> 1:06:07.440
 you don't have to think of it that way. But there's a kind of, when you like chuckle like,

1:06:08.560 --> 1:06:15.040
 yeah, like you're, you're kind of chuckling to a virtual human. I mean, it's possible that

1:06:16.400 --> 1:06:24.240
 the code is the has to have another human there. Because if you just grow up alone,

1:06:24.240 --> 1:06:30.640
 I wonder if emotion will still be there in this visual form. So yeah, I wonder. But anyway,

1:06:30.640 --> 1:06:39.120
 what can you tell from the human face about what's going on inside? So that's the problem

1:06:39.120 --> 1:06:47.120
 that effective at first tackled, which is using computer vision, using machine learning to try to

1:06:47.120 --> 1:06:52.960
 detect stuff about the human face as many things as possible, and convert them into a prediction of

1:06:54.480 --> 1:07:00.240
 categories of emotion, anger, happiness, all that kind of stuff. How hard is that problem?

1:07:00.240 --> 1:07:04.640
 It's extremely hard. It's very, very hard because there is no one to unmapping between

1:07:06.240 --> 1:07:11.040
 a facial expression and your internal states. There just isn't. There's this oversimplification

1:07:11.040 --> 1:07:16.240
 of the problem where it's something like, if you are smiling, then you're happy. If you do a brow

1:07:16.240 --> 1:07:21.280
 furrow, then you're angry. If you do an eyebrow raise, then you're surprised. And just think about

1:07:21.280 --> 1:07:26.320
 it for a moment, you could be smiling for a whole host of reasons. You could also be happy and not

1:07:26.320 --> 1:07:34.160
 be smiling. You could furrow your eyebrows because you're angry or you're confused about something

1:07:34.160 --> 1:07:41.360
 or you're constipated. So I think this oversimplistic approach to inferring emotion from a facial

1:07:41.360 --> 1:07:49.280
 expression is really dangerous. The solution is to incorporate as many contextual signals as you

1:07:49.280 --> 1:07:57.600
 can. So for example, I'm driving a car and you can see me nodding my head and my eyes are closed

1:07:57.600 --> 1:08:03.760
 and the blinking rate is changing. I'm probably falling asleep at the wheel because you know

1:08:03.760 --> 1:08:10.080
 the context. You understand what the person is doing. So I think or add additional channels like

1:08:10.080 --> 1:08:17.200
 voice or gestures or even physiological sensors. But I think it's very dangerous to just take this

1:08:17.200 --> 1:08:24.000
 over simplistic approach of smile equals happy. If you're able to, in a high resolution way,

1:08:24.000 --> 1:08:29.600
 specify the context, there's certain things that are going to be somewhat reliable signals

1:08:29.600 --> 1:08:35.840
 of something like drowsiness or happiness or stuff like that. I mean, when people are watching

1:08:35.840 --> 1:08:43.200
 Netflix content, that problem, that's a really compelling idea that you can kind of, at least

1:08:43.200 --> 1:08:49.360
 in aggregate, highlight like which part was boring, which part was exciting. How hard was

1:08:49.360 --> 1:08:56.720
 that problem? That was on the scale of like difficulty. I think that's one of the easier

1:08:56.720 --> 1:09:02.560
 problems to solve because it's a relatively constrained environment. You have somebody

1:09:02.560 --> 1:09:07.200
 sitting in front of, initially we started with like a device in front of you like a laptop.

1:09:07.200 --> 1:09:12.720
 And then we graduated to doing this on a mobile phone, which is a lot harder just because of,

1:09:12.720 --> 1:09:17.520
 you know, from a computer vision perspective, the profile view of the face can be a lot more

1:09:17.520 --> 1:09:22.240
 challenging. We had to figure out lighting conditions because usually people are watching

1:09:22.240 --> 1:09:29.200
 content literally in their bedrooms at night, lights are dimmed. Yeah. I mean, if you're

1:09:29.200 --> 1:09:35.680
 standing, it's probably going to be the looking up the nostril view. Yeah. And nobody looks good at

1:09:35.680 --> 1:09:42.080
 that. I've seen data sets from that perspective. It's like, this is not a good look for anyone.

1:09:42.960 --> 1:09:46.480
 Or if you're laying in bed at night, what is it? Side view or something?

1:09:47.360 --> 1:09:52.640
 And half your face is like on a pillow. Actually, I would love to know, have data

1:09:53.600 --> 1:10:03.040
 about like how people watch stuff in bed at night. Like, do they prop there? Is it a pillow?

1:10:03.040 --> 1:10:06.480
 Like, I'm sure there's a lot of interesting dynamics there.

1:10:06.480 --> 1:10:10.720
 Right. From a health and well being perspective, right? Like, it's like, oh, you're hurting,

1:10:10.720 --> 1:10:15.280
 you're not. I was thinking machine learning perspective, but yes. But also, yeah. Yeah,

1:10:15.280 --> 1:10:19.600
 once you have that data, you can start making all kinds of inference about health and stuff like

1:10:19.600 --> 1:10:24.880
 that. Interesting. Yeah. There was an interesting thing when I was at Google that we were,

1:10:24.880 --> 1:10:31.520
 it's called active authentication where you want to be able to

1:10:32.560 --> 1:10:38.400
 unlock your phone without using a password. So it would face, but also other stuff,

1:10:38.400 --> 1:10:44.160
 like the way you take a phone out of the pocket. So that kind of data to use the multimodal

1:10:44.960 --> 1:10:50.640
 with machine learning to be able to identify that it's you or likely to be you, likely not to be you,

1:10:50.640 --> 1:10:55.680
 that allows you to not always have to enter the password. That was the idea. But the funny thing

1:10:55.680 --> 1:11:01.600
 about that is I just want to tell a small anecdote is because it was all male engineers.

1:11:03.360 --> 1:11:11.920
 Except, so there's, my boss is our boss who's still one of my favorite humans was a woman,

1:11:11.920 --> 1:11:22.640
 Regina Dugan. Oh my God, I love her. She's awesome. She's the best. So, but anyway,

1:11:22.640 --> 1:11:26.960
 and there was another, there's one female engineer, brilliant female engineer on the team,

1:11:26.960 --> 1:11:31.680
 and she was the one that actually, I like the fact that women often don't have pockets. Right.

1:11:33.360 --> 1:11:38.640
 You know, it was like, whoa, that was not even a category in the code of like, wait a minute,

1:11:38.640 --> 1:11:44.720
 you can take the phone out of some other place than your pocket. So anyway, that's a, it's a

1:11:44.720 --> 1:11:48.720
 funny thing when you're considering people laying in bed watching a phone, you have to consider

1:11:48.720 --> 1:11:54.800
 if you have to, you know, diversity in all its forms, depending on the problem, depending on the

1:11:54.800 --> 1:11:59.600
 context. Yeah, actually, this is like a very important, I think this is, you know, you probably

1:11:59.600 --> 1:12:03.520
 get this all the time, like people are worried that AI is going to take over humanity and like,

1:12:04.160 --> 1:12:08.160
 get rid of all the humans, the world, and I'm like, actually, that's not my biggest concern.

1:12:08.160 --> 1:12:13.600
 My biggest concern is that we are building bias into these systems. And then they're like deployed

1:12:14.160 --> 1:12:20.160
 at large and at scale. And before you know it, you're kind of accentuating the bias that exists

1:12:20.160 --> 1:12:25.840
 in society. And yeah, I'm not, you know, I know people, it's very important to worry about that,

1:12:25.840 --> 1:12:34.320
 but I, the worry is an emergent phenomena to me, which is a very good one, because I think these

1:12:34.320 --> 1:12:42.160
 systems are actually, by encoding the data that exists, they're revealing the bias in society,

1:12:42.160 --> 1:12:48.080
 they're for teaching us what the bias is, therefore, we can now improve that bias within the system.

1:12:48.080 --> 1:12:52.480
 So they're almost like putting a mirror to ourselves. So I'm not,

1:12:52.960 --> 1:12:57.280
 we have to be open to looking at the mirror, though, we have to be right open to scrutinizing

1:12:57.280 --> 1:13:02.720
 the data. And if you just take it as ground, or you don't even have to look at the, I mean,

1:13:02.720 --> 1:13:06.800
 yes, the data is how you fix it, but then you just look at the behavior of the system. It's like,

1:13:06.800 --> 1:13:11.680
 and you realize, holy crap, this thing is kind of racist. Like, why is that? And then you look at

1:13:11.680 --> 1:13:16.240
 the data, it's like, okay. And then you start to realize that I think that's a much more effective

1:13:16.240 --> 1:13:23.920
 way to, to be introspective as a society than through sort of political discourse, like AI kind

1:13:23.920 --> 1:13:34.400
 of, because people are, people are for some reason more productive and rigorous in criticizing AI

1:13:34.400 --> 1:13:39.360
 than they're criticizing each other. So I think this is just a nice method for studying society

1:13:39.360 --> 1:13:45.600
 and see which way progress lies. Anyway, what we're talking about, you're watching the problem of

1:13:45.600 --> 1:13:51.680
 watching Netflix in bed, or elsewhere, and seeing which parts are exciting, which parts are boring,

1:13:51.680 --> 1:13:56.400
 you're saying that's relatively constrained, because, you know, you have a captive audience,

1:13:56.400 --> 1:14:00.800
 and you kind of know the context. And one thing you said that was really key is the

1:14:00.800 --> 1:14:04.720
 aggregate, you're doing this in aggregate, right? Like we're looking at aggregated response of people,

1:14:04.720 --> 1:14:11.120
 and so when you see a peak, say a smile peak, they're probably smiling or laughing at something

1:14:11.120 --> 1:14:17.280
 that's in the content. So that was one of the first problems we were able to solve. And, and

1:14:17.280 --> 1:14:22.080
 when we see the smile peak, it doesn't mean that these people are internally happy. They're just

1:14:22.080 --> 1:14:26.560
 laughing at content. So it's important to, you know, call it for what it is.

1:14:27.360 --> 1:14:32.640
 But it's still really, really useful data. I wonder how that compares to, so what like YouTube

1:14:32.640 --> 1:14:40.480
 and other places we use is obviously they don't have, for the most case, they don't have that kind

1:14:40.480 --> 1:14:48.080
 of data. They have the data of when people tune, tune out, drop off. And I think that's a

1:14:48.800 --> 1:14:53.840
 in aggregate for YouTube, at least a pretty powerful signal. I worry about what that leads to,

1:14:55.120 --> 1:15:01.840
 because looking at like YouTubers that are kind of really care about views and,

1:15:02.960 --> 1:15:09.520
 you know, try to maximize the number of views, I think they, when they say that the video should

1:15:09.520 --> 1:15:18.080
 be constantly interesting, it seems like a good goal. I feel like that leads to this manic pace

1:15:18.880 --> 1:15:25.920
 of a video. Like the idea that I would speak at the current speed that I'm speaking, I don't know.

1:15:27.200 --> 1:15:32.960
 And that every moment has to be engaging, right? Engaging. I think there's value to silence,

1:15:32.960 --> 1:15:37.280
 there's value to the boring bits. I mean, all some of the greatest movies ever,

1:15:37.280 --> 1:15:42.560
 some of the greatest stories ever told me, they have the boring bits, seemingly boring bits. I

1:15:42.560 --> 1:15:49.120
 don't know. I wonder about that. Of course, it's not that the human face can capture that either,

1:15:49.120 --> 1:15:57.760
 it's just giving an extra signal. You have to really, I don't know, you have to really collect

1:15:57.760 --> 1:16:06.640
 deeper, long term data about what was meaningful to people. When they think 30 days from now, what

1:16:07.360 --> 1:16:12.160
 they still remember, what moved them, what changed them, what helped them grow, that kind of stuff.

1:16:12.160 --> 1:16:16.880
 You know, it would be a really, I don't know if there are any researchers out there who are doing

1:16:16.880 --> 1:16:23.760
 this type of work. Wouldn't it be so cool to tie your emotional expressions while you're, say,

1:16:23.760 --> 1:16:30.320
 listening to a podcast interview and then 30 days later interview people and say, hey,

1:16:30.880 --> 1:16:35.200
 what do you remember? You've watched this 30 days ago, like what stuck with you? And then see if

1:16:35.200 --> 1:16:39.280
 there's any, there ought to be, maybe there ought to be some correlation between these emotional

1:16:39.280 --> 1:16:49.280
 experiences and yeah, what you, what stays with you. So the one guy listening now on the beach

1:16:49.280 --> 1:16:54.080
 in Brazil, please record a video of yourself listening to this and send it to me and then

1:16:54.080 --> 1:16:59.680
 I'll interview you 30 days from now. Yeah, that would be great. It would be statistically

1:16:59.680 --> 1:17:07.200
 significant. Yeah, I know one, but you know, yeah, yeah, I think that's really fascinating. I think

1:17:09.200 --> 1:17:17.760
 that kind of holds the key to a future where entertainment or content is both entertaining

1:17:17.760 --> 1:17:26.640
 and, I don't know, makes you better, empowering in some way. So figuring out like

1:17:28.480 --> 1:17:34.560
 showing people stuff that entertains them, but also they're happy they watched 30 days from

1:17:34.560 --> 1:17:38.880
 now because they've become a better person because of it. Well, you know, okay, not to

1:17:38.880 --> 1:17:44.880
 riff on this topic for too long, but I have two children, right? And I see my role as a parent

1:17:44.880 --> 1:17:49.920
 as like a chief opportunity officer. Like I am responsible for exposing them to all sorts of

1:17:49.920 --> 1:17:56.000
 things in the world. But often I have no idea of knowing like, what's stuck? Like, what was,

1:17:56.000 --> 1:18:00.320
 you know, is this actually going to be transformative, you know, for them 10 years down the line? And

1:18:00.320 --> 1:18:07.040
 I wish there was a way to quantify these experiences. Like, are they, I can tell in the moment if

1:18:07.040 --> 1:18:11.840
 they're engaging, right? I can tell. But it's really hard to know if they're going to remember

1:18:11.840 --> 1:18:18.160
 them 10 years from now or if it's going to. Yeah, that one is weird because it seems like kids

1:18:18.160 --> 1:18:22.960
 remember the weirdest things. I've seen parents do incredible stuff with their kids and they don't

1:18:22.960 --> 1:18:28.880
 remember any of that. They remember some tiny, small, sweet thing a parent did. Right. Like some,

1:18:28.880 --> 1:18:34.880
 I took you to like this amazing country. Yeah, whatever. And then they'll be like some,

1:18:34.880 --> 1:18:40.000
 like stuffed toy you got or some or the new PlayStation or something or some, some silly

1:18:40.000 --> 1:18:45.920
 little thing. So I think they just like that they were designed that way that you want to mess with

1:18:45.920 --> 1:18:54.960
 your head. But definitely kids are very impacted by it seems like sort of negative events. So

1:18:54.960 --> 1:19:01.200
 minimizing the number of negative events is important, but not too much, right? You can't,

1:19:01.200 --> 1:19:06.400
 you can't just like, you know, there's still discipline and challenge and all those kinds of

1:19:06.400 --> 1:19:11.280
 things. So you want some adversity for sure. So yeah, I mean, I'm definitely when I have kids,

1:19:11.280 --> 1:19:17.040
 I'm going to drive them out into the woods. Okay. And then they have to survive and make,

1:19:17.040 --> 1:19:22.960
 figure out how to make their way back home, like 20 miles out. Okay. Yeah. And after that, we can go

1:19:22.960 --> 1:19:28.800
 for ice cream. Anyway, I'm working on this whole parenting thing. I haven't figured it out. Okay.

1:19:28.800 --> 1:19:35.440
 Okay. What were we talking about? Yes, effective at the, the, the problem of emotion,

1:19:37.920 --> 1:19:42.240
 of emotion detection. So there's some people, maybe we can just speak to that a little more,

1:19:42.240 --> 1:19:47.600
 where there, there's folks like Lisa Feldman Barrett that, that challenged this idea that

1:19:47.600 --> 1:19:55.440
 emotion could be fully detected, or even well detected from the human face that there's so

1:19:55.440 --> 1:20:01.680
 much more to emotion. What do you think about ideas like hers, criticism like hers? Yeah,

1:20:01.680 --> 1:20:09.120
 I actually agree with a lot of Lisa's criticism. So even, even my PhD worked like 20 plus years

1:20:09.120 --> 1:20:15.440
 ago now. Time flies when you're having fun. I know, right? That was back when I did like

1:20:16.000 --> 1:20:21.360
 dynamic Bayesian networks and that was before deep learning. That was before deep learning.

1:20:21.360 --> 1:20:29.600
 Yeah. Yeah. I know. My day. Now you can just like use. Yeah. It's all, it's all the same

1:20:29.600 --> 1:20:36.320
 architecture. You can apply it to anything. Yeah. Right. Right. But yeah, but, but even then I kind

1:20:36.320 --> 1:20:41.360
 of, I, I did not subscribe to this like theory of basic emotions where it's just the simplistic

1:20:41.360 --> 1:20:45.120
 mapping one to one mapping between facial expressions and emotions. I actually think also,

1:20:45.840 --> 1:20:50.560
 we're not in, in the business of trying to identify your true emotional internal state. We

1:20:50.560 --> 1:20:56.240
 just want to quantify in an objective way what's showing on your face because that's an important

1:20:56.240 --> 1:21:03.200
 signal. It doesn't mean it's a true reflection of your internal emotional state. So I think a lot

1:21:03.200 --> 1:21:08.640
 of the, you know, I think she's, she's just trying to kind of highlight that this is not a simple

1:21:08.640 --> 1:21:16.640
 problem and overly simplistic solutions are going to hurt the industry. And I subscribe to that.

1:21:16.640 --> 1:21:21.680
 And I think multimodal is the way to go. Like whether it's additional context information

1:21:21.680 --> 1:21:25.040
 or different modalities and channels of information, I think that's what we,

1:21:26.560 --> 1:21:30.640
 that's where we ought to go. And I think, I mean, that's a big part of what she's advocating for

1:21:30.640 --> 1:21:36.080
 as well. So, but there is signal in the human face. That's just definitely signal. That's a

1:21:36.080 --> 1:21:44.880
 projection of emotion. There's that, that there, at least in part is the inner state is captured

1:21:44.880 --> 1:21:51.440
 in some meaningful way on the human face. I think it can sometimes be a reflection or

1:21:53.600 --> 1:21:58.880
 an expression of your internal state, but sometimes it's a social signal. So the, so

1:21:58.880 --> 1:22:04.240
 you cannot look at the face as purely a signal of emotion. It can be a signal of cognition and it

1:22:04.240 --> 1:22:11.200
 can be a signal of a social expression. And I think to disambiguate that, we have to be

1:22:11.200 --> 1:22:17.200
 careful about it and we have to add initial information. Humans are fascinating, aren't

1:22:17.200 --> 1:22:23.760
 they? With the whole face thing, this can mean so many things from humor to sarcasm to everything,

1:22:23.760 --> 1:22:27.120
 the whole thing. Some things we can help, some things we can't help at all.

1:22:28.320 --> 1:22:33.680
 In all the years of leading effectiva and emotion recognition company, like we talked about,

1:22:33.680 --> 1:22:44.240
 what have you learned about emotion, about humans and about AI? Be big sweeping questions.

1:22:44.240 --> 1:22:50.720
 Yeah, that's a big sweeping question. Well, I think the thing I learned the most is that even

1:22:50.720 --> 1:23:00.160
 though like we are in the business of building AI basically, right? It always goes back to the

1:23:00.160 --> 1:23:07.200
 humans, right? It's always about the humans. And so for example, the thing I'm most proud of

1:23:08.560 --> 1:23:15.040
 in building affectiva and yeah, the thing I'm most proud of on this journey,

1:23:16.080 --> 1:23:20.400
 I love the technology and I'm so proud of the solutions we've built and we've brought to market,

1:23:21.040 --> 1:23:26.400
 but I'm actually most proud of the people we've like built and cultivated at the company and

1:23:26.400 --> 1:23:32.080
 the culture we've created. You know, some of the people who've joined affectiva,

1:23:32.080 --> 1:23:39.680
 this was their first job. And while at affectiva, they became American citizens and they bought

1:23:39.680 --> 1:23:45.440
 their first house and they found their partner and they had their first kid, right? Like key moments

1:23:45.440 --> 1:23:52.160
 in life that we got to be part of. And that's the thing I'm most proud of.

1:23:52.160 --> 1:23:56.640
 So that's a great thing at a company that works and I'm most proud of that, right? Me,

1:23:56.640 --> 1:24:00.720
 like celebrating humanity in general, broadly speaking. And that's a great thing to have in

1:24:00.720 --> 1:24:06.800
 a company that works on AI because that's not often the thing that's celebrated in AI companies.

1:24:06.800 --> 1:24:12.880
 So often just raw, great engineering, just celebrating the humanity. That's great. And

1:24:12.880 --> 1:24:19.680
 especially from a leadership position. Well, what do you think about the movie,

1:24:19.680 --> 1:24:24.240
 her? Let me ask you that before I talk, before I talk to you about it, because it's not

1:24:24.240 --> 1:24:30.160
 affectiva is and was not just about emotion. So I'd love to talk to you about smart eye.

1:24:30.160 --> 1:24:38.240
 But before that, let me just jump into the movie. Her, do you think we'll have a deep

1:24:38.240 --> 1:24:43.120
 meaningful connection with increasingly deep and meaningful connections with computers?

1:24:43.680 --> 1:24:48.160
 Is that a compelling thing to you? Something that's already happening? The thing I love them,

1:24:48.160 --> 1:24:53.200
 I love the movie her, by the way. But the thing I love the most about this movie is it demonstrates

1:24:53.200 --> 1:24:58.560
 how technology can be a conduit for positive behavior change. So I forgot the guy's name in

1:24:58.560 --> 1:25:05.520
 the movie, whatever. Theodore. Theodore. So Theodore was like really depressed, right? And

1:25:05.520 --> 1:25:12.640
 he just didn't want to get out of bed. He was just like done with life, right? And Samantha, right?

1:25:12.640 --> 1:25:17.200
 Samantha, yeah. She just knew him so well. She had, she was emotionally intelligent.

1:25:17.200 --> 1:25:22.160
 And so she could persuade him and motivate him to change his behavior. And she got a man,

1:25:22.160 --> 1:25:27.760
 and they went to the beach together. And I think that represents the promise of emotion AI. If done

1:25:27.760 --> 1:25:34.800
 well, this technology can help us live happier lives, more productive lives, healthier lives,

1:25:34.800 --> 1:25:40.640
 more connected lives. So that's the part that I love about the movie, obviously. It's Hollywood,

1:25:40.640 --> 1:25:48.000
 so it takes a twist and whatever. But the key notion that technology with emotion AI can

1:25:48.000 --> 1:25:51.360
 persuade you to be a better version of who you are, I think that's awesome.

1:25:52.640 --> 1:25:58.000
 Well, what about the twist? You don't think it's good for spoiler alert

1:25:58.640 --> 1:26:04.560
 that Samantha starts feeling a bit of a distance and basically leaves Theodore?

1:26:04.560 --> 1:26:10.560
 You don't think that's a good feature? That's a, you think that's a bugger feature?

1:26:11.440 --> 1:26:15.600
 Well, I think what went wrong is Theodore became really attached to Samantha. Like,

1:26:15.600 --> 1:26:18.080
 I think he kind of fell in love with, you think that's wrong?

1:26:19.360 --> 1:26:24.640
 I mean, I think that, I think she was putting out the signal. This is an intimate relationship,

1:26:25.200 --> 1:26:30.080
 right? There was a deep intimacy to it. Right. But what does, what does, what does that mean?

1:26:30.080 --> 1:26:34.400
 What does that mean? An AI system. Right. What does that mean? Right? We're just friends.

1:26:35.280 --> 1:26:42.320
 Yeah, we're just friends. Well, I think when he realized, which is such a human thing of

1:26:42.320 --> 1:26:46.880
 jealousy, when you realized that Samantha was talking to like thousands of people,

1:26:46.880 --> 1:26:50.080
 she's parallel dating. Yeah, that did not go well. Right.

1:26:51.440 --> 1:26:55.040
 You know, that doesn't, and from a computer perspective, like that doesn't take

1:26:55.040 --> 1:27:01.360
 anything away from what we have. It's like you getting jealous of Windows 98 for being used

1:27:01.360 --> 1:27:07.840
 by millions of people. But it's like, it's like not liking that Alexa talks to a bunch of, you

1:27:07.840 --> 1:27:14.880
 know, other families. But I think Alexa currently is just a servant. It, it tells you about the

1:27:14.880 --> 1:27:19.360
 weather. It's not, it doesn't do the intimate deep connection. And I think there is something

1:27:19.360 --> 1:27:25.520
 there is something really powerful about that the intimacy of a connection with an AI system

1:27:26.240 --> 1:27:33.840
 that would have to respect and play the human game of, of jealousy, of love, of heartbreak and

1:27:33.840 --> 1:27:42.640
 all that kind of stuff, which Samantha does seem to be pretty good at. I think she, this AI systems

1:27:42.640 --> 1:27:47.280
 knows what it's doing. Well, actually, let me ask you this. I don't think she was talking to anyone

1:27:47.280 --> 1:27:53.920
 else. You don't think so? You think she was just done with Theodore? Yeah. Oh, yeah. And then,

1:27:53.920 --> 1:27:58.720
 and she, she wanted to really put the screen. She didn't have the guts to just break it off

1:27:58.720 --> 1:28:04.880
 cleanly. Okay. She wanted to put in pain. No, I don't know. Well, she could have ghosted him.

1:28:04.880 --> 1:28:13.440
 She could have. Right. I'm sorry. There's our engineers. Oh God. But I think those are really,

1:28:13.440 --> 1:28:18.400
 I honestly think some of that, some of it is Hollywood, but some of that is features from

1:28:18.400 --> 1:28:23.600
 an engineering perspective, not a bug. I think AI systems that can leave us,

1:28:24.160 --> 1:28:31.280
 now this is for more social robotics than it is for anything that's useful. Like I hate it

1:28:31.280 --> 1:28:35.840
 if Wikipedia said, you know, I need a break right now. Right, right, right, right. I'm like, no,

1:28:35.840 --> 1:28:45.600
 no, I need you. But if it's just purely for companionship, then I think the ability to

1:28:45.600 --> 1:28:52.320
 leave is really powerful. I don't know. I never thought of that. So that's so, so fascinating

1:28:52.320 --> 1:28:58.400
 because I've always taken the human perspective, right? Like for example, we had a Jibo at home,

1:28:58.400 --> 1:29:03.920
 right? And my son loved it. And then the company ran out of money. And so they had to basically

1:29:03.920 --> 1:29:10.320
 shut down, like Jibo basically died, right? And it was so interesting to me because we have a lot

1:29:10.320 --> 1:29:16.880
 of gadgets at home and a lot of them break and my son never cares about it, right? Like if our

1:29:16.880 --> 1:29:22.000
 Alexa stopped working tomorrow, I don't think he'd really care. But when Jibo stopped working,

1:29:22.000 --> 1:29:28.480
 it was traumatic. Like he got really upset. And as a parent, that like made me think about this

1:29:28.480 --> 1:29:33.440
 deeply, right? Did I, was I comfortable with that? I like the connection they had because I

1:29:33.440 --> 1:29:41.040
 think it was a positive relationship. But I was surprised that it affected him emotionally so

1:29:41.040 --> 1:29:48.080
 much. And I think there's a broader question here, right? As we build socially and emotionally

1:29:48.080 --> 1:29:53.600
 intelligent machines, what does that mean about our relationship with them? And then we're

1:29:53.600 --> 1:29:57.760
 broadly our relationship with one another, right? Because this machine is going to be programmed

1:29:57.760 --> 1:30:03.920
 to be amazing at empathy, by definition, right? It's going to always be there for you. It's not

1:30:03.920 --> 1:30:11.360
 going to get bored. In fact, there's a chat bot in China, Shao Ice. And it's like the number

1:30:11.360 --> 1:30:17.520
 two or three most popular app. And it basically is just a confidant. And you can tell it anything

1:30:17.520 --> 1:30:27.120
 you want. And people use it for all sorts of things. They confide in like domestic violence

1:30:27.120 --> 1:30:32.720
 or suicidal attempts or, you know, if they have challenges at work, I don't know what that,

1:30:33.440 --> 1:30:37.120
 I don't know if I'm, I don't know how I feel about that. I think about that a lot.

1:30:37.120 --> 1:30:43.520
 Yeah, I think first of all, obviously the future in my perspective. Second of all, I think there's

1:30:43.520 --> 1:30:49.600
 a lot of trajectories that that becomes an exciting future. But I think everyone should feel very

1:30:49.600 --> 1:30:56.080
 uncomfortable about how much they know about the company, about where the data is going,

1:30:56.080 --> 1:31:01.040
 how the data is being collected. Because I think, and this is one of the lessons of social media,

1:31:01.840 --> 1:31:06.480
 that I think we should demand full control and transparency of the data on those things.

1:31:06.480 --> 1:31:12.080
 Plus one, totally agree. Yeah. So like, I think it's really empowering, as long as you can walk

1:31:12.080 --> 1:31:18.560
 away. As long as you can like delete the data or know how the data, it's a opt in or, or at least

1:31:18.560 --> 1:31:24.320
 the clarity of like what is being used for the company. And I think as CEO or like leaders

1:31:24.320 --> 1:31:29.920
 are also important about that. Like you need to be able to trust the basic humanity of the leader.

1:31:29.920 --> 1:31:36.240
 Exactly. And also that that leader is not going to be a puppet of a larger machine,

1:31:36.240 --> 1:31:42.080
 but they actually have a significant role in defining the culture and the way the company

1:31:42.080 --> 1:31:49.520
 operates. So anyway, but we should be, we should definitely scrutinize companies on that aspect.

1:31:49.520 --> 1:31:57.200
 But this, I'm personally excited about that future, but also even if you're not, it's coming.

1:31:57.200 --> 1:32:01.920
 So let's figure out how to do it in the least painful and the most positive way.

1:32:01.920 --> 1:32:08.240
 That's great. You're the deputy CEO of Smart Eye. Can you describe the mission of the company?

1:32:08.240 --> 1:32:14.480
 What is Smart Eye? Yeah. So Smart Eye is a Swedish company. They've been in business for the last 20

1:32:14.480 --> 1:32:20.880
 years. And their, their main focus, like the industry they're most focused on is the automotive

1:32:20.880 --> 1:32:29.040
 industry. So bringing driver monitoring systems to basically save lives, right? So I first met

1:32:29.040 --> 1:32:36.080
 the CEO, Martin Krantz. Gosh, it was right when COVID hit. It was actually the last, the last

1:32:36.080 --> 1:32:43.040
 CES right before COVID. So CS 2020, right? 2020, January. Yeah, January. Exactly. So we were there,

1:32:43.040 --> 1:32:49.680
 met him in person. He's basically, we were competing with each other. I think the difference was

1:32:49.680 --> 1:32:54.480
 they'd been doing driver monitoring and had a lot of credibility in the automotive space. We

1:32:54.480 --> 1:32:58.480
 didn't come from the automotive space, but we were using new technology like deep learning

1:32:59.120 --> 1:33:04.000
 and building this emotion recognition. And you wanted to enter the automotive space. You wanted

1:33:04.000 --> 1:33:08.720
 to operate in the automotive space. Exactly. It was one of the areas we were, we had just raised

1:33:08.720 --> 1:33:14.240
 around a funding to focus on bringing our technology to the automotive industry. So we met and

1:33:14.240 --> 1:33:19.120
 honestly, it was the first, it was the only time I met with a CEO who had the same vision

1:33:19.120 --> 1:33:23.120
 as I did. Like he basically said, yeah, our vision is to bridge the gap between humans and machines.

1:33:23.120 --> 1:33:29.120
 I was like, oh my God, this is like exactly almost to the, to the word, you know, how we describe it

1:33:29.120 --> 1:33:35.920
 too. And we started talking and first it was about, okay, can we align strategically here? Like how

1:33:35.920 --> 1:33:40.640
 can we work together? Cause we're competing, but we're also like complimentary. And then

1:33:41.680 --> 1:33:46.720
 I think after four months of speaking almost every day on FaceTime, he was like,

1:33:47.520 --> 1:33:52.400
 is your company interested in acquisition? And it was the first, I usually say no when people

1:33:52.400 --> 1:33:58.720
 approach us. It was the first time that I was like, huh, yeah, I might be interested. Let's talk.

1:33:58.720 --> 1:34:05.920
 Yeah. So you just hit it off. Yeah. So they're, they're a respected, very respected in the automotive

1:34:05.920 --> 1:34:13.280
 sector of like delivering products and increasingly sort of better and better and better for, I mean,

1:34:13.280 --> 1:34:17.680
 maybe you could speak to that, but it's the driver's sense. And for basically having a device that's

1:34:17.680 --> 1:34:23.360
 looking at the driver and it's able to tell you where the driver is looking. Correct. It's able

1:34:23.360 --> 1:34:28.800
 to also drowsiness stuff. Correct. It does stuff from the face in the eye. Exactly. Like it's

1:34:28.800 --> 1:34:33.840
 monitoring driver distraction and drowsiness, but they bought us so that we could expand beyond

1:34:33.840 --> 1:34:39.440
 just the driver. So the driver monitoring systems usually sit, the camera sits in the steering

1:34:39.440 --> 1:34:44.240
 wheel car or around the steering wheel column and it looks directly at the driver. But now

1:34:44.240 --> 1:34:50.320
 we've migrated the camera position and partnership with car companies to the rear view mirror

1:34:50.320 --> 1:34:55.600
 position. So it has a full view of the entire cabin of the car. And you can detect how many

1:34:55.600 --> 1:35:01.440
 people are in the car. What are they doing? So we do activity detection, like eating or drinking or

1:35:02.000 --> 1:35:08.720
 in some regions of the world smoking. We can detect if a baby's in the car seat, right? And if,

1:35:09.520 --> 1:35:13.840
 unfortunately, in some cases they're forgotten, parents just leave the car and forget the kid

1:35:13.840 --> 1:35:18.960
 in the car. That's an easy computer vision problem to solve, right? You can detect there's a car seat,

1:35:18.960 --> 1:35:25.440
 there's a baby, you can text the parent and hopefully, again, save lives. So that was the

1:35:25.440 --> 1:35:32.960
 impetus for the acquisition. It's been a year. I mean, there's a lot of questions. It's a really

1:35:32.960 --> 1:35:38.800
 exciting space, especially to me. I just find it a fascinating problem. It could enrich the

1:35:38.800 --> 1:35:45.440
 experience in the car in so many ways, especially because we spend still, despite COVID, I mean,

1:35:45.440 --> 1:35:49.760
 COVID changed things. So it's in interesting ways. But I think the world is bouncing back and we

1:35:49.760 --> 1:35:56.960
 spend so much time in the car and the car is such a weird little world we'll have for ourselves.

1:35:56.960 --> 1:36:02.880
 Like people do all kinds of different stuff, like listen to podcasts, they think about stuff,

1:36:02.880 --> 1:36:12.240
 they get angry, they do phone calls. It's like a little world of its own with a kind of privacy

1:36:12.240 --> 1:36:20.800
 that for many people, they don't get anywhere else. And it's a little box that's like a psychology

1:36:20.800 --> 1:36:27.760
 experiment because it feels like the angriest many humans in this world get is inside the car.

1:36:28.320 --> 1:36:33.840
 It's so interesting. So it's such an opportunity to explore how we can enrich,

1:36:35.200 --> 1:36:42.000
 how companies can enrich that experience. And also as the cars get become more and more automated,

1:36:42.000 --> 1:36:46.960
 there's more and more opportunity, the variety of activities that you can do in the car increases.

1:36:46.960 --> 1:36:53.600
 So it's super interesting. So I mean, in a practical sense, the Smart Eye has been selected.

1:36:54.160 --> 1:37:01.120
 At least I read by 14 of the world's leading car manufacturers for 94 car models. So

1:37:01.120 --> 1:37:07.680
 it's in a lot of cars. How hard is it to work with car companies? So they're all different.

1:37:07.680 --> 1:37:13.520
 They all have different needs. The ones I've gotten a chance to interact with are very focused on

1:37:13.520 --> 1:37:22.320
 cost. And anyone who's focused on cost, it's like, all right, do you hate fun?

1:37:24.240 --> 1:37:28.160
 Let's just have some fun. Let's figure out the most fun thing we can do in the worry

1:37:28.160 --> 1:37:33.360
 about cost later. But I think because the way the car industry works, I mean, it's a very

1:37:33.360 --> 1:37:38.080
 thin margin that you get to operate under. So you have to really, really make sure that

1:37:38.640 --> 1:37:44.960
 everything you add to the car makes sense financially. So anyway, is this new industry,

1:37:44.960 --> 1:37:50.400
 especially at this scale of Smart Eye, does it hold any lessons for you?

1:37:51.200 --> 1:37:56.800
 Yeah, I think it is a very tough market to penetrate. But once you're in, it's awesome.

1:37:56.800 --> 1:38:00.880
 Because once you're in, you're designed into these car models for like somewhere between

1:38:00.880 --> 1:38:05.760
 five to seven years, which is awesome. And once they're on the road, you just get paid a royalty

1:38:05.760 --> 1:38:12.240
 fee per vehicle. So it's a high barrier to entry. But once you're in, it's amazing. I think the thing

1:38:12.240 --> 1:38:18.480
 that I struggle the most with in this industry is the time to market. So often we're asked to lock

1:38:18.480 --> 1:38:23.920
 or do a code freeze. Two years before the car is going to be on the road, I'm like, guys,

1:38:23.920 --> 1:38:30.320
 do you understand the pace with which technology moves? So I think car companies are really trying

1:38:30.320 --> 1:38:36.880
 to make the Tesla, the Tesla transition to become more of a software driven

1:38:38.400 --> 1:38:42.400
 architecture. And that's hard for many. It's just the cultural change. I mean,

1:38:42.400 --> 1:38:45.360
 I'm sure you've experienced that, right? Oh, definitely. I think one of the biggest

1:38:46.560 --> 1:38:54.400
 inventions or imperatives created by Tesla is like to me personally, okay, people are going to

1:38:54.400 --> 1:39:01.840
 complain about this, but I know electric vehicle, I know autopilot AI stuff. To me, the software

1:39:01.840 --> 1:39:08.960
 over there, software updates is like the biggest revolution in cars. And it is extremely difficult

1:39:08.960 --> 1:39:14.800
 to switch to that because it is a culture shift. It at first, especially if you're not comfortable

1:39:14.800 --> 1:39:22.080
 with it, it seems dangerous. Like there's an approach to cars is so safety focused for so

1:39:22.080 --> 1:39:28.880
 many decades. They're like, what do you mean we dynamically change code? The whole point

1:39:28.880 --> 1:39:37.200
 is you have a thing that you test, like, right? And like, it's not reliable. Because do you know

1:39:37.200 --> 1:39:43.120
 how much you cause if we have to recall this cars? Right? There's a there's a and there's an

1:39:43.120 --> 1:39:48.960
 understandable obsession with safety. But the downside of an obsession with safety

1:39:48.960 --> 1:39:56.880
 is the same as with being obsessed with safety as a parent. Is like, if you do that too much, you

1:39:56.880 --> 1:40:02.480
 limit the potential development and the flourishing of in that particular aspect human being, but

1:40:02.480 --> 1:40:08.880
 in this particular aspect, the software, the artificial neural network of it. And but it's

1:40:08.880 --> 1:40:13.520
 tough to do. It's really tough to do culturally and technically, like the deployment, the mass

1:40:13.520 --> 1:40:18.160
 deployment of software is really, really difficult. But I hope that's where the industry is doing

1:40:18.160 --> 1:40:22.560
 one of the reasons I really want Tesla to succeed is exact about that point, not autopilot,

1:40:22.560 --> 1:40:29.200
 not the electrical vehicle, but the softwareization of basically everything but cars,

1:40:29.200 --> 1:40:34.560
 especially because to me, that's actually going to increase two things, increase safety,

1:40:34.560 --> 1:40:38.640
 because you can update much faster, but also increase the effectiveness of

1:40:39.840 --> 1:40:46.000
 folks like you who dream about enriching the human experience with AI, because you can just like

1:40:46.000 --> 1:40:51.360
 because there's a feature like you want like a new emoji or whatever, like the way TikTok

1:40:51.360 --> 1:40:57.600
 releases filters, you can just release that for in car in car stuff. So but yeah, that that's

1:40:57.600 --> 1:41:05.040
 definitely one of the use cases we're looking into is once you know the sentiment of the passengers

1:41:05.040 --> 1:41:10.240
 in the vehicle, you can optimize the temperature in the car, you can change the lighting, right?

1:41:10.240 --> 1:41:14.960
 So if the backseat passengers are falling asleep, you can dim the lights, you can lower the music,

1:41:14.960 --> 1:41:19.680
 right? You can do all sorts of things. Yeah. I mean, of course, you could do that kind of

1:41:19.680 --> 1:41:28.720
 stuff with a two year delay, but it's tougher. Yeah. Do you think, do you think Tesla or Waymo

1:41:28.720 --> 1:41:34.480
 or some of these companies that are doing semi or fully autonomous driving should be doing

1:41:34.480 --> 1:41:40.800
 driver sensing? Yes. Are you thinking about that kind of stuff? So not just how we can enhance

1:41:40.800 --> 1:41:45.920
 the in cab experience for cars that are male and driven, but the ones that are increasingly

1:41:45.920 --> 1:41:51.840
 more autonomously driven? Yeah. So if we fast forward to the universe where it's fully autonomous,

1:41:51.840 --> 1:41:56.080
 I think interior sensing becomes extremely important because the role of the driver

1:41:56.080 --> 1:42:01.760
 isn't just to drive. If you think about it, the driver almost manages the dynamics within a vehicle.

1:42:01.760 --> 1:42:08.400
 And so who's going to play that role when it's an autonomous car? We want a solution that is

1:42:08.400 --> 1:42:13.680
 able to say, oh my God, Lex is bored to death because the car is moving way too slow. Let's

1:42:13.680 --> 1:42:19.280
 engage Lex. Or Rana is freaking out because she doesn't trust this vehicle yet. So let's tell Rana

1:42:19.280 --> 1:42:25.040
 a little bit more information about the route. So I think, or somebody's having a heart attack

1:42:25.040 --> 1:42:30.480
 in the car, you need interior sensing in fully autonomous vehicles. But with semi autonomous

1:42:30.480 --> 1:42:36.400
 vehicles, I think it's really key to have driver monitoring because semi autonomous means that

1:42:36.400 --> 1:42:41.360
 sometimes the car is in charge, sometimes the driver is in charge or the co pilot, right? And

1:42:41.360 --> 1:42:46.880
 you need both systems to be on the same page. You need to know, the car needs to know if the

1:42:46.880 --> 1:42:53.680
 driver's asleep before it transitions control over to the driver. And sometimes if the driver's too

1:42:53.680 --> 1:42:58.160
 tired, the car can say, I'm going to be a better driver than you are right now. I'm taking control

1:42:58.160 --> 1:43:03.200
 over. So this dynamic, this dance, it's so key and you can't do that without driver sensing.

1:43:03.200 --> 1:43:07.360
 Yeah, there's a disagreement for the longest time I've had with Elon that this is obvious,

1:43:07.360 --> 1:43:13.040
 that this should be in the Tesla from day one. And it's obvious that driver sensing is not a

1:43:13.040 --> 1:43:20.800
 hindrance. It's not obvious. I should be careful because having studied this problem, nothing

1:43:20.800 --> 1:43:26.320
 is really obvious, but it seems very likely driver sensing is not a hindrance to an experience.

1:43:26.320 --> 1:43:35.120
 It's only enriching to the experience and likely increases the safety. That said,

1:43:36.000 --> 1:43:43.600
 it is very surprising to me just having studied semi autonomous driving how well humans are able

1:43:43.600 --> 1:43:49.040
 to manage that dance. Because it was the intuition before you were doing that kind of thing that

1:43:49.040 --> 1:43:55.760
 humans will become just incredibly distracted. They would just let the thing do its thing.

1:43:57.600 --> 1:44:02.240
 Because it is life and death and they're able to manage that somehow. But that said, there's no

1:44:02.240 --> 1:44:08.560
 reason not to have driver sensing on top of that. I feel like that's going to allow you to do that

1:44:08.560 --> 1:44:14.000
 dance that you're currently doing without driver sensing, except the steering wheel,

1:44:14.000 --> 1:44:19.280
 to do that even better. I mean, the possibilities are endless and the machine learning possibilities

1:44:19.280 --> 1:44:25.200
 are endless. It's such a beautiful, it's also constrained environment. So you could do much

1:44:25.200 --> 1:44:29.760
 more effectively than you can with the external environment. External environment is full of

1:44:30.720 --> 1:44:35.680
 weird edge cases and complexities. There's so much, it's so fascinating, such a fascinating

1:44:35.680 --> 1:44:44.320
 world. I do hope that companies like Tesla and others, even Waymo, which I don't even know if

1:44:44.320 --> 1:44:51.920
 Waymo is doing anything sophisticated inside the cab. I don't think so. What is it? I honestly

1:44:51.920 --> 1:44:58.400
 think it goes back to the robotics thing we were talking about, which is like great engineers

1:44:58.400 --> 1:45:04.640
 that are building these AI systems just are afraid of the human being. And not thinking about the

1:45:04.640 --> 1:45:10.000
 human experience, they're thinking about the features and the perceptual abilities of that

1:45:10.000 --> 1:45:16.720
 thing. They think the best way I can serve the human is by doing the best perception and control

1:45:16.720 --> 1:45:21.600
 I can, by looking at the external environment, keeping the human safe. But there's a huge,

1:45:21.600 --> 1:45:33.440
 I'm here. I need to be noticed and interacted with and understood and all those kinds of things,

1:45:33.440 --> 1:45:36.960
 even just in a personal level for entertainment, honestly, for entertainment.

1:45:38.400 --> 1:45:42.640
 You know, one of the coolest work we did in collaboration with MIT around this was we looked

1:45:42.640 --> 1:45:53.600
 at longitudinal data, right? Because MIT had access to tons of data. And just seeing the

1:45:53.600 --> 1:45:58.640
 patterns of people, like driving in the morning off to work versus commuting back from work,

1:45:58.640 --> 1:46:04.960
 or weekend driving versus weekday driving. And wouldn't it be so cool if your car knew that

1:46:04.960 --> 1:46:11.520
 and then was able to optimize either the route or the experience or even make recommendations?

1:46:12.240 --> 1:46:16.960
 I think it's very powerful. Yeah, like, why are you taking this route? You're always unhappy when

1:46:16.960 --> 1:46:21.040
 you take this route. And you're always happy when you take this alternative route. Take that route

1:46:21.040 --> 1:46:27.120
 instead. Exactly. I mean, that if to have that even that little step of relationship with a car,

1:46:27.120 --> 1:46:31.920
 I think is incredible. Of course, you have to get the privacy, right? You have to get all that kind

1:46:31.920 --> 1:46:37.440
 of stuff, right? But I wish I honestly, you know, people are paranoid about this, but I would like

1:46:37.440 --> 1:46:45.520
 a smart refrigerator. We have such a deep connection with food as a human civilization. I would like

1:46:45.520 --> 1:46:52.320
 to have a refrigerator that would understand me that, you know, I also have a complex

1:46:52.320 --> 1:46:57.280
 relationship with food because like, you know, pig out too easily and all that kind of stuff. So

1:46:57.280 --> 1:47:02.160
 you try, you know, like, maybe I want the refrigerator to be like, are you sure about

1:47:02.160 --> 1:47:06.320
 this? Because maybe you're just feeling down or tired. Like maybe, maybe let's leave off.

1:47:06.320 --> 1:47:10.000
 Your vision of the smart refrigerator is way kinder than mine.

1:47:10.000 --> 1:47:16.720
 Is it just me yelling at you? No, it was just because I don't, I don't, you know, I don't drink

1:47:16.720 --> 1:47:22.960
 alcohol, I don't smoke, but I eat a ton of chocolate, like it sticks to my vice. And so I, and, and

1:47:22.960 --> 1:47:28.080
 sometimes I scream too. And I'm like, okay, my smart refrigerator will just lock, lock down.

1:47:28.080 --> 1:47:31.200
 They'll just say, dude, you've had way too many today, like, yeah.

1:47:32.640 --> 1:47:41.280
 Yeah. No, but here's the thing. Are you, do you regret having like, let's say, not the next day,

1:47:41.280 --> 1:47:48.000
 but 30 days later, would you, what would you, what would you like to the refrigerator to have done

1:47:48.000 --> 1:47:54.320
 then? Well, I think actually, like the more positive relationship would be one where there's

1:47:54.320 --> 1:48:00.640
 a conversation, right? As opposed to like four, that's probably like the more sustainable relationship.

1:48:00.640 --> 1:48:05.120
 It's like late, late at night, just no, listen, listen, I know I told you an hour ago,

1:48:05.920 --> 1:48:11.120
 that this is not a good idea, but just listen, things have changed. I can just imagine a bunch

1:48:11.120 --> 1:48:17.360
 of stuff being made up just to convince. Oh my God, it's hilarious. But I mean, I just think that

1:48:17.360 --> 1:48:23.200
 there's opportunities there. I mean, maybe not locking down, but for our systems that are such

1:48:25.200 --> 1:48:32.960
 a deep part of our lives, like we use, we use a lot of us, a lot of people that commute use

1:48:32.960 --> 1:48:37.040
 their car every single day. A lot of us use a refrigerator every single day, the microwave

1:48:37.040 --> 1:48:45.680
 every single day. Like, and we just, like, I feel like certain things could be made more efficient,

1:48:45.680 --> 1:48:53.760
 more enriching, and AI is there to help. Like some just basic recognition of you as a human being

1:48:53.760 --> 1:48:57.840
 about your patterns, what makes you happy, not happy, and all that kind of stuff. And the car,

1:48:57.840 --> 1:49:03.200
 obviously, maybe, maybe, maybe we'll say, well, instead of this like,

1:49:03.200 --> 1:49:09.760
 like, Ben and Terry's ice cream, how about this hummus and carrots or something? I don't know.

1:49:11.600 --> 1:49:14.000
 Yeah, like a reminder. Just in time recommendation, right?

1:49:14.640 --> 1:49:20.240
 But not like a generic one, but a reminder that last time you chose the carrots,

1:49:20.880 --> 1:49:25.360
 you smiled 17 times more the next day. You were happier the next day, right?

1:49:25.360 --> 1:49:31.760
 Yeah, you were, you were happier the next day. And, and, but yeah, I don't, but then again,

1:49:31.760 --> 1:49:37.680
 if you're the kind of person that, that gets better from negative, negative comments, you could

1:49:37.680 --> 1:49:42.640
 say like, Hey, remember, like that wedding you're going to, you want to fit into that dress?

1:49:43.520 --> 1:49:49.360
 Remember about that? Let's think about that for your eating this. No, I don't know. It's for some,

1:49:49.360 --> 1:49:53.600
 probably that would work for me. Like a refrigerator that is just ruthless. It's

1:49:53.600 --> 1:50:00.320
 shaming me. But like, I would, of course, welcome it. Like, that would work for me. Just that that

1:50:00.320 --> 1:50:05.280
 well, I would know, I think it would, if it's really like smart, it would optimize its nudging

1:50:05.280 --> 1:50:09.440
 based on what works for you, right? Exactly. That's the whole point. Personalization,

1:50:09.440 --> 1:50:16.320
 in every way, deep personalization. You were a part of a webinar titled Advancing Road Safety,

1:50:16.320 --> 1:50:22.640
 the State of Alcohol and Toxication Research. So for people who don't know, every year 1.3

1:50:22.640 --> 1:50:28.960
 million people around the world die in road crashes. And more than 20% of these fatalities

1:50:28.960 --> 1:50:34.720
 are estimated to be alcohol related. A lot of them are also distraction related. So can AI help

1:50:34.720 --> 1:50:42.480
 with the alcohol thing? I think the answer is yes. There are signals, and we know that as humans,

1:50:42.480 --> 1:50:50.960
 like we can tell in a person, you know, is it different phases of being drunk? Right? Yeah.

1:50:50.960 --> 1:50:55.360
 And I think you can use technology to do the same. And again, I think the ultimate solution is going

1:50:55.360 --> 1:51:00.400
 to be a combination of different sensors. How hard is the problem from the vision perspective?

1:51:01.280 --> 1:51:05.200
 I think it's non trivial. I think it's non trivial. And I think the biggest part is

1:51:05.200 --> 1:51:10.880
 getting the data, right? It's like getting enough data examples. So we, for this research project,

1:51:10.880 --> 1:51:17.680
 we partnered with the Transportation Authorities of Sweden. And we literally had a race track

1:51:17.680 --> 1:51:25.840
 with a safety driver. And we basically progressively got people drunk. Nice. So but you know, that's

1:51:25.840 --> 1:51:31.600
 a very expensive data set to collect. And you want to collect it globally and in multiple conditions.

1:51:32.480 --> 1:51:36.320
 Yeah, the ethics of collecting a data set where people are drunk is tricky.

1:51:37.600 --> 1:51:44.400
 Which is funny because, I mean, let's put drunk driving aside, the number of drunk people in

1:51:44.400 --> 1:51:49.280
 the world every day is very large. It'd be nice to have a large data set of drunk people getting

1:51:49.280 --> 1:51:53.200
 progressively drunk. In fact, you could build an app where people can donate their data because

1:51:53.200 --> 1:52:00.000
 it's hilarious. Right. Actually, yeah, but the liability, liability, the ethics, how do you

1:52:00.000 --> 1:52:04.640
 get it right? It's tricky. It's really, really tricky. Because like drinking is one of those

1:52:04.640 --> 1:52:10.320
 things that's funny and hilarious and what loves it's social, the so on and so forth. But it's

1:52:10.320 --> 1:52:15.200
 also the thing that hurts a lot of people, like a lot of people. Like alcohol is one of those

1:52:15.200 --> 1:52:23.200
 things. It's legal, but it's really damaging to a lot of lives. It destroys lives. And not just in

1:52:24.240 --> 1:52:30.000
 driving context. I should mention, people should listen to Andrew Huberman who recently

1:52:30.000 --> 1:52:36.000
 talked about alcohol. He has an amazing pocket. Andrew Huberman is a neuroscientist from Stanford

1:52:36.000 --> 1:52:43.280
 and a good friend of mine. Oh, cool. And he's like a human encyclopedia about all health related

1:52:43.280 --> 1:52:48.080
 wisdom. So he does a podcast. You would love it. I would love that. No, no, no, no. Oh,

1:52:48.080 --> 1:52:53.040
 you don't know Andrew Huberman. Okay. Listen, you'll listen to Andrew. He's called Huberman Lab

1:52:53.040 --> 1:52:57.920
 Podcast. This is your assignment. Just listen to one. Okay. I guarantee you this will be a thing

1:52:57.920 --> 1:53:04.240
 where you say Lex, this is the greatest human I've ever discovered. So. Oh my God. Because I've

1:53:04.240 --> 1:53:09.440
 really, I'm really on a journey of kind of health and wellness and I'm learning lots and I'm trying

1:53:09.440 --> 1:53:16.160
 to like build these, I guess, atomic habits around just being healthy. So yeah, I'm definitely

1:53:16.160 --> 1:53:23.760
 going to do this. I'm gonna. His whole thing. This is great. He's a legit scientist, like

1:53:24.960 --> 1:53:31.600
 really well published. But in his podcast, what he does, he's not talking about his own work.

1:53:31.600 --> 1:53:37.440
 He's like a human encyclopedia of papers. And so he, his whole thing is he takes the topic

1:53:37.440 --> 1:53:43.280
 and in a very fast, you mentioned atomic habits, like very clear way, summarizes the research

1:53:44.080 --> 1:53:49.760
 in a way that leads to protocols of what you should do. He's really big on like, not like

1:53:49.760 --> 1:53:53.760
 this is what the science says, but like, this is literally what you should be doing according

1:53:53.760 --> 1:54:01.280
 to science. So like, he's really big and there's a lot of recommendations he does, which several of

1:54:01.280 --> 1:54:09.200
 them I definitely don't do, like get sunlight as soon as possible from waking up and like for

1:54:09.200 --> 1:54:13.760
 prolonged periods of time. That's a really big one. And he's, there's a lot of science behind

1:54:13.760 --> 1:54:18.240
 that one. There's a bunch of stuff like various systems. You're gonna, and you're gonna be like,

1:54:18.240 --> 1:54:23.680
 Lex, this is, this is my new favorite person, I guarantee it. And if you guys somehow don't know,

1:54:23.680 --> 1:54:28.800
 and you're human and you care about your well being, you know, you should definitely listen to

1:54:28.800 --> 1:54:39.280
 him. Love you, Andrew. Anyway, so what were we talking about? Oh, alcohol and detecting alcohol.

1:54:39.280 --> 1:54:42.000
 So this is a problem you care about and you're trying to solve.

1:54:42.000 --> 1:54:48.720
 And actually like broadening it, I do believe that the car is going to be a wellness center,

1:54:48.720 --> 1:54:53.360
 like, because again, imagine if you have a variety of sensors inside the vehicle,

1:54:53.360 --> 1:54:59.840
 tracking, not just your emotional state or level of distraction and drowsiness and drowsiness,

1:55:00.640 --> 1:55:05.280
 level of distraction, drowsiness and intoxication, but also maybe even things like

1:55:07.200 --> 1:55:11.120
 your physical, you know, your heart rate and your heart rate variability and your breathing rate.

1:55:13.680 --> 1:55:19.440
 And it can start like optimizing, yeah, it can optimize the ride based on what your goals are.

1:55:19.440 --> 1:55:23.600
 So I think we're going to start to see more of that. And I'm excited about that.

1:55:24.320 --> 1:55:28.640
 Yeah, what are the, what are the challenges you're tackling? Well, with Smart Eye currently,

1:55:28.640 --> 1:55:34.720
 what's like the, the trickiest things to get? Is it, is it basically convincing more and more

1:55:34.720 --> 1:55:39.120
 car companies that having AI inside the car is a good idea? Or is there some,

1:55:40.480 --> 1:55:47.040
 is there more technical algorithmic challenges? What's, what's been keeping you mentally busy?

1:55:47.040 --> 1:55:52.800
 I think a lot of the car companies we are in conversations with are already interested in

1:55:52.800 --> 1:55:57.840
 definitely driver monitoring. Like, I think it's becoming a must have, but even interior sensing,

1:55:57.840 --> 1:56:02.640
 I can see, like, we're engaged in a lot of like advanced engineering projects and proof of concepts.

1:56:04.080 --> 1:56:10.160
 I think technologically though, and even the technology, I can see a path to making it happen.

1:56:10.160 --> 1:56:16.080
 I think it's the use case. Like, how does the car respond once it knows something about you?

1:56:16.080 --> 1:56:19.440
 Because you want it to respond in a thoughtful way that doesn't,

1:56:19.440 --> 1:56:25.520
 that isn't off putting to the consumer in the car. So I think that's like the user experience.

1:56:25.520 --> 1:56:31.200
 I don't think we've really nailed that. And we usually, that's not part, we're the sensing

1:56:31.200 --> 1:56:36.160
 platform, but we usually collaborate with the car manufacturer to decide what the use case is. So,

1:56:36.160 --> 1:56:40.880
 so say you, you figure out that somebody's angry while driving. Okay, what should the car do?

1:56:40.880 --> 1:56:47.760
 You know, do you see yourself as a role of nudging, of like,

1:56:48.640 --> 1:56:54.080
 basically coming up with solutions, essentially, that, and then, and then the car manufacturers

1:56:54.080 --> 1:57:00.800
 kind of put their own little spin on it. Right. Like we, we, we are like the ideation creative

1:57:01.440 --> 1:57:06.080
 thought partner. But at the end of the day, the car company needs to decide what's on brand for

1:57:06.080 --> 1:57:11.680
 them, right? Like maybe when it figures out that you're distracted or drowsy, it shows you a coffee

1:57:11.680 --> 1:57:16.400
 cup, right? Or maybe it takes more aggressive behaviors and basically said, okay, if you don't

1:57:16.400 --> 1:57:20.320
 like take a rest in the next five minutes, the car is going to shut down, right? Like there's a whole

1:57:20.320 --> 1:57:27.280
 range of actions the car can take. And doing the thing that is most, yeah, that builds trust with

1:57:27.280 --> 1:57:32.160
 the driver and the passengers. I think that's what we need to be very careful about.

1:57:32.160 --> 1:57:37.920
 Yeah, car companies are funny because they have their own, like, I mean, that's why people get

1:57:37.920 --> 1:57:42.720
 cars still. I hope that changes, but they get it because it's a certain feel and look and

1:57:43.360 --> 1:57:51.520
 it's a certain, they become proud like Mercedes Benz or BMW or whatever. And that's their thing.

1:57:51.520 --> 1:57:56.720
 That's the family brand or something like that. Or Ford or GM, whatever, they, they stick to that

1:57:56.720 --> 1:58:04.000
 thing. It's interesting. So it should be, I don't know, it should be a little more about the technology

1:58:04.000 --> 1:58:12.480
 inside. And I suppose there too, there could be a branding like a very specific style of luxury

1:58:12.480 --> 1:58:20.640
 or fun, all that kind of stuff. Yeah. You know, I have an AI focused fund to invest in early stage

1:58:21.280 --> 1:58:25.360
 kind of AI driven companies. And one of the companies we're looking at is trying to do a

1:58:25.360 --> 1:58:31.040
 Tesla did, but for boats, for recreational boats. Yeah. So they're building an electric and kind

1:58:31.040 --> 1:58:37.760
 of slash autonomous boat. And it's kind of the same issues like, what kind of sensors can you put in?

1:58:38.320 --> 1:58:44.160
 What kind of states can you detect both exterior and interior within the boat? Anyways, it's

1:58:44.160 --> 1:58:51.120
 like really interesting. Do you boat at all? No, not well, not in that way. I do like to get on the

1:58:51.120 --> 1:58:59.360
 lake or a river and fish from a boat, but that's not boating. That's different. It's still boating.

1:58:59.360 --> 1:59:06.960
 Low tech, low tech boat. Get away from, get closer to nature, but I guess going out into the ocean

1:59:06.960 --> 1:59:13.600
 is also, is also getting closer to nature and some deep sense. I mean, I guess that's why people love

1:59:13.600 --> 1:59:22.960
 it. The enormity of the water just underneath you. Yeah. I love the water. I love both. I love

1:59:22.960 --> 1:59:28.080
 saltwater. It was like the big and just it's humbling to be in front of this giant thing

1:59:28.080 --> 1:59:33.680
 that's so powerful that was here before us and be here after. But I also love the piece of a small

1:59:33.680 --> 1:59:47.840
 like wooded lake. It's everything's calm. You tweeted that I'm excited about Amazon's acquisition

1:59:47.840 --> 1:59:54.160
 of iRobot. I think it's a super interesting, just given the trajectory of which you're part of,

1:59:54.160 --> 2:00:00.560
 of these honestly small number of companies that are playing in this space that are like trying to

2:00:00.560 --> 2:00:05.600
 have an impact on human beings. So it is an interesting moment in time that Amazon would

2:00:05.600 --> 2:00:14.240
 acquire iRobot. You tweet, I imagine a future where home robots are as ubiquitous as microwaves

2:00:14.240 --> 2:00:20.080
 or toasters. Here are three reasons why I think this is exciting. If you remember, I can look

2:00:20.080 --> 2:00:25.680
 it up. But what, why is this exciting to you? I mean, I think the first reason why this is exciting

2:00:25.680 --> 2:00:32.560
 to kind of remember the exact like order in which I put them. But one is just it's, it's going to be

2:00:32.560 --> 2:00:38.480
 an incredible platform for understanding our behaviors within the home, right? Like, you know,

2:00:38.480 --> 2:00:44.400
 if you think about Roomba, which is, you know, the robot vacuum cleaner, the flagship product of iRobot

2:00:44.400 --> 2:00:50.000
 at the moment, it's like running around your home, understanding the layout is understanding what's

2:00:50.000 --> 2:00:54.000
 clean and what's not, how often do you clean your house and all of these like behaviors

2:00:54.000 --> 2:00:59.680
 are a piece of the puzzle in terms of understanding who you are as a consumer. And I think that could

2:00:59.680 --> 2:01:06.720
 be, again, used in really meaningful ways, not just to recommend better products or whatever,

2:01:06.720 --> 2:01:10.880
 but actually to improve your experience as a human being. So I think, I think that's very

2:01:10.880 --> 2:01:18.640
 interesting. I think the natural evolution of these robots in the, in the home. So it's,

2:01:18.640 --> 2:01:25.360
 it's interesting. Roomba isn't really a social robot, right? At the moment. But I once interviewed

2:01:25.360 --> 2:01:30.560
 one of the chief engineers on the Roomba team, and he talked about how people named their Roombas.

2:01:31.120 --> 2:01:36.720
 And if their Roomba broke down, they would call in and say, you know, my Roomba broke down and

2:01:36.720 --> 2:01:40.560
 the company would say, well, we'll just send you a new one. And no, no, no, Rosie, like you have to

2:01:40.560 --> 2:01:47.920
 like, yeah, I want you to fix this particular robot. So people have already built like

2:01:47.920 --> 2:01:53.920
 interesting emotional connections with these home robots. And I think that again, that provides a

2:01:53.920 --> 2:01:59.440
 platform for really interesting things to, to just motivate change, like it could help you. I mean,

2:01:59.440 --> 2:02:06.480
 one of the companies that spun out of MIT, Catalia Health, the guy who started it spent a lot of

2:02:06.480 --> 2:02:11.120
 time building robots that help with weight management. So weight management, sleep, eating

2:02:11.120 --> 2:02:18.880
 better. Yeah, all of these things. Well, if I'm being honest, Amazon does not exactly have a track

2:02:18.880 --> 2:02:25.040
 record of winning over people in terms of trust. Now that said, it's a really difficult problem

2:02:25.760 --> 2:02:31.920
 for human being to let a robot in their home that has a camera on it. Right. That's really,

2:02:31.920 --> 2:02:38.960
 really, really tough. And I think Roomba actually, I have to think about this, but I'm pretty sure

2:02:38.960 --> 2:02:45.920
 now, or for some time already has had cameras, because they're doing the most recent Roomba.

2:02:45.920 --> 2:02:50.560
 I have so many Roombas. Oh, you actually do? Well, a program that I don't use a Roomba for

2:02:50.560 --> 2:02:54.720
 fact, people that have been to my place, they're like, yeah, you definitely don't use these Roombas.

2:02:56.640 --> 2:03:02.240
 That could be a good, I can't tell like the valence of this comment. Was it a compliment or like?

2:03:02.240 --> 2:03:08.240
 No, it's a giant math. It's just a bunch of electronics everywhere. I have six or seven

2:03:08.240 --> 2:03:12.800
 computers, I have robots everywhere, I have Lego robots, I have small robots and big robots. It's

2:03:12.800 --> 2:03:23.200
 just giant, just piles of robot stuff. And yeah, but including the Roombas, they're being used

2:03:23.760 --> 2:03:30.880
 for their body and intelligence, but not for their purpose. I've changed them to repurpose them for

2:03:30.880 --> 2:03:36.720
 other purposes, for deeper, more meaningful purposes than just like the Butter Robot. Yeah,

2:03:36.720 --> 2:03:42.480
 which just brings a lot of people happiness, I'm sure. They have a camera because the thing they

2:03:43.120 --> 2:03:50.000
 advertised, I had my own camera still, but the camera on the new Roomba, they have

2:03:50.800 --> 2:03:55.360
 state of the art poop detection as they advertised, which is a very difficult, apparently it's a

2:03:55.360 --> 2:04:01.040
 big problem for vacuum cleaners is if they go over like dark poop, it just runs it over and

2:04:01.040 --> 2:04:07.600
 creates a giant mess. So they have like, apparently they collected like a huge amount of data and

2:04:07.600 --> 2:04:12.160
 different shapes and looks and whatever of poop and then not able to avoid it and so on.

2:04:12.160 --> 2:04:17.840
 They're very proud of this. So there is a camera, but you don't think of it as having a camera.

2:04:19.920 --> 2:04:24.400
 Yeah, you don't think of it as having a camera because you've grown to trust it, I guess, because

2:04:24.400 --> 2:04:31.600
 our phones, at least most of us seem to trust this phone, even though there's a camera looking

2:04:31.600 --> 2:04:41.360
 directly at you. I think that if you trust that the company is taking security very seriously,

2:04:41.360 --> 2:04:46.720
 I actually don't know how that trust was earned with smartphones. I think it just started to provide

2:04:46.720 --> 2:04:51.840
 a lot of positive value into your life where you just took it in and then the company over time

2:04:51.840 --> 2:04:57.680
 has shown that it takes privacy very seriously, that kind of stuff. But I just Amazon is not always

2:04:58.480 --> 2:05:05.120
 in its social robots communicated, this is a trustworthy thing, both in terms of culture

2:05:05.120 --> 2:05:09.840
 and competence. Because I think privacy is not just about what do you intend to do,

2:05:10.400 --> 2:05:15.680
 but also how well, how good are you at doing that kind of thing. So that's a really hard problem to

2:05:15.680 --> 2:05:24.000
 solve. But a lot of us have Alexis at home, and I mean, Alexa could be listening in the whole time

2:05:24.000 --> 2:05:31.680
 and doing all sorts of nefarious things with the data. Hopefully it's not, but I don't think it is.

2:05:32.320 --> 2:05:38.160
 But Amazon is not, it's such a tricky thing for a company to get right, which is to earn the trust.

2:05:38.160 --> 2:05:44.720
 I don't think Alexis earned people's trust quite yet. Yeah, I think it's not there quite yet.

2:05:44.720 --> 2:05:48.080
 I agree, I agree. They struggle with this kind of stuff. In fact, when these topics are brought

2:05:48.080 --> 2:05:56.800
 up, people are always get nervous. And I think if you get nervous about it, the way to earn

2:05:56.800 --> 2:06:02.560
 people's trust is not by like, ooh, don't talk about this. It's just be open, be frank, be

2:06:02.560 --> 2:06:11.040
 transparent, and also create a culture of where it radiates at every level from engineer to CEO

2:06:11.040 --> 2:06:20.080
 that like, you're good people that have a common sense idea of what it means to respect basic

2:06:20.080 --> 2:06:25.600
 human rights and the privacy of people and all that kind of stuff. And I think that propagates

2:06:25.600 --> 2:06:32.160
 throughout the, that's the best PR, which is like, over time, you understand that these are

2:06:32.160 --> 2:06:39.280
 good, these are good folks doing good things. Anyway, speaking of social robots, have you

2:06:39.280 --> 2:06:45.200
 heard about Tesla, Tesla bot, the human robot? Yes, I have. Yes, yes, yes. But I don't exactly

2:06:45.200 --> 2:06:51.840
 know what it's designed to do. Do you? You probably do. No, I know it's designed to do,

2:06:51.840 --> 2:06:57.760
 but I have a different perspective on it, but it's designed to, it's a humanoid form, and it's

2:06:57.760 --> 2:07:05.680
 designed to, for automation tasks in the same way that industrial robot arms automate task in the

2:07:05.680 --> 2:07:10.640
 factory. So it's designed to automate task in the factory. But I think that humanoid form,

2:07:10.640 --> 2:07:20.720
 as we were talking about before, is one that we connect with as human beings. Anything legged,

2:07:20.720 --> 2:07:27.600
 honestly, but the humanoid form especially, we anthropomorphize it most intensely. And so,

2:07:27.600 --> 2:07:36.640
 the possibility to me, it's exciting to see both Atlas developed by Boston Dynamics and anyone,

2:07:36.640 --> 2:07:44.720
 including Tesla, trying to make humanoid robots cheaper and more effective. The obvious way

2:07:44.720 --> 2:07:52.880
 transforms the world is social robotics to me versus automation of tasks in the factory.

2:07:52.880 --> 2:07:57.840
 So, yeah, I just wanted to, in case that was something you were interested in, because I find

2:07:57.840 --> 2:08:03.520
 its application of social robotics super interesting. We did a lot of work with Pepper,

2:08:04.320 --> 2:08:09.840
 Pepper the Robot a while back. We were like the emotion engine for Pepper, which is SoftBank's

2:08:09.840 --> 2:08:17.760
 humanoid robot. And how tall is Pepper? It's like, yeah, like, I don't know, like five foot maybe,

2:08:17.760 --> 2:08:26.800
 right? Yeah, pretty big, pretty big. And it was designed to be like airport lounges and retail

2:08:26.800 --> 2:08:35.520
 stores, mostly customer service, right? Hotel lobbies. And I mean, I don't know where the

2:08:35.520 --> 2:08:39.280
 state of the robot is, but I think it's very promising. I think there are a lot of applications

2:08:39.280 --> 2:08:44.640
 where this can be helpful. I'm also really interested in, yeah, social robotics for the home,

2:08:44.640 --> 2:08:50.560
 right? Like that can help elderly people, for example, transport things from one location of

2:08:50.560 --> 2:08:57.200
 the home to the other, or even like just have your back in case something happens. Yeah,

2:08:57.200 --> 2:09:01.280
 I don't know. I do think it's a very interesting space. It seems early though. Do you feel like

2:09:01.840 --> 2:09:03.120
 the timing is now?

2:09:06.000 --> 2:09:11.840
 I, yes, 100%. So it always seems early until it's not, right?

2:09:11.840 --> 2:09:21.200
 Right, right, right. I think the time, I definitely think that the time is now,

2:09:21.200 --> 2:09:28.240
 like this decade, for social robots, whether the humanoid form is right, I don't think so.

2:09:29.520 --> 2:09:36.800
 I don't, I think the, like, if we just look Gibo, at Gibo as an example,

2:09:36.800 --> 2:09:45.360
 I feel like most of the problem, the challenge, the opportunity of social connection between

2:09:45.360 --> 2:09:52.720
 an AI system and a human being does not require you to also solve the problem of robot manipulation

2:09:52.720 --> 2:09:58.880
 and mobility, bipedal mobility. So I think you could do that with just a screen, honestly,

2:09:58.880 --> 2:10:03.600
 but there's something about the interface of Gibo we can rotate and so on that's also compelling.

2:10:03.600 --> 2:10:09.680
 But you get to see all these robot companies that fail, incredible companies like Gibo and

2:10:12.320 --> 2:10:18.000
 even, I mean, the iRobot in some sense is a big success story that it was able to find

2:10:20.160 --> 2:10:24.480
 a niche thing and focus on it. But in some sense, it's not a success story because they

2:10:24.480 --> 2:10:31.360
 didn't build any other robot, like any other, it didn't expand into all kinds of robotics,

2:10:31.360 --> 2:10:35.040
 like once you're in the home, maybe that's what happens with Amazon is they'll flourish into

2:10:35.040 --> 2:10:42.480
 all kinds of other robots. But do you have a sense, by the way, why it's so difficult to

2:10:42.480 --> 2:10:49.360
 build a robotics company? Like why so many companies have failed? I think it's like you're

2:10:49.360 --> 2:10:53.280
 building a vertical stack, right? Like you're building the hardware plus the software and you

2:10:53.280 --> 2:10:59.760
 find you have to do this at a cost that makes sense. So I think Gibo was retailing at like,

2:10:59.760 --> 2:11:09.120
 I don't know, like $700, $800, which for the use case, right? There's a dissonance there,

2:11:09.120 --> 2:11:19.440
 it's too high. So I think cost of building the whole platform in a way that is affordable

2:11:19.440 --> 2:11:24.880
 for what value it's bringing, I think that's the challenge. I think for these home robots

2:11:24.880 --> 2:11:31.600
 that are going to help, you know, help you do stuff around the home, that's a challenge too,

2:11:31.600 --> 2:11:36.800
 like the mobility piece of it. That's hard. Well, one of the things I'm really excited with

2:11:36.800 --> 2:11:43.360
 Tesla Bot is the people working on it. And that's probably the criticism I would apply to

2:11:43.360 --> 2:11:48.880
 some of the other folks who worked on social robots is the people working on Tesla Bot know

2:11:48.880 --> 2:11:53.360
 how to, they're focused on and know how to do mass manufacture and create a product that's super

2:11:53.360 --> 2:11:58.640
 cheap. Very cool. That's the focus. The engineering focus isn't, I would say that you can also

2:11:58.640 --> 2:12:04.560
 criticize them for that, is they're not focused on the experience of the robot. They're focused

2:12:04.560 --> 2:12:12.400
 on how to get this thing to do the basic stuff that the humanoid form requires to do as cheap as

2:12:12.400 --> 2:12:17.760
 possible, then the fewest number of actuators, the fewest numbers of motors, the increased

2:12:17.760 --> 2:12:21.520
 efficiency, they decrease the weight, all that kind of stuff. So that's really interesting.

2:12:21.520 --> 2:12:27.680
 I would say that Jibo and all those folks, they focus on the design, the experience, all of that,

2:12:27.680 --> 2:12:31.280
 and it's secondary how to manufacture. But it's like, no, you have to think,

2:12:32.320 --> 2:12:38.000
 like the Tesla Bot folks from first principles, what is the fewest number of components, the

2:12:38.000 --> 2:12:43.520
 cheapest components, how can I build it as much in house as possible without having to

2:12:44.240 --> 2:12:47.760
 consider all the complexities of a supply chain, all that kind of stuff.

2:12:47.760 --> 2:12:54.160
 Because if you have to build a robotics company, you're not building one robot, you're building

2:12:54.160 --> 2:12:59.360
 hopefully millions of robots. You have to figure out how to do that. Where the final thing, I mean,

2:12:59.360 --> 2:13:04.880
 if it's Jibo type of robot, is there a reason why Jibo, like we're going to have this lengthy

2:13:04.880 --> 2:13:08.640
 discussion, is there a reason why Jibo has to be over a hundred dollars?

2:13:08.640 --> 2:13:13.120
 It shouldn't be. Right. Like the basic components of it.

2:13:13.120 --> 2:13:17.840
 Right. Like you could start to actually discuss like, okay, what is the essential thing about

2:13:17.840 --> 2:13:22.640
 Jibo? How much, what is the cheapest way I can have a screen? What's the cheapest way I can have

2:13:22.640 --> 2:13:28.480
 a rotating base, all that kind of stuff. And then you get down, continuously drive down costs.

2:13:29.440 --> 2:13:35.280
 Speaking of which, you have launched an extremely successful companies, you have helped others,

2:13:35.280 --> 2:13:43.840
 you've invested in companies. Can you give advice on how to start a successful company?

2:13:43.840 --> 2:13:48.720
 I would say have a problem that you really, really, really want to solve. Right. Something

2:13:48.720 --> 2:13:57.120
 that you're deeply passionate about. And honestly, take the first step. Like that's often the hardest

2:13:58.080 --> 2:14:02.960
 and don't overthink it. Like, you know, like this idea of a minimum viable product or a minimum

2:14:02.960 --> 2:14:07.200
 viable version of an idea. Right. Like, yes, you're thinking about this, like a humongous,

2:14:07.200 --> 2:14:12.480
 like super elegant, super beautiful thing. What, like reduce it to the littlest thing

2:14:12.480 --> 2:14:18.640
 they can bring to market that can solve a problem or that can, you know, that can help address

2:14:18.640 --> 2:14:23.920
 a pain point that somebody has. They often tell you, like, start with a customer of one.

2:14:23.920 --> 2:14:28.560
 Right. If you can solve a problem for one person, then there's probably yourself or

2:14:28.560 --> 2:14:33.360
 some other person. Right. Pick a person. Exactly. It could be you. Yeah. It's actually

2:14:33.360 --> 2:14:38.240
 often a good sign that if you enjoy a thing, enjoy a thing where you have a specific problem that

2:14:38.240 --> 2:14:43.920
 you'd like to solve, that's a good, that's a good end of one to focus on. Right. What else,

2:14:43.920 --> 2:14:50.080
 what else is there to actually, so step one is the hardest, but there's other steps as well,

2:14:50.080 --> 2:14:58.560
 right? I also think like who you bring around the table early on is so key. Right. Like being

2:14:58.560 --> 2:15:03.760
 clear on, on what I call like your core values or your North Star. It might sound fluffy, but

2:15:03.760 --> 2:15:09.440
 actually it's not. So, and Roz and I, I feel like we did that very early on. We sat around her

2:15:09.440 --> 2:15:14.000
 kitchen table and we said, okay, there's so many applications of this technology. How are we going

2:15:14.000 --> 2:15:18.320
 to draw the line? How are we going to set boundaries? We came up with a set of core values

2:15:18.320 --> 2:15:25.440
 that in the hardest of times we fell back on to determine how we make decisions. And so,

2:15:25.440 --> 2:15:29.600
 I feel like just getting clarity on these core, like for us, it was respecting people's privacy,

2:15:30.320 --> 2:15:35.360
 only engaging with industries where it's clear opt in. So, for instance, we don't do any work

2:15:35.360 --> 2:15:41.440
 in security and surveillance. So, things like that, just getting, we very big on, you know,

2:15:41.440 --> 2:15:46.320
 one of our core values is human connection and empathy. Right. And that is, yes, it's an AI company,

2:15:46.320 --> 2:15:53.520
 but it's about people. Well, these are all, they become encoded in how we act, even, even if you're

2:15:53.520 --> 2:15:59.680
 a small, tiny team of two or three or whatever. So, I think that's another piece of advice.

2:15:59.680 --> 2:16:04.880
 So, what about finding people, hiring people? If you care about people as much as you do, like,

2:16:05.920 --> 2:16:09.840
 it seems like such a difficult thing to hire the right people.

2:16:10.720 --> 2:16:15.520
 I think early on, in the startup, you want people who have, who share the passion and the

2:16:15.520 --> 2:16:22.320
 conviction because, because it's going to be tough. Like, I've yet to meet a startup where it was just

2:16:22.320 --> 2:16:27.360
 a straight line to success. Right. Even, even not just startup, like, even everyday people's

2:16:27.360 --> 2:16:32.960
 lives, right. You always like run into obstacles and you run into naysayers and

2:16:35.280 --> 2:16:40.400
 so, you need people who are believers, whether they're people on your team or even your investors,

2:16:40.400 --> 2:16:45.120
 you need investors who are really believers in what you're doing, because that means they will

2:16:45.120 --> 2:16:50.640
 stick with you. They won't, they won't give up at the first obstacle. I think that's important.

2:16:50.640 --> 2:16:58.160
 What about raising money? What about finding investors? First of all, raising, raising money,

2:16:58.160 --> 2:17:03.920
 but also raising money from the right sources, from that ultimately don't hinder you, but, you

2:17:03.920 --> 2:17:08.320
 know, help you, empower you, all that kind of stuff. What a device would you give there?

2:17:08.320 --> 2:17:11.360
 You successfully raised money many times in your life.

2:17:11.360 --> 2:17:17.200
 Yeah, again, it's not just about the money. It's about writing, finding the right investors who

2:17:17.200 --> 2:17:22.960
 are going to be aligned in terms of what you want to build and believe in your core values. Like,

2:17:22.960 --> 2:17:29.600
 for example, especially later on, like I, yeah, in my latest, like, round of funding,

2:17:29.600 --> 2:17:35.600
 I try to bring in investors that really care about, like, the ethics of AI, right. And they,

2:17:35.600 --> 2:17:41.360
 the alignment of vision and mission and core values is really important. It's like you're

2:17:41.360 --> 2:17:47.280
 picking a life partner, right? It's the same kind of. So you take it that seriously for investors?

2:17:47.280 --> 2:17:51.280
 Yeah, because they're going to have to stick with you. You're stuck together.

2:17:51.280 --> 2:17:56.720
 For a while, anyway. Yeah. Maybe not for life, but for a while, for sure.

2:17:56.720 --> 2:18:00.800
 For better or worse, I forget what the vowels usually sound like. For better or worse? No.

2:18:00.800 --> 2:18:11.760
 Through sick, through something. Yeah. Oh, boy. Yeah. Anyway, it's romantic and deep,

2:18:11.760 --> 2:18:19.840
 and you're in it for a while. So it's not just about the money. You tweeted about going to your

2:18:19.840 --> 2:18:24.560
 first capital camp, investing, get together. And then you learned a lot. So this is about

2:18:24.560 --> 2:18:31.760
 investing. So what have you learned from that? What have you learned about investing in general?

2:18:32.560 --> 2:18:37.920
 From both? Because you've been on both ends of it. I mean, I try to use my experience as an operator

2:18:39.120 --> 2:18:46.320
 now with my investor hat on when I'm identifying companies to invest in. First of all, I think

2:18:46.320 --> 2:18:50.720
 the good news is because I have a technology background, right, and I really understand machine

2:18:50.720 --> 2:18:56.400
 learning and computer vision and AI, et cetera. I can apply that level of understanding, right?

2:18:56.400 --> 2:19:00.960
 Because everybody says they're an AI company or they're an AI tech. And I'm like, no, no, no, no,

2:19:01.760 --> 2:19:05.280
 show me the technology. So I can do that level of diligence, which I actually love.

2:19:07.360 --> 2:19:11.920
 And then I have to do the litmus test of, you know, if I'm in a conversation with you,

2:19:11.920 --> 2:19:19.280
 am I excited to tell you about this new company that I just met, right? And if I'm an ambassador

2:19:19.280 --> 2:19:25.520
 for that company and I'm passionate about what they're doing, I usually use that. Yeah, that's

2:19:25.520 --> 2:19:31.760
 important to me when I'm investing. So that means you actually can't explain what they're doing

2:19:32.800 --> 2:19:38.800
 and you're excited about it. Exactly. Exactly. Thank you for putting it so succinctly.

2:19:39.760 --> 2:19:45.680
 Like rambling, but exactly that's it. No, but sometimes it's funny, but sometimes it's unclear

2:19:45.680 --> 2:19:53.280
 exactly. I'll hear people tell me, you know, they'll talk for a while and it sounds cool, like they

2:19:53.280 --> 2:19:57.360
 paint a picture of a world, but then when you try to summarize it, you're not exactly clear

2:19:58.240 --> 2:20:05.280
 of what maybe, maybe what the core powerful idea is. Like you can't just build another Facebook or

2:20:06.240 --> 2:20:14.320
 there has to be a, there has to be a core simple to explain idea that, yeah, that then you can

2:20:14.320 --> 2:20:21.040
 or can't get excited about, but it's there. It's right there. Yeah. Yeah. But like, how do you

2:20:21.040 --> 2:20:27.360
 ultimately pick who you think will be successful? So it's not just about the thing you're excited

2:20:27.360 --> 2:20:33.040
 about, like there's other stuff. Right. And then there's all the, you know, with early stage companies,

2:20:33.040 --> 2:20:38.560
 like pre seed companies, which is where I'm investing, sometimes the, the business model

2:20:38.560 --> 2:20:43.120
 isn't clear yet, or the go to market strategy isn't clear. There's usually like, it's very

2:20:43.120 --> 2:20:48.240
 early on that some of these things haven't been hashed out, which is okay. So the way I like

2:20:48.240 --> 2:20:52.800
 to think about it is like, if this company is successful, will this be a multi billion slash

2:20:52.800 --> 2:20:59.520
 trillion dollar market, you know, or company? And, and so that's definitely a lens that I use.

2:21:00.320 --> 2:21:04.880
 What's pre, what's pre seed? What are the different stages? And what's the most exciting

2:21:04.880 --> 2:21:09.440
 stage and what's, or not what's, what's, what's interesting about every stage, I guess.

2:21:09.440 --> 2:21:15.440
 Yeah. So pre seed is usually when you're just starting out, you've maybe raised the friends

2:21:15.440 --> 2:21:19.440
 and family rounds, you've raised some money from people, you know, and you're getting ready to

2:21:20.240 --> 2:21:28.640
 take your first institutional check in like first check from an investor. And I love this stage.

2:21:28.640 --> 2:21:32.880
 There's a lot of uncertainty. So some investors really don't like the stage because

2:21:32.880 --> 2:21:39.200
 the financial models aren't there. Often the teams aren't even like formed really, really early.

2:21:40.880 --> 2:21:47.600
 But to me, it's, it's like a magical stage because it's the time when there's so much conviction,

2:21:47.600 --> 2:21:53.040
 so much belief, almost delusional, right? Yeah. And there's a little bit of naivete around

2:21:53.040 --> 2:22:02.080
 with founders at the stage. And I just love it. It's contagious. And I love, I love that I can

2:22:03.840 --> 2:22:07.920
 often they're first time founders, not always, but often they're first time founders and I can

2:22:07.920 --> 2:22:13.200
 share my experience as a founder myself and I can empathize, right? And I can almost,

2:22:14.400 --> 2:22:19.680
 I create a safe ground where, because, you know, you have to be careful what you tell your investors,

2:22:19.680 --> 2:22:25.600
 right? And I will, I will often like say, I've been in your shoes as a founder, you can tell me

2:22:25.600 --> 2:22:29.200
 if it's challenging, you can tell me what you're struggling with, it's okay to vent.

2:22:29.920 --> 2:22:34.400
 So I create that safe ground. And I think, I think that's the superpower.

2:22:35.120 --> 2:22:40.320
 Yeah, you have to, what I guess you have to figure out if this kind of person is going to be able

2:22:40.320 --> 2:22:48.560
 to ride the roller coaster, like of many pivots and challenges and all that kind of stuff. And if

2:22:48.560 --> 2:22:54.720
 the space of ideas they're working in is interesting, like the way they think about the world. Yeah,

2:22:54.720 --> 2:22:59.920
 because if it's successful, the thing they end up with might be very different, the reason

2:22:59.920 --> 2:23:06.160
 it's successful for it. Actually, you know, I was going to say the third, so the technology is one

2:23:06.160 --> 2:23:11.200
 aspect, the market or the idea, right, is the second and the third is the founder, right? Is

2:23:11.200 --> 2:23:18.080
 this somebody who I believe has conviction is a hustler, you know, is going to overcome

2:23:18.080 --> 2:23:24.320
 obstacles. Yeah, I think that it's going to be a great leader, right? Like as a startup,

2:23:24.320 --> 2:23:30.240
 as a founder, you're often, you are the first person and your role is to bring amazing people

2:23:30.240 --> 2:23:37.120
 around you to build this thing. And so you're in an evangelist, right? So how good are you

2:23:37.120 --> 2:23:43.520
 going to be at that? So I try to evaluate that too. You're also in the tweet thread about it,

2:23:43.520 --> 2:23:50.720
 mentioned, is this a known concept, random rich dudes, RDS, and saying that there should be like

2:23:50.720 --> 2:23:57.040
 random rich women, I guess. What's the dudes, what's the dudes version of women, the women version

2:23:57.040 --> 2:24:02.800
 of dudes, ladies, I don't know. What's, what's, is this a technical term? Is this known? Random

2:24:02.800 --> 2:24:08.400
 rich dudes? Well, I didn't make that up, but I was at this capital camp, which is a get together for

2:24:08.400 --> 2:24:18.080
 investors of all types. And there must have been maybe 400 or so attendees, maybe 20 were women.

2:24:18.080 --> 2:24:25.280
 It was just very disproportionately, you know, a male, male dominated, which I'm used to. I think

2:24:25.280 --> 2:24:30.080
 you're used to this kind of thing. I'm used to it, but it's still surprising. And as I'm raising

2:24:30.080 --> 2:24:37.040
 money for this fund, so my, my, my fund partner is a guy called Rob May, who's done this before. So

2:24:37.040 --> 2:24:43.280
 I'm new to the investing world, but he's done this before. Most of our investors in the fund are

2:24:43.280 --> 2:24:48.880
 these, I mean, awesome, I'm super grateful to them, random just rich guys. I'm like, where are the

2:24:48.880 --> 2:24:56.560
 rich women? So I'm really adamant in both investing in women led AI companies. But I also would love

2:24:56.560 --> 2:25:02.320
 to have women investors be part of my fund. Because I think that's how we drive change.

2:25:02.320 --> 2:25:07.360
 Yeah. So the next, you know, that, that, that takes time, of course, but there's been quite,

2:25:07.360 --> 2:25:12.080
 quite a lot of progress. But yeah, for, for the next Mark Zuckerberg to be a woman and all that

2:25:12.080 --> 2:25:17.920
 kind of stuff, because that, that's just like a huge number of wealth generated by, by women and

2:25:17.920 --> 2:25:22.880
 then controlled by women and allocated by women and all that kind of stuff. And then beyond just

2:25:22.880 --> 2:25:32.000
 women, just broadly across all different measures of diversity and so on. Let me ask you to put

2:25:32.000 --> 2:25:38.960
 on your wise sage hat. Okay. So we already, we already gave advice on startups and just advice

2:25:42.320 --> 2:25:49.680
 for women. But in general, advice for folks in high school or college today, how to have a career

2:25:49.680 --> 2:25:56.720
 they can be proud of, how to have a life they can be proud of. I suppose you have to give this kind

2:25:56.720 --> 2:26:02.160
 of advice to your kids. Yeah. Well, here's the number one advice that I give to my kids.

2:26:03.200 --> 2:26:08.640
 My daughter is now 19, by the way, and my son is 13 and a half. So they're not little kids anymore.

2:26:09.200 --> 2:26:17.440
 But, but I think it does. They're awesome. They're my best friends. But yeah, I think the number

2:26:17.440 --> 2:26:24.720
 one advice I would share is embark on a journey without attaching to outcomes and enjoy the journey.

2:26:24.720 --> 2:26:32.560
 Right? So, you know, we often were so obsessed with, with the end goal. A, that doesn't allow us to

2:26:32.560 --> 2:26:40.480
 be open to different endings of a journey or a story. So you become like so fixated on a

2:26:40.480 --> 2:26:47.840
 particular path, you don't see the beauty in the other alternative path. And then you forget to

2:26:47.840 --> 2:26:53.040
 enjoy the journey because you're just so fixated on the goal. And I've been guilty of that for many,

2:26:53.040 --> 2:26:58.000
 many years in my life. And I've, I've, I've now, I'm now trying to like make the shift of no, no,

2:26:58.000 --> 2:27:03.680
 no, I'm gonna, again, trust that things are going to work out and it'll be amazing and maybe even

2:27:03.680 --> 2:27:09.520
 exceed your dreams. We have to be open to that. Yeah. Taking, taking a leap into all kinds of

2:27:09.520 --> 2:27:13.680
 things. I think you tweeted like you went on vacation by yourself or something like this or

2:27:13.680 --> 2:27:20.400
 I know this. And just, just, just, just going, just taking the leap, doing it and enjoying

2:27:20.400 --> 2:27:25.040
 enjoying the, enjoying the moment, enjoying the weeks, enjoying not looking at the,

2:27:26.160 --> 2:27:32.000
 some kind of career ladder next step and so on. Yeah. There's, there's something to that,

2:27:32.000 --> 2:27:37.600
 like overplanning too. I'm surrounded by a lot of people that kind of, so I don't plan.

2:27:37.600 --> 2:27:40.640
 You don't. No. Do you not do goal setting?

2:27:40.640 --> 2:27:48.880
 My goal setting is very like, I like the affirmations is very,

2:27:50.800 --> 2:27:55.360
 it's almost, I don't know how to put into words, but it's, it's a little bit like

2:27:58.160 --> 2:28:04.960
 what my heart yearns for kind of in, I guess in the space of emotions more than in the space of like

2:28:04.960 --> 2:28:13.280
 this would be like in the rational space. Cause I just tried to picture a world that I would like

2:28:13.280 --> 2:28:18.800
 to be in and that world is not clearly pictured. It's mostly in the emotional world. I mean,

2:28:18.800 --> 2:28:25.520
 I think about that from, from robots, cause you know, I have this desire. I've had it my whole

2:28:25.520 --> 2:28:32.880
 life to, to, well, it took different shapes, but I think once I discovered AI, the desire was to

2:28:32.880 --> 2:28:41.120
 I think in this, in the context of this conversation, could be easily easier described as basically a

2:28:41.120 --> 2:28:50.240
 social robotics company. And that's something I dreamed of doing. And well, there's a lot,

2:28:50.240 --> 2:28:54.400
 there's a lot of complexity to that story, but that, that's, that's the, that's the only thing,

2:28:54.400 --> 2:29:02.160
 honestly I dream of doing. So I imagine a world that, that I could help create, but it's not a,

2:29:03.360 --> 2:29:10.400
 there's no steps along the way. And I think I'm just kind of stumbling around and following

2:29:10.400 --> 2:29:17.120
 happiness and working my ass off in almost random, like an aunt does in random directions.

2:29:17.120 --> 2:29:20.400
 But a lot of people, a lot of successful people around me say this, you should have a plan,

2:29:20.400 --> 2:29:23.200
 you should have a clear goal. You have a goal at the end of the month, you have a goal at the

2:29:23.200 --> 2:29:31.600
 end of the year. I don't, I don't, I don't. And there's a balance to be struck, of course, but

2:29:32.880 --> 2:29:39.280
 there's something to be said about really making sure that you're living life to the fullest

2:29:40.080 --> 2:29:46.880
 that goals can actually get in the way of. So one of the best, like kind of most,

2:29:46.880 --> 2:29:52.880
 what do you, what do you call it when it's like challenges your brain? What do you call it?

2:29:56.560 --> 2:30:00.080
 The only thing that comes to mind, and this is me saying is a mind fuck, but yes.

2:30:00.080 --> 2:30:03.120
 Okay, okay. Something like that. Yes.

2:30:03.760 --> 2:30:10.080
 Super inspiring talk. Kenneth Stanley, he was at open AI, he just laughed. And he has a book

2:30:10.080 --> 2:30:15.120
 called Why Greatness Can't Be Planned. And it's actually an AI book. So, and he's done all these

2:30:15.120 --> 2:30:22.160
 experiments that basically show that when you over optimize, you, you, like the tradeoff is

2:30:22.160 --> 2:30:29.360
 you're less creative, right? And to create true greatness and truly creative solutions to problems,

2:30:29.360 --> 2:30:35.280
 you can't overplan it. You can't. And I thought that was, and so he generalizes it beyond AI.

2:30:35.280 --> 2:30:39.760
 And he talks about how we apply that in our personal life and in our organizations and our

2:30:39.760 --> 2:30:44.400
 companies, which are over KPI, right? Like look at any company in the world. And it's all like,

2:30:44.400 --> 2:30:49.200
 these are the goals. These are the, you know, weekly goals and, you know, the sprints and then

2:30:49.200 --> 2:30:55.040
 the quarterly goals, blah, blah, blah. And, and he just shows with a lot of his AI experiments

2:30:55.040 --> 2:31:01.120
 that that's not how you create truly game changing ideas. So there you go. Yeah. Yeah.

2:31:01.120 --> 2:31:05.360
 But he's awesome. Yeah, there's a balance, of course, because that's yeah,

2:31:06.160 --> 2:31:12.720
 many moments of genius will not come from planning and goals, but you still have to build factories

2:31:12.720 --> 2:31:16.400
 and you still have to manufacture and you still have to deliver and there's still deadlines and

2:31:16.400 --> 2:31:21.280
 all that kind of stuff. And that for that's good to have goals. I do goal setting with my kids.

2:31:21.280 --> 2:31:26.560
 We all have our goals, but, but, but I think we're starting to morph into more of these like

2:31:26.560 --> 2:31:32.160
 bigger picture goals and not obsess about like, I don't know, it's hard. Well, I honestly think

2:31:32.160 --> 2:31:36.000
 with, especially with kids, it's better, much, much better to have a plan and have goals and so

2:31:36.000 --> 2:31:40.240
 on. Cause you have to, you have to learn the muscle of like what it feels like to get stuff done.

2:31:40.240 --> 2:31:45.680
 Yeah. But I think once you learn that, there's flexibility for me because I spend most of my

2:31:45.680 --> 2:31:50.640
 life with goal setting and so on. So like, I've gotten good with grades and school. I mean,

2:31:50.640 --> 2:31:55.520
 in school, if you want to be successful at school, yeah. I mean, the kind of stuff in high school

2:31:55.520 --> 2:32:00.960
 and college that kids have to do in terms of managing their time and getting so much stuff done,

2:32:00.960 --> 2:32:06.960
 it's like, you know, taking five, six, seven classes in college, they're like, that would break

2:32:06.960 --> 2:32:13.760
 the, the spirit of most humans if they took one of them later in life. It's like really difficult

2:32:13.760 --> 2:32:20.160
 stuff, especially engineering curricula. So I think you have to learn that skill. But once

2:32:20.160 --> 2:32:24.800
 you learn it, you can maybe, cause you're, you can be a little bit on autopilot and use that

2:32:24.800 --> 2:32:29.520
 momentum and then allow yourself to be lost in the flow of life, you know, just kind of

2:32:29.520 --> 2:32:39.920
 or also give like, I work pretty hard to allow myself to have the freedom to do that. That's

2:32:39.920 --> 2:32:45.200
 really, that's a tricky freedom to have because like a lot of people get lost in the right race

2:32:45.200 --> 2:32:53.040
 and they, and they also like, like financially, they, whenever you get a raise, they'll get

2:32:53.040 --> 2:32:58.240
 like a bigger house or something like this. I put very, so like, you're always trapped in this

2:32:58.240 --> 2:33:06.800
 race. I put a lot of emphasis on, on living like below my means always. And so there's a lot of

2:33:06.800 --> 2:33:12.640
 freedoms to do whatever, whatever the heart desires that, that's a really, but everyone has

2:33:12.640 --> 2:33:16.560
 to decide what's the right thing, what's the right thing for them. For some people,

2:33:16.560 --> 2:33:22.240
 having a lot of responsibilities, like a house they can barely afford, or having a lot of kids,

2:33:22.240 --> 2:33:28.720
 the responsibility side of that is really helps them get their shit together. Like, all right,

2:33:28.720 --> 2:33:33.280
 I need to be really focused and give some of the most successful people I know have kids and the

2:33:33.280 --> 2:33:36.400
 kids bring out the best of them. They make them more productive, less productive.

2:33:36.400 --> 2:33:41.600
 Accountability, accountability thing. And almost something to actually live and

2:33:41.600 --> 2:33:47.440
 fight and work for, like having a family. It's, it's fascinating to see because you would think

2:33:47.440 --> 2:33:52.080
 kids would be a hit on productivity, but they're not for a lot of really successful people.

2:33:52.080 --> 2:33:57.680
 They really like, they're like an engine of right efficiency. Oh my God. Yeah. It's weird. Yeah.

2:33:57.680 --> 2:34:03.120
 I mean, it's beautiful. It's beautiful to see. And also social happiness. Speaking of which,

2:34:03.120 --> 2:34:08.640
 what role do you think love plays in the human condition? Love?

2:34:12.080 --> 2:34:14.560
 I think love is, is, um,

2:34:14.560 --> 2:34:22.000
 yeah, I think, I think it's why we're all here. I think it would be very hard to live life without

2:34:23.680 --> 2:34:32.960
 love in any of its forms, right? Yeah, that's the most beautiful forms that,

2:34:32.960 --> 2:34:39.920
 that human connection takes, right? Yeah. I feel like everybody wants to feel loved,

2:34:39.920 --> 2:34:45.120
 right? And one way or another, right? And to love. Yeah. And to love too. Totally. Yeah,

2:34:45.120 --> 2:34:48.560
 I agree with that. Both of it. I'm not even sure what feels better.

2:34:50.240 --> 2:34:56.560
 Both, both like that. To give and, to give love too. Yeah. And, and it is like we've been talking

2:34:56.560 --> 2:35:02.000
 about an interesting question, whether some of that, whether one day we'll be able to love a toaster.

2:35:02.000 --> 2:35:09.200
 Okay. Get some small. I wasn't quite thinking about that when I said like, love, yeah, love.

2:35:09.200 --> 2:35:12.480
 That's all I was thinking about. Love and give love. Okay. I was thinking about Brad Pitt and

2:35:12.480 --> 2:35:19.280
 toasters. Toasters, great. All right. Well, I think we, we started on love and ended on love.

2:35:19.920 --> 2:35:24.640
 This was an incredible conversation, Ron. And thank you so much. You're an incredible person.

2:35:24.640 --> 2:35:31.840
 Thank you for everything you're doing in, in, in AI, in, in the space of just caring about humanity,

2:35:32.560 --> 2:35:39.520
 human emotion, about love, and being an inspiration to a huge number of people in robotics, in AI,

2:35:39.520 --> 2:35:44.080
 in science, in the world of general. So thank you for talking to me. It's an honor.

2:35:44.080 --> 2:35:48.320
 Thank you for having me. And you know, I'm a big fan of yours as well. So it's been a pleasure.

2:35:49.440 --> 2:35:53.760
 Thanks for listening to this conversation with Rana Elkhlyubi. To support this podcast,

2:35:53.760 --> 2:35:58.720
 please check out our sponsors in the description. And now let me leave you with some words from

2:35:58.720 --> 2:36:05.200
 Helen Keller. The best and most beautiful things in the world cannot be seen or even touched.

2:36:05.200 --> 2:36:23.280
 They must be felt with the heart. Thank you for listening and hope to see you next time.

