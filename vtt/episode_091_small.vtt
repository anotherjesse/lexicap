WEBVTT

00:00.000 --> 00:08.160
 The following is a conversation with Jack Dorsey, cofounder and CEO of Twitter and founder and CEO of Square.

00:08.760 --> 00:24.800
 Given the happenings at the time related to Twitter leadership and the very limited time we had, we decided to focus this conversation on Square and some broader philosophical topics and to save an in depth conversation on engineering and AI at Twitter for a second appearance in this podcast.

00:24.800 --> 00:37.800
 This conversation was recorded before the outbreak of the pandemic. For everyone feeling the medical, psychological and financial burden of this crisis, I'm sending love your way. Stay strong. We're in this together. We'll beat this thing.

00:37.800 --> 00:55.800
 As an aside, let me mention that Jack moved $1 billion of Square equity, which is 28% of his wealth to form an organization that funds COVID 19 relief. First, as Andrew Yang tweeted, this is a spectacular commitment.

00:55.800 --> 01:07.800
 And second, it is amazing that it operates transparently by posting all its donations to a single Google Doc. To me, true transparency is simple, and this is as simple as it gets.

01:07.800 --> 01:26.800
 This is the artificial intelligence podcast. If you enjoy it, subscribe on YouTube, review it with five stars and Apple podcast, support it on Patreon or simply connect with me on Twitter and Lex Friedman spelled F R I D M A N. As usual, I'll do a few minutes of ads now and never any ads in the middle that can break the flow of the conversation.

01:26.800 --> 01:39.800
 I hope that works for you and doesn't hurt the listening experience. This show is presented by masterclass. Sign up on masterclass.com slash Lex to get a discount and to support this podcast.

01:39.800 --> 01:52.800
 When I first heard about masterclass, I thought it was too good to be true. For $180 a year, you get an all access pass to watch courses from to list some of my favorites. Chris Hadfield on space exploration.

01:52.800 --> 02:12.800
 Neil deGrasse Tyson on scientific thinking communication will write creator of SimCity and Sims, both one of my favorite games on game design, Jane Goodall on conservation, Carlos Santana on guitar, one of my favorite guitar players, Gary Kasparov on chess, Daniel Negrano on poker, and many, many more.

02:12.800 --> 02:26.800
 Chris Hadfield explaining how raucous work and the experience of being launched into space alone is worth the money. For me, the key is to not be overwhelmed by the abundance of choice. Pick three courses you want to complete. Watch each all the way through.

02:27.800 --> 02:35.800
 It's not that long, but it's an experience that will stick with you for a long time. It's easily worth the money. You can watch it on basically any device.

02:35.800 --> 02:46.800
 Once again, sign up on masterclass.com slash Lex to get a discount and to support this podcast. And now here's my conversation with Jack Dorsey.

02:46.800 --> 03:07.800
 You've been on several podcasts, Joe Rogan, Sam Harris, Rich Roll, others, excellent conversations. But I think there's several topics that you didn't talk about that I think are fascinating that I love to talk to you about sort of machine learning, artificial intelligence, both the narrow kind and the general kind and engineering at scale.

03:07.800 --> 03:20.800
 So there's a lot of incredible engineering going on that you're part of crypto, cryptocurrency, blockchain, UBI, all kinds of philosophical questions maybe we'll get to while life and death and meaning and beauty.

03:21.800 --> 03:33.800
 So you're involved in building some of the biggest network systems in the world, sort of trillions of interactions a day. The cool thing about that is the infrastructure, the engineering at scale.

03:33.800 --> 03:39.800
 You started as a programmer with C building. I'm a hacker. I'm not really an engineer.

03:40.800 --> 03:51.800
 Not a legit software engineer. You're a hacker at heart. But to achieve scale, you have to do some unfortunately legit, large scale engineering. So how do you make that magic happen?

03:51.800 --> 04:08.800
 Hire people that I can learn from, number one. I mean, I'm a hacker in the sense that my approach has always been do whatever it takes to make it work so that I can see and feel the thing and then learn what needs to come next.

04:08.800 --> 04:32.800
 And oftentimes what needs to come next is a matter of being able to bring it to more people, which is scale and there's a lot of great people out there that either have experience or are extremely fast learners that we've been lucky enough to find and work with for years.

04:32.800 --> 04:47.800
 But I think a lot of it, we benefit a ton from the open source community and just all the learnings there that are laid bare in the open, all the mistakes, all the success, all the problems.

04:47.800 --> 05:01.800
 It's a very slow moving process usually open source, but it's very deliberate. And you get to see because of the pace, you get to see what it takes to really build something meaningful.

05:01.800 --> 05:28.800
 So I learned most of everything I learned about hacking and programming and engineering has been due to open source and the generosity that people have given to give up their time, sacrifice their time without any expectation return other than being a part of something much larger than themselves, which I think is great.

05:28.800 --> 05:54.800
 The open source movement is amazing, but if you just look at the scale like Square has to take care of, is this fundamentally a software problem or hardware problem? You mentioned hiring a bunch of people, but it's not, maybe from my perspective, not often talked about how incredible that is to sort of have a system that doesn't go down often that is secure, is able to take care of all these transactions.

05:54.800 --> 06:10.800
 Like maybe I'm also a hacker at heart and it's incredible to me that that kind of scale could be achieved. Is there some insight, some lessons, some interesting tidbits that you can say how to make that scale happen?

06:10.800 --> 06:26.800
 Is it the hardware fundamentally challenge? Is it a software challenge? Is it like, is it a social challenge of building large teams of engineers that work together, that kind of thing? Like what's the interesting challenges there?

06:27.800 --> 06:29.800
 By the way, you're the best stress hacker I've met.

06:29.800 --> 06:52.800
 I think the enumeration you just went through, I don't think there's one, you have to kind of focus on all and the ability to focus on all that really comes down to how you face problems and whether you can break them down into parts that you can focus on.

06:52.800 --> 07:18.800
 Because I think the biggest mistake is trying to solve or address too many at once or not going deep enough with the questions or not being critical of the answers you find or not taking the time to form credible hypotheses that you can actually test and you can see the results of.

07:18.800 --> 07:43.800
 So all of those fall in the face of ultimately critical thinking skills, problem solving skills, and if there's one skill I want to improve every day, it's that. That's what contributes to learning and the only way we can evolve any of these things is learning what it's currently doing and how to take it to the next step.

07:43.800 --> 07:49.800
 And questioning assumptions, the first principle is kind of thinking seems like fundamental to this whole process.

07:50.800 --> 08:05.800
 Yeah, but if you get too over extended into, well, this is a hardware issue, you miss all the software solutions and vice versa, if you focus too much on the software, there are hardware solutions that can 10x a thing.

08:05.800 --> 08:16.800
 So I try to resist the categories of thinking and look for the underlying systems that make all these things work.

08:16.800 --> 08:34.800
 But those only emerge when you have a skill around creative thinking, problem solving and being able to ask critical questions and having the patients to go deep.

08:34.800 --> 08:46.800
 So one of the amazing things if we look at the mission of Square is to increase people's access to the economy. Maybe you can correct me if I'm wrong, that's from my perspective.

08:47.800 --> 08:58.800
 So from the perspective of merchants, peer to peer payments, even cryptocurrency, digital cryptocurrency, what do you see as the major ways our society can increase participation in the economy?

08:58.800 --> 09:06.800
 So if we look at today and the next 10 years, next 20 years, you're going to Africa, maybe in Africa and all kinds of other places outside of the North America.

09:08.800 --> 09:16.800
 If there was one word that I think represents what we're trying to do at Square, it is that word access.

09:16.800 --> 09:30.800
 One of the things we found is that we weren't expecting this at all. When we started, we thought we were just building a piece of hardware to enable people to plug it into their phone and swipe a credit card.

09:31.800 --> 09:45.800
 And then as we talked with people who actually tried to accept credit cards in the past, we found a consistent theme, which many of them weren't even enabled but allowed to process credit cards.

09:45.800 --> 10:01.800
 And we dug a little bit deeper, again, asking that question, and we found that a lot of them would go to banks or these merchant acquirers, and waiting for them was a credit check and looking at a FICA score.

10:01.800 --> 10:19.800
 And many of the businesses that we talked to and many small businesses, they don't have good credit or a credit history. They're entrepreneurs who are just getting started, taking a lot of personal risk, financial risk.

10:19.800 --> 10:32.800
 And it just felt ridiculous to us that for the job of being able to accept money from people, you had to get your credit checked.

10:32.800 --> 10:47.800
 And as we dug deeper, we realized that that wasn't the intention of the financial industry, but it's the only tool they had available to them to understand authenticity, intent, predictor of future behavior.

10:47.800 --> 10:55.800
 So that's the first thing we actually looked at. And that's where the, you know, we built the hardware, but the software really came in terms of risk modeling.

10:56.800 --> 11:12.800
 And that's when we started down the path that eventually leads to AI. We started with a very strong data science discipline because we knew that our business was not necessarily about making hardware.

11:12.800 --> 11:17.800
 It was more about enabling more people to come into the system.

11:17.800 --> 11:29.800
 So the fundamental challenge there is to enable more people to come into the system, you have to lower the barrier of checking that that person will be a legitimate vendor.

11:29.800 --> 11:31.800
 Is that the fundamental problem here?

11:31.800 --> 11:52.800
 Yeah, and a different mindset. I think a lot of the financial industry had a mindset of kind of distrust and just constantly looking for opportunities to prove why people shouldn't get into the system.

11:52.800 --> 11:58.800
 Whereas we took on a mindset of trust and then verify, verify, verify, verify, verify.

11:58.800 --> 12:09.800
 So we moved, you know, when we entered the space, only about 30 to 40% of the people who applied to accept credit cards would actually get through the system.

12:09.800 --> 12:16.800
 We took that number to 99%. And that's because we reframed the problem.

12:16.800 --> 12:27.800
 We built credible models and we had this mindset of we're going to watch not at the merchant level, but we're going to watch at the transaction level.

12:27.800 --> 12:31.800
 So come in, perform some transactions.

12:31.800 --> 12:42.800
 And as long as you're doing things that feel high integrity, credible, and don't look suspicious, we'll continue to serve you.

12:42.800 --> 12:52.800
 If we see any interestingness in how you use our system, that will be bubbled up to people to review to figure out if there's something nefarious going on.

12:52.800 --> 12:54.800
 And that's when we might ask you to leave.

12:54.800 --> 13:08.800
 So the change in the mindset led to the technology that we needed to enable more people to get through and to enable more people to access system.

13:08.800 --> 13:16.800
 What role does machine learning play into that in that context of you said, first of all, it's a beautiful shift.

13:16.800 --> 13:29.800
 Anytime you shift your viewpoint into seeing that people are fundamentally good, and then you just have to verify and catch the ones who are not, as opposed to assuming everybody's bad.

13:29.800 --> 13:31.800
 This is a beautiful thing.

13:31.800 --> 13:39.800
 So what role does the to you throughout the history of the company has machine learning played in doing that verification?

13:39.800 --> 13:41.800
 It was immediate.

13:41.800 --> 13:44.800
 I mean, we weren't calling it machine learning, but it was data science.

13:44.800 --> 13:49.800
 And then as the industry evolved, machine learning became more of the nomenclature.

13:49.800 --> 13:55.800
 And as that evolved, it became more sophisticated with deep learning.

13:55.800 --> 13:59.800
 And as that continues to evolve, it'll be another thing.

13:59.800 --> 14:01.800
 But they're all in the same vein.

14:01.800 --> 14:06.800
 But we built that discipline up within the first year of the company.

14:06.800 --> 14:24.800
 Because we also had to partner with a bank, we had to partner with Visa MasterCard, and we had to show that by bringing more people into the system that we could do so in a responsible way that would not compromise their systems and that they would trust us.

14:24.800 --> 14:35.800
 How do you convince this upstart company with some cool machine learning tricks is able to deliver on this sort of a trustworthy set of merchants?

14:35.800 --> 14:37.800
 We staged it out in tiers.

14:37.800 --> 14:49.800
 We had a bucket of 500 people using it, and then we showed results, and then 1,000, and then 10,000, then 50,000, and then the constraint was lifted.

14:49.800 --> 14:53.800
 So again, it's kind of getting something tangible out there.

14:53.800 --> 14:57.800
 I want to show what we can do rather than talk about it.

14:57.800 --> 15:02.800
 And that put a lot of pressure on us to do the right things.

15:02.800 --> 15:17.800
 And it also created a culture of accountability, of a little bit more transparency, and I think incentivized all of our early folks and the company in the right way.

15:17.800 --> 15:21.800
 So what does the future look like in terms of increasing people's access?

15:21.800 --> 15:35.800
 Or if you look at IoT, Internet of Things, there's more and more intelligent devices. You can see there's some people even talking about our personal data as a thing that we could monetize more explicitly versus implicitly.

15:35.800 --> 15:38.800
 Sort of everything can become part of the economy.

15:38.800 --> 15:48.800
 So what does the future of Square look like in sort of giving people access in all kinds of ways to being part of the economy as merchants and as consumers?

15:48.800 --> 15:55.800
 I believe that the currency we use is a huge part of the answer.

15:55.800 --> 16:01.800
 And I believe that the Internet deserves and requires a native currency.

16:01.800 --> 16:15.800
 And that's why I'm such a huge believer in Bitcoin because our biggest problem as a company right now is we cannot act like an Internet company.

16:15.800 --> 16:19.800
 We have to open a new market. We have to have a partnership with the local bank.

16:19.800 --> 16:24.800
 We have to pay attention to different regulatory onboarding environments.

16:24.800 --> 16:40.800
 And a digital currency like Bitcoin takes a bunch of that away where we can potentially launch a product in every single market around the world because they're all using the same currency.

16:40.800 --> 16:49.800
 And we have consistent understanding of regulation and onboarding and what that means.

16:49.800 --> 16:56.800
 So I think the Internet continuing to be accessible to people is number one.

16:56.800 --> 17:00.800
 And then I think currency is number two.

17:00.800 --> 17:08.800
 And it will just allow for a lot more innovation, a lot more speed in terms of what we can build and others can build.

17:08.800 --> 17:11.800
 And it's just really exciting.

17:11.800 --> 17:15.800
 So I mean, I want to be able to see that and feel that in my lifetime.

17:15.800 --> 17:23.800
 So in this aspect and in other aspects, you have a deep interest in cryptocurrency and distributed ledger tech in general.

17:23.800 --> 17:27.800
 I talked to Vitalik Buterin yesterday on this podcast.

17:27.800 --> 17:28.800
 He says hi by the way.

17:28.800 --> 17:29.800
 Hey.

17:29.800 --> 17:35.800
 He's a brilliant, brilliant person to talk a lot about Bitcoin and Ethereum, of course.

17:35.800 --> 17:38.800
 So can you maybe linger on this point?

17:38.800 --> 17:43.800
 What do you find appealing about Bitcoin, about digital currency?

17:43.800 --> 17:46.800
 Where do you see it going in the next 10, 20 years?

17:46.800 --> 17:58.800
 And what are some of the challenges with respect to Square, but also just bigger for our globally, for our world, for the way we think about money?

17:58.800 --> 18:04.800
 I think the most beautiful thing about it is there's no one person setting the direction.

18:04.800 --> 18:09.800
 And there's no one person on the other side that can stop it.

18:09.800 --> 18:18.800
 So we have something that is pretty organic in nature and very principled in its original design.

18:18.800 --> 18:27.800
 And I think the Bitcoin White Paper is one of the most seminal works of computer science in the last 20, 30 years.

18:27.800 --> 18:29.800
 It's poetry.

18:29.800 --> 18:31.800
 It's pretty cool technology.

18:31.800 --> 18:34.800
 That's not often talked about.

18:34.800 --> 18:41.800
 There's so much hype around digital currency about the financial impacts of it, but the actual technology is quite beautiful from a computer science perspective.

18:41.800 --> 18:42.800
 Yeah.

18:42.800 --> 18:47.800
 And the underlying principles behind it that went into it, even to the point of releasing it under a pseudonym.

18:47.800 --> 18:50.800
 I think that's a very, very powerful statement.

18:50.800 --> 18:53.800
 The timing of when it was released is powerful.

18:53.800 --> 18:57.800
 It was a total activist move.

18:57.800 --> 19:11.800
 I mean, it's moving the world forward in a way that I think is extremely noble and honorable and enables everyone to be part of the story, which is also really cool.

19:11.800 --> 19:15.800
 So you asked a question around 10 years and 20 years.

19:15.800 --> 19:18.800
 I mean, I think the amazing thing is no one knows.

19:18.800 --> 19:32.800
 And it can emerge and every person that comes into the ecosystem, whether they be a developer or someone who uses it, can change its direction in small and large ways.

19:32.800 --> 19:37.800
 And that's what I think it should be because that's what the internet has shown is possible.

19:37.800 --> 19:51.800
 Now, there's complications with that, of course, and there's certainly companies that own large ports, so the internet can direct it more than others, and there's not equal access to every single person in the world just yet.

19:51.800 --> 19:56.800
 But all those problems are visible enough to speak about them.

19:56.800 --> 20:02.800
 And to me, that gives confidence that they're solvable in a relatively short time frame.

20:02.800 --> 20:17.800
 I think the world changes a lot as we get these satellites projecting the internet down to earth because it just removes a bunch of the former constraints and really levels the playing field.

20:17.800 --> 20:26.800
 But a global currency, which a native currency for the internet is a proxy for, is a very powerful concept.

20:26.800 --> 20:31.800
 And I don't think any one person on this planet truly understands the ramifications of that.

20:31.800 --> 20:35.800
 There's a lot of positives to it. There's some negatives as well.

20:35.800 --> 20:54.800
 Do you think it's possible, sorry to interrupt, do you think it's possible that this kind of digital currency would redefine the nature of money, so become the main currency of the world as opposed to being tied to fiat currency of different nations and sort of really push the decentralization of control of money?

20:54.800 --> 21:01.800
 Definitely. But I think the bigger ramification is how it affects how society works.

21:01.800 --> 21:05.800
 And I think there are many positive ramifications.

21:05.800 --> 21:07.800
 Outside of just money.

21:07.800 --> 21:11.800
 Outside of just money. Money is a foundational layer that enables so much more.

21:11.800 --> 21:20.800
 I was meeting with an entrepreneur in Ethiopia and payments is probably the number one problem to solve across the continent.

21:20.800 --> 21:31.800
 Both in terms of moving money across borders between nations on the continent or the amount of corruption within the current system.

21:31.800 --> 21:41.800
 But the lack of easy ways to pay people makes starting anything really difficult.

21:41.800 --> 21:53.800
 I met an entrepreneur who started the Lyft slash Uber of Ethiopia and one of the biggest problems she has is that it's not easy for her writers to pay the company.

21:53.800 --> 21:56.800
 It's not easy for her to pay the drivers.

21:56.800 --> 22:01.800
 And that definitely has stunted her growth and made everything more challenging.

22:01.800 --> 22:14.800
 So the fact that she even has to think about payments instead of thinking about the best writer experience and the best driver experience is pretty telling.

22:14.800 --> 22:25.800
 So I think as we get a more durable, resilient and global standard, we see a lot more innovation everywhere.

22:25.800 --> 22:41.800
 And I think there's no better case study for this than the various countries within Africa and their entrepreneurs who are trying to start things within health or sustainability or transportation or a lot of the companies that we've seen that we've seen here.

22:41.800 --> 22:51.800
 So the majority of companies I met in November when I spent a month on the continent were payments oriented.

22:51.800 --> 23:00.800
 You mentioned, there's a small tangent, you mentioned the anonymous launch of Bitcoin is a sort of profound philosophical statement.

23:00.800 --> 23:01.800
 Pseudonymus.

23:01.800 --> 23:02.800
 What's that even mean?

23:02.800 --> 23:04.800
 There's a pseudonym.

23:04.800 --> 23:06.800
 There's an identity tied to it.

23:06.800 --> 23:09.800
 It's not just anonymous, it's Nakamoto.

23:09.800 --> 23:13.800
 So Nakamoto might represent one person or multiple people.

23:13.800 --> 23:15.800
 Let me ask, are you Satoshi Nakamoto?

23:15.800 --> 23:16.800
 Just checking.

23:16.800 --> 23:18.800
 If I were, would I tell you?

23:18.800 --> 23:19.800
 Yes, sure.

23:19.800 --> 23:20.800
 But maybe you slip.

23:20.800 --> 23:24.800
 A pseudonym is constructed identity.

23:24.800 --> 23:31.800
 Anonymity is just kind of this random drop something off and leave.

23:31.800 --> 23:34.800
 There's no intention to build an identity around it.

23:34.800 --> 23:45.800
 And while the identity being built was a short time window, it was meant to stick around, I think, and to be known.

23:45.800 --> 23:52.800
 And it's being honored in, you know, how the community thinks about building it.

23:52.800 --> 23:58.800
 Like the concept of Satoshi's, for instance, is one such example.

23:58.800 --> 24:14.800
 But I think it was smart not to do it anonymous, not to do it as a real identity, but to do it as pseudonym because I think it builds tangibility and a little bit of empathy that this was a human or a set of humans behind it.

24:14.800 --> 24:19.800
 And there's this natural identity that I can imagine.

24:19.800 --> 24:21.800
 But there is also a sacrifice of ego.

24:21.800 --> 24:23.800
 That's a pretty powerful thing.

24:23.800 --> 24:24.800
 Yeah, which is beautiful.

24:24.800 --> 24:34.800
 Would you do sort of philosophically to ask you the question, would you do all the same things you're doing now if your name wasn't attached to it?

24:34.800 --> 24:44.800
 Sort of if you had to sacrifice the ego, put another way, is your ego deeply tied in the decisions you've been making?

24:44.800 --> 24:45.800
 I hope not.

24:45.800 --> 24:52.800
 I mean, I believe I would certainly attempt to do the things without my name having to be attached with it.

24:52.800 --> 25:00.800
 But it's hard to do that in a corporation, legally.

25:00.800 --> 25:01.800
 That's the issue.

25:01.800 --> 25:12.800
 If I were to do more open source things than absolutely, I don't need my particular identity, my real identity associated with it.

25:12.800 --> 25:30.800
 But I think the appreciation that comes from doing something good and being able to see it and see people use it is pretty overwhelming and powerful, more so than maybe seeing your name in the headlines.

25:30.800 --> 25:34.800
 Let's talk about artificial intelligence a little bit, if we could.

25:34.800 --> 25:37.800
 70 years ago, Alan Turing formulated the Turing test.

25:37.800 --> 25:45.800
 To me, natural language is one of the most interesting spaces of problems that are tackled by artificial intelligence.

25:45.800 --> 25:48.800
 It's the canonical problem of what it means to be intelligent.

25:48.800 --> 25:50.800
 He formulated it as the Turing test.

25:50.800 --> 25:58.800
 Let me ask sort of the broad question, how hard do you think is it to pass the Turing test in the space of language?

25:58.800 --> 26:16.800
 From a very practical standpoint, I think where we are now and for at least years out is one where the artificial intelligence, machine learning, and deep learning models can bubble up interestingness very, very quickly.

26:16.800 --> 26:29.800
 And pair that with human discretion around severity, around depth, around nuance and meaning.

26:29.800 --> 26:42.800
 I think for me, the chasm across for general intelligence is to be able to explain why and the meaning behind something.

26:42.800 --> 26:48.800
 Behind a decision or a set of data.

26:48.800 --> 26:55.800
 So the explainability part is kind of essential to be able to explain using natural language, why the decisions were made, that kind of thing.

26:55.800 --> 27:08.800
 Yeah, I think that's one of our biggest risks in artificial intelligence going forward is we are building a lot of black boxes that can't necessarily explain why they made a decision or what criteria they used to make the decision.

27:08.800 --> 27:17.800
 And we're trusting them more and more from lending decisions to content recommendation to driving to health.

27:17.800 --> 27:21.800
 Like, you know, a lot of us have watches that tell us when to stand.

27:21.800 --> 27:24.800
 How is it deciding that? I mean, that one's pretty, pretty simple.

27:24.800 --> 27:34.800
 But you can imagine how complex they get and being able to explain the reasoning behind some of those recommendations seems to be an essential part.

27:34.800 --> 27:40.800
 Which is a very hard problem because sometimes even we can't explain why we make decisions.

27:40.800 --> 27:49.800
 That's what I was, I think we're being sometimes a little bit unfair to artificial intelligence systems because we're not very good at some of these things.

27:49.800 --> 28:02.800
 Do you think, I apologize for the ridiculous romanticized question, but on that line of thought, do you think we'll ever be able to build a system like in the movie Her that you could fall in love with?

28:02.800 --> 28:05.800
 So have that kind of deep connection with.

28:05.800 --> 28:12.800
 Hasn't that already happened? Hasn't someone in Japan fallen in love with his AI?

28:12.800 --> 28:15.800
 There's always going to be somebody that does that kind of thing.

28:15.800 --> 28:21.800
 I mean, at a much larger scale of actually building relationships of being deeper connections.

28:21.800 --> 28:26.800
 It doesn't have to be love, but it's just deeper connections with artificial intelligence systems.

28:26.800 --> 28:33.800
 That's less a function of the artificial intelligence and more a function of the individual and how they find meaning and where they find meaning.

28:33.800 --> 28:37.800
 Do you think we humans can find meaning in technology in this kind of way?

28:37.800 --> 28:39.800
 Yeah, 100%.

28:39.800 --> 28:49.800
 And I don't necessarily think it's a negative, but it's constantly going to evolve.

28:49.800 --> 29:06.800
 So I don't know, but meaning is something that's entirely subjective and I don't think it's going to be a function of finding the magic algorithm that enables everyone to love it.

29:06.800 --> 29:08.800
 But maybe, I don't know.

29:08.800 --> 29:19.800
 But that question really gets at the difference between human and machine. You had a little bit of an exchange with Elon Musk.

29:19.800 --> 29:26.800
 Basically, I mean, it's a trivial version of that, but I think there's a more fundamental question of, is it possible to tell the difference between a bot and a human?

29:26.800 --> 29:39.800
 And do you think it's, if we look into the future 10, 20 years out, do you think it would be possible or is it even necessary to tell the difference in the digital space between a human and a robot?

29:39.800 --> 29:45.800
 Can we have fulfilling relationships with each or do we need to tell the difference between them?

29:45.800 --> 29:51.800
 I think it's certainly useful in certain problem domains to be able to tell the difference.

29:51.800 --> 29:55.800
 I think in others it might not be as useful.

29:55.800 --> 30:02.800
 I think it's possible for us today to tell the difference is to reverse the meta of the Turing test.

30:02.800 --> 30:12.800
 Well, what's interesting is, I think the technology to create is moving much faster than the technology to detect.

30:12.800 --> 30:13.800
 You think so?

30:13.800 --> 30:20.800
 So if you look at like adversarial machine learning, there's a lot of systems that try to fool machine learning systems.

30:20.800 --> 30:28.800
 And at least for me, the hope is that the technology to defend will always be right there, at least.

30:28.800 --> 30:30.800
 Your sense is that...

30:30.800 --> 30:31.800
 I don't know if it'll be right there.

30:31.800 --> 30:33.800
 I mean, it's a race, right?

30:33.800 --> 30:41.800
 So the detection technologies have to be two or 10 steps ahead of the creation technologies.

30:41.800 --> 30:50.800
 This is a problem that I think the financial industry will face more and more because a lot of our risk models, for instance, are built around identity.

30:50.800 --> 30:52.800
 Payments ultimately comes down to identity.

30:52.800 --> 31:05.800
 And you can imagine a world where all this conversation around deep fakes goes towards a direction of driver's license or passports or state identities.

31:05.800 --> 31:14.800
 And people construct identities in order to get through a system such as ours to start accepting credit cards or into the cash up.

31:14.800 --> 31:18.800
 And those technologies seem to be moving very, very quickly.

31:18.800 --> 31:24.800
 Our ability to detect them, I think, is probably lagging at this point.

31:24.800 --> 31:29.800
 But certainly with more focus, we can get ahead of it.

31:29.800 --> 31:34.800
 But this is going to touch everything.

31:34.800 --> 31:38.800
 So I think it's like security.

31:38.800 --> 31:43.800
 We're never going to be able to build a perfect detection system.

31:43.800 --> 31:45.800
 We're only going to be able to...

31:45.800 --> 31:59.800
 What we should be focused on is the speed of evolving it and being able to take signals that show correctness or errors as quickly as possible and move.

31:59.800 --> 32:04.800
 And to be able to build that into our newer models or the self learning models.

32:04.800 --> 32:13.800
 Do you have other worries like some people like Elon and others have worries of existential threats of artificial intelligence, of artificial general intelligence?

32:13.800 --> 32:23.800
 Or if you think more narrowly about threats and concerns about more narrow artificial intelligence, like what are your thoughts in this domain?

32:23.800 --> 32:26.800
 Do you have concerns or are you more optimistic?

32:26.800 --> 32:34.800
 I think you've all, in his book 21 Lessons for the 21st Century, his last chapter is around meditation.

32:34.800 --> 32:39.800
 And you looked at the title of the chapter and you're like, oh, it's all meditation.

32:39.800 --> 32:50.800
 But what was interesting about that chapter is he believes that kids being born today, growing up today,

32:50.800 --> 33:00.800
 Google has a stronger sense of their preferences than they do, which you can easily imagine.

33:00.800 --> 33:09.800
 I can easily imagine today that Google probably knows my preference is more than my mother does.

33:09.800 --> 33:22.800
 Maybe not me per se, but for someone growing up only knowing the internet, only knowing what Google is capable of, or Facebook, or Twitter, or Square, or any of these things,

33:22.800 --> 33:31.800
 the self awareness is being offloaded to other systems and particularly these algorithms.

33:31.800 --> 33:44.800
 And his concern is that we lose that self awareness because the self awareness is now outside of us and it's doing such a better job at helping us direct our decisions around,

33:44.800 --> 33:49.800
 should I stand? Should I walk today? What doctor should I choose? Who should I date?

33:49.800 --> 33:53.800
 All these things we're now seeing play out very quickly.

33:53.800 --> 34:01.800
 So he sees meditation as a tool to build that self awareness and to bring the focus back on why do I make these decisions?

34:01.800 --> 34:07.800
 Why do I react in this way? Why did I have this thought? Where did that come from?

34:07.800 --> 34:09.800
 That's the way to regain control.

34:09.800 --> 34:25.800
 Your awareness, maybe not control, but awareness so that you can be aware that, yes, I am offloading this decision to this algorithm that I don't fully understand and can't tell me why it's doing the things it's doing because it's so complex.

34:25.800 --> 34:38.800
 That's not to say that the algorithm can't be a good thing. And to me, recommender systems, the best of what they can do is to help guide you on a journey of learning new ideas, of learning period.

34:38.800 --> 34:44.800
 It can be a great thing, but do you know you're doing that? Are you aware that you're inviting it to do that to you?

34:44.800 --> 34:57.800
 I think that's the risk he identifies, right? That's perfectly okay, but are you aware that you have that imitation and it's being acted upon?

34:57.800 --> 35:09.800
 And so that's a concern you're kind of highlighting that without a lack of awareness, you can just be like floating at sea, so awareness is key in the future of these artificial intelligence systems.

35:09.800 --> 35:17.800
 Yeah, the movie Wally, which I think is one of Pixar's best movies besides Ratatouille.

35:17.800 --> 35:19.800
 Ratatouille was incredible.

35:19.800 --> 35:21.800
 You had me until Ratatouille, okay.

35:21.800 --> 35:28.800
 Ratatouille was incredible. All right, we've come to the first point where we disagree.

35:28.800 --> 35:32.800
 It's the entrepreneurial story in the form of a rat.

35:32.800 --> 35:37.800
 I just remember just the soundtrack was really good.

35:37.800 --> 35:40.800
 Excellent.

35:40.800 --> 35:45.800
 What are your thoughts sticking on artificial intelligence a little bit about the displacement of jobs?

35:45.800 --> 35:49.800
 That's another perspective that candidates like Andrew Yang talk about.

35:49.800 --> 35:52.800
 Yang gang forever.

35:52.800 --> 35:57.800
 Yang gang. So he unfortunately, speaking of Yang gang, has recently dropped out.

35:57.800 --> 35:59.800
 I know, it was very disappointing and depressing.

35:59.800 --> 36:03.800
 Yeah, but on the positive side, he's I think launching a podcast.

36:03.800 --> 36:05.800
 Really? Cool.

36:05.800 --> 36:10.800
 Yeah, he just announced that. I'm sure he'll try to talk you into trying to come on to the podcast.

36:10.800 --> 36:13.800
 What about Ratatouille?

36:13.800 --> 36:17.800
 Yeah, maybe he'll be more welcoming of the Ratatouille argument.

36:17.800 --> 36:22.800
 What are your thoughts on his concerns of the displacement of jobs of automations?

36:22.800 --> 36:26.800
 Of course, there's positive impacts that could come from automation and AI,

36:26.800 --> 36:29.800
 but there could also be negative impacts.

36:29.800 --> 36:33.800
 And within that framework, what are your thoughts about universal basic income?

36:33.800 --> 36:40.800
 So these interesting new ideas of how we can empower people in the economy.

36:40.800 --> 36:46.800
 I think he was 100% right on almost every dimension.

36:46.800 --> 36:48.800
 We see this in squares business.

36:48.800 --> 36:53.800
 I mean, he identified truck drivers. I'm from Missouri.

36:53.800 --> 37:05.800
 And he certainly pointed to the concern and the issue that people from where I'm from feel every single day

37:05.800 --> 37:09.800
 that is often invisible and not talked about enough.

37:09.800 --> 37:15.800
 The next big one is cashiers. This is where it pertains to squares business.

37:15.800 --> 37:23.800
 We are seeing more and more of the point of sale move to the individual customer's hand in the form of their phone

37:23.800 --> 37:26.800
 and apps and preorder and order ahead.

37:26.800 --> 37:31.800
 We're seeing more kiosks. We're seeing more things like Amazon Go.

37:31.800 --> 37:47.800
 And the number of workers as a cashier in retail is immense and there's no real answers on how they transform their skills

37:47.800 --> 37:50.800
 and work into something else.

37:50.800 --> 37:55.800
 And I think that does lead to a lot of really negative ramifications.

37:55.800 --> 38:03.800
 And the important point that he brought up around universal basic income is given that the shift is going to come

38:03.800 --> 38:14.800
 and given it's going to take time to set people up with new skills and new careers,

38:14.800 --> 38:17.800
 they need to have a floor to be able to survive.

38:17.800 --> 38:21.800
 And this $1,000 a month is such a floor.

38:21.800 --> 38:25.800
 It's not going to incentivize you to quit your job because it's not enough.

38:25.800 --> 38:34.800
 But it will enable you to not have to worry as much about just getting on day to day

38:34.800 --> 38:43.800
 so that you can focus on what am I going to do now and what skills do I need to acquire.

38:43.800 --> 38:52.800
 And I think a lot of people point to the fact that during the industrial age,

38:52.800 --> 38:57.800
 we had the same concerns around automation, factory lines, and everything worked out okay.

38:57.800 --> 39:08.800
 But the biggest change is just the velocity and the centralization of a lot of the things that make this work,

39:08.800 --> 39:14.800
 which is the data and the algorithms that work on this data.

39:14.800 --> 39:24.800
 I think the second biggest scary thing is just how around AI is just who actually owns the data

39:24.800 --> 39:32.800
 and who can operate on it and are we able to share the insights from the data

39:32.800 --> 39:38.800
 so that we can also build algorithms that help our needs or help our business or whatnot.

39:38.800 --> 39:45.800
 So that's where I think regulation could play a strong and positive part.

39:45.800 --> 39:52.800
 First, looking at the primitives of AI and the tools we use to build these services

39:52.800 --> 39:55.800
 that will ultimately touch every single aspect of the human experience.

39:55.800 --> 40:04.800
 And then where data is owned and how it's shared.

40:04.800 --> 40:11.800
 So those are the answers that as a society, as a world, we need to have better answers around,

40:11.800 --> 40:13.800
 which we're currently not.

40:13.800 --> 40:17.800
 They're just way too centralized into a few very, very large companies.

40:17.800 --> 40:25.800
 But I think it was spot on with identifying the problem and proposing solutions that would actually work.

40:25.800 --> 40:30.800
 At least that we'd learned from that you could expand or evolve.

40:30.800 --> 40:38.800
 But I think UBI is well past its due.

40:38.800 --> 40:43.800
 It was certainly trumpeted by Martin Luther King and even before him as well.

40:43.800 --> 40:49.800
 And like you said, the exact $1,000 mark might not be the correct one,

40:49.800 --> 40:55.800
 but you should take the steps to try to implement these solutions and see what works.

40:55.800 --> 40:56.800
 100%.

40:56.800 --> 40:58.800
 So I think you and I eat similar diets.

40:58.800 --> 41:00.800
 At least I was...

41:00.800 --> 41:02.800
 The first time I've heard this.

41:02.800 --> 41:04.800
 Yeah, so I was doing it...

41:04.800 --> 41:07.800
 The first time anyone has said that to me in this case.

41:07.800 --> 41:10.800
 Yeah, but it's becoming more and more cool.

41:10.800 --> 41:13.800
 But I was doing it before it was cool.

41:13.800 --> 41:15.800
 So intermittent fasting and fasting in general.

41:15.800 --> 41:16.800
 I really enjoy...

41:16.800 --> 41:22.800
 I love food, but I also love suffering because I'm Russian.

41:22.800 --> 41:31.800
 So fasting kind of makes you appreciate what it is to be human somehow.

41:31.800 --> 41:37.800
 But outside the philosophical stuff, I have a more specific question.

41:37.800 --> 41:43.800
 It also helps me as a programmer and a deep thinker from the scientific perspective

41:43.800 --> 41:46.800
 to sit there for many hours and focus deeply.

41:46.800 --> 41:50.800
 Maybe you were a hacker before you were CEO.

41:50.800 --> 41:57.800
 What have you learned about diet, lifestyle, mindset that helps you maximize mental performance

41:57.800 --> 42:03.800
 to be able to focus, to think deeply in this world of distractions?

42:03.800 --> 42:07.800
 I think I just took it for granted for too long.

42:07.800 --> 42:09.800
 Which aspects?

42:09.800 --> 42:15.800
 Just the social structure of we eat three meals a day and there's snacks in between.

42:15.800 --> 42:18.800
 I just never really asked the question why.

42:18.800 --> 42:21.800
 Oh, by the way, in case people don't know, I think a lot of people know

42:21.800 --> 42:25.800
 but you at least you famously eat once a day.

42:25.800 --> 42:27.800
 You still eat once a day?

42:27.800 --> 42:29.800
 Yep, I eat dinner.

42:29.800 --> 42:31.800
 By the way, what made you decide to eat once a day?

42:31.800 --> 42:34.800
 Because to me, that was a huge revolution that you don't have to eat breakfast.

42:34.800 --> 42:36.800
 I felt like I was a rebel.

42:36.800 --> 42:40.800
 I abandoned my parents or something and became an anarchist.

42:40.800 --> 42:44.800
 The first week you start doing it, it feels like you have a superpower.

42:44.800 --> 42:46.800
 Then you realize it's not really a superpower.

42:46.800 --> 42:57.800
 But I think you realize, at least I realize, how much our mind dictates what we're possible with.

42:57.800 --> 43:04.800
 Sometimes we have structures around us that incentivize this three meal a day thing,

43:04.800 --> 43:12.800
 which is purely social structure versus necessity for our health and for our bodies.

43:12.800 --> 43:20.800
 I started doing it because I played a lot with my diet when I was a kid

43:20.800 --> 43:25.800
 and I was vegan for two years and just went all over the place.

43:25.800 --> 43:33.800
 Just because health is the most precious thing we have and none of us really understand it.

43:33.800 --> 43:43.800
 Being able to ask the question through experiments that I can perform on myself and learn about is compelling to me.

43:43.800 --> 43:53.800
 I heard this one guy on a podcast, Wim Hof, who's famous for doing ice baths and holding his breath and all these things.

43:53.800 --> 43:56.800
 He said he only eats one meal a day.

43:56.800 --> 43:59.800
 I'm like, wow, that sounds super challenging and uncomfortable.

43:59.800 --> 44:01.800
 I'm going to do it.

44:01.800 --> 44:09.800
 I learned the most when I make myself, I wouldn't say suffer, but when I make myself feel uncomfortable

44:09.800 --> 44:13.800
 because everything comes to bear in those moments.

44:13.800 --> 44:19.800
 You really learn what you're about or what you're not.

44:19.800 --> 44:23.800
 I've been doing that my whole life.

44:23.800 --> 44:27.800
 When I was a kid, I could not speak.

44:27.800 --> 44:31.800
 I had to go to a speech therapist and it made me extremely shy.

44:31.800 --> 44:39.800
 Then one day I realized I can't keep doing this and I signed up for the speech club.

44:39.800 --> 44:49.800
 It was the most uncomfortable thing I could imagine doing, getting a topic on a note card,

44:49.800 --> 44:53.800
 having five minutes to write a speech about whatever that topic is,

44:53.800 --> 44:59.800
 not being able to use the note card while speaking and speaking for five minutes about that topic.

44:59.800 --> 45:09.800
 It gave me so much perspective around the power of communication, around my own deficiencies

45:09.800 --> 45:13.800
 and around if I set my mind to do something, I'll do it.

45:13.800 --> 45:16.800
 It gave me a lot more confidence.

45:16.800 --> 45:18.800
 I see fasting in the same light.

45:18.800 --> 45:23.800
 This is something that was interesting, challenging, uncomfortable

45:23.800 --> 45:29.800
 and has given me so much learning and benefit as a result.

45:29.800 --> 45:35.800
 It will lead to other things that I'll experiment with and play with.

45:35.800 --> 45:39.800
 It does feel a little bit like a superpower sometimes.

45:39.800 --> 45:42.800
 The most boring superpower one can imagine.

45:42.800 --> 45:47.800
 It's quite incredible. The clarity of mind is pretty interesting.

45:47.800 --> 45:53.800
 Speaking of suffering, you talk about facing difficult ideas. You meditate.

45:53.800 --> 45:59.800
 You think about the broad context of life of our society.

45:59.800 --> 46:05.800
 Let me apologize again for the romanticized question, but do you ponder your own mortality?

46:05.800 --> 46:13.800
 Do you think about death, about the finiteness of human existence when you meditate,

46:13.800 --> 46:21.800
 when you think about it, and how do you make sense of it that this thing ends?

46:21.800 --> 46:25.800
 Well, I don't try to make sense of it. I do think about it every day.

46:25.800 --> 46:28.800
 It's a daily multiple times a day.

46:28.800 --> 46:30.800
 Are you afraid of death?

46:30.800 --> 46:32.800
 No, I'm not afraid of it.

46:32.800 --> 46:36.800
 I think it's a transformation. I don't know to what.

46:36.800 --> 46:43.800
 It's also a tool to feel the importance of every moment.

46:43.800 --> 46:51.800
 I just use as a reminder, I have an hour. Is this really what I'm going to spend the hour doing?

46:51.800 --> 46:55.800
 I only have so many more sunsets and sunrises to watch.

46:55.800 --> 47:02.800
 I'm not going to get up for it. I'm not going to make sure that I try to see it.

47:02.800 --> 47:08.800
 It just puts a lot into perspective, and it helps me prioritize.

47:08.800 --> 47:15.800
 I don't see it as something that I dread or is dreadful.

47:15.800 --> 47:19.800
 It's a tool that is available to every single person to use every day,

47:19.800 --> 47:23.800
 because it shows how precious life is, and there's reminders every single day,

47:23.800 --> 47:29.800
 whether it be your own health, or a friend, or a coworker, or something you see in the news.

47:29.800 --> 47:34.800
 To me, it's just a question of what we do with that daily reminder.

47:34.800 --> 47:39.800
 For me, am I really focused on what matters?

47:39.800 --> 47:46.800
 Sometimes that might be work, sometimes that might be friendships, or family, or relationships, or whatnot,

47:46.800 --> 47:50.800
 but it's the ultimate clarifier in that sense.

47:50.800 --> 47:57.800
 On the question of what matters, another ridiculously big question of, once you try to make sense of it,

47:57.800 --> 48:01.800
 what do you think is the meaning of it all, the meaning of life?

48:01.800 --> 48:06.800
 What gives you purpose, happiness, meaning?

48:06.800 --> 48:18.800
 A lot does. Just being able to be aware of the fact that I'm alive is pretty meaningful.

48:18.800 --> 48:27.800
 The connections I feel with individuals, whether they're people I just meet, or long lasting friendships,

48:27.800 --> 48:30.800
 or my family is meaningful.

48:30.800 --> 48:38.800
 Seeing people use something that I helped build is really meaningful and powerful to me.

48:38.800 --> 48:43.800
 But that sense of, I think ultimately comes down to the sense of connection,

48:43.800 --> 48:49.800
 and just feeling like I am bigger, I am part of something that's bigger than myself,

48:49.800 --> 48:55.800
 and I can feel it directly in small ways or large ways, however it manifests.

48:55.800 --> 48:59.800
 This is probably it.

48:59.800 --> 49:04.800
 Last question, do you think we're living in a simulation?

49:04.800 --> 49:08.800
 I don't know. It's a pretty fun one, if we are.

49:08.800 --> 49:16.800
 But also crazy and random and with tons of problems.

49:16.800 --> 49:18.800
 Would you have it any other way?

49:18.800 --> 49:26.800
 Yeah. I mean, I just think it's taken us way too long as a planet to realize we're all in this together,

49:26.800 --> 49:33.800
 and we all are connected in very significant ways.

49:33.800 --> 49:41.800
 I think we hide our connectivity very well through ego, through whatever it is of the day.

49:41.800 --> 49:50.800
 But that is the one thing I would want to work towards changing, and that's how I would have it another way.

49:50.800 --> 49:54.800
 Because if we can't do that, then how are we going to connect to all the other simulations?

49:54.800 --> 49:58.800
 Because that's the next step is what's happening in the other simulation.

49:58.800 --> 50:06.800
 We're escaping this one and spanning across the multiple simulations and sharing it and on the fun.

50:06.800 --> 50:09.800
 I don't think there's a better way to end it.

50:09.800 --> 50:11.800
 Jack, thank you so much for all the work you do.

50:11.800 --> 50:15.800
 There's probably other ways that we've ended this and other simulations that may have been better.

50:15.800 --> 50:17.800
 Well, to wait and see.

50:17.800 --> 50:19.800
 Thanks so much for talking to me.

50:19.800 --> 50:20.800
 Thank you.

50:20.800 --> 50:26.800
 Thanks for listening to this conversation with Jack Dorsey, and thank you to our sponsor, Masterclass.

50:26.800 --> 50:33.800
 Please consider supporting this podcast by signing up to masterclass at masterclass.com slash lex.

50:33.800 --> 50:38.800
 If you enjoy this podcast, subscribe on YouTube, review it with five stars on Apple Podcasts.

50:38.800 --> 50:44.800
 Support on Patreon or simply connect with me on Twitter at Lex Freedman.

50:44.800 --> 50:49.800
 And now let me leave you with some words about Bitcoin from Paul Graham.

50:49.800 --> 50:51.800
 I'm very intrigued by Bitcoin.

50:51.800 --> 50:54.800
 It has all the signs of a paradigm shift.

50:54.800 --> 51:01.800
 Hackers love it, yet it is described as a toy, just like microcomputers.

51:01.800 --> 51:25.800
 Thank you for listening and hope to see you next time.

