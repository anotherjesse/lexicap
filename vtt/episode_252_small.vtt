WEBVTT

00:00.000 --> 00:02.540
 The following is a conversation with Elon Musk.

00:02.540 --> 00:06.960
 His third time on this, the Lex Friedman podcast.

00:06.960 --> 00:08.400
 Yeah, make yourself comfortable.

00:08.400 --> 00:09.240
 Boo.

00:09.240 --> 00:11.040
 No, wow, okay.

00:11.040 --> 00:12.760
 Do you don't do the headphones thing?

00:12.760 --> 00:13.600
 No.

00:13.600 --> 00:14.440
 Okay.

00:14.440 --> 00:15.280
 I mean, how close do I get?

00:15.280 --> 00:16.120
 I need to get to this thing.

00:16.120 --> 00:17.560
 The closer you are, the sexier you sound.

00:17.560 --> 00:18.400
 Hey, babe.

00:18.400 --> 00:19.240
 Yep.

00:19.240 --> 00:20.920
 Can't get enough of the all that, baby.

00:20.920 --> 00:25.920
 I'm gonna clip that out anytime somebody messaged me about it.

00:26.720 --> 00:28.280
 Yeah, you want my body.

00:28.280 --> 00:30.080
 And you think I'm sexy.

00:30.080 --> 00:32.060
 Come right out and tell me so.

00:33.560 --> 00:35.880
 Do, do, do, do, do.

00:38.880 --> 00:39.720
 So good.

00:39.720 --> 00:40.560
 Okay.

00:40.560 --> 00:41.880
 Serious mode activate.

00:41.880 --> 00:43.200
 All right.

00:43.200 --> 00:44.320
 Serious mode.

00:44.320 --> 00:45.720
 Come on, you're Russian, you can be serious.

00:45.720 --> 00:46.560
 Yeah, I know.

00:46.560 --> 00:47.680
 Everyone's serious all the time in Russia.

00:47.680 --> 00:48.520
 Yeah.

00:48.520 --> 00:49.360
 Yeah.

00:49.360 --> 00:50.600
 We'll get there.

00:50.600 --> 00:51.920
 We'll get there.

00:51.920 --> 00:52.760
 Yeah.

00:52.760 --> 00:53.600
 It's gotten soft.

00:55.280 --> 00:57.720
 Allow me to say that the SpaceX launch

00:57.720 --> 01:00.920
 of human beings to orbit on May 30th, 2020

01:02.120 --> 01:03.880
 was seen by many as the first step

01:03.880 --> 01:07.400
 in a new era of human space exploration.

01:07.400 --> 01:10.600
 These human spaceflight missions were a beacon of hope

01:10.600 --> 01:12.720
 to me and to millions over the past two years

01:12.720 --> 01:14.640
 as our world has been going through

01:14.640 --> 01:18.040
 one of the most difficult periods in recent human history.

01:18.040 --> 01:21.720
 We saw, we see the rise of division, fear, cynicism,

01:21.720 --> 01:24.600
 and the loss of common humanity,

01:24.600 --> 01:26.360
 right when it is needed most.

01:26.360 --> 01:29.040
 So first, Elon, let me say thank you

01:29.040 --> 01:30.600
 for giving the world hope and reason

01:30.600 --> 01:32.480
 to be excited about the future.

01:32.480 --> 01:34.160
 Oh, it's kind of you to say.

01:34.160 --> 01:35.440
 I do want to do that.

01:35.440 --> 01:38.280
 Humanity has obviously a lot of issues

01:38.280 --> 01:42.600
 and people at times do bad things,

01:42.600 --> 01:47.240
 but despite all that, I love humanity

01:47.240 --> 01:52.240
 and I think we should make sure we do everything

01:52.240 --> 01:54.440
 we can to have a good future and an exciting future

01:54.440 --> 01:58.360
 and one where that maximizes the happiness of the people.

01:58.360 --> 02:00.960
 Let me ask about Crew Dragon demo two.

02:00.960 --> 02:04.400
 So that first flight with humans on board,

02:04.400 --> 02:06.200
 how did you feel leading up to that launch?

02:06.200 --> 02:07.280
 Were you scared?

02:07.280 --> 02:08.320
 Were you excited?

02:08.320 --> 02:09.680
 Was it going through your mind?

02:09.680 --> 02:11.400
 So much was at stake.

02:14.080 --> 02:17.360
 Yeah, no, that was extremely stressful, no question.

02:17.360 --> 02:21.760
 We obviously could not let them down in any way.

02:21.760 --> 02:28.720
 So extremely stressful, I'd say, to say the least.

02:28.720 --> 02:30.880
 I was confident that at the time that we launched,

02:30.880 --> 02:36.840
 that no one could think of anything at all to do

02:36.840 --> 02:40.240
 that would improve the probability of success

02:40.240 --> 02:43.320
 and we racked our brains to think of any possible way

02:43.320 --> 02:44.840
 to improve the probability of success.

02:44.840 --> 02:48.040
 We could not think of anything more and nor could NASA

02:48.040 --> 02:51.440
 and so that's just the best that we could do.

02:51.440 --> 02:55.680
 So then we went ahead and launched.

02:55.680 --> 03:00.320
 Now, I'm not a religious person, but I nonetheless

03:00.320 --> 03:03.760
 got on my knees and prayed for that mission.

03:03.760 --> 03:05.840
 Were you able to sleep?

03:05.840 --> 03:07.600
 No.

03:07.600 --> 03:10.800
 How did it feel when it was a success?

03:10.800 --> 03:12.760
 First, when the launch was a success

03:12.760 --> 03:16.920
 and when they returned back home or back to Earth?

03:16.920 --> 03:20.160
 It was a great relief.

03:20.160 --> 03:23.400
 Yeah, for high stress situations,

03:23.400 --> 03:25.760
 I find it's not so much elation as relief.

03:28.400 --> 03:33.920
 And I think once, as we got more comfortable and proved out

03:33.920 --> 03:39.520
 the systems, because we really got to make sure everything

03:39.520 --> 03:43.120
 works, it was definitely a lot more enjoyable

03:43.120 --> 03:46.440
 with the subsequent astronaut missions.

03:46.440 --> 03:50.680
 And I thought the inspiration mission was actually

03:50.680 --> 03:53.560
 very inspiring, the inspiration for mission.

03:53.560 --> 03:56.560
 I'd encourage people to watch the inspiration

03:56.560 --> 03:57.560
 documentary on Netflix.

03:57.560 --> 04:00.200
 It's actually really good.

04:00.200 --> 04:01.440
 And it really isn't.

04:01.440 --> 04:03.320
 I was actually inspired by that.

04:03.320 --> 04:09.320
 And so that one, I felt I was able to enjoy the actual mission

04:09.320 --> 04:10.880
 and not just be super stressed all the time.

04:10.880 --> 04:13.680
 So for people that somehow don't know,

04:13.680 --> 04:17.400
 it's the all civilian first time, all civilian out

04:17.400 --> 04:19.320
 to space, out to orbit.

04:19.320 --> 04:22.440
 Yeah, and it was, I think, the highest orbit

04:22.440 --> 04:26.080
 that in like 30 or 40 years or something.

04:26.080 --> 04:29.800
 The only one that was higher was the one shuttle, sorry,

04:29.800 --> 04:32.480
 Hubble servicing mission.

04:32.480 --> 04:37.800
 And then before that, it would have been Apollo in 72.

04:37.800 --> 04:39.120
 It's pretty wild.

04:39.120 --> 04:40.480
 So it's cool.

04:40.480 --> 04:45.000
 And I think as a species, we want

04:45.000 --> 04:49.720
 to be continuing to do better and reach higher ground.

04:49.720 --> 04:52.880
 And I think it would be extremely tragic

04:52.880 --> 04:57.040
 if Apollo was the high watermark for humanity.

04:57.040 --> 05:00.080
 And that's as far as we ever got.

05:00.080 --> 05:06.400
 And it's concerning that here we are, 49 years

05:06.400 --> 05:09.040
 after the last mission to the moon.

05:09.040 --> 05:14.440
 And so almost half a century, and we've not been back.

05:14.440 --> 05:16.240
 And that's worrying.

05:16.240 --> 05:20.240
 It's like, does that mean we've peaked as a civilization

05:20.240 --> 05:20.920
 or what?

05:20.920 --> 05:24.440
 So I think we've got to get back to the moon

05:24.440 --> 05:27.120
 and build a base there, a science base.

05:27.120 --> 05:29.760
 I think we could learn a lot about the nature of the universe

05:29.760 --> 05:33.200
 if we have a proper science base on the moon.

05:33.200 --> 05:35.480
 Like we have a science base in Antarctica

05:35.480 --> 05:38.040
 and many other parts of the world.

05:38.040 --> 05:41.840
 And so that's the next big thing.

05:41.840 --> 05:45.640
 We've got to have a serious moon base

05:45.640 --> 05:49.920
 and then get people to Mars and get out there

05:49.920 --> 05:52.520
 and be a space bearing civilization.

05:52.520 --> 05:54.440
 I'll ask you about some of those details.

05:54.440 --> 05:57.480
 But since you're so busy with the hard engineering

05:57.480 --> 06:00.440
 challenges of everything that's involved,

06:00.440 --> 06:02.960
 are you still able to marvel at the magic of it all,

06:02.960 --> 06:05.840
 of space travel, of every time the rocket goes up,

06:05.840 --> 06:08.160
 especially when it's a crewed mission?

06:08.160 --> 06:11.440
 Or are you just so overwhelmed with all the challenges

06:11.440 --> 06:13.520
 that you have to solve?

06:13.520 --> 06:16.640
 And actually, to add to that, the reason

06:16.640 --> 06:19.240
 that I wanted to ask this question of May 30th,

06:19.240 --> 06:20.840
 it's been some time.

06:20.840 --> 06:23.520
 So you can look back and think about the impact already.

06:23.520 --> 06:26.360
 It's already, at the time, it was an engineering problem,

06:26.360 --> 06:27.240
 maybe.

06:27.240 --> 06:29.320
 Now it's becoming a historic moment.

06:29.320 --> 06:31.640
 Like it's a moment that, how many moments

06:31.640 --> 06:33.720
 would be remembered about the 21st century?

06:33.720 --> 06:37.280
 To me, that or something like that,

06:37.280 --> 06:39.120
 maybe inspiration for one of those

06:39.120 --> 06:41.600
 would be remembered as the early steps of a new age

06:41.600 --> 06:44.200
 of space exploration.

06:44.200 --> 06:46.640
 Yeah, I mean, during the launches itself,

06:46.640 --> 06:49.040
 so I think maybe some people will know,

06:49.040 --> 06:50.160
 but a lot of people don't know.

06:50.160 --> 06:52.080
 It's like, I'm actually the chief engineer of SpaceX.

06:52.080 --> 06:59.040
 So I've signed off on pretty much all the design decisions.

06:59.040 --> 07:03.680
 And so if there's something that goes wrong with that,

07:03.680 --> 07:07.120
 a vehicle, it's fundamentally my fault.

07:07.120 --> 07:13.920
 So I'm really just thinking about all the things that.

07:13.920 --> 07:16.400
 So when I see the rocket, I see all the things

07:16.400 --> 07:18.520
 that could go wrong and the things that could be better.

07:18.520 --> 07:21.480
 And the same with the Dragon spacecraft,

07:21.480 --> 07:23.680
 it's like, a lot of people will say, oh, this

07:23.680 --> 07:25.560
 is a spacecraft or a rocket.

07:25.560 --> 07:27.000
 And this looks really cool.

07:27.000 --> 07:31.200
 I'm like, I've like a readout of like, these are the risks.

07:31.200 --> 07:32.560
 These are the problems.

07:32.560 --> 07:33.840
 That's what I see.

07:33.840 --> 07:36.280
 Like, tch, tch, tch, tch, tch, tch.

07:36.280 --> 07:40.400
 So it's not what other people see when they see the product.

07:40.400 --> 07:44.360
 So let me ask you then to analyze Starship in that same way.

07:44.360 --> 07:48.000
 I know you'll talk about it in more detail about Starship

07:48.000 --> 07:49.560
 in the near future, perhaps.

07:49.560 --> 07:51.760
 Yeah, we'll talk about it now if you want.

07:51.760 --> 07:54.120
 But just in that same way, like you said,

07:54.120 --> 07:59.160
 you see when you see a rocket, you see a list of risks.

07:59.160 --> 08:01.040
 In that same way, you said that Starship

08:01.040 --> 08:03.280
 was a really hard problem.

08:03.280 --> 08:05.480
 So there's many ways I can ask this.

08:05.480 --> 08:09.600
 But if you magically could solve one problem perfectly,

08:09.600 --> 08:13.040
 one engineering problem perfectly, which one would it be?

08:13.040 --> 08:14.600
 On Starship?

08:14.600 --> 08:15.560
 Sorry, on Starship.

08:15.560 --> 08:20.040
 So is it maybe related to the efficiency, the engine,

08:20.040 --> 08:22.000
 the weight of the different components, the complexity

08:22.000 --> 08:25.400
 of various things, maybe the controls of the crazy thing

08:25.400 --> 08:26.760
 as to do the land?

08:26.760 --> 08:31.440
 Oh, actually, by far, the biggest thing absorbing my time

08:31.440 --> 08:38.160
 is engine production, not the design of the engine.

08:38.160 --> 08:42.640
 I've often said prototypes are easy, production is hard.

08:45.360 --> 08:48.680
 So we have the most advanced rocket engine

08:48.680 --> 08:52.680
 that's ever been designed.

08:52.680 --> 08:55.240
 Because I say currently the best rocket engine ever

08:55.240 --> 09:01.880
 is probably the RD180 or RD170, the Dora Russian engine,

09:01.880 --> 09:03.640
 basically.

09:03.640 --> 09:06.760
 And still, I think an engine should only count

09:06.760 --> 09:09.000
 if it's gotten something to orbit.

09:09.000 --> 09:12.680
 So our engine has not gotten anything to orbit yet.

09:12.680 --> 09:13.760
 But it is.

09:13.760 --> 09:16.560
 It's the first engine that's actually

09:16.560 --> 09:20.240
 better than the Russian RD engines,

09:20.240 --> 09:22.640
 which were an amazing design.

09:22.640 --> 09:24.120
 So you're talking about Raptor engine.

09:24.120 --> 09:25.520
 What makes it amazing?

09:25.520 --> 09:29.680
 What are the different aspects of it that make it?

09:29.680 --> 09:31.920
 What do you get the most excited about

09:31.920 --> 09:35.080
 if the whole thing works in terms of efficiency,

09:35.080 --> 09:37.120
 all those kinds of things?

09:37.120 --> 09:45.880
 Well, it's, but Raptor is a full flow staged combustion

09:45.880 --> 09:50.880
 engine, and it's operating at a very high chamber pressure.

09:50.880 --> 09:55.200
 So one of the key figures perhaps the key figure of merit

09:55.200 --> 09:59.920
 is what is the chamber pressure at which the rocket engine

09:59.920 --> 10:00.800
 can operate?

10:00.800 --> 10:03.040
 That's the combustion chamber pressure.

10:03.040 --> 10:07.720
 So Raptor is designed to operate at 300 bar, possibly

10:07.720 --> 10:10.360
 maybe higher, 300 atmospheres.

10:10.360 --> 10:15.920
 So the record right now for operational engine

10:15.920 --> 10:18.120
 is the RD engine that I mentioned, the Russian RD, which

10:18.120 --> 10:22.600
 is, I believe, around 267 bar.

10:22.600 --> 10:25.880
 And the difficulty of the chamber pressure

10:25.880 --> 10:27.880
 is increases on a nonlinear basis.

10:27.880 --> 10:36.960
 So 10% more chamber pressure is more like 50%, more difficult.

10:36.960 --> 10:39.320
 But that chamber pressure, that is

10:39.320 --> 10:42.280
 what allows you to get a very high power

10:42.280 --> 10:46.160
 density for the engine.

10:46.160 --> 10:53.280
 So enabling a very high thrust to weight ratio

10:53.280 --> 10:57.000
 and a very high specific impulse.

10:57.000 --> 10:59.000
 So specific impulse is like a measure

10:59.000 --> 11:01.680
 of the efficiency of a rocket engine.

11:01.680 --> 11:07.120
 It's really the effect of exhaust

11:07.120 --> 11:09.920
 velocity of the gas coming out of the engine.

11:09.920 --> 11:17.160
 So with a very high chamber pressure,

11:17.160 --> 11:22.200
 you can have a compact engine that nonetheless

11:22.200 --> 11:24.160
 has a high expansion ratio, which

11:24.160 --> 11:31.800
 is the ratio between the exit nozzle and the throat.

11:31.800 --> 11:35.960
 So you see a rocket engine's got sort of like a hourglass shape.

11:35.960 --> 11:39.720
 It's like a chamber and then it necks down and there's a nozzle.

11:39.720 --> 11:41.840
 And the ratio of the exit diameter

11:41.840 --> 11:45.960
 to the throat is the expansion ratio.

11:45.960 --> 11:51.360
 So why is it such a hard engine to manufacture at scale?

11:51.360 --> 11:53.160
 It's very complex.

11:53.160 --> 11:55.160
 So what is complexity mean here?

11:55.160 --> 11:56.760
 There's a lot of components involved.

11:56.760 --> 12:03.080
 There's a lot of components and a lot of unique materials.

12:03.080 --> 12:07.040
 So we had to invent several alloys

12:07.040 --> 12:11.240
 that don't exist in order to make this engine work.

12:11.240 --> 12:14.600
 So it's a materials problem, too.

12:14.600 --> 12:16.280
 It's a materials problem.

12:16.280 --> 12:20.840
 And in a full flow stage combustion,

12:20.840 --> 12:24.200
 there are many feedback loops in the system.

12:24.200 --> 12:31.840
 So basically, you've got propellants and hot gas

12:31.840 --> 12:39.120
 flowing simultaneously to so many different places on the engine.

12:39.120 --> 12:43.680
 And they all have a recursive effect on each other.

12:43.680 --> 12:44.880
 So you change one thing here.

12:44.880 --> 12:46.080
 It has a recursive effect here.

12:46.080 --> 12:47.440
 It changes something over there.

12:47.440 --> 12:52.480
 And it's quite hard to control.

12:52.480 --> 12:54.400
 Like there's a reason no one's made this before.

12:54.400 --> 13:04.560
 But the reason we're doing a stage combustion full flow

13:04.560 --> 13:12.640
 is because it has the highest theoretical possible efficiency.

13:12.640 --> 13:19.920
 So in order to make a fully reusable rocket,

13:19.920 --> 13:25.360
 that's really the holy grail of orbital rocketry.

13:25.360 --> 13:28.320
 You have to have everything's got to be the best.

13:28.320 --> 13:30.880
 It's got to be the best engine, the best airframe,

13:30.880 --> 13:38.120
 the best heat shield, extremely light avionics,

13:38.120 --> 13:40.640
 very clever control mechanisms.

13:40.640 --> 13:45.120
 You've got to shed mass in any possible way that you can.

13:45.120 --> 13:48.280
 For example, instead of putting landing legs on the booster

13:48.280 --> 13:50.720
 and ship, we are going to catch them with a tower

13:50.720 --> 13:53.200
 to save the weight of the landing legs.

13:53.200 --> 13:56.680
 So that's like, I mean, we're talking

13:56.680 --> 14:02.080
 about catching the largest flying object ever made

14:02.080 --> 14:06.640
 with on a giant tower with chopstick arms.

14:06.640 --> 14:09.520
 It's like a karate kid with the fly, but much bigger.

14:12.520 --> 14:17.560
 This probably won't work the first time.

14:17.560 --> 14:18.800
 Anyway, so this is bananas.

14:18.800 --> 14:19.760
 This is banana stuff.

14:19.760 --> 14:23.080
 So you mentioned that you doubt, well, not you doubt,

14:23.080 --> 14:26.440
 but there's days or moments when you doubt

14:26.440 --> 14:28.320
 that this is even possible.

14:28.320 --> 14:30.120
 It's so difficult.

14:30.120 --> 14:35.400
 The possible part is, well, at this point,

14:35.400 --> 14:37.320
 I think we will get Starship to work.

14:41.360 --> 14:42.880
 This is a question of timing.

14:42.880 --> 14:45.440
 How long will it take us to do this?

14:45.440 --> 14:47.680
 How long will it take us to actually achieve

14:47.680 --> 14:50.480
 full and rapid reusability?

14:50.480 --> 14:52.680
 Because it will take probably many launches

14:52.680 --> 14:57.640
 before we are able to have full and rapid reusability.

14:57.640 --> 15:06.120
 But I can say that the physics pencils out, like we're not,

15:06.120 --> 15:10.120
 at this point, I'd say we're confident that, like let's say,

15:10.120 --> 15:12.640
 I'm very confident success is in the set

15:12.640 --> 15:14.760
 of all possible outcomes.

15:14.760 --> 15:16.880
 It's not an all set.

15:16.880 --> 15:18.280
 For a while there, I was not convinced

15:18.280 --> 15:22.000
 that success was in the set of possible outcomes, which

15:22.000 --> 15:23.760
 is very important, actually.

15:23.760 --> 15:29.640
 But so we were saying there's a chance.

15:29.640 --> 15:33.200
 I'm saying there's a chance, exactly.

15:33.200 --> 15:38.200
 Just not sure how long it will take.

15:38.200 --> 15:39.560
 We have a very talented team.

15:39.560 --> 15:43.400
 They're working night and day to make it happen.

15:43.400 --> 15:47.680
 And like I said, the critical thing

15:47.680 --> 15:49.520
 to achieve for the revolution in spaceflight

15:49.520 --> 15:52.000
 and for humanity to be a space frame civilization

15:52.000 --> 15:54.360
 is to have a fully and rapidly reusable rocket,

15:54.360 --> 15:56.480
 orbital rocket.

15:56.480 --> 15:59.000
 There's not even been any orbital rocket that's

15:59.000 --> 16:00.080
 been fully reusable ever.

16:00.080 --> 16:05.960
 And this has always been the holy grail of rocketry.

16:05.960 --> 16:09.680
 And many smart people, very smart people,

16:09.680 --> 16:12.680
 have tried to do this before and have not succeeded.

16:12.680 --> 16:16.960
 So because it's such a hard problem.

16:16.960 --> 16:21.160
 What's your source of belief in situations like this?

16:21.160 --> 16:23.520
 When the engineering problem is so difficult,

16:23.520 --> 16:26.160
 there's a lot of experts, many of whom

16:26.160 --> 16:29.120
 you admire who have failed in the past.

16:29.120 --> 16:29.960
 Yes.

16:29.960 --> 16:37.960
 And a lot of people, a lot of experts, maybe journalists,

16:37.960 --> 16:39.840
 all the kind of the public in general,

16:39.840 --> 16:43.600
 have a lot of doubt about whether it's possible.

16:43.600 --> 16:46.040
 And you yourself know that even if it's

16:46.040 --> 16:49.480
 a non null set, non empty set of success,

16:49.480 --> 16:52.000
 it's still unlikely or very difficult.

16:52.000 --> 16:53.400
 Like where do you go to?

16:53.400 --> 16:57.240
 Both personally, intellectually as an engineer,

16:57.240 --> 17:00.360
 as a team, like for source of strength,

17:00.360 --> 17:03.440
 needed to sort of persevere through this

17:03.440 --> 17:05.160
 and to keep going with the project,

17:05.160 --> 17:18.440
 take it to completion.

17:18.440 --> 17:19.800
 It's also strength.

17:19.800 --> 17:23.600
 It doesn't really know how I think about things.

17:23.600 --> 17:25.560
 I mean, for me, it's simply this is something

17:25.560 --> 17:28.120
 that is important to get done.

17:28.120 --> 17:32.560
 And we should just keep doing it or die trying.

17:32.560 --> 17:35.960
 And I don't need source of strength.

17:35.960 --> 17:39.080
 So quitting is not even like.

17:39.080 --> 17:41.560
 That's not in my nature.

17:41.560 --> 17:46.280
 And I don't care about optimism or pessimism.

17:46.280 --> 17:47.920
 Fuck that, we're going to get it done.

17:47.920 --> 17:48.640
 Going to get it done.

17:51.760 --> 17:55.160
 Can you then zoom back in to specific problems

17:55.160 --> 17:58.360
 with starship or any engineering problems you work on?

17:58.360 --> 18:01.520
 Can you try to introspect your particular biological

18:01.520 --> 18:03.600
 and neural network, your thinking process

18:03.600 --> 18:06.200
 and describe how you think through problems,

18:06.200 --> 18:08.040
 the different engineering and design problems?

18:08.040 --> 18:10.240
 Is there like a systematic process you've

18:10.240 --> 18:11.920
 spoken about first principles thinking,

18:11.920 --> 18:14.200
 but is there a kind of process to it?

18:14.200 --> 18:19.280
 Well, like saying like physics is low

18:19.280 --> 18:21.280
 and everything else is a recommendation.

18:21.280 --> 18:23.240
 Like I've met a lot of people that can break the law,

18:23.240 --> 18:25.480
 but I haven't met anyone who could break physics.

18:25.480 --> 18:32.520
 So for any kind of technology problem,

18:32.520 --> 18:37.320
 you have to just make sure you're not violating physics.

18:39.960 --> 18:45.120
 And first principles analysis, I think,

18:45.120 --> 18:47.400
 is something that could be applied to really

18:47.400 --> 18:49.840
 any walk of life, anything really.

18:49.840 --> 18:54.680
 It's really just saying, let's boil something down

18:54.680 --> 18:58.440
 to the most fundamental principles.

18:58.440 --> 19:00.280
 The things that we are most confident

19:00.280 --> 19:02.520
 are true at a foundational level.

19:02.520 --> 19:05.080
 And that sets your axiomatic base.

19:05.080 --> 19:07.040
 And then you reason up from there.

19:07.040 --> 19:09.040
 And then you cross check your conclusion

19:09.040 --> 19:11.200
 against the axiomatic truths.

19:13.680 --> 19:18.120
 So some basics in physics would be like,

19:18.120 --> 19:20.280
 are you violating conservation of energy or momentum

19:20.280 --> 19:21.400
 or something like that?

19:21.400 --> 19:25.800
 Then it's not going to work.

19:25.800 --> 19:33.000
 So that's just to establish, is it possible?

19:33.000 --> 19:35.160
 And another good physics tool is thinking

19:35.160 --> 19:36.520
 about things in the limit.

19:36.520 --> 19:40.760
 If you take a particular thing and you scale it

19:40.760 --> 19:42.960
 to a very large number or to a very small number,

19:42.960 --> 19:46.000
 how do things change?

19:46.000 --> 19:48.960
 Well, it's like in number of things you manufacture,

19:48.960 --> 19:51.680
 something like that, and then in time.

19:51.680 --> 19:56.120
 Yeah, let's say you take an example of manufacturing, which

19:56.120 --> 20:00.360
 I think is just a very underrated problem.

20:00.360 --> 20:04.560
 And like I said, it's much harder

20:04.560 --> 20:09.360
 to take an advanced technology product

20:09.360 --> 20:11.040
 and bring it into volume manufacturing

20:11.040 --> 20:12.880
 than it is to design it in the first place.

20:12.880 --> 20:14.520
 My orders of magnitude.

20:14.520 --> 20:19.520
 So let's say you're trying to figure out

20:19.520 --> 20:24.000
 why is this part or product expensive?

20:24.000 --> 20:27.400
 Is it because of something fundamentally foolish

20:27.400 --> 20:28.080
 that we're doing?

20:28.080 --> 20:30.960
 Or is it because our volume is too low?

20:30.960 --> 20:32.960
 And then you say, OK, well, what if our volume was

20:32.960 --> 20:34.240
 a million units a year?

20:34.240 --> 20:35.560
 Is it still expensive?

20:35.560 --> 20:38.160
 That's what I'm thinking about things in the limit.

20:38.160 --> 20:40.160
 If it's still expensive at a million units a year,

20:40.160 --> 20:42.520
 then volume is not the reason why your thing is expensive.

20:42.520 --> 20:44.680
 There's something fundamental about design.

20:44.680 --> 20:47.440
 And then you then can focus on reducing complexity

20:47.440 --> 20:48.800
 or something like that in the design?

20:48.800 --> 20:51.280
 Change the design to change the part to be something

20:51.280 --> 20:57.120
 that is not fundamentally expensive.

20:57.120 --> 20:58.920
 That's a common thing in rocketry,

20:58.920 --> 21:01.800
 because the unit volume is relatively low.

21:01.800 --> 21:04.280
 And so a common excuse would be, well,

21:04.280 --> 21:06.760
 it's expensive because our unit volume is low.

21:06.760 --> 21:08.760
 And if we were in automotive or something like that

21:08.760 --> 21:10.920
 or consumer electronics, then our costs would be lower.

21:10.920 --> 21:13.480
 And I'm like, OK, so let's say now you're

21:13.480 --> 21:14.640
 making a million units a year.

21:14.640 --> 21:16.080
 Is it still expensive?

21:16.080 --> 21:20.760
 If the answer is yes, then economies of scale are not

21:20.760 --> 21:22.080
 the issue.

21:22.080 --> 21:24.080
 Do you throw into manufacturing?

21:24.080 --> 21:26.040
 Do you throw like supply chain?

21:26.040 --> 21:28.480
 Talk about resources and materials and stuff like that.

21:28.480 --> 21:29.920
 Do you throw that into the calculation

21:29.920 --> 21:31.760
 of trying to reason from first principles

21:31.760 --> 21:34.600
 like how we're going to make the supply chain work here?

21:34.600 --> 21:35.720
 Yeah, yeah.

21:35.720 --> 21:37.800
 And then the cost of materials, things like that.

21:37.800 --> 21:38.880
 Or is that too much?

21:38.880 --> 21:43.080
 Exactly, so another good example of thinking about things

21:43.080 --> 21:52.160
 in the limit is if you take any product, any machine,

21:52.160 --> 21:56.040
 or whatever, take a rocket or whatever

21:56.040 --> 22:03.640
 and say, if you look at the raw materials in the rocket,

22:03.640 --> 22:09.240
 so you're going to have aluminum, steel, titanium, incanal,

22:09.240 --> 22:13.480
 specialty alloys, copper.

22:13.480 --> 22:19.200
 And you say, what's the weight of the constituent elements

22:19.200 --> 22:20.440
 of each of these elements?

22:20.440 --> 22:22.560
 And what is their raw material value?

22:22.560 --> 22:25.680
 And that sets the asymptotic limit

22:25.680 --> 22:29.200
 for how low the cost of the vehicle

22:29.200 --> 22:32.920
 can be unless you change the materials.

22:32.920 --> 22:35.480
 And then when you do that, I call it maybe the magic one

22:35.480 --> 22:36.680
 number or something like that.

22:36.680 --> 22:42.120
 So that would be if you had just a pile of these raw materials

22:42.120 --> 22:43.640
 here and you could wave the magic one

22:43.640 --> 22:47.160
 and rearrange the atoms into the final shape,

22:47.160 --> 22:49.440
 that would be the lowest possible cost

22:49.440 --> 22:50.960
 that you could make this thing for

22:50.960 --> 22:52.720
 unless you change the materials.

22:52.720 --> 22:57.760
 So then, and that is always almost always a very low number.

22:57.760 --> 23:01.200
 So then what's actually causing these to be expensive

23:01.200 --> 23:06.000
 is how you put the atoms into the desired shape.

23:06.000 --> 23:10.120
 Yeah, actually, if you don't mind me taking a tiny tangent,

23:10.120 --> 23:12.240
 I often talk to Jim Keller, who's

23:12.240 --> 23:14.360
 somebody who worked with you as a friend.

23:14.360 --> 23:17.800
 Jim was a great work at Tesla.

23:17.800 --> 23:21.280
 So I suppose he carries the flame of the same kind

23:21.280 --> 23:26.280
 of thinking that you're talking about now.

23:26.280 --> 23:30.920
 And I guess I see that same thing at Tesla and SpaceX folks

23:30.920 --> 23:33.800
 who worked there, they kind of learned this way of thinking.

23:33.800 --> 23:36.720
 And it kind of becomes obvious almost.

23:36.720 --> 23:40.880
 But anyway, I had argument, not argument,

23:40.880 --> 23:44.480
 he educated me about how cheap it

23:44.480 --> 23:46.640
 might be to manufacture a Tesla bot.

23:46.640 --> 23:49.000
 We just, we had an argument, how can you

23:49.000 --> 23:52.120
 reduce the cost of the scale of producing a robot?

23:52.120 --> 23:56.000
 Because I've gotten a chance to interact quite a bit,

23:56.000 --> 23:59.520
 obviously, in the academic circles with human robots

23:59.520 --> 24:01.920
 and the Boston Dynamics and stuff like that.

24:01.920 --> 24:04.520
 And they're very expensive to build.

24:04.520 --> 24:08.240
 And then Jim kind of schooled me on saying, OK,

24:08.240 --> 24:10.080
 this kind of first principles thinking

24:10.080 --> 24:13.760
 of how can we get the cost of manufacturing down?

24:13.760 --> 24:16.840
 I suppose you do that, you have done that kind of thinking

24:16.840 --> 24:22.040
 for Tesla bot and for all kinds of complex systems that

24:22.040 --> 24:23.640
 are traditionally seen as complex.

24:23.640 --> 24:27.160
 And you say, OK, how can we simplify everything now?

24:27.160 --> 24:32.360
 Yeah, I mean, I think if you are really good at manufacturing,

24:32.360 --> 24:35.320
 you can basically make at high volume,

24:35.320 --> 24:38.480
 you can basically make anything for a cost

24:38.480 --> 24:42.040
 that asymptotically approaches the raw material value

24:42.040 --> 24:44.600
 of the constituents, plus any intellectual property

24:44.600 --> 24:46.520
 that you need to do license.

24:46.520 --> 24:49.400
 Anything.

24:49.400 --> 24:50.240
 But it's hard.

24:50.240 --> 24:52.240
 It's not like that's a very hard thing to do,

24:52.240 --> 24:54.720
 but it is possible for anything.

24:54.720 --> 24:57.480
 Anything in volume can be made, like I said,

24:57.480 --> 25:00.400
 for a cost that asymptotically approaches

25:00.400 --> 25:03.760
 its raw material constituents, plus intellectual property

25:03.760 --> 25:05.360
 license rights.

25:05.360 --> 25:08.280
 So what will often happen in trying to design a product

25:08.280 --> 25:12.400
 is people will start with the tools and parts and methods

25:12.400 --> 25:15.920
 that they are familiar with and then try

25:15.920 --> 25:21.200
 to create a product using their existing tools and methods.

25:21.200 --> 25:24.120
 The other way to think about it is actually

25:24.120 --> 25:27.920
 imagine the try to imagine the platonic ideal of the perfect

25:27.920 --> 25:31.320
 product or technology, whatever it might be.

25:31.320 --> 25:35.640
 And so what is the perfect arrangement of atoms

25:35.640 --> 25:38.480
 that would be the best possible product?

25:38.480 --> 25:39.960
 And now let us try to figure out how

25:39.960 --> 25:41.120
 to get the atoms in that shape.

25:43.800 --> 25:49.320
 I mean, it sounds almost like Rick and Morty

25:49.320 --> 25:52.080
 absurd until you start to really think about it.

25:52.080 --> 25:56.400
 And you really should think about it in this way,

25:56.400 --> 25:59.000
 because everything else is kind of,

25:59.000 --> 26:02.320
 if you think you might fall victim

26:02.320 --> 26:04.440
 to the momentum of the way things were done in the past,

26:04.440 --> 26:06.000
 unless you think in this way.

26:06.000 --> 26:07.680
 Well, just as a function of inertia,

26:07.680 --> 26:10.640
 people will want to use the same tools and methods

26:10.640 --> 26:13.680
 that they are familiar with.

26:13.680 --> 26:16.040
 That's what they'll do by default.

26:16.040 --> 26:18.680
 And then that will lead to an outcome of things

26:18.680 --> 26:20.560
 that can be made with those tools and methods,

26:20.560 --> 26:24.320
 that is unlikely to be the platonic ideal of the perfect

26:24.320 --> 26:26.040
 product.

26:26.040 --> 26:30.000
 So that's why it's good to think of things in both directions.

26:30.000 --> 26:32.200
 They're like, what can we build with the tools that we have?

26:32.200 --> 26:37.280
 But also, what is the theoretical perfect product look like?

26:37.280 --> 26:39.560
 And that theoretical perfect product

26:39.560 --> 26:42.720
 is going to be a moving target, because as you learn more,

26:42.720 --> 26:46.600
 the definition for that perfect product will change,

26:46.600 --> 26:48.760
 because you don't actually know what the perfect product is,

26:48.760 --> 26:52.560
 but you can successfully approximate a more perfect

26:52.560 --> 26:54.320
 product.

26:54.320 --> 26:57.440
 So think about it like that, and then saying, OK, now,

26:57.440 --> 26:59.920
 what tools, methods, materials, whatever

26:59.920 --> 27:06.040
 do we need to create in order to get the atoms in that shape?

27:06.040 --> 27:10.160
 But people rarely think about it that way.

27:10.160 --> 27:12.720
 But it's a powerful tool.

27:12.720 --> 27:15.920
 I should mention that the brilliant Siobhan Zillis is

27:15.920 --> 27:19.440
 hanging out with us, in case you hear

27:19.440 --> 27:25.720
 a voice of wisdom from outside, from up above.

27:25.720 --> 27:28.200
 OK, so let me ask you about Mars.

27:28.200 --> 27:30.440
 You mentioned it would be great for science

27:30.440 --> 27:34.960
 to put a base on the moon to do some research.

27:34.960 --> 27:39.600
 But the truly big leap, again, in this category

27:39.600 --> 27:43.800
 of seemingly impossible, is to put a human being on Mars.

27:43.800 --> 27:46.760
 When do you think SpaceX will land a human being on Mars?

27:46.760 --> 27:54.760
 Hmm.

27:54.760 --> 28:04.760
 Hmm.

28:08.760 --> 28:16.760
 Best case is about five years, worst case, 10 years.

28:16.760 --> 28:18.520
 What are the determining factors,

28:18.520 --> 28:21.160
 would you say, from an engineering perspective,

28:21.160 --> 28:24.080
 or is that not the bottlenecks?

28:24.080 --> 28:28.520
 You know, it's fundamentally engineering the vehicle.

28:32.520 --> 28:36.560
 I mean, Starship is the most complex and advanced rocket

28:36.560 --> 28:39.320
 that's ever been made by, I don't know,

28:39.320 --> 28:40.800
 water magnitude or something like that.

28:40.800 --> 28:42.080
 It's a lot.

28:42.080 --> 28:43.040
 It's really next level.

28:43.040 --> 28:49.240
 So, and the fundamental optimization of Starship

28:49.240 --> 28:51.560
 is minimizing cost per ton to orbit,

28:51.560 --> 28:54.760
 and ultimately cost per ton to the surface of Mars.

28:54.760 --> 28:56.360
 This may seem like a Mugantile objective,

28:56.360 --> 29:00.360
 but it is actually the thing that needs to be optimized.

29:00.360 --> 29:04.040
 Like, there is a certain cost per ton to the surface of Mars

29:04.040 --> 29:08.800
 where we can afford to establish a self sustaining city.

29:08.800 --> 29:12.800
 And then above that, we cannot afford to do it.

29:12.800 --> 29:16.160
 So, right now, you couldn't fly to Mars for a trillion dollars.

29:16.160 --> 29:19.160
 There's no amount of money could get you a ticket to Mars.

29:19.160 --> 29:22.440
 So, we need to get that above, you know,

29:22.440 --> 29:25.160
 to get that like something that is actually possible at all.

29:27.800 --> 29:32.240
 But then, that's, we don't just want to have, you know,

29:32.240 --> 29:33.800
 with Mars flags and footprints,

29:33.800 --> 29:35.880
 and then not come back for a half century

29:35.880 --> 29:37.960
 like we did with the moon.

29:37.960 --> 29:43.360
 In order to pass a very important great filter,

29:43.360 --> 29:45.520
 I think, we need to be a multi planet species.

29:45.520 --> 29:51.400
 That may sound somewhat esoteric to a lot of people,

29:51.400 --> 29:55.320
 but eventually, given enough time,

29:55.320 --> 29:58.040
 there's something,

29:58.040 --> 30:01.240
 Earth is likely to experience some calamity

30:01.240 --> 30:06.680
 that could be something that humans do to themselves

30:06.680 --> 30:09.120
 or an external event like happen to the dinosaurs.

30:09.120 --> 30:17.800
 And eventually, if none of that happens,

30:17.800 --> 30:20.520
 and somehow, magically, we keep going,

30:20.520 --> 30:23.800
 then the sun will, the sun is gradually expanding

30:23.800 --> 30:25.960
 and will engulf the Earth.

30:25.960 --> 30:30.840
 And probably Earth gets too hot for life

30:30.840 --> 30:34.680
 in about 500 million years.

30:34.680 --> 30:37.160
 It's a long time, but that's only 10% longer

30:37.160 --> 30:38.240
 than Earth has been around.

30:38.240 --> 30:43.240
 And so, if you think about like the current situation,

30:43.240 --> 30:45.640
 it's really remarkable and kind of hard to believe,

30:45.640 --> 30:50.080
 but Earth's been around 4.5 billion years,

30:50.080 --> 30:52.360
 and this is the first time in 4.5 billion years

30:52.360 --> 30:55.720
 that it's been possible to extend life beyond Earth.

30:55.720 --> 30:58.800
 And that window of charity may be open for a long time,

30:58.800 --> 31:01.760
 and I hope it is, but it also may be open for a short time.

31:01.760 --> 31:09.440
 And I think it was wise for us to act quickly

31:09.440 --> 31:13.800
 while the window is open, just in case it closes.

31:13.800 --> 31:17.840
 Yeah, the existence of nuclear weapons, pandemics,

31:17.840 --> 31:25.160
 all kinds of threats should kind of give us some motivation.

31:25.160 --> 31:31.520
 I mean, civilization could die with a bang or a whimper.

31:31.520 --> 31:35.000
 If it dies, a demographic collapse,

31:35.000 --> 31:38.080
 then it's more of a whimper, obviously.

31:38.080 --> 31:40.680
 But if it's World War III, it's more of a bang.

31:40.680 --> 31:43.160
 But these are all risks.

31:43.160 --> 31:44.560
 I mean, it's important to think of these things

31:44.560 --> 31:48.600
 and just think of things as probabilities, not certainties.

31:48.600 --> 31:52.360
 There's a probability that something bad will happen on Earth.

31:52.360 --> 31:56.520
 I think most likely the future will be good.

31:56.520 --> 31:59.400
 But there's, let's say, for argument's sake,

31:59.400 --> 32:03.600
 a 1% chance per century of a civilization ending event.

32:03.600 --> 32:07.720
 Like, that was Stephen Hawking's estimate.

32:07.720 --> 32:10.440
 I think he might be right about that.

32:10.440 --> 32:18.000
 So then we should basically think of this

32:18.000 --> 32:20.200
 like being a multiplanet species is like taking out

32:20.200 --> 32:21.480
 insurance for life itself.

32:21.480 --> 32:22.600
 Like, life insurance?

32:22.600 --> 32:23.000
 For life.

32:26.680 --> 32:29.280
 Well, it's turned into an infomercial real quick.

32:29.280 --> 32:31.520
 Life insurance for life, yes.

32:31.520 --> 32:36.720
 And we can bring the creatures from plants and animals

32:36.720 --> 32:41.200
 from Earth to Mars and breathe life into the planet

32:41.200 --> 32:44.600
 and have a second planet with life.

32:44.600 --> 32:45.920
 That would be great.

32:45.920 --> 32:47.480
 They can't bring themselves there.

32:47.480 --> 32:49.960
 So if we don't bring them to Mars,

32:49.960 --> 32:54.280
 then they will just for sure all die when the sun expands anyway.

32:54.280 --> 32:56.160
 And then that'll be it.

32:56.160 --> 32:59.560
 What do you think is the most difficult aspect of building

32:59.560 --> 33:02.200
 a civilization on Mars, terraforming Mars,

33:02.200 --> 33:03.720
 like from an engineering perspective,

33:03.720 --> 33:07.200
 from a financial perspective, human perspective,

33:07.200 --> 33:14.280
 to get a large number of folks there who will never return

33:14.280 --> 33:15.800
 back to Earth?

33:15.800 --> 33:17.080
 No, they could certainly return.

33:17.080 --> 33:18.400
 Some will return back to Earth.

33:18.400 --> 33:21.360
 They will choose to stay there for the rest of their lives.

33:21.360 --> 33:23.680
 Many will.

33:23.680 --> 33:29.400
 But we need the spaceships back, like the ones that go to Mars.

33:29.400 --> 33:30.000
 We need them back.

33:30.000 --> 33:32.520
 So you can hop on if you want.

33:32.520 --> 33:34.880
 But we can't just not have the spaceships come back.

33:34.880 --> 33:35.760
 Those things are expensive.

33:35.760 --> 33:36.480
 We need them back.

33:36.480 --> 33:38.720
 I'd like to come back after the trip.

33:38.720 --> 33:40.680
 I mean, do you think about the terraforming aspect,

33:40.680 --> 33:41.600
 like actually building?

33:41.600 --> 33:44.760
 Are you so focused right now on the spaceships part that's

33:44.760 --> 33:46.880
 so critical to get to Mars?

33:46.880 --> 33:50.480
 We absolutely, if you can't get there, nothing else matters.

33:50.480 --> 33:54.080
 And like I said, we can't get there at some extraordinarily

33:54.080 --> 33:54.640
 high cost.

33:54.640 --> 33:57.840
 I mean, the current cost of, let's say,

33:57.840 --> 34:02.640
 one ton to the surface of Mars is on the order of $1 billion.

34:02.640 --> 34:05.200
 So because you don't just need the rocket and the launch

34:05.200 --> 34:09.040
 and everything, you need like heat shield, you need guidance

34:09.040 --> 34:12.240
 system, you need deep space communications,

34:12.240 --> 34:14.960
 you need some kind of landing system.

34:14.960 --> 34:16.920
 So like rough approximation would

34:16.920 --> 34:22.200
 be $1 billion per ton to the surface of Mars right now.

34:22.200 --> 34:26.880
 This is obviously way too expensive

34:26.880 --> 34:30.680
 to create a self sustaining civilization.

34:30.680 --> 34:36.840
 So we need to improve that by at least a factor of 1,000.

34:36.840 --> 34:38.440
 A million per ton?

34:38.440 --> 34:40.880
 Yes, ideally much less than a million ton.

34:40.880 --> 34:44.360
 But if it's not, like it's got to be,

34:44.360 --> 34:47.960
 obviously like how much can society afford to spend

34:47.960 --> 34:52.360
 or just want to spend on a self sustaining city on Mars?

34:52.360 --> 34:53.800
 The self sustaining part is important.

34:53.800 --> 34:57.800
 Like it's just the key threshold,

34:57.800 --> 35:01.320
 the great filter will have been passed

35:01.320 --> 35:06.480
 when the city on Mars can survive even if the spaceships

35:06.480 --> 35:08.520
 from Earth stop coming for any reason.

35:08.520 --> 35:10.000
 It doesn't matter what the reason is.

35:10.000 --> 35:12.120
 But if they stop coming for any reason,

35:12.120 --> 35:13.680
 will it die out or will it not?

35:13.680 --> 35:16.360
 And if there's even one critical ingredient missing,

35:16.360 --> 35:18.320
 then it still doesn't count.

35:18.320 --> 35:20.400
 It's like if you're in a long sea voyage

35:20.400 --> 35:23.680
 and you've got everything except vitamin C,

35:23.680 --> 35:26.240
 and it's only a matter of time, you know, you're going to die.

35:26.240 --> 35:29.240
 So we're going to get Mars, a Mars city to the point

35:29.240 --> 35:31.880
 where it's self sustaining.

35:31.880 --> 35:33.840
 I'm not sure this will really happen in my lifetime,

35:33.840 --> 35:37.240
 but I hope to see it at least have a lot of momentum.

35:37.240 --> 35:39.760
 And then you could say, what is the minimum tonnage

35:39.760 --> 35:45.080
 necessary to have a self sustaining city?

35:45.080 --> 35:46.480
 And there's a lot of uncertainty about this.

35:46.480 --> 35:48.520
 You could say like, I don't know,

35:48.520 --> 35:52.000
 it's probably at least a million tons.

35:52.000 --> 35:55.400
 Because you have to set up a lot of infrastructure on Mars.

35:55.400 --> 35:58.680
 Like I said, you can't be missing anything

35:58.680 --> 36:01.560
 that in order to be self sustaining, you can't be missing.

36:01.560 --> 36:04.480
 Like you need a sand baking doctor, fabs,

36:04.480 --> 36:09.360
 you need iron ore refineries, like you need lots of things.

36:09.360 --> 36:13.440
 So, and Mars is not super hospitable.

36:13.440 --> 36:15.680
 It's the least inhospitable planet,

36:15.680 --> 36:18.160
 but it's definitely a fixer of a planet.

36:18.160 --> 36:19.400
 Outside of Earth.

36:19.400 --> 36:20.080
 Yes.

36:20.080 --> 36:20.800
 Earth is pretty good.

36:20.800 --> 36:22.240
 Earth is like easy.

36:22.240 --> 36:25.480
 And also we should clarify in the solar system.

36:25.480 --> 36:26.600
 Yes, in the solar system.

36:26.600 --> 36:29.760
 There might be nice like vacation spots.

36:29.760 --> 36:32.720
 There might be some great planets out there, but it's hopeless.

36:32.720 --> 36:33.800
 Too hard to get there?

36:33.800 --> 36:37.440
 Yeah, way, way, way, way too hard to say the least.

36:37.440 --> 36:38.720
 Let me push back on that.

36:38.720 --> 36:42.000
 Not really a pushback, but a quick curve ball of a question.

36:42.000 --> 36:44.800
 So you did mention physics as the first starting point.

36:44.800 --> 36:51.400
 So general relativity allows for warm holes.

36:51.400 --> 36:53.040
 They technically can exist.

36:53.040 --> 36:56.200
 Do you think those can ever be leveraged by humus

36:56.200 --> 36:59.400
 to travel fast in the speed of light?

36:59.400 --> 37:06.280
 Well, the one whole thing is debatable.

37:06.280 --> 37:08.920
 We currently do not know of any means of going fast

37:08.920 --> 37:09.720
 in the speed of light.

37:17.640 --> 37:21.840
 There are some ideas about having space.

37:21.840 --> 37:27.360
 So you can only move at the speed of light through space,

37:27.360 --> 37:31.640
 but if you can make space itself move,

37:31.640 --> 37:36.160
 that's what we're warming space.

37:36.160 --> 37:40.760
 Space is capable of moving faster than the speed of light.

37:40.760 --> 37:42.120
 Like the universe, in the Big Bang,

37:42.120 --> 37:46.480
 the universe expanded much more than the speed of light by a lot.

37:55.320 --> 37:57.400
 If this is possible, the amount of energy

37:57.400 --> 38:03.120
 required to walk space is so gigantic, it boggles the mind.

38:03.120 --> 38:05.080
 So all the work you've done with propulsion,

38:05.080 --> 38:08.120
 how much innovation is possible with rocket propulsion?

38:08.120 --> 38:11.200
 Is this, I mean, you've seen it all,

38:11.200 --> 38:14.480
 and you're constantly innovating in every aspect.

38:14.480 --> 38:15.360
 How much is possible?

38:15.360 --> 38:17.400
 Like, how much can you get 10x somehow?

38:17.400 --> 38:19.680
 Is there something in there in physics

38:19.680 --> 38:21.520
 that you can get significant improvement in terms

38:21.520 --> 38:24.680
 of efficiency of engines and all those kinds of things?

38:24.680 --> 38:27.960
 Well, as I was saying, really the Holy Grail

38:27.960 --> 38:33.080
 is a fully and rapidly reusable orbital system.

38:33.080 --> 38:40.560
 So right now, the Falcon 9 is the only reusable rocket out

38:40.560 --> 38:44.320
 there, but the booster comes back and lands,

38:44.320 --> 38:46.800
 and you've seen the videos, and we get the nose

38:46.800 --> 38:49.760
 cone fairing back, but we do not get the upper stage back.

38:49.760 --> 38:54.280
 So that means that we have a minimum cost

38:54.280 --> 38:57.680
 of building an upper stage.

38:57.680 --> 38:59.760
 You can think of like a two stage rocket of sort

38:59.760 --> 39:01.520
 of like two airplanes, like a big airplane

39:01.520 --> 39:04.600
 and a small airplane, and we get the big airplane back,

39:04.600 --> 39:05.920
 but not the small airplane.

39:05.920 --> 39:07.880
 And so it still costs a lot.

39:07.880 --> 39:13.360
 So that upper stage is at least $10 million.

39:13.360 --> 39:18.240
 And then the degree of the booster is not as rapidly

39:18.240 --> 39:20.960
 and completely reusable as we'd like in order of the fairings.

39:20.960 --> 39:26.080
 So our kind of minimum marginal cost, not counting overhead,

39:26.080 --> 39:33.360
 for per flight is on the order of $15 to $20 million maybe.

39:33.360 --> 39:38.720
 So that's extremely good for, it's by far better

39:38.720 --> 39:41.600
 than any rocket ever in history.

39:41.600 --> 39:45.360
 But with full and rapid reusability,

39:45.360 --> 39:53.200
 we can reduce the cost per ton to orbit by a factor of 100.

39:53.200 --> 39:56.640
 Just think of it like imagine if you had an aircraft

39:56.640 --> 40:00.000
 or something or a car.

40:00.000 --> 40:03.440
 And if you had to buy in your car every time

40:03.440 --> 40:06.760
 you went for a drive, it would be very expensive,

40:06.760 --> 40:08.240
 every silly, frankly.

40:08.240 --> 40:13.440
 But in fact, you just refuel the car or recharge the car.

40:13.440 --> 40:20.280
 And that makes your trip, I don't know, 1,000 times cheaper.

40:20.280 --> 40:23.800
 So it's the same for rockets.

40:23.800 --> 40:27.360
 It's very difficult to make this complex machine that

40:27.360 --> 40:28.560
 can go to orbit.

40:28.560 --> 40:31.120
 And so if you cannot reuse it and have

40:31.120 --> 40:34.040
 to throw even any significant part of it away,

40:34.040 --> 40:36.640
 that massively increases the cost.

40:36.640 --> 40:43.880
 So Starship, in theory, could do a cost per launch of like

40:43.880 --> 40:48.280
 a million, maybe $2 million or something like that

40:48.280 --> 40:53.520
 and put over 100 tons in orbit, which is crazy.

40:53.520 --> 40:55.920
 Yeah, that's incredible.

40:55.920 --> 40:58.680
 So you're saying like it's by far the biggest bank for the block

40:58.680 --> 41:02.320
 is to make it fully reusable versus like some kind

41:02.320 --> 41:05.720
 of brilliant breakthrough in theoretical physics?

41:05.720 --> 41:06.360
 Yeah, no.

41:06.360 --> 41:07.680
 There's no brilliant break.

41:07.680 --> 41:11.240
 No, just make the rocket reusable.

41:11.240 --> 41:13.440
 This is an extremely difficult engineering problem.

41:13.440 --> 41:14.040
 Got it.

41:14.040 --> 41:17.880
 But no new physics is required.

41:17.880 --> 41:19.280
 Just brilliant engineering.

41:19.280 --> 41:22.120
 Let me ask a slightly philosophical, fun question.

41:22.120 --> 41:22.960
 Got to ask.

41:22.960 --> 41:24.800
 I know you're focused on getting to Mars,

41:24.800 --> 41:27.160
 but once we're there on Mars, what do you

41:27.160 --> 41:32.280
 what form of government, economic system, political system

41:32.280 --> 41:35.560
 do you think would work best for an early civilization

41:35.560 --> 41:38.080
 of humans?

41:38.080 --> 41:41.520
 I mean, the interesting reason to talk about this stuff,

41:41.520 --> 41:44.280
 it also helps people dream about the future.

41:44.280 --> 41:47.240
 I know you're really focused about the short term

41:47.240 --> 41:49.280
 engineering dream, but it's like, I don't know.

41:49.280 --> 41:51.760
 There's something about imagining an actual civilization

41:51.760 --> 41:55.360
 on Mars that gives people, really gives people help.

41:55.360 --> 41:57.640
 Well, it would be a new frontier and an opportunity

41:57.640 --> 41:59.840
 to rethink the whole nature of government, just

41:59.840 --> 42:02.640
 as was done in the creation of the United States.

42:02.640 --> 42:14.400
 So I mean, I would suggest having direct democracy,

42:14.400 --> 42:16.160
 like people vote directly on things,

42:16.160 --> 42:18.440
 as opposed to representative democracy.

42:18.440 --> 42:21.520
 So representative democracy, I think,

42:21.520 --> 42:25.120
 is too subject to a special interest

42:25.120 --> 42:31.400
 and a coercion of the politicians and that kind of thing.

42:31.400 --> 42:39.320
 So I'd recommend that there's just direct democracy.

42:39.320 --> 42:40.560
 People vote on laws.

42:40.560 --> 42:42.680
 The population votes on laws themselves.

42:42.680 --> 42:44.480
 And then the laws must be short enough

42:44.480 --> 42:46.880
 that people can understand them.

42:46.880 --> 42:50.000
 Yeah, and then keeping a well informed populist,

42:50.000 --> 42:52.240
 really being transparent about all the information,

42:52.240 --> 42:53.440
 about what they're voting for.

42:53.440 --> 42:54.800
 Absolutely transparency.

42:54.800 --> 42:55.520
 Yeah.

42:55.520 --> 42:57.440
 And not make it as annoying as those cookies

42:57.440 --> 42:58.800
 where you have to accept the cookies.

42:58.800 --> 42:59.800
 Accept cookies.

42:59.800 --> 43:03.640
 Like always, there's always a slight amount of trepidation

43:03.640 --> 43:05.360
 when you click accept cookies.

43:05.360 --> 43:07.760
 Like, I feel as though there's, perhaps,

43:07.760 --> 43:10.680
 a very tiny chance that it'll open a portal to hell

43:10.680 --> 43:12.200
 or something like that.

43:12.200 --> 43:13.760
 That's exactly how I feel.

43:13.760 --> 43:16.600
 Why do they want me to accept that?

43:16.600 --> 43:18.960
 What do they want with this cookie?

43:18.960 --> 43:21.440
 Like, somebody got upset with accepting cookies or something

43:21.440 --> 43:22.400
 somewhere.

43:22.400 --> 43:23.960
 Who cares?

43:23.960 --> 43:26.920
 So annoying to keep accepting all these cookies.

43:26.920 --> 43:29.560
 To me, this is just a grand accept.

43:29.560 --> 43:30.720
 Yes, you can have my damn cookie.

43:30.720 --> 43:31.280
 I don't care.

43:31.280 --> 43:32.360
 Whatever.

43:32.360 --> 43:33.720
 He heard it from me on first.

43:33.720 --> 43:35.960
 He accepts all your damn cookies.

43:35.960 --> 43:37.880
 Yeah.

43:37.880 --> 43:39.920
 It's not asking me.

43:39.920 --> 43:41.400
 It's annoying.

43:41.400 --> 43:46.000
 Yeah, it's one example of implementation

43:46.000 --> 43:50.200
 of a good idea done really horribly.

43:50.200 --> 43:52.520
 Yeah, it's somebody who has some good intentions

43:52.520 --> 43:54.880
 of privacy or whatever.

43:54.880 --> 43:57.320
 But now, everyone just has to accept cookies.

43:57.320 --> 43:59.320
 And it's not, you know, you have billions of people

43:59.320 --> 44:00.720
 who have to keep clicking accept cookie.

44:00.720 --> 44:02.440
 It's super annoying.

44:02.440 --> 44:04.040
 Then we just accept the damn cookie.

44:04.040 --> 44:05.120
 It's fine.

44:05.120 --> 44:07.960
 There is, I think, a fundamental problem

44:07.960 --> 44:12.280
 that we're, because we've not really had a major,

44:12.280 --> 44:14.320
 like a world war or something like that in a while.

44:14.320 --> 44:18.320
 And obviously, we'd like to not have world wars.

44:18.320 --> 44:19.800
 There's not been a cleansing function

44:19.800 --> 44:22.720
 for rules and regulations.

44:22.720 --> 44:25.760
 So wars did have some sort of lining

44:25.760 --> 44:30.080
 in that there would be a reset on rules and regulations

44:30.080 --> 44:31.120
 after a war.

44:31.120 --> 44:33.000
 So World War I and II, there were huge resets

44:33.000 --> 44:35.720
 on rules and regulations.

44:35.720 --> 44:39.400
 Now, if society does not have a war

44:39.400 --> 44:41.720
 and there's no cleansing function or garbage collection

44:41.720 --> 44:43.840
 for rules and regulations, then rules and regulations

44:43.840 --> 44:46.480
 will accumulate every year, because they're immortal.

44:46.480 --> 44:50.280
 There's no actual humans die, but the laws don't.

44:50.280 --> 44:53.160
 So we need a garbage collection function

44:53.160 --> 44:54.560
 for rules and regulations.

44:54.560 --> 44:57.480
 They should not just be immortal,

44:57.480 --> 45:00.000
 because some of the rules and regulations that are put in place

45:00.000 --> 45:02.800
 will be counterproductive, done with good intentions,

45:02.800 --> 45:03.720
 but counterproductive.

45:03.720 --> 45:05.600
 Sometimes not done with good intentions.

45:05.600 --> 45:10.600
 So if rules and regulations just accumulate every year

45:10.840 --> 45:12.360
 and you get more and more of them,

45:12.360 --> 45:14.840
 then eventually you won't be able to do anything.

45:14.840 --> 45:17.320
 You're just like Gulliver tied down

45:17.320 --> 45:19.560
 by thousands of little strings.

45:19.560 --> 45:24.560
 And we see that in US and basically all economies

45:27.600 --> 45:30.360
 that have been around for a while,

45:31.400 --> 45:34.400
 and regulators and legislators

45:34.400 --> 45:36.560
 create new rules and regulations every year,

45:36.560 --> 45:38.680
 but they don't put effort into removing them.

45:38.680 --> 45:40.240
 And I think that's very important that we put effort

45:40.240 --> 45:42.040
 into removing rules and regulations.

45:44.000 --> 45:45.560
 But it gets tough, because you get special interests

45:45.560 --> 45:50.520
 that then are dependent on, like they have a vested interest

45:50.520 --> 45:51.920
 in that whatever rule and regulation,

45:51.920 --> 45:55.560
 and then they fight to not get it removed.

45:57.560 --> 46:00.880
 Yeah, so I mean, I guess the problem with the Constitution

46:00.880 --> 46:04.080
 is it's kind of like C versus Java,

46:04.080 --> 46:06.720
 because it doesn't have any garbage collection built in.

46:06.720 --> 46:07.920
 I think there should be,

46:07.920 --> 46:10.800
 when you first said the metaphor of garbage collection,

46:10.800 --> 46:11.640
 I loved it.

46:11.640 --> 46:12.480
 For the coding standpoint.

46:12.480 --> 46:14.320
 For the coding standpoint, yeah, yeah.

46:14.320 --> 46:16.880
 It would be interesting if the laws themselves

46:16.880 --> 46:20.000
 kind of had a built in thing where they kind of die

46:20.000 --> 46:23.600
 after a while and somebody explicitly publicly defends them.

46:23.600 --> 46:26.200
 So that's sort of, it's not like somebody has to kill them,

46:26.200 --> 46:29.160
 they kind of die themselves, they disappear.

46:29.160 --> 46:30.000
 Yeah.

46:32.520 --> 46:33.920
 Not to defend Java or anything,

46:33.920 --> 46:38.520
 but the C++, you could also have a great garbage collection

46:38.520 --> 46:39.920
 in Python and so on.

46:39.920 --> 46:43.760
 Yeah, so yeah, something needs to happen,

46:43.760 --> 46:48.600
 or just the civilization's already just hardened over time.

46:48.600 --> 46:50.840
 And you can just get less and less done

46:50.840 --> 46:53.160
 because there's just a rule against everything.

46:54.840 --> 46:57.640
 So I think like, I don't know, for Mars,

46:57.640 --> 47:00.280
 whatever I say, I would say for Earth as well,

47:00.280 --> 47:02.640
 like I think there should be an active process

47:02.640 --> 47:04.960
 for removing rules and regulations

47:04.960 --> 47:07.120
 and questioning their existence.

47:07.120 --> 47:10.280
 Just like, if we've got a function

47:10.280 --> 47:11.560
 for creating rules and regulations,

47:11.560 --> 47:13.480
 because rules and regulations can also think of as like,

47:13.480 --> 47:15.760
 they're like software or lines of code

47:15.760 --> 47:18.880
 for operating civilization.

47:18.880 --> 47:21.280
 That's the rules and regulations.

47:21.280 --> 47:22.960
 So it's not like we shouldn't have rules and regulations,

47:22.960 --> 47:27.120
 but you have code accumulation, but no code removal.

47:27.120 --> 47:30.320
 And so it just gets to become basically

47:30.320 --> 47:33.480
 archaic bloatware after a while.

47:33.480 --> 47:37.920
 And it's just, it makes it hard for things to progress.

47:37.920 --> 47:42.560
 So I don't know, maybe Mars, you'd have like any given law

47:42.560 --> 47:48.560
 must have a sunset and require active voting

47:48.560 --> 47:52.160
 to keep it up there, you know?

47:52.160 --> 47:54.640
 And I should also say like, and these are just,

47:54.640 --> 47:57.000
 I don't know, recommendations or thoughts,

47:57.000 --> 48:00.120
 and ultimately we'll be up to the people in Mars

48:00.120 --> 48:05.000
 to decide, but I think it should be easier

48:05.000 --> 48:07.520
 to remove a law than to add one

48:07.520 --> 48:10.680
 because of the, just to overcome the inertia of laws.

48:10.680 --> 48:15.200
 So maybe it's like, for argument's sake,

48:15.200 --> 48:19.800
 you need like say 60% vote to have a law take effect,

48:19.800 --> 48:22.040
 but only a 40% vote to remove it.

48:23.440 --> 48:26.640
 So let me be the guy, you posted a meme on Twitter recently

48:26.640 --> 48:30.160
 where there's like a row of urinals

48:30.160 --> 48:33.080
 and guy just walks all the way across

48:33.080 --> 48:34.640
 and he tells you about crypto.

48:36.320 --> 48:38.360
 I mean, that's happened to be so many times,

48:38.360 --> 48:40.400
 I think maybe even literally.

48:40.400 --> 48:41.800
 Yeah.

48:41.800 --> 48:43.480
 Do you think, technologically speaking,

48:43.480 --> 48:47.360
 there's any room for ideas of smart contracts or so on,

48:47.360 --> 48:49.320
 because you mentioned laws,

48:49.320 --> 48:52.960
 that's an interesting use of things like smart contracts

48:52.960 --> 48:57.280
 to implement the laws by which governments function.

48:57.280 --> 48:58.960
 And like something built on Ethereum

48:58.960 --> 49:03.960
 or maybe a dog coin that enables smart contracts somehow.

49:04.840 --> 49:07.680
 I don't quite understand this whole smart contract thing.

49:09.960 --> 49:14.960
 I mean, I'm too down to understand smart contracts.

49:14.960 --> 49:15.960
 That's a good line.

49:17.880 --> 49:21.040
 I mean, my general approach to any kind of like deal

49:21.040 --> 49:23.760
 or whatever is just make sure there's clarity of understanding.

49:23.760 --> 49:25.800
 That's the most important thing.

49:25.800 --> 49:29.080
 And just keep any kind of deal very, very short

49:29.080 --> 49:31.320
 and simple, plain language.

49:31.320 --> 49:34.240
 And just make sure everyone understands this is the deal.

49:34.240 --> 49:36.520
 Is everyone, is it clear?

49:36.520 --> 49:39.760
 And what are the consequences

49:39.760 --> 49:41.360
 if various things don't happen?

49:42.600 --> 49:47.200
 But usually deals are business deals or whatever

49:47.200 --> 49:50.920
 are way too long and complex and overly lawyered.

49:50.920 --> 49:52.720
 And pointlessly.

49:52.720 --> 49:56.400
 You mentioned that Doge is the people's coin.

49:57.680 --> 50:00.720
 And you said that you were literally going SpaceX,

50:00.720 --> 50:05.720
 may consider literally putting a Doge coin on the moon.

50:07.480 --> 50:11.960
 Is this something you're still considering Mars perhaps?

50:11.960 --> 50:13.640
 Do you think there's some chance

50:13.640 --> 50:16.080
 we've talked about political systems on Mars

50:16.080 --> 50:20.240
 that Doge coin is the official currency of Mars

50:20.240 --> 50:22.560
 at some point in the future?

50:22.560 --> 50:25.720
 Well, I think Mars itself will need to have

50:25.720 --> 50:29.040
 a different currency because you can't synchronize

50:29.040 --> 50:32.680
 due to speed of light or not easily.

50:32.680 --> 50:35.080
 So it must be completely standalone from Earth?

50:36.480 --> 50:41.320
 Well, yeah, because Mars is at closest approach,

50:41.320 --> 50:43.000
 it's four light minutes away roughly.

50:43.000 --> 50:45.000
 And then at most approach,

50:45.000 --> 50:48.480
 it's roughly 20 light minutes away, maybe a little more.

50:48.480 --> 50:52.760
 So you can't really have something synchronizing.

50:52.760 --> 50:55.520
 You know, if you've got a 20 minutes speed of light issue,

50:55.520 --> 50:58.200
 if it's got a one minute block chain,

50:58.200 --> 51:00.000
 it's not gonna synchronize probably.

51:01.200 --> 51:04.720
 So Mars, I don't know if Mars would have a crypto currency

51:04.720 --> 51:07.640
 as a thing, but probably seems likely,

51:07.640 --> 51:10.200
 but it would be some kind of localized thing on Mars.

51:12.320 --> 51:13.920
 And you let the people decide?

51:14.840 --> 51:16.560
 Yeah, absolutely.

51:16.560 --> 51:19.840
 The future of Mars should be up to the Martians.

51:20.720 --> 51:25.720
 Yeah, so I mean, I think the crypto currency thing

51:25.800 --> 51:30.800
 is an interesting approach to reducing the error

51:34.080 --> 51:38.080
 in the database that is called money.

51:41.200 --> 51:42.960
 You know, I think I have a pretty deep understanding

51:42.960 --> 51:46.760
 of what money actually is on a practical day to day basis

51:46.760 --> 51:48.080
 because of PayPal.

51:50.120 --> 51:52.960
 You know, we really got in deep there.

51:55.120 --> 51:57.280
 And right now the money system,

51:57.280 --> 51:59.080
 actually for practical purposes,

51:59.080 --> 52:04.080
 is really a bunch of heterogeneous mainframes

52:04.080 --> 52:05.800
 running old cobalt.

52:07.480 --> 52:09.000
 Okay, you mean literally?

52:09.000 --> 52:09.840
 Literally.

52:09.840 --> 52:10.840
 Literally what's happening.

52:10.840 --> 52:11.680
 In batch mode.

52:11.680 --> 52:14.080
 Okay, in batch mode.

52:14.080 --> 52:16.440
 Yeah, pretty the poor bastards

52:16.440 --> 52:19.000
 who have to maintain that code.

52:19.000 --> 52:22.200
 Okay, that's a pain.

52:22.200 --> 52:24.200
 Not even Fortran, it's cobalt, yep.

52:24.200 --> 52:25.040
 It's cobalt.

52:26.000 --> 52:30.080
 And the banks are still buying mainframes in 2021

52:30.080 --> 52:32.000
 and running ancient cobalt code.

52:32.960 --> 52:37.760
 And the Federal Reserve is probably even older

52:37.760 --> 52:38.920
 than what the banks have

52:38.920 --> 52:41.840
 and they have an old cobalt mainframe.

52:41.840 --> 52:45.760
 And so now, and so the government effectively

52:45.760 --> 52:48.640
 has editing privileges on the money database.

52:49.400 --> 52:51.480
 And they use those editing privileges

52:51.480 --> 52:56.400
 to make more money whenever they want.

52:56.400 --> 53:00.000
 And this increases the error in the database that is money.

53:00.000 --> 53:01.800
 So I think money should really be viewed

53:01.800 --> 53:04.560
 through the lens of information theory.

53:04.560 --> 53:09.240
 And so it's, you're kind of like an internet connection.

53:09.240 --> 53:13.640
 Like what's the bandwidth, you know, total bit rate?

53:13.640 --> 53:17.560
 What is the latency, jitter, packet drop,

53:17.560 --> 53:21.760
 you know, errors in network communication?

53:22.800 --> 53:25.440
 Just think of money like that, basically.

53:25.440 --> 53:27.400
 I think that's probably why we really think of it.

53:27.400 --> 53:29.480
 And then say what system

53:31.160 --> 53:32.880
 from an information theory standpoint

53:32.880 --> 53:35.160
 allows an economy to function the best.

53:36.520 --> 53:41.520
 And, you know, crypto is an attempt to reduce the error

53:44.600 --> 53:49.600
 in money that is contributed by governments

53:51.720 --> 53:53.280
 diluting the money supply

53:53.280 --> 53:57.200
 as basically a pernicious form of taxation.

53:57.200 --> 54:02.200
 So both policy in terms of with inflation and actual,

54:03.280 --> 54:07.080
 like technological cobalt, like cryptocurrency

54:07.080 --> 54:08.840
 takes us into the 21st century

54:08.840 --> 54:10.720
 in terms of the actual systems

54:10.720 --> 54:12.120
 that allow you to do the transaction,

54:12.120 --> 54:14.120
 to store wealth, all those kinds of things.

54:16.880 --> 54:18.560
 Like I said, just think of money as information.

54:18.560 --> 54:20.840
 People often will think of money

54:20.840 --> 54:22.680
 as having power in and of itself.

54:24.080 --> 54:24.960
 It does not.

54:24.960 --> 54:26.800
 Money is information.

54:26.800 --> 54:29.840
 And it does not have power in and of itself.

54:31.400 --> 54:35.000
 Like, you know, applying the physics tools

54:35.000 --> 54:37.480
 of thinking about things in the limit is helpful.

54:37.480 --> 54:39.960
 If you are stranded on a tropical island

54:41.000 --> 54:45.600
 and you have a trillion dollars, it's useless.

54:47.640 --> 54:50.280
 Because there's no resource allocation.

54:50.280 --> 54:52.640
 Money is a database for resource allocation.

54:52.640 --> 54:55.000
 If there's no resource to allocate except yourself.

54:55.000 --> 54:57.220
 So money is useless.

55:01.000 --> 55:04.080
 If you're stranded on a desert island with no food,

55:05.640 --> 55:10.640
 all the Bitcoin in the world will not stop you from starving.

55:16.320 --> 55:20.840
 So just think of money as a database

55:20.840 --> 55:24.040
 for resource allocation across time and space.

55:24.040 --> 55:29.040
 And then what system in what form should that database

55:35.160 --> 55:39.080
 or data system, what would be most effective?

55:39.080 --> 55:43.880
 Now, there is a fundamental issue with, say, Bitcoin

55:43.880 --> 55:47.960
 in its current form in that the transaction volume

55:47.960 --> 55:48.800
 is very limited.

55:48.800 --> 55:53.800
 And the latency for a properly confirmed transaction

55:57.640 --> 56:00.160
 is too long, much longer than you'd like.

56:00.160 --> 56:04.520
 So it's actually not great from a transaction volume

56:04.520 --> 56:06.560
 standpoint or a latency standpoint.

56:09.200 --> 56:14.200
 So it is perhaps useful to solve an aspect

56:14.360 --> 56:16.200
 of the money database problem,

56:16.200 --> 56:19.960
 which is the sort of store of wealth

56:19.960 --> 56:24.720
 or an accounting of relative obligations, I suppose.

56:25.680 --> 56:29.440
 But it is not useful as a currency,

56:29.440 --> 56:30.840
 as a day to day currency.

56:30.840 --> 56:33.320
 But people have proposed different technological solutions.

56:33.320 --> 56:34.320
 Like Lightning.

56:34.320 --> 56:36.800
 Yeah, Lightning Network and the layer two technologies

56:36.800 --> 56:37.880
 on top of that.

56:37.880 --> 56:40.880
 I mean, it's all, it seems to be all kind of a trade off.

56:40.880 --> 56:43.160
 But the point is, it's kind of brilliant to say

56:43.160 --> 56:44.560
 that just think about it information,

56:44.560 --> 56:46.240
 think about what kind of database,

56:46.240 --> 56:48.120
 what kind of infrastructure enables

56:48.120 --> 56:48.960
 the exchange of information.

56:48.960 --> 56:51.360
 Like if you're operating in an economy,

56:51.360 --> 56:55.320
 and you need to have some thing that allows

56:55.320 --> 56:59.800
 for the efficient, to have efficient value ratios

56:59.800 --> 57:01.440
 between products and services.

57:01.440 --> 57:03.240
 So you've got this massive number of products

57:03.240 --> 57:07.320
 and services and you need to, you can't just barter.

57:07.320 --> 57:09.680
 It's like, that would be extremely unwieldy.

57:09.680 --> 57:14.680
 So you need something that gives you a ratio of exchange

57:17.520 --> 57:19.280
 between goods and services.

57:20.600 --> 57:25.240
 And then something that allows you to shift obligations

57:25.240 --> 57:27.320
 across time, like debt, debt and equity,

57:27.320 --> 57:29.280
 shift obligations across time.

57:29.280 --> 57:31.440
 Then what does the best job of that?

57:33.360 --> 57:36.080
 Part of the reason why I think there's some

57:36.080 --> 57:38.760
 marriage to dogecoin, even though it was obviously created

57:38.760 --> 57:43.760
 as a joke, is that it actually does have

57:44.160 --> 57:48.400
 a much higher transaction volume capability than Bitcoin.

57:49.880 --> 57:53.040
 And the costs of doing a transaction,

57:53.040 --> 57:56.040
 the dogecoin fee is very low.

57:56.040 --> 57:58.280
 Like right now, if you want to do a Bitcoin transaction,

57:58.280 --> 58:00.520
 the price of doing that transaction is very high.

58:00.520 --> 58:04.400
 So you could not use it effectively for most things.

58:04.400 --> 58:09.400
 And nor could it even scale to a high volume.

58:11.920 --> 58:15.320
 And when Bitcoin started, I guess around 2008

58:15.320 --> 58:18.800
 or something like that, the internet connections

58:18.800 --> 58:20.880
 were much worse than they are today.

58:20.880 --> 58:24.960
 Like order of magnitude, I mean, there's the way,

58:24.960 --> 58:26.880
 way worse in 2008.

58:26.880 --> 58:31.880
 So like having a small block size or whatever

58:31.880 --> 58:35.760
 is, and a long synchronization time

58:35.760 --> 58:37.840
 is made sense in 2008.

58:37.840 --> 58:41.400
 But to 2021 or fast forward 10 years,

58:41.400 --> 58:46.400
 it's like, it's like comically low, it's a,

58:47.000 --> 58:52.000
 so, and I think there's some value

58:52.280 --> 58:57.000
 to having a linear increase in the amount of currency

58:57.000 --> 58:58.640
 that is generated.

58:58.640 --> 59:01.960
 So because some amount of the currency,

59:01.960 --> 59:06.960
 like if a currency is too deflationary or like,

59:07.240 --> 59:11.560
 or should say if a currency is expected

59:11.560 --> 59:13.040
 to increase in value over time,

59:13.040 --> 59:15.240
 there's reluctance to spend it.

59:15.240 --> 59:16.920
 Cause you're like, oh, if I,

59:16.920 --> 59:18.560
 I'll just hold it and not spend it

59:18.560 --> 59:20.640
 because it's scarcity is increasing with time.

59:20.640 --> 59:23.440
 So if I spend it now, then I will regret spending it.

59:23.440 --> 59:26.080
 So I will just, you know, total it.

59:26.080 --> 59:30.680
 But if there's some dilution of the currency

59:30.680 --> 59:32.840
 occurring over time, that's more of an incentive

59:32.840 --> 59:34.280
 to use it as a currency.

59:34.280 --> 59:38.280
 So those coins somewhat randomly has a,

59:41.320 --> 59:45.000
 just a fixed number of sort of coins

59:45.000 --> 59:49.640
 or hash strings that are generated every year.

59:49.640 --> 59:52.800
 So there's some inflation, but it's not a percentage base.

59:52.800 --> 59:55.720
 It's a fixed number.

59:55.720 --> 59:58.400
 So the percentage of inflation

59:58.400 --> 1:00:00.080
 will necessarily decline over time.

1:00:02.720 --> 1:00:04.440
 So it just, I'm not saying

1:00:04.440 --> 1:00:07.600
 that it's like the ideal system for a currency,

1:00:07.600 --> 1:00:10.600
 but I think it actually is just fundamentally better

1:00:10.600 --> 1:00:14.400
 than anything else I've seen, just by accident.

1:00:15.800 --> 1:00:16.640
 So.

1:00:16.640 --> 1:00:19.800
 Like I said, around 2008.

1:00:19.800 --> 1:00:23.480
 So you're not, you know, some people suggested

1:00:23.480 --> 1:00:24.840
 you might be set to Oshinakamoto.

1:00:24.840 --> 1:00:27.120
 You've previously said you're not, let me ask.

1:00:27.120 --> 1:00:28.800
 You're not for sure.

1:00:28.800 --> 1:00:30.160
 Would you tell us if you were?

1:00:30.160 --> 1:00:31.000
 Yes.

1:00:31.000 --> 1:00:31.840
 Okay.

1:00:31.840 --> 1:00:34.800
 Do you think it's a feature or bug

1:00:34.800 --> 1:00:37.280
 that he's anonymous or she or they?

1:00:39.000 --> 1:00:41.680
 It's an interesting kind of quirk of human history

1:00:41.680 --> 1:00:43.600
 that there is a particular technology

1:00:43.600 --> 1:00:46.240
 that is a completely anonymous inventor.

1:00:46.240 --> 1:00:53.240
 Or creator.

1:01:03.400 --> 1:01:08.040
 Well, I mean, you can look at the evolution of ideas

1:01:10.080 --> 1:01:11.880
 before the launch of Bitcoin

1:01:11.880 --> 1:01:16.880
 and see who wrote, you know, about those ideas.

1:01:20.640 --> 1:01:22.800
 And then I don't know exactly,

1:01:22.800 --> 1:01:25.160
 obviously I don't know who created Bitcoin

1:01:25.160 --> 1:01:26.120
 for practical purposes,

1:01:26.120 --> 1:01:29.960
 but the evolution of ideas is pretty clear for that.

1:01:29.960 --> 1:01:33.000
 And like it seems as though like Nick Szabo

1:01:33.000 --> 1:01:35.680
 is probably more than anyone else

1:01:35.680 --> 1:01:38.040
 responsible for the evolution of those ideas.

1:01:38.040 --> 1:01:41.880
 So he claims not to be Nakamoto,

1:01:41.880 --> 1:01:45.360
 but I'm not sure that's neither here nor there,

1:01:45.360 --> 1:01:48.280
 but he seems to be the one more responsible

1:01:48.280 --> 1:01:50.880
 for the ideas behind Bitcoin than anyone else.

1:01:50.880 --> 1:01:53.360
 So it's not perhaps like singular figures

1:01:53.360 --> 1:01:56.240
 aren't even as important as the figures involved

1:01:56.240 --> 1:01:58.200
 in the evolution of ideas that led to a thing.

1:01:58.200 --> 1:02:03.200
 So, you know, most perhaps it's sad to think about history,

1:02:03.600 --> 1:02:06.920
 but maybe most names will be forgotten anyway.

1:02:06.920 --> 1:02:08.040
 What is the name anyway?

1:02:08.040 --> 1:02:10.760
 It's a name attached to an idea.

1:02:12.200 --> 1:02:13.760
 What does it even mean really?

1:02:13.760 --> 1:02:16.320
 I think Shakespeare had a thing about roses and stuff,

1:02:16.320 --> 1:02:17.240
 whatever you said.

1:02:17.240 --> 1:02:18.720
 Roses by any other name.

1:02:18.720 --> 1:02:19.560
 It smells sweet.

1:02:22.440 --> 1:02:24.360
 I gotta yield on to quote Shakespeare.

1:02:24.360 --> 1:02:26.920
 I feel like I accomplished something today.

1:02:26.920 --> 1:02:28.840
 Shall I compare thee to a summer's day?

1:02:30.840 --> 1:02:34.040
 I'm gonna clip that out instead of doing it.

1:02:34.040 --> 1:02:37.040
 It's a lot more temperate and more fair.

1:02:39.040 --> 1:02:41.040
 Autopilot, Tesla autopilot.

1:02:46.160 --> 1:02:48.560
 Tesla autopilot has been through an incredible journey

1:02:48.560 --> 1:02:50.560
 over the past six years,

1:02:50.560 --> 1:02:52.800
 or perhaps even longer in the minds of,

1:02:52.800 --> 1:02:55.200
 in your mind, in the minds of many involved.

1:02:57.080 --> 1:02:58.840
 I think that's where we first like connected

1:02:58.840 --> 1:03:01.960
 really was the autopilot stuff, autonomy and.

1:03:01.960 --> 1:03:05.200
 The whole journey was incredible to me to watch.

1:03:05.200 --> 1:03:06.040
 I was,

1:03:07.680 --> 1:03:10.320
 because I knew, well, part of it was I was at MIT

1:03:10.320 --> 1:03:13.160
 and I knew the difficulty of computer vision.

1:03:13.160 --> 1:03:15.240
 And I knew the whole, I had a lot of colleagues

1:03:15.240 --> 1:03:16.760
 and friends about the DARPA challenge.

1:03:16.760 --> 1:03:18.440
 I knew how difficult it is.

1:03:18.440 --> 1:03:20.120
 And so there was a natural skepticism.

1:03:20.120 --> 1:03:23.680
 When I first drove a Tesla with the initial system

1:03:23.680 --> 1:03:26.520
 based on Mobileye, I thought there's no way.

1:03:27.480 --> 1:03:29.880
 So the first one I got in, I thought there's no way

1:03:29.880 --> 1:03:34.120
 this car could maintain, like staying in the lane

1:03:34.120 --> 1:03:35.880
 and create a comfortable experience.

1:03:35.880 --> 1:03:39.480
 So my intuition initially was that the lane keeping problem

1:03:39.480 --> 1:03:41.720
 is way too difficult to solve.

1:03:41.720 --> 1:03:43.600
 Oh, lane keeping, yeah, that's relatively easy.

1:03:43.600 --> 1:03:48.400
 Well, like, but not the, but solve in the way that we just,

1:03:48.400 --> 1:03:52.600
 we talked about previous is prototype versus a thing

1:03:52.600 --> 1:03:54.360
 that actually creates a pleasant experience

1:03:54.360 --> 1:03:57.400
 over hundreds of thousands of miles and millions.

1:03:57.400 --> 1:03:58.240
 Yeah, so.

1:03:58.240 --> 1:04:01.680
 I mean, we had to wrap a lot of code around the Mobileye thing.

1:04:01.680 --> 1:04:04.360
 It doesn't just work by itself.

1:04:04.360 --> 1:04:06.360
 I mean, that's part of the story

1:04:06.360 --> 1:04:07.960
 of how you approach things sometimes.

1:04:07.960 --> 1:04:09.640
 Sometimes you do things from scratch.

1:04:09.640 --> 1:04:12.440
 Sometimes at first you kind of see what's out there

1:04:12.440 --> 1:04:14.320
 and then you decide to do from scratch.

1:04:14.320 --> 1:04:17.160
 That was one of the boldest decisions I've seen

1:04:17.160 --> 1:04:18.800
 is both on the hardware and the software

1:04:18.800 --> 1:04:20.960
 to decide to eventually go from scratch.

1:04:20.960 --> 1:04:22.640
 I thought, again, I was skeptical

1:04:22.640 --> 1:04:24.440
 of whether that's going to be able to work out

1:04:24.440 --> 1:04:26.840
 because it's such a difficult problem.

1:04:26.840 --> 1:04:28.880
 And so it was an incredible journey.

1:04:28.880 --> 1:04:31.440
 What I see now with everything,

1:04:31.440 --> 1:04:33.200
 the hardware, the compute, the sensors,

1:04:33.200 --> 1:04:37.280
 the things I maybe care and love about most

1:04:37.280 --> 1:04:40.040
 is the stuff that Andre Carpathi is leading

1:04:40.040 --> 1:04:41.720
 with the data set selection,

1:04:41.720 --> 1:04:43.080
 the whole data engine process,

1:04:43.080 --> 1:04:45.000
 the neural network architectures,

1:04:45.000 --> 1:04:47.280
 the way that's in the real world,

1:04:47.280 --> 1:04:49.360
 that network is tested, validated,

1:04:49.360 --> 1:04:50.920
 all the different test sets,

1:04:52.320 --> 1:04:54.720
 versus the ImageNet model of computer vision

1:04:54.720 --> 1:04:58.360
 like what's in academia is like real world

1:04:58.360 --> 1:04:59.840
 artificial intelligence.

1:04:59.840 --> 1:05:02.560
 So, and Andre is awesome

1:05:02.560 --> 1:05:04.200
 and obviously plays an important role,

1:05:04.200 --> 1:05:07.680
 but we have a lot of really talented people driving things.

1:05:07.680 --> 1:05:11.560
 So, and Ashok is actually the head

1:05:11.560 --> 1:05:12.760
 of autopilot engineering.

1:05:14.760 --> 1:05:16.360
 Andre is the director of AI.

1:05:16.360 --> 1:05:17.680
 AI stuff, yeah, yeah.

1:05:17.680 --> 1:05:20.560
 So yeah, there's, I'm aware that there's an incredible team

1:05:20.560 --> 1:05:22.040
 of just a lot going on.

1:05:22.040 --> 1:05:26.040
 Yeah, I just, you know, people will give me too much credit

1:05:26.040 --> 1:05:28.680
 and they'll give Andre too much credit, so.

1:05:28.680 --> 1:05:31.680
 And people should realize how much is going on

1:05:31.680 --> 1:05:32.520
 under the hood.

1:05:32.520 --> 1:05:34.680
 Yeah, it's just a lot of really talented people.

1:05:36.480 --> 1:05:40.000
 The Tesla autopilot AI team is extremely talented.

1:05:40.000 --> 1:05:42.600
 It's like some of the smartest people in the world.

1:05:43.640 --> 1:05:45.000
 So yeah, we're getting it done.

1:05:45.000 --> 1:05:47.640
 What are some insights you've gained

1:05:47.640 --> 1:05:51.280
 over those five, six years of autopilot

1:05:51.280 --> 1:05:54.240
 about the problem of autonomous driving?

1:05:54.240 --> 1:05:59.240
 So, you leaped in having some sort of first principles,

1:05:59.280 --> 1:06:02.280
 kinds of intuitions, but nobody knows

1:06:02.280 --> 1:06:05.320
 how difficult the problem, like the problem.

1:06:05.320 --> 1:06:07.120
 I thought the self driving problem would be hard,

1:06:07.120 --> 1:06:08.960
 but it was harder than I thought.

1:06:08.960 --> 1:06:09.920
 It's not like I thought it'd be easy.

1:06:09.920 --> 1:06:10.760
 I thought it'd be very hard,

1:06:10.760 --> 1:06:14.200
 but it was actually way harder than even that.

1:06:14.200 --> 1:06:17.000
 So, what it comes down to at the end of the day

1:06:17.000 --> 1:06:21.560
 is to solve self driving, you have to solve,

1:06:21.560 --> 1:06:26.560
 you basically need to recreate what humans do to drive,

1:06:28.720 --> 1:06:31.560
 which is humans drive with optical sensors,

1:06:31.560 --> 1:06:33.840
 eyes, and biological neural nets.

1:06:34.920 --> 1:06:37.880
 And so in order to, that's how the entire road system

1:06:37.880 --> 1:06:42.880
 is designed to work with basically passive optical

1:06:42.880 --> 1:06:46.120
 and neural nets, biologically.

1:06:46.120 --> 1:06:47.920
 And now that we need to,

1:06:47.920 --> 1:06:50.080
 so for actually for full self driving to work,

1:06:50.080 --> 1:06:52.960
 we have to recreate that in digital form.

1:06:52.960 --> 1:06:56.120
 So we have to, that means cameras

1:06:56.120 --> 1:07:01.120
 with advanced neural nets in silicon form,

1:07:04.040 --> 1:07:08.000
 and then it will obviously solve for full self driving.

1:07:08.000 --> 1:07:09.000
 That's the only way.

1:07:09.000 --> 1:07:10.320
 I don't think there's any other way.

1:07:10.320 --> 1:07:12.920
 But the question is, what aspects of human nature

1:07:12.920 --> 1:07:15.560
 do you have to encode into the machine, right?

1:07:15.560 --> 1:07:18.760
 Do you have to solve the perception problem, like detect?

1:07:18.760 --> 1:07:21.320
 And then you first, while it realize,

1:07:21.320 --> 1:07:23.080
 what is the perception problem for driving?

1:07:23.080 --> 1:07:25.400
 Like all the kinds of things you have to be able to see.

1:07:25.400 --> 1:07:27.880
 Like what do we even look at when we drive?

1:07:27.880 --> 1:07:32.440
 There's, I just recently heard Andre talked about at MIT

1:07:32.440 --> 1:07:33.720
 about like car doors.

1:07:33.720 --> 1:07:35.560
 I think it was the world's greatest talk

1:07:35.560 --> 1:07:36.960
 of all time about car doors.

1:07:36.960 --> 1:07:41.360
 The fine details of car doors.

1:07:41.360 --> 1:07:44.440
 Like what is even an open car door, man?

1:07:44.440 --> 1:07:46.880
 So like the the ontology of that,

1:07:46.880 --> 1:07:48.000
 that's the perception problem.

1:07:48.000 --> 1:07:49.840
 We humans solve that perception problem

1:07:49.840 --> 1:07:51.640
 and Tesla has to solve that problem.

1:07:51.640 --> 1:07:53.360
 And then there's the control and the planning

1:07:53.360 --> 1:07:54.960
 coupled with the perception.

1:07:54.960 --> 1:07:58.280
 You have to figure out like what's involved in driving,

1:07:58.280 --> 1:08:00.880
 like especially in all the different edge cases.

1:08:02.320 --> 1:08:06.560
 And then, I mean, maybe you can comment on this,

1:08:06.560 --> 1:08:10.440
 how much game theoretic kind of stuff needs to be involved,

1:08:10.440 --> 1:08:13.280
 you know, at a four way stop sign.

1:08:13.280 --> 1:08:15.560
 You know, as humans, when we drive,

1:08:15.560 --> 1:08:18.040
 our actions affect the world.

1:08:18.040 --> 1:08:20.760
 Like it changes how others behave.

1:08:20.760 --> 1:08:22.080
 Most of the time was driving,

1:08:22.080 --> 1:08:27.080
 if you're usually just responding to the scene,

1:08:27.400 --> 1:08:31.320
 as opposed to like really asserting yourself in the scene.

1:08:31.320 --> 1:08:32.160
 Do you think?

1:08:33.080 --> 1:08:37.560
 I think these sort of control logic conundrums

1:08:37.560 --> 1:08:39.320
 are not the hard part.

1:08:39.320 --> 1:08:43.600
 The, you know, let's see.

1:08:45.560 --> 1:08:46.840
 What do you think is the hard part

1:08:46.840 --> 1:08:50.600
 in this whole beautiful, complex problem?

1:08:50.600 --> 1:08:53.000
 So it's a lot of frigging software, man.

1:08:53.000 --> 1:08:54.400
 A lot of smart lines of code.

1:08:57.320 --> 1:09:02.320
 For sure, in order to have create an accurate vector space.

1:09:03.960 --> 1:09:08.280
 So like you're coming from image space,

1:09:08.280 --> 1:09:12.560
 which is like this flow of photons.

1:09:12.560 --> 1:09:14.400
 You're going to the camera cameras

1:09:14.400 --> 1:09:19.400
 and then you have this massive bit stream in image space.

1:09:23.240 --> 1:09:26.800
 And then you have to effectively compress

1:09:29.560 --> 1:09:34.560
 a massive bit stream corresponding to photons

1:09:34.560 --> 1:09:39.560
 that knocked off an electron in a camera sensor

1:09:41.440 --> 1:09:45.320
 and turn that bit stream into vector space.

1:09:47.920 --> 1:09:49.640
 By vector space, I mean like,

1:09:51.680 --> 1:09:54.920
 you know, you've got cars and humans

1:09:54.920 --> 1:09:59.920
 and lane lines and curves and traffic lights

1:10:01.440 --> 1:10:02.440
 and that kind of thing.

1:10:02.440 --> 1:10:07.440
 Once you have an accurate vector space,

1:10:08.520 --> 1:10:11.680
 the control problem is similar to that of a video game,

1:10:11.680 --> 1:10:14.120
 like a grand theft order of cyberpunk.

1:10:14.120 --> 1:10:16.240
 If you have accurate, accurate, best vector space.

1:10:16.240 --> 1:10:18.280
 It's the control problem is,

1:10:18.280 --> 1:10:20.880
 I wouldn't say it's trivial, it's not trivial, but it's,

1:10:22.880 --> 1:10:27.880
 it's not like some insurmountable thing.

1:10:28.880 --> 1:10:32.120
 But having an accurate vector space is very difficult.

1:10:32.120 --> 1:10:35.520
 Yeah, I think we humans don't give enough respect

1:10:35.520 --> 1:10:37.880
 to how incredibly human perception system is,

1:10:37.880 --> 1:10:41.520
 to mapping the raw photons

1:10:41.520 --> 1:10:44.680
 to the vector space representation in our heads.

1:10:44.680 --> 1:10:47.480
 Your brain is doing an incredible amount of processing

1:10:47.480 --> 1:10:51.360
 and giving you an image that is a very cleaned up image.

1:10:51.360 --> 1:10:53.360
 Like when we look around here, we see,

1:10:53.360 --> 1:10:55.320
 like you see color in the corners of your eyes,

1:10:55.320 --> 1:10:59.400
 but actually your eyes have very few cones,

1:10:59.400 --> 1:11:02.240
 like cone receptors in the peripheral vision.

1:11:02.240 --> 1:11:05.640
 Your eyes are painting color in the peripheral vision.

1:11:05.640 --> 1:11:07.240
 You don't realize it, but their eyes

1:11:07.240 --> 1:11:09.040
 are actually painting color.

1:11:09.040 --> 1:11:12.240
 And your eyes also have like this blood vessels

1:11:12.240 --> 1:11:13.480
 and also to gnarly things.

1:11:13.480 --> 1:11:16.360
 And there's a blind spot, but do you see your blind spot?

1:11:16.360 --> 1:11:21.160
 No, your brain is painting in the missing, the blind spot.

1:11:21.160 --> 1:11:23.960
 You can do these like, see these things online

1:11:23.960 --> 1:11:25.960
 where you look here and look at this point

1:11:25.960 --> 1:11:27.360
 and then look at this point.

1:11:27.360 --> 1:11:30.480
 And it's, if it's in your blind spot,

1:11:30.480 --> 1:11:33.240
 your brain will just fill in the missing bits.

1:11:33.240 --> 1:11:35.280
 It's so cool. The peripheral vision is so cool.

1:11:35.280 --> 1:11:37.920
 It makes you realize all the illusions for vision science

1:11:37.920 --> 1:11:40.640
 and so it makes you realize just how incredible the brain is.

1:11:40.640 --> 1:11:42.640
 The brain is doing crazy amount of post processing

1:11:42.640 --> 1:11:45.840
 on the vision signals from your eyes.

1:11:45.840 --> 1:11:47.680
 It's insane.

1:11:47.680 --> 1:11:53.360
 So, and then even once you get all those vision signals,

1:11:53.360 --> 1:11:57.720
 your brain is constantly trying to forget as much as possible.

1:11:57.720 --> 1:12:00.360
 So, human memory is perhaps the weakest thing

1:12:00.360 --> 1:12:01.920
 about the brain is memory.

1:12:01.920 --> 1:12:05.280
 So, because memory is so expensive to a brain

1:12:05.280 --> 1:12:08.240
 and so limited, your brain is trying to forget

1:12:08.240 --> 1:12:12.160
 as much as possible and distill the things that you see

1:12:12.160 --> 1:12:16.520
 into the smallest amounts of information possible.

1:12:16.520 --> 1:12:19.400
 So, your brain is trying to not just get to a vector space,

1:12:19.400 --> 1:12:20.920
 but get to a vector space

1:12:20.920 --> 1:12:23.600
 that is the smallest possible vector space

1:12:23.600 --> 1:12:26.640
 of only relevant objects.

1:12:26.640 --> 1:12:29.920
 And I think like, you can sort of look inside your brain,

1:12:29.920 --> 1:12:33.160
 or at least I can, like when you drive down the road

1:12:33.160 --> 1:12:37.480
 and try to think about what your brain is actually doing

1:12:37.480 --> 1:12:39.000
 consciously.

1:12:39.000 --> 1:12:43.680
 And it's, it's, it's, it's, it's, it's like, you'll see a car

1:12:43.680 --> 1:12:46.600
 that's, because you're, you're, you don't have cameras.

1:12:46.600 --> 1:12:48.760
 You, I don't have eyes in the back of your head or the side.

1:12:48.760 --> 1:12:52.520
 You know, so you say like, you basically, your, your head is

1:12:52.520 --> 1:12:55.960
 like a, you know, you basically have like two cameras

1:12:55.960 --> 1:12:56.880
 on a slow gimbal.

1:12:58.200 --> 1:13:01.720
 And, and what's your, and I said, it's not that great.

1:13:01.720 --> 1:13:02.560
 Okay.

1:13:02.560 --> 1:13:05.120
 You and I is, you know, like, and people are constantly

1:13:05.120 --> 1:13:07.240
 distracted and thinking about things and texting

1:13:07.240 --> 1:13:09.320
 and doing all sorts of things they shouldn't do in a car,

1:13:09.320 --> 1:13:11.000
 changing the radio station.

1:13:11.000 --> 1:13:16.000
 So, having arguments, you know, is like, so, so then,

1:13:16.000 --> 1:13:21.000
 like, say like, like, like when's the last time you looked

1:13:21.000 --> 1:13:24.320
 right and left and, you know, or, and, and rearward or even

1:13:24.320 --> 1:13:28.480
 diagonally, you know, forward to actually refresh your vector

1:13:28.480 --> 1:13:29.480
 space.

1:13:29.480 --> 1:13:32.680
 So you're glancing around and what your mind is doing is, is,

1:13:32.680 --> 1:13:37.680
 is trying to still the relevant vectors, basically objects

1:13:37.680 --> 1:13:43.680
 with a position and motion and, and, and then, and then, and

1:13:43.680 --> 1:13:48.680
 then editing that down to the least amount that's necessary

1:13:48.800 --> 1:13:49.960
 for you to drive.

1:13:49.960 --> 1:13:54.240
 It does seem to be able to edit it down or compress it even

1:13:54.240 --> 1:13:55.800
 further into things like concepts.

1:13:55.800 --> 1:13:58.440
 So it's not, it's like, it goes beyond the human mind seems

1:13:58.440 --> 1:14:02.560
 to go sometimes beyond vector space to sort of space of

1:14:02.560 --> 1:14:05.120
 concepts to where you'll see a thing.

1:14:05.120 --> 1:14:07.560
 It's no longer represented spatially somehow.

1:14:07.560 --> 1:14:10.120
 It's almost like a concept that you should be aware of.

1:14:10.120 --> 1:14:13.320
 Like if this is a school zone, you'll remember that.

1:14:13.320 --> 1:14:14.160
 Yeah.

1:14:14.160 --> 1:14:16.560
 As a concept, which is a weird thing to represent, but

1:14:16.560 --> 1:14:20.040
 perhaps for driving, you don't need to fully represent

1:14:20.040 --> 1:14:23.040
 those things, or maybe you get those kind of, um,

1:14:24.440 --> 1:14:27.440
 well, you, you, you, you, you need to like establish vector

1:14:27.440 --> 1:14:33.440
 space and then actually have predictions for, uh, that those

1:14:33.440 --> 1:14:34.120
 vector spaces.

1:14:34.120 --> 1:14:39.360
 So like, um, you know, like if, uh, you know, like you drive

1:14:39.360 --> 1:14:44.600
 fast, say, say, uh, uh, uh, uh, uh, a bus and the, and you

1:14:44.600 --> 1:14:48.480
 see that this, this people, before you drove past the bus,

1:14:48.480 --> 1:14:51.200
 you saw people crossing like, or some just imagine there's

1:14:51.200 --> 1:14:54.080
 like a large truck or something blocking site.

1:14:54.640 --> 1:14:57.720
 Um, but you, before you came out to the truck, you saw

1:14:57.720 --> 1:15:01.000
 that there were some kids about to cross the road in front

1:15:01.000 --> 1:15:01.440
 of the truck.

1:15:01.440 --> 1:15:04.320
 Now you can no longer see the kids, but you, you, you need

1:15:04.320 --> 1:15:06.800
 to be able, but you would now know, okay, those kids are

1:15:06.800 --> 1:15:10.760
 probably going to pass by the truck and cross the road, even

1:15:10.760 --> 1:15:11.600
 though you cannot see them.

1:15:11.920 --> 1:15:17.920
 So you have to have, um, memory, uh, you have to need to

1:15:17.920 --> 1:15:20.320
 remember that there were kids there and you need to have

1:15:20.360 --> 1:15:23.960
 some forward prediction of what their position will be.

1:15:23.960 --> 1:15:25.600
 It's a really hard problem at the time of relevance.

1:15:25.600 --> 1:15:29.200
 So with, with occlusions and computer vision, when you can't

1:15:29.200 --> 1:15:32.640
 see an object anymore, even when it just walks behind a tree

1:15:32.640 --> 1:15:35.840
 and reappears, that's a really, really, I mean, at least in

1:15:35.840 --> 1:15:39.280
 academic literature, it's tracking through occlusions.

1:15:39.280 --> 1:15:40.080
 It's very difficult.

1:15:40.600 --> 1:15:40.840
 Yeah.

1:15:40.840 --> 1:15:41.320
 We're doing it.

1:15:41.520 --> 1:15:42.680
 Um, I understand this.

1:15:43.440 --> 1:15:45.800
 So some of it, it's like object permanence, like the same

1:15:45.800 --> 1:15:48.720
 thing happens with humans, with neural nets, like when, like

1:15:48.720 --> 1:15:51.800
 a toddler grows up, like there's a, there's a point in time

1:15:51.800 --> 1:15:55.600
 where, uh, they develop, they have a sense of object

1:15:55.600 --> 1:15:56.120
 permanence.

1:15:56.200 --> 1:15:59.560
 So before a certain age, if you have a ball, uh, or a toy

1:15:59.560 --> 1:16:01.920
 or whatever, and you put it behind your back and you pop it

1:16:01.920 --> 1:16:04.840
 out, if they don't, before they have object permanence, it's

1:16:04.840 --> 1:16:05.800
 like a new thing every time.

1:16:05.880 --> 1:16:08.240
 It's like, whoa, this toy went, poof, just spared.

1:16:08.280 --> 1:16:09.240
 And now it's back again.

1:16:09.280 --> 1:16:09.960
 And they can't believe it.

1:16:10.000 --> 1:16:12.440
 And that they can play peekaboo all day long because the

1:16:12.440 --> 1:16:13.600
 peekaboo is fresh every time.

1:16:16.160 --> 1:16:18.200
 But then we figured out object permanence.

1:16:18.240 --> 1:16:20.360
 Then they realized, oh, no, the object is not gone.

1:16:20.360 --> 1:16:21.200
 It's just behind your back.

1:16:21.920 --> 1:16:25.920
 Um, sometimes I wish we never did figure out object permanence.

1:16:26.320 --> 1:16:26.560
 Yeah.

1:16:26.560 --> 1:16:30.880
 So that's, uh, that's an important problem to solve.

1:16:31.640 --> 1:16:32.040
 Yes.

1:16:32.040 --> 1:16:34.840
 So, so, and like an important evolution of the neural nets in

1:16:34.840 --> 1:16:42.680
 the car is, uh, um, memory across, memory across both time and

1:16:42.680 --> 1:16:43.320
 space.

1:16:43.440 --> 1:16:47.080
 Um, so, uh, no, you can't remember, like you have to say,

1:16:47.080 --> 1:16:48.960
 like how long do you want to remember things for?

1:16:48.960 --> 1:16:51.880
 And, and it's, it doesn't, there's a cost to remembering

1:16:51.880 --> 1:16:53.240
 things for a long time.

1:16:53.240 --> 1:16:57.000
 So you get, you know, like run out of memory to try to remember

1:16:57.000 --> 1:16:57.960
 too much for too long.

1:16:58.520 --> 1:17:01.240
 Um, and, and then you also have things that are stale.

1:17:01.240 --> 1:17:03.880
 If, if, if they're, if you remember them for too long, and

1:17:03.880 --> 1:17:06.160
 then you also need things that are remembered, uh, remembered

1:17:06.160 --> 1:17:06.880
 over time.

1:17:06.880 --> 1:17:11.520
 So even if you like, say, have like, for a good sake, five

1:17:11.520 --> 1:17:15.240
 seconds of memory, uh, on a time basis, but like, let's say

1:17:15.240 --> 1:17:19.640
 you, you, you're parked at a light and you, and you saw, you

1:17:19.640 --> 1:17:22.680
 use a pedestrian example that people were waiting to cross

1:17:22.680 --> 1:17:26.520
 the, across the road and you can't, you can't quite see them

1:17:26.520 --> 1:17:27.480
 because of an occlusion.

1:17:28.040 --> 1:17:31.200
 Uh, but they might wait for a minute before the light

1:17:31.200 --> 1:17:32.920
 changes for them to cross the road.

1:17:33.160 --> 1:17:35.840
 You still need to, to remember that they, that that's where

1:17:35.840 --> 1:17:38.800
 they were, um, and that they're probably going to cross the

1:17:38.800 --> 1:17:39.680
 road type of thing.

1:17:39.800 --> 1:17:44.400
 Um, so even if that exceeds your, your, your time based

1:17:44.400 --> 1:17:46.840
 memory should not exceed your space of memory.

1:17:48.120 --> 1:17:50.480
 And I just think the data engine side of that.

1:17:50.480 --> 1:17:53.760
 So getting the data to learn all of the concepts that you're

1:17:53.760 --> 1:17:56.160
 saying now is an incredible process.

1:17:56.160 --> 1:17:59.080
 It's this iterative process of just, it's this, this

1:17:59.080 --> 1:18:04.560
 hydranet of many, hydranets, we're changing the name to

1:18:04.560 --> 1:18:05.200
 something else.

1:18:05.440 --> 1:18:05.760
 Okay.

1:18:05.920 --> 1:18:11.960
 I'm sure it'd be equally as Rick and Morty like, yeah, we've

1:18:11.960 --> 1:18:15.840
 rearchitected the neural net, uh, neural nets in the cars.

1:18:15.880 --> 1:18:17.240
 So many times it's crazy.

1:18:18.000 --> 1:18:20.680
 Oh, so every time there's a new major version, you'll rename it

1:18:20.680 --> 1:18:24.600
 to something more ridiculous or, or memorable and beautiful.

1:18:24.600 --> 1:18:26.200
 Sorry, not ridiculous, of course.

1:18:26.200 --> 1:18:30.400
 If you, if you see the full, the full like, uh, array of neural

1:18:30.400 --> 1:18:33.280
 nets that, that, that are operating in the car, it's kind

1:18:33.280 --> 1:18:34.000
 of boggles the mind.

1:18:34.280 --> 1:18:36.280
 There's so, there's so many layers.

1:18:36.280 --> 1:18:36.960
 It's crazy.

1:18:36.960 --> 1:18:40.440
 Um, so yeah.

1:18:40.960 --> 1:18:47.360
 Um, but, and, and we, we started off with, uh, simple neural

1:18:47.360 --> 1:18:52.640
 nets that were, uh, basically image recognition on a single

1:18:52.640 --> 1:18:59.080
 frame, from a single camera, uh, and then, uh, trying to knit

1:18:59.080 --> 1:19:03.920
 those together with it, you know, it with a C, uh, I should

1:19:03.920 --> 1:19:07.360
 say we, we're really primarily running C here because C plus

1:19:07.360 --> 1:19:10.400
 plus is too much overhead and we have our own C compiler.

1:19:10.840 --> 1:19:14.480
 So to get maximum performance, we actually wrote our own C

1:19:14.480 --> 1:19:17.400
 compiler and are continuing to optimize our C compiler, uh, for

1:19:17.960 --> 1:19:19.240
 maximum efficiency.

1:19:19.240 --> 1:19:23.080
 In fact, we've just recently, uh, done a new river on a, on a

1:19:23.080 --> 1:19:25.640
 C compiler that will compile directly to our autopilot

1:19:25.640 --> 1:19:26.080
 hardware.

1:19:26.080 --> 1:19:26.600
 Um,

1:19:26.600 --> 1:19:28.720
 Do you want to compile the whole thing down and with your

1:19:28.720 --> 1:19:29.680
 own compiler?

1:19:29.720 --> 1:19:30.160
 Yeah.

1:19:30.160 --> 1:19:33.360
 Like so efficiency here, cause there's all kinds of compute.

1:19:33.360 --> 1:19:36.520
 There's CPU, GPU, there's like the ASIC type of thing that's,

1:19:36.760 --> 1:19:39.040
 and you have to somehow figure out the scheduling across all

1:19:39.040 --> 1:19:39.520
 of those things.

1:19:39.520 --> 1:19:41.240
 And so you're compiling the code down.

1:19:41.480 --> 1:19:41.720
 Yeah.

1:19:41.720 --> 1:19:42.520
 It does all the, okay.

1:19:43.240 --> 1:19:45.920
 This is, so that's why there's a lot of people involved.

1:19:45.920 --> 1:19:50.800
 There's, there's a lot of hardcore, uh, software engineering at a

1:19:50.800 --> 1:19:55.880
 very sort of bare metal level, uh, cause you, we're trying to do

1:19:55.880 --> 1:20:01.600
 a lot of compute, uh, that's constrained to the, you know, our

1:20:01.880 --> 1:20:02.880
 full self driving computer.

1:20:02.960 --> 1:20:07.600
 So, and we want to try to have the highest frames per second, um,

1:20:07.680 --> 1:20:12.560
 possible, um, with, with sort of very, very finite amount of

1:20:12.560 --> 1:20:15.040
 compute, um, and power.

1:20:15.040 --> 1:20:20.800
 So, um, we really put a lot of effort into the efficiency

1:20:20.800 --> 1:20:21.480
 of our compute.

1:20:22.000 --> 1:20:27.600
 Um, and, and, uh, so there's actually a lot of work done by some

1:20:27.600 --> 1:20:31.880
 very talented software engineers at Tesla that, uh, at a very

1:20:31.880 --> 1:20:35.480
 foundational level to improve the efficiency of compute and how

1:20:35.480 --> 1:20:39.800
 we use the, the, the trip accelerators, uh, which are basically,

1:20:39.800 --> 1:20:45.240
 um, dot, you know, uh, doing matrix math dot, dot products, uh,

1:20:45.280 --> 1:20:46.560
 like a bazillion dot products.

1:20:47.120 --> 1:20:49.360
 And it's like, what, what, what, what are neural nets?

1:20:49.360 --> 1:20:52.880
 It's like computer wise, like 99% dot products.

1:20:54.120 --> 1:20:56.760
 So, you know, um,

1:20:56.840 --> 1:21:00.280
 And you want to achieve as many high frame rates like a video game.

1:21:00.360 --> 1:21:06.400
 You want full resolution, high frame rate, high frame rate, low latency,

1:21:06.400 --> 1:21:17.280
 um, low jitter, uh, so, um, I think one of the things for, um, moving

1:21:17.280 --> 1:21:25.600
 towards now is no post processing of the image through the, um, uh, the

1:21:25.600 --> 1:21:26.600
 image signal processor.

1:21:26.600 --> 1:21:32.960
 So, um, like for, for what happens for cameras is that almost all

1:21:32.960 --> 1:21:37.960
 cameras is they, um, there's a lot of post processing done in order

1:21:37.960 --> 1:21:39.240
 to make pictures look pretty.

1:21:40.200 --> 1:21:42.320
 Uh, and so we don't care about pictures looking pretty.

1:21:42.440 --> 1:21:44.560
 Um, we, we just want the data.

1:21:44.960 --> 1:21:47.920
 We, we, so we're, we're moving to just roll, roll photon counts.

1:21:48.720 --> 1:21:55.120
 So the system will, like the image that, that, that the computer sees is

1:21:55.120 --> 1:21:59.000
 actually much more than what you'd see if you're represented on a camera.

1:21:59.040 --> 1:22:00.040
 It's got much more data.

1:22:00.040 --> 1:22:02.840
 Uh, and even in very low light conditions, you can see that there's

1:22:02.840 --> 1:22:07.640
 a small photon count difference between, you know, the spot here and

1:22:07.640 --> 1:22:11.040
 that's about there, which means that, so it can see in the dark incredibly

1:22:11.040 --> 1:22:15.440
 well, um, because it can detect these tiny differences in photon counts.

1:22:16.440 --> 1:22:19.120
 Much better than you'd possibly imagine.

1:22:19.840 --> 1:22:25.920
 Um, so, and then we also save, uh, 13 milliseconds on a latency.

1:22:25.920 --> 1:22:30.720
 Uh, so, uh, from removing the post processing and the image.

1:22:30.720 --> 1:22:31.080
 Yes.

1:22:31.080 --> 1:22:31.320
 Yeah.

1:22:31.320 --> 1:22:36.720
 It's like, um, because we've got eight cameras and, and then there's, uh,

1:22:38.320 --> 1:22:42.640
 roughly, I don't know, one and a half milliseconds or so, maybe 1.6 milliseconds

1:22:42.640 --> 1:22:45.680
 of latency, um, for each camera.

1:22:45.680 --> 1:22:55.120
 And so it, like, um, going to just, uh, it basically bypassing the image processor.

1:22:55.120 --> 1:23:00.600
 Uh, gets us back 13 milliseconds of latency, which is important, um, and we

1:23:00.600 --> 1:23:05.360
 track latency all the way from, you know, photon hits the, the camera to, you

1:23:05.360 --> 1:23:08.720
 know, all the steps that it's got to go through to get, you know, go through the,

1:23:08.720 --> 1:23:13.920
 um, the various neural nets and the, the C code and, uh, and there's a

1:23:13.920 --> 1:23:15.560
 little bit of C plus plus there as well.

1:23:15.600 --> 1:23:21.280
 Um, well, I can maybe a lot, but it, the core stuff is the heavy duty computers

1:23:21.280 --> 1:23:27.760
 all in C, um, and, uh, and so, so we track that latency all the way to an

1:23:27.760 --> 1:23:33.520
 output command to the, um, drive unit to accelerate, uh, the brakes just to slow

1:23:33.520 --> 1:23:36.160
 down, steering your turn left or right.

1:23:36.160 --> 1:23:40.160
 Um, so, cause you got to output a command that's going to go to a controller

1:23:40.160 --> 1:23:43.680
 and like some of these controllers have an update frequency that's maybe, uh, 10

1:23:43.680 --> 1:23:45.680
 hertz or something like that, which is slow.

1:23:45.680 --> 1:23:48.080
 That's like, now you lose a hundred milliseconds potentially.

1:23:48.080 --> 1:23:55.360
 So, um, so then we want to update the, the drivers on the, like, say, steering and

1:23:55.360 --> 1:24:00.480
 braking control to have, um, more like, uh, 100 hertz instead of 10 hertz and you

1:24:00.480 --> 1:24:03.920
 got a 10 millisecond latency instead of a hundred milliseconds worst case latency.

1:24:03.920 --> 1:24:06.880
 And actually jitter is more of a challenge than, than, than latency.

1:24:06.880 --> 1:24:09.680
 Because latency is like, you can, you can, you can anticipate and predict, but if

1:24:09.680 --> 1:24:13.440
 you're, but if you've got a stack up of things going from the camera to the, to

1:24:13.440 --> 1:24:16.160
 the computer through, then you can, you can, you can, you can anticipate and

1:24:16.160 --> 1:24:19.360
 the computer through then a series of other computers.

1:24:19.360 --> 1:24:21.680
 And finally to an actuator on the car.

1:24:22.080 --> 1:24:27.200
 If you have a stack up of, uh, of tolerances of timing tolerances, then you

1:24:27.200 --> 1:24:29.680
 can have quite a variable latency, which is called jitter.

1:24:30.320 --> 1:24:35.840
 And, and that makes it a hard to, to, to anticipate exactly what, how you should

1:24:35.840 --> 1:24:37.440
 turn the car or accelerate.

1:24:37.440 --> 1:24:42.560
 Because, you know, if you've got maybe 150 to 200 milliseconds of jitter, then

1:24:42.560 --> 1:24:45.040
 you could be off by, you know, up to.2 seconds.

1:24:45.040 --> 1:24:46.720
 And this could make, this could make a big difference.

1:24:46.720 --> 1:24:51.760
 So you have to interpolate somehow to, to, to, to, uh, deal with the effects of jitter.

1:24:51.760 --> 1:24:56.080
 So you, you, you can make like robust control decisions.

1:24:57.440 --> 1:24:57.680
 Yeah.

1:24:57.680 --> 1:25:02.000
 Again, you have to, uh, so the jitter is in the sensor information or is it, the jitter

1:25:02.000 --> 1:25:04.320
 can occur at any stage in the pipeline.

1:25:04.320 --> 1:25:10.080
 You can, if you have just, if you have a fixed latency, you can anticipate, um, and,

1:25:10.080 --> 1:25:16.400
 and, and, uh, like say, okay, we know that, uh, our information is, for argument's sake,

1:25:16.400 --> 1:25:18.080
 150 milliseconds stale.

1:25:18.880 --> 1:25:24.240
 Like, so for, for, um, 140, for argument's sake, 150 milliseconds from photon second

1:25:24.240 --> 1:25:30.480
 camera to, um, where you can measure a change in the acceleration of the vehicle.

1:25:33.600 --> 1:25:38.400
 So then, uh, then you're going to say, okay, well, we're going to enter, we know it's

1:25:38.400 --> 1:25:43.600
 150 milliseconds, so we're going to take that into account and, uh, and compensate for that

1:25:43.600 --> 1:25:44.080
 latency.

1:25:44.080 --> 1:25:49.040
 However, if you, if you've got then 150 milliseconds of latency plus 100 milliseconds of jitter,

1:25:49.040 --> 1:25:52.080
 that's, which could be anywhere from zero to 100 milliseconds on top.

1:25:52.080 --> 1:25:55.600
 So, so then your latency could be from 150 to 250 milliseconds.

1:25:55.600 --> 1:25:59.680
 Now you've got 100 milliseconds that you don't know what to do with and, and that's basically random.

1:26:01.280 --> 1:26:03.200
 So getting rid of jitter is extremely important.

1:26:04.080 --> 1:26:07.280
 And that affects your control decisions and all those kinds of things.

1:26:07.280 --> 1:26:11.680
 Okay. Yeah, the, the cars is going to fundamentally maneuver better with lower jitter.

1:26:12.800 --> 1:26:13.120
 Got it.

1:26:13.120 --> 1:26:17.840
 And the, the, the, the cars will maneuver with superhuman ability and reaction time

1:26:17.840 --> 1:26:18.800
 much faster than a human.

1:26:20.240 --> 1:26:26.880
 I mean, I think over time the autopilot, full cell driving will be capable of maneuvers that,

1:26:26.880 --> 1:26:29.440
 um, you know, uh,

1:26:32.240 --> 1:26:36.240
 you know, are far more than what like James Bond could do in like the best movie type of thing.

1:26:36.240 --> 1:26:39.120
 That's exactly where I was imagining my mind as you said it.

1:26:40.080 --> 1:26:42.640
 It's like impossible maneuvers that a human couldn't do.

1:26:44.880 --> 1:26:50.080
 Well, let me ask sort of, uh, looking back the six years, looking out into the future,

1:26:50.080 --> 1:26:53.440
 based on your current understanding, how, how hard do you think this,

1:26:53.440 --> 1:26:58.400
 this full self driving problem, when do you think Tesla will solve level four FSD?

1:27:00.800 --> 1:27:03.120
 I mean, it's looking quite likely that it will be next year.

1:27:03.120 --> 1:27:09.760
 And what does the solution look like? Is it the current pool of FSD beta candidates?

1:27:11.200 --> 1:27:15.600
 They start getting greater and greater as they have been degrees of autonomy.

1:27:15.600 --> 1:27:21.360
 And then there's a certain level beyond which they can, they can do their own, they can read a book.

1:27:22.800 --> 1:27:27.600
 Yeah. So, uh, I mean, you can see that anybody who's been following the

1:27:27.600 --> 1:27:35.600
 full self driving beta closely will see that the, um, the rate of disengagement has been

1:27:35.600 --> 1:27:41.280
 dropping rapidly. So like disengagement be where, where the driver intervenes to prevent the car

1:27:41.280 --> 1:27:52.800
 from doing something dangerous potentially. So, um, so the interventions, you know, per million

1:27:52.800 --> 1:28:00.560
 miles has been dropping dramatically at some point the, and that trend looks like it happens next

1:28:00.560 --> 1:28:09.600
 year is the, the, the, the probability of an accident on FSD is less than that of the average

1:28:09.600 --> 1:28:16.480
 human and then, and then significantly less than that of the average human. Um, so it certainly

1:28:16.480 --> 1:28:23.840
 appears like we will get there next year. Um, then, then of course that, that, then there's

1:28:23.840 --> 1:28:27.360
 going to be a case of, okay, well, we now have to prove this to regulators and prove it to,

1:28:27.360 --> 1:28:31.920
 you know, and, and we, we, we want a standard that is not just equivalent to a human, but

1:28:32.720 --> 1:28:36.880
 uh, much better than the average human. I think it's got to be at least two or three times, uh,

1:28:36.880 --> 1:28:41.760
 higher safety than a human. So two or three times lower probability of injury than a human.

1:28:41.760 --> 1:28:46.400
 Um, before, before we would actually say like, okay, it's okay to go. It's not going to be a

1:28:46.400 --> 1:28:53.280
 cool, it's going to be much better. So if you look at 10 point FSD, 10.6 just came out recently,

1:28:53.280 --> 1:28:59.920
 10.7 is on the way, maybe 11 is on the way to where in the future. Yeah. Um, we were hoping

1:28:59.920 --> 1:29:06.640
 to get 11 out this year, but it's, uh, 11 actually has a whole bunch of, uh, fundamental

1:29:06.640 --> 1:29:14.080
 rewrites on the neural, neural net architecture, um, and, and some fundamental improvements, uh, in

1:29:15.280 --> 1:29:23.120
 creating vector space. Uh, so, uh, there is a, some fundamental like leap that really deserves

1:29:23.120 --> 1:29:30.240
 the 11. I mean, that's a pretty cool number. Yeah. Yeah. Uh, 11 would be a single stack for

1:29:30.240 --> 1:29:37.680
 all, you know, one stack to rule them all. Um, and, uh, but, but there, there's just some really

1:29:37.680 --> 1:29:47.040
 fundamental, uh, neural net architecture changes that are, that will allow for, uh, much more

1:29:47.040 --> 1:29:52.880
 capability, but, but, you know, at first they're going to have issues. So like we have this working

1:29:52.880 --> 1:30:00.160
 on like sort of alpha software and it's good, but it's, uh, it's, it's, it's, it's basically taking

1:30:00.160 --> 1:30:05.200
 a whole bunch of C C plus plus code and, and, and leading a massive amount of C plus plus code

1:30:05.200 --> 1:30:09.200
 and replacing it with the neural net. And you know, Andre, um, makes this point a lot, which

1:30:09.200 --> 1:30:13.200
 he's like neural nets, that kind of eating software, you know, over time there's like

1:30:14.160 --> 1:30:18.560
 less and less conventional software, more and more neural net, uh, which is still software, but it's,

1:30:18.560 --> 1:30:25.360
 you know, still comes out the lines of software, but, uh, it's more, more neural net stuff, uh, and

1:30:25.360 --> 1:30:35.360
 less, uh, you know, heuristics basically, um, if you're more, more, more, uh, matrix based

1:30:35.360 --> 1:30:49.200
 stuff, unless, uh, heuristics based stuff. Um, and, um, you know, like, like, like one of the big changes

1:30:49.200 --> 1:31:00.160
 will be, um, like right now the neural nets, uh, will, um, deliver a giant bag of points

1:31:00.160 --> 1:31:07.200
 to the C plus plus or C and C plus plus code. Yeah. Um, we call it the giant bag of points.

1:31:07.200 --> 1:31:12.960
 Yeah. Uh, and it's like, so you go to pixel and, and, and, and something associated with that pixel,

1:31:12.960 --> 1:31:18.320
 like this pixel is probably car, the pixel is probably lane line. Um, then you've got to

1:31:18.320 --> 1:31:27.520
 assemble this giant bag of points in the C code and turn it into, uh, vectors. Um, and, uh,

1:31:27.520 --> 1:31:34.080
 it does a pretty good job of it, but it's, it's, uh, it's, we want to just, we need another layer

1:31:34.080 --> 1:31:40.000
 of neural nets on top of that to take the, the giant bag of points and distill that down to

1:31:41.200 --> 1:31:46.400
 vector space in the, in the neural net part of the software as opposed to the heuristics

1:31:46.960 --> 1:31:52.560
 part of the software. This is a big improvement. Um, neural nets all the way down. That's what

1:31:52.560 --> 1:31:58.080
 you want. It's not even all neural nets, but it's, it's, it's, uh, this will be just a, this

1:31:58.080 --> 1:32:03.920
 is a game changer to not have the bag of points, the giant bag of points that has to be assembled

1:32:03.920 --> 1:32:10.560
 with, um, many lines of C C plus plus, uh, and, and have the, and have a neural net just

1:32:10.560 --> 1:32:18.480
 assemble those into vectors. So, so the, the neural net is outputting, um, much, much less

1:32:18.480 --> 1:32:23.200
 data. It's, it's, it's outputting this, this is a lane line. This is a curb. This is drivable

1:32:23.200 --> 1:32:29.120
 space. This is a car. This is, uh, you know, a pedestrian or cyclist or something like that.

1:32:29.120 --> 1:32:38.720
 It's outputting, um, it's really outputting, um, proper vectors to the, the C C plus plus control

1:32:38.720 --> 1:32:50.880
 control code as opposed to the sort of constructing the, the vectors, uh, in, in C. Um, we're done,

1:32:50.880 --> 1:32:56.480
 I think, quite a good job of, but it's, it's a, it's kind of hitting a local maximum on the,

1:32:56.480 --> 1:33:03.040
 how well the C can do this. Um, so this is, this is really, this is really a big deal. And, and

1:33:03.040 --> 1:33:07.040
 just all of the networks in the car need, need to move to surround video. There's still some

1:33:07.040 --> 1:33:13.920
 legacy networks that are not, uh, surround video. Um, and all of the training needs to move to

1:33:13.920 --> 1:33:18.880
 surround video and the efficiency of the training, uh, it needs to get better than it is. Uh, and

1:33:18.880 --> 1:33:27.840
 then we need to move everything to, uh, raw, uh, photon, uh, counts as opposed to, um, processed

1:33:27.840 --> 1:33:32.480
 images. Okay. It's just, it's just quite a big reset on the training because the system's trained

1:33:32.480 --> 1:33:39.440
 on post processed image images. So we need to redo all the training, uh, to train against

1:33:39.440 --> 1:33:45.280
 the, the raw photon counts instead of the post processed image. So ultimately it's kind of

1:33:45.280 --> 1:33:50.480
 reducing the complexity of the whole thing. So, uh, reducing, reducing the lines of code will

1:33:50.480 --> 1:33:56.000
 actually go, go lower. Yeah. That's fascinating. Um, so you're doing fusion of all the sensors

1:33:56.000 --> 1:33:59.920
 and reducing the complexity of having to deal with these cameras. There's a lot of cameras

1:33:59.920 --> 1:34:07.680
 really. Right. Yes. Um, same with humans. Uh, well, I guess we got years too. Okay. Yeah.

1:34:07.680 --> 1:34:12.400
 Well, we'll actually need to incorporate, um, sound as well. Um, cause you know, you need to like

1:34:12.400 --> 1:34:18.240
 listen for ambulance sirens or fire, you know, fire trucks, you know, uh, if somebody like

1:34:19.360 --> 1:34:22.400
 you know, yelling at you or something, I don't know, just that there's, there's a little bit of

1:34:22.400 --> 1:34:26.560
 audio that needs to be incorporated as well. Do you need to go back to break? Yeah, let's

1:34:26.560 --> 1:34:33.600
 just, let's take a break. Okay. Honestly, frankly, like the ideas are, are the easy thing and the

1:34:33.600 --> 1:34:37.600
 implementation is the hard thing. Like the idea of going to the moon is, is the easy part,

1:34:37.600 --> 1:34:42.160
 but going to the moon is the hard part. It's the hard part. Um, and there's a lot of like hardcore

1:34:42.160 --> 1:34:48.000
 engineering that's got to get done at the hardware and software level. Uh, likes optimizing the

1:34:48.000 --> 1:34:57.440
 C compiler and, uh, just, you know, uh, cutting out latency everywhere. Like this is, if we don't

1:34:57.440 --> 1:35:02.880
 do this, the system will not work properly. Um, so the work of the engineers doing this,

1:35:02.880 --> 1:35:07.840
 they are like the unsung heroes to some, you know, but they are critical to the success of the

1:35:07.840 --> 1:35:11.680
 situation. I think he made it clear. I mean, at least to me, it's super exciting. Everything

1:35:11.680 --> 1:35:16.800
 that's going on outside of what Andre is doing. Yeah. Just the whole infrastructure, the software.

1:35:16.800 --> 1:35:21.040
 I mean, everything is going on with data engine, uh, whatever, whatever it's called,

1:35:21.600 --> 1:35:24.480
 the whole process is, is just work of art to me.

1:35:24.480 --> 1:35:27.200
 Yeah. I think the, the, the sure scale of it is, is boggles mind. Like the training,

1:35:27.200 --> 1:35:31.600
 the amount of work done with the, like we've written all this custom software for training

1:35:31.600 --> 1:35:38.800
 and labeling, um, and to do auto labeling, auto labeling is essential. Um, because especially

1:35:38.800 --> 1:35:44.960
 when you've got like surround video, it's very difficult to like label surround video from scratch

1:35:44.960 --> 1:35:51.760
 is extremely difficult. Um, like take a human's such a long time to even label one video clip

1:35:51.760 --> 1:35:58.160
 like several hours, uh, or the order label it, uh, basically we're just apply a heavy, like heavy duty,

1:35:59.040 --> 1:36:06.800
 uh, like a lot of compute to the, to the video clips, um, to pre assign and guess what all the

1:36:06.800 --> 1:36:10.240
 things are that are going on in this round video. And then there's like correcting it.

1:36:10.240 --> 1:36:14.800
 Yeah. And then all the human has to do is like tweet, like say the, you know, change, adjust

1:36:14.800 --> 1:36:20.080
 what is incorrect. This, this is like increase, increase this productivity by effect a hundred

1:36:20.080 --> 1:36:25.440
 or more. Yeah. Uh, so you've presented Tesla bot as primarily useful in the factory. First of all,

1:36:25.440 --> 1:36:32.240
 I think human robots are incredible from a fan of robotics. I think, uh, the elegance of movement

1:36:32.240 --> 1:36:39.360
 that human, um, the human robots that by Peter robots show are just so cool. So it's, uh, really

1:36:39.360 --> 1:36:43.440
 interesting that you're working on this and also talking about applying the same kind of all the

1:36:43.440 --> 1:36:47.520
 ideas of some of which you've talked about with data engine, all the things that we're talking

1:36:47.520 --> 1:36:53.520
 about with Tesla autopilot, just, uh, transferring that over to the, just yet another robotics problem.

1:36:54.400 --> 1:36:59.040
 I have to ask, since I care about human robot interaction, so the human side of that,

1:36:59.040 --> 1:37:04.400
 so you've talked about mostly in the factory. Do you see it, uh, also, do you see part of this

1:37:04.400 --> 1:37:08.720
 problem that Tesla bot has to solve as interacting with humans and potentially having a place,

1:37:08.720 --> 1:37:15.040
 like in the home. So interacting, not just not replacing labor, but also like, I don't know,

1:37:15.600 --> 1:37:22.160
 being a friend or an assistant. Yeah. Yeah. I think the, the possibilities are, you know, endless.

1:37:27.040 --> 1:37:32.240
 Yeah. I mean, it's, it's, it's obviously like a, it's not quite in Tesla's primary mission

1:37:32.240 --> 1:37:37.680
 direction of accelerating sustainable energy, but, uh, it is a, an extremely useful thing

1:37:37.680 --> 1:37:42.800
 that we can do for the world, which is to make a useful humanoid robot. Um, that is capable of

1:37:43.360 --> 1:37:49.920
 interacting with the world and, um, helping in, in many different ways. Uh, so,

1:37:51.120 --> 1:37:57.440
 certainly in factories and really just, just, I mean, I think if you say like, uh, extrapolate

1:37:57.440 --> 1:38:03.760
 to, you know, many years in the future, it's like, I think, uh, work will become optional.

1:38:03.760 --> 1:38:10.400
 Yeah. So like there's a lot of jobs that if you, if you, if people weren't paid to do it,

1:38:10.400 --> 1:38:14.400
 they, they wouldn't do it. Like it's not, it's not fun, you know, necessarily. Like

1:38:15.040 --> 1:38:19.520
 if you're washing dishes all day, it's like, uh, you know, even if you really like washing dishes,

1:38:19.520 --> 1:38:26.240
 you really want to do it for eight hours a day every day. Probably not. So, um, and then there's

1:38:26.240 --> 1:38:31.440
 like dangerous work and basically if it's dangerous, boring, uh, it has like potential

1:38:31.440 --> 1:38:35.760
 for repetitive stress injury, injury, that kind of thing. Um, then that's really where

1:38:36.480 --> 1:38:44.560
 humanoid robots would add the most value initially. Um, so that's what we're aiming for is, is to, um,

1:38:45.840 --> 1:38:49.920
 for, for the humanoid robots to do jobs that people don't, don't voluntarily want to do.

1:38:50.880 --> 1:38:55.680
 Um, and then we'll have to pair that obviously with some kind of universal basic income in the

1:38:55.680 --> 1:39:04.320
 future. Uh, so I think, um, do you see a world when there's like hundreds of millions of Tesla

1:39:04.320 --> 1:39:09.520
 bots doing different performing different tasks throughout the world?

1:39:12.160 --> 1:39:15.440
 Yeah. I haven't really thought about it that far into the future, but I guess that there may be

1:39:15.440 --> 1:39:24.560
 something like that. Um, so I guess it's a wild question. So the, the number of Tesla cars has

1:39:24.560 --> 1:39:30.320
 been accelerating. It's been close to 2 million produced. Many of them have autopilot. I think

1:39:30.320 --> 1:39:34.720
 we're over 2 million now. Yeah. Do you think there will ever be a time when there'll be more Tesla

1:39:34.720 --> 1:39:44.400
 bots than Tesla cars? Yeah. I, I, I, you know, actually it's funny you ask this question because

1:39:44.400 --> 1:39:48.000
 normally I do try to think I'm pretty far into the future, but I haven't really thought that far

1:39:48.000 --> 1:39:56.640
 into the future with the, with the Tesla bot or it's code named Optimus. I call it Optimus subprime

1:39:59.120 --> 1:40:05.760
 because it's not, it's not like a giant, you know, transformer robot. Um, so, uh,

1:40:06.880 --> 1:40:10.640
 but it's meant to be a general purpose helpful, helpful bot. Um,

1:40:10.640 --> 1:40:21.360
 um, and, and basically like the things that we're basically like, like Tesla, I think, um,

1:40:21.360 --> 1:40:26.240
 is the, has the most advanced real world AI, uh, for interacting with the real world,

1:40:26.240 --> 1:40:32.880
 which you developed as a function of to, to make self driving work. Um, and so along with custom

1:40:32.880 --> 1:40:39.520
 hardware and like a lot of, you know, uh, hardcore low level software to have it run efficiently and

1:40:39.520 --> 1:40:43.840
 be, you know, power efficient because, you know, it's one thing to do neural nets. If you've got a

1:40:43.840 --> 1:40:48.240
 gigantic solar room with 10,000 computers, but now let's say you just, you have to now distill

1:40:48.240 --> 1:40:53.360
 that down into one computer that's running at low power in a humanoid robot or a car. Um,

1:40:53.360 --> 1:40:57.280
 that's actually very difficult and a lot of hardcore software is required for that. Um,

1:40:58.960 --> 1:41:06.480
 so, so since we're kind of like solving the navigate the real world with neural nets problem

1:41:06.480 --> 1:41:12.080
 for cars, which are kind of robots with four wheels, then it's like kind of a natural extension

1:41:12.080 --> 1:41:20.560
 of that is to put it in a robot with arms and legs, uh, and actually, you know, actuators. Um, so,

1:41:23.200 --> 1:41:30.560
 um, like, like the, the, the two, like hard things are like, you basically need to make the,

1:41:30.560 --> 1:41:36.160
 have the robot be intelligent enough to interact in a sensible way with the environment. Um,

1:41:36.880 --> 1:41:43.360
 so you should need real, real world AI and you need to be very good at, um, manufacturing,

1:41:43.360 --> 1:41:49.360
 which is a very hard problem. Tesla is very good at manufacturing and also has the real world AI.

1:41:49.360 --> 1:42:00.480
 So making the humanoid robot work is, uh, basically means developing custom, uh, motors and sensors,

1:42:00.480 --> 1:42:06.320
 uh, that, that are different for what a car would use. Um, but we, we're also, we have a, um,

1:42:07.920 --> 1:42:13.840
 I think we have the, the, the best expertise in developing advanced electric motors and

1:42:13.840 --> 1:42:19.920
 power electronics. So it just has to be for a humanoid robot application on a car.

1:42:22.160 --> 1:42:28.480
 Still, you do talk about love sometimes. So let me ask, this isn't like for like sex robots

1:42:28.480 --> 1:42:35.680
 or something like that. Love is the answer. Yes. Uh, there is something compelling to us,

1:42:35.680 --> 1:42:40.480
 not compelling, but we connect with, um, humanoid robots or even legged robots,

1:42:40.480 --> 1:42:46.240
 like with the dog and shapes of dogs. It just, it seems like, you know, there's a huge amount

1:42:46.240 --> 1:42:51.200
 of loneliness in this world. All of us seek companionship and with other humans, friendship

1:42:51.200 --> 1:42:55.040
 and all those kinds of things. We have a lot of here in Austin, a lot of people have dogs.

1:42:55.840 --> 1:43:00.800
 That's right. Um, there seems to be a huge opportunity to also have robots that decrease

1:43:01.840 --> 1:43:08.480
 the, uh, the, the amount of loneliness in the world or help us humans connect with each,

1:43:08.480 --> 1:43:13.600
 with each other. So in the way that dogs can, um, do you think about that?

1:43:13.600 --> 1:43:19.520
 We'll test about it all. Or is it really focused on the problem of, of performing specific tasks,

1:43:19.520 --> 1:43:25.920
 not connecting with humans? Um, I mean, to be, to be honest, I have not actually thought about it

1:43:25.920 --> 1:43:30.960
 from the companionship standpoint, but I think it actually would end up being, it could be actually

1:43:30.960 --> 1:43:42.160
 a very good companion. Um, and it could, you develop like a personality, uh, over time that is,

1:43:42.160 --> 1:43:47.280
 that is like unique. Like, uh, you know, it's not like they're just all the robots are the same.

1:43:47.280 --> 1:43:56.000
 And that personality could evolve to be, you know, uh, match, match the, the, the owner or the,

1:43:56.000 --> 1:44:02.880
 you know, yes, the owner, uh, well, uh, whatever you want to call it, uh, the other companion,

1:44:02.880 --> 1:44:08.720
 the other half, right? Uh, in the same way that friends do. See, I think that's a huge opportunity.

1:44:08.720 --> 1:44:16.560
 I think, yeah, no, that's interesting. Like, um, the, because, you know, like there's, uh,

1:44:16.560 --> 1:44:21.360
 Japanese phrase, I like the, uh, Wabi Savi, you know, uh, the subtle imperfections

1:44:21.360 --> 1:44:27.280
 are what makes something special. And the subtle imperfections of the personality of the robot

1:44:27.920 --> 1:44:35.040
 mapped to the subtle imperfections of the robot's human friend, I don't know,

1:44:35.040 --> 1:44:41.360
 owner sounds like maybe the wrong word, but, um, could actually make an incredible buddy,

1:44:41.360 --> 1:44:46.320
 basically. And in that way, the imperfections, like R2D2 or like a C3PO sort of thing, you know.

1:44:46.320 --> 1:44:53.840
 So from a machine learning perspective, I think the flaws being a feature is really nice.

1:44:53.840 --> 1:44:58.800
 You could be quite terrible at being a robot for quite a while in the general home environment

1:44:58.800 --> 1:45:04.560
 or all in the general world. And that's kind of adorable. And that's like, those are your flaws

1:45:04.560 --> 1:45:09.280
 and you fall in love with those flaws. So it's, it's a, it's very different than autonomous

1:45:09.280 --> 1:45:14.800
 driving where it's a very high stakes environment. You cannot mess up. And so it's, yeah, it's more

1:45:14.800 --> 1:45:22.000
 fun to be a robot in the home. Yeah. In fact, if you think of like a C3PO and R2D2, like they

1:45:22.000 --> 1:45:26.480
 actually had a lot of like flaws and imperfections and silly things and they would argue with each

1:45:26.480 --> 1:45:32.960
 other. And, um, were they actually good at doing anything? I'm not exactly sure.

1:45:33.760 --> 1:45:40.480
 I definitely added a lot to the story. Um, but, but, but there's, there's sort of quirky elements

1:45:40.480 --> 1:45:47.040
 and, you know, that they would like make mistakes and do things. Like it was like, uh, it made them

1:45:48.480 --> 1:45:55.280
 relatable, I don't know, um, enduring. So, so yeah, I think that that could be something

1:45:55.280 --> 1:46:03.280
 that probably would happen. Um, but our initial focus is just to make it useful. Uh, so, so,

1:46:04.240 --> 1:46:08.400
 um, I'm confident we'll get it done. I'm not sure what the exact timeframe is, but uh,

1:46:08.400 --> 1:46:13.920
 like we'll probably have, I don't know, a decent prototype towards the end of next year or something

1:46:13.920 --> 1:46:21.520
 like that. And it's cool that it's connected to Tesla, the car. So, so yeah, it's, it's, it's using

1:46:21.520 --> 1:46:27.360
 a lot of, you know, it would use the autopilot inference computer and, um, a lot of the training

1:46:27.360 --> 1:46:32.800
 that we've done for the four cars in terms of recognizing real world things could be applied

1:46:32.800 --> 1:46:40.160
 directly to the, to the robot. Um, so it, but, but there's, there's a lot of custom actuators

1:46:40.160 --> 1:46:44.800
 and sensors that need to be developed. And an extra module on top of the vector space,

1:46:45.600 --> 1:46:52.080
 uh, for love. Uh, yeah. That's amazing. Okay. We can add that to the car too.

1:46:53.120 --> 1:46:58.240
 That's true. Um, that could be useful in all environments. Like you said, a lot of people

1:46:58.240 --> 1:47:03.440
 argue in the car. So maybe we can help them out. Uh, you're a student of history,

1:47:03.440 --> 1:47:08.400
 fan of Dan Carlin's hardcore history podcast. Yeah, that's great. Greatest podcast ever.

1:47:08.400 --> 1:47:14.960
 Yeah. I think it is actually, it almost doesn't really count as a podcast. Yeah. It's more like

1:47:14.960 --> 1:47:19.920
 a audio book. Yeah. So you were on the podcast with Dan, I just had a chat with him about it.

1:47:20.960 --> 1:47:25.120
 He said, you guys want military and all that kind of stuff. Uh, yeah, it's literally, uh, it was

1:47:25.120 --> 1:47:33.440
 basically, um, uh, I think it should be titled engineer wars. Uh, essentially like, like when

1:47:33.440 --> 1:47:40.160
 there's a rapid change in the rate of technology, then, uh, engineering plays a pivotal role in,

1:47:40.160 --> 1:47:46.080
 in victory and battle. Um, do you get, how far back in history did you go? Did you go World War

1:47:46.080 --> 1:47:53.360
 II? Uh, it was mostly, well, it was supposed to be a deep dive on fighters and bomber, uh,

1:47:53.360 --> 1:47:57.760
 technology in World War II. Um, but that ended up being more wide ranging than that. Um,

1:47:58.400 --> 1:48:03.600
 because I just went down the, a total rathole of like studying all of the fighters and bombers

1:48:03.600 --> 1:48:09.680
 of World War II and like the constant rock, paper, scissors game that like, you know, uh,

1:48:09.680 --> 1:48:12.800
 one country would make this plane, then it'd make a plane to beat that and that's what I'm

1:48:12.800 --> 1:48:16.960
 trying to make a plane to beat that. And then the, and really what matters is like the pace of

1:48:16.960 --> 1:48:25.840
 innovation, um, and also access to high quality, uh, fuel and, uh, raw materials. So like Germany

1:48:25.840 --> 1:48:30.160
 had like some amazing designs, but they couldn't make them, uh, because they couldn't get their

1:48:30.160 --> 1:48:36.560
 raw materials. Uh, and, uh, they, they had a real problem with the oil and, and, and, uh, fuel

1:48:36.560 --> 1:48:41.440
 basically the fuel quality was extremely, uh, variable. So the design wasn't the bottleneck

1:48:41.440 --> 1:48:47.120
 because, uh, yeah, like the US had kick ass fuel, uh, that was like very consistent. Like the

1:48:47.120 --> 1:48:50.960
 problem is if you make a very high performance aircraft engine, um, in order to make high

1:48:50.960 --> 1:49:00.560
 performance, you have to, um, the, the, the, the, the fuel, the aviation gas, uh, has to be a consistent

1:49:00.560 --> 1:49:08.800
 mixture and, uh, uh, it has to have a high octane. Um, like high octane is the most important thing,

1:49:08.800 --> 1:49:13.040
 but also can't have like impurities and stuff, uh, because you'll, you'll foul up the engine

1:49:13.920 --> 1:49:17.520
 and, and, and Germany just never had good access to oil. Like they try to get it by invading the

1:49:17.520 --> 1:49:23.200
 Caucasus, um, but that didn't work too well. Never works well.

1:49:26.240 --> 1:49:30.400
 That's, that's for you. So there was, Germany was always struggling with, with basically shitty

1:49:30.400 --> 1:49:36.080
 oil. Um, and then they could not, uh, they, they couldn't count on a, on high quality fuel for

1:49:36.080 --> 1:49:44.000
 their aircraft. So then they had to have all these additives and stuff. Uh, so, um, uh, whereas the

1:49:44.000 --> 1:49:50.400
 US had awesome fuel, um, and that provided that to Britain as well. Um, so that allowed the British

1:49:50.400 --> 1:49:55.200
 and the Americans to design aircraft engines that were, uh, super high performance, better than

1:49:55.200 --> 1:50:00.320
 anything else in the world. Germany could, could, could design the engines. They just didn't have

1:50:00.320 --> 1:50:05.680
 the fuel. Uh, and then also the, like I said, the, the, uh, the quality of the aluminum allies that

1:50:05.680 --> 1:50:08.320
 they were getting was also not that great. And so, you know,

1:50:08.320 --> 1:50:11.360
 is your, is this like, uh, do you talk about all this with them?

1:50:11.360 --> 1:50:18.400
 Yeah. Awesome. Broadly looking at history, when you look at Genghis Khan, when you look at Stalin,

1:50:18.400 --> 1:50:23.760
 Hitler, the darkest moments of human history, uh, what do you take away from those moments?

1:50:23.760 --> 1:50:27.120
 Does it help you gain insight about human nature, about human behavior today,

1:50:27.120 --> 1:50:36.080
 whether it's the wars or the individuals or just the behavior of people, any aspects of history?

1:50:40.960 --> 1:50:44.240
 Yeah, I find history fascinating. Um,

1:50:49.360 --> 1:50:54.560
 um, there's just a lot of incredible things that have been done, good and bad, um, that they

1:50:54.560 --> 1:51:04.560
 help, you just help you understand the nature of civilization, um, and individuals and

1:51:06.080 --> 1:51:10.320
 Does it make you sad that humans do these kinds of things to each other? You look at the 20th

1:51:10.320 --> 1:51:19.200
 century, World War II, the cruelty, the abuse of power, talk about communism, Marxism and Stalin.

1:51:19.200 --> 1:51:24.320
 Um, I mean, some of these things do, I mean, if you, like there's a lot of human history,

1:51:24.320 --> 1:51:29.760
 um, most of it is actually people just getting on with their lives. Uh, you know, and it's not like

1:51:30.560 --> 1:51:37.440
 human history is just, uh, what nonstop war and disaster is, those are actually just

1:51:37.440 --> 1:51:42.960
 those are intermittent and rare. And if they weren't, then, you know, humans would soon cease to exist.

1:51:46.400 --> 1:51:51.600
 Uh, but it's just that wars tend to be written about a lot. And whereas, like, uh,

1:51:51.600 --> 1:51:57.440
 uh, something being like, well, a normal year where nothing major happened was doesn't get

1:51:57.440 --> 1:52:02.720
 written about much, but that's, you know, most people just like farming and kind of like living

1:52:02.720 --> 1:52:10.480
 their life, you know, um, being a villager somewhere. Um, and every now and again, there's a war

1:52:10.480 --> 1:52:21.520
 and a thing. So, um, and, um, you know, I'd say like that there aren't very many books that I,

1:52:21.520 --> 1:52:28.480
 where I just had to start reading because it was just too, too dark. But, uh, the book about Stalin,

1:52:28.480 --> 1:52:36.160
 the court of the red czar, I had to start reading. It was just too, too dark, rough.

1:52:36.160 --> 1:52:46.240
 Yeah. Um, the thirties, uh, there's a lot, a lot of lessons there to me in particular that it feels

1:52:46.240 --> 1:52:54.240
 like humans, like all of us have that as the old soldiers in line, um, that the line between good

1:52:54.240 --> 1:52:58.720
 and evil runs to the heart of every man that all of us are capable of evil. All of us are capable

1:52:58.720 --> 1:53:06.400
 of good. It's almost like this kind of responsibility that, um, all of us have to, to, to tend towards

1:53:06.400 --> 1:53:12.080
 the good. And so like to me, looking at history is almost like an example of, look, you have some

1:53:12.080 --> 1:53:20.080
 charismatic leader that, uh, convinces you of things is too easy based on that story to do evil

1:53:20.080 --> 1:53:25.280
 onto each other, onto your family and to others. And so it's like our responsibility to do good.

1:53:25.280 --> 1:53:31.200
 Um, it's not like now is somehow different from history. That can happen again. All of it can

1:53:31.200 --> 1:53:37.520
 happen again. And yes, most of the time you're right. I mean, the optimistic view here is mostly

1:53:37.520 --> 1:53:44.240
 people are just living life. And as you've often memed about, uh, the quality of life was way worse

1:53:44.240 --> 1:53:48.640
 back in the day and keeps improving over time through innovation to technology.

1:53:48.640 --> 1:53:57.760
 But still it's somehow notable that these blimps of atrocities happen. Sure. Yeah. I mean life was

1:53:57.760 --> 1:54:06.880
 really tough for most of history. Um, I mean, for most of human history, um, a good year would be

1:54:06.880 --> 1:54:13.440
 one where not that many people in your village died of the plague, starvation, freezing to death,

1:54:13.440 --> 1:54:18.080
 or being killed by a neighboring village. It's like, well, it wasn't that bad. You know, it was

1:54:18.080 --> 1:54:23.440
 only like, you know, we lost 5% this year. That was, uh, it was a good year. You know, that would

1:54:23.440 --> 1:54:28.560
 be part of the course. Like just, just not starving to death would have been like the primary goal

1:54:28.560 --> 1:54:33.120
 of most people in through throughout history is making sure we'll have no foods last for the

1:54:33.120 --> 1:54:42.880
 winter and not get, not freeze or whatever. So, um, now food is, is plentiful. I have an obesity

1:54:42.880 --> 1:54:49.520
 problem. Um, you know, so. Well, yeah. The lesson there is to be grateful for the way things are

1:54:49.520 --> 1:54:58.080
 now for, for some of us. We've spoken about this offline. I'd love to get your thought about it here.

1:54:59.760 --> 1:55:05.120
 If I sat down for a long form in person conversation with the president of Russia,

1:55:05.120 --> 1:55:12.000
 Vladimir Putin, would you potentially want to call in for a few minutes to join in on a conversation

1:55:12.000 --> 1:55:17.040
 with a moderated translated by me? Sure. Yeah. Sure. I'd be happy to do that.

1:55:19.440 --> 1:55:23.840
 You've shown interest in the Russian language. Is this grounded in your interest in history

1:55:23.840 --> 1:55:30.880
 of linguistics, culture, general curiosity? I think it sounds cool. Sounds cool. Now it looks cool.

1:55:32.480 --> 1:55:37.280
 Well, it's, it's, you know, it's, it's a, it's, it takes a moment to read Cyrillic.

1:55:37.280 --> 1:55:45.440
 Once you know what the Cyrillic characters stand for, actually, then reading Russian

1:55:45.440 --> 1:55:49.200
 becomes a lot easier because there are a lot of words that are actually the same.

1:55:49.200 --> 1:55:58.640
 Like bank is bank. So find the words that are exactly the same and now you start to understand

1:55:58.640 --> 1:56:05.760
 Cyrillic. Yeah. If you can, if you can sound it out, it's much, there's at least some commonality

1:56:05.760 --> 1:56:12.800
 of words. What about the culture? You, you love great engineering, physics. There's a tradition

1:56:12.800 --> 1:56:18.160
 of the sciences there. Sure. You look at the 20th century from rocketry. So, you know,

1:56:18.160 --> 1:56:22.480
 some of the greatest rockets of the space exploration has been done in the Soviet and the

1:56:22.480 --> 1:56:28.720
 former Soviet Union. Yeah. So do you draw inspiration from that history? Just how this

1:56:28.720 --> 1:56:32.480
 culture that in many ways, I mean, one of the sad things is because of the language,

1:56:32.480 --> 1:56:37.440
 which a lot of it is lost to history because it's not translated to all those kinds of,

1:56:37.440 --> 1:56:43.040
 because it, it is in some ways an isolated culture. It flourishes within its, within its borders.

1:56:44.400 --> 1:56:50.480
 Yeah. So do you draw inspiration from those folks from, from the history of science engineering

1:56:50.480 --> 1:57:00.480
 there? I mean, the Soviet Union, Russia, and Ukraine as well, and have a really strong history

1:57:00.480 --> 1:57:05.280
 in spaceflight. Like some of the most advanced and impressive things in history were done,

1:57:07.760 --> 1:57:16.880
 you know, by the Soviet Union. So one can, cannot help but admire the

1:57:17.840 --> 1:57:23.680
 impressive rocket technology that was developed. You know, after the sort of full Soviet Union,

1:57:23.680 --> 1:57:30.960
 the, there's, there's much less that, that, than happened. But

1:57:33.360 --> 1:57:36.080
 still things are happening, but it's not, not quite at the

1:57:37.200 --> 1:57:42.160
 frenetic pace that was happening before the Soviet Union kind of

1:57:43.840 --> 1:57:49.680
 dissolved into separate republics. Yeah. I mean, I, I, you know, there's Roscosmos,

1:57:49.680 --> 1:57:56.960
 the Russian agency. I look forward to a time when those countries with China are working together,

1:57:57.840 --> 1:58:01.760
 the United States are all working together. Maybe a little bit of friendly competition, but

1:58:01.760 --> 1:58:06.960
 like friendly competition is good. You know, government's so slow, and the only thing slower

1:58:06.960 --> 1:58:14.720
 than one government is a collection of governments. So the Olympics would be boring if everyone just

1:58:14.720 --> 1:58:19.760
 crossed the finishing line at the same time. Yeah. Nobody would watch. Yeah. And, and people wouldn't

1:58:19.760 --> 1:58:24.480
 try hard to run fast and stuff. So I think friendly competition is a good thing.

1:58:26.400 --> 1:58:30.720
 This is also a good place to give a shout out to a video titled the entire Soviet rocket engine

1:58:30.720 --> 1:58:36.240
 family tree by Tim Dodd, aka everyday astronaut. It's like an hour and a half. It gives a full

1:58:36.240 --> 1:58:41.360
 history of Soviet rockets. And people should definitely go check on support Tim in general.

1:58:41.360 --> 1:58:46.480
 That guy's super excited about the future, super excited about a spaceflight. Every time I see

1:58:46.480 --> 1:58:50.480
 anything by him, I just have a stupid smile on my face because he's so excited about stuff.

1:58:51.120 --> 1:58:55.280
 Yeah. Tim Dodd is really, really great. If you're interested in anything to do with space,

1:58:56.160 --> 1:59:02.560
 he's, in terms of explaining rocket technology to your average person, he's awesome. The best,

1:59:02.560 --> 1:59:12.400
 I'd say. And I should say like the part of the reason like I switched us from, like Raptor at

1:59:12.400 --> 1:59:17.760
 one point was going to be a hydrogen engine. But hydrogen has a lot of challenges. It's very low

1:59:17.760 --> 1:59:22.560
 density. It's a deep cryogen. So it's only liquid at a very, you know, very close to absolute zero

1:59:23.200 --> 1:59:28.560
 requires a lot of insulation. It's, so it is a lot of challenges there.

1:59:28.560 --> 1:59:35.920
 And I was actually reading a bit about Russian rocket engine developments. And

1:59:37.440 --> 1:59:43.840
 at least the impression I had was that Soviet Union Russia and Ukraine primarily were

1:59:45.520 --> 1:59:52.320
 actually in the process of switching to methalox. And there was some interesting

1:59:52.320 --> 2:00:00.240
 test and data for ISP, like they were able to get like up to like a 382nd ISP with the

2:00:00.240 --> 2:00:05.520
 methalox engine. And I was like, well, okay, that's, that's actually really impressive. So

2:00:08.640 --> 2:00:15.360
 so I think we could, you could actually get a much lower cost, like in optimizing cost per

2:00:15.360 --> 2:00:24.000
 time to orbit cost per time to Mars. It's, I think methane oxygen is the way to go.

2:00:25.040 --> 2:00:31.200
 And I was partly inspired by the Russian work on the test stands with methalox engines.

2:00:32.560 --> 2:00:38.560
 And now for something completely different. Do you mind doing a bit of a meme review in the

2:00:38.560 --> 2:00:44.080
 spirit of the great, the powerful beauty pie, let's say one to 11, just go over a few documents

2:00:44.080 --> 2:00:52.320
 printed out. We can try. Let's try this. I present to you document number Uno.

2:00:56.320 --> 2:01:01.920
 I don't know. Okay. Vlad the Impaler discovers marshmallows.

2:01:03.840 --> 2:01:04.560
 Yeah, that's not bad.

2:01:04.560 --> 2:01:13.440
 So you get it because he's failing things. I don't know, three, whatever.

2:01:14.480 --> 2:01:15.360
 That's not very good.

2:01:19.040 --> 2:01:23.120
 This is grounded in some engineering, some history.

2:01:28.560 --> 2:01:30.000
 Yeah, give us an eight out of 10.

2:01:31.040 --> 2:01:32.480
 What do you think about nuclear power?

2:01:32.480 --> 2:01:40.800
 I'm in favor of nuclear power. I think it's in a place that is not subject to extreme natural

2:01:40.800 --> 2:01:46.160
 disasters. I think it's a nuclear power is a great way to generate electricity.

2:01:47.840 --> 2:01:50.400
 I don't think we should be shutting down nuclear power stations.

2:01:51.680 --> 2:01:52.880
 Yeah, but what about Chernobyl?

2:01:52.880 --> 2:02:02.240
 Exactly. So I think people, there's like a lot of fear of radiation and stuff.

2:02:04.160 --> 2:02:07.440
 And it's, I guess, probably like a lot of people just don't

2:02:09.440 --> 2:02:15.280
 even study engineering or physics. It's just the word radiation just sounds scary.

2:02:15.280 --> 2:02:22.000
 You know, so they don't, they can't calibrate what radiation means. But radiation is much

2:02:22.000 --> 2:02:35.840
 less dangerous than you think. So like, for example, Fukushima, when the Fukushima problem

2:02:35.840 --> 2:02:44.240
 happened due to the tsunami, I got people in California asking me if they should worry about

2:02:44.240 --> 2:02:52.080
 radiation from Fukushima. And I'm like, definitely not, not even slightly, not at all. That is crazy.

2:02:54.000 --> 2:03:04.960
 And just to show, like, look, this is how, like, the danger is so much overplayed compared to what

2:03:04.960 --> 2:03:14.160
 it really is that I actually flew to Fukushima and I donated a solar power system for what

2:03:14.160 --> 2:03:25.760
 a treatment plant. And, and I made a point of eating locally grown vegetables on TV in Fukushima.

2:03:28.640 --> 2:03:30.720
 Like, I'm still alive. Okay.

2:03:31.360 --> 2:03:36.000
 So it's not even at the risk of these events as low, but the impact of them is.

2:03:36.000 --> 2:03:39.360
 Impact is greatly exaggerated. It's just human nature.

2:03:39.920 --> 2:03:43.280
 It's people who don't know what radiation is, like I've had people ask me, like, what about

2:03:43.280 --> 2:03:47.600
 radiation from cell phones, quoting, causing brain cancer? I'm like, when you say radiation,

2:03:47.600 --> 2:03:51.440
 do you mean photons or particles than like that? I don't know what, what do you mean

2:03:51.440 --> 2:03:58.240
 photons particles? So do you mean, let's say photons? What, what, what frequency,

2:03:58.240 --> 2:04:03.760
 wavelength? And they're like, no idea. Like, do you know that everything's radiating all the time?

2:04:04.720 --> 2:04:07.440
 Like, what do you mean? Like, yeah, everything's radiating all the time.

2:04:07.440 --> 2:04:12.560
 Photons are being emitted by, by all objects all the time, basically. So,

2:04:14.320 --> 2:04:19.360
 and if you want to know what it's, it's what, what it means to stand in front of nuclear fire,

2:04:20.080 --> 2:04:27.280
 go outside. The sun is a gigantic, you know, thermonuclear reactor that you're staring

2:04:27.280 --> 2:04:30.480
 right at it. Are you still alive? Yes. Okay. Amazing.

2:04:30.480 --> 2:04:37.840
 Yeah. I guess radiation is one of the words that could be used as a tool to, to, to fear

2:04:37.840 --> 2:04:42.400
 monger by certain people. That's it. And I think people just don't, don't understand. So, I mean,

2:04:42.400 --> 2:04:45.920
 that's the way to fight that, that fear, I suppose, is to understand, is to learn.

2:04:46.480 --> 2:04:50.640
 Yeah. Just say like, okay, how many people have actually died from nuclear accidents? It's like

2:04:50.640 --> 2:04:57.360
 practically nothing. And say how many people have, have died from, you know, coal plants? And

2:04:57.360 --> 2:05:04.320
 it's a very big number. So like, obviously, we should not be starting up coal plants and shutting

2:05:04.320 --> 2:05:09.280
 down nuclear plants. It just doesn't make any sense at all. Coal plants, like, I don't know,

2:05:09.280 --> 2:05:13.760
 a hundred to a thousand times worse for, for health than nuclear power plants.

2:05:15.040 --> 2:05:17.440
 You want to go to the next one? This is really bad.

2:05:17.440 --> 2:05:27.280
 So that 90, 180 and 360 degrees, everybody loves the math, nobody gives a shit about 270.

2:05:27.840 --> 2:05:35.360
 It's not super funny. I don't like 203. Yeah. This is not, you know, LOL situation.

2:05:37.360 --> 2:05:37.680
 Yeah.

2:05:37.680 --> 2:05:43.840
 Yeah. That's pretty good.

2:05:43.840 --> 2:05:48.400
 The United States oscillating between establishing and destroying dictatorships.

2:05:48.400 --> 2:05:51.440
 It's like a metro. Is that a metro? Yeah. What does that mean? Yeah.

2:05:51.440 --> 2:05:54.880
 Yeah. It's a 7 out of 10. It's kind of true. Oh, yeah. This is,

2:05:55.760 --> 2:06:00.480
 this is kind of personal for me. Next one. Oh, man. This is Leica?

2:06:00.480 --> 2:06:03.760
 Yeah. Well, no, this is... Or it's like referring to Leica or something?

2:06:03.760 --> 2:06:11.360
 As Leica is like a husband. Hello. Yes. This is dog. Your wife was launched to space.

2:06:11.360 --> 2:06:15.840
 And then the last one is him with his eyes closed and a bottle of vodka.

2:06:16.400 --> 2:06:20.880
 Yeah. Leica didn't come back. No. They don't tell you the full story of, you know,

2:06:21.920 --> 2:06:24.160
 what the love, the impact they had on the loved ones.

2:06:24.960 --> 2:06:27.120
 True. That one gets an 11 for me. Sure.

2:06:27.120 --> 2:06:34.080
 The Soviet set up. Yeah. This keeps going on the Russian theme. First man in space,

2:06:34.720 --> 2:06:37.840
 nobody cares. First man on the moon. Well, I think people do care.

2:06:37.840 --> 2:06:38.560
 No, I know. But...

2:06:41.040 --> 2:06:45.440
 Yuri Gagarin's names will be forever in history, I think.

2:06:45.440 --> 2:06:52.000
 There is something special about placing like stepping foot onto another totally foreign land.

2:06:52.000 --> 2:06:57.360
 It's not the journey like people that explore the oceans. It's not as important to explore the

2:06:57.360 --> 2:07:01.920
 oceans as to land on a whole new continent. Yeah.

2:07:02.880 --> 2:07:07.360
 Well, this is about you. Oh, yeah. I'd love to get your comment on this.

2:07:07.360 --> 2:07:14.240
 Elon Musk, after sending $6.6 billion to the UN to end world hunger, you have three hours.

2:07:14.240 --> 2:07:20.560
 Yeah. Well, I mean, obviously $6 billion is not going to end world hunger. So

2:07:24.000 --> 2:07:27.600
 so I mean, the reality is at this point, the world is producing

2:07:29.280 --> 2:07:33.600
 far more food than it can really consume it. Like we don't have a caloric

2:07:34.800 --> 2:07:39.040
 constraint to this point. So where there is hunger, it is almost always due to

2:07:39.040 --> 2:07:46.960
 like civil war, strife or some like... It's not a thing that is

2:07:48.960 --> 2:07:53.600
 extremely rare for it to be just a matter of lack of money. It's like,

2:07:54.640 --> 2:07:59.040
 you know, it's like some, the civil war in some country and like one part of the country is

2:07:59.040 --> 2:08:04.400
 literally trying to starve the other part of the country. So it's much more complex than

2:08:04.400 --> 2:08:10.480
 something that money could solve. It's geopolitics. It's a lot of things. It's human nature.

2:08:10.480 --> 2:08:14.560
 It's governments. It's money, monetary systems, all that kind of stuff.

2:08:14.560 --> 2:08:17.760
 Yeah. Food is extremely cheap these days. It's like,

2:08:20.720 --> 2:08:27.440
 I mean, the US at this point, you know, among low income families, obesity is actually another

2:08:27.440 --> 2:08:34.640
 problem. It's not, like obviously it's not hunger. It's like too many calories. So

2:08:36.160 --> 2:08:43.120
 it's not that nobody's hungry anywhere. It's just, this is not a simple matter of adding money and

2:08:43.120 --> 2:08:50.720
 solving it. What do you think that one gets? It's getting... Two.

2:08:50.720 --> 2:08:58.320
 Just going after Empire's world. Where did you get those artifacts? The British Museum

2:08:59.040 --> 2:09:02.080
 has a shout out to Monty Python. We found them.

2:09:02.800 --> 2:09:05.920
 Yeah. The British Museum is pretty great. I mean,

2:09:06.880 --> 2:09:10.320
 immediately Britain did take these historical artifacts from all around the world and put

2:09:10.320 --> 2:09:17.680
 them in London. But, you know, it's not like people can't go see them. So it is a convenient

2:09:17.680 --> 2:09:24.080
 place to see these ancient artifacts is London for, you know, for a large segment of the world.

2:09:24.800 --> 2:09:29.520
 So I think, you know, on balance, the British Museum is a net good. Although I'm sure

2:09:29.520 --> 2:09:33.920
 that a lot of countries argue about that. Yeah. It's like you want to make these historical

2:09:33.920 --> 2:09:38.960
 artifacts accessible to as many people as possible. And the British Museum, I think,

2:09:38.960 --> 2:09:44.720
 does a good job of that. Even if there's a darker aspect to, like, the history of Empire in general,

2:09:44.720 --> 2:09:52.080
 whatever the Empire is, however things were done, it is the history that happened. You

2:09:52.080 --> 2:09:55.680
 can't sort of erase that history, unfortunately. You could just become better in the future.

2:09:56.400 --> 2:10:03.040
 That's the point. Yeah. I mean, it's like, well, how are we going to pass from all judgment on

2:10:03.040 --> 2:10:10.160
 these things? Like, it's like, you know, if one is going to judge, say, the British Empire,

2:10:10.160 --> 2:10:14.720
 you got to judge, you know, what everyone was doing at the time, and how were the British

2:10:14.720 --> 2:10:21.760
 relative to everyone? And I think they were first would actually get like a relatively good grade,

2:10:21.760 --> 2:10:27.280
 relatively good grade, not an absolute terms, but compared to what everyone else was doing.

2:10:29.680 --> 2:10:33.600
 They were not the worst. Like I said, you got to look at these things in the context of the

2:10:33.600 --> 2:10:37.840
 history at the time, and say, what were the alternatives? And what are you comparing it

2:10:37.840 --> 2:10:46.080
 against? Yes. And I do not think it would be the case that Britain would get a bad grade

2:10:46.080 --> 2:10:52.240
 in when looking at history at the time. You know, if you judge history from, you know,

2:10:53.200 --> 2:10:57.680
 from what is morally acceptable today, you basically are going to give everyone a failing

2:10:57.680 --> 2:11:04.320
 grade. I'm not clear. I don't think anyone would get a passing grade in their morality of,

2:11:04.320 --> 2:11:09.280
 like you go back 300 years ago, like, who's getting a passing grade? Basically, no one.

2:11:10.480 --> 2:11:18.080
 And we might not get a passing grade from generations that come after us. What does that one get?

2:11:20.080 --> 2:11:22.320
 Sure. Six, seven, seven.

2:11:22.320 --> 2:11:25.440
 For the Monty Python, maybe. I always love Monty Python. They're great.

2:11:26.480 --> 2:11:29.200
 I like for Brian and the Quist of the Holy Grail are incredible.

2:11:29.200 --> 2:11:36.560
 Yeah, yeah. Yeah, those serious eyebrows. Brezhnev. How important do you think is facial hair to

2:11:36.560 --> 2:11:42.560
 great leadership? Well, you got a new haircut. How does that affect your leadership?

2:11:43.840 --> 2:11:46.080
 I don't know. Hopefully not. It doesn't.

2:11:48.240 --> 2:11:49.360
 Yeah, the second is no one.

2:11:51.200 --> 2:11:53.520
 There is no one competing with Brezhnev. No one, too.

2:11:53.520 --> 2:12:00.160
 Those are like epic eyebrows. So, sure. That's ridiculous.

2:12:00.160 --> 2:12:04.800
 Give it a six or seven. I don't know. I like this, like, Shakespeare analysis of memes.

2:12:05.520 --> 2:12:11.280
 Brezhnev, he had a flair for drama as well. Like, you know, showmanship.

2:12:11.280 --> 2:12:17.920
 Yeah, yeah. It must come from the eyebrows. All right. Invention, great engineering.

2:12:17.920 --> 2:12:21.360
 Look what I invented. That's the best thing since ripped up bread.

2:12:21.360 --> 2:12:27.520
 Yeah. Because they invented sliced bread. Am I just explaining memes at this point?

2:12:29.680 --> 2:12:31.040
 This is what my life has become.

2:12:33.520 --> 2:12:35.120
 You're going to be more of a meme explainer.

2:12:35.120 --> 2:12:42.240
 Yeah, I'm a meme. Like a scribe that like runs around with the kings and just like writes down

2:12:43.120 --> 2:12:47.200
 memes. I mean, when was the cheeseburger invented? That's like an epic invention.

2:12:47.200 --> 2:12:50.080
 Yeah. Like, wow.

2:12:50.080 --> 2:12:53.120
 You know, versus just like a burger?

2:12:53.120 --> 2:12:56.080
 Or burger. I guess a burger in general. It's like, you know.

2:12:57.200 --> 2:13:00.640
 Then there's like, what is a burger? What's the sandwich? And then you start getting

2:13:00.640 --> 2:13:05.760
 the pizza sandwich and what is the original? It gets into an ontology argument.

2:13:05.760 --> 2:13:08.800
 Yeah. But everybody knows like, if you order like a burger or cheeseburger or whatever and you

2:13:08.800 --> 2:13:12.560
 like, you got like, you know, tomato and some lettuce and onions and whatever and, you know,

2:13:12.560 --> 2:13:16.320
 you know, mayo and ketchup and mustard. It's like epic.

2:13:16.320 --> 2:13:20.560
 Yeah. But I'm sure they've had bread and meat separately for a long time and it was kind of

2:13:20.560 --> 2:13:24.880
 a burger on the same plate, but somebody who actually combined them into the same thing

2:13:25.600 --> 2:13:30.960
 and you buy it and hold it, makes it convenient. It's a materials problem. Like your hands don't

2:13:30.960 --> 2:13:33.920
 get dirty and whatever. Yeah, it's brilliant.

2:13:38.240 --> 2:13:40.240
 Well, that is not what I would have guessed.

2:13:40.240 --> 2:13:44.240
 But everybody knows like, if you order a cheeseburger, you know what you're getting,

2:13:44.240 --> 2:13:48.080
 you know, it's not like some obtuse. Like, I wonder what I'll get, you know.

2:13:49.920 --> 2:13:56.160
 You know, fries are, I mean, great. I mean, they're the devil, but fries are awesome. And

2:13:57.920 --> 2:14:02.640
 yeah, pizza is incredible. Food innovation doesn't get enough love.

2:14:03.440 --> 2:14:04.800
 Yeah. I guess is what we're getting at.

2:14:04.800 --> 2:14:11.120
 It's great. What about the Matthew McGonaghey Austinite here?

2:14:11.840 --> 2:14:15.040
 President Kennedy, do you know how to put men on the moon yet?

2:14:15.040 --> 2:14:18.400
 NASA know. President Kennedy, be a lot cooler if you did.

2:14:20.160 --> 2:14:22.480
 Pretty much. Sure. Six, six or seven.

2:14:26.000 --> 2:14:29.120
 And this is the last one. That's funny.

2:14:29.120 --> 2:14:35.440
 Someone drew a bunch of dicks all over the walls of Sistine Chapel Boys Bathroom.

2:14:35.440 --> 2:14:38.080
 Sure. I'll give it nine. It's really true.

2:14:39.360 --> 2:14:41.680
 This is our highest ranking meme for today.

2:14:42.240 --> 2:14:43.680
 I mean, it's true. Like, how do they get away with that?

2:14:44.480 --> 2:14:48.960
 Lots of nakedness. I mean, dick pics are, I mean, just something throughout history.

2:14:50.400 --> 2:14:52.800
 As long as people can draw things, there's been a dick pic.

2:14:52.800 --> 2:14:54.880
 It's a staple of human history.

2:14:54.880 --> 2:14:58.320
 It's a staple. Consistence throughout human history.

2:14:58.320 --> 2:15:01.680
 You tweeted that you aspire to comedy. Your friends with Joe Rogan

2:15:02.320 --> 2:15:05.920
 might you do a short stand up comedy set at some point in the future?

2:15:06.480 --> 2:15:11.120
 Maybe open for Joe, something like that. Is that really stand up?

2:15:11.120 --> 2:15:14.880
 Actual just full on stand up? Full on stand up. Is that in there or is that?

2:15:14.880 --> 2:15:15.760
 I've never thought about that.

2:15:17.360 --> 2:15:22.960
 It's extremely difficult if at least that's what Joe says in the comedians say.

2:15:22.960 --> 2:15:28.560
 Huh. I wonder if I could. The only one way to find out.

2:15:29.360 --> 2:15:34.400
 You know, I have done stand up for friends just impromptu.

2:15:35.600 --> 2:15:41.280
 You know, I'll get on like a roof and they do laugh, but they're our friends too.

2:15:41.280 --> 2:15:45.040
 So I don't know if you've got to call, you know, like a room of strangers.

2:15:45.040 --> 2:15:50.480
 Are they going to actually also find it funny? But I could try. See what happens.

2:15:50.480 --> 2:15:55.040
 I think you'd learn something either way. Yeah, I kind of love

2:15:56.400 --> 2:16:00.400
 both the, when you bomb and when, when you do great just watching people,

2:16:00.400 --> 2:16:04.800
 how they deal with it. It's so difficult. It's so, you're so fragile

2:16:05.760 --> 2:16:09.600
 up there. It's just you and you think you're going to be funny.

2:16:09.600 --> 2:16:12.800
 And when it completely falls flat, it's just, it's beautiful to see people

2:16:13.360 --> 2:16:16.800
 deal with like that. I might have enough material to do stand up.

2:16:16.800 --> 2:16:21.040
 I've never thought about it, but I might have enough material.

2:16:23.440 --> 2:16:27.840
 I don't know, like 15 minutes or something. Oh yeah. Yeah. Do a Netflix special.

2:16:28.640 --> 2:16:33.840
 Netflix special. Sure. What's your favorite Rick and Morty concept?

2:16:34.960 --> 2:16:38.960
 Just to spring that on you. Is there, there's a lot of sort of scientific engineering ideas

2:16:38.960 --> 2:16:43.120
 explored there. There's the favorite. There's the butter robot. It's great.

2:16:43.120 --> 2:16:47.040
 Yeah. It's a great show. You like it? Yeah. Rick and Morty is awesome.

2:16:47.040 --> 2:16:51.120
 Somebody that's exactly like you from an alternate dimension showed up there, Elon Tusk.

2:16:51.920 --> 2:16:53.520
 Yeah. That's right. That you voiced.

2:16:53.520 --> 2:16:56.800
 Yeah. Rick and Morty certainly explores a lot of interesting concepts.

2:16:59.120 --> 2:17:01.840
 Like what's the favorite one? I don't know. The butter robot certainly is,

2:17:02.800 --> 2:17:06.560
 you know, it's like, it's certainly possible to have too much sentence in a device.

2:17:07.680 --> 2:17:11.840
 Like you don't want to have your toast to be like a super genius toaster.

2:17:11.840 --> 2:17:16.400
 It's going to hate life because all it could just make is toast. But if, you know, it's like,

2:17:16.400 --> 2:17:20.240
 you don't want to have like super intelligence stuck in a very limited device.

2:17:21.040 --> 2:17:24.880
 Do you think it's too easy from a, if we're talking about from the engineering perspective

2:17:24.880 --> 2:17:30.720
 of super intelligence, like with Marvin, the robot, like is it just, it seems like it might

2:17:30.720 --> 2:17:37.040
 be very easy to engineer just a depressed robot. Like it, it's not obvious to engineer a robot

2:17:37.040 --> 2:17:45.200
 that's going to find a fulfilling existence. Same as humans, I suppose. But I wonder if that's like

2:17:45.200 --> 2:17:50.640
 the default. If you don't do a good job on building a robot, it's going to be sad a lot.

2:17:52.480 --> 2:18:01.120
 Well, we can reprogram robots easier than we can reprogram humans. So I guess if you let it evolve

2:18:01.120 --> 2:18:08.240
 without tinkering, then it might get sad. But you can change the optimization function and

2:18:08.800 --> 2:18:10.000
 have it be a cheery robot.

2:18:12.800 --> 2:18:17.840
 You, like I mentioned with, with SpaceX, you give a lot of people hope. And a lot of people look

2:18:17.840 --> 2:18:23.360
 up to you, millions of people look up to you. If we think about young people in high school,

2:18:23.360 --> 2:18:30.800
 maybe in college, what advice would you give to them about if they want to try to do something

2:18:30.800 --> 2:18:35.040
 big in this world, they want to really have a big positive impact, what advice would you give them

2:18:35.040 --> 2:18:37.360
 about their career, maybe about life in general?

2:18:39.200 --> 2:18:46.400
 Try to be useful. You do things that are useful to your fellow human beings to the world. It's

2:18:46.400 --> 2:18:56.960
 very hard to be useful. Very hard. You know, are you contributing more than you consume?

2:18:56.960 --> 2:19:05.280
 You know, like, like, can you try to have a positive net contribution to society?

2:19:07.520 --> 2:19:11.840
 I think that's the thing to aim for, you know, not to try to be sort of a leader for,

2:19:13.040 --> 2:19:20.560
 for the sake of being a leader or whatever. A lot of time people who, a lot of time, the people

2:19:20.560 --> 2:19:30.560
 you want as leaders are the people who don't want to be leaders. So if you can live a useful life,

2:19:32.080 --> 2:19:41.680
 that is a good life, a life worth having lived. You know, and like I said, I would encourage people

2:19:41.680 --> 2:19:49.120
 to use the mental tools of physics and apply them broadly in life. They are the best tools.

2:19:49.120 --> 2:19:55.280
 When you think about education and self education, what do you recommend? So there's the university,

2:19:55.280 --> 2:20:03.840
 there's self study, there is a hands on sort of finding a company or a place or a set of people

2:20:03.840 --> 2:20:06.960
 that do the thing you're passionate about and joining them as early as possible.

2:20:08.800 --> 2:20:13.200
 There's taking a road trip across Europe for a few years and writing some poetry,

2:20:13.200 --> 2:20:22.400
 which trajectory do you suggest in terms of learning about how you can become useful,

2:20:22.400 --> 2:20:24.960
 as you mentioned, how you can have the most positive impact?

2:20:32.480 --> 2:20:40.160
 Well, I encourage people to read a lot of books. Basically, try to ingest as much information as

2:20:40.160 --> 2:20:50.480
 you can and try to also just develop a good general knowledge. So you at least have a rough

2:20:50.480 --> 2:20:56.720
 lay of the land of the knowledge landscape. Try to learn a little bit about a lot of things.

2:20:58.160 --> 2:21:00.320
 Because you might not know what you're really interested in. How would you know what you're

2:21:00.320 --> 2:21:05.920
 really interested in if you at least aren't doing it peripheral exploration or broadly of

2:21:05.920 --> 2:21:15.360
 the knowledge landscape? And you talk to people from different walks of life and different

2:21:16.720 --> 2:21:24.480
 industries and professions and skills and occupations. Just try to learn as much as possible.

2:21:27.200 --> 2:21:28.240
 Man's search for meaning.

2:21:30.720 --> 2:21:32.400
 Isn't the whole thing a search for meaning?

2:21:32.400 --> 2:21:38.880
 Yeah, what's the meaning of life and all? But just generally, like I said, I would encourage

2:21:38.880 --> 2:21:47.440
 people to read broadly in many different subject areas and then try to find something where there's

2:21:47.440 --> 2:21:53.440
 an overlap of your talents and what you're interested in. So people may be good at something,

2:21:53.440 --> 2:21:57.360
 but they may have skill at a particular thing, but they don't like doing it.

2:21:57.360 --> 2:22:06.880
 So you want to try to find a thing where that's a good combination of the things that you're

2:22:06.880 --> 2:22:15.760
 inherently good at, but you also like doing. And reading is a super fast shortcut to

2:22:15.760 --> 2:22:22.160
 figure out where are you. You're both good at it. You like doing it, and it will actually have positive

2:22:22.160 --> 2:22:28.800
 impact. Well, you got to learn about things somehow. So reading a broad range, it's just

2:22:28.800 --> 2:22:37.680
 really read it. One important one is that kid I read through the encyclopedia. So that's pretty

2:22:37.680 --> 2:22:44.720
 helpful. And those are things I didn't even know existed. Well, lots, obviously. It's like as

2:22:44.720 --> 2:22:55.680
 broad as it gets. Encyclophilias were digestible, I think, whatever, 40 years ago. So maybe read

2:22:55.680 --> 2:23:01.440
 through the condensed version of the encyclopedia Britannica. I'd recommend that. You can always

2:23:01.440 --> 2:23:06.080
 like skip subjects or you read a few paragraphs, and no, you're not interested. Just jump to the

2:23:06.080 --> 2:23:18.720
 next one. So read the encyclopedia or scan through it. And you know, put a lot of stock and

2:23:18.720 --> 2:23:24.640
 certainly have a lot of respect for someone who puts in an honest day's work to do useful things.

2:23:25.920 --> 2:23:33.120
 And just generally to have like not a zero sum mindset, or like have more of a

2:23:33.120 --> 2:23:41.360
 like, grow the pie mindset, like the, if you sort of say like, when we see people like,

2:23:41.360 --> 2:23:48.000
 perhaps, including some very smart people, kind of taking an attitude of like, like,

2:23:48.000 --> 2:23:52.400
 like doing things that seem like morally questionable. It's often because they have

2:23:52.400 --> 2:23:59.920
 at a base sort of axiomatic level, a zero sum mindset. And, and they without realizing it,

2:23:59.920 --> 2:24:04.880
 they don't realize they have a zero sum mindset, or at least they don't realize it consciously.

2:24:05.760 --> 2:24:09.600
 And so if you have a zero sum mindset, then the only way to get ahead is by taking things from

2:24:09.600 --> 2:24:17.360
 others. If it's like, if the, if the, if the pie is fixed, then the only way to have more pie is to

2:24:17.360 --> 2:24:22.160
 take someone else's pie. But, but this is false, like obviously the pie has grown dramatically

2:24:22.160 --> 2:24:30.960
 over time, the economic pie. So the reality, in reality, you can have, so over useless analogy,

2:24:30.960 --> 2:24:40.400
 if you have a lot of, there's a lot of pie. Pie pie is not fixed. So you really want to make sure

2:24:40.400 --> 2:24:46.720
 you don't, you're not operating without realizing it from a zero sum mindset, where, where the only

2:24:46.720 --> 2:24:49.680
 way to get ahead is to take things from others, then that's going to result in you trying to

2:24:49.680 --> 2:24:55.920
 take things from others, which is not, not good. It's much better to work on adding to the economic

2:24:55.920 --> 2:25:05.120
 pie, maybe, you know, so creating, like I said, creating more than you consume, doing more than

2:25:05.120 --> 2:25:11.520
 you. Yeah. So that's a big deal. I think there's like, you know, a fair number of people in,

2:25:12.720 --> 2:25:16.560
 in finance that do have a bit of a zero sum mindset.

2:25:16.560 --> 2:25:21.280
 I mean, it's all walks of life. I've seen that. And one of the, one of the reasons

2:25:23.040 --> 2:25:29.200
 Rogan inspires me is he celebrates others a lot. This is not, not creating a constant competition.

2:25:29.200 --> 2:25:34.080
 Like there's a scarcity of resources. What happens when you celebrate others and you promote others,

2:25:34.800 --> 2:25:40.640
 the ideas of others, it, it, it actually grows that pie. I mean, it, every, like the,

2:25:41.360 --> 2:25:46.240
 the resource, the resources become less scarce. And that, that applies in a lot of kinds of

2:25:46.240 --> 2:25:51.520
 domains. It applies in academia where a lot of people are very, see some funding for academic

2:25:51.520 --> 2:25:56.320
 research is zero sum. And it is not, if you celebrate each other, if you make, if you get

2:25:56.320 --> 2:26:01.440
 everybody to be excited about AI, about physics, about mathematics, I think it, there'll be more,

2:26:01.440 --> 2:26:05.280
 more funding. And I think everybody wins. Yeah. That applies, I think broadly.

2:26:06.480 --> 2:26:11.280
 Yeah. Yeah. Exactly. So last, last, last question about love and meaning.

2:26:11.280 --> 2:26:19.360
 What is the role of love in the human condition broadly and more specific to you? How has love,

2:26:20.000 --> 2:26:23.760
 romantic love, or otherwise made you a better person, a better human being?

2:26:27.040 --> 2:26:27.840
 Better engineer?

2:26:29.040 --> 2:26:37.600
 Now you're asking really perplexing questions. It's hard to give up. I mean, there are many

2:26:37.600 --> 2:26:45.040
 books, poems, and songs written about what is love and what is, what exactly, you know,

2:26:47.600 --> 2:26:49.200
 you know, what is love? Maybe you don't hurt me.

2:26:52.320 --> 2:26:56.160
 That's one of the great ones. Yes. Yeah. You've, you've earlier quoted Shakespeare,

2:26:56.160 --> 2:27:01.120
 but that, that's really up there. Yeah. Love is a many splendid thing.

2:27:01.120 --> 2:27:08.320
 I mean, there's, because we've talked about so many inspiring things like be useful in the world,

2:27:08.320 --> 2:27:14.800
 sort of like solve problems, alleviate suffering, but it seems like connection between humans is a

2:27:14.800 --> 2:27:21.920
 source, you know, it's a source of joy is a source of meaning. And that, that's what love is, friendship,

2:27:21.920 --> 2:27:29.280
 love. I just wonder if you think about that kind of thing when you talk about preserving the light

2:27:29.280 --> 2:27:34.000
 of human consciousness or it's becoming a multi planetary multi planetary species.

2:27:35.120 --> 2:27:43.680
 I mean, to me, at least that, that means like, if we're just alone and conscious and intelligent,

2:27:44.240 --> 2:27:50.240
 it doesn't mean nearly as much as if we're with others, right? And there's some magic created

2:27:50.240 --> 2:27:56.640
 when we're together. The, the French of it, and I think the highest form of it is love,

2:27:56.640 --> 2:28:01.840
 which I think broadly is, is much bigger than just sort of romantic, but also yes,

2:28:01.840 --> 2:28:06.000
 romantic love and family and those kinds of things.

2:28:06.000 --> 2:28:10.400
 Well, I mean, the reason I guess I care about us becoming multi planetary species in a space

2:28:10.400 --> 2:28:20.560
 frank civilization is foundationally, I love humanity. And, and so I wish to see it prosper and

2:28:20.560 --> 2:28:28.480
 do great things and be happy. And if I did not love humanity, I would not care about these things.

2:28:31.120 --> 2:28:35.040
 So when you look at the whole of it, the human history, all the people who's ever lived, all

2:28:35.040 --> 2:28:44.480
 the people alive now, it's pretty, we're okay. On the whole, we're pretty interesting bunch.

2:28:44.480 --> 2:28:50.960
 Yeah. All things considered. And I've read a lot of history, including the darkest,

2:28:50.960 --> 2:28:57.840
 worst parts of it. And despite all that, I think on balance, I still love humanity.

2:28:59.280 --> 2:29:03.840
 You joked about it with the 42. What do you think is the meaning of this whole thing?

2:29:05.280 --> 2:29:08.240
 Is it like, is there a non numerical representation?

2:29:08.240 --> 2:29:13.520
 Yeah. Well, really, I think what Dr. Sattons was saying in Hitchhiker's Guide to the Galaxy is that

2:29:15.680 --> 2:29:23.440
 the universe is the answer. And what we really need to figure out are what questions to ask

2:29:23.440 --> 2:29:28.560
 about the answer that is the universe. And that the question is the really the hard part. And

2:29:28.560 --> 2:29:32.160
 if you can properly frame the question, then the answer relatively speaking is easy.

2:29:32.160 --> 2:29:40.720
 So therefore, if you want to understand what questions to ask about the universe,

2:29:40.720 --> 2:29:45.200
 you want to understand the meaning of life. We need to expand the scope and scale of consciousness

2:29:45.760 --> 2:29:51.120
 so that we're better able to understand the nature of the universe and understand the

2:29:51.120 --> 2:29:56.400
 meaning of life. And ultimately, the most important part would be to ask the right question.

2:29:56.400 --> 2:30:01.440
 Yes. Thereby elevating the role of the interviewer.

2:30:02.720 --> 2:30:05.600
 Yes, exactly. As the most important human in the room.

2:30:07.600 --> 2:30:13.520
 Good questions are, it's hard to come up with good questions. Absolutely.

2:30:15.040 --> 2:30:21.520
 But yeah, it's like that is the foundation of my philosophy is that I am curious about the

2:30:21.520 --> 2:30:30.240
 nature of the universe. And obviously, I will die. I don't know when I'll die, but I won't live

2:30:30.240 --> 2:30:36.560
 forever. But I would like to know that we are on a path to understanding the nature of the universe

2:30:36.560 --> 2:30:40.400
 and the meaning of life and what questions to ask about the answer that is the universe.

2:30:41.440 --> 2:30:46.320
 And so if we expand the scope and scale of humanity and consciousness in general,

2:30:46.320 --> 2:30:52.880
 which includes Silicon Consciousness, then that seems like a fundamentally good thing.

2:30:55.040 --> 2:31:01.040
 Elon, like I said, I'm deeply grateful that you have spent your extremely valuable time with me

2:31:01.040 --> 2:31:07.840
 today and also that you have given millions of people hope in this difficult time, this divisive

2:31:07.840 --> 2:31:14.080
 time in this cynical time. So I hope you do continue doing what you're doing. Thank you

2:31:14.080 --> 2:31:17.440
 so much for talking today. You're welcome. Thanks for excellent questions.

2:31:18.560 --> 2:31:22.560
 Thanks for listening to this conversation with Elon Musk. To support this podcast,

2:31:22.560 --> 2:31:27.360
 please check out our sponsors in the description. And now let me leave you with some words from

2:31:27.360 --> 2:31:33.600
 Elon Musk himself. When something is important enough, you do it, even if the odds are not

2:31:33.600 --> 2:31:47.440
 in your favor. Thank you for listening and hope to see you next time.

