WEBVTT

00:00.000 --> 00:03.440
 The following is a conversation with Peter Singer,

00:03.440 --> 00:06.200
 professor of bioethics and personal university,

00:06.200 --> 00:10.280
 best known for his 1975 book, Animal Liberation,

00:10.280 --> 00:14.240
 that makes an ethical case against eating meat.

00:14.240 --> 00:17.680
 He has written brilliantly from an ethical perspective

00:17.680 --> 00:21.480
 on extreme poverty, euthanasia, human genetic selection,

00:21.480 --> 00:23.720
 sports doping, the sale of kidneys,

00:23.720 --> 00:26.880
 and generally happiness,

00:26.880 --> 00:30.200
 including in his books, Ethics in the Real World,

00:30.200 --> 00:32.960
 and The Life You Can Save.

00:32.960 --> 00:36.320
 He was a key popularizer of the effective altruism movement

00:36.320 --> 00:37.800
 and is generally considered

00:37.800 --> 00:41.040
 one of the most influential philosophers in the world.

00:42.200 --> 00:43.760
 Quick summary of the ads.

00:43.760 --> 00:47.080
 Two sponsors, Cash App and Masterclass.

00:47.080 --> 00:48.840
 Please consider supporting the podcast

00:48.840 --> 00:52.240
 by downloading Cash App and using code LEX Podcast

00:52.240 --> 00:55.920
 and signing up at masterclass.com slash LEX.

00:55.920 --> 00:57.840
 Click the links by the stuff.

00:57.840 --> 01:00.080
 It really is the best way to support the podcast

01:00.080 --> 01:02.480
 and the journey I'm on.

01:02.480 --> 01:05.920
 As you may know, I primarily eat a ketogenic

01:05.920 --> 01:07.520
 or carnivore diet,

01:07.520 --> 01:10.400
 which means that most of my diet is made up of meat.

01:10.400 --> 01:12.600
 I do not hunt the food I eat,

01:12.600 --> 01:14.480
 though one day I hope to.

01:15.360 --> 01:17.800
 I love fishing, for example.

01:17.800 --> 01:21.000
 Fishing and eating the fish I catch has always felt

01:21.000 --> 01:23.640
 much more honest than participating

01:23.640 --> 01:26.440
 in the supply chain of factory farming.

01:26.440 --> 01:28.400
 From an ethics perspective,

01:28.400 --> 01:31.960
 this part of my life has always had a cloud over it.

01:31.960 --> 01:33.680
 It makes me think.

01:33.680 --> 01:35.960
 I've tried a few times in my life

01:35.960 --> 01:37.960
 to reduce the amount of meat I eat,

01:37.960 --> 01:41.280
 but for some reason, whatever the makeup of my body,

01:41.280 --> 01:44.120
 whatever the way I practice the dieting I have,

01:44.120 --> 01:48.040
 I get a lot of mental and physical energy

01:48.040 --> 01:50.640
 and performance from eating meat.

01:50.640 --> 01:54.040
 So both intellectually and physically,

01:54.040 --> 01:56.200
 it's a continued journey for me.

01:56.200 --> 02:00.400
 I return to Peter's work often to reevaluate the ethics

02:00.400 --> 02:03.400
 of how I live this aspect of my life.

02:03.400 --> 02:06.200
 Let me also say that you may be a vegan

02:06.200 --> 02:07.840
 or you may be a meat eater

02:07.840 --> 02:11.440
 and may be upset by the words I say or Peter says,

02:11.440 --> 02:13.800
 but I ask for this podcast

02:13.800 --> 02:16.080
 and other episodes of this podcast

02:16.080 --> 02:18.280
 that you keep an open mind.

02:18.280 --> 02:21.640
 I may and probably will talk with people you disagree with.

02:21.640 --> 02:24.800
 Please try to really listen,

02:24.800 --> 02:27.400
 especially to people you disagree with

02:27.400 --> 02:29.800
 and give me and the world the gift

02:29.800 --> 02:33.080
 of being a participant in a patient, intelligent

02:33.080 --> 02:34.840
 and nuanced discourse.

02:34.840 --> 02:38.640
 If your instinct and desire is to be a voice of mockery

02:38.640 --> 02:42.520
 towards those you disagree with, please unsubscribe.

02:42.520 --> 02:44.840
 My source of joy and inspiration here

02:44.840 --> 02:46.840
 has been to be a part of a community

02:46.840 --> 02:51.000
 that thinks deeply and speaks with empathy and compassion.

02:51.000 --> 02:53.840
 That is what I hope to continue being a part of

02:53.840 --> 02:56.160
 and I hope you join as well.

02:56.160 --> 02:58.920
 If you enjoy this podcast, subscribe on YouTube,

02:58.920 --> 03:01.320
 review it with 5 Stars on Apple Podcast,

03:01.320 --> 03:04.240
 follow on Spotify, support on Patreon

03:04.240 --> 03:07.880
 or connect with me on Twitter at Lex Freedman.

03:07.880 --> 03:09.920
 As usual, I'll do a few minutes of ads now

03:09.920 --> 03:11.280
 and never any ads in the middle

03:11.280 --> 03:14.000
 that can break the flow of the conversation.

03:14.000 --> 03:16.520
 This show is presented by Cash App,

03:16.520 --> 03:18.920
 the number one finance app in the App Store.

03:18.920 --> 03:21.960
 When you get it, use code LEX Podcast.

03:21.960 --> 03:25.200
 Cash App lets you send money to friends by Bitcoin

03:25.200 --> 03:29.480
 and invest in the stock market with as little as $1.

03:29.480 --> 03:31.760
 Since Cash App allows you to buy Bitcoin,

03:31.760 --> 03:34.560
 let me mention that cryptocurrency in the context

03:34.560 --> 03:37.360
 of the history of money is fascinating.

03:37.360 --> 03:41.440
 I recommend Ascent of Money as a great book in this history.

03:41.440 --> 03:45.920
 Debits and credits on ledgers started around 30,000 years ago.

03:45.920 --> 03:48.520
 The US dollar created over 200 years ago

03:48.520 --> 03:51.040
 and the first decentralized cryptocurrency

03:51.040 --> 03:53.720
 released just over 10 years ago.

03:53.720 --> 03:55.000
 So given that history,

03:55.000 --> 03:56.960
 cryptocurrency is still very much

03:56.960 --> 03:58.680
 in its early days of development,

03:58.680 --> 04:02.080
 but it's still aiming to and just might redefine

04:02.080 --> 04:04.280
 the nature of money.

04:04.280 --> 04:06.960
 So again, if you get Cash App from the App Store

04:06.960 --> 04:10.440
 or Google Play and use the code LEX Podcast,

04:10.440 --> 04:14.840
 you get $10 and Cash App will also donate $10 to first,

04:14.840 --> 04:17.520
 an organization that is helping to advance robotic system

04:17.520 --> 04:20.840
 education for young people around the world.

04:20.840 --> 04:23.400
 This show is sponsored by Masterclass.

04:23.400 --> 04:27.080
 Sign up at masterclass.com slash LEX to get a discount

04:27.080 --> 04:29.600
 and to support this podcast.

04:29.600 --> 04:31.280
 When I first heard about Masterclass,

04:31.280 --> 04:33.120
 I thought it was too good to be true.

04:33.120 --> 04:36.640
 For $180 a year, you get an all access pass

04:36.640 --> 04:40.360
 to watch courses from to list some of my favorites,

04:40.360 --> 04:42.880
 Chris Hadfield on Space Exploration,

04:42.880 --> 04:46.160
 Nielugas Tyson on Scientific Thinking and Communication,

04:46.160 --> 04:50.360
 Will Wright, creator of SimCity and Sims on Game Design.

04:50.360 --> 04:53.840
 I promise I'll start streaming games at some point soon.

04:53.840 --> 04:55.800
 Carlos Santana on Guitar,

04:55.800 --> 04:59.720
 Gary Kasparov on Chess, Daniel Lagrano on Poker,

04:59.720 --> 05:01.560
 and many more.

05:01.560 --> 05:04.200
 Chris Hadfield explaining how rockets work

05:04.200 --> 05:07.240
 and the experience of being launched into space alone

05:07.240 --> 05:08.680
 is worth the money.

05:08.680 --> 05:12.760
 By the way, you can watch it on basically any device.

05:12.760 --> 05:16.560
 Once again, sign up at masterclass.com slash LEX

05:16.560 --> 05:20.200
 to get a discount and to support this podcast.

05:20.200 --> 05:24.000
 And now here's my conversation with Peter Singer.

05:25.000 --> 05:27.560
 When did you first become conscious of the fact

05:27.560 --> 05:30.280
 that there is much suffering in the world?

05:32.200 --> 05:33.680
 I think I was conscious of the fact

05:33.680 --> 05:35.680
 that there's a lot of suffering in the world

05:35.680 --> 05:38.440
 pretty much as soon as I was able to understand

05:38.440 --> 05:40.880
 anything about my family and its background

05:40.880 --> 05:44.640
 because I lost three of my four grandparents

05:44.640 --> 05:45.640
 in the Holocaust.

05:45.640 --> 05:50.640
 And obviously I knew why I only had one grandparent

05:52.040 --> 05:54.480
 and she herself had been in the camps and survived.

05:54.480 --> 05:58.000
 So I think I knew a lot about that pretty early.

05:58.000 --> 06:01.120
 My entire family comes from the Soviet Union.

06:01.120 --> 06:03.600
 I was born in the Soviet Union.

06:03.600 --> 06:07.800
 Sort of World War II has deep roots in the culture

06:07.800 --> 06:11.080
 and the suffering that the war brought the millions

06:11.080 --> 06:13.920
 of people who died is in the music,

06:13.920 --> 06:16.800
 is in the literature, is in the culture.

06:16.800 --> 06:18.880
 What do you think was the impact

06:18.880 --> 06:22.320
 of the war broadly on our society?

06:24.960 --> 06:26.720
 The war had many impacts.

06:28.080 --> 06:31.360
 I think one of them, a beneficial impact

06:31.360 --> 06:34.240
 is that it showed what racism

06:34.240 --> 06:37.920
 and authoritarian government can do.

06:37.920 --> 06:41.040
 And at least as far as the West was concerned,

06:41.040 --> 06:43.160
 I think that meant that I grew up in an era

06:43.160 --> 06:48.000
 in which there wasn't the kind of overt racism

06:48.000 --> 06:51.760
 and anti semitism that had existed for my parents

06:51.760 --> 06:53.800
 in Europe, I was growing up in Australia.

06:53.800 --> 06:57.560
 And certainly that was clearly seen

06:57.560 --> 06:59.400
 as something completely unacceptable.

06:59.400 --> 07:04.400
 There was also a fear of a further outbreak of war,

07:04.520 --> 07:07.720
 which this time we expected would be nuclear

07:07.720 --> 07:10.400
 because of the way the Second World War had ended.

07:10.400 --> 07:15.400
 So there was this overshadowing of my childhood

07:15.400 --> 07:18.680
 about the possibility that I would not live to grow up

07:18.680 --> 07:22.680
 and be an adult because of a catastrophic nuclear war.

07:23.680 --> 07:27.680
 The film on the beach was made in which the city

07:27.680 --> 07:30.840
 that I was living, Melbourne, was the last place on earth

07:30.840 --> 07:35.040
 to have living human beings because of the nuclear cloud

07:35.040 --> 07:36.800
 that was spreading from the North.

07:37.800 --> 07:40.440
 So that certainly gave us a bit of that sense.

07:41.800 --> 07:44.280
 There were many, there were clearly many other legacies

07:44.280 --> 07:46.280
 that we got of the war as well

07:46.280 --> 07:48.240
 and the whole setup of the world

07:48.240 --> 07:50.480
 and the Cold War that followed.

07:50.480 --> 07:53.840
 All of that has its roots in the Second World War.

07:53.840 --> 07:56.400
 You know, there is much beauty that comes from war.

07:56.400 --> 08:00.080
 Sort of, I had a conversation with Eric Weinstein.

08:00.080 --> 08:02.640
 He said, everything is great about war

08:02.640 --> 08:05.040
 except all the death and suffering.

08:06.840 --> 08:08.840
 Do you think there's something positive

08:09.840 --> 08:12.480
 that came from the war,

08:12.480 --> 08:15.480
 the mirror that it put to our society,

08:15.480 --> 08:18.840
 sort of the ripple effects on it, ethically speaking,

08:18.840 --> 08:22.640
 do you think there are positive aspects to war?

08:22.640 --> 08:26.520
 I find it hard to see positive aspects in war

08:26.520 --> 08:29.280
 and some of the things that other people think of

08:29.280 --> 08:34.280
 as positive and beautiful may be questioning.

08:34.280 --> 08:37.040
 So there's a certain kind of patriotism.

08:37.040 --> 08:39.680
 People say, you know, during wartime, we all pull together,

08:39.680 --> 08:42.480
 we all work together against the common enemy.

08:42.480 --> 08:43.920
 And that's true.

08:43.920 --> 08:46.040
 An outside enemy does unite a country

08:46.040 --> 08:48.600
 and in general, it's good for countries to be united

08:48.600 --> 08:49.800
 and have common purposes.

08:49.800 --> 08:54.000
 But it also engenders a kind of a nationalism

08:54.000 --> 08:56.800
 and a patriotism that can't be questioned

08:56.800 --> 09:00.480
 and that I'm more skeptical about.

09:00.480 --> 09:04.440
 What about the brotherhood that people talk about

09:04.440 --> 09:11.440
 from soldiers, the sort of counterintuitive sad idea

09:11.440 --> 09:15.000
 that the closest that people feel to each other

09:15.000 --> 09:16.600
 is in those moments of suffering,

09:16.600 --> 09:18.560
 of being at the sort of the edge

09:18.560 --> 09:23.400
 of seeing your comrades dying in your arms.

09:23.400 --> 09:25.840
 That somehow brings people extremely closely together.

09:25.840 --> 09:28.000
 Suffering brings people closer together.

09:28.000 --> 09:30.200
 How do you make sense of that?

09:30.200 --> 09:31.800
 It may bring people close together,

09:31.800 --> 09:34.800
 but there are other ways of bonding

09:34.800 --> 09:36.160
 and being close to people, I think,

09:36.160 --> 09:40.200
 without the suffering and death that war entails.

09:40.200 --> 09:42.000
 Perhaps you could see,

09:42.000 --> 09:46.000
 you could already hear the romanticized Russian enemy.

09:46.000 --> 09:49.000
 We tend to romanticize suffering just a little bit

09:49.000 --> 09:52.000
 in our literature and culture and so on.

09:52.000 --> 09:54.000
 Could you take a step back?

09:54.000 --> 09:56.000
 I apologize if it's a ridiculous question,

09:56.000 --> 09:58.000
 but what is suffering?

09:58.000 --> 10:02.000
 If you would try to define what suffering is,

10:02.000 --> 10:04.000
 how would you go about it?

10:04.000 --> 10:08.000
 Suffering is a conscious state.

10:08.000 --> 10:10.000
 There can be no suffering for a being

10:10.000 --> 10:13.000
 who is completely unconscious.

10:13.000 --> 10:18.000
 And it's distinguished from other conscious states

10:18.000 --> 10:20.000
 in terms of being one that,

10:20.000 --> 10:23.000
 considered just in itself,

10:23.000 --> 10:25.000
 we would rather be without.

10:25.000 --> 10:27.000
 It's a conscious state that we want to stop

10:27.000 --> 10:29.000
 if we're experiencing

10:29.000 --> 10:31.000
 or we want to avoid having again

10:31.000 --> 10:34.000
 if we've experienced it in the past.

10:34.000 --> 10:37.000
 And that's, I emphasize, for its own sake,

10:37.000 --> 10:39.000
 because, of course, people will say,

10:39.000 --> 10:41.000
 well, suffering strengthens the spirit.

10:41.000 --> 10:44.000
 It has good consequences.

10:44.000 --> 10:47.000
 And sometimes it does have those consequences.

10:47.000 --> 10:50.000
 And of course, sometimes we might undergo suffering.

10:50.000 --> 10:53.000
 We set ourselves a challenge to run a marathon

10:53.000 --> 10:55.000
 or climb a mountain,

10:55.000 --> 10:57.000
 or even just to go to the dentist

10:57.000 --> 10:59.000
 so that the toothache doesn't get worse,

10:59.000 --> 11:01.000
 even though we know the dentist is going to hurt us

11:01.000 --> 11:02.000
 to some extent.

11:02.000 --> 11:04.000
 So I'm not saying that we never choose suffering,

11:04.000 --> 11:07.000
 but I am saying that other things being equal,

11:07.000 --> 11:10.000
 we would rather not be in that state of consciousness.

11:10.000 --> 11:12.000
 Is the ultimate goal, sort of,

11:12.000 --> 11:15.000
 you have the new 10 year anniversary release

11:15.000 --> 11:17.000
 of the Life You Can Save Book,

11:17.000 --> 11:19.000
 really influential book.

11:19.000 --> 11:21.000
 We'll talk about it a bunch of times

11:21.000 --> 11:22.000
 throughout this conversation.

11:22.000 --> 11:25.000
 But do you think it's possible

11:25.000 --> 11:28.000
 to eradicate suffering?

11:28.000 --> 11:30.000
 Or is that the goal?

11:30.000 --> 11:33.000
 Or do we want to achieve

11:33.000 --> 11:37.000
 a kind of minimum threshold of suffering

11:37.000 --> 11:42.000
 and then keeping a little drop of poison

11:42.000 --> 11:47.000
 to keep things interesting in the world?

11:47.000 --> 11:50.000
 In practice, I don't think we ever will eliminate suffering.

11:50.000 --> 11:53.000
 So I think that little drop of poison, as you put it,

11:53.000 --> 11:56.000
 or if you like, the contrasting dash

11:56.000 --> 11:59.000
 of an unpleasant color, perhaps something like that,

11:59.000 --> 12:03.000
 in an otherwise harmonious and beautiful composition,

12:03.000 --> 12:06.000
 that is going to always be there.

12:06.000 --> 12:09.000
 If you ask me whether, in theory,

12:09.000 --> 12:12.000
 if we could get rid of it, we should.

12:12.000 --> 12:15.000
 I think the answer is whether, in fact,

12:15.000 --> 12:18.000
 we would be better off,

12:18.000 --> 12:20.000
 or whether in terms of, by eliminating the suffering,

12:20.000 --> 12:22.000
 we would also eliminate some of the highs,

12:22.000 --> 12:24.000
 the positive highs.

12:24.000 --> 12:27.000
 And if that's so, then we might be prepared to say

12:27.000 --> 12:30.000
 it's worth having a minimum of suffering

12:30.000 --> 12:34.000
 in order to have the best possible experiences as well.

12:34.000 --> 12:38.000
 Is there a relative aspect to suffering?

12:38.000 --> 12:44.000
 When you talk about eradicating poverty in the world,

12:44.000 --> 12:47.000
 is this the more you succeed,

12:47.000 --> 12:50.000
 the more the bar of what defines poverty raises,

12:50.000 --> 12:53.000
 or is there, at the basic human ethical level,

12:53.000 --> 12:57.000
 a bar that's absolute, that once you get above it,

12:57.000 --> 13:02.000
 then we can morally converge to feeling

13:02.000 --> 13:06.000
 like we have eradicated poverty?

13:06.000 --> 13:08.000
 I think they're both,

13:08.000 --> 13:11.000
 and I think this is true for poverty as well as suffering.

13:11.000 --> 13:15.000
 There's an objective level of suffering,

13:15.000 --> 13:19.000
 or of poverty, where we're talking about objective indicators,

13:19.000 --> 13:23.000
 like you're constantly hungry,

13:23.000 --> 13:26.000
 you can't get enough food,

13:26.000 --> 13:30.000
 you're constantly cold, you can't get warm,

13:30.000 --> 13:34.000
 you have some physical pains that you're never rid of.

13:34.000 --> 13:37.000
 I think those things are objective.

13:37.000 --> 13:40.000
 But it may also be true that if you do get rid of that

13:40.000 --> 13:43.000
 and you get to the stage where all of those basic needs

13:43.000 --> 13:45.000
 have been met,

13:45.000 --> 13:49.000
 there may still be then new forms of suffering that develop.

13:49.000 --> 13:53.000
 And perhaps that's what we're seeing in the affluent societies we have,

13:53.000 --> 13:56.000
 that people get bored, for example.

13:56.000 --> 13:58.000
 They don't need to spend so many hours a day

13:58.000 --> 14:01.000
 earning money to get enough to eat and shelter.

14:01.000 --> 14:04.000
 So now they're bored, they lack a sense of purpose.

14:04.000 --> 14:06.000
 That can happen.

14:06.000 --> 14:10.000
 And that then is a kind of a relative suffering

14:10.000 --> 14:14.000
 that is distinct from the objective forms of suffering.

14:14.000 --> 14:17.000
 But in your focus on eradicating suffering,

14:17.000 --> 14:19.000
 you don't think about that kind of...

14:19.000 --> 14:22.000
 the kind of interesting challenges and suffering

14:22.000 --> 14:24.000
 that emerges in affluent societies.

14:24.000 --> 14:28.000
 That's just not...in your ethical, philosophical brain,

14:28.000 --> 14:31.000
 is that of interest at all?

14:31.000 --> 14:34.000
 It would be of interest to me if we had eliminated

14:34.000 --> 14:36.000
 all of the objective forms of suffering,

14:36.000 --> 14:40.000
 which I think are generally more severe

14:40.000 --> 14:45.000
 and also perhaps easier at this stage anyway to know how to eliminate.

14:45.000 --> 14:48.000
 So, yes, in some future state,

14:48.000 --> 14:50.000
 when we've eliminated those objective forms of suffering,

14:50.000 --> 14:53.000
 I would be interested in trying to eliminate

14:53.000 --> 14:56.000
 the relative forms as well.

14:56.000 --> 15:00.000
 But that's not a practical need for me at the moment.

15:00.000 --> 15:02.000
 Sorry to linger on it because you kind of said it,

15:02.000 --> 15:05.000
 but just to...

15:05.000 --> 15:08.000
 Is elimination the goal for the affluent society?

15:08.000 --> 15:11.000
 So, is there a...

15:11.000 --> 15:14.000
 Do you see a suffering as a creative force?

15:14.000 --> 15:17.000
 Suffering can be a creative force.

15:17.000 --> 15:20.000
 I think I'm repeating what I said about the highs

15:20.000 --> 15:24.000
 and whether we need some of the lows to experience the highs.

15:24.000 --> 15:26.000
 So, it may be that suffering makes us more creative

15:26.000 --> 15:29.000
 and we regard that as worthwhile.

15:29.000 --> 15:32.000
 Maybe that brings some of those highs with it

15:32.000 --> 15:36.000
 that we would not have had if we'd had no suffering.

15:36.000 --> 15:38.000
 I don't really know.

15:38.000 --> 15:40.000
 Many people have suggested that

15:40.000 --> 15:44.000
 and I certainly can't have no basis for denying it.

15:44.000 --> 15:46.000
 And if it's true,

15:46.000 --> 15:50.000
 I would not want to eliminate suffering completely.

15:50.000 --> 15:53.000
 But the focus is on the absolute,

15:53.000 --> 15:56.000
 not to be cold, not to be hungry.

15:56.000 --> 15:58.000
 Yes.

15:58.000 --> 16:01.000
 At the present stage of where the world's population is,

16:01.000 --> 16:03.000
 that's the focus.

16:03.000 --> 16:06.000
 Talking about human nature for a second,

16:06.000 --> 16:08.000
 do you think people are inherently good

16:08.000 --> 16:11.000
 or do we all have good and evil in us

16:11.000 --> 16:14.000
 that basically everyone is capable of evil

16:14.000 --> 16:17.000
 based on the environment?

16:17.000 --> 16:21.000
 Certainly most of us have potential for both good and evil.

16:21.000 --> 16:24.000
 I'm not prepared to say that everyone is capable of evil.

16:24.000 --> 16:27.000
 Maybe some people who even in the worst of circumstances

16:27.000 --> 16:29.000
 would not be capable of it.

16:29.000 --> 16:32.000
 But most of us are very susceptible

16:32.000 --> 16:34.000
 to environmental influences.

16:34.000 --> 16:38.000
 So, when we look at things that we were talking about previously,

16:38.000 --> 16:43.000
 let's say, what the Nazis did during the Holocaust,

16:43.000 --> 16:46.000
 I think it's quite difficult to say,

16:46.000 --> 16:50.000
 I know that I would not have done those things,

16:50.000 --> 16:54.000
 even if I were in the same circumstances as those who did them.

16:54.000 --> 16:58.000
 Even if, let's say, I had grown up under the Nazi regime

16:58.000 --> 17:02.000
 and had been indoctrinated with racist ideas,

17:02.000 --> 17:07.000
 had also had the idea that I must obey orders,

17:07.000 --> 17:10.000
 follow the commands of the Fuhrer.

17:10.000 --> 17:14.000
 Plus, of course, perhaps the threat that if I didn't do certain things,

17:14.000 --> 17:16.000
 I might get sent to the Russian front,

17:16.000 --> 17:19.000
 and that would be a pretty grim fate.

17:19.000 --> 17:22.000
 I think it's really hard for anybody to say,

17:22.000 --> 17:28.000
 nevertheless, I know I would not have killed those Jews or whatever else it was.

17:28.000 --> 17:32.000
 What's your intuition? How many people will be able to say that?

17:32.000 --> 17:34.000
 Truly to be able to say it.

17:34.000 --> 17:37.000
 I think very few, less than 10%.

17:37.000 --> 17:41.000
 To me, it seems a very interesting and powerful thing to meditate on.

17:41.000 --> 17:45.000
 So I've read a lot about the war, the World War II,

17:45.000 --> 17:51.000
 and I can't escape the thought that I would have not been one of the 10%.

17:51.000 --> 17:55.000
 Right. I have to say, I simply don't know.

17:55.000 --> 17:59.000
 I would like to hope that I would have been one of the 10%,

17:59.000 --> 18:05.000
 but I don't really have any basis for claiming that I would have been different from the majority.

18:05.000 --> 18:09.000
 Is it a worthwhile thing to contemplate?

18:09.000 --> 18:13.000
 It would be interesting if we could find a way of really finding these answers.

18:13.000 --> 18:19.000
 There obviously is quite a bit of research on people during the Holocaust,

18:19.000 --> 18:25.000
 on how ordinary Germans got led to do terrible things,

18:25.000 --> 18:28.000
 and there are also studies of the resistance.

18:28.000 --> 18:32.000
 Some heroic people in the White Rose group, for example,

18:32.000 --> 18:37.000
 who resisted even though they knew they were likely to die for it.

18:37.000 --> 18:43.000
 But I don't know whether these studies really can answer your larger question

18:43.000 --> 18:47.000
 of how many people would have been capable of doing that.

18:47.000 --> 18:52.000
 Well, the reason I think it's interesting is in the world,

18:52.000 --> 19:00.000
 as you described, when there are things that you'd like to do that are good,

19:00.000 --> 19:02.000
 that are objectively good,

19:02.000 --> 19:07.000
 it's useful to think about whether I'm not willing to do something,

19:07.000 --> 19:11.000
 or I'm not willing to acknowledge something as good and the right thing to do

19:11.000 --> 19:19.000
 because I'm simply scared of damaging my life in some kind of way.

19:19.000 --> 19:22.000
 And that kind of thought exercise is helpful to understand

19:22.000 --> 19:27.000
 what is the right thing in my current skill set and the capacity to do.

19:27.000 --> 19:30.000
 There are things that are convenient,

19:30.000 --> 19:33.000
 and I wonder if there are things that are highly inconvenient,

19:33.000 --> 19:38.000
 where I would have to experience derision, or hatred, or death,

19:38.000 --> 19:41.000
 or all those kinds of things, but it's truly the right thing to do.

19:41.000 --> 19:45.000
 And that kind of balance is, I feel like in America,

19:45.000 --> 19:50.000
 it's difficult to think in the current times,

19:50.000 --> 19:53.000
 it seems easier to put yourself back in history,

19:53.000 --> 19:57.000
 where you can sort of objectively contemplate whether,

19:57.000 --> 20:03.000
 how willing you are to do the right thing when the cost is high.

20:03.000 --> 20:06.000
 True, but I think we do face those challenges today,

20:06.000 --> 20:10.000
 and I think we can still ask ourselves those questions.

20:10.000 --> 20:15.000
 So one stand that I took more than 40 years ago now was to stop eating meat

20:15.000 --> 20:21.000
 and become a vegetarian at a time when you hardly met anybody who was a vegetarian,

20:21.000 --> 20:24.000
 or if you did, they might have been a Hindu,

20:24.000 --> 20:30.000
 or they might have had some weird theories about meat and health.

20:30.000 --> 20:33.000
 And I know thinking about making that decision,

20:33.000 --> 20:35.000
 I was convinced that it was the right thing to do,

20:35.000 --> 20:37.000
 but I still did have to think,

20:37.000 --> 20:40.000
 are all my friends going to think that I'm a crank,

20:40.000 --> 20:44.000
 because I'm now refusing to eat meat?

20:44.000 --> 20:48.000
 So I'm not saying there were any terrible sanctions, obviously,

20:48.000 --> 20:51.000
 but I thought about that, and I guess I decided,

20:51.000 --> 20:54.000
 well, I still think this is the right thing to do,

20:54.000 --> 20:56.000
 and I'll put up with that if it happens.

20:56.000 --> 21:00.000
 And one or two friends were clearly uncomfortable with that decision,

21:00.000 --> 21:07.000
 but that was pretty minor compared to the historical examples that we've been talking about.

21:07.000 --> 21:12.000
 But other issues that we have around too, like global poverty

21:12.000 --> 21:15.000
 and what we ought to be doing about that is another question

21:15.000 --> 21:19.000
 where people, I think, can have the opportunity to take a stand

21:19.000 --> 21:21.000
 on what's the right thing to do now.

21:21.000 --> 21:23.000
 Climate change would be a third question

21:23.000 --> 21:26.000
 where, again, people are taking a stand.

21:26.000 --> 21:29.000
 I can look at Greta Thunberg there and say,

21:29.000 --> 21:34.000
 well, I think it must have taken a lot of courage for a schoolgirl

21:34.000 --> 21:37.000
 to say, I'm going to go on strike about climate change

21:37.000 --> 21:41.000
 and see what happened.

21:41.000 --> 21:43.000
 Yeah, especially in this divisive world,

21:43.000 --> 21:47.000
 she gets exceptionally huge amounts of support and hatred both.

21:47.000 --> 21:54.000
 Which is very difficult for a teenager to operate in.

21:54.000 --> 21:56.000
 In your book, Ethics in the Real World,

21:56.000 --> 21:58.000
 an amazing book, people should check it out.

21:58.000 --> 22:00.000
 Very easy read.

22:00.000 --> 22:03.000
 82 brief essays on things that matter.

22:03.000 --> 22:07.000
 One of the essays asks, should robots have rights?

22:07.000 --> 22:11.000
 You've written about this, so let me ask, should robots have rights?

22:11.000 --> 22:16.000
 If we ever develop robots capable of consciousness,

22:16.000 --> 22:20.000
 capable of having their own internal perspective

22:20.000 --> 22:24.000
 on what's happening to them so that their lives can go well

22:24.000 --> 22:27.000
 or badly for them, then robots should have rights.

22:27.000 --> 22:30.000
 Until that happens, they shouldn't.

22:30.000 --> 22:36.000
 So, is consciousness essentially a prerequisite to suffering?

22:36.000 --> 22:41.000
 So, everything that possesses consciousness

22:41.000 --> 22:44.000
 is capable of suffering put another way.

22:44.000 --> 22:48.000
 And if so, what is consciousness?

22:48.000 --> 22:53.000
 I certainly think that consciousness is a prerequisite for suffering.

22:53.000 --> 22:58.000
 You can't suffer if you're not conscious.

22:58.000 --> 23:02.000
 But is it true that every being that is conscious

23:02.000 --> 23:05.000
 will suffer or has to be capable of suffering?

23:05.000 --> 23:08.000
 I suppose you could imagine a kind of consciousness,

23:08.000 --> 23:11.000
 especially if we can construct it artificially,

23:11.000 --> 23:14.000
 that's capable of experiencing pleasure,

23:14.000 --> 23:17.000
 but just automatically cuts out the consciousness

23:17.000 --> 23:20.000
 when they're suffering, sort of like an instant anesthesia

23:20.000 --> 23:22.000
 as soon as something is going to cause you suffering.

23:22.000 --> 23:26.000
 So, that's possible, but doesn't exist

23:26.000 --> 23:31.000
 as far as we know on this planet yet.

23:31.000 --> 23:34.000
 You asked what is consciousness?

23:34.000 --> 23:39.000
 Philosophers often talk about it as there being a subject of experiences.

23:39.000 --> 23:44.000
 So, you and I and everybody listening to this is a subject of experience.

23:44.000 --> 23:48.000
 There is a conscious subject who is taking things in,

23:48.000 --> 23:51.000
 responding to it in various ways,

23:51.000 --> 23:54.000
 feeling good about it, feeling bad about it.

23:54.000 --> 24:00.000
 And that's different from the kinds of artificial intelligence we have now.

24:00.000 --> 24:06.000
 I take out my phone, I ask Google directions to where I'm going,

24:06.000 --> 24:10.000
 Google gives me the directions, and I choose to take a different way.

24:10.000 --> 24:14.000
 Google doesn't care. It's not like I'm offending Google or anything like that.

24:14.000 --> 24:16.000
 There is no subject of experiences there.

24:16.000 --> 24:23.000
 And I think that's the indication that Google AI we have now

24:23.000 --> 24:27.000
 is not conscious or at least that level of AI is not conscious.

24:27.000 --> 24:29.000
 And that's the way to think about it.

24:29.000 --> 24:34.000
 It may be difficult to tell, of course, whether a certain AI is or isn't conscious.

24:34.000 --> 24:37.000
 It may mimic consciousness, and we can't tell if it's only mimicking it

24:37.000 --> 24:39.000
 or if it's the real thing.

24:39.000 --> 24:41.000
 But that's what we're looking for.

24:41.000 --> 24:45.000
 Is there a subject of experience, a perspective on the world

24:45.000 --> 24:50.000
 from which things can go well or badly from that perspective?

24:50.000 --> 24:54.000
 So, our idea of what suffering looks like

24:54.000 --> 25:01.000
 comes from just watching ourselves when we're in pain.

25:01.000 --> 25:03.000
 Or when we're experiencing pleasure. It's not only...

25:03.000 --> 25:05.000
 Pleasure and pain.

25:05.000 --> 25:09.000
 And then you could actually push back on this,

25:09.000 --> 25:14.000
 but I would say that's how we kind of build an intuition about animals

25:14.000 --> 25:18.000
 is we can infer the similarities between humans and animals

25:18.000 --> 25:22.000
 and so infer that they're suffering or not based on certain things

25:22.000 --> 25:24.000
 and they're conscious or not.

25:24.000 --> 25:28.000
 So, what if robots...

25:28.000 --> 25:30.000
 You mentioned Google Maps.

25:30.000 --> 25:35.000
 And I've done this experiment, so I work in robotics just for my own self.

25:35.000 --> 25:37.000
 I have several Roomba robots

25:37.000 --> 25:42.000
 and I play with different speech interaction, voice based interaction.

25:42.000 --> 25:46.000
 And if the Roomba or the robot or Google Maps

25:46.000 --> 25:50.000
 shows any signs of pain, like screaming or moaning

25:50.000 --> 25:54.000
 or being displeased by something you've done,

25:54.000 --> 25:59.000
 that, in my mind, I can't help but immediately upgrade it.

25:59.000 --> 26:02.000
 And even when I myself programmed it in,

26:02.000 --> 26:07.000
 just having another entity that's now, for the moment, disjoint from me,

26:07.000 --> 26:11.000
 showing signs of pain, makes me feel like it is conscious.

26:11.000 --> 26:13.000
 Like, I immediately...

26:13.000 --> 26:18.000
 Whatever, I immediately realize that it's not, obviously,

26:18.000 --> 26:20.000
 that feeling is there.

26:20.000 --> 26:23.000
 So, sort of, I guess...

26:23.000 --> 26:26.000
 I guess, what do you think about a world

26:26.000 --> 26:32.000
 where Google Maps and Roombas are pretending to be conscious

26:32.000 --> 26:37.000
 and the descendants of apes are not smart enough to realize they're not

26:37.000 --> 26:41.000
 or whatever, or that is conscious, they appear to be conscious

26:41.000 --> 26:44.000
 and so you then have to give them rights.

26:44.000 --> 26:52.000
 The reason I'm asking that is that kind of capability may be closer than we realize.

26:52.000 --> 26:58.000
 Yes, that kind of capability may be closer,

26:58.000 --> 27:01.000
 but I don't think it follows that we have to give them rights.

27:01.000 --> 27:05.000
 I suppose the argument for saying that in those circumstances

27:05.000 --> 27:08.000
 we should give them rights is that if we don't,

27:08.000 --> 27:13.000
 we'll harden ourselves against other beings who are not robots

27:13.000 --> 27:15.000
 and who really do suffer.

27:15.000 --> 27:18.000
 That's a possibility that, you know,

27:18.000 --> 27:21.000
 if we get used to looking at it being suffering

27:21.000 --> 27:23.000
 and saying, yeah, we don't have to do anything about that,

27:23.000 --> 27:25.000
 that being doesn't have any rights,

27:25.000 --> 27:29.000
 maybe we'll feel the same about animals, for instance.

27:29.000 --> 27:35.000
 And interestingly, among philosophers and thinkers

27:35.000 --> 27:40.000
 who denied that we have any direct duties to animals,

27:40.000 --> 27:44.000
 and this includes people like Thomas Aquinas and Immanuel Kant,

27:44.000 --> 27:49.000
 they did say, yes, but still it's better not to be cruel to them,

27:49.000 --> 27:52.000
 not because of the suffering we're inflicting on the animals,

27:52.000 --> 27:57.000
 but because if we are, we may develop a cruel disposition

27:57.000 --> 28:00.000
 and this will be bad for humans, you know,

28:00.000 --> 28:02.000
 because we're more likely to be cruel to other humans

28:02.000 --> 28:05.000
 and that would be wrong.

28:05.000 --> 28:07.000
 But you don't accept that kind of...

28:07.000 --> 28:10.000
 I don't accept that as the basis of the argument

28:10.000 --> 28:12.000
 for why we shouldn't be cruel to animals.

28:12.000 --> 28:14.000
 I think the basis of the argument for why we shouldn't be cruel to animals

28:14.000 --> 28:16.000
 is just that we're inflicting suffering on them

28:16.000 --> 28:18.000
 and the suffering is a bad thing.

28:18.000 --> 28:23.000
 But possibly I might accept some sort of parallel of that argument

28:23.000 --> 28:28.000
 as a reason why you shouldn't be cruel to these robots

28:28.000 --> 28:30.000
 that mimic the symptoms of pain

28:30.000 --> 28:33.000
 if it's going to be harder for us to distinguish.

28:33.000 --> 28:36.000
 I would venture to say, I'd like to disagree with you

28:36.000 --> 28:39.000
 and with most people, I think.

28:39.000 --> 28:42.000
 At the risk of sounding crazy,

28:42.000 --> 28:47.000
 I would like to say that if that Roomba is dedicated

28:47.000 --> 28:50.000
 to faking the consciousness and the suffering,

28:50.000 --> 28:55.000
 I think it would be impossible for us...

28:55.000 --> 29:00.000
 I would like to apply the same argument as with animals to robots

29:00.000 --> 29:02.000
 that they deserve rights in that sense.

29:02.000 --> 29:07.000
 Now, we might outlaw the addition of those kinds of features into Roombas,

29:07.000 --> 29:11.000
 but once you do, I think...

29:11.000 --> 29:16.000
 I'm quite surprised by the upgrade in consciousness

29:16.000 --> 29:20.000
 that the display of suffering creates.

29:20.000 --> 29:22.000
 It's a totally open world,

29:22.000 --> 29:27.000
 but I'd like to just serve the difference between animals and other humans

29:27.000 --> 29:32.000
 is that in the robot case, we've added it in ourselves.

29:32.000 --> 29:37.000
 Therefore, we can say something about how real it is.

29:37.000 --> 29:41.000
 But I would like to say that the display of it is what makes it real.

29:41.000 --> 29:45.000
 And I'm not a philosopher, I'm not making that argument,

29:45.000 --> 29:48.000
 but I'd at least like to add that as a possibility.

29:48.000 --> 29:51.000
 And I've been surprised by it.

29:51.000 --> 29:55.000
 It's all I'm trying to sort of articulate poorly, I suppose.

29:55.000 --> 30:00.000
 So, there is a philosophical view has been held about humans,

30:00.000 --> 30:04.000
 which is rather like what you're talking about, and that's behaviorism.

30:04.000 --> 30:07.000
 So, behaviorism was employed both in psychology,

30:07.000 --> 30:10.000
 people like B.F. Skinner was a famous behaviorist,

30:10.000 --> 30:14.000
 but in psychology, it was more a kind of a,

30:14.000 --> 30:17.000
 what is it that makes this science where you need to have behavior

30:17.000 --> 30:20.000
 because that's what you can observe, you can't observe consciousness.

30:20.000 --> 30:24.000
 But in philosophy, the view is defended by people like Gilbert Ryle,

30:24.000 --> 30:26.000
 who was a professor of philosophy at Oxford,

30:26.000 --> 30:29.000
 wrote a book called The Concept of Mind,

30:29.000 --> 30:32.000
 in which, you know, in this kind of phase,

30:32.000 --> 30:35.000
 this is in the 40s of linguistic philosophy,

30:35.000 --> 30:39.000
 he said, well, the meaning of a term is its use,

30:39.000 --> 30:42.000
 and we use terms like so and so is in pain

30:42.000 --> 30:45.000
 when we see somebody writhing or screaming

30:45.000 --> 30:47.000
 or trying to escape some stimulus,

30:47.000 --> 30:49.000
 and that's the meaning of the term.

30:49.000 --> 30:53.000
 So, that's what it is to be in pain, and you point to the behavior.

30:53.000 --> 31:00.000
 And Norman Malcolm, who was another philosopher in the school from Cornell,

31:00.000 --> 31:04.000
 had the view that, you know, so, what is it to dream?

31:04.000 --> 31:07.000
 After all, we can't see other people's dreams.

31:07.000 --> 31:10.000
 Well, when people wake up and say,

31:10.000 --> 31:13.000
 I've just had a dream of, you know, here I was,

31:13.000 --> 31:17.000
 undressed, walking down the main street or whatever it is you've dreamt,

31:17.000 --> 31:19.000
 that's what it is to have a dream,

31:19.000 --> 31:22.000
 it's basically to wake up and recall something.

31:22.000 --> 31:27.000
 So, you could apply this to what you're talking about and say,

31:27.000 --> 31:31.000
 so, what it is to be in pain is to exhibit these symptoms of pain behavior,

31:31.000 --> 31:36.000
 and therefore, these robots are in pain, that's what the word means.

31:36.000 --> 31:39.000
 But nowadays, not many people think that

31:39.000 --> 31:42.000
 Riles kind of philosophical behaviorism is really very plausible.

31:42.000 --> 31:45.000
 So, I think they would say the same about your view.

31:45.000 --> 31:48.000
 So, yes, I just spoke with Noam Chomsky,

31:48.000 --> 31:54.000
 who basically was part of dismantling the behaviorist movement.

31:54.000 --> 32:00.000
 But, and I'm with that 100% for studying human behavior,

32:00.000 --> 32:03.000
 but I am one of the few people in the world

32:03.000 --> 32:08.000
 who has made rumbas scream in pain,

32:08.000 --> 32:14.000
 and I just don't know what to do with that empirical evidence,

32:14.000 --> 32:19.000
 because it's hard, sort of philosophically I agree,

32:19.000 --> 32:23.000
 but the only reason I philosophically agree in that case

32:23.000 --> 32:25.000
 is because I was the programmer,

32:25.000 --> 32:27.000
 but if somebody else was a programmer,

32:27.000 --> 32:29.000
 I'm not sure I would be able to interpret that well.

32:29.000 --> 32:37.000
 So, I think it's a new world that I was just curious what your thoughts are.

32:37.000 --> 32:46.000
 For now, you feel that the display of what we can kind of intellectually say

32:46.000 --> 32:50.000
 is a fake display of suffering is not suffering.

32:50.000 --> 32:53.000
 That's right. That would be my view.

32:53.000 --> 32:57.000
 But that's consistent, of course, with the idea that it's part of our nature

32:57.000 --> 33:01.000
 to respond to this display if it's reasonably authentically done.

33:01.000 --> 33:06.000
 And therefore, it's understandable that people would feel this

33:06.000 --> 33:11.000
 and maybe, as I said, it's even a good thing that they do feel it,

33:11.000 --> 33:13.000
 and you wouldn't want to harden yourself against it

33:13.000 --> 33:17.000
 because then you might harden yourself against beings who are really suffering.

33:17.000 --> 33:20.000
 But there's this line, you know, so you said,

33:20.000 --> 33:23.000
 once an artificial journal intelligence system,

33:23.000 --> 33:26.000
 a human level intelligence system, become conscious,

33:26.000 --> 33:28.000
 I guess if I could just linger on it.

33:28.000 --> 33:34.000
 Now, I've wrote really dumb programs that just say things that I told them to say,

33:34.000 --> 33:40.000
 but how do you know when a system like Alexa, which is officially complex,

33:40.000 --> 33:42.000
 that you can't introspect of how it works,

33:42.000 --> 33:48.000
 starts giving you signs of consciousness through natural language.

33:48.000 --> 33:52.000
 There's a feeling there's another entity there that's self aware,

33:52.000 --> 33:55.000
 that has a fear of death, a mortality,

33:55.000 --> 34:03.000
 that has awareness of itself that we kind of associate with other living creatures.

34:03.000 --> 34:07.000
 I guess I'm sort of trying to do the slippery slope from the very naive thing

34:07.000 --> 34:12.000
 where I started into something where it's sufficiently a black box

34:12.000 --> 34:16.000
 to where it's starting to feel like it's conscious.

34:16.000 --> 34:20.000
 Where's that threshold where you would start getting uncomfortable

34:20.000 --> 34:25.000
 with the idea of robot suffering, do you think?

34:25.000 --> 34:29.000
 I don't know enough about the programming that we're going to this really

34:29.000 --> 34:31.000
 to answer this question.

34:31.000 --> 34:37.000
 But I presume that somebody who does know more about this could look at the program

34:37.000 --> 34:43.000
 and see whether we can explain the behaviors in a parsimonious way

34:43.000 --> 34:49.000
 that doesn't require us to suggest that some sort of consciousness has emerged

34:49.000 --> 34:53.000
 or alternatively whether you're in a situation where you say,

34:53.000 --> 34:56.000
 I don't know how this is happening.

34:56.000 --> 35:01.000
 The program does generate a kind of artificial general intelligence

35:01.000 --> 35:08.000
 which starts to do things itself and is autonomous of the basic programming

35:08.000 --> 35:10.000
 that's set it up.

35:10.000 --> 35:15.000
 And so it's quite possible that actually we have achieved consciousness

35:15.000 --> 35:18.000
 in a system of artificial intelligence.

35:18.000 --> 35:22.000
 The approach that I work with most of the community is really excited about now

35:22.000 --> 35:26.000
 is with learning methods, so machine learning.

35:26.000 --> 35:31.000
 And the learning methods unfortunately are not capable of revealing

35:31.000 --> 35:34.000
 which is why somebody like Noam Chomsky criticizes them.

35:34.000 --> 35:37.000
 You've created powerful systems that are able to do certain things

35:37.000 --> 35:42.000
 without understanding the theory, the physics, the science of how it works.

35:42.000 --> 35:46.000
 And so it's possible if those are the kinds of methods that succeed

35:46.000 --> 35:52.000
 we won't be able to know exactly, sort of try to reduce,

35:52.000 --> 35:56.000
 try to find whether this thing is conscious or not,

35:56.000 --> 35:58.000
 this thing is intelligent or not.

35:58.000 --> 36:04.000
 It's simply giving, when we talk to it it displays wit and humor

36:04.000 --> 36:09.000
 and cleverness and emotion and fear

36:09.000 --> 36:14.000
 and then we won't be able to say where in the billions of nodes,

36:14.000 --> 36:20.000
 neurons in this artificial neural network is the fear coming from.

36:20.000 --> 36:24.000
 So in that case that's a really interesting place where we do now start

36:24.000 --> 36:28.000
 to return to behaviorism and say...

36:28.000 --> 36:34.000
 Yeah, that is an interesting issue.

36:34.000 --> 36:39.000
 I would say that if we have serious doubts and think it might be conscious

36:39.000 --> 36:43.000
 then we ought to try to give it the benefit of the doubt.

36:43.000 --> 36:47.000
 Just as I would say with animals, I think we can be highly confident

36:47.000 --> 36:52.000
 that vertebrates are conscious but when we get down

36:52.000 --> 36:57.000
 and some invertebrates like the octopus but with insects

36:57.000 --> 37:01.000
 it's much harder to be confident of that.

37:01.000 --> 37:04.000
 I think we should give them the benefit of the doubt where we can

37:04.000 --> 37:09.000
 which means I think it would be wrong to torture an insect

37:09.000 --> 37:13.000
 but doesn't necessarily mean it's wrong to slap a mosquito

37:13.000 --> 37:16.000
 that's about to bite you and stop you getting to sleep.

37:16.000 --> 37:22.000
 So I think you try to achieve some balance in these circumstances of uncertainty.

37:22.000 --> 37:26.000
 If it's okay with you, if you can go back just briefly.

37:26.000 --> 37:29.000
 So 44 years ago, like you mentioned, 40 plus years ago

37:29.000 --> 37:33.000
 you've written Animal Liberation, the classic book that started

37:33.000 --> 37:39.000
 that was a foundation of the movement of Animal Liberation.

37:39.000 --> 37:44.000
 Can you summarize the key set of ideas that are underpinning that book?

37:44.000 --> 37:52.000
 Certainly, the key idea that underlies that book is the concept of speciesism

37:52.000 --> 37:55.000
 which I did not invent that term.

37:55.000 --> 37:59.000
 I took it from a man called Richard Ryder who was in Oxford when I was

37:59.000 --> 38:05.000
 a pamphlet that he'd written about experiments on chimpanzees that used that term.

38:05.000 --> 38:09.000
 But I think I contributed to making it philosophically more precise

38:09.000 --> 38:12.000
 and to getting it into a broader audience.

38:12.000 --> 38:17.000
 And the idea is that we have a bias or a prejudice

38:17.000 --> 38:23.000
 against taking seriously the interests of beings who are not members of our species.

38:23.000 --> 38:27.000
 Just as in the past, Europeans, for example,

38:27.000 --> 38:31.000
 have had a bias against taking seriously the interests of Africans, racism.

38:31.000 --> 38:37.000
 And men have had a bias against taking seriously the interests of women, sexism.

38:37.000 --> 38:41.000
 So I think something analogous, not completely identical,

38:41.000 --> 38:46.000
 but something analogous, goes on and has gone on for a very long time

38:46.000 --> 38:50.000
 with the way humans see themselves vis a vis animals.

38:50.000 --> 38:54.000
 We see ourselves as more important.

38:54.000 --> 38:59.000
 We see animals as existing to serve our needs in various ways.

38:59.000 --> 39:03.000
 And you can find this very explicit in earlier philosophers

39:03.000 --> 39:06.000
 from Aristotle through to Kant and others.

39:06.000 --> 39:13.000
 And either we don't need to take their interests into account at all

39:13.000 --> 39:18.000
 or we can discount it because they're not humans.

39:18.000 --> 39:23.000
 They can't a little bit, but they don't count nearly as much as humans do.

39:23.000 --> 39:28.000
 My book argues that that attitude is responsible for a lot of the things

39:28.000 --> 39:32.000
 that we do to animals that are wrong, confining them indoors

39:32.000 --> 39:36.000
 in very crowded cramped conditions in factory farms

39:36.000 --> 39:39.000
 to produce meat or eggs or milk more cheaply,

39:39.000 --> 39:47.000
 using them in some research that's by no means essential for our survival or well being

39:47.000 --> 39:52.000
 and a whole lot, you know, some of the sports and things that we do to animals.

39:52.000 --> 40:01.000
 So I think that's unjustified because I think the significance of pain and suffering

40:01.000 --> 40:05.000
 does not depend on the species of the being who is in pain or suffering anymore

40:05.000 --> 40:10.000
 than it depends on the race or sex of the being who is in pain or suffering.

40:10.000 --> 40:16.000
 And I think we ought to rethink our treatment of animals along the lines of saying

40:16.000 --> 40:24.000
 if the pain is just as great in animal, then it's just as bad that it happens as if it were a human.

40:24.000 --> 40:29.000
 Maybe if I could ask, I apologize, hopefully it's not a ridiculous question,

40:29.000 --> 40:36.000
 but so as far as we know, we cannot communicate with animals through natural language,

40:36.000 --> 40:40.000
 but we would be able to communicate with robots.

40:40.000 --> 40:46.000
 So returning to sort of a small parallel between perhaps animals in the future of AI,

40:46.000 --> 40:53.000
 if we do create an AGI system or as we approach creating that AGI system,

40:53.000 --> 41:05.000
 what kind of questions would you ask her to try to intuit whether there is consciousness

41:05.000 --> 41:12.000
 or, more importantly, whether there's capacity to suffer?

41:12.000 --> 41:18.000
 I might ask the AGI what she was feeling.

41:18.000 --> 41:20.000
 Well, does she have feelings?

41:20.000 --> 41:25.000
 And if she says yes to describe those feelings, to describe what they were like,

41:25.000 --> 41:33.000
 to see what the phenomenal account of consciousness is like, that's one question.

41:33.000 --> 41:41.000
 I might also try to find out if the AGI has a sense of itself.

41:41.000 --> 41:46.000
 So for example, the idea, we often ask people,

41:46.000 --> 41:51.000
 suppose you're in a car accident and your brain were transplanted into someone else's body,

41:51.000 --> 41:56.000
 do you think you would survive or would it be the person whose body was still surviving,

41:56.000 --> 41:58.000
 your body having been destroyed?

41:58.000 --> 42:04.000
 And most people say, I think if my brain was transplanted along with my memories and so on, I would survive.

42:04.000 --> 42:08.000
 So we could ask AGI those kinds of questions.

42:08.000 --> 42:13.000
 If they were transferred to a different piece of hardware, would they survive?

42:13.000 --> 42:15.000
 What would survive?

42:15.000 --> 42:19.000
 Sort of on that line, another perhaps absurd question,

42:19.000 --> 42:25.000
 but do you think having a body is necessary for consciousness?

42:25.000 --> 42:31.000
 So do you think digital beings can suffer?

42:31.000 --> 42:37.000
 Presumably digital beings need to be running on some kind of hardware, right?

42:37.000 --> 42:42.000
 Yes, it ultimately boils down to, but this is exactly what you just said, is moving the brain.

42:42.000 --> 42:46.000
 So you could move it to a different kind of hardware, you know, and they could say, look,

42:46.000 --> 42:52.000
 your hardware is getting worn out, we're going to transfer you to a fresh piece of hardware,

42:52.000 --> 42:55.000
 so we're going to shut you down for a time.

42:55.000 --> 43:00.000
 But don't worry, you know, you'll be running very soon on a nice fresh piece of hardware.

43:00.000 --> 43:05.000
 And you could imagine this conscious AGI saying, that's fine, I don't mind having a little rest.

43:05.000 --> 43:08.000
 Just make sure you don't lose me or something like that.

43:08.000 --> 43:15.000
 Yeah, I mean, that's an interesting thought that even with us humans, the suffering is in the software.

43:15.000 --> 43:19.000
 We right now don't know how to repair the hardware.

43:19.000 --> 43:23.000
 But we're getting better at it and better in the idea.

43:23.000 --> 43:33.000
 I mean, a lot of some people dream about one day being able to transfer certain aspects of the software to another piece of hardware.

43:33.000 --> 43:34.000
 What do you think?

43:34.000 --> 43:42.000
 Just on that topic, there's been a lot of exciting innovation in brain computer interfaces.

43:42.000 --> 43:46.000
 I don't know if you're familiar with the companies like Neuralink with Elon Musk,

43:46.000 --> 43:51.000
 communicating both ways from a computer, being able to send, activate neurons,

43:51.000 --> 43:58.000
 and being able to read spikes from neurons with the dream of being able to expand,

43:58.000 --> 44:05.000
 instead of increase the bandwidth at which your brain can like look up articles on Wikipedia kind of thing,

44:05.000 --> 44:08.000
 to expand the knowledge capacity of the brain.

44:08.000 --> 44:15.000
 Do you think that notion is that interesting to you as the expansion of the human mind?

44:15.000 --> 44:17.000
 Yes, that's very interesting.

44:17.000 --> 44:20.000
 I'd love to be able to have that increased bandwidth.

44:20.000 --> 44:24.000
 And I, you know, I want better access to my memory, I have to say too.

44:24.000 --> 44:30.000
 As a yet older, you know, I talked to my wife about things that we did 20 years ago or something.

44:30.000 --> 44:33.000
 Her memory is often better about particular events.

44:33.000 --> 44:35.000
 Where were we? Who was at that event?

44:35.000 --> 44:37.000
 What did he or she wear even?

44:37.000 --> 44:41.000
 She may know and I have not the faintest idea about this, but perhaps it's somewhere in my memory.

44:41.000 --> 44:47.000
 And if I had this extended memory, I could, I could search that particular year and re run those things.

44:47.000 --> 44:49.000
 I think that would be great.

44:49.000 --> 44:55.000
 In some sense, we already have that by storing so much of our data online, like pictures of different events.

44:55.000 --> 45:00.000
 Yes. Well, Gmail is fantastic for that because, you know, people, people email me as if they know me well.

45:00.000 --> 45:03.000
 And I haven't got a clue who they are, but then I searched for their name.

45:03.000 --> 45:07.000
 I emailed me in 2007 and I know who they are now.

45:07.000 --> 45:11.000
 Yeah, so we're already taking the first steps already.

45:11.000 --> 45:19.000
 So on the flip side of AI, people like Stuart Russell and others focus on the control problem, value alignment in AI,

45:19.000 --> 45:25.000
 which is the problem of making sure we build systems that align to our own values, our ethics.

45:25.000 --> 45:31.000
 Do you think sort of high level, how do we go about building systems?

45:31.000 --> 45:36.000
 Do you think is it possible that align with our values, align with our human ethics?

45:36.000 --> 45:39.000
 Or living being ethics?

45:39.000 --> 45:43.000
 Presumably, it's possible to do that.

45:43.000 --> 45:51.000
 I know that a lot of people who think that there's a real danger that we won't, that we'll more or less accidentally lose control of AI.

45:51.000 --> 45:56.000
 Do you have that fear yourself personally?

45:56.000 --> 45:58.000
 I'm not quite sure what to think.

45:58.000 --> 46:04.000
 I talked to philosophers like Nick Bostrom and Toby Ord and they think that this is a real problem.

46:04.000 --> 46:07.000
 We need to worry about.

46:07.000 --> 46:14.000
 Then I talked to people who work for Microsoft or DeepMind or somebody and they say,

46:14.000 --> 46:19.000
 no, we're not really that close to producing AI, super intelligence.

46:19.000 --> 46:25.000
 So if you look at Nick Bostrom, the argument, it's very hard to defend.

46:25.000 --> 46:32.000
 So of course, I am a self engineer AI system, so I'm more with the DeepMind folks where it seems that we're really far away.

46:32.000 --> 46:39.000
 But then the counter argument is, is there any fundamental reason that we'll never achieve it?

46:39.000 --> 46:44.000
 And if not, then eventually there'll be a dire existential risk.

46:44.000 --> 46:46.000
 So we should be concerned about it.

46:46.000 --> 46:52.000
 And do you have, do you have, do you find that argument at all appealing in this domain or any domain?

46:52.000 --> 46:56.000
 That eventually this will be a problem, so we should be worried about it?

46:56.000 --> 46:58.000
 Yes, I think it's a problem.

46:58.000 --> 47:03.000
 I think that's a valid point.

47:03.000 --> 47:11.000
 Of course, when you say eventually, that raises the question, how far off is that?

47:11.000 --> 47:13.000
 And is there something that we can do about it now?

47:13.000 --> 47:23.000
 Because if we're talking about this is going to be 100 years in the future and you consider how rapidly our knowledge of artificial intelligence has grown in the last 10 or 20 years,

47:23.000 --> 47:33.000
 it seems unlikely that there's anything much we could do now that would influence whether this is going to happen 100 years in the future.

47:33.000 --> 47:41.000
 People in 80 years in the future would be in a much better position to say, this is what we need to do to prevent this happening than we are now.

47:41.000 --> 47:44.000
 So to some extent, I find that reassuring.

47:44.000 --> 47:55.000
 But I'm all in favor of some people doing research into this to see if indeed it is that far off or if we are in a position to do something about it sooner.

47:55.000 --> 48:00.000
 I'm very much of the view that extinction is a terrible thing.

48:00.000 --> 48:11.000
 And therefore, even if the risk of extinction is very small, if we can reduce that risk, that's something that we ought to do.

48:11.000 --> 48:20.000
 My disagreement with some of these people who talk about long term risks, extinction risks, is only about how much priority that should have as compared to present questions.

48:20.000 --> 48:28.000
 So essentially, if you look at the math of it from a utilitarian perspective, if it's existential risk so everybody dies,

48:28.000 --> 48:39.000
 it feels like an infinity in the math equation that makes the math with the priorities difficult to do.

48:39.000 --> 48:48.000
 That if we don't know the time scale, and you can legitimately argue that it's not zero probability that it'll happen tomorrow,

48:48.000 --> 48:57.000
 that how do you deal with these kinds of existential risks like from nuclear war, from nuclear weapons, from biological weapons,

48:57.000 --> 49:04.000
 from I'm not sure global warming falls into that category because global warming is a lot more gradual.

49:04.000 --> 49:10.000
 And people say it's not an existential risk because there'll always be possibilities of some humans existing,

49:10.000 --> 49:14.000
 farming Antarctica or Northern Siberia or something of that sort.

49:14.000 --> 49:25.000
 But you don't find the complete existential risks a fundamental, like an overriding part of the equations of ethics.

49:25.000 --> 49:32.000
 No, certainly if you treat it as an infinity, then it plays havoc with any calculations.

49:32.000 --> 49:35.000
 But arguably we shouldn't.

49:35.000 --> 49:46.000
 One of the ethical assumptions that goes into this is that the loss of future lives, that is of merely possible lives of beings who may never exist at all,

49:46.000 --> 49:54.000
 is in some way comparable to the sufferings or deaths of people who do exist at some point.

49:54.000 --> 49:57.000
 And that's not clear to me.

49:57.000 --> 50:01.000
 I think there's a case for saying that, but I also think there's a case for taking the other view.

50:01.000 --> 50:04.000
 So that has some impact on it.

50:04.000 --> 50:12.000
 Of course, you might say, ah, yes, but still if there's some uncertainty about this and the costs of extinction are infinite,

50:12.000 --> 50:15.000
 then still it's going to overwhelm everything else.

50:15.000 --> 50:20.000
 But I suppose I'm not convinced of that.

50:20.000 --> 50:23.000
 I'm not convinced that it's really infinite here.

50:23.000 --> 50:31.000
 And even Nick Bostrom in his discussion of this doesn't claim that there'll be an infinite number of lives lived.

50:31.000 --> 50:33.000
 What is it, 10 to the 56th or something?

50:33.000 --> 50:36.000
 It's a vast number that I think he calculates.

50:36.000 --> 50:45.000
 This is assuming we can upload consciousness onto these, you know, digital forms and therefore there'll be much more energy efficient.

50:45.000 --> 50:48.000
 But he calculates the amount of energy in the universe or something like that.

50:48.000 --> 50:55.000
 So the numbers are vast, but not infinite, which gives you some prospect maybe of resisting some of the argument.

50:55.000 --> 51:01.000
 The beautiful thing with Nick's arguments is he quickly jumps from the individual scale to the universal scale,

51:01.000 --> 51:08.000
 which is just awe inspiring to think of when you think about the entirety of the span of time of the universe.

51:08.000 --> 51:15.000
 It's both interesting from a computer science perspective, AI perspective and from an ethical perspective, the idea of utilitarianism.

51:15.000 --> 51:19.000
 Could you say what is utilitarianism?

51:19.000 --> 51:28.000
 Utilitarianism is the ethical view that the right thing to do is the act that has the greatest expected utility,

51:28.000 --> 51:34.000
 where what that means is it's the act that will produce the best consequences,

51:34.000 --> 51:40.000
 discounted by the odds that you won't be able to produce those consequences that something will go wrong.

51:40.000 --> 51:46.000
 But in simple case, let's assume we have certainty about what the consequence of actions will be,

51:46.000 --> 51:50.000
 then the right action is the action that will produce the best consequences.

51:50.000 --> 51:58.000
 Is that always, and by the way, there's a bunch of nuanced stuff that you talked with Sam Harris on this podcast on that people should go listen to.

51:58.000 --> 52:00.000
 It's great.

52:00.000 --> 52:03.000
 It's like two hours of moral philosophy discussion.

52:03.000 --> 52:05.000
 But is that an easy calculation?

52:05.000 --> 52:13.000
 No, it's a difficult calculation and actually there's one thing that I need to add and that is utilitarians,

52:13.000 --> 52:21.000
 certainly the classical utilitarians think that by best consequences, we're talking about happiness and the absence of pain and suffering.

52:21.000 --> 52:29.000
 There are other consequentialists who are not really utilitarians who say there are different things that could be good consequences.

52:29.000 --> 52:35.000
 Justice, freedom, human dignity, knowledge, they all count as good consequences too.

52:35.000 --> 52:40.000
 And that makes the calculations even more difficult because then you need to know how to balance these things off.

52:40.000 --> 52:48.000
 If you are just talking about well being using that term to express happiness and the absence of suffering,

52:48.000 --> 52:56.000
 I think that the calculation becomes more manageable in a philosophical sense.

52:56.000 --> 52:59.000
 It's still in practice, we don't know how to do it.

52:59.000 --> 53:02.000
 We don't know how to measure quantities of happiness and misery.

53:02.000 --> 53:08.000
 We don't know how to calculate the probabilities that different actions will produce this or that.

53:08.000 --> 53:25.000
 So at best we can use it as a rough guide to different actions and one where we have to focus on the short term consequences because we just can't really predict all of the longer term ramifications.

53:25.000 --> 53:32.000
 So what about the extreme suffering of very small groups?

53:32.000 --> 53:37.000
 Utilitarianism is focused on the overall aggregate, right?

53:37.000 --> 53:41.000
 Would you say you yourself are utilitarian?

53:41.000 --> 53:43.000
 Yes, I'm utilitarian.

53:43.000 --> 53:55.000
 What do you make of the difficult, ethical, maybe poetic suffering of very few individuals?

53:55.000 --> 54:00.000
 I think it's possible that that gets overridden by benefits to very large numbers of individuals.

54:00.000 --> 54:03.000
 I think that can be the right answer.

54:03.000 --> 54:12.000
 But before we conclude that it is the right answer, we have to know how severe the suffering is and how that compares with the benefits.

54:12.000 --> 54:27.000
 So I tend to think that extreme suffering is worse than or is further, if you like, below the neutral level than extreme happiness or bliss is above it.

54:27.000 --> 54:36.000
 So when I think about the worst experience is possible and the best experience is possible, I don't think of them as equidistant from neutral.

54:36.000 --> 54:43.000
 So like it's a scale that goes from minus 100 through zero as a neutral level to plus 100.

54:43.000 --> 54:52.000
 Because I know that I would not exchange an hour of my most pleasurable experiences for an hour of my most painful experiences.

54:52.000 --> 55:01.000
 Even I wouldn't have an hour of my most painful experiences even for two hours or 10 hours of my most painful experiences.

55:01.000 --> 55:03.000
 Did I say that correctly?

55:03.000 --> 55:07.000
 Yeah, maybe 20 hours then. Is it 21? What's the exchange rate?

55:07.000 --> 55:11.000
 So that's the question. What is the exchange rate? But I think it can be quite high.

55:11.000 --> 55:21.000
 So that's why you shouldn't just assume that it's okay to make one person suffer extremely in order to make two people much better off.

55:21.000 --> 55:23.000
 It might be a much larger number.

55:23.000 --> 55:39.000
 But at some point, I do think you should aggregate and the result will be even though it violates our intuitions of justice and fairness, whatever it might be, giving priority to those who are worse off.

55:39.000 --> 55:43.000
 At some point, I still think that will be the right thing to do.

55:43.000 --> 55:46.000
 Yeah, some complicated nonlinear function.

55:46.000 --> 55:55.000
 Can I ask a sort of out there question is the more and more we put our data out there, the more we're able to measure a bunch of factors of each of our individual human lives.

55:55.000 --> 56:02.000
 And I can foresee the ability to estimate well being over the whatever we public.

56:02.000 --> 56:08.000
 We together collectively agree and is a good objective function for from a utilitarian perspective.

56:08.000 --> 56:28.000
 Do you think it will be possible and is a good idea to push that kind of analysis to make then public decisions, perhaps with the help of AI, that here's a tax rate, here's a tax rate at which well being will be optimized.

56:28.000 --> 56:32.000
 Yeah, that would be great if we really knew that, if we really could calculate that.

56:32.000 --> 56:41.000
 No, but do you think it's possible to converge towards an agreement amongst humans towards an objective function or is it just a hopeless pursuit?

56:41.000 --> 56:43.000
 I don't think it's hopeless.

56:43.000 --> 56:55.000
 I think it would be difficult to get converged towards agreement at least at present because some people would say, you know, I've got different views about justice and I think you ought to give priority to those who are worse off.

56:55.000 --> 57:05.000
 Even though I acknowledge that the gains that the worse off are making are less than the gains that those who are sort of medium badly off could be making.

57:05.000 --> 57:09.000
 So we still have all of these intuitions that we we argue about.

57:09.000 --> 57:17.000
 So I don't think we would get agreement, but the fact that we wouldn't get agreement doesn't show that there isn't a right answer there.

57:17.000 --> 57:21.000
 Do you think who gets to say what is right and wrong?

57:21.000 --> 57:26.000
 Do you think there's place for ethics oversight from from the government?

57:26.000 --> 57:33.000
 So I'm thinking in the case of AI overseeing what is what kind of decisions they can make and not.

57:33.000 --> 57:50.000
 But also if you look at animal animal rights or rather not rights or perhaps rights, but the ideas you've explored in animal liberation, who gets to so you eloquently and beautifully write in your book that this here, you know, we shouldn't do this.

57:50.000 --> 57:53.000
 But is there some harder rules that should be imposed?

57:53.000 --> 58:01.000
 Or is this a collective thing we converse towards the society and thereby make the better and better ethical decisions?

58:01.000 --> 58:10.000
 Politically, I'm still a Democrat despite looking at the flaws in democracy and the way it doesn't work always very well.

58:10.000 --> 58:20.000
 So I don't see a better option than allowing the public to vote for governments in accordance with their policies.

58:20.000 --> 58:34.000
 And I hope that they will vote for policies, policies that reduce the suffering of animals and reduce the suffering of distant humans, whether geographically distant or distant because they're future humans.

58:34.000 --> 58:40.000
 But I recognize that democracy isn't really well set up to do that.

58:40.000 --> 58:51.000
 And in a sense, you could imagine a wise and benevolent, you know, omnibenevolent leader who would do that better than democracies could.

58:51.000 --> 59:01.000
 But in the world in which we live, it's difficult to imagine that this leader isn't going to be corrupted by a variety of influences.

59:01.000 --> 59:12.000
 You know, we've had so many examples of people who've taken power with good intentions and then have ended up being corrupt and favoring themselves.

59:12.000 --> 59:20.000
 So I don't know, you know, that's why as I say, I don't know that we have a better system than democracy to make these decisions.

59:20.000 --> 59:41.000
 Well, so you also discussed effective altruism, which is a mechanism for going around government, for putting the power in the hands of the people to donate money towards causes to help, you know, to, you know, remove the middleman and give it directly to the cause that they care about.

59:41.000 --> 59:51.000
 Sort of maybe this is a good time to ask you've 10 years ago wrote the life you can save that's now I think available for free online.

59:51.000 --> 59:52.000
 That's right.

59:52.000 --> 59:58.000
 You can download either the ebook or the audio book free from the life you can save.org.

59:58.000 --> 1:00:03.000
 And what are the key ideas that you present in the book?

1:00:03.000 --> 1:00:13.000
 The main thing I want to do in the book is to make people realize that it's not difficult to help people in extreme poverty.

1:00:13.000 --> 1:00:25.000
 That there are highly effective organizations now that are doing this that they've been independently assessed and verified by research teams that are expert in this area.

1:00:25.000 --> 1:00:44.000
 And that it's a fulfilling thing to do to, for at least part of your life, you know, we can't all be saints, but at least one of your goals should be to really make a positive contribution to the world and to do something to help people who through no fault of their own are in very dire circumstances and

1:00:44.000 --> 1:00:52.000
 and living a life that is barely or perhaps not at all a decent life for a human being to live.

1:00:52.000 --> 1:01:09.000
 So you describe a minimum ethical standard of giving what what advice would you give to people that want to be effectively altruistic in their life like live an effective altruism life.

1:01:09.000 --> 1:01:14.000
 There are many different kinds of ways of living as an effective altruist.

1:01:14.000 --> 1:01:27.000
 And if you're at the point where you're thinking about your long term career, I'd recommend you take a look at a website called 80,000 hours, 80,000 hours dot org, which looks at ethical career choices.

1:01:27.000 --> 1:01:43.000
 And they range from, for example, going to work on Wall Street so that you can earn a huge amount of money and then donate most of it to effective charities to going to work for a really good nonprofit organization so that you can directly use your

1:01:43.000 --> 1:01:55.000
 skills and ability and hard work to further a good cause or perhaps going into politics, maybe small chances but big payoffs in politics.

1:01:55.000 --> 1:02:01.000
 Go to work in the public service where if you're talented you might rise to a high level where you can influence decisions.

1:02:01.000 --> 1:02:05.000
 Do research in an area where the payoffs could be great.

1:02:05.000 --> 1:02:16.000
 There are a lot of different opportunities but too few people are even thinking about those questions. They're just going along in some sort of preordained rut to particular careers.

1:02:16.000 --> 1:02:25.000
 Maybe they think they'll earn a lot of money and have a comfortable life but they may not find that as fulfilling as actually knowing that they're making a positive difference to the world.

1:02:25.000 --> 1:02:37.000
 What about in terms of, so that's like long term 80,000 hours, shorter term giving part of, well actually it's a part of that and go to work at Wall Street.

1:02:37.000 --> 1:02:52.000
 If you would like to give a percentage of your income that you talk about and life you can save, I mean I was looking through it's quite a compelling, I mean I'm just a dumb engineer so I like there's simple rules.

1:02:52.000 --> 1:02:59.000
 So I do actually set out suggested levels of giving because people often ask me about this.

1:02:59.000 --> 1:03:08.000
 A popular answer is give 10% the traditional tithes that's recommended in Christianity and also Judaism.

1:03:08.000 --> 1:03:13.000
 But why should it be the same percentage irrespective of your income?

1:03:13.000 --> 1:03:23.000
 Tax scales reflect the idea that the more income you have the more you can pay tax and I think the same is true in what you can give.

1:03:23.000 --> 1:03:34.000
 So I do set out a progressive donor scale which starts at 1% for people on modest incomes and rises to 33 and a third percent for people who are really earning a lot.

1:03:34.000 --> 1:03:45.000
 And my idea is that I don't think any of these amounts really impose real hardship on people because they are progressive and get to income.

1:03:45.000 --> 1:04:01.000
 So I think anybody can do this and can know that they're doing something significant to play their part in reducing the huge gap between people in extreme poverty in the world and people living affluent lives.

1:04:01.000 --> 1:04:21.000
 And aside from it being an ethical life it's one they find more fulfilling because like there's something about our human nature that or some of our human natures maybe most of our human nature that enjoys doing the ethical thing.

1:04:21.000 --> 1:04:30.000
 Yes I make both those arguments that it is an ethical requirement in the kind of world we live in today to help people in great need when we can easily do so.

1:04:30.000 --> 1:04:40.000
 But also that it is a rewarding thing and there's good psychological research showing that people who give more tend to be more satisfied with their lives.

1:04:40.000 --> 1:04:53.000
 And I think this has something to do with with having a purpose that's larger than yourself and therefore never being if you like never never been bored sitting around oh you know what will I do next I've got nothing to do.

1:04:53.000 --> 1:04:59.000
 In a world like this there are many good things that you can do and enjoy doing them.

1:04:59.000 --> 1:05:15.000
 Plus you're working with other people in the effective altruism movement who are forming a community of other people with similar ideas and they tend to be interesting thoughtful and good people as well and having friends of that sort is another big contribution to having a good life.

1:05:15.000 --> 1:05:18.000
 So we talked about.

1:05:18.000 --> 1:05:22.000
 Big things that are beyond ourselves but we were.

1:05:22.000 --> 1:05:27.000
 We're also just human and mortal do you ponder your own mortality.

1:05:27.000 --> 1:05:31.000
 Is there insights about your philosophy the ethics that you gain from.

1:05:31.000 --> 1:05:35.000
 Pondering your own mortality.

1:05:35.000 --> 1:05:42.000
 Clearly you know as you get into your seventies you can't help thinking about your own mortality.

1:05:42.000 --> 1:05:46.000
 But I don't know that I have great insights into that from my philosophy.

1:05:46.000 --> 1:05:56.000
 I don't think there's anything after the death of my body assuming that we won't be able to upload my mind into anything at the time when I die.

1:05:56.000 --> 1:06:00.000
 So I don't think there's any afterlife for anything to look forward to in that sense.

1:06:00.000 --> 1:06:04.000
 Fear death so if you look at Ernest Becker and.

1:06:04.000 --> 1:06:08.000
 Describing the motivating aspects.

1:06:08.000 --> 1:06:14.000
 Of the our ability to be cognizant of our mortality.

1:06:14.000 --> 1:06:20.000
 Do you have any of those elements in your driving your motivation life.

1:06:20.000 --> 1:06:28.000
 I suppose the fact that you have only a limited time to achieve the things that you want to achieve gives you some sort of motivation to.

1:06:28.000 --> 1:06:35.000
 Get going and achieving them and if we thought we're immortal we might say I can put that off for another decade or two.

1:06:35.000 --> 1:06:44.000
 So there's that about it but otherwise you know I'd rather have more time to do more I'd also like to be able to see.

1:06:44.000 --> 1:06:53.000
 How things go that I'm interested in is climate change going to turn out to be as dire as a lot of scientists say that it is going to be.

1:06:53.000 --> 1:07:02.000
 Will we somehow scrape through with less damage than we thought I'd really like to know the answers to those questions but I guess I'm not going to.

1:07:02.000 --> 1:07:10.000
 Well you said there's nothing afterwards so let me ask the even more absurd question what do you think is the meaning of it all.

1:07:10.000 --> 1:07:21.000
 I think the meaning of life is the meaning we give to it I don't think that we were brought into the universe for any kind of larger purpose.

1:07:21.000 --> 1:07:30.000
 But given that we exist I think we can recognize that some things are objectively bad.

1:07:30.000 --> 1:07:40.000
 Extreme suffering is an example and other things are objectively good like having a rich fulfilling enjoyable pleasurable life.

1:07:40.000 --> 1:07:47.000
 And we can try to do our part in reducing the bad things and increasing the good things.

1:07:47.000 --> 1:07:55.000
 So one way the meaning is to do a little bit more of the good things objectively good things and a little bit less of the bad things.

1:07:55.000 --> 1:08:08.000
 Yes do as much of the good things as you can and as little of the bad things beautifully put I don't think there's a better place to end it thank you so much for talking today thanks very much like it's been really interesting talking to you.

1:08:08.000 --> 1:08:15.000
 Thanks for listening to this conversation with Peter singer and thank you to our sponsors cash app and masterclass.

1:08:15.000 --> 1:08:25.000
 Please consider supporting the podcast by downloading cash app and use the code Lex podcast and signing up and masterclass.com slash Lex.

1:08:25.000 --> 1:08:34.000
 Click the links by all the stuff is the best way to support this podcast and the journey I'm on my research and startup.

1:08:34.000 --> 1:08:48.000
 If you enjoy this thing subscribe on YouTube review it with 5,000 up a podcast support on Patreon or connect with me on Twitter at Lex freedman spelled without the E just F R I D M A N.

1:08:48.000 --> 1:08:52.000
 And now let me leave you some words from Peter singer.

1:08:52.000 --> 1:08:57.000
 Well one generation finds ridiculous the next accepts.

1:08:57.000 --> 1:09:02.000
 And the third shutters when looks back what the first did.

1:09:02.000 --> 1:09:05.000
 Thank you for listening and hope to see you next time.

