WEBVTT

00:00.000 --> 00:05.920
 The following is a conversation with Russ Tedrick, a roboticist and professor at MIT and vice

00:05.920 --> 00:13.840
 president of robotics research at Toyota Research Institute, or TRI. He works on control of robots

00:13.840 --> 00:19.920
 in interesting, complicated, underactuated stochastic, difficult to model situations.

00:19.920 --> 00:27.200
 He's a great teacher and a great person, one of my favorites at MIT. We get into a lot of topics

00:27.200 --> 00:34.320
 in this conversation from his time leading MIT's Dabra Robotics Challenge team to the awesome fact

00:34.320 --> 00:42.320
 that he often runs close to a marathon a day to and from work barefoot. For a world class roboticist

00:42.320 --> 00:48.480
 interested in elegant efficient control of underactory dynamical systems like the human body,

00:49.200 --> 00:52.880
 this fact makes Russ one of the most fascinating people I know.

00:52.880 --> 01:00.880
 Quick summary of the ads. Three sponsors. Magic Spoon Serial, BetterHelp, and ExpressVPN. Please

01:00.880 --> 01:07.840
 consider supporting this podcast by going to magicspoon.com slash lex and using code lex at checkout,

01:07.840 --> 01:15.120
 going to betterhelp.com slash lex and signing up at expressvpn.com slash lex pod. Click the links

01:15.120 --> 01:20.960
 in the description, buy the stuff, get the discount, it really is the best way to support this podcast.

01:20.960 --> 01:26.080
 If you enjoy this thing, subscribe on YouTube, review it with 5 stars on Apple Podcasts,

01:26.080 --> 01:30.400
 support it on Patreon, or connect with me on Twitter at Lex Freedman.

01:31.120 --> 01:35.280
 As usual, I'll do a few minutes of ads now and never any ads in the middle that can break

01:35.280 --> 01:42.400
 the flow of the conversation. This episode is supported by Magic Spoon Low Carb Keto Friendly

01:42.400 --> 01:48.800
 Serial. I've been on a mix of keto or carnivore diet for a very long time now. That means eating

01:48.800 --> 01:54.800
 very little carbs. I used to love cereal. Obviously, most of crazy amounts of sugar,

01:54.800 --> 02:01.360
 which is terrible for you. So I quit years ago. But Magic Spoon is a totally new thing. Zero sugar,

02:01.360 --> 02:07.840
 11 grams of protein, and only three net grams of carbs. It tastes delicious. It has a bunch of

02:07.840 --> 02:13.120
 flavors. They're all good. But if you know what's good for you, you'll go with cocoa, my favorite

02:13.120 --> 02:19.360
 flavor, and the flavor of champions. Click the magic spoon dot com slash Lex link in the description,

02:19.360 --> 02:25.680
 use code Lex at checkout to get the discount and to let them know I sent you. So buy all of their

02:25.680 --> 02:32.400
 cereal. It's delicious and good for you. You won't regret it. The show is also sponsored by Better

02:32.400 --> 02:40.080
 Help spelled H E L P help. Check it out at better help dot com slash Lex. They figure out what you

02:40.080 --> 02:45.600
 need and match you with a licensed professional therapist in under 48 hours. It's not a crisis

02:45.600 --> 02:51.760
 line. It's not self help. It is professional counseling done securely online. As you may know,

02:51.760 --> 02:56.960
 I'm a bit from the David Goggins line of creatures and so have some demons to contend with,

02:56.960 --> 03:03.600
 usually on long runs or all nighters full of self doubt. I think suffering is essential for creation.

03:04.160 --> 03:09.440
 But you can suffer beautifully in a way that doesn't destroy you. For most people, I think a

03:09.440 --> 03:14.880
 good therapist can help in this. So it's the least worth a try. Check out the reviews. They're all

03:14.880 --> 03:21.520
 good. It's easy, private, affordable, available worldwide. You can communicate by text anytime

03:21.520 --> 03:27.520
 and schedule weekly audio and video sessions. Check it out at better help dot com slash Lex.

03:28.400 --> 03:35.040
 This show is also sponsored by Express VPN. Get it at express VPN dot com slash Lex pod to get a

03:35.040 --> 03:41.040
 discount and to support this podcast. Have you ever watched the office? If you have, you probably

03:41.040 --> 03:47.600
 know it's based on a UK series also called the office, not to stir up trouble. But I personally

03:47.600 --> 03:52.960
 think the British version is actually more brilliant than the American one. But both are amazing.

03:52.960 --> 03:58.720
 Anyway, there are actually nine other countries with their own version of the office. You can get

03:58.720 --> 04:05.120
 access to them with no geo restriction when you use Express VPN. It lets you control where you want

04:05.120 --> 04:10.720
 sites to think you're located. You can choose from nearly 100 different countries, giving you

04:10.720 --> 04:17.280
 access to content that isn't available in your region. So again, get it on any device at express

04:17.280 --> 04:25.440
 VPN dot com slash Lex pod to get an extra three months free and to support this podcast. And now

04:25.440 --> 04:33.280
 here's my conversation with Russ Tedrick. What is the most beautiful motion of animal or robot

04:33.280 --> 04:39.680
 that you've ever seen? I think the most beautiful motion of a robot has to be the passive dynamic

04:39.680 --> 04:44.160
 walkers. I think there's just something fundamentally beautiful. The ones in particular

04:44.160 --> 04:51.440
 that Steve Collins built with Andy Ruina at Cornell, a 3d walking machine. So it was not

04:51.440 --> 04:58.240
 confined to a boom or a plane that you put it on top of a small ramp, give it a little push.

04:58.960 --> 05:05.280
 It's powered only by gravity, no controllers, no batteries whatsoever. It just falls down the ramp.

05:06.080 --> 05:12.160
 And at the time, it looked more natural, more graceful, more human like than any robot we'd

05:12.160 --> 05:18.320
 seen to date, powered only by gravity. How does it work? Well, okay, the simplest model,

05:18.320 --> 05:22.480
 it's kind of like a slinky, it's like an elaborate slinky. One of the simplest models we

05:22.480 --> 05:28.560
 used to think about it is actually a rimless wheel. So imagine taking a bicycle wheel,

05:28.560 --> 05:33.520
 but take the rim off. So it's now just got a bunch of spokes. If you give that a push,

05:33.520 --> 05:38.320
 it still wants to roll down the ramp. But every time it's foot, it's spoke comes around and hits

05:38.320 --> 05:44.240
 the ground, it loses a little energy. Every time it takes a step forward, it gains a little energy.

05:44.240 --> 05:49.680
 Those things can come into perfect balance. And actually, they want to, it's a stable phenomenon.

05:49.680 --> 05:55.120
 If it's going too slow, it'll speed up. If it's going too fast, it'll slow down. And it comes into

05:55.120 --> 06:01.760
 a stable periodic motion. Now, you can take that rimless wheel, which doesn't look very much like

06:01.760 --> 06:07.680
 a human walking, take all the extra spokes away, put a hinge in the middle. Now it's two legs.

06:07.680 --> 06:12.400
 That's called our compass gate walker. That can still, you give it a little push,

06:12.400 --> 06:18.080
 starts falling down a ramp. It looks a little bit more like walking. At least it's a biped.

06:19.520 --> 06:24.480
 But what Steve and Andy and Ted McGeer started the whole exercise, but what Steve and Andy did

06:24.480 --> 06:30.640
 was they took it to this beautiful conclusion where they built something that had knees,

06:30.640 --> 06:37.600
 arms, a torso, the arms swung naturally. Give it a little push and that looked like a stroll

06:37.600 --> 06:41.360
 through the park. How do you design something like that? I mean, is that art or science?

06:41.360 --> 06:46.880
 It's on the boundary. I think there's a science to getting close to the solution.

06:47.440 --> 06:51.120
 I think there's certainly art in the way that they made a beautiful robot,

06:51.920 --> 06:58.800
 but then the finesse, because they were working with a system that wasn't perfectly modeled,

06:58.800 --> 07:04.320
 wasn't perfectly controlled, there's all these little tricks that you have to tune the suction

07:04.320 --> 07:09.200
 cups at the knees, for instance, so that they stick, but then they release at just the right

07:09.200 --> 07:14.320
 time or there's all these little tricks of the trade, which really are art, but it was a point.

07:14.320 --> 07:21.040
 I mean, it made the point. At that time, the best walking robot in the world was Hondo's Asamo,

07:21.760 --> 07:27.280
 absolutely marvel of modern engineering. This was in the 97 when they first released,

07:27.280 --> 07:31.920
 it sort of announced P2 and then it went through. It was Asamo by then in 2004.

07:31.920 --> 07:40.960
 It looks like this very cautious walking. You're walking on hot coals or something like that.

07:42.240 --> 07:46.880
 I think it gets a bad rap. Asamo is a beautiful machine. It does walk with its knees bent.

07:46.880 --> 07:51.600
 Our Atlas walking had its knees bent, but actually, Asamo was pretty fantastic,

07:52.160 --> 07:59.120
 but it wasn't energy efficient. Neither was Atlas when we worked on Atlas. None of our robots

07:59.120 --> 08:07.760
 that have been that complicated have been very energy efficient, but there's a thing that happens

08:07.760 --> 08:13.120
 when you do control, when you try to control a system of that complexity. You try to use your

08:13.120 --> 08:19.680
 motors to basically counteract gravity. Take whatever the world's doing to you and push back,

08:20.560 --> 08:25.760
 erase the dynamics of the world and impose the dynamics you want because you can make them

08:25.760 --> 08:34.400
 simple and analyzable, mathematically simple. This was a very sort of beautiful example that

08:34.400 --> 08:40.640
 you don't have to do that. You can just let physics do most of the work and you just have

08:40.640 --> 08:44.560
 to give it a little bit of energy. This one only walked down a ramp. It would never walk on the flat.

08:45.120 --> 08:47.360
 To walk on the flat, you have to give a little energy at some point,

08:48.320 --> 08:52.800
 but maybe instead of trying to take the forces imparted to you by the world

08:52.800 --> 08:57.280
 and replacing them, what we should be doing is letting the world push us around

08:58.000 --> 09:00.880
 and we go with the flow. Very Zen, very Zen robot.

09:00.880 --> 09:06.560
 Yeah, but okay, so that sounds very Zen, but I can also imagine how many

09:07.600 --> 09:14.560
 like failed versions they had to go through. I would say it's probably, would you say it's in

09:14.560 --> 09:18.800
 the thousands that they've had to have the system fall down before they figured out?

09:18.800 --> 09:24.080
 I don't know if it's thousands, but it's a lot. It takes some patience. There's no question.

09:24.960 --> 09:28.000
 So in that sense, control might help a little bit.

09:29.280 --> 09:34.160
 I think everybody, even at the time, said that the answer is to do that with control,

09:34.800 --> 09:38.240
 but it was just pointing out that maybe the way we're doing control right now

09:39.040 --> 09:45.040
 isn't the way we should. Got it. So what about on the animal side, the ones that figured out

09:45.040 --> 09:50.240
 how to move efficiently? Is there anything you find inspiring or beautiful in the movement of

09:50.240 --> 09:55.760
 any particular animal? I do have a favorite example. Okay. So it sort of goes with the

09:55.760 --> 10:02.080
 passive walking idea. So is there, how energy efficient are animals? Okay, there's a great

10:02.080 --> 10:06.480
 series of experiments by George Lauder at Harvard and Mike Tranifillo at MIT.

10:07.280 --> 10:14.080
 They were studying fish swimming in a water tunnel. Okay. And one of these, the type of fish

10:14.080 --> 10:20.160
 they were studying were these rainbow trout because there was a phenomenon well understood

10:20.160 --> 10:24.320
 that rainbow trout, when they're swimming upstream at mating season, they kind of hang out behind

10:24.320 --> 10:28.320
 the rocks. And it looks like, I mean, that's tiring work swimming upstream. They're hanging

10:28.320 --> 10:32.240
 out behind the rocks. Maybe there's something energetically interesting there. So they tried

10:32.240 --> 10:38.400
 to recreate that. They put in this water tunnel, a rock basically, a cylinder that had the same sort

10:38.400 --> 10:44.160
 of vortex street, the eddies coming off the back of the rock that you would see in a stream. And

10:44.160 --> 10:50.080
 they put a real fish behind this and watched how it swims. And the amazing thing is that if you

10:50.080 --> 10:54.960
 watch from above, what the fish swims when it's not behind a rock, it has a particular gait.

10:55.920 --> 10:59.920
 You can identify the fish the same way you look at a human walking down the street. You

10:59.920 --> 11:03.680
 sort of have a sense of how a human walks. The fish has a characteristic gait.

11:03.680 --> 11:11.280
 You put that fish behind the rock, its gait changes. And what they saw was that it was

11:11.280 --> 11:19.040
 actually resonating and kind of surfing between the vortices. Now, here was the experiment that

11:19.040 --> 11:22.960
 really was the clincher because there was still, it wasn't clear how much of that was mechanics

11:22.960 --> 11:28.640
 of the fish, how much of that is control the brain. So the clincher experiment and maybe

11:28.640 --> 11:36.640
 one of my favorites to date, although there are many good experiments. This was now a dead fish.

11:38.160 --> 11:44.160
 They took a dead fish. They put a string that tied the mouse of the fish to the rock so it

11:44.160 --> 11:48.960
 couldn't go back and get caught in the grates. And then they asked, what would that dead fish do

11:48.960 --> 11:52.960
 when it was hanging up behind the rock? And so what you'd expect is sort of flopped around like a

11:52.960 --> 11:59.840
 dead fish in the vortex wake until something sort of amazing happens. And this video is worth putting

12:00.480 --> 12:09.920
 in. What happens? The dead fish basically starts swimming upstream. It's completely dead, no brain,

12:09.920 --> 12:15.520
 no motors, no control, but it somehow the mechanics of the fish resonate with the vortex

12:15.520 --> 12:19.360
 street and it starts swimming upstream. It's one of the best examples ever.

12:19.360 --> 12:26.080
 Who do you credit for that too? Is that just evolution constantly just

12:27.040 --> 12:32.320
 figuring out by killing a lot of generations of animals like the most efficient motion?

12:33.200 --> 12:39.840
 Is that or maybe the physics of our world completely like, it's like evolution applied

12:39.840 --> 12:46.080
 not only to animals, but just the entirety of it somehow drives to efficiency like nature likes

12:46.080 --> 12:50.800
 efficiency. I don't know if that question even makes any sense. I understand the question.

12:53.040 --> 12:58.640
 Do they co evolve? Yeah, somehow, yeah. I don't know if an environment can evolve, but

12:59.920 --> 13:03.840
 I mean, there are experiments that people do, careful experiments that show that

13:04.880 --> 13:09.360
 animals can adapt to unusual situations and recover efficiency. So there seems like at

13:09.360 --> 13:14.160
 least in one direction, I think there is reason to believe that the animals motor system

13:14.160 --> 13:20.960
 and probably its mechanics adapt in order to be more efficient, but efficiency isn't the only

13:20.960 --> 13:26.880
 goal of course. Sometimes it's too easy to think about only efficiency, but we have to do a lot

13:26.880 --> 13:33.920
 of other things first, not get eaten, and then all other things being equal try to save energy.

13:33.920 --> 13:40.080
 By the way, let's draw a distinction between control and mechanics. How would you define each?

13:40.080 --> 13:47.680
 Yeah. I think part of the point is that we shouldn't draw a line as clearly as we tend to,

13:47.680 --> 13:55.200
 but on a robot, we have motors and we have the links of the robot, let's say. If the motors

13:55.200 --> 14:01.680
 are turned off, the robot has some passive dynamics. Gravity does the work. You can put

14:01.680 --> 14:05.840
 springs, I would call that mechanics. If we have springs and dampers, which our muscles are springs

14:05.840 --> 14:10.560
 and dampers and tendons, but then you have something that's doing active work, putting

14:10.560 --> 14:15.840
 energy in which your motors on the robot, the controller's job is to send commands to the

14:15.840 --> 14:22.720
 motor that add new energy into the system. So the mechanics and control interplay somewhere

14:22.720 --> 14:28.000
 the divide is around, did you decide to send some commands to your motor or did you just

14:28.000 --> 14:35.440
 leave the motors off and let them do their work? Would you say is most of nature on the

14:37.280 --> 14:41.840
 dynamic side or the control side? If you look at biological systems,

14:43.440 --> 14:52.080
 we're living in a pandemic now. Do you think a virus is a dynamic system or is there a lot of

14:52.080 --> 14:57.600
 control, intelligence? I think it's both, but I think we maybe have underestimated how important

14:57.600 --> 15:05.360
 the dynamics are. I mean, even our bodies, the mechanics of our bodies, certainly with exercise,

15:05.360 --> 15:13.040
 they evolve, but so I actually, I lost a finger in early 2000s and it's my fifth

15:13.040 --> 15:18.320
 metacarpal. It turns out you use that a lot in ways you don't expect when you're opening jars,

15:19.120 --> 15:23.520
 even when I'm just walking around, if I bump it on something, there's a bone there that was used

15:23.520 --> 15:29.600
 to taking contact. My fourth metacarpal wasn't used to taking contact. It used to hurt. It still

15:29.600 --> 15:38.720
 does a little bit, but actually my bone has remodeled. Over a couple of years, the geometry,

15:39.360 --> 15:45.760
 the mechanics of that bone changed to address the new circumstances. So the idea that somehow

15:45.760 --> 15:48.640
 it's only our brain that's adapting or evolving is not right.

15:48.640 --> 15:55.280
 Right. Maybe sticking on evolution for a bit because it's tended to create some interesting

15:55.280 --> 16:04.960
 things. Bipedal walking, why the heck did evolution give us, I think we're, are we the only mammals

16:04.960 --> 16:12.400
 that walk on two feet? No. I mean, there's a bunch of animals that do it a bit. I think we are the

16:12.400 --> 16:22.240
 most successful bipeds. I think I read somewhere that the reason the, you know, evolution made us

16:22.240 --> 16:28.560
 walk on two feet is because there's an advantage to being able to carry food back to the tribe or

16:28.560 --> 16:34.480
 something like that. So like you can carry, it's kind of this communal cooperative thing. So like

16:34.480 --> 16:42.160
 to carry stuff back to a place of shelter and so on to share with others. Do you understand

16:42.160 --> 16:48.720
 at all the value of walking on two feet from both the robotics and the human perspective?

16:49.280 --> 16:54.960
 Yeah. There are some great books written about evolution of, walking evolution of the human

16:54.960 --> 17:02.720
 body. I think it's easy though to make bad evolutionary arguments. Sure. Most of them

17:02.720 --> 17:11.040
 are probably bad, but what else can we do? I mean, I think a lot of what dominated our evolution

17:11.040 --> 17:19.040
 probably was not the things that worked well sort of in the steady state, you know, when things are,

17:19.040 --> 17:25.360
 when things are good, but, but for instance, people talk about what we should eat now because

17:25.360 --> 17:31.840
 our ancestors were meat eaters or, or whatever. Oh yeah, I love that. Yeah. But probably, you know,

17:31.840 --> 17:40.560
 the reason that one pre, prehomo sapien species versus another survived was not because of

17:40.560 --> 17:47.840
 whether they ate well when there was lots of food. But when the Ice Age came, you know,

17:47.840 --> 17:51.680
 probably one of them happened to be in the wrong place. One of them happened to

17:52.480 --> 17:58.080
 forage a food that was okay, even, even when the glaciers came or something like that. I mean,

17:58.080 --> 18:03.120
 there's a million variables that contributed and we can't, and our actually the amount of

18:03.120 --> 18:08.080
 information we're working with and telling these stories, these evolutionary stories is,

18:08.080 --> 18:14.560
 is very little. So yeah, just like you said, it seems like if we, if we study history, it seems

18:14.560 --> 18:21.600
 like history turns on like these little events that, that otherwise would seem meaningless, but

18:22.480 --> 18:28.720
 in the grant, like when you in retrospect, or turning points, absolutely. And then that's

18:28.720 --> 18:33.920
 probably how like somebody got hit in the head with a rock, because somebody slept with the wrong

18:33.920 --> 18:41.040
 person back in the cave days and somebody get angry and that turned, you know, warring tribes

18:41.760 --> 18:46.240
 combined with the environment, all those millions of things. And the meat eating,

18:46.240 --> 18:50.960
 which I get a lot of criticism because I don't know, I don't know what your dietary processes

18:50.960 --> 18:58.080
 are like, but these days I've been eating only meat, which is there's, there's a large community

18:58.080 --> 19:02.560
 people who say, yeah, probably make evolutionary arguments and say, you're doing a great job.

19:02.560 --> 19:07.760
 There's probably an even larger community of people, including my mom, who says it's a deeply

19:07.760 --> 19:12.480
 unhealthy, it's wrong, but I just feel good doing it. But you're right, these evolutionary

19:12.480 --> 19:16.080
 arguments can be flawed. But is there anything interesting to pull out for?

19:17.120 --> 19:22.640
 There's a great book, by the way, look, a series of books by Nicholas Taylor about fooled by randomness

19:22.640 --> 19:29.360
 and black swan, highly recommend them. But yeah, they make the point nicely that probably it was

19:29.360 --> 19:37.040
 a few random events that, yes, maybe it was someone getting hit by a rock, as you say.

19:39.440 --> 19:43.920
 That said, do you think, I don't know how to ask this question or how to talk about this,

19:43.920 --> 19:48.640
 but there's something elegant and beautiful about moving on two feet, obviously biased,

19:48.640 --> 19:55.040
 because I'm human. But from a robotics perspective to you work with robots on two feet,

19:55.040 --> 20:01.840
 is it all useful to build robots that are on two feet as opposed to four? Is there something useful

20:01.840 --> 20:08.480
 about it? The reason I spent a long time working on bipedal walking was because it was hard.

20:10.240 --> 20:16.080
 It challenged control theory in ways that I thought were important. I wouldn't have

20:17.120 --> 20:23.120
 ever tried to convince you that you should start a company around bipeds or something like this.

20:23.120 --> 20:27.520
 There are people that make pretty compelling arguments. I think the most compelling one

20:27.520 --> 20:33.520
 is that the world is built for the human form. And if you want a robot to work in the world we

20:33.520 --> 20:41.200
 have today, then having a human form is a pretty good way to go. There are places that a biped

20:41.200 --> 20:50.400
 can go that would be hard for other form factors to go, even natural places. But at some point,

20:50.400 --> 20:55.120
 in the long run, we'll be building our environments for our robots probably. And so maybe that argument

20:55.120 --> 21:02.960
 falls aside. So you famously run barefoot. Do you still run barefoot? I still run barefoot.

21:02.960 --> 21:09.280
 That's so awesome. Much to my wife's chagrin. Do you want to make an evolutionary argument for

21:09.280 --> 21:17.600
 why running barefoot is advantageous? What have you learned about human and robot movement in

21:17.600 --> 21:24.960
 general from running barefoot? Human or robot and or? Well, you know, it happened the other way.

21:25.520 --> 21:32.480
 So I was studying walking robots and there's a great conference called the Dynamic Walking

21:33.040 --> 21:38.400
 Conference where it brings together both the biomechanics community and the walking robots

21:38.400 --> 21:44.240
 community. And so I've been going to this for years and hearing talks by people who study

21:44.240 --> 21:50.160
 barefoot running and other the mechanics of running. So I did eventually read Born to Run.

21:50.160 --> 21:55.360
 Most people read Born to Run in the first day, right? The other thing I had going for me is

21:55.360 --> 22:02.080
 actually that I wasn't a runner before and I learned to run after I had learned about

22:02.080 --> 22:06.480
 barefoot running or I mean, started running longer distances. So I didn't have to unlearn.

22:07.200 --> 22:14.080
 And I'm definitely, I'm a big fan of it for me, but I tend to not try to convince other

22:14.080 --> 22:20.720
 people. There's people who run beautifully with shoes on and that's good. But here's why it makes

22:20.720 --> 22:28.640
 sense for me. It's all about the long term game, right? So I think it's just too easy to run 10

22:28.640 --> 22:34.000
 miles, feel pretty good. And then you get home at night and you realize my knees hurt. I did

22:34.000 --> 22:44.080
 something wrong, right? If you take your shoes off, then if you hit hard with your foot at all,

22:44.080 --> 22:50.640
 then it hurts. You don't like run 10 miles and then realize you've done something, some damage.

22:50.640 --> 22:55.280
 You have immediate feedback telling you that you've done something that's maybe suboptimal

22:55.280 --> 22:59.440
 and you change your gait. I mean, it's even subconscious. If I right now, having run many

22:59.440 --> 23:04.480
 miles barefoot, if I put a shoe on my gait changes in a way that I think is not as good.

23:05.760 --> 23:14.800
 So it makes me land softer. And I think my goals for running are to do it for as long as I can

23:14.800 --> 23:22.480
 into old age, not to win any races. And so for me, this is a way to protect myself.

23:22.480 --> 23:30.400
 Yeah, I think, first of all, I've tried running barefoot many years ago, probably the other way,

23:30.400 --> 23:37.840
 just reading born to run. But just to understand, because I felt like I couldn't

23:38.560 --> 23:45.040
 put in the miles that I wanted to. And it feels like running for me, and I think for a lot of

23:45.040 --> 23:51.280
 people, was one of those activities that we do often and we never really try to learn to do

23:51.280 --> 23:58.000
 correctly. Like it's funny, there's so many activities we do every day, like brushing our

23:58.000 --> 24:05.280
 teeth. I think a lot of us, at least me, probably have never deeply studied how to properly brush

24:05.280 --> 24:11.680
 my teeth or wash as now with a pandemic or how to properly wash our hands and do it every day.

24:11.680 --> 24:16.640
 But we haven't really studied, like, am I doing this correctly? But running felt like one of those

24:16.640 --> 24:21.440
 things that was absurd, not to study how to do correctly, because it's the source of so much

24:21.440 --> 24:27.760
 pain and suffering. Like I hate running, but I do it. I do it because I hate it, but I feel

24:27.760 --> 24:32.000
 good afterwards. But I think it feels like you need to learn how to do it properly. So that's

24:32.000 --> 24:38.400
 where barefoot running came in. And then I quickly realized that my gait was completely wrong. I was

24:38.400 --> 24:46.960
 taking huge steps and landing hard on the heel, all those elements. And so yeah, from that I

24:46.960 --> 24:53.040
 actually learned to take really small steps. Look, I already forgot the number, but I feel like it was

24:53.040 --> 25:02.320
 180 a minute or something like that. And I remember I actually just took songs that are 180 beats per

25:02.320 --> 25:09.360
 minute and then tried to run at that beat. And just to teach myself, it took a long time. And I

25:09.360 --> 25:15.200
 feel like after a while you learn to run properly, you adjust it properly without going all the way

25:15.200 --> 25:21.520
 to barefoot. But I feel like barefoot is the legit way to do it. I mean, I think a lot of people

25:21.520 --> 25:27.680
 would be really curious about it. If they're interested in trying, how would you recommend

25:27.680 --> 25:35.280
 they start or try or explore? Slowly. That's the biggest thing people do is they are excellent

25:35.280 --> 25:39.200
 runners and they're used to running long distances or running fast and they take their shoes off and

25:39.200 --> 25:44.560
 they hurt themselves instantly trying to do something that they were used to doing. I think

25:44.560 --> 25:50.800
 I lucked out in the sense that I couldn't run very far when I first started trying. And I run with

25:50.800 --> 25:56.000
 minimal shoes too. I mean, I will bring along a pair of actually like aqua socks or something like

25:56.000 --> 26:01.680
 this. I can just slip on or running sandals. I've tried all of them. What's the difference between

26:01.680 --> 26:05.920
 a minimal shoe and nothing at all? What's like feeling wise? What does it feel like?

26:06.880 --> 26:14.960
 There is it. I mean, I noticed my gait changing, right? So, I mean, your foot has as many muscles

26:14.960 --> 26:22.160
 and sensors as your hand does, right? Sensors. Ooh, okay. And we do amazing things with our hands.

26:22.160 --> 26:29.520
 And we stick our foot in a big salad shoe, right? So, there's, I think, when you're barefoot,

26:29.520 --> 26:34.480
 you're just giving yourself more proprioception. And that's why you're more aware of some of

26:34.480 --> 26:39.440
 the gait flaws and stuff like this. Now, you have less protection too. So.

26:40.640 --> 26:45.920
 Rocks and stuff. I mean, yeah. So, I think people who are afraid of barefoot running

26:45.920 --> 26:50.720
 are worried about getting cuts or getting stepping on rocks. First of all, even if that

26:50.720 --> 26:55.200
 was a concern, I think those are all very short term. If I get a scratch or something,

26:55.200 --> 26:58.960
 it'll heal in a week. If I blow out my knees, I'm done running forever. So, I will trade

26:58.960 --> 27:05.760
 the short term for the long term anytime. But even then, this, again, to my wife's chagrin,

27:06.400 --> 27:11.200
 your feet get tough, right? And. Cow's. Okay. Yeah. I can run over animals to anything now.

27:11.200 --> 27:22.400
 I mean, what, maybe, can you talk about, is there, like, is there tips or tricks that you have

27:23.440 --> 27:29.600
 suggestions about? Like, if I wanted to try it. You know, there is a good book, actually. There's

27:29.600 --> 27:35.360
 probably more good books since I read them. But Ken Bob, barefoot Ken Bob Saxton.

27:35.360 --> 27:43.520
 He's an interesting guy. But I think his book captures the right way to describe running,

27:43.520 --> 27:46.000
 barefoot running to somebody better than any other I've seen.

27:48.560 --> 27:56.800
 So, you run pretty good distances in your bike. And is there, you know, if we talk about bucket

27:56.800 --> 28:02.480
 list items, is there something crazy on your bucket list, athletically, that you hope to do one day?

28:02.480 --> 28:08.240
 I mean, my commute is already a little crazy. What are we talking about here?

28:08.960 --> 28:13.600
 What, what, what distance are we talking about? Well, I live about 12 miles from MIT.

28:14.560 --> 28:18.640
 But you can find lots of different ways to get there. So, I mean, I've run there for a long,

28:18.640 --> 28:24.320
 many years, a bike there. And ways? Yeah. But normally, I would try to run in and then bike

28:24.320 --> 28:30.720
 home, bike in, run home. But you have run there and back before? Sure. Barefoot? Yeah. Or with

28:30.720 --> 28:37.280
 minimal shoes or whatever that? 12 times two. Yeah. Okay. It became kind of a game of how can I get

28:37.280 --> 28:42.560
 to work. I've rollerbladed. I've done all kinds of weird stuff. But my favorite one these days,

28:42.560 --> 28:50.000
 I've been taking the Charles River to work. So, I can put in a little robot, not so far from my

28:50.000 --> 28:55.600
 house. But the Charles River takes a long way to get the MIT. So, I can spend a long time getting

28:55.600 --> 29:02.000
 there. And it's, you know, it's not about, I don't know, it's just about, I've had people

29:02.000 --> 29:10.000
 ask me, how can you justify taking that time? But for me, it's just a magical time to think,

29:10.000 --> 29:16.160
 to compress, decompress. You know, especially, I'll wake up, do a lot of work in the morning,

29:16.160 --> 29:21.200
 and then I kind of have to just let that settle before I'm ready for all my meetings. And then

29:21.200 --> 29:28.400
 on the way home, it's a great time to sort of let that settle. You lead a, like a large group of

29:28.400 --> 29:36.240
 people. I mean, is there days where you're like, oh, shit, I got to get to work in an hour?

29:39.680 --> 29:47.760
 I mean, is there, is there a tension there? And like, if we look at the grand scheme of things,

29:47.760 --> 29:52.560
 just like you said, long term, that meeting probably doesn't matter. Like, you can always say,

29:53.200 --> 29:58.480
 I'll just, I'll run and let the meeting happen, how it happens. Like, what, how do you,

30:00.080 --> 30:05.440
 that Zen, how do you, what do you do with that tension between the real world saying urgently,

30:05.440 --> 30:11.600
 you need to be there, this is important, everything is melting down, how we're going to fix this robot,

30:11.600 --> 30:18.560
 there's this critical meeting, and then there's this, the Zen beauty of just running the simplicity

30:18.560 --> 30:24.000
 of it, you along with nature. What do you do with that? I would say I'm not a fast runner,

30:24.000 --> 30:29.280
 particularly. Probably my fastest splits ever was when I had to get to daycare on time, because

30:29.280 --> 30:34.240
 they were going to charge me, you know, some, some dollar per minute that I was late. I've run some

30:34.240 --> 30:43.840
 fast splits to daycare. But that those times are past now. I think work, you can find a work life

30:43.840 --> 30:50.320
 balance in that way. I think you just have to. I think I am better at work because I take time to

30:50.320 --> 30:59.920
 think on the way in. So I plan my day around it. And I rarely feel that those are really in at odds.

30:59.920 --> 31:08.400
 So what the bucket list item, if we're talking 12 times two, or approaching a marathon,

31:10.480 --> 31:17.360
 what have you run an ultra marathon before? Do you do races? Is there what's

31:17.920 --> 31:24.400
 to win? I'm not going to like take a dingy across the Atlantic or something, if that's what you

31:24.400 --> 31:29.120
 want. But, but if someone does and wants to write a book, I would totally read it because I have a

31:29.120 --> 31:33.920
 sucker for that kind of thing. No, I do have some fun things that I will try. I like to,

31:34.560 --> 31:38.640
 when I travel, I almost always bike to Logan airport and fold up a little folding bike on

31:38.640 --> 31:43.280
 and then take it with me and bike to wherever I'm going. And it's taken me or I'll take a

31:43.280 --> 31:46.960
 stand up paddle board these days on the airplane. And then I'll try to paddle around where I'm going

31:46.960 --> 31:54.240
 or whatever. And I've done some crazy things. But, but not for the, you know, I now talk,

31:54.240 --> 32:00.240
 I don't know if you know who David Goggins is by any chance, but I talk to him now every day. So

32:00.240 --> 32:09.520
 he's the person who made me do this stupid challenge. So he, he's insane. And he does things for the

32:09.520 --> 32:16.800
 purpose in the best kind of way. He does things like for the explicit purpose of suffering.

32:16.800 --> 32:23.680
 Like he picks the thing that like whatever he thinks he can do, he does more. So is that,

32:23.680 --> 32:28.320
 do you have that thing in you or you? I think it's become the opposite.

32:29.600 --> 32:30.320
 It's, uh,

32:30.320 --> 32:34.240
 So you're like that dynamical system that the walk or the efficient, uh,

32:34.240 --> 32:40.400
 Yeah, it's, uh, leave no pain, right? You should end feeling better than you started.

32:41.360 --> 32:48.080
 But, um, it's mostly, I think, and COVID has tested this because I've lost my commute. I think

32:48.080 --> 32:54.400
 I'm perfectly happy walking around, uh, around town with my wife and, uh, kids if they could get

32:54.400 --> 32:59.440
 them to go. Uh, and it's more about just getting outside and getting away from the keyboard for

32:59.440 --> 33:04.960
 some time just to let things compress. Let's go into robotics a little bit. What to use the most

33:04.960 --> 33:11.760
 beautiful idea in robotics? Whether we're talking about control or whether we're talking about

33:11.760 --> 33:17.280
 optimization and the math side of things or the engineering side of things or the philosophical

33:17.280 --> 33:26.160
 side of things. I think I've been lucky to experience something that not so many roboticists

33:26.160 --> 33:34.640
 have experienced, which is to hang out with some really amazing control theorists and,

33:38.640 --> 33:43.520
 the clarity of thought that some of the more mathematical control theory can bring

33:43.520 --> 33:53.360
 to even very complex, messy looking problems is really, it really had a big impact on me. And,

33:53.360 --> 33:59.360
 and, uh, I had a day even, uh, just a couple of weeks ago where I had spent the day on a zoom

33:59.920 --> 34:02.880
 robotics conference, having great conversations with lots of people.

34:03.840 --> 34:10.480
 Felt really good, um, about the ideas that were flowing and, and the like. And then I had a,

34:10.480 --> 34:15.600
 you know, late afternoon meeting with a, one of my favorite control theorists and,

34:17.920 --> 34:23.200
 and we went from these, from these abstract discussions about maybes and what ifs and,

34:23.200 --> 34:31.680
 and what a great idea to these super precise statements about systems that are that much

34:31.680 --> 34:39.840
 much more simple or, or abstract than the ones I care about deeply. And the contrast of that is,

34:39.840 --> 34:51.120
 um, I don't know, it really gets me. I think people underestimate, um, maybe the power of

34:51.120 --> 35:02.000
 clear thinking. Uh, and so for instance, deep learning is amazing. Um, I use it heavily in our

35:02.000 --> 35:09.200
 work. I think it's changed the world unquestionable. It makes it easy to get things to work without

35:09.200 --> 35:14.400
 thinking as critically about it. So I think one of the challenges as an educator is to think about,

35:14.400 --> 35:21.600
 um, how do we make sure people get a taste of the more rigorous thinking that I think goes

35:21.600 --> 35:26.320
 along, uh, with, with some different approaches. Yeah. So that's really interesting. So

35:27.200 --> 35:33.680
 understanding like the fundamentals, the first principles of the, of the, the, the problem

35:33.680 --> 35:41.280
 more in this case is mechanics, like how a thing moves, how a thing behaves, like all the forces

35:41.280 --> 35:46.800
 involved, like really getting a deep understanding of that. I mean, from physics, the first principle

35:46.800 --> 35:53.760
 thing comes from physics. And here it's literally physics. Yeah. And this applies in deep learning.

35:53.760 --> 36:00.560
 This applies to, um, not just, I mean, it applies so cleanly in robotics, but it also applies to

36:00.560 --> 36:08.800
 just in any data set. I find this true. I mean, driving as well. There's a lot of folks in it

36:08.800 --> 36:20.800
 that work on autonomous vehicles that don't study driving like deeply. I might be coming

36:20.800 --> 36:27.680
 a little bit from the psychology side, but, um, I remember I spent a ridiculous number of hours

36:28.240 --> 36:35.200
 at lunch, uh, at this like lawn chair and I would sit somewhere, um, somewhere in MIT's

36:35.200 --> 36:40.160
 campus. There's a few interesting intersections and we just watch people cross. So we were studying,

36:40.160 --> 36:46.960
 um, pedestrian behavior. And I felt like as we record a lot of video to try and then there's

36:46.960 --> 36:51.600
 the computer vision extracts their movement, how they move their head and so on. But like every

36:51.600 --> 36:59.040
 time I felt like I didn't understand enough, I just, I felt like I wasn't understanding what,

36:59.040 --> 37:05.280
 how are people signaling to each other? What are they thinking? How cognizant are they

37:05.280 --> 37:11.440
 of their fear of death? Like what are we, like what's the game, what's the underlying game theory

37:11.440 --> 37:16.640
 here? What are, what are the, the incentives? And then I finally found a live stream of an

37:16.640 --> 37:21.520
 intersection that's like high def that I just, I would watch so I wouldn't have to sit out there.

37:21.520 --> 37:25.360
 But that's interesting. So like, I feel, that's tough. That's a tough example because

37:25.360 --> 37:30.400
 I mean, the learning humans involved, not just because human, but I think, um,

37:31.600 --> 37:36.480
 the learning mantra is the, basically the statistics of the data will tell me things I

37:36.480 --> 37:43.680
 need to know, right? And, uh, you know, for the example you gave of all the nuances of, um, you

37:43.680 --> 37:48.720
 know, eye contact or hand gestures or whatever that are happening for these subtle interactions

37:48.720 --> 37:53.600
 between pedestrians and traffic, right? Maybe the data will tell the, tell, tell that story.

37:53.600 --> 38:02.000
 I may be even, uh, uh, one level more meta than, than what you're saying. Um, for a particular

38:02.000 --> 38:07.680
 problem, I think it might be the case that data should tell us the story. But I think

38:07.680 --> 38:13.760
 there's a rigorous thinking that is just an essential skill for a mathematician or an engineer

38:14.560 --> 38:21.120
 that, um, I just don't want to lose it. There are, there are certainly super rigorous, um,

38:21.120 --> 38:26.720
 rigorous control, or sorry, um, machine learning people. I just think deep learning makes it so

38:26.720 --> 38:35.680
 easy to do some things that, um, our next generation are, um, not immediately rewarded

38:35.680 --> 38:39.680
 for going through some of the more rigorous approaches. And then I wonder where that takes us.

38:40.240 --> 38:45.440
 I just, well, I'm, I'm actually optimistic about it. I just want to, um, do my part to try to steer

38:45.440 --> 38:53.360
 that rigorous thinking. So there's like two questions I want to ask. Do you have sort of a,

38:53.360 --> 39:00.720
 a good example of rigorous thinking where it's easy to get lazy and not do the rigorous thinking?

39:00.720 --> 39:03.120
 And the other question I have is like, do you have advice

39:05.120 --> 39:13.520
 of, um, how to practice rigorous thinking in, um, you know, in all the computer science disciplines

39:13.520 --> 39:22.800
 that we've mentioned? Yeah. I mean, there are times where problems that can be solved with well

39:22.800 --> 39:30.480
 known, mature methods, um, could also be solved with, uh, with a deep learning approach. And,

39:32.160 --> 39:38.160
 um, there's an argument that you must use learning even for the parts we already think we know,

39:38.160 --> 39:42.560
 because if the human has touched it, then you've, you've, you've biased the system and you've

39:42.560 --> 39:47.520
 suddenly put a bottleneck in there that is your own mental model, but something like inverting

39:47.520 --> 39:51.440
 a matrix. You know, I, I think we know how to do that pretty well, even if it's a pretty big

39:51.440 --> 39:55.520
 matrix and we understand that pretty well and you could train a deep network to do it, but

39:55.520 --> 40:02.240
 you shouldn't probably. So, so in that sense, rigorous thinking is, uh, understanding the,

40:03.440 --> 40:08.640
 the scope and limitations of the method of the methods that we have, like how to use the tools

40:08.640 --> 40:16.320
 of mathematics properly. Yeah. I think, you know, taking a class on analysis is all I'm sort of

40:16.320 --> 40:21.440
 arguing is to take, take a chance to stop and enforce yourself to think rigorously about even,

40:22.240 --> 40:27.600
 you know, the rational numbers or something, you know, it doesn't have to be the end all problem,

40:27.600 --> 40:34.080
 but that exercise of clear thinking, I think, uh, goes a long way and I just want to make

40:34.080 --> 40:38.720
 sure we, we keep preaching. We don't lose it. Yeah. But do you think, uh, when you're doing, like

40:38.720 --> 40:45.760
 rigorous thinking or like maybe, uh, trying to write down equations or sort of explicitly,

40:45.760 --> 40:51.760
 like formally describe a system, do you think we naturally simplify things too much? Is that a

40:51.760 --> 40:56.720
 danger you run into? Like, uh, in order to be able to understand something about the system

40:56.720 --> 41:03.200
 mathematically, we, uh, make it too much of a toy example. But I think that's the good stuff.

41:03.200 --> 41:08.800
 Right? Um, that's how you understand the fundamentals. I think so. I think maybe even

41:08.800 --> 41:14.000
 that's a key to intelligence or something, but I mean, okay, what if Newton and Galileo had deep

41:14.000 --> 41:20.640
 learning and, and, and they had done a bunch of experiments and they told the world, here's your

41:20.640 --> 41:24.640
 weights of your neural network. I've, we've solved the problem. I am. You know, where would we be

41:24.640 --> 41:29.120
 today? I don't, I don't think we'd be as far as we, as we are. There's something to be said about

41:29.120 --> 41:37.040
 having a, the simplest explanation for a phenomenon. So I don't doubt that we can train neural networks

41:37.040 --> 41:48.560
 to predict even physical, um, you know, uh, F equals MA type equations. But, um, I maybe,

41:50.000 --> 41:53.360
 I want another Newton to come along because I think there's more to do in terms of

41:53.360 --> 41:59.040
 coming up with the simple models for more complicated tasks.

41:59.040 --> 42:05.600
 Yeah. Uh, let's not offend the AI systems from 50 years from now that are listening to this

42:05.600 --> 42:12.160
 that are probably better at, might be better coming up with F equals MA equations themselves.

42:12.160 --> 42:17.840
 Oh, sorry. I actually think, um, learning is probably a route to, to achieving this.

42:17.840 --> 42:25.760
 Um, but the representation matters, right? And I think, uh, having a function that takes my

42:25.760 --> 42:31.920
 inputs to outputs that is arbitrarily complex may not be the end goal. I think, um, there's still,

42:32.640 --> 42:38.080
 you know, the most simple or parsimonious explanation for the data, um, simple doesn't

42:38.080 --> 42:42.080
 mean low dimensional. That's one thing I think that we've, a lesson that we've learned. So,

42:42.080 --> 42:48.480
 you know, a standard way to do, um, model reduction or system identification and controls is to,

42:48.480 --> 42:53.280
 the typical formulation is that you try to find the minimal state dimension realization

42:53.280 --> 42:58.800
 of a system that hits some error bounds or something like that. And that's maybe not,

42:58.800 --> 43:03.840
 I think we're, we're learning that, that was, the dimension, state dimension is not the right

43:03.840 --> 43:09.360
 metric. Of complexity. Of complexity. But for me, I think a lot about contact,

43:09.360 --> 43:13.360
 the mechanics of contact, the robot hand is picking up an object or something.

43:14.400 --> 43:19.040
 And when I write down the equations of motion for that, they're, they look incredibly complex,

43:19.040 --> 43:26.560
 not because, um, actually not so much because of the dynamics of the hand when it's moving,

43:26.560 --> 43:32.800
 but it's just the interactions and when they turn on and off, right? So having a high dimensional,

43:32.800 --> 43:36.960
 you know, but simple description of what's happening out here is fine. But if when I

43:36.960 --> 43:43.680
 actually start touching, if I write down a different dynamical system for every polygon

43:43.680 --> 43:48.800
 on my robot hand and every polygon on the object, whether it's in contact or not,

43:48.800 --> 43:55.040
 with all the combinatorics that explodes there, then that's too complex. So I need to somehow

43:55.040 --> 44:03.360
 summarize that with a more intuitive physics way of thinking. And, uh, yeah, I'm very optimistic

44:03.360 --> 44:08.400
 that machine learning will get us there. First of all, I mean, I'll probably do it in the

44:08.400 --> 44:13.600
 introduction, but you're, uh, one of the great robotics people at MIT. You're a professor at

44:13.600 --> 44:19.840
 MIT. You've teach a lot of amazing courses. You run a large group, uh, and you have a

44:20.720 --> 44:25.520
 important history for MIT, I think, as, uh, being a part of the DARPA robotics challenge.

44:26.160 --> 44:30.160
 Can you maybe first say what is the DARPA robotics challenge and then

44:30.160 --> 44:36.720
 tell your story around it, your journey with it? Yeah, sure.

44:39.120 --> 44:44.960
 So the DARPA robotics challenge, it came on the tales of the DARPA grand challenge and DARPA

44:44.960 --> 44:51.200
 urban challenge, which were the challenges that brought us, um, put a spotlight on self driving

44:51.200 --> 45:03.200
 cars. Um, Gil Pratt was at DARPA and pitched a new challenge that involved disaster response.

45:04.800 --> 45:08.640
 It didn't explicitly require humanoids, although humanoids came into the picture.

45:10.080 --> 45:16.000
 This happened shortly after the Fukushima disaster in Japan. And our challenge was

45:16.000 --> 45:21.760
 motivated roughly by that because that was a case where if we had had robots that were ready to be

45:21.760 --> 45:29.040
 sent in, there's a chance that we could have, um, averted disaster. And certainly after the, um,

45:29.040 --> 45:32.720
 in the disaster response, there were times we would love, we would have loved to have sent

45:32.720 --> 45:40.400
 robots in. So in practice, what we ended up with was, uh, a grand challenge, a DARPA robotics

45:40.400 --> 45:49.600
 challenge, um, where Boston Dynamics was, uh, was to make humanoid robots. People like me

45:49.600 --> 45:57.440
 and the amazing team at MIT, um, were competing first in a simulation challenge to try to be one

45:57.440 --> 46:03.280
 of the ones that wins the right to work on one of the, uh, the Boston Dynamics humanoids in

46:03.280 --> 46:09.200
 order to compete in the final challenge, which was a physical challenge. And at that point,

46:09.200 --> 46:13.200
 it was already, so it was decided that it's humanoid robots early on.

46:13.200 --> 46:17.120
 So there were, there were two tracks that you could enter as a hardware team where you brought

46:17.120 --> 46:22.720
 your own robot, or you could enter through the virtual robotics challenge as a software team

46:22.720 --> 46:25.760
 that would try to win the right to use one of the Boston Dynamics robots.

46:25.760 --> 46:27.280
 Which are called Atlas.

46:27.280 --> 46:27.760
 Atlas.

46:27.760 --> 46:28.480
 Humanoid robots.

46:28.480 --> 46:34.160
 Yeah. It was a 400 pound marvel, but a, you know, pretty big, scary looking robot.

46:35.360 --> 46:36.160
 Expensive too.

46:36.160 --> 46:36.560
 Expensive.

46:36.560 --> 46:37.360
 At least at the time.

46:37.360 --> 46:38.160
 Yeah.

46:38.160 --> 46:44.640
 Okay. So, uh, I mean, how did you feel at the prospect of this kind of challenge?

46:44.640 --> 46:50.960
 I mean, it seems, you know, autonomous vehicles, yeah, I guess that sounds hard,

46:50.960 --> 46:55.920
 but, uh, not really from a robotics perspective. It's like, didn't they do it in the 80s?

46:55.920 --> 47:01.120
 Is it kind of feeling I would have, uh, like when you first look at the problem and sound

47:01.120 --> 47:11.200
 wheels, but like humanoid robots, that sounds really hard. Uh, so what, like, what, what are

47:11.200 --> 47:16.800
 you, psychologically speaking, what were you feeling excited, scared? Why the heck did you

47:16.800 --> 47:19.520
 get yourself involved in this kind of messy challenge?

47:19.520 --> 47:25.280
 We didn't really know for sure what we were signing up for, um, in the sense that you could

47:25.280 --> 47:31.120
 have had something that as it was described in the call for participation, um, that could have

47:31.600 --> 47:36.640
 put a huge emphasis on the dynamics of walking and not falling down and walking over rough terrain,

47:37.200 --> 47:42.720
 or the same description, because the robot had to go into this disaster area and turn valves and,

47:42.720 --> 47:46.880
 and pick up a drill, cut the hole through a wall. It had to do some interesting things.

47:48.240 --> 47:53.440
 The challenge could have really highlighted perception and autonomous planning,

47:53.440 --> 48:02.320
 or it ended up that, you know, locomoting over a complex, uh, terrain played a pretty big role

48:02.320 --> 48:05.360
 in the competition. So, um,

48:05.360 --> 48:07.360
 And the degree of autonomy wasn't clear.

48:08.160 --> 48:13.760
 The degree of autonomy was always a central part of the discussion. So, um, what wasn't clear was

48:13.760 --> 48:19.920
 how we would be able, how far we'd be able to get with it. So the idea was always, uh, that you

48:19.920 --> 48:25.440
 want semi autonomy, that you want the robot to have enough compute that you can have a degraded

48:25.440 --> 48:31.360
 network link to a human. And so the same way you, we had degraded networks at, uh, at a many

48:31.360 --> 48:37.360
 natural disasters, you'd send your robot in, you'd be able to get a few bits back and forth,

48:37.360 --> 48:42.720
 but you don't get to have enough, potentially to fully, uh, operate the robot in every joint of the

48:42.720 --> 48:49.440
 robot. So, and then the question was, and the gamesmanship of the organizers was to figure out

48:49.440 --> 48:55.680
 what we're capable of, push us as far as we could so that, um, it would differentiate the teams that

48:55.680 --> 49:00.480
 put more autonomy on the robot and had a few clicks and just said, go there or do this, go there,

49:00.480 --> 49:04.320
 do this versus someone who's picking every footstep or something like that.

49:05.200 --> 49:14.480
 So what were some, uh, memories, painful, triumphant from the experience? Like, what was that

49:14.480 --> 49:20.160
 journey? Maybe if you can dig in a little deeper, maybe even on the technical side, on the team side,

49:20.960 --> 49:27.200
 that, that whole process of, um, from the early idea stages to actually competing.

49:28.000 --> 49:33.360
 I mean, this was a defining experience for me. It was, it came at the right time for me in my

49:33.360 --> 49:38.960
 career. I had gotten tenure before I was due a sabbatical and most people do something, you know,

49:38.960 --> 49:44.720
 relaxing and restorative for sabbatical. So you got tenure before the, the, before this? Yeah.

49:44.720 --> 49:49.360
 Yeah. Yeah. It was a good time for me. I had, I had, we had a bunch of algorithms that we

49:49.920 --> 49:53.280
 were very happy with. We wanted to see how far we could push them. And this was a chance to

49:53.280 --> 50:00.240
 really test our metal to do more proper software engineering. Um, the team, we all just worked

50:00.240 --> 50:08.240
 our butts off. We, you know, we're in that lab almost all the time. Um, okay. So there, I mean,

50:08.240 --> 50:12.960
 there were some, of course, high highs and low lows throughout that, uh, anytime you're, you know,

50:12.960 --> 50:19.600
 not sleeping and devoting your life to a 400 pound humanoid. Um, I remember actually one funny

50:20.240 --> 50:24.960
 moment where we're all super tired and so Atlas had to walk across cinder blocks. That was one of

50:24.960 --> 50:30.160
 the obstacles. And I remember Atlas was powered down and hanging limp, you know, on the, on its

50:30.160 --> 50:35.040
 harness and the humans were there like laying, you know, picking up and laying the brick down

50:35.040 --> 50:40.160
 so that the robot could walk over it. And I thought, what is wrong with this? You know, we've got a

50:40.160 --> 50:46.000
 robot just watching us do all the manual labor so that it can take its little, um, scroll across

50:46.000 --> 50:53.520
 the tree. But, um, I mean, even the, even the virtual robotics challenge was, was super nerve

50:53.520 --> 51:02.160
 racking and dramatic. I remember, um, so, so we were using gazebo as a simulator on the cloud.

51:02.160 --> 51:07.920
 And there was all these interesting challenges. I think, um, the investment that, that OSRs, um,

51:07.920 --> 51:12.880
 FC, whatever they were called at that time, Brian Gerkey's team at open source robotics, um,

51:14.000 --> 51:17.840
 they were pushing on the capabilities of gazebo in order to scale it to the complexity

51:18.400 --> 51:25.600
 of these challenges. So, um, you know, up to the virtual competition. So the virtual competition

51:25.600 --> 51:30.800
 was you will sign on at a certain time and we'll have a network connection to another machine on

51:30.800 --> 51:36.640
 the cloud that is running the simulator of your robot. And your controller will run on this computer,

51:36.640 --> 51:41.360
 this controller, this computer, and, and the physics will run on the other and you have to,

51:41.360 --> 51:48.640
 to connect. Now, um, the physics, they wanted it to run at real time rates because there was

51:48.640 --> 51:53.600
 an element of human interaction. Um, and humans could, if you do want to tell you, it works

51:53.600 --> 51:58.800
 way better if it's at frame rate. Oh, cool. But it was very hard to simulate these

51:58.800 --> 52:05.680
 complex, these complex scenes at real time rate. So right up to like days before the competition,

52:06.320 --> 52:13.120
 the, the simulator wasn't quite at real time rate. And that was great for me because my controller

52:13.120 --> 52:18.160
 was solving a big, pretty big optimization problem and it wasn't quite at real time rate. So I was

52:18.160 --> 52:23.280
 fine. I was keeping up with the simulator. We were both running at about 0.7. And I remember

52:23.280 --> 52:29.440
 getting this email, and by the way, the perception folks on our team hated that they knew that if

52:29.440 --> 52:33.520
 my controller was too slow, the robot was going to fall down. And, and you know, no matter how

52:33.520 --> 52:38.000
 good their perception system was, if I can't make my controller fast, anyways, we get this email like

52:38.000 --> 52:41.600
 three days before the virtual competition, you know, it's for all the marbles, we're going to

52:41.600 --> 52:47.200
 either get a humanoid robot or we're not. And we get an email saying, good news, we made the robot,

52:47.200 --> 52:54.160
 does the simulator faster? It's now one point. And I, we're, I was just like, oh man, what are we

52:54.160 --> 53:01.680
 going to do here? So that came in late at night for me. A few days ahead. A few days ahead. I went

53:01.680 --> 53:07.840
 over, there was, it happened at Frank Permanter, who's a very, very sharp. He's a student at the

53:07.840 --> 53:16.080
 time working on optimization. He was still in lab. Frank, we need to make the quadratic programming

53:16.080 --> 53:22.640
 solver faster, not like a little faster. It's actually, you know, and we wrote a new solver for

53:22.640 --> 53:31.680
 that QP together that night. And he's terrifying. So there's a really hard optimization problem

53:31.680 --> 53:37.520
 that you're constantly solving. You didn't make the optimization problem simpler. You wrote a new

53:37.520 --> 53:44.560
 solver. So, I mean, your observation is almost spot on. What we did was what everybody, I mean,

53:44.560 --> 53:49.840
 people know how to do this, but we had not yet done this idea of worm starting. So we are solving

53:49.840 --> 53:54.800
 a big optimization problem at every time step. But if you're running fast enough, the optimization

53:54.800 --> 53:58.640
 problem you're solving on the last time step is pretty similar to the optimization you're going

53:58.640 --> 54:03.680
 to solve with the next. We had course had told our commercial solver to use worm starting. But

54:03.680 --> 54:11.040
 even the interface to that commercial solver was causing us these delays. So what we did was we

54:11.040 --> 54:17.680
 basically wrote, we called it fast QP at the time, we wrote a very lightweight, very fast layer,

54:18.320 --> 54:24.720
 which would basically check if nearby solutions to the quadratic program were, which were very

54:24.720 --> 54:29.840
 easily checked, could stabilize the robot. And if they couldn't, we would fall back to the solver.

54:30.640 --> 54:32.160
 You couldn't really test this well, right?

54:32.160 --> 54:39.760
 Right. So we always knew that if we fell back, it got to the point where if for some reason

54:40.320 --> 54:43.600
 things slowed down and we fell back to the original solver, the robot would actually

54:43.600 --> 54:52.640
 literally fall down. So it was a harrowing sort of ledge we're sort of on. But I mean,

54:52.640 --> 54:57.280
 actually, like the 400 pound human could come crashing to the ground if your solver's not

54:57.280 --> 55:05.360
 fast enough. But, you know, we have lots of good experiences. So can I ask a weird question I get

55:06.560 --> 55:15.280
 about the idea of hard work? So actually people like students of yours that I've interacted with

55:15.840 --> 55:25.520
 and just in robotics people in general, but they have moments at moments of work harder than

55:25.520 --> 55:31.680
 most people I know in terms of if you look at different disciplines of how hard people work.

55:32.240 --> 55:38.560
 But they're also like the happiest. Like just like, I don't know. It's the same thing with

55:38.560 --> 55:43.920
 like running people that push themselves to like the limit. They also seem to be like the most like

55:43.920 --> 55:50.640
 full of life somehow. And I get often criticized like, you're not getting enough sleep. What are

55:50.640 --> 55:56.480
 you doing to your body, blah, blah, blah, like this kind of stuff. And I usually just kind of

55:56.480 --> 56:03.840
 respond like I'm doing what I love. I'm passionate about it. I love it. I feel like it's it's

56:03.840 --> 56:09.200
 invigorating. I actually think I don't think the lack of sleep is what hurts you. I think what

56:09.200 --> 56:14.160
 hurts you is stress and lack of doing things that you're passionate about. But in this world,

56:14.160 --> 56:20.960
 yeah, I mean, can you comment about why the heck robotics people are

56:23.840 --> 56:29.120
 willing to push themselves to that degree? Is there value in that? And why are they so happy?

56:30.240 --> 56:35.360
 I think you got it right. I mean, I think the causality is not that we work hard.

56:36.240 --> 56:40.320
 And I think other disciplines work very hard too. But I don't think it's that we work hard and

56:40.320 --> 56:46.320
 therefore we are happy. I think we found something that we're truly passionate about.

56:47.920 --> 56:53.360
 It makes us very happy. And then we get a little involved with it and spend a lot of time on it.

56:54.400 --> 56:57.840
 What a luxury to have something that you want to spend all your time on, right?

56:59.040 --> 57:02.800
 We could talk about this for many hours, but maybe if we could pick,

57:03.760 --> 57:07.520
 is there something on the technical side on the approach that you took that's interesting

57:07.520 --> 57:13.040
 that turned out to be a terrible failure or a success that you carry into your work today

57:13.680 --> 57:20.720
 about all the different ideas that were involved in making, whether in the simulation

57:20.720 --> 57:25.040
 or in the real world, making the semi autonomous system work?

57:26.720 --> 57:32.560
 I mean, it really did teach me something fundamental about what it's going to take to

57:32.560 --> 57:38.000
 get robustness out of a system of this complexity. I would say the DARPA challenge really

57:39.360 --> 57:43.600
 was foundational in my thinking. I think the autonomous driving community thinks about this.

57:43.600 --> 57:47.760
 I think lots of people thinking about safety critical systems that might have machine learning

57:47.760 --> 57:53.360
 in the loop are thinking about these questions. For me, the DARPA challenge was the moment where

57:53.360 --> 58:00.800
 I realized we've spent every waking minute running this robot. And again, for the physical

58:00.800 --> 58:04.800
 competition, days before the competition, we saw the robot fall down in a way it had never

58:04.800 --> 58:11.440
 fallen down before. I thought, you know, how could we have found that? We only have one robot.

58:11.440 --> 58:16.400
 It's running almost all the time. We just didn't have enough hours in the day to test that robot.

58:17.040 --> 58:22.400
 Something has to change, right? And then I think that, I mean, I would say that the team that won

58:23.600 --> 58:30.400
 was from KAIST was the team that had two robots and was able to do not only incredible engineering,

58:30.400 --> 58:36.160
 just absolutely top rate engineering, but also they were able to test at a rate and

58:37.440 --> 58:42.080
 discipline that we didn't keep up with. What does testing look like? What are we talking about here?

58:42.080 --> 58:48.560
 Like, what's a loop of tests? Like, from start to finish, what is a loop of testing?

58:48.560 --> 58:53.200
 Yeah, I mean, I think there's a whole philosophy to testing. There's the unit tests. And you can

58:53.200 --> 58:56.960
 do that on a hardware. You can do that in a small piece of code. You write one function,

58:56.960 --> 59:01.520
 you should write a test that checks that function's input outputs. You should also write an

59:01.520 --> 59:05.120
 integration test at the other extreme of running the whole system together, you know,

59:06.000 --> 59:12.080
 that try to turn on all the different functions that you think are correct. It's much harder to

59:12.080 --> 59:17.280
 write the specifications for a system level test, especially if that system is as complicated as a

59:17.280 --> 59:24.000
 humanoid robot. But the philosophy is sort of the same. On the real robot, it's no different,

59:24.000 --> 59:31.440
 but on a real robot, it's impossible to run the same experiment twice. So if you see a failure,

59:32.320 --> 59:36.320
 you hope you caught something in the logs that tell you what happened, but you'd probably never

59:36.320 --> 59:43.920
 be able to run exactly that experiment again. And right now, I think our philosophy is just

59:45.520 --> 59:51.600
 basically Monte Carlo estimation is just run as many experiments as we can, maybe try to set up

59:51.600 --> 1:00:00.160
 the environment to make the things we are worried about happen as often as possible. But really,

1:00:00.160 --> 1:00:05.360
 we're relying on somewhat random search in order to test. Maybe that's all we'll ever be able to,

1:00:05.360 --> 1:00:11.920
 but I think, you know, because there's an argument that the things that'll get you are the things

1:00:11.920 --> 1:00:16.720
 that are really nuanced in the world. And it'd be very hard to, for instance, put back in a simulation.

1:00:16.720 --> 1:00:23.520
 Yeah, the I guess the edge cases. What was the hardest thing? Like, so you said walking over

1:00:23.520 --> 1:00:31.280
 rough terrain, like that just taking footsteps. I mean, people, it's so dramatic and painful in

1:00:31.280 --> 1:00:38.560
 a certain kind of way to watch these videos from the DRC of robots falling. Yeah, I just so

1:00:38.560 --> 1:00:44.800
 heartbreaking. I don't know. Maybe it's because for me, at least we anthropomorphize the robot.

1:00:44.800 --> 1:00:52.880
 Of course, it's funny for some reason, like humans falling is funny. For I don't it's some dark

1:00:52.880 --> 1:01:00.080
 reason. I'm not sure why it is so, but it's also like tragic and painful. And so speaking of which,

1:01:00.080 --> 1:01:06.240
 I mean, what what made the robots fall and fail in your view? So I can tell you exactly what

1:01:06.240 --> 1:01:10.480
 happened on we I contributed one of those our team contributed one of those spectacular falls.

1:01:10.480 --> 1:01:17.760
 Every one of those falls, the has a complicated story. I mean, at one time, the power effectively

1:01:17.760 --> 1:01:23.440
 went out on the robot. Because it had been sitting at the door waiting for a green light to be able

1:01:23.440 --> 1:01:28.480
 to proceed and its batteries, you know, and therefore it just fell backwards and smashed its head

1:01:28.480 --> 1:01:34.720
 to ground and it was hilarious. But it wasn't because of bad software, right? But for ours,

1:01:34.720 --> 1:01:40.240
 so the hardest part of the challenge, the hardest task, in my view, was getting out of the Polaris.

1:01:40.240 --> 1:01:46.480
 It was actually relatively easy to drive the Polaris. Can you tell the story of the car?

1:01:49.920 --> 1:01:54.240
 People should watch this video. I mean, the thing you've come up with is just brilliant. But

1:01:54.240 --> 1:01:59.200
 anyway, sorry, what's, we kind of joke, we call it the big robot little car problem, because

1:01:59.200 --> 1:02:05.360
 somehow the race organizers decided to give us a 400 pound humoid. And then they also provided the

1:02:05.360 --> 1:02:11.360
 vehicle, which was a little Polaris. And the robot didn't really fit in the car. So you couldn't

1:02:11.360 --> 1:02:17.440
 drive the car with your feet under the steering column. We actually had to straddle the the main

1:02:17.440 --> 1:02:23.920
 column of the and have basically one foot in the passenger seat, one foot in the driver's seat,

1:02:23.920 --> 1:02:30.240
 in the driver's seat, and then drive with our left hand. But the hard part was we had to then

1:02:30.240 --> 1:02:36.880
 park the car, get out of the car. It didn't have a door, that was okay. But it's just getting up

1:02:36.880 --> 1:02:42.560
 from crouched from sitting when you're in this very constrained environment. First of all,

1:02:42.560 --> 1:02:46.960
 I remember after watching those videos, I was much more cognizant of how hard is it,

1:02:46.960 --> 1:02:51.600
 it is for me to get in and out of the car, and out of the car especially. Like,

1:02:51.600 --> 1:02:57.600
 it's actually a really difficult control problem. Yeah. I'm very cognizant of it when I'm like

1:02:57.600 --> 1:03:03.360
 injured for whatever reason. It's really hard. Yeah. So how did you, how did you approach this

1:03:03.360 --> 1:03:09.440
 problem? So we had a, you know, you think of NASA's operations, and they have these checklists,

1:03:09.440 --> 1:03:12.880
 you know, prelaunch checklists and the like, we weren't far off from that, we had this big

1:03:12.880 --> 1:03:17.680
 checklist. And on the first day of the competition, we were running down our checklist. And one of

1:03:17.680 --> 1:03:23.200
 the things we had to do, we had to turn off the controller, the piece of software that was running

1:03:23.200 --> 1:03:27.200
 that would drive the left foot of the robot in order to accelerate on the gas.

1:03:28.000 --> 1:03:33.120
 And then we turned on our balancing controller. And the nerves jitters of the first day of the

1:03:33.120 --> 1:03:39.360
 competition, someone forgot to check that box and turn that controller off. So we used a lot of

1:03:39.360 --> 1:03:47.360
 motion planning to figure out a sort of configuration of the robot that we get up and over. We

1:03:47.360 --> 1:03:53.440
 relied heavily on our balancing controller. And, and basically there were, when the robot was in

1:03:53.440 --> 1:03:59.680
 one of its most precarious, you know, sort of configurations trying to sneak its big leg out

1:03:59.680 --> 1:04:05.440
 of the, out of the side, the other controller that thought it was still driving, told it's

1:04:05.440 --> 1:04:13.280
 left foot to go like this. And, and that wasn't good. But, but it turned disastrous for us,

1:04:13.280 --> 1:04:20.160
 because what happened was a little bit of push here. Actually, if we have videos of us, you know,

1:04:20.160 --> 1:04:25.600
 running into the robot with a 10 foot pole, and it kind of will recover. But this is a case where

1:04:26.320 --> 1:04:30.800
 there's no space to recover. So a lot of our secondary balancing mechanisms about,

1:04:30.800 --> 1:04:34.320
 like take a step to recover, they were all disabled because we were in the car and there's no place

1:04:34.320 --> 1:04:40.400
 to step. So we're relying on our just lowest level reflexes. And even then, I think just hitting

1:04:40.400 --> 1:04:45.440
 the foot on the seat on the floor, we probably could have recovered from it. But the thing that

1:04:45.440 --> 1:04:51.280
 was bad that happened is when we did that, and we jostled a little bit, the tailbone of our robot

1:04:52.000 --> 1:04:56.880
 was only a little off the seat, it hit the seat. And the other foot came off the ground just a

1:04:56.880 --> 1:05:03.680
 little bit. And nothing in our plans had ever told us what to do if your butt's on the seat

1:05:03.680 --> 1:05:09.280
 and your feet are in the air. And then the thing is, once you get off the script,

1:05:09.280 --> 1:05:13.600
 things can go very wrong. Because even our state estimation, our system that was trying to

1:05:14.240 --> 1:05:17.680
 collect all the data from the sensors and understand what's happening with the robot,

1:05:18.400 --> 1:05:21.920
 it didn't know about this situation. So it was predicting things that were just wrong.

1:05:22.720 --> 1:05:29.120
 And then we did a violent shake and fell off in our face first on out of the robot.

1:05:29.120 --> 1:05:32.400
 But like into the destination.

1:05:32.400 --> 1:05:34.960
 That's true. We fell in and we got our point for egress.

1:05:34.960 --> 1:05:40.720
 But so is there any hope for, that's interesting, is there any hope for

1:05:41.840 --> 1:05:46.240
 Atlas to be able to do something when it's just on its butt and feet in the air?

1:05:46.240 --> 1:05:46.960
 Absolutely.

1:05:46.960 --> 1:05:48.320
 So you can, what do you?

1:05:48.320 --> 1:05:51.920
 No, so that's, that is one of the big challenges. And I think it's still true.

1:05:53.440 --> 1:05:59.440
 You know, Boston Dynamics and, and, and EMI and there's this incredible work on,

1:05:59.440 --> 1:06:01.680
 on legged robots happening around the world.

1:06:01.680 --> 1:06:09.360
 Most of them still are, are very good at the case where you're making contact with the world at

1:06:09.360 --> 1:06:13.760
 your feet. And they have typically point feet relatively, their balls on their feet, for instance.

1:06:14.400 --> 1:06:19.760
 If that, if those robots get in a situation where the elbow hits the wall or something like this,

1:06:19.760 --> 1:06:24.080
 that's a pretty different situation. Now they have layers of mechanisms that will make,

1:06:24.080 --> 1:06:30.160
 I think the more mature solutions have, have ways in which the controller won't do stupid things.

1:06:30.160 --> 1:06:35.440
 But a human, for instance, is able to leverage incidental contact in order to accomplish a goal.

1:06:35.440 --> 1:06:39.840
 In fact, I might, if you push me, I might actually put my hand out and make a new, brand new contact.

1:06:40.800 --> 1:06:47.840
 The feet of the robot are doing this on quadrupeds, but we mostly in robotics are afraid of contact

1:06:47.840 --> 1:06:54.880
 on the rest of our body, which is crazy. There's this whole field of motion planning,

1:06:54.880 --> 1:07:00.160
 motion planning, collision free motion planning. And we write very complex algorithms so that the

1:07:00.160 --> 1:07:07.040
 robot can dance around and make sure it doesn't touch the world. So people are just afraid of

1:07:07.040 --> 1:07:12.800
 contact because contact is seen as a difficult. It's still a difficult control problem and sensing

1:07:12.800 --> 1:07:21.760
 problem. Now you're a serious person. I'm a little bit of an idiot and I'm going to ask

1:07:21.760 --> 1:07:29.680
 you some dumb questions. So I do, I do martial arts. So like Jiu Jitsu, I wrestled my whole life.

1:07:30.240 --> 1:07:36.160
 So let me, let me ask the question, you know, like whenever people learn that I do any kind

1:07:36.160 --> 1:07:41.760
 of AI or like I mentioned robots and things like that, they say, when are we going to have robots

1:07:41.760 --> 1:07:50.640
 that, you know, they can win in a wrestling match or in a fight against a human. So we just mentioned

1:07:50.640 --> 1:07:54.800
 sitting on your butt, if you're in the air, that's a common position. Jiu Jitsu, when you're on the

1:07:54.800 --> 1:08:04.080
 ground, you're your down opponent. Like what, how difficult do you think is the problem? And when

1:08:04.080 --> 1:08:09.840
 will we have a robot that can defeat a human in a wrestling match? And we're talking about a lot,

1:08:09.840 --> 1:08:13.520
 like, I don't know if you're familiar with wrestling, but essentially,

1:08:13.520 --> 1:08:22.400
 not very, it's basically the art of contact. It's like, it's because you're, you're, you're

1:08:22.400 --> 1:08:31.280
 picking contact points, and then using like leverage, like to off balance to, to trick people,

1:08:31.280 --> 1:08:38.720
 like you make them feel like you're doing one thing, and then they, they change their balance,

1:08:38.720 --> 1:08:44.640
 and then you switch what you're doing, and then results in a throw or whatever. So like, it's

1:08:44.640 --> 1:08:51.520
 basically the art of multiple contacts. So awesome. It's a nice description of it. So there's also an

1:08:51.520 --> 1:09:00.320
 opponent in there, right? So, so if very dynamic, right? If you are wrestling a human, and are in

1:09:00.320 --> 1:09:08.160
 a game theoretic situation with a human, that's still hard. But just to speak to the, you know,

1:09:08.160 --> 1:09:11.200
 quickly reasoning about contact part of it, for instance,

1:09:11.200 --> 1:09:13.840
 yeah, maybe even throwing the game theory out of it, almost like,

1:09:15.040 --> 1:09:19.920
 yeah, almost like a non dynamic opponent, right? There's reasons to be optimistic,

1:09:19.920 --> 1:09:25.360
 but I think our best understanding of those problems are still pretty hard. I have been

1:09:26.640 --> 1:09:32.000
 increasingly focused on manipulation, partly where that's a case where the contact has to be

1:09:32.000 --> 1:09:40.880
 much more rich. And there are some really impressive examples of deep learning policies,

1:09:40.880 --> 1:09:50.240
 controllers, that, that can appear to do good things through contact. We've even got new examples of,

1:09:50.240 --> 1:09:54.560
 of, you know, deep learning models of predicting what's going to happen to objects as they go

1:09:54.560 --> 1:10:00.800
 through contact. But I think the challenge you just offered there still eludes us,

1:10:00.800 --> 1:10:04.880
 right? The ability to make a decision based on those models quickly.

1:10:07.360 --> 1:10:11.520
 You know, I have to think though, it's hard for humans to, when you get that complicated. I think

1:10:11.520 --> 1:10:19.360
 probably you had maybe a slow motion version of where you learn the basic skills, and you've

1:10:19.360 --> 1:10:25.760
 probably gotten better at it. And there's, there's much more subtlety, but it might still be hard

1:10:25.760 --> 1:10:32.560
 to actually, you know, really on the fly, take a, you know, model of your humanoid and figure out

1:10:32.560 --> 1:10:36.480
 how to, how to plan the optimal sequence that might be a problem we never solve.

1:10:36.480 --> 1:10:42.320
 Well, the, I mean, one of the most amazing things to me about the, we can talk about

1:10:42.320 --> 1:10:47.280
 martial arts. We could also talk about dancing. It doesn't really matter. It's too human.

1:10:48.880 --> 1:10:53.520
 I think it's the most interesting study of contact. It's not even the dynamic element of it. It's the,

1:10:53.520 --> 1:11:00.800
 the, like when you get good at it, it's so effortless. Like I can just, I'm very cognizant

1:11:00.800 --> 1:11:07.600
 of the entirety of the learning process being essentially like learning how to move my body

1:11:07.600 --> 1:11:17.600
 in a way that I could throw very large weights around effortlessly. Like, and I can feel the

1:11:17.600 --> 1:11:23.120
 learning, like I'm a huge believer in drilling of techniques. And you can just like feel your,

1:11:23.120 --> 1:11:29.040
 I don't, you're not feeling, you're feeling, um, sorry, you're learning it intellectually a little

1:11:29.040 --> 1:11:34.640
 bit, but a lot of it is the body learning it somehow, like instinctually. And whatever that

1:11:34.640 --> 1:11:42.960
 learning is, that's really, I'm not even sure if that's equivalent to a, like a deep learning,

1:11:42.960 --> 1:11:49.200
 learning a controller. I think it's something more, it feels like there's a lot of distributed

1:11:49.200 --> 1:11:58.000
 learning going on. Yeah, I think there's hierarchy and composition, probably in the systems that

1:11:58.000 --> 1:12:03.200
 we don't capture very well yet. You have layers of control systems. You have reflexes at the

1:12:03.200 --> 1:12:09.200
 bottom layer and you have a, you know, a system that's capable of planning a vacation to

1:12:10.320 --> 1:12:14.960
 some distant country, which is probably, you probably don't have a controller, a policy for

1:12:14.960 --> 1:12:21.600
 every possible destination you'll ever pick, right? But there's something magical in the

1:12:21.600 --> 1:12:26.640
 in between. And how do you go from these low level feedback loops to something that feels

1:12:27.440 --> 1:12:33.840
 like a pretty complex set of outcomes? You know, my guess is, I think, I think there's

1:12:33.840 --> 1:12:39.280
 evidence that you can plan at some of these levels, right? So Josh Tenenbaum just showed

1:12:39.280 --> 1:12:43.840
 it in his talk the other day. He's got a game he likes to talk about. I think he calls it the

1:12:43.840 --> 1:12:51.120
 Pick 3 game or something, where he puts a bunch of clutter down in front of a person and he says,

1:12:51.120 --> 1:12:58.560
 okay, pick three objects and it might be a telephone or a shoe or a Kleenex box or whatever.

1:12:59.680 --> 1:13:03.120
 And apparently you pick three items and then you pick, he says, okay, pick the first one

1:13:03.120 --> 1:13:07.440
 up with your right hand, the second one up with your left hand. Now using those objects,

1:13:07.440 --> 1:13:15.600
 those now as tools pick up the third object, right? So that's down at the level of physics

1:13:15.600 --> 1:13:22.080
 and mechanics and contact mechanics that I think we do learning or we do have policies for, we do

1:13:22.080 --> 1:13:27.840
 control for almost feedback. But somehow we're able to still, I mean, I've never picked up a

1:13:27.840 --> 1:13:32.480
 telephone with a shoe and a water bottle before and somehow, and it takes me a little longer to

1:13:32.480 --> 1:13:40.480
 do that the first time. But most of the time we can sort of figure that out. So yeah, I think the

1:13:40.480 --> 1:13:46.480
 amazing thing is this ability to be flexible with our models, plan when we need to use our

1:13:47.040 --> 1:13:53.840
 well oiled controllers when we don't, when we're in familiar territory. Having models,

1:13:53.840 --> 1:13:58.320
 I think the other thing you just said was something about, I think your awareness of what's

1:13:58.320 --> 1:14:03.680
 happening is even changing as you improve your expertise, right? So maybe you have a very

1:14:03.680 --> 1:14:10.800
 approximate model of the mechanics to begin with. And as you gain expertise, you get a more refined

1:14:10.800 --> 1:14:17.520
 version of that model. You're aware of muscles or balanced components that you just weren't

1:14:17.520 --> 1:14:26.560
 even aware of before. So how do you scaffold that? Yeah, plus the fear of injury, the ambition of goals

1:14:26.560 --> 1:14:34.160
 of excelling and fear of mortality. Let's see what else is in there as the motivations,

1:14:35.680 --> 1:14:42.800
 overinflated ego in the beginning, like, and then a crash of confidence in the middle,

1:14:42.800 --> 1:14:48.000
 all of those seem to be essential for the learning process. And if all that's good,

1:14:48.000 --> 1:14:53.200
 then you're probably optimizing energy efficiency. Yeah, right. So we have to get that right. So

1:14:53.200 --> 1:14:59.680
 you know, there was this idea that you would have robots play soccer better

1:15:01.120 --> 1:15:09.280
 than human players by 2050. That was the goal. Basically, was the goal to beat world champion

1:15:09.280 --> 1:15:15.040
 team to become a world cup, be like a world cup level team. So are we going to see that first?

1:15:15.040 --> 1:15:22.640
 Or a robot, if you're familiar, there's an organization called UFC for mixed martial arts.

1:15:23.280 --> 1:15:29.920
 Are we going to see a world cup championship soccer team that are robots? Or a UFC champion

1:15:30.560 --> 1:15:37.040
 mixed martial artist as a robot? I mean, it's very hard to say one thing is a harder one.

1:15:37.040 --> 1:15:44.320
 Some problems are harder than the other. What probably matters is who started the organization

1:15:44.320 --> 1:15:49.200
 that I mean, I think Robocop has a pretty serious following and there is a history now of people

1:15:49.840 --> 1:15:53.920
 playing that game, learning about that game, building robots to play that game, building

1:15:53.920 --> 1:16:00.720
 increasingly more human robots. It's got momentum. So if you want to have mixed martial arts compete,

1:16:00.720 --> 1:16:07.680
 you better start your organization now, right? I think almost independent of which problem is

1:16:07.680 --> 1:16:11.840
 technically harder because they're both hard and they're both different. That's a good point.

1:16:11.840 --> 1:16:17.520
 I mean, those videos are just hilarious that like, especially the human robot's trying to

1:16:19.840 --> 1:16:24.800
 try to play soccer. I mean, they're kind of terrible right now. I mean, I guess there is Robo

1:16:24.800 --> 1:16:30.080
 Sumo wrestling. There's like the Robo one competitions where they do have these robots

1:16:30.080 --> 1:16:34.080
 that go on the table and basically fight. So maybe I'm wrong. Maybe first of all,

1:16:34.080 --> 1:16:38.880
 do you have a year in mind for Robocop? Just from a robotics perspective,

1:16:38.880 --> 1:16:46.720
 seems like a super exciting possibility that like in the physical space, this is what's

1:16:46.720 --> 1:16:54.560
 interesting. I think the world is captivated. I think it's really exciting. It's it inspires

1:16:54.560 --> 1:17:02.720
 just a huge number of people when machine beats a human at a game that humans are really damn

1:17:02.720 --> 1:17:10.720
 good at. So you're talking about chess and go, but that's in the world of digital. I don't think

1:17:11.680 --> 1:17:16.800
 machines have beat humans at a game in the physical space yet, but that would be just...

1:17:17.520 --> 1:17:22.160
 You have to make the rules very carefully, right? I mean, if Atlas kicked me in the shins,

1:17:22.160 --> 1:17:31.040
 I'm down and game over. So it's very subtle on what's fair.

1:17:31.040 --> 1:17:35.200
 I think the fighting one is a weird one. Yeah, because you're talking about a machine that's

1:17:35.200 --> 1:17:39.440
 much stronger than you. But yeah, in terms of soccer, basketball, all those kinds.

1:17:39.440 --> 1:17:44.080
 Even soccer, right? I mean, as soon as there's contact or whatever. And there's,

1:17:44.080 --> 1:17:50.160
 there are some things that the robot will do better. I think if you really set yourself up to

1:17:50.160 --> 1:17:56.240
 try to see could robots win the game of soccer as the rules were written. The right thing for

1:17:56.240 --> 1:18:01.840
 the robot to do is to play very differently than a human would play. You're not going to get the

1:18:01.840 --> 1:18:08.640
 perfect soccer player robot. You're going to get something that exploits the rules, exploits its

1:18:08.640 --> 1:18:15.040
 super actuators. It's super low band with feedback loops or whatever. And it's going to play the

1:18:15.040 --> 1:18:21.200
 game differently than you want it to play. And I bet there's ways, I bet there's loopholes.

1:18:21.200 --> 1:18:28.800
 We saw that in the DARPA challenge that it's very hard to write a set of rules that someone can't

1:18:28.800 --> 1:18:36.560
 find a way to exploit. Let me ask another ridiculous question. I think this might be

1:18:36.560 --> 1:18:44.400
 the last ridiculous question, but I doubt it. I aspire to ask as many ridiculous questions

1:18:44.400 --> 1:18:52.000
 of a brilliant MIT professor. Okay. I don't know if you've seen the black mirror.

1:18:53.440 --> 1:18:59.760
 It's funny. I never watched the episode. I know when it happened though, because I gave a talk

1:19:00.400 --> 1:19:06.080
 to some MIT faculty one day on a, on assuming, you know, Monday or whatever, I was telling them

1:19:06.080 --> 1:19:11.440
 about the state of robotics. And I showed some video from Boston Dynamics of the quadruped

1:19:11.440 --> 1:19:17.680
 spot at the time. It was the early version of spot. And there was a look of horror that went

1:19:17.680 --> 1:19:23.280
 across the room. And I said, what, you know, I've shown videos like this a lot of times. What

1:19:23.280 --> 1:19:28.160
 happened? And it turns out that this video had got, yeah, this black mirror episode had changed

1:19:28.160 --> 1:19:34.160
 the way people watched. Yeah, the videos I was putting out. The way they see these kinds of

1:19:34.160 --> 1:19:39.120
 robots. So I talked to so many people who are just terrified because of that episode probably

1:19:39.120 --> 1:19:43.600
 of these kinds of robots. Hey, I almost want to say that you almost kind of like enjoy being

1:19:43.600 --> 1:19:49.120
 terrified. I don't even know what it is about human psychology that kind of imagine doomsday,

1:19:49.120 --> 1:19:57.520
 the destruction of the universe or our society and kind of like enjoy being afraid. I don't want

1:19:57.520 --> 1:20:04.000
 to simplify it, but it feels like they talk about it so often. It almost, there does seem to be an

1:20:04.000 --> 1:20:11.440
 addictive quality to it. I talked to a guy, a guy named Joe Rogan, who's kind of the flag bearer

1:20:11.440 --> 1:20:18.480
 for being terrified of these robots. Do you have two questions? One, do you have an understanding

1:20:18.480 --> 1:20:25.440
 of why people are afraid of robots? And the second question is, in black mirror, just to tell you

1:20:25.440 --> 1:20:30.960
 the episode, I don't even remember it that much anymore, but these robots, I think they can shoot

1:20:30.960 --> 1:20:38.560
 like a pellet or something. They basically, it's basically a spot with a gun. And how far are we

1:20:38.560 --> 1:20:47.440
 away from having robots that go rogue like that, you know, basically spot that goes rogue for some

1:20:47.440 --> 1:20:57.920
 reason and somehow finds a gun. Right. So I mean, I'm not a psychologist. I think I don't know exactly

1:20:57.920 --> 1:21:08.480
 why people react the way they do. I think, I think we have to be careful about the way robots influence

1:21:08.480 --> 1:21:13.280
 our society and the like. I think that's something that's a responsibility that roboticists need to

1:21:13.280 --> 1:21:18.720
 embrace. I don't think robots are going to come after me with a kitchen knife or a

1:21:18.720 --> 1:21:24.880
 pellet gun right away. And I mean, if they were programmed in such a way, but I used to joke with

1:21:24.880 --> 1:21:31.280
 Atlas that all I had to do was run for five minutes and its battery would run out. But actually,

1:21:31.280 --> 1:21:34.160
 they've got a very big battery in there by the end. So it was over an hour.

1:21:37.040 --> 1:21:43.440
 I think the fear is a bit cultural though, because I mean, you notice that like, I think

1:21:43.440 --> 1:21:49.840
 in my age, in the US, we grew up watching Terminator. If I'd grown up at the same time in

1:21:49.840 --> 1:21:54.800
 Japan, I probably would have been watching Astro Boy. And there's a very different reaction to

1:21:54.800 --> 1:22:00.080
 robots in different countries, right? So I don't know if it's a human innate fear of

1:22:01.280 --> 1:22:07.040
 metal marvels, or if it's something that we've done to ourselves with our sci fi.

1:22:09.600 --> 1:22:13.520
 Yeah, the stories we tell ourselves through, through movies, through just

1:22:15.360 --> 1:22:20.880
 through popular media. But if, if I were to tell, you know, if, if you were my therapist,

1:22:20.880 --> 1:22:25.840
 and I said, I'm really terrified that we're going to have these robots

1:22:27.200 --> 1:22:35.120
 very soon that will hurt us. Like, how do you approach making me feel better?

1:22:36.400 --> 1:22:42.160
 Like, why shouldn't people be afraid? There's a, I think there's a video that went viral

1:22:43.840 --> 1:22:48.160
 recently. Everything, everything was spot in Boston. Nameless goes viral in general.

1:22:48.160 --> 1:22:52.400
 But usually it's like really cool stuff, like they're doing flips and stuff, or like sad stuff.

1:22:54.400 --> 1:23:00.080
 Atlas being hit with a broomstick or something like that. But there's a video where I think

1:23:00.880 --> 1:23:05.920
 one of the new productions bought robots, which are awesome. It was like patrolling

1:23:05.920 --> 1:23:11.760
 somewhere in like, in some country. And like, people immediately were like, saying, like,

1:23:11.760 --> 1:23:17.200
 this is like the dystopian future, like the surveillance state. For some reason, like,

1:23:17.200 --> 1:23:23.280
 you can just have a camera, like something about spot being able to walk on four feet

1:23:23.280 --> 1:23:30.080
 with like really terrified people. So what, what do you say to those people?

1:23:30.880 --> 1:23:35.840
 I think there is a legitimate fear there, because so much of our future is uncertain.

1:23:37.680 --> 1:23:42.240
 But at the same time, technically speaking, it seems like we're not there yet. So what do you

1:23:42.240 --> 1:23:51.680
 say? I mean, I think technology is complicated. It can be used in many ways. I think there are purely

1:23:51.680 --> 1:24:01.600
 software attacks somebody could use to do great damage. Maybe they have already. You know, I think

1:24:01.600 --> 1:24:16.800
 wheeled robots could be used in bad ways to drones, right? I don't think that. Let's see. I don't want

1:24:16.800 --> 1:24:22.240
 to be building technology just because I'm compelled to build technology. And I don't think

1:24:22.240 --> 1:24:29.280
 about it. But I would consider myself a technological optimist, I guess, in the sense that

1:24:29.280 --> 1:24:38.160
 I think we should continue to create and evolve and our world will change. And if we will introduce

1:24:38.160 --> 1:24:45.280
 new challenges, we'll screw something up maybe. But I think also we'll invent ourselves out of

1:24:45.280 --> 1:24:50.560
 those challenges and life will go on. So it's interesting because you didn't mention, like,

1:24:50.560 --> 1:24:57.040
 this is technically too hard. I don't think robots are, I think people attribute a robot that looks

1:24:57.040 --> 1:25:02.720
 like an animal as maybe having a level of self awareness or consciousness or something that

1:25:02.720 --> 1:25:10.480
 they don't have yet, right? So it's not, I think our ability to anthropomorphize those robots is

1:25:10.480 --> 1:25:16.960
 probably, we're assuming that they have a level of intelligence that they don't yet have. And that

1:25:16.960 --> 1:25:23.600
 might be part of the fear. So in that sense, it's too hard. But, you know, there are many scary

1:25:23.600 --> 1:25:29.680
 things in the world, right? So I think we're right to ask those questions. We're right to

1:25:30.800 --> 1:25:32.640
 think about the implications of our work.

1:25:34.000 --> 1:25:40.480
 Right. In the short term, as we're working on it for sure, is there something long term

1:25:41.280 --> 1:25:50.880
 that scares you about our future with AI and robots? A lot of folks from Elon Musk to Sam

1:25:50.880 --> 1:25:57.120
 Harris to a lot of folks talk about the, you know, existential threats about artificial

1:25:57.120 --> 1:26:05.520
 intelligence. Oftentimes robots kind of inspire that the most because of the anthropomorphism.

1:26:05.520 --> 1:26:06.320
 Do you have any fears?

1:26:07.040 --> 1:26:16.320
 It's an important question. I actually, I think I like Rod Brooks answer maybe the best on this,

1:26:16.320 --> 1:26:23.040
 I think, and it's not the only answer he's given over the years, but maybe one of my favorites is,

1:26:23.040 --> 1:26:27.200
 he says, it's not going to be, he's got a book Flesh and Machines, I believe.

1:26:29.200 --> 1:26:32.960
 It's not going to be the robots versus the people. We're all going to be robot people

1:26:34.080 --> 1:26:40.960
 because, you know, we already have smartphones, some of us have serious technology implanted

1:26:40.960 --> 1:26:48.400
 in our bodies already, whether we have a hearing aid or a pacemaker or anything like this. People

1:26:48.400 --> 1:26:57.280
 with amputations might have prosthetics. That's a trend, I think, that is likely to continue. I

1:26:57.280 --> 1:27:06.080
 mean, this is now wild speculation. But I mean, when do we get to cognitive implants and the like?

1:27:06.080 --> 1:27:11.120
 And yeah, with Neuralink, brain computer interfaces. That's interesting. So there's a dance

1:27:11.120 --> 1:27:19.040
 between humans and robots that's going to be, it's going to be impossible to be scared of

1:27:20.560 --> 1:27:26.400
 the other out there, the robot, because the robot will be part of us, essentially. It'd be so

1:27:26.400 --> 1:27:32.880
 intricately sort of part of our society that it might not even be implanted part of us,

1:27:32.880 --> 1:27:38.800
 but just it's so much a part of our society. So in that sense, the smartphone is already the

1:27:38.800 --> 1:27:44.400
 robot we should be afraid of. Yeah. I mean, yeah. And then all the usual fears arise,

1:27:46.160 --> 1:27:53.120
 the misinformation, the manipulation, all those kinds of things that,

1:27:55.360 --> 1:28:00.560
 the problems are all the same. They're human problems, essentially, it feels like.

1:28:00.560 --> 1:28:06.400
 Yeah. I mean, I think the way we interact with each other online is changing the value we put on

1:28:07.600 --> 1:28:11.760
 personal interaction. And that's a crazy big change that's going to happen and rip through

1:28:11.760 --> 1:28:16.000
 our, has already been ripping through our society, right? And that has implications that are

1:28:17.440 --> 1:28:20.880
 massive. I don't know if they should be scared of it or go with the flow, but

1:28:22.480 --> 1:28:29.440
 I don't see some battle lines between humans and robots being the first thing to worry about.

1:28:29.440 --> 1:28:35.360
 I mean, I do want to just, as a kind of comment, maybe you can comment about your just feelings

1:28:35.360 --> 1:28:40.640
 about Boston Dynamics in general, but you know, I love science. I love engineering. I think there's

1:28:40.640 --> 1:28:46.880
 so many beautiful ideas in it. And when I look at Boston Dynamics or legged robots in general,

1:28:47.520 --> 1:28:57.360
 I think they inspire people curiosity and feelings in general, excitement about engineering

1:28:57.360 --> 1:29:02.720
 more than almost anything else in popular culture. And I think that's such an exciting

1:29:03.680 --> 1:29:09.840
 like responsibility and possibility for robotics. And Boston Dynamics is riding that wave pretty

1:29:09.840 --> 1:29:15.680
 damn well. Like they found it, they've discovered that hunger and curiosity in the people and they're

1:29:15.680 --> 1:29:20.560
 doing magic with it. I don't care if the, I mean, I guess that their company have to make money,

1:29:20.560 --> 1:29:26.800
 right? But they're already doing incredible work and inspiring the world about technology.

1:29:26.800 --> 1:29:33.120
 I mean, do you have thoughts about Boston Dynamics and maybe others, your own work

1:29:33.760 --> 1:29:36.320
 in robotics and inspiring the world in that way?

1:29:37.600 --> 1:29:45.120
 I completely agree. I think Boston Dynamics is absolutely awesome. I think I show my kids those

1:29:45.120 --> 1:29:49.520
 videos, you know, and the best thing that happens is sometimes they've already seen them, you know,

1:29:49.520 --> 1:29:58.240
 right. I think, I just think it's a pinnacle of success in robotics that is just one of the

1:29:58.240 --> 1:30:04.320
 best things that's happened. I absolutely completely agree. One of the heartbreaking things to me

1:30:05.120 --> 1:30:12.320
 is how many robotics companies fail. How hard it is to make money with a robotics company.

1:30:12.320 --> 1:30:20.000
 Like iRobot went through hell just to arrive at a Roomba to figure out one product. And then

1:30:20.000 --> 1:30:32.080
 there's so many home robotics companies like Gebo and Anki, Anki, the cutest toy. There's a great

1:30:32.080 --> 1:30:37.840
 robot I thought went down. I'm forgetting a bunch of them, but a bunch of robotics companies fail,

1:30:37.840 --> 1:30:48.240
 Rod's company rethink robotics. Do you have anything hopeful to say about the possibility

1:30:48.240 --> 1:30:54.320
 of making money with robots? Oh, I think you can't just look at the failures. I mean,

1:30:54.320 --> 1:30:58.640
 Boston Dynamics is a success. There's lots of companies that are still doing amazingly good

1:30:59.200 --> 1:31:05.920
 work in robotics. I mean, this is the capitalist ecology or something, right? I think you have

1:31:05.920 --> 1:31:11.360
 many companies. You have many startups and they push each other forward and many of them fail and

1:31:11.360 --> 1:31:17.360
 some of them get through and that's sort of the natural way of those things. I don't know that

1:31:18.720 --> 1:31:23.920
 is robotics really that much worse. I feel the pain that you feel too. Every time I read one of

1:31:23.920 --> 1:31:33.920
 these, sometimes it's friends and I definitely wish it went better differently. But I think it's

1:31:33.920 --> 1:31:41.680
 healthy and good to have bursts of ideas, bursts of activities, ideas. If they are really aggressive,

1:31:41.680 --> 1:31:47.120
 they should fail sometimes. Certainly, that's the research mantra, right? If you're

1:31:48.080 --> 1:31:53.200
 succeeding at every problem you attempt, then you're not choosing aggressively enough.

1:31:53.200 --> 1:31:57.440
 Is it exciting to you, the new spot? Oh, it's so good.

1:31:57.440 --> 1:32:04.560
 When are you getting them as a pet? Yeah, I mean, I have to dig up 75k right now. It's so cool

1:32:04.560 --> 1:32:11.040
 that there's a price tag. You can go and actually buy it. I have a Skydio R1. Love it.

1:32:13.440 --> 1:32:20.960
 No, I would absolutely be a customer. I wonder what your kids would think about it. I actually,

1:32:20.960 --> 1:32:27.120
 Zach from Boston Dynamics would let my kid drive in one of their demos one time,

1:32:27.120 --> 1:32:33.200
 and that was just so good. I'll forever be grateful for that.

1:32:34.160 --> 1:32:39.680
 And there's something magical about the anthropomorphization of that arm. It has another

1:32:39.680 --> 1:32:48.080
 level of human connection. I'm not sure we understand from a control aspect the value of

1:32:48.080 --> 1:32:55.680
 anthropomorphization. I think that's an understudied and under understood

1:32:55.680 --> 1:33:02.320
 engineering problem. Psychologists have been studying it. I think it's part, manipulating

1:33:02.320 --> 1:33:08.400
 our mind to believe things is a valuable engineering. This is another degree of

1:33:08.400 --> 1:33:11.600
 freedom that can be controlled. I like that. Yeah, I think that's right. I think,

1:33:11.600 --> 1:33:18.720
 there's something that humans seem to do, or maybe my dangerous introspection, is

1:33:20.320 --> 1:33:26.960
 I think we are able to make very simple models that assume a lot about the world very quickly.

1:33:27.760 --> 1:33:32.640
 And then it takes us a lot more time, like your wrestling. You probably thought you knew what

1:33:32.640 --> 1:33:36.800
 you were doing with wrestling, and you were fairly functional as a complete wrestler,

1:33:36.800 --> 1:33:43.040
 and then you slowly got more expertise. So maybe it's natural that our first

1:33:44.800 --> 1:33:50.480
 level of defense against seeing a new robot is to think of it in our existing models of how

1:33:50.480 --> 1:33:54.880
 humans and animals behave. And it's just, as you spend more time with it,

1:33:54.880 --> 1:33:59.120
 then you'll develop more sophisticated models that will appreciate the differences.

1:33:59.120 --> 1:34:06.960
 Exactly. Can you say, what does it take to control a robot? Like, what is the control

1:34:06.960 --> 1:34:11.920
 problem of a robot? And in general, what is a robot in your view? Like, how do you think of this

1:34:13.200 --> 1:34:19.920
 system? What is a robot? What is a robot? I think, I told you ridiculous questions.

1:34:19.920 --> 1:34:26.320
 No, no, it's good. I mean, there's standard definitions of combining computation with

1:34:26.320 --> 1:34:31.360
 some ability to do mechanical work. I think that gets us pretty close. But I think

1:34:32.720 --> 1:34:38.720
 robotics has this problem that once things really work, we don't call them robots anymore.

1:34:40.320 --> 1:34:46.240
 My dishwasher at home is pretty sophisticated. Beautiful mechanisms. There's actually a pretty

1:34:46.240 --> 1:34:49.920
 good computer, probably a couple of chips in there doing amazing things. We don't think of

1:34:49.920 --> 1:34:55.280
 that as a robot anymore, which isn't fair. Because then, roughly, it means that robotics

1:34:55.280 --> 1:35:00.480
 always has to solve the next problem and doesn't get to celebrate its past successes.

1:35:00.480 --> 1:35:09.040
 I mean, even factory room floor robots are super successful. They're amazing. But that's not the

1:35:09.040 --> 1:35:13.280
 ones, I mean, people think of them as robots, but they don't, if you ask what are the successes of

1:35:13.280 --> 1:35:20.080
 robotics, somehow it doesn't come to your mind immediately. So the definition of robot is a

1:35:20.080 --> 1:35:25.840
 system with some level automation that fails frequently. Something like it's the computation

1:35:25.840 --> 1:35:36.880
 plus mechanical work and unsolved problems. Yeah. So from a perspective of control and mechanics,

1:35:36.880 --> 1:35:43.120
 dynamics, what is a robot? So there are many different types of robots. The control that you

1:35:43.120 --> 1:35:52.240
 need for a Jibo robot, some robot that's sitting on your countertop and interacting with you,

1:35:52.240 --> 1:35:56.640
 but not touching you, for instance, is very different than what you need for an autonomous car

1:35:56.640 --> 1:36:01.920
 or an autonomous drone. It's very different than what you need for a robot that's going to walk or

1:36:02.480 --> 1:36:09.520
 pick things up with its hands. My passion has always been for the places where you're

1:36:09.520 --> 1:36:17.440
 interacting or doing more dynamic interactions with the world. So walking, now manipulation.

1:36:18.560 --> 1:36:25.600
 And the control problems there are beautiful. I think contact is one thing that differentiates

1:36:25.600 --> 1:36:31.440
 them from many of the control problems we've solved classically. The modern control grew up

1:36:31.440 --> 1:36:36.240
 stabilizing fighter jets that were passively unstable. And there's amazing success stories

1:36:36.240 --> 1:36:43.360
 from control all over the place. Power grid, I mean, there's all kinds of, it's everywhere

1:36:43.360 --> 1:36:50.320
 that we don't even realize, just like AI is now. Do you mention contact? Like, what's contact?

1:36:51.440 --> 1:36:57.840
 So an airplane is an extremely complex system or a spacecraft landing or whatever. But at least

1:36:57.840 --> 1:37:04.800
 it has the luxury of things change relatively continuously. That's an oversimplification.

1:37:04.800 --> 1:37:11.920
 But if I make a small change in the command I send to my actuator, then the path that the robot will

1:37:11.920 --> 1:37:18.640
 take tends to take a change only by a small amount. And there's a feedback mechanism here.

1:37:18.640 --> 1:37:22.240
 That's what we're talking about. And there's a feedback mechanism. And thinking about this as

1:37:22.880 --> 1:37:29.600
 locally like a linear system, for instance, I can use more linear algebra tools to study

1:37:29.600 --> 1:37:35.200
 systems like that, generalizations of linear algebra to these smooth systems.

1:37:36.240 --> 1:37:42.400
 What is contact? The robot has something very discontinuous that happens when it

1:37:42.400 --> 1:37:47.360
 makes or breaks, when it starts touching the world. And even the way it touches or the order of

1:37:47.360 --> 1:37:54.720
 contacts can change the outcome in potentially unpredictable ways. Not unpredictable, but

1:37:54.720 --> 1:38:03.280
 complex ways. I do think there's a little bit of... A lot of people will say that contact is hard

1:38:03.280 --> 1:38:10.000
 in robotics, even to simulate. And I think there's a little bit of a... There's truth to that, but

1:38:10.000 --> 1:38:19.440
 maybe a misunderstanding around that. So what is limiting is that when we think about our robots

1:38:19.440 --> 1:38:27.200
 and we write our simulators, we often make an assumption that objects are rigid. And when it

1:38:27.200 --> 1:38:33.040
 comes down, that their mass moves all... It stays in a constant position relative to each other

1:38:33.040 --> 1:38:41.520
 itself. And that leads to some paradoxes when you go to try to talk about rigid body mechanics

1:38:41.520 --> 1:38:50.320
 and contact. And so, for instance, if I have a three legged stool with just a... Imagine it comes

1:38:50.320 --> 1:38:56.800
 to a point at the leg. So it's only touching the world at a point. If I draw my physics...

1:38:56.800 --> 1:39:02.160
 My high school physics diagram of this system, then there's a couple of things that I'm given

1:39:02.160 --> 1:39:07.280
 by elementary physics. I know if the system... If the table is at rest, if it's not moving,

1:39:07.280 --> 1:39:13.680
 zero velocities. That means that the normal force... All the forces are in balance. So the

1:39:14.640 --> 1:39:19.840
 force of gravity is being countered by the forces that the ground is pushing on my table legs.

1:39:21.200 --> 1:39:28.160
 I also know, since it's not rotating, that the moments have to balance. And since it's a three

1:39:28.160 --> 1:39:34.000
 dimensional table, it could fall in any direction, it actually tells me uniquely what those three

1:39:34.000 --> 1:39:40.400
 normal forces have to be. If I have four legs on my table, four legged table,

1:39:41.760 --> 1:39:46.080
 and they were perfectly machined to be exactly the right same height and they're set down and

1:39:46.080 --> 1:39:53.520
 the table's not moving, then the basic conservation laws don't tell me... There are many solutions for

1:39:53.520 --> 1:39:58.560
 the forces that the ground could be putting on my legs that would still result in the table not

1:39:58.560 --> 1:40:05.120
 moving. Now, the reason that seems fine, I could just pick one. But it gets funny now because if

1:40:05.120 --> 1:40:12.720
 you think about friction, what we think about with friction is our standard model says the amount

1:40:12.720 --> 1:40:17.920
 of force that the table will push back if I were to now try to push my table sideways,

1:40:17.920 --> 1:40:25.280
 I guess I have a table here, is proportional to the normal force. So if I have... If I'm

1:40:25.280 --> 1:40:29.440
 barely touching and I push, I'll slide, but if I'm pushing more and I push, I will slide less.

1:40:30.320 --> 1:40:35.040
 It's called Coulomb friction is our standard model. Now, if you don't know what the normal

1:40:35.040 --> 1:40:41.760
 force is on the four legs and you push the table, then you don't know what the friction forces are

1:40:41.760 --> 1:40:48.400
 going to be. And so you can't actually tell, the laws just aren't explicit yet about which

1:40:48.400 --> 1:40:52.640
 way the table's going to go. It could veer off to the left, it could veer off to the right,

1:40:52.640 --> 1:40:59.760
 it could go straight. So the rigid body assumption of contact leaves us with some paradoxes,

1:40:59.760 --> 1:41:04.000
 which are annoying for writing simulators and for writing controllers.

1:41:06.080 --> 1:41:13.040
 We still do that sometimes because soft contact is potentially harder numerically or whatever,

1:41:13.040 --> 1:41:17.920
 and the best simulators do both or do some combination of the two. But anyways, because

1:41:17.920 --> 1:41:23.520
 of these kind of paradoxes, there's all kinds of paradoxes in contact, mostly due to these

1:41:23.520 --> 1:41:30.000
 rigid body assumptions. It becomes very hard to write the same kind of control laws that we've

1:41:30.000 --> 1:41:35.280
 been able to be successful with for fighter jets. We haven't been as successful writing those

1:41:35.280 --> 1:41:40.080
 controllers for manipulation. And so you don't know what's going to happen at the point of

1:41:40.080 --> 1:41:44.240
 contact, at the moment of contact. There are situations absolutely where you... Where our

1:41:44.240 --> 1:41:50.320
 laws don't tell us. So the standard approach, that's okay. I mean, instead of having a differential

1:41:50.320 --> 1:41:55.440
 equation, you end up with a differential inclusion, it's called. It's a set valued

1:41:55.440 --> 1:42:00.160
 equation. It says that I'm in this configuration, I have these forces applied on me,

1:42:01.520 --> 1:42:03.680
 and there's a set of things that could happen.

1:42:05.280 --> 1:42:12.960
 And those aren't continuous, I mean, so when you're saying non smooth, they're not only not

1:42:12.960 --> 1:42:19.760
 smooth, but this is discontinuous. The non smooth comes in when I make or break a new contact first,

1:42:20.320 --> 1:42:25.200
 or when I transition from stick to slip. So you typically have static friction,

1:42:25.200 --> 1:42:30.640
 and then you'll start sliding, and that'll be a discontinuous change in velocity, for instance.

1:42:31.200 --> 1:42:34.400
 Especially if you come to rest or... That's so fascinating.

1:42:34.400 --> 1:42:44.320
 Okay, so what do you do? Sorry, I interrupted you. What's the hope under so much uncertainty about

1:42:44.320 --> 1:42:48.400
 what's going to happen? What are you supposed to do? I mean, control has an answer for this.

1:42:48.400 --> 1:42:54.080
 Robust control is one approach, but roughly, you can write controllers which try to still

1:42:54.080 --> 1:42:58.880
 perform the right task, despite all the things that could possibly happen. The world might want

1:42:58.880 --> 1:43:03.520
 the table to go this way and this way, but if I write a controller that pushes a little bit more

1:43:03.520 --> 1:43:07.280
 and pushes a little bit, I can certainly make the table go in the direction I want.

1:43:07.840 --> 1:43:13.760
 It just puts a little bit more of a burden on the control system. And these discontinuities

1:43:13.760 --> 1:43:19.360
 do change the control system, because the way we write it down right now,

1:43:21.120 --> 1:43:27.680
 every different control configuration, including sticking or sliding or parts of my body that

1:43:27.680 --> 1:43:32.880
 are in contact or not, looks like a different system. And I think of them, I reason about them

1:43:32.880 --> 1:43:39.760
 separately or differently, and the combinatorics of that blow up. So I just don't have enough time

1:43:39.760 --> 1:43:48.080
 to compute all the possible contact configurations of my humanoid. Interestingly, I mean, I'm a

1:43:48.080 --> 1:43:54.160
 humanoid. I have lots of degrees of freedom, lots of joints. I've only been around for a

1:43:54.160 --> 1:44:00.000
 handful of years, it's getting up there, but I haven't had time in my life to visit all of the

1:44:00.000 --> 1:44:08.240
 states in my system, certainly all the contact configurations. So if step one is to consider

1:44:08.240 --> 1:44:14.960
 every possible contact configuration that I'll ever be in, that's probably not a problem I need to

1:44:14.960 --> 1:44:22.640
 solve, right? Just as a small tangent, what's the contact configuration? Just so we can

1:44:22.640 --> 1:44:29.920
 enumerate what are we talking about? How many are there? The simplest example maybe would be

1:44:29.920 --> 1:44:37.280
 imagine a robot with a flat foot. And we think about the phases of gate where the heel strikes,

1:44:37.280 --> 1:44:44.960
 and then the front toe strikes, and then you can heel up, toe off. Those are each different

1:44:44.960 --> 1:44:49.840
 contact configurations. I only had two different contacts, but I ended up with four different

1:44:49.840 --> 1:44:57.840
 contact configurations. Now, of course, my robot might actually have bumps on it or other things,

1:44:57.840 --> 1:45:03.040
 so it could be much more subtle than that, right? But it's just even with one sort of box

1:45:03.040 --> 1:45:08.000
 interacting with the ground already in the plane has that many, right? And if I was just even a

1:45:08.000 --> 1:45:13.360
 3D foot, then probably my left toe might touch just before my right toe and things get subtle.

1:45:13.360 --> 1:45:20.400
 Now, if I'm a dexterous hand, and I go to talk about just grabbing a water bottle,

1:45:22.240 --> 1:45:29.920
 if I have to enumerate every possible order that my hand came into contact with the bottle,

1:45:30.800 --> 1:45:35.920
 then I'm dead in the water. Any approach that we were able to get away with that in walking,

1:45:36.960 --> 1:45:40.160
 because we mostly touch the ground with a small number of points, for instance,

1:45:40.160 --> 1:45:43.520
 and we haven't been able to get dexterous hands that way.

1:45:45.040 --> 1:45:55.600
 So, you've mentioned that people think that contact is really hard, and that's the reason

1:45:55.600 --> 1:46:05.520
 that robotic manipulation is problem is really hard. Is there any flaws in that thinking?

1:46:05.520 --> 1:46:14.080
 So, I think simulating contact is one aspect, and people often say that one of the reasons

1:46:14.080 --> 1:46:19.200
 that we have a limit in robotics is because we do not simulate contact accurately in our

1:46:19.200 --> 1:46:26.480
 simulators. And I think that is the extent to which that's true is partly because our

1:46:27.200 --> 1:46:33.120
 simulators, we haven't got mature enough simulators. There are some things that are still hard,

1:46:33.120 --> 1:46:41.760
 difficult that we should change. But we actually know what the governing equations are. They have

1:46:41.760 --> 1:46:46.560
 some foibles like this indeterminacy, but we should be able to simulate them accurately.

1:46:48.480 --> 1:46:53.040
 We have incredible open source community in robotics, but it actually just takes a professional

1:46:53.040 --> 1:46:57.440
 engineering team a lot of work to write a very good simulator like that.

1:46:57.440 --> 1:47:01.840
 Now, where does I believe you've written Drake?

1:47:03.120 --> 1:47:06.640
 There's a team of people. I certainly spent a lot of hours on it myself.

1:47:07.840 --> 1:47:17.200
 What is Drake? What does it take to create a simulation environment for the kind of

1:47:17.200 --> 1:47:19.120
 difficult control problems we're talking about?

1:47:20.560 --> 1:47:25.840
 Right. So, Drake is the simulator that I've been working on. There are other good simulators out

1:47:25.840 --> 1:47:31.360
 there. I don't like to think of Drake as just a simulator because we write our controllers in

1:47:31.360 --> 1:47:36.560
 Drake. We write our perception systems a little bit in Drake, but we write all of our low level

1:47:36.560 --> 1:47:45.840
 control and even planning and optimization capabilities. Drake is three things roughly.

1:47:45.840 --> 1:47:53.600
 It's an optimization library, which provides a layer of abstraction in C++ and Python

1:47:53.600 --> 1:47:59.600
 for commercial solvers. You can write linear programs, quadratic programs,

1:48:00.640 --> 1:48:05.040
 semi definite programs, sums of squares programs, the ones we've used mixed integer programs,

1:48:05.600 --> 1:48:09.680
 and it will do the work to curate those and send them to whatever the right solver is,

1:48:09.680 --> 1:48:16.240
 for instance, and it provides a level of abstraction. The second thing is a system

1:48:16.240 --> 1:48:23.440
 modeling language, a bit like LabView or Simulink, where you can make block diagrams out of complex

1:48:23.440 --> 1:48:29.680
 systems, or it's like ROS in that sense, where you might have lots of ROS nodes that are each

1:48:29.680 --> 1:48:37.200
 doing some part of your system, but to contrast it with ROS, we try to write, if you write a Drake

1:48:37.200 --> 1:48:43.520
 system, then you have to, it asks you to describe a little bit more about the system. If you have any

1:48:44.400 --> 1:48:47.840
 state, for instance, in the system, there are any variables that are going to persist, you have to

1:48:47.840 --> 1:48:53.680
 declare them, parameters can be declared and the like, but the advantage of doing that is that you

1:48:53.680 --> 1:49:00.080
 can, if you like, run things all on one process, but you can also do control design against it,

1:49:00.080 --> 1:49:07.760
 you can do, I mean, simple things like rewinding and playing back your simulations. For instance,

1:49:07.760 --> 1:49:12.000
 these things, you get some rewards for spending a little bit more upfront cost in describing each

1:49:12.000 --> 1:49:20.000
 system. And I was inspired to do that because I think the complexity of Atlas, for instance,

1:49:21.200 --> 1:49:25.280
 is just so great. And I think although, I mean, ROS has been incredible,

1:49:26.080 --> 1:49:34.080
 absolute huge fan of what it's done for the robotics community, but the ability to rapidly

1:49:34.080 --> 1:49:41.040
 put different pieces together and have a functioning thing is very good. But I do think that it's

1:49:41.040 --> 1:49:47.520
 hard to think clearly about a bag of disparate parts, Mr. Potato Head kind of software stack.

1:49:48.080 --> 1:49:54.880
 And if you can, you know, ask a little bit more out of each of those parts, then you can understand

1:49:54.880 --> 1:50:00.960
 there the way they work better, you can try to verify them and the like, or you can do learning

1:50:00.960 --> 1:50:06.240
 against them. And then one of those systems, the last thing I said the first two things that Drake

1:50:06.240 --> 1:50:12.400
 is, but the last thing is that there is a set of multi body equations, rigid body equations,

1:50:12.400 --> 1:50:19.680
 that is trying to provide a system that simulates physics. And we also have renderers and other

1:50:19.680 --> 1:50:25.600
 things, but I think the physics component of Drake is special in the sense that we have done

1:50:26.800 --> 1:50:31.440
 excessive amount of engineering to make sure that we've written the equations correctly.

1:50:31.440 --> 1:50:35.760
 Every possible tumbling satellite or spinning top or anything that we could possibly write as a test

1:50:35.760 --> 1:50:42.880
 is tested. We are making some, you know, I think fundamental improvements on the way you simulate

1:50:42.880 --> 1:50:51.200
 contact. Just what does it take to simulate contact? I mean, it just seems, I mean, there's

1:50:51.200 --> 1:50:55.920
 something just beautiful the way you're like explaining contact and you're like tapping

1:50:55.920 --> 1:51:02.080
 your fingers on the on the table while you're while you're doing it, just easily easily,

1:51:02.080 --> 1:51:11.360
 just like, just not even like, it was like helping you think, I guess. What I do, you have this like

1:51:11.360 --> 1:51:22.880
 awesome demo of loading or unloading a dishwasher, just picking up a plate, grasping it like for the

1:51:22.880 --> 1:51:34.480
 first time. That just seems like so difficult. How do you simulate any of that? So it was really

1:51:34.480 --> 1:51:39.840
 interesting that what happened was that we started getting more professional about our software

1:51:39.840 --> 1:51:46.000
 development during the DARPA Robotics Challenge. I learned the value of software engineering and

1:51:46.000 --> 1:51:53.120
 how to bridle complexity. I guess that's what I want to somehow fight against and bring some

1:51:53.120 --> 1:51:57.920
 of the clear thinking of controls into these complex systems we're building for robots.

1:52:00.400 --> 1:52:04.480
 Shortly after the DARPA Robotics Challenge, Toyota opened a research institute,

1:52:04.480 --> 1:52:11.120
 TRI, Toyota Research Institute. They put one of their, there's three locations, one of them is

1:52:11.120 --> 1:52:20.320
 just down the street from MIT, and I helped ramp that up as a part of the end of my sabbatical,

1:52:20.320 --> 1:52:31.600
 I guess. So TRI has given me the TRI Robotics effort, has made this investment in simulation

1:52:31.600 --> 1:52:37.120
 in Drake, and Michael Sherman leads a team there of just absolutely top notch dynamics experts

1:52:37.120 --> 1:52:42.800
 that are trying to write those simulators that can pick up the dishes. And there's also a team

1:52:42.800 --> 1:52:47.920
 working on manipulation there that is taking problems like loading the dishwasher,

1:52:48.880 --> 1:52:54.560
 and we're using that to study these really hard corner cases kind of problems in manipulation.

1:52:55.200 --> 1:53:02.080
 So for me, this simulating the dishes, we could actually write a controller, if we just cared

1:53:02.080 --> 1:53:06.720
 about picking up dishes in the sink once, we could write a controller without any simulation

1:53:06.720 --> 1:53:14.160
 whatsoever, and we could call it done. But we want to understand what is the path you take

1:53:14.160 --> 1:53:21.280
 to actually get to a robot that could perform that for any dish in anybody's kitchen

1:53:22.000 --> 1:53:27.920
 with enough confidence that it could be a commercial product, right? And it has deep

1:53:27.920 --> 1:53:31.760
 learning perception in the loop, it has complex dynamics in the loop, it has controller, it has

1:53:31.760 --> 1:53:38.160
 a planner, and how do you take all of that complexity and put it through this engineering

1:53:38.160 --> 1:53:45.360
 discipline and verification and validation process to actually get enough confidence to deploy?

1:53:46.240 --> 1:53:51.840
 I mean, the DARPA challenge made me realize that that's not something you throw over the fence

1:53:51.840 --> 1:53:57.280
 and hope that somebody will harden it for you, that there are really fundamental challenges

1:53:57.280 --> 1:54:02.000
 in closing that last gap. During the validation and the testing?

1:54:03.360 --> 1:54:07.680
 I think it might even change the way we have to think about the way we write systems.

1:54:09.680 --> 1:54:18.240
 What happens if you have the robot running lots of tests and it screws up, it breaks a dish, right?

1:54:18.880 --> 1:54:24.880
 How do you capture that? I said you can't run the same simulation or the same experiment twice

1:54:24.880 --> 1:54:32.640
 on a real robot. Do we have to be able to bring that one off failure back into simulation in

1:54:32.640 --> 1:54:39.360
 order to change our controllers, study it, make sure it won't happen again? Is it enough to just try

1:54:39.360 --> 1:54:44.880
 to add that to our distribution and understand that on average, we're going to cover that situation

1:54:44.880 --> 1:54:51.760
 again? There's really subtle questions at the corner cases that I think we don't yet have

1:54:51.760 --> 1:54:57.440
 satisfying answers for. How do you find the corner cases? That's one kind of... Do you think

1:54:57.440 --> 1:55:03.760
 it's possible to create a systematized way of discovering corner cases efficiently

1:55:05.120 --> 1:55:09.760
 in whatever the problem is? Yes. I mean, I think we have to get better at that.

1:55:10.640 --> 1:55:16.560
 I mean, control theory has, for decades, talked about active experiment design.

1:55:16.560 --> 1:55:23.440
 What's that? People call it curiosity these days. It's roughly this idea of trying to

1:55:24.000 --> 1:55:29.600
 exploration or exploitation, but the active experiment design is even more specific. You

1:55:29.600 --> 1:55:36.000
 could try to understand the uncertainty in your system, design the experiment that will

1:55:36.000 --> 1:55:41.200
 provide the maximum information to reduce that uncertainty. If there's a parameter you want

1:55:41.200 --> 1:55:46.640
 to learn about, what is the optimal trajectory I could execute to learn about that parameter,

1:55:46.640 --> 1:55:52.880
 for instance. Scaling that up to something that has a deep network in the loop and a

1:55:52.880 --> 1:56:00.880
 planning in the loop is tough. We've done some work with Matt O. Kelly and Amansina. We've worked

1:56:00.880 --> 1:56:06.160
 on some falsification algorithms that are trying to do rare event simulation that try to just

1:56:06.160 --> 1:56:15.600
 hammer on your simulator. If your simulator is good enough, you can write good algorithms

1:56:15.600 --> 1:56:23.200
 that try to spend most of their time in the corner cases. You basically imagine you're building

1:56:24.880 --> 1:56:28.800
 an autonomous car and you want to put it in, I don't know, downtown New Delhi all the time,

1:56:29.360 --> 1:56:35.200
 accelerated testing. If you can write sampling strategies which figure out where your controller

1:56:35.200 --> 1:56:39.440
 is performing badly in simulation and start generating lots of examples around that.

1:56:40.720 --> 1:56:47.200
 It's just the space of possible places where things can go wrong is very big,

1:56:47.840 --> 1:56:52.880
 so it's hard to write those algorithms. Rare event simulation is just a really compelling

1:56:52.880 --> 1:56:58.480
 notion if it's possible. We joke and we call it the black swan generator.

1:56:59.920 --> 1:57:03.600
 Because you don't just want the rare events, you want the ones that are highly impactful.

1:57:03.600 --> 1:57:14.800
 I mean, those are the most profound questions we ask of our world. What's the worst that

1:57:14.800 --> 1:57:21.840
 can happen? What we're really asking isn't some kind of computer science worst case analysis.

1:57:22.400 --> 1:57:28.960
 We're asking what are the millions of ways this can go wrong? That's our curiosity.

1:57:28.960 --> 1:57:38.160
 We humans, I think are pretty bad at, we just run into it. I think there's a distributed

1:57:38.160 --> 1:57:44.320
 sense because there's now like 7.5 billion of us. There's a lot of them and then a lot of them write

1:57:44.320 --> 1:57:48.480
 blog posts about the stupid thing they've done, so we learn in a distributed way.

1:57:50.720 --> 1:57:55.840
 I think that's going to be important for robots too. That's another massive theme

1:57:55.840 --> 1:58:04.640
 at Toyota Research for Robotics is this fleet learning concept. The idea that I as a human,

1:58:04.640 --> 1:58:10.640
 I don't have enough time to visit all of my states. It's very hard for one robot to experience

1:58:10.640 --> 1:58:16.800
 all the things, but that's not actually the problem we have to solve. We're going to have

1:58:16.800 --> 1:58:24.080
 fleets of robots that can have very similar appendages. At some point, maybe collectively,

1:58:24.080 --> 1:58:30.480
 they have enough data that their computational processes should be set up differently than ours.

1:58:34.320 --> 1:58:43.840
 All these dishwasher unloading robots, that robot dropping a plate and a human looking

1:58:43.840 --> 1:58:52.240
 at the robot probably pissed off, but that's a special moment to record. I think one thing

1:58:52.240 --> 1:58:58.000
 in terms of fleet learning, and I've seen that because I've talked to a lot of folks just like

1:58:58.960 --> 1:59:04.480
 Tesla users or Tesla drivers. They're another company that's using this kind of fleet learning

1:59:04.480 --> 1:59:12.160
 idea. One hopeful thing I have about humans is they really enjoy when a system improves

1:59:12.160 --> 1:59:18.800
 learns, so they enjoy fleet learning. The reason it's hopeful for me is they're willing to put up

1:59:18.800 --> 1:59:27.920
 with something that's kind of dumb right now. If it's improving, they almost enjoy being part

1:59:27.920 --> 1:59:33.840
 of the teaching it, almost like if you have kids, you're teaching them something. I think that's

1:59:33.840 --> 1:59:40.560
 a beautiful thing because that gives me hope that we can put dumb robots out there. The problem

1:59:41.120 --> 1:59:46.960
 on the Tesla side with cars, cars can kill you. That makes the problem so much harder.

1:59:46.960 --> 1:59:54.800
 Dishwasher unloading is a little safe. That's why Home Robotics is really exciting. Just to

1:59:54.800 --> 2:00:03.360
 clarify, for people who might not know, I mean, TRI, Toyota Research Institute, they're pretty

2:00:03.360 --> 2:00:10.000
 well known for autonomous vehicle research, but they're also interested in Home Robotics.

2:00:11.120 --> 2:00:15.040
 There's a big group working on multiple groups working on Home Robotics. It's a major part

2:00:15.040 --> 2:00:21.840
 of the portfolio. There's also a couple other projects and advanced materials discovery using

2:00:21.840 --> 2:00:27.200
 AI and machine learning to discover new materials for car batteries and the like,

2:00:27.200 --> 2:00:32.320
 for instance. That's been actually an incredibly successful team. There's new projects starting

2:00:32.320 --> 2:00:44.080
 up too. Do you see a future of where robots are in our home and robots that have actuators that

2:00:44.080 --> 2:00:51.600
 look like arms in our home or more like humanoid type robots? We're going to do the same thing

2:00:51.600 --> 2:00:57.200
 that you just mentioned that dishwasher is no longer a robot. We're going to just not even

2:00:57.200 --> 2:01:03.680
 see them as robots. What's your vision of the home of the future, 10, 20 years from now,

2:01:03.680 --> 2:01:10.960
 50 years if you get crazy? I think we already have Roombas cruising around. We have

2:01:10.960 --> 2:01:17.920
 Alexis or Google Homes on our kitchen counter. It's only a matter of time till they spring arms

2:01:17.920 --> 2:01:26.000
 and start doing something useful like that. I do think it's coming. I think lots of people

2:01:26.000 --> 2:01:31.680
 have lots of motivations for doing it. It's been super interesting actually learning about

2:01:32.320 --> 2:01:36.000
 Toyota's vision for it, which is about helping people age in place,

2:01:36.000 --> 2:01:44.160
 because I think that's not necessarily the first entry, the most lucrative entry point,

2:01:44.160 --> 2:01:49.200
 but it's the problem maybe that we really need to solve no matter what.

2:01:52.480 --> 2:01:57.360
 I think there's a real opportunity. It's a delicate problem. How do you work with people,

2:01:57.360 --> 2:02:07.040
 help people, keep them active, engaged, but improve the quality of life and help them age

2:02:07.040 --> 2:02:13.360
 in place, for instance? It's interesting because older folks are also, I mean, there's a contrast

2:02:13.360 --> 2:02:19.600
 there because they're not always the folks who are the most comfortable with technology, for

2:02:19.600 --> 2:02:28.160
 example. There's a division that's interesting there that you can do so much good with a robot

2:02:28.160 --> 2:02:37.360
 for older folks, but there's a gap to feel of understanding. I mean, it's actually kind of

2:02:37.360 --> 2:02:42.960
 beautiful. Robot is learning about the human and the human is kind of learning about this

2:02:42.960 --> 2:02:53.200
 new robot thing. Also, when I talk to my parents about robots, there's a little bit of a blank slate

2:02:53.200 --> 2:03:02.000
 there too. I mean, they don't know anything about robotics, so it's completely wide open.

2:03:03.920 --> 2:03:10.160
 My parents haven't seen Black Mirror, so it's a blank slate. Here's a cool thing,

2:03:10.160 --> 2:03:14.240
 like what can you do for me? It's an exciting space.

2:03:14.240 --> 2:03:22.000
 I think it's a really important space. I do feel like a few years ago, drones were successful enough

2:03:22.000 --> 2:03:28.240
 in academia. They kind of broke out and started an industry and autonomous cars have been happening.

2:03:28.960 --> 2:03:35.600
 It does feel like manipulation in logistics, of course, first, but in the home shortly after,

2:03:35.600 --> 2:03:41.920
 seems like one of the next big things that's going to really pop. I don't think we talked about it,

2:03:41.920 --> 2:03:50.720
 but what's soft robotics? We talked about rigid bodies. If we can just linger on this

2:03:50.720 --> 2:04:01.360
 whole touch thing. What's soft robotics? I told you that I really dislike the fact that robots

2:04:01.360 --> 2:04:07.280
 are afraid of touching the world all over their body. There's a couple reasons for that. If you

2:04:07.280 --> 2:04:11.600
 look carefully at all the places that robots actually do touch the world, they're almost

2:04:11.600 --> 2:04:16.640
 always soft. They have some sort of pad on their fingers or a rubber sole on their foot,

2:04:17.680 --> 2:04:26.240
 but if you look up and down the arm, we're just pure aluminum or something. That makes it hard,

2:04:26.240 --> 2:04:34.160
 actually. In fact, hitting the table with your rigid arm or nearly rigid arm has some of the

2:04:34.160 --> 2:04:38.640
 problems that we talked about in terms of simulation. I think it fundamentally changes

2:04:38.640 --> 2:04:44.880
 the mechanics of contact when you're soft. You turn point contacts into patch contacts,

2:04:44.880 --> 2:04:50.560
 which can have torsional friction. You can have distributed load. If I want to pick up an egg,

2:04:50.560 --> 2:04:56.640
 right? If I pick it up with two points, then in order to put enough force to sustain the

2:04:56.640 --> 2:05:01.360
 weight of the egg, I might have to put a lot of force to break the egg. If I envelop it with

2:05:02.400 --> 2:05:08.320
 contact all around, then I can distribute my force across the shell of the egg and have a

2:05:08.320 --> 2:05:13.920
 better chance of not breaking it. Soft robotics is for me a lot about changing the mechanics

2:05:13.920 --> 2:05:21.840
 of contact. Does it make the problem a lot harder? Quite the opposite.

2:05:23.920 --> 2:05:32.320
 It changes the computational problem. I think our world and our mathematics has

2:05:32.320 --> 2:05:36.560
 biased us towards rigid, but it really should make things better in some ways.

2:05:36.560 --> 2:05:45.120
 Right? I think the future is unwritten there, but the other thing is...

2:05:45.120 --> 2:05:48.560
 I think ultimately, sorry to interrupt, but I think ultimately it will make things simpler

2:05:49.360 --> 2:05:58.720
 if we embrace the softness of the world. It makes things smoother, right? The result of

2:05:58.720 --> 2:06:04.160
 small actions is less discontinuous, but it also means potentially less

2:06:04.160 --> 2:06:11.120
 instantaneously bad, for instance. I won't necessarily contact something and send it flying off.

2:06:13.200 --> 2:06:17.680
 The other aspect of it that just happens to dovetail really well is that soft robotics tends

2:06:17.680 --> 2:06:24.240
 to be a place where we can embed a lot of sensors to. If you change your hardware and make it more

2:06:24.240 --> 2:06:28.560
 soft, then you can potentially have a tactile sensor, which is measuring the deformation.

2:06:28.560 --> 2:06:38.080
 There's a team at TRI that's working on soft hands, and you get so much more information.

2:06:38.080 --> 2:06:46.080
 If you can put a camera behind the skin roughly and get fantastic tactile information, which is

2:06:47.920 --> 2:06:50.960
 it's super important. In manipulation, one of the things that really

2:06:51.600 --> 2:06:56.160
 is frustrating is if you work super hard on your head mounted on your perception system for your

2:06:56.160 --> 2:07:01.040
 head mounted cameras, and then you've identified an object, you reach down to touch it, and the

2:07:01.040 --> 2:07:04.800
 last thing that happens right before the most important time, you stick your hand and you're

2:07:04.800 --> 2:07:11.120
 occluding your head mounted sensors. In all the part that really matters, all of your offboard

2:07:11.120 --> 2:07:17.680
 sensors are occluded. Really, if you don't have tactile information, then you're blind in an

2:07:17.680 --> 2:07:24.160
 important way. It happens that soft robotics and tactile sensing tend to go hand in hand.

2:07:24.160 --> 2:07:30.800
 I think we've kind of talked about it, but you taught a course on underactuated robotics.

2:07:30.800 --> 2:07:38.000
 I believe that was the name of it, actually. Can you talk about it in that context? What is

2:07:38.000 --> 2:07:45.680
 underactuated robotics? Underactuated robotics is my graduate course. It's online mostly now,

2:07:46.480 --> 2:07:48.560
 in the sense that the lectures. Several versions of it, I think.

2:07:48.560 --> 2:07:54.400
 Right. It's really great. I recommend it highly. Look on YouTube for the 2020 versions

2:07:55.280 --> 2:07:58.960
 until March, and then you have to go back to 2019, thanks to COVID.

2:08:00.880 --> 2:08:06.960
 No, I've poured my heart into that class. Lecture one is basically explaining what the

2:08:06.960 --> 2:08:12.320
 word underactuated means. People are very kind to show up and then maybe have to learn

2:08:12.320 --> 2:08:14.880
 what the title of the course means over the course of the first lecture.

2:08:14.880 --> 2:08:18.320
 That first lecture is really good. You should watch it.

2:08:20.000 --> 2:08:28.080
 It's a strange name, but I thought it captured the essence of what control was good at doing

2:08:28.080 --> 2:08:34.640
 and what control was bad at doing. What do I mean by underactuated? A mechanical system

2:08:36.640 --> 2:08:41.360
 has many degrees of freedom, for instance. I think of a joint as a degree of freedom,

2:08:41.360 --> 2:08:49.360
 and it has some number of actuators, motors. If you have a robot that's bolted to the table

2:08:49.360 --> 2:08:55.920
 that has five degrees of freedom and five motors, then you have a fully actuated robot.

2:08:58.480 --> 2:09:02.480
 If you take away one of those motors, then you have an underactuated robot.

2:09:03.520 --> 2:09:07.920
 Why on earth? I have a good friend who likes to tease me. He said,

2:09:07.920 --> 2:09:11.360
 Russ, if you had more research funding, would you work on fully actuated robots?

2:09:14.000 --> 2:09:19.280
 The answer is no. The world gives us underactuated robots, whether we like it or not. I'm a human.

2:09:19.920 --> 2:09:25.360
 I'm an underactuated robot, even though I have more muscles than my big degrees of freedom,

2:09:25.360 --> 2:09:29.680
 because I have in some places multiple muscles attached to the same joint.

2:09:30.960 --> 2:09:36.160
 But still, there's a really important degree of freedom that I have, which is the location of my

2:09:36.160 --> 2:09:44.560
 center of mass in space, for instance. I can jump into the air, and there's no motor that connects

2:09:44.560 --> 2:09:50.080
 my center of mass to the ground, in that case. I have to think about the implications of not

2:09:50.080 --> 2:09:56.640
 having control over everything. The passive dynamic walkers are the extreme view of that,

2:09:56.640 --> 2:09:59.440
 where you've taken away all the motors, and you have to let physics do the work.

2:10:00.080 --> 2:10:04.640
 But it shows up in all of the walking robots, where you have to use some of the actuators

2:10:04.640 --> 2:10:08.880
 to push and pull even the degrees of freedom that you don't have an actuator on.

2:10:10.160 --> 2:10:15.200
 That's referring to walking if you're falling forward. Is there a way to walk that's fully

2:10:15.200 --> 2:10:24.080
 actuated? It's a subtle point. When you're in contact and you have your feet on the ground,

2:10:24.080 --> 2:10:28.640
 there are still limits to what you can do. Unless I have suction cups on my feet,

2:10:29.280 --> 2:10:33.920
 I cannot accelerate my center of mass towards the ground faster than gravity,

2:10:33.920 --> 2:10:39.600
 because I can't get a force pushing me down. But I can still do most of the things that I want to.

2:10:39.600 --> 2:10:44.000
 So you can get away with basically thinking of the system as fully actuated, unless you

2:10:44.000 --> 2:10:50.880
 suddenly needed to accelerate down super fast. But as soon as I take a step, I get into more

2:10:50.880 --> 2:10:59.360
 nuanced territory. And to get to really dynamic robots, or airplanes, or other things, I think

2:10:59.360 --> 2:11:05.120
 you have to embrace the underactuated dynamics. Manipulation, people think is manipulation

2:11:05.120 --> 2:11:11.760
 underactuated? Even if my arm is fully actuated, I have a motor, if my goal is to control

2:11:12.640 --> 2:11:19.280
 the position and orientation of this cup, then I don't have an actuator for that directly. So I

2:11:19.280 --> 2:11:25.040
 have to use my actuators over here to control this thing. Now it gets even worse, like what if I have

2:11:25.040 --> 2:11:33.600
 to button my shirt? What are the degrees of freedom of my shirt? That's a hard question

2:11:33.600 --> 2:11:38.640
 to think about. It kind of makes me queasy as thinking about my state space control ideas.

2:11:40.480 --> 2:11:44.400
 But actually, those are the problems that make me so excited about manipulation right now,

2:11:44.400 --> 2:11:50.880
 is that it breaks a lot of the foundational control stuff that I've been thinking about.

2:11:50.880 --> 2:11:57.120
 Is there what are some interesting insights you can say about trying to solve an underactuated

2:11:58.000 --> 2:12:05.040
 control in an underactuated system? So I think the philosophy there is let

2:12:05.040 --> 2:12:12.720
 physics do more of the work. The technical approach has been optimization. So you typically

2:12:12.720 --> 2:12:17.600
 formulate your decision making for control as an optimization problem. And you use the

2:12:17.600 --> 2:12:23.920
 language of optimal control and sometimes often numerical optimal control in order to make those

2:12:23.920 --> 2:12:31.520
 decisions and balance these complicated equations and in order to control. You don't have to use

2:12:31.520 --> 2:12:36.880
 optimal control to do underactuated systems, but that has been the technical approach that has

2:12:36.880 --> 2:12:44.640
 borne the most fruit at least in our line of work. So in underactuated systems, when you say

2:12:44.640 --> 2:12:51.280
 let physics do some of the work, so there's a kind of feedback loop that observes the state

2:12:52.000 --> 2:12:59.440
 that the physics brought you to. So there's a perception there. There's a feedback

2:12:59.440 --> 2:13:06.160
 somehow. Do you ever loop in complicated perception systems into this whole picture?

2:13:06.720 --> 2:13:10.880
 Right. Right around the time of the DARPA challenge, we had a complicated perception

2:13:10.880 --> 2:13:16.800
 system in the DARPA challenge. We also started to embrace perception for our flying vehicles at

2:13:16.800 --> 2:13:22.480
 the time. We had a really good project on trying to make airplanes fly at high speeds through

2:13:22.480 --> 2:13:30.800
 forests. Sir Tash Karaman was on that project, and it was a really fun team to work on. He's

2:13:30.800 --> 2:13:35.840
 carried it much farther forward since then. And that's using cameras for perception?

2:13:35.840 --> 2:13:42.320
 So that was using cameras. At the time, we felt like LiDAR was too heavy and too power

2:13:44.400 --> 2:13:50.240
 heavy to be carried on a light UAV, and we were using cameras. And that was a big part of it,

2:13:50.240 --> 2:13:56.080
 was just how do you do even stereo matching at a fast enough rate with a small camera,

2:13:56.080 --> 2:14:03.280
 a small onboard compute. Since then, we have now, so the deep learning revolution unquestionably

2:14:03.280 --> 2:14:10.000
 changed what we can do with perception for robotics and control. So in manipulation,

2:14:10.000 --> 2:14:16.720
 we can address, we can use perception, you know, I think a much deeper way. And we get into not

2:14:16.720 --> 2:14:23.360
 only, I think the first use of it naturally would be to ask your deep learning system to

2:14:23.360 --> 2:14:28.160
 look at the cameras and produce the state, which is like the pose of my thing, for instance.

2:14:28.160 --> 2:14:33.280
 But I think we've quickly found out that that's not always the right thing to do.

2:14:34.320 --> 2:14:40.560
 Why is that? Because what's the state of my shirt? Imagine, I'm very noisy, I mean,

2:14:42.640 --> 2:14:48.480
 if the first step of me trying to button my shirt is estimate the full state of my shirt,

2:14:48.480 --> 2:14:50.880
 including like what's happening in the back, you know, whatever, whatever.

2:14:50.880 --> 2:14:57.920
 Yeah. That's just not the right specification. There are aspects of the state that are very

2:14:57.920 --> 2:15:05.040
 important to the task. There are many that are unobservable and not important to the task.

2:15:05.680 --> 2:15:11.920
 So you really need, it begs new questions about state representation. Another example that we've

2:15:11.920 --> 2:15:18.400
 been playing with in lab has been just the idea of chopping onions, okay? Or carrots,

2:15:18.400 --> 2:15:25.120
 turns out to be better. So onions stink up the lab and they're hard to see in a camera.

2:15:27.440 --> 2:15:30.240
 The details matter, yeah. Details matter, you know. So,

2:15:32.800 --> 2:15:36.640
 if I'm moving around a particular object, right, then I think about, oh, it's got a position

2:15:36.640 --> 2:15:42.640
 on orientation and space, that's the description I want. Now, when I'm chopping an onion, okay,

2:15:42.640 --> 2:15:49.520
 the first chop comes down. I have now a hundred pieces of onion. Does my control system really

2:15:49.520 --> 2:15:53.840
 need to understand the position and orientation and even the shape of the hundred pieces of onion

2:15:53.840 --> 2:15:58.800
 in order to make a decision? Probably not, you know, and like, if I keep going, I'm just getting,

2:15:58.800 --> 2:16:06.160
 more and more is my state space getting bigger as I cut. It's not right. So,

2:16:06.160 --> 2:16:15.520
 somehow there's, I think there's a richer idea of state. It's not the state that is given to us

2:16:15.520 --> 2:16:21.120
 by Lagrangian mechanics. There is a, there is a proper Lagrangian state of the system,

2:16:21.120 --> 2:16:28.400
 but the relevant state for this is some latent state is what we call it in machine learning.

2:16:28.400 --> 2:16:32.000
 But, you know, there's some, some different state representation.

2:16:32.000 --> 2:16:38.080
 Some compressed representation. And that's what I worry about saying compressed because it doesn't,

2:16:38.080 --> 2:16:44.800
 I don't mind that it's low dimensional or not, but it has to be something that's easier to think

2:16:44.800 --> 2:16:54.000
 about. By us humans. Or my algorithms. Or the algorithms being like control optimal. So, for

2:16:54.000 --> 2:16:59.520
 instance, if the contact mechanics of all of those onion pieces and all the permutations

2:16:59.520 --> 2:17:04.160
 of possible touches between those onion pieces, you know, you can give me a high dimensional

2:17:04.160 --> 2:17:08.640
 state representation, I'm okay if it's linear. But if I have to think about all the possible

2:17:08.640 --> 2:17:14.000
 shattering combinatorics of that, then my robot's going to sit there thinking and

2:17:15.360 --> 2:17:21.280
 the soup's going to get cold or something. So, since you taught the course, it kind of entered my

2:17:21.280 --> 2:17:27.840
 mind, the idea of under actuated as really compelling to see the, to see the world in this

2:17:27.840 --> 2:17:34.160
 kind of way. Do you ever, you know, if we talk about onions or you talk about the world with

2:17:34.160 --> 2:17:40.080
 people in it, in general, do you see the world as a basically an under actuated system? Do you

2:17:40.080 --> 2:17:44.320
 like often look at the world in this way? Or is this overreach?

2:17:46.880 --> 2:17:51.120
 Under actuated as a way of life, man. Exactly. I guess that's what I'm asking.

2:17:51.120 --> 2:17:56.880
 I do think it's everywhere. I think some, in some places,

2:17:58.640 --> 2:18:02.480
 we already have natural tools to deal with it. You know, it rears its head. I mean,

2:18:02.480 --> 2:18:07.600
 in linear systems, it's not a problem. We just, like an under actuated linear system is really

2:18:07.600 --> 2:18:13.440
 not sufficiently distinct from a fully actuated linear system. It's a subtle point about when

2:18:13.440 --> 2:18:18.400
 that becomes a bottleneck in what we know how to do with control. It happens to be a bottleneck.

2:18:18.400 --> 2:18:24.400
 Although we've gotten incredibly good solutions now, but for a long time that I felt that that was

2:18:24.400 --> 2:18:29.200
 the key bottleneck in legged robots. And roughly now, the under actuated course is,

2:18:30.240 --> 2:18:36.960
 you know, me trying to tell people everything I can about how to make Atlas do a backflip, right?

2:18:38.240 --> 2:18:42.880
 I have a second course now in that I teach in the other semesters, which is on manipulation.

2:18:43.440 --> 2:18:48.000
 And that's where we get into now more of the, that's a newer class. I'm hoping to put it online

2:18:48.000 --> 2:18:55.280
 this fall completely. And that's going to have much more aspects about these perception problems

2:18:55.280 --> 2:18:59.440
 and the state representation questions. And then how do you do control? And the,

2:19:00.640 --> 2:19:07.280
 the thing that's a little bit sad is that for me, at least, is there's a lot of manipulation tasks

2:19:07.280 --> 2:19:11.440
 that people want to do and should want to do. They could start a company with it and be very

2:19:11.440 --> 2:19:17.040
 successful that don't actually require you to think that much about under act or dynamics at all,

2:19:17.040 --> 2:19:22.880
 even, but certainly under actuated dynamics. Once I have, if I, if I reach out and grab something,

2:19:22.880 --> 2:19:26.320
 if it, if I can sort of assume it's rigidly attached to my hand, then I can do a lot of

2:19:26.320 --> 2:19:31.200
 interesting meaningful things with it without really ever thinking about the dynamics of that

2:19:31.200 --> 2:19:37.760
 object. So they built, we've built systems that kind of reduce the need for that,

2:19:39.040 --> 2:19:44.400
 enveloping grasps and the like. But I think the really good problems in manipulation. So

2:19:44.400 --> 2:19:50.960
 I, manipulation, by the way, is more than just pick and place. That's like a lot of people think

2:19:50.960 --> 2:19:56.800
 of that, just grasping. I don't mean that. I mean, buttoning my shirt. I mean, tying shoelaces.

2:19:57.600 --> 2:20:03.280
 How do you program a robot to tie shoelaces and not just one shoe, but every shoe, right?

2:20:04.000 --> 2:20:09.360
 That's a really good problem. It's tempting to write down like the infinite dimensional

2:20:09.360 --> 2:20:16.640
 state of the, of the laces. That's probably not needed to write a good controller. I know we

2:20:16.640 --> 2:20:21.040
 could hand design a controller that would do it, but I don't want that. I want to understand the

2:20:21.040 --> 2:20:27.040
 principles that would allow me to solve another problem that's kind of like that. But I think

2:20:27.840 --> 2:20:35.440
 if we can stay pure in our approach, then the challenge of tying anybody's shoes is a great

2:20:35.440 --> 2:20:40.960
 challenge. That's a great challenge. I mean, and the soft touch comes into play there. That's

2:20:40.960 --> 2:20:48.320
 really interesting. Let me ask another ridiculous question on this topic. How important is

2:20:49.280 --> 2:20:54.400
 touch? We haven't talked much about humans, but I have this argument with my dad,

2:20:56.320 --> 2:21:03.600
 where like, I think you can fall in love with a robot based on language alone. And he believes

2:21:03.600 --> 2:21:14.240
 that touch is essential. Touch and smell, he says, but so in terms of robots, you know, connecting

2:21:14.240 --> 2:21:20.560
 with humans and we can go philosophical in terms of like a deep meaningful connection,

2:21:20.560 --> 2:21:26.960
 like love, but even just like collaborating in an interesting way. How important is touch? Like

2:21:26.960 --> 2:21:31.680
 from an engineering perspective and a philosophical one?

2:21:32.640 --> 2:21:38.480
 I think it's super important. Even just in a practical sense, if we forget about the emotional

2:21:38.480 --> 2:21:46.080
 part of it, but for robots to interact safely while they're doing meaningful mechanical work

2:21:47.120 --> 2:21:54.800
 in the close contact with or vicinity of people that need help, I think we have to have them,

2:21:54.800 --> 2:22:00.000
 we have to build them differently. They have to be afraid, not afraid of touching the world. So

2:22:01.200 --> 2:22:07.680
 I think Baymax is just awesome. That's just like the movie of Big Hero 6 and the concept of Baymax,

2:22:07.680 --> 2:22:13.600
 that's just awesome. I think we should, and we have some folks at Toyota that are trying to,

2:22:13.600 --> 2:22:20.160
 Toyota Research that are trying to build Baymax roughly. And I think it's just a fantastically

2:22:20.160 --> 2:22:26.080
 good project. I think it will change the way people physically interact. The same way,

2:22:26.080 --> 2:22:31.840
 I mean, you gave a couple of examples earlier, but if the robot that was walking around my home

2:22:31.840 --> 2:22:37.440
 looked more like a teddy bear and a little less like the Terminator, that could change completely

2:22:37.440 --> 2:22:42.720
 the way people perceive it and interact with it. And maybe they'll even want to teach it, like you

2:22:42.720 --> 2:22:50.480
 said, right? You could not quite gamify it, but somehow instead of people judging it and looking

2:22:50.480 --> 2:22:55.680
 at it as if it's not doing as well as a human, they're going to try to help out the cute teddy

2:22:55.680 --> 2:23:03.120
 bear, right? Who knows? But I think we're building robots wrong and being more soft and more

2:23:04.000 --> 2:23:11.120
 contact is important, right? Yeah, like all the magical moments I can remember with robots,

2:23:11.120 --> 2:23:18.960
 well, first of all, just visiting your lab and seeing Atlas, but also Spotmini. When I first

2:23:18.960 --> 2:23:28.240
 saw Spotmini in person and hung out with him, her, it, I don't have trouble in gendering robots.

2:23:28.240 --> 2:23:34.160
 I feel robotics people really say, oh, is it it? I kind of like the idea that it's a her or a him.

2:23:34.160 --> 2:23:40.400
 There's a magical moment, but there's no touching. I guess the question I have, have you ever been,

2:23:41.520 --> 2:23:50.720
 like, have you had a human robot experience where like a robot touched you? And like it was like,

2:23:50.720 --> 2:23:57.920
 wait, like, was there a moment that you've forgotten that a robot is a robot? And like the

2:23:57.920 --> 2:24:05.200
 anthropomorphization stepped in and for a second you forgot that it's not human? I mean, I think

2:24:05.200 --> 2:24:12.560
 when you're in on the details, then we, of course, anthropomorphized our work with Atlas, but in,

2:24:13.680 --> 2:24:20.240
 you know, in verbal communication and the like, I think we were pretty aware of it as a machine

2:24:20.240 --> 2:24:27.600
 that needed to be respected. I actually, I worry more about the smaller robots that could still,

2:24:27.600 --> 2:24:32.080
 you know, move quickly if programmed wrong. And we have to be careful, actually, about safety

2:24:32.080 --> 2:24:38.000
 and the like right now. And that if we build our robots correctly, I think then those, a lot of

2:24:38.000 --> 2:24:42.640
 those concerns could go away. And we're seeing that trend. We're seeing the lower cost, lighter

2:24:42.640 --> 2:24:52.240
 weight arms now that could be fundamentally safe. I mean, I do think touch is so fundamental. Ted

2:24:52.240 --> 2:25:01.280
 Adelson is great. He's a perceptual scientist at MIT. And he studied vision most of his life.

2:25:01.280 --> 2:25:07.520
 And he said, when I had kids, I expected to be fascinated by their perceptual development.

2:25:09.600 --> 2:25:15.040
 But what really, what he noticed was felt more impressive, more dominant was the way that they

2:25:15.040 --> 2:25:19.280
 would touch everything and lick everything and pick things up to get on their tongue and whatever.

2:25:19.280 --> 2:25:27.040
 And he said, watching his daughter convinced him that actually he needed to study tactile

2:25:27.040 --> 2:25:35.600
 sensing more. So there's something very important. I think it's a little bit also of the passive

2:25:35.600 --> 2:25:41.040
 versus active part of the world, right? You can passively perceive the world.

2:25:43.840 --> 2:25:47.840
 But it's fundamentally different if you can do an experiment, right? And if you can change the

2:25:47.840 --> 2:25:56.080
 world, you can learn a lot more than a passive observer. So you can in dialogue, that was your

2:25:56.080 --> 2:26:01.760
 initial example, you could have an active experiment exchange. But I think if you're just a camera

2:26:01.760 --> 2:26:08.320
 watching YouTube, I think that's a very different problem than if you're a robot that can apply

2:26:08.320 --> 2:26:17.360
 force and touch. I think it's important. Yeah, I think it's just an exciting area of

2:26:17.360 --> 2:26:24.000
 research. I think you're probably right that this hasn't been under researched. It's, to me,

2:26:24.000 --> 2:26:28.320
 as a person who's captivated by the idea of human robot interaction, it feels like

2:26:30.240 --> 2:26:36.400
 such a rich opportunity to explore touch, not even from a safety perspective, but like you said,

2:26:36.400 --> 2:26:42.480
 the emotional too. I mean, safety comes first. But the next step is like,

2:26:42.480 --> 2:26:50.400
 like, you know, like a real human connection, even in the world, like even in the industrial

2:26:50.400 --> 2:26:58.000
 setting, it just feels like it's nice for the robot. I don't know, you might disagree with this,

2:26:58.000 --> 2:27:06.560
 but because I think it's important to see robots as tools often. But I don't know. I think they're

2:27:06.560 --> 2:27:13.520
 just always going to be more effective once you humanize them. Like, it's convenient now to think

2:27:13.520 --> 2:27:18.880
 of them as tools because we want to focus on the safety. But I think ultimately, to create

2:27:19.840 --> 2:27:28.080
 like a good experience for the worker, for the person, there has to be a human element. I don't

2:27:28.080 --> 2:27:34.720
 know, for me, it feels like like an industrial robotic arm would be better if has a human element.

2:27:34.720 --> 2:27:40.480
 I think like rethink robotics had that idea with the Baxter and having eyes and so on having,

2:27:41.120 --> 2:27:47.760
 I don't know, I'm a big believer in that. It's not my area, but I am also a big believer.

2:27:49.040 --> 2:27:54.800
 Do you have an emotional connection to Alice? Do you miss them?

2:27:54.800 --> 2:28:04.800
 I mean, yes, I don't know if I'd more so than if I had a different science project that I'd

2:28:04.800 --> 2:28:14.160
 worked on super hard, right? But yeah, I mean, the robot, we basically had to do heart surgery

2:28:14.160 --> 2:28:21.920
 on the robot in the final competition because we melted the core. And yeah, there was something

2:28:21.920 --> 2:28:25.920
 about watching that robot hanging there. We know we had to compete with it in an hour and it was

2:28:25.920 --> 2:28:32.240
 getting its guts ripped out. Those are all historic moments. I think if you look back like 100 years

2:28:32.240 --> 2:28:39.440
 from now, yeah, I think those are important moments in robotics. I mean, these are the early

2:28:39.440 --> 2:28:43.600
 day, you look at like the early days of a lot of scientific disciplines, they look ridiculous,

2:28:43.600 --> 2:28:52.560
 it's full of failure. But it feels like robotics will be important in the coming 100 years. And

2:28:52.560 --> 2:29:00.400
 these are the early days. So I think a lot of people are, look at a brilliant person such as

2:29:00.400 --> 2:29:08.240
 yourself and are curious about the intellectual journey they've took. Is there maybe three books,

2:29:08.240 --> 2:29:14.640
 technical fiction, philosophical, that had a big impact on your life that you would recommend,

2:29:15.280 --> 2:29:22.160
 perhaps others reading? Yeah, so I actually didn't read that much as a kid, but I read

2:29:22.800 --> 2:29:30.080
 fairly voraciously now. There are some recent books that if you're interested in this kind of

2:29:30.080 --> 2:29:40.000
 topic, like AI Superpowers by Kaifuli is just a fantastic read. You must read that. Yuval Harari

2:29:40.960 --> 2:29:48.960
 is just, I think that can open your mind. Sapiens. Sapiens is the first one, Homo Deus is the

2:29:48.960 --> 2:29:55.760
 second. We mentioned The Black Swan by Talib. I think that's a good sort of mind opener.

2:29:55.760 --> 2:30:05.440
 I actually, so there's maybe a more controversial recommendation I could give.

2:30:05.440 --> 2:30:07.840
 Great. I would love that first.

2:30:09.040 --> 2:30:13.840
 In some sense, it's so classical, it might surprise you. But I actually recently read

2:30:14.560 --> 2:30:19.120
 Mortimer Adler's How to Read a Book not so long ago. It was a while ago, but

2:30:19.120 --> 2:30:28.480
 some people hate that book. I loved it. I think we're in this time right now where,

2:30:30.720 --> 2:30:35.440
 boy, we're just inundated with research papers that you could read on archive with

2:30:37.360 --> 2:30:40.560
 limited peer review and just this wealth of information.

2:30:40.560 --> 2:30:51.360
 I don't know. I think the passion of what you can get out of a book, a really good book or

2:30:51.360 --> 2:30:56.000
 a really good paper if you find it, the attitude, the realization that you're only going to find

2:30:56.000 --> 2:31:02.480
 a few that really are worth all your time. But then once you find them, you should just dig in

2:31:02.480 --> 2:31:11.200
 and understand it very deeply and it's worth marking it up and having the hard copy,

2:31:11.200 --> 2:31:21.040
 writing in the side notes, side margins. I read it at the right time where I was just

2:31:21.040 --> 2:31:28.400
 feeling just overwhelmed with really low quality stuff, I guess. And similarly,

2:31:28.400 --> 2:31:35.200
 I'm just giving more than three now. I'm sorry if I've exceeded my quota.

2:31:35.200 --> 2:31:42.480
 But on that topic just real quick is so basically finding a few companions to keep for the rest of

2:31:42.480 --> 2:31:49.040
 your life in terms of papers and books and so on. And those are the ones like not doing

2:31:50.800 --> 2:31:54.640
 what is it, foam wall fear, missing out, constantly trying to update yourself,

2:31:54.640 --> 2:31:59.280
 but really deeply making a life journey of studying a particular paper essentially,

2:31:59.920 --> 2:32:07.600
 set of papers. Yeah, I think when you really find something, a book that resonates with you

2:32:07.600 --> 2:32:12.720
 might not be the same book that resonates with me. But when you really find one that resonates

2:32:12.720 --> 2:32:17.280
 with you, I think the dialogue that happens and that's what I love that Adler was saying,

2:32:17.280 --> 2:32:26.160
 you know, I think Socrates and Plato say the written word is never going to capture the beauty

2:32:26.160 --> 2:32:34.480
 of dialogue, right? But Adler says, no, no, a really good book is a dialogue between you and

2:32:34.480 --> 2:32:40.560
 the author and it crosses time and space. And I don't know, I think it's a very romantic,

2:32:40.560 --> 2:32:46.240
 there's a bunch of like specific advice, which you can just gloss over, but the romantic view of

2:32:46.240 --> 2:32:53.840
 how to read and really appreciate it is so good. And similarly, teaching, I

2:32:57.280 --> 2:33:03.040
 thought a lot about teaching. And so Isaac Asimov, great science fiction writer,

2:33:03.040 --> 2:33:08.400
 has also actually spent a lot of his career writing nonfiction, right? His memoir is fantastic.

2:33:09.840 --> 2:33:13.760
 He was passionate about explaining things, right? He wrote all kinds of books on all

2:33:13.760 --> 2:33:21.280
 kinds of topics in science. He was known as the great explainer. And I do really resonate with

2:33:21.280 --> 2:33:30.400
 his style and just his way of talking about, by communicating and explaining to something

2:33:30.400 --> 2:33:36.400
 is really the way that you learn something. I think about problems very differently because

2:33:36.400 --> 2:33:43.280
 of the way I've been given the opportunity to teach them at MIT. And we have questions asked,

2:33:43.280 --> 2:33:49.280
 the fear of the lecture, the experience of the lecture and the questions I get and the interactions

2:33:50.080 --> 2:33:55.520
 just forces me to be rock solid on these ideas in a way that I didn't have that. I don't know,

2:33:56.160 --> 2:34:00.640
 I would be in a different intellectual space. Also video, does that scare you that your

2:34:00.640 --> 2:34:05.760
 lectures are online? And people like me in sweatpants can sit sipping coffee and watch

2:34:05.760 --> 2:34:12.640
 you give lectures that I think it's great. I do think that something's changed right now,

2:34:12.640 --> 2:34:18.000
 which is, you know, right now we're giving lectures over Zoom, I mean, giving seminars

2:34:18.000 --> 2:34:23.120
 over Zoom and everything. I'm trying to figure out, I think it's a new medium.

2:34:24.160 --> 2:34:34.400
 Do you think it's possible? Yeah, I've been, I've been quite cynical

2:34:34.400 --> 2:34:41.840
 about the human to human connection over that medium. But I think that's because it's

2:34:41.840 --> 2:34:44.880
 hasn't been explored fully. And teaching is a different thing.

2:34:45.680 --> 2:34:50.640
 Every lecture is a, I'm sorry, every seminar even, I think every talk I give,

2:34:52.240 --> 2:34:57.120
 you know, it's an opportunity to give that differently. I can deliver content directly

2:34:57.120 --> 2:35:04.400
 into your browser. You have a WebGL engine right there. I could, I can throw 3D content into your

2:35:04.400 --> 2:35:09.600
 browser while you're listening to me, right? Yeah. And I can assume that you have a, you know,

2:35:09.600 --> 2:35:13.840
 at least a powerful enough laptop or something to watch Zoom while I'm doing that while I'm

2:35:13.840 --> 2:35:20.160
 giving a lecture. That's a new communication tool that I didn't have last year, right? And

2:35:21.360 --> 2:35:24.960
 I think robotics can potentially benefit a lot from teaching that way.

2:35:24.960 --> 2:35:28.080
 Okay. We'll see. It's going to be an experiment this fall.

2:35:28.080 --> 2:35:29.760
 It's interesting. I'm thinking a lot about it.

2:35:30.320 --> 2:35:39.680
 Yeah. And also like the, the length of lectures or the length of like, there's something, so like,

2:35:39.680 --> 2:35:44.880
 I guarantee you, you know, it's like 80% of people who started listening to our conversation

2:35:44.880 --> 2:35:50.800
 are still listening to now, which is crazy to me. But so there's a, there's a patience and

2:35:50.800 --> 2:35:56.560
 interest in long form content, but at the same time, there's a magic to forcing yourself to

2:35:56.560 --> 2:36:05.200
 condense an idea to the shortest possible, uh, shortest possible like clip. It can be a part

2:36:05.200 --> 2:36:10.080
 of a longer thing, but like just a really beautifully condensed an idea. There's a lot of

2:36:10.080 --> 2:36:19.360
 a opportunity there that's easier to do in remote with, I don't know, uh, with editing too. Editing

2:36:19.360 --> 2:36:25.520
 is an interesting thing. Like what, uh, you know, when most professors don't get, when they give a

2:36:25.520 --> 2:36:30.880
 lecture, you don't get to go back and edit out parts like Chris, like Chris bit up a little bit.

2:36:31.600 --> 2:36:38.640
 That's also, it can do magic. Like if you remove like five to 10 minutes from an hour lecture,

2:36:39.520 --> 2:36:44.560
 it can, it can actually, it can make something special of a lecture. I've, uh, I've seen that

2:36:44.560 --> 2:36:50.480
 of myself and, and, and in others too, cause I added other people's lectures to extract clips.

2:36:50.480 --> 2:36:55.120
 It's like there's certain tangents they're like that lose, they're not interesting. They're,

2:36:55.120 --> 2:36:59.200
 they're, they're mumbling. They're just not, they're not clarifying. They're, they're not helpful

2:36:59.200 --> 2:37:04.960
 at all. And once you remove them, it's just, I don't know, editing can be magic. It takes a

2:37:04.960 --> 2:37:10.080
 lot of time. Yeah. It takes, it depends like what is teaching you have to ask. Yeah. Um,

2:37:10.080 --> 2:37:20.160
 um, um, yeah. Cause I find the editing process is also beneficial as, uh, for teaching, but also

2:37:20.160 --> 2:37:25.120
 for your own learning. I don't know if, have you watched yourself on the other day? Have you watched

2:37:25.120 --> 2:37:30.960
 those videos? It's, I mean, not all of them. It could be, it could be painful and to see like how

2:37:30.960 --> 2:37:37.920
 to improve. So do you find that, uh, I know you segment your, um, your podcast. Do you think that

2:37:37.920 --> 2:37:44.160
 it helps people with the, the attention span aspect of it? Or is it segment like sections? Like,

2:37:44.160 --> 2:37:49.360
 yeah, we're talking about this topic, whatever. Nope. Nope. That just helps me. It's actually bad.

2:37:49.360 --> 2:37:56.320
 So, uh, and you've been incredible. Um, so I'm, I'm learning, like I'm afraid of conversation.

2:37:56.320 --> 2:38:02.000
 This is even today. I'm terrified of talking to you. I mean, it's something I'm, um, trying to

2:38:02.000 --> 2:38:08.000
 remove for myself. I, there's, there's a guy, I mean, I've learned from a lot of people, but

2:38:08.000 --> 2:38:13.280
 really, um, there's been a few people who's been inspirational to me in terms of conversation.

2:38:14.000 --> 2:38:18.960
 Whatever people think of him, uh, Joe Rogan has been inspirational to me because, uh, comedians

2:38:18.960 --> 2:38:25.440
 have been to being able to just have fun and enjoy themselves and lose themselves in conversation

2:38:25.440 --> 2:38:31.520
 that requires you to be a great storyteller, to be able to, uh, pull a lot of different pieces of

2:38:31.520 --> 2:38:37.440
 information together, but mostly just to enjoy yourself in conversations. I'm trying to learn

2:38:37.440 --> 2:38:43.600
 that these notes are, you see me looking down. That's like a safety blanket that I'm trying to

2:38:43.600 --> 2:38:49.280
 let go of more and more. Cool. Um, so that's that people love just regular conversation.

2:38:49.280 --> 2:38:56.000
 That's, that's what they, the structure is like, whatever. Uh, I would say, I would say maybe

2:38:56.000 --> 2:39:02.880
 like 10 to like, so there's a bunch of, you know, there's, uh, probably a couple of thousand

2:39:02.880 --> 2:39:08.800
 PhD students listening to this right now, right? And they might know what we're talking about,

2:39:09.360 --> 2:39:16.400
 but there is somebody I guarantee you right now in Russia, some kid who's just like,

2:39:16.400 --> 2:39:22.480
 who's just smoked some weed is sitting back and just enjoying the hell out of this conversation,

2:39:22.480 --> 2:39:27.040
 not really understanding. You kind of watch some Boston Dynamics videos. He's just enjoying it.

2:39:27.600 --> 2:39:33.600
 Um, and I salute you, sir. Uh, no, but just like there's a, so much variety of people,

2:39:33.600 --> 2:39:40.080
 uh, that just have curiosity about engineering, about sciences, about mathematics and, um, and

2:39:40.080 --> 2:39:48.880
 also like I should, I mean, um, enjoying it is one thing, but also often notice it in inspires

2:39:48.880 --> 2:39:53.680
 people to, there's a lot of people who are like in their undergraduate studies, trying to figure

2:39:53.680 --> 2:40:00.000
 out what, uh, trying to figure out what to pursue. And these conversations can really spark the

2:40:00.000 --> 2:40:06.320
 direction of their, of their life. And in terms of robotics, I hope it does. Cause, uh, I'm excited

2:40:06.320 --> 2:40:14.160
 about the possibilities of robotics brings on that topic. Um, do you have advice? Like what advice

2:40:14.160 --> 2:40:21.680
 would you give to a young person about life? A young person about life or a young person about

2:40:21.680 --> 2:40:27.520
 life in robotics? Uh, it could be in robotics. It could be in life in general. It could be career.

2:40:28.400 --> 2:40:33.200
 It could be, uh, relationship advice. It could be running advice, just like

2:40:34.000 --> 2:40:39.680
 they're, um, that's one of the things I see, like we talked like 20 year olds. They're, they're like,

2:40:39.680 --> 2:40:46.960
 how do I, how do I do this thing? What, what do I do? Um, if they come up to you, what would you

2:40:46.960 --> 2:40:57.120
 tell them? I think it's an interesting time to be a kid these days. Everything points to this being

2:40:57.120 --> 2:41:03.760
 sort of a winner take all economy and the like, I think the people that will really excel in my

2:41:03.760 --> 2:41:12.080
 opinion are going to be the ones that can think deeply about problems. Um, you have to be able to

2:41:12.080 --> 2:41:16.400
 ask questions, agilely and use the internet for everything it's good for and stuff like this.

2:41:16.400 --> 2:41:24.320
 And I think a lot of people will develop those skills. I think the leaders, thought leaders,

2:41:24.320 --> 2:41:29.520
 you know, robotics leaders, whatever, are going to be the ones that can do more and they can think

2:41:29.520 --> 2:41:36.240
 very deeply and critically. Um, and that's a harder thing to learn. I think one, one path to

2:41:36.240 --> 2:41:42.960
 learning that is through mathematics, through engineering. Um, I would encourage people to

2:41:42.960 --> 2:41:49.920
 start math early. I mean, I didn't really start. I mean, I, I was always in the, the better math

2:41:49.920 --> 2:41:55.600
 classes that I could take, but I wasn't pursuing super advanced mathematics or anything like that

2:41:55.600 --> 2:42:04.000
 until I got to MIT. I think MIT lit me up and, uh, really started the life that I'm living now.

2:42:05.520 --> 2:42:12.400
 But, uh, yeah, I really want kids to, to dig deep, really understand things, building things too.

2:42:12.400 --> 2:42:17.520
 I mean, pull things apart, put them back together. Like that's just such a good way to really

2:42:17.520 --> 2:42:26.240
 understand things and expect it to be a long journey, right? It's, uh, you don't have to

2:42:26.240 --> 2:42:28.240
 know everything. You're never going to know everything.

2:42:29.280 --> 2:42:31.200
 So think deeply and stick with it.

2:42:32.720 --> 2:42:39.760
 Enjoy the ride, but just make sure you're not, um, yeah, just, just make sure you're, you're,

2:42:39.760 --> 2:42:42.480
 you're stopping to think about why things work.

2:42:42.480 --> 2:42:48.720
 And it's true. It's, uh, it's easy to lose yourself in the, in the, in the distractions of the world.

2:42:50.960 --> 2:42:52.800
 We're overwhelmed with content right now, but

2:42:54.560 --> 2:42:57.600
 you have to stop and pick some of it and, and really understand it.

2:42:58.640 --> 2:43:05.920
 Yeah. On the book point, I've read, um, Animal Farm by George Orwell, a ridiculous number of times.

2:43:05.920 --> 2:43:10.320
 So for me, like that book, I don't know if it's a good book in general, but for me,

2:43:10.320 --> 2:43:18.080
 it connects deeply somehow. Uh, it somehow connects. So I was born in the Soviet Union,

2:43:18.080 --> 2:43:23.040
 so it connects to me to the entirety of the history of the Soviet Union and to World War II,

2:43:23.040 --> 2:43:32.320
 and to the love and hatred and suffering that went on there and the, uh, the corrupting nature of

2:43:32.320 --> 2:43:37.920
 power and greed. And just somehow I just, that, that, that book has taught me more about life

2:43:37.920 --> 2:43:45.040
 than like anything else, even though it's just like a silly, like childlike book about pigs.

2:43:46.240 --> 2:43:52.320
 I don't know why it just connects and inspires. And the same, there's a few, um, yeah, there's a

2:43:52.320 --> 2:43:58.880
 few technical books too and algorithms that just, yeah, you return too often. Right. I'm, I'm, I'm

2:43:58.880 --> 2:44:05.200
 with you. Um, yeah, there's, uh, I don't, and I've been losing that because of the internet.

2:44:05.200 --> 2:44:11.680
 I've been like, uh, going on, I've been going on archive and blog posts and GitHub and, and the

2:44:11.680 --> 2:44:19.920
 new thing and, uh, you lose your ability to really master an idea. Right. Well, exactly right.

2:44:20.960 --> 2:44:31.280
 What's the fond memory from childhood when baby Russ Tedrick? Well, I guess I just said that, um,

2:44:31.280 --> 2:44:37.920
 um, at least my current life begins, began when I got to MIT. If I have to go farther than that.

2:44:38.720 --> 2:44:45.760
 Yeah. What was, was there a life before MIT? Oh, absolutely. But, but let me actually tell

2:44:47.040 --> 2:44:52.400
 you what happened when I first got to MIT. Cause that, I think might be relevant here. But I,

2:44:53.760 --> 2:44:58.640
 you know, I had taken a computer engineering degree at Michigan. I enjoyed it immensely,

2:44:58.640 --> 2:45:06.000
 learned a bunch of stuff. I was, I liked computers. I liked programming. Um, but when I did get to

2:45:06.000 --> 2:45:11.520
 MIT and started working with Sebastian Song, theoretical physicist, computational neuroscientist,

2:45:14.240 --> 2:45:20.080
 the culture here was just different. Um, it demanded more of me, certainly mathematically

2:45:20.080 --> 2:45:27.920
 and in the critical thinking. And I remember the day that I, uh, to borrowed one of the books from

2:45:27.920 --> 2:45:33.200
 my advisor's office and walked down to the Charles River and was like, I'm getting my butt kicked,

2:45:33.200 --> 2:45:39.840
 you know? Um, and I think that's going to happen to everybody who's doing this kind of stuff.

2:45:39.840 --> 2:45:46.800
 Right. I think, uh, I expected you to ask me the meaning of life. You know, I think that the, uh,

2:45:48.640 --> 2:45:51.680
 somehow I think that's, that's got to be part of it. This.

2:45:51.680 --> 2:45:58.000
 I'm doing hard things. Yeah. Did you, uh, did you consider quitting at any point?

2:45:58.000 --> 2:46:02.160
 Did you consider this isn't for me? No, never that. I mean, I was,

2:46:03.600 --> 2:46:07.600
 I was working hard, but I was loving it. Right. I mean, there's, I think the,

2:46:07.600 --> 2:46:11.920
 there's this magical thing where you, uh, you know, I'm lucky to surround myself with people that

2:46:11.920 --> 2:46:18.960
 basically almost every day I'll, I'll, I'll see something. I'll be told something or something

2:46:18.960 --> 2:46:24.000
 that I realized, wow, I don't understand that. And if I could just understand that there's,

2:46:24.000 --> 2:46:28.960
 there's something else to learn that if I could just learn that thing, I would connect another

2:46:28.960 --> 2:46:36.400
 piece of the puzzle. And, and, uh, you know, I think that is just such an important aspect and

2:46:36.400 --> 2:46:42.800
 being willing to understand what you can and can't do and, and loving the journey of going

2:46:43.360 --> 2:46:45.680
 and learning those other things. I think that's the best part.

2:46:45.680 --> 2:46:52.880
 I don't think there's a better way to end it for us. I've, um, you've been an inspiration to me

2:46:52.880 --> 2:46:58.320
 since I showed up at MIT. Uh, your work has been an inspiration to the world. This conversation

2:46:58.320 --> 2:47:03.600
 was amazing. I can't wait to see what you do next with robotics, home robots. I,

2:47:03.600 --> 2:47:08.000
 I hope to see you work in my home one day. So thanks so much for talking today. It's been awesome.

2:47:08.000 --> 2:47:14.000
 Cheers. Thanks for listening to this conversation with Rostedrik and thank you to our sponsors,

2:47:14.000 --> 2:47:20.480
 Magic Spoon Serial, BetterHelp and ExpressVPN. Please consider supporting this podcast by going

2:47:20.480 --> 2:47:27.760
 to magicspoon.com slash Lex and using code Lex at checkout, going to betterhelp.com slash Lex

2:47:27.760 --> 2:47:36.080
 and signing up at expressvpn.com slash Lex pod. Click the links, buy the stuff, get the discount.

2:47:36.080 --> 2:47:41.360
 It really is the best way to support this podcast. If you enjoy this thing, subscribe on YouTube,

2:47:41.360 --> 2:47:46.400
 review it with five stars and up a podcast, support on Patreon or connect with me on Twitter

2:47:46.400 --> 2:47:54.320
 at Lex Friedman spelled somehow without the E just F R I D M A N. And now let me leave you

2:47:54.320 --> 2:47:59.920
 with some words from Neil deGrasse Tyson talking about robots in space and the emphasis we humans

2:47:59.920 --> 2:48:07.760
 put on human based space exploration. Robots are important. If I don my pure scientist hat,

2:48:07.760 --> 2:48:13.840
 I would say just send robots. I'll stay down here and get the data. But nobody's ever given a parade

2:48:13.840 --> 2:48:20.000
 for a robot. Nobody's ever named a high school after a robot. So when I don my public educator hat,

2:48:20.000 --> 2:48:25.600
 I have to recognize the elements of exploration that excite people. It's not only the discoveries

2:48:25.600 --> 2:48:31.280
 and the beautiful photos that come down from the heavens. It's the vicarious participation

2:48:31.280 --> 2:48:38.960
 in discovery itself. Thank you for listening and hope to see you next time.

